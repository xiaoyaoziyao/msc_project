<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group>
<journal-title>PLoS ONE</journal-title></journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-13-04545</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0067343</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Computational biology</subject></subj-group><subj-group><subject>Molecular cell biology</subject><subj-group><subject>Cytometry</subject></subj-group></subj-group><subj-group><subject>Proteomics</subject><subj-group><subject>Protein abundance</subject><subject>Proteomic databases</subject></subj-group></subj-group><subj-group><subject>Systems biology</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Computer science</subject><subj-group><subject>Algorithms</subject></subj-group><subj-group><subject>Computer modeling</subject></subj-group><subj-group><subject>Computing methods</subject><subj-group><subject>Computer inferencing</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Mining Proteins with Non-Experimental Annotations Based on an Active Sample Selection Strategy for Predicting Protein Subcellular Localization</article-title>
<alt-title alt-title-type="running-head">Mining Non-Experimental Proteins</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Cao</surname><given-names>Junzhe</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Liu</surname><given-names>Wenqi</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>He</surname><given-names>Jianjun</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Gu</surname><given-names>Hong</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>School of Control Science and Engineering, Dalian University of Technology, Dalian, China</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>College of Information and Communication Engineering, Dalian Nationalities University, Dalian, China</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>te Pas</surname><given-names>Marinus F.W.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Wageningen UR Livestock Research, The Netherlands</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">wqliu@dlut.edu.cn</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: JC WL. Performed the experiments: JC JH. Analyzed the data: JC WL. Contributed reagents/materials/analysis tools: HG. Wrote the paper: JC. Designed the program used in experiments: JC JH.</p></fn>
</author-notes>
<pub-date pub-type="collection"><year>2013</year></pub-date>
<pub-date pub-type="epub"><day>26</day><month>6</month><year>2013</year></pub-date>
<volume>8</volume>
<issue>6</issue>
<elocation-id>e67343</elocation-id>
<history>
<date date-type="received"><day>23</day><month>1</month><year>2013</year></date>
<date date-type="accepted"><day>16</day><month>5</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2013</copyright-year>
<copyright-holder>Cao et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Subcellular localization of a protein is important to understand proteins’ functions and interactions. There are many techniques based on computational methods to predict protein subcellular locations, but it has been shown that many prediction tasks have a training data shortage problem. This paper introduces a new method to mine proteins with non-experimental annotations, which are labeled by non-experimental evidences of protein databases to overcome the training data shortage problem. A novel active sample selection strategy is designed, taking advantage of active learning technology, to actively find useful samples from the entire data pool of candidate proteins with non-experimental annotations. This approach can adequately estimate the “value” of each sample, automatically select the most valuable samples and add them into the original training set, to help to retrain the classifiers. Numerical experiments with for four popular multi-label classifiers on three benchmark datasets show that the proposed method can effectively select the valuable samples to supplement the original training set and significantly improve the performances of predicting classifiers.</p>
</abstract>
<funding-group><funding-statement>This work was supported by a Specialized Research Fund for the Doctoral Program of Higher Education (Grant No. 20120041110008), National Science and Technology Mega-Project Program of China (Grant No. 2011ZX09101-008-09), National Natural Science Foundation of China (Grant No. 61174027), and the Program for Liaoning Excellent Talents in University (Grant No. LJQ2012005). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="9"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>A good understanding of protein subcellular location is a key for deducing protein functions, revealing disease pathogenesis, and identifying drag targets. In the last ten years, the rapid growth of protein data has made it faster and more economical to predict subcellular localization via computational methods. Since the first protein location prediction system emerged <xref ref-type="bibr" rid="pone.0067343-Nakai1">[1]</xref>, many prediction approaches and predictors have been proposed. These methods are mostly based on classification algorithms, e.g. k-nearest neighbor (KNN) <xref ref-type="bibr" rid="pone.0067343-Chou1">[2]</xref>–<xref ref-type="bibr" rid="pone.0067343-Cao1">[5]</xref>, support vector machine (SVM) <xref ref-type="bibr" rid="pone.0067343-Gray1">[6]</xref>–<xref ref-type="bibr" rid="pone.0067343-Briesemeister1">[8]</xref>, Bayesian methods <xref ref-type="bibr" rid="pone.0067343-Bulashevska1">[9]</xref>, <xref ref-type="bibr" rid="pone.0067343-He1">[10]</xref>, and neural network <xref ref-type="bibr" rid="pone.0067343-Emanuelsson1">[11]</xref>, <xref ref-type="bibr" rid="pone.0067343-Ma1">[12]</xref>, etc. A comprehensive review <xref ref-type="bibr" rid="pone.0067343-Chou3">[13]</xref> provides the process to establish a robust predictor of protein subcellular localization, with following aspects: (a) selecting or constructing an effective benchmark dataset to train and test the predictor; (b) formulating the protein samples with a valid mathematical expression; (c) proposing a powerful algorithm (classifier) for prediction tasks; and (d) performing proper tests to objectively evaluate the performance of the predictor. Among these aspects, one key factor of building a high-accuracy prediction method is to obtain a valid dataset with sufficient useful information to train a powerful classifier.</p>
<p>Normally, the training data of a subcellular localization predictor are acquired from the “proteins with experimental annotations (referred as PEAs hereafter)” in protein databases, which are labeled by sufficient experimental evidences. However, as we know, experimental methods require a long time to obtain conclusive evidence to assign an annotation. Therefore, these experimental protein sequences are just a small part of the overall sequences. According to the record (version 2012_05) of the central protein databank UniProtKB/Swiss-Prot, the PEAs only occupy 13.22% of the total reviewed protein contained therein. In this study, we also counted the number of the protein sequences over the past ten years in UniProtKB/Swiss-Prot and summarized the statistics in <xref ref-type="table" rid="pone-0067343-t001"><bold>Table 1</bold></xref>. Over the last decade, there was a tenfold increase in the amount of all protein sequences, but the growth of the experimental sequences was less than doubled. While more PEAs of all types are needed to provide useful information for increasing undetermined proteins, the gap between the amount of PEAs and the entire protein data are becoming larger and larger. In addition, for computational prediction methods, excess of homologous or similar protein data will cause the over-fitting problem and these data are redundant for training, consequently, most of these PEAs have to be abandoned in practice. Besides, some special subcellular locations are correlated with very few PEAs and it also restricts the number of data used for learning. Therefore, there are often insufficient PEAs when constructing a proper dataset for a prediction task. For instance, the virus benchmark dataset in paper <xref ref-type="bibr" rid="pone.0067343-Shen1">[14]</xref> merely consists of 207 proteins, and there are only eight proteins located in “viral capsid”. The problem of lacking high-quality training data nearly occurs within each species and it has been a major problem in many bioinformatics researches because the prediction with sparse data would mostly obtain disappointing results <xref ref-type="bibr" rid="pone.0067343-Xu1">[15]</xref>.</p>
<table-wrap id="pone-0067343-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0067343.t001</object-id><label>Table 1</label><caption>
<title>Number of protein sequences over the past ten years (2003–2012) in the UniProtKB/Swiss-Prot protein knowledgebase.</title>
</caption><alternatives><graphic id="pone-0067343-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0067343.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Release date</td>
<td align="left" rowspan="1" colspan="1">Database version</td>
<td align="left" rowspan="1" colspan="1">Total</td>
<td align="left" rowspan="1" colspan="1">PEAs</td>
<td align="left" rowspan="1" colspan="1">PNEAs</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">2003-12-15</td>
<td align="left" rowspan="1" colspan="1">1.0</td>
<td align="left" rowspan="1" colspan="1">135938</td>
<td align="left" rowspan="1" colspan="1">38903</td>
<td align="left" rowspan="1" colspan="1">45391</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">2004-07-05</td>
<td align="left" rowspan="1" colspan="1">2.0</td>
<td align="left" rowspan="1" colspan="1">148277</td>
<td align="left" rowspan="1" colspan="1">41031</td>
<td align="left" rowspan="1" colspan="1">50806</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">2005-05-10</td>
<td align="left" rowspan="1" colspan="1">5.0</td>
<td align="left" rowspan="1" colspan="1">178998</td>
<td align="left" rowspan="1" colspan="1">45606</td>
<td align="left" rowspan="1" colspan="1">65084</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">2006-10-31</td>
<td align="left" rowspan="1" colspan="1">9.0</td>
<td align="left" rowspan="1" colspan="1">239174</td>
<td align="left" rowspan="1" colspan="1">53510</td>
<td align="left" rowspan="1" colspan="1">94897</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">2007-07-24</td>
<td align="left" rowspan="1" colspan="1">12.0</td>
<td align="left" rowspan="1" colspan="1">274311</td>
<td align="left" rowspan="1" colspan="1">57490</td>
<td align="left" rowspan="1" colspan="1">113135</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">2008-07-22</td>
<td align="left" rowspan="1" colspan="1">14.0</td>
<td align="left" rowspan="1" colspan="1">390787</td>
<td align="left" rowspan="1" colspan="1">64733</td>
<td align="left" rowspan="1" colspan="1">167972</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">2009-09-01</td>
<td align="left" rowspan="1" colspan="1">15.7</td>
<td align="left" rowspan="1" colspan="1">495368</td>
<td align="left" rowspan="1" colspan="1">68029</td>
<td align="left" rowspan="1" colspan="1">220091</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">2010-07-13</td>
<td align="left" rowspan="1" colspan="1">2010_08</td>
<td align="left" rowspan="1" colspan="1">516934</td>
<td align="left" rowspan="1" colspan="1">70180</td>
<td align="left" rowspan="1" colspan="1">232546</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">2011-07-27</td>
<td align="left" rowspan="1" colspan="1">2011_08</td>
<td align="left" rowspan="1" colspan="1">531326</td>
<td align="left" rowspan="1" colspan="1">70552</td>
<td align="left" rowspan="1" colspan="1">241226</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">2012-05-16</td>
<td align="left" rowspan="1" colspan="1">2012_05</td>
<td align="left" rowspan="1" colspan="1">536029</td>
<td align="left" rowspan="1" colspan="1">70868</td>
<td align="left" rowspan="1" colspan="1">245342</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt101"><label/><p>The statistics is only from the UniProtKB/Swiss-Prot manually reviewed entries, and the unreviewed entries in the UniProtKB/TrEMBL are not included.</p></fn></table-wrap-foot></table-wrap>
<p>To overcome this shortage of training data, seeking extra protein training data becomes a very natural idea. Besides the PEAs, we recently find that we can take advantage of the huge number of “proteins with non-experimental annotations (referred as PNEAs hereafter)” in the central protein database UniProtKB/Swiss-Prot. Since the observations are not marked from direct experiments, non-experimental annotations are labeled based on non-experimentally proven findings such as logical or conclusive evidences, sequence analysis results, biological events and characteristics <xref ref-type="bibr" rid="pone.0067343-Junker1">[16]</xref>. A PNEA has at least one non-experimental label in its “Subcellular location” item, and a non-experimental label corresponds to one of the following three types <xref ref-type="bibr" rid="pone.0067343-Boutet1">[17]</xref>: “Probable” - from non-direct experimental evidences; “Potential” - from computer prediction, logical or conclusive evidences; “By similarity” - from experimental evidences in a close member of the family. The details of the three non-experimental labels can be found in the UniProtKB/Swiss-Prot manual at <ext-link ext-link-type="uri" xlink:href="http://www.uniprot.org/manual" xlink:type="simple">http://www.uniprot.org/manual</ext-link>. For protein subcellular location prediction based on computational methods, the PNEAs who are being ignored are important and valuable. Unlike unknown protein data, the PNEAs provide a lot of high reliable reference location information. Additionally, as shown in <xref ref-type="table" rid="pone-0067343-t001"><bold>Table 1</bold></xref>, PNEAs have a much larger number and grow much faster than PEAs. If such abundant PNEAs can be effectively exploited, they would provide a huge supplement to PEAs for training more powerful predictors. Despite the big advantage of PNEAs, not all of them can be indiscriminately used as supplementary training data. The reason is that the non-experimental evidence is still weaker than the experimental proof, so some portion of PNEAs may have inaccurate non-experimental labels. Therefore, a feasible rule is needed to select the useful members of the PNEAs with a low risk and high quality for training a classifier.</p>
<p>In order to develop a proper rule for the active selection process, a machine learning technique named “active learning” is adopted in our study. This active learning method is a paradigm for using unlabeled data to complement labeled data, as it can actively select and learn from the most informative unlabeled data. The idea of actively selecting new samples is suitable for our work. However, there are some issues with the active learning process that need to be resolved before it can be properly used in this study. The active learner always actively asks the user to label the unlabeled data so that it can learn a good classifier with as few manual labeled samples as possible <xref ref-type="bibr" rid="pone.0067343-Settles1">[18]</xref>; while in our study, the candidate PNEA samples are not unlabeled but rather have special non-experimental labels, and the proposed algorithm should automatically pick out enough but not redundant samples from the whole PNEA dataset. Therefore, inspired by an active learning algorithm <xref ref-type="bibr" rid="pone.0067343-Hoi1">[19]</xref>, this paper proposes such a novel active sample selection strategy for PNEAs to increase the amount of training data available. For the weak basic classifiers learned via only the original data, this strategy measures the usefulness of all candidate PNEAs, and picks out these most useful PNEAs as supplementary training data. The weak classifiers are then retrained on the new training set to obtain improved prediction performances.</p>
<p>The effectiveness of the proposed approach is tested on three protein benchmark datasets from virus, plant and gram-negative bacteria cells, by four popular multi-label learning classification algorithms which are based on KNN, SVM, Bayesian method and neural network. The results show that the proposed method can effectively pick out the useful PNEAs and there are obvious enhancements for the prediction performances of each basic classifier.</p>
</sec><sec id="s2" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s2a">
<title>The Datasets</title>
<p>Three existing benchmark experimental datasets of different species are used for cross-validation tests, which include a virus dataset <xref ref-type="bibr" rid="pone.0067343-Shen1">[14]</xref> consisting of 207 proteins and 6 different subcellular location classifications, a plant dataset <xref ref-type="bibr" rid="pone.0067343-Chou4">[20]</xref> consisting of 978 proteins and 12 different subcellular location classifications, and a Gram-negative bacteria (referred as Gneg hereafter) dataset <xref ref-type="bibr" rid="pone.0067343-Shen2">[21]</xref> consisting of 1392 proteins and 8 different subcellular location classifications. In order to obtain effective candidates for supplementary training data, we extracted numerous PNEAs of the three species by parsing the “Subcellular location” section of the “Comments” field in UniProtKB/Swiss-Prot database (release 2012_05). Protein fragments and those containing less than 50 amino acid residues were discarded. Similarly, we also collected several new PEAs which were not included in the above-mentioned benchmark datasets for an independent test. In order to reduce the redundancy and avoid homology bias, we used a public server PISCES <xref ref-type="bibr" rid="pone.0067343-Wang1">[22]</xref> based on PSI-BLAST alignments to identify and cull protein sequences from all the sequence data extracted to ensure that none of these proteins have a ≥25% sequence similarity to one another as well as any sequence in the benchmark dataset for the same species.</p>
<p>After culling, we created three supplementary training sample pools as candidates for active selection, which consist of 238 virus PNEAs, 758 plant PNEAs and 248 Gneg PNEAs. We also constructed three additional independent test sets, consisting of 69 virus PEAs, 261 plant PEAs and 207 Gneg PEAs. Note that, because some proteins occur in more than one location, the concept of “locative protein” in the literature <xref ref-type="bibr" rid="pone.0067343-Shen2">[21]</xref> is employed to compute performance indexes of the classifiers. This concept means that a protein coexisting at <italic>N</italic> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e001" xlink:type="simple"/></inline-formula>) different location sites will be counted as <italic>N</italic> locative proteins even if they have an identical sequence. The amounts of active/locative proteins in the three groups of datasets are shown in <xref ref-type="table" rid="pone-0067343-t002"><bold>Table 2</bold></xref>. More details about the datasets can be found in <bold>Table S1–S3</bold> in  <xref ref-type="supplementary-material" rid="pone.0067343.s001">Material S1</xref>. The new PNEAs and new PEAs used in our research are all listed in <xref ref-type="supplementary-material" rid="pone.0067343.s002">Material S2</xref> (<bold>Supplementary Dataset S1–S6</bold>).</p>
<table-wrap id="pone-0067343-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0067343.t002</object-id><label>Table 2</label><caption>
<title>Number of active/locative proteins in the three groups of datasets.</title>
</caption><alternatives><graphic id="pone-0067343-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0067343.t002" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Dataset</td>
<td align="left" rowspan="1" colspan="1">Number of classes</td>
<td align="left" rowspan="1" colspan="1">benchmark datasets <xref ref-type="bibr" rid="pone.0067343-Shen1">[14]</xref>, <xref ref-type="bibr" rid="pone.0067343-Chou4">[20]</xref>, <xref ref-type="bibr" rid="pone.0067343-Shen2">[21]</xref></td>
<td align="left" rowspan="1" colspan="1">Supplementary training sample pool</td>
<td align="left" rowspan="1" colspan="1">Independent test set</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Virus</td>
<td align="left" rowspan="1" colspan="1">6</td>
<td align="left" rowspan="1" colspan="1">207/252</td>
<td align="left" rowspan="1" colspan="1">238/289</td>
<td align="left" rowspan="1" colspan="1">69/93</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Plant</td>
<td align="left" rowspan="1" colspan="1">12</td>
<td align="left" rowspan="1" colspan="1">978/1055</td>
<td align="left" rowspan="1" colspan="1">758/813</td>
<td align="left" rowspan="1" colspan="1">261/301</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Gneg</td>
<td align="left" rowspan="1" colspan="1">8</td>
<td align="left" rowspan="1" colspan="1">1392/1456</td>
<td align="left" rowspan="1" colspan="1">248/271</td>
<td align="left" rowspan="1" colspan="1">207/225</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap></sec><sec id="s2b">
<title>Active Sample Selection Strategy</title>
<p>In this study, because some proteins have multiple subcellular localization sites, the final prediction task is also a multi-label learning problem. Accordingly, the active sample selection strategy should have the ability to deal with the multi-label cases. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e002" xlink:type="simple"/></inline-formula> denote the original training data set consisting of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e003" xlink:type="simple"/></inline-formula> PEAs classified in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e004" xlink:type="simple"/></inline-formula> different subcellular locations, where each protein <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e005" xlink:type="simple"/></inline-formula> can be represented by a feature vector of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e006" xlink:type="simple"/></inline-formula> dimensions as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e007" xlink:type="simple"/></inline-formula>, and the label set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e008" xlink:type="simple"/></inline-formula> denotes the protein subcellular locations of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e009" xlink:type="simple"/></inline-formula>. For each protein <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e010" xlink:type="simple"/></inline-formula>, if it inhabits the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e011" xlink:type="simple"/></inline-formula> subcellular location, mark <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e012" xlink:type="simple"/></inline-formula>, otherwise <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e013" xlink:type="simple"/></inline-formula>. The basic classifier <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e014" xlink:type="simple"/></inline-formula> is trained by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e015" xlink:type="simple"/></inline-formula> to output a set of labels for each unseen protein. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e016" xlink:type="simple"/></inline-formula> denote the supplementary training sample pool containing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e017" xlink:type="simple"/></inline-formula> PNEAs, where a protein <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e018" xlink:type="simple"/></inline-formula> has the label set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e019" xlink:type="simple"/></inline-formula>. For each protein <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e020" xlink:type="simple"/></inline-formula>, if it has the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e021" xlink:type="simple"/></inline-formula> subcellular location labeled by experimental/non-experimental evidences, we mark that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e022" xlink:type="simple"/></inline-formula>, otherwise <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e023" xlink:type="simple"/></inline-formula>. Note that, both <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e024" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e025" xlink:type="simple"/></inline-formula> mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e026" xlink:type="simple"/></inline-formula>, and the subscripts are merely used for recognizing that this positive label is obtained by corresponding experimental or non-experimental annotations.</p>
<p>In order to actively pick out the useful samples from the supplementary training sample pool, the key is to create a feasible evaluation function to measure the usefulness of a non-experimental sample and decide which samples should be added into the original training set. In this paper, the classification risk of a sample is used for reflecting the sample’s usefulness, where a lower risk means a higher usefulness. For a sample <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e027" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e028" xlink:type="simple"/></inline-formula>, let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e029" xlink:type="simple"/></inline-formula> be the classification risk which is brought by adding <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e030" xlink:type="simple"/></inline-formula> into the original training set, and the evaluation function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e031" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e032" xlink:type="simple"/></inline-formula> is defined by its maximum risk. Our motivation is to evaluate the risks, and pick out the optimal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e033" xlink:type="simple"/></inline-formula> by minimizing the maximum risk, that leads to the following min-max combinatorial optimization problem:<disp-formula id="pone.0067343.e034"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e034" xlink:type="simple"/><label>(1)</label></disp-formula><disp-formula id="pone.0067343.e035"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e035" xlink:type="simple"/><label>(2)</label></disp-formula>where, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e036" xlink:type="simple"/></inline-formula> represents the unknown actual label set for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e037" xlink:type="simple"/></inline-formula>, where, for each label <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e038" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e039" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e040" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e041" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e042" xlink:type="simple"/></inline-formula>, but if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e043" xlink:type="simple"/></inline-formula> then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e044" xlink:type="simple"/></inline-formula> may be 1 or −1. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e045" xlink:type="simple"/></inline-formula> is the regularization item which measures the model complexity of the classifier, here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e046" xlink:type="simple"/></inline-formula> is a reproducing kernel Hilbert space endowed with kernel function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e047" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e048" xlink:type="simple"/></inline-formula> is a quadratic loss function and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e049" xlink:type="simple"/></inline-formula> is a weighted quadratic loss function, i.e.,<disp-formula id="pone.0067343.e050"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e050" xlink:type="simple"/><label>(3)</label></disp-formula><disp-formula id="pone.0067343.e051"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e051" xlink:type="simple"/><label>(4)</label></disp-formula></p>
<p>where, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e052" xlink:type="simple"/></inline-formula> is the weight function. For a PNEA, its associated label set is uncertain because its non-experimental label may not be the active label, and it is hard to directly calculate its loss. Therefore, the weight function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e053" xlink:type="simple"/></inline-formula> is added to reflect the probability that a non-experimental label is the active label, which can be written as:<disp-formula id="pone.0067343.e054"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e054" xlink:type="simple"/><label>(5)</label></disp-formula>here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e055" xlink:type="simple"/></inline-formula> is the posterior probability of the event that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e056" xlink:type="simple"/></inline-formula> just equals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e057" xlink:type="simple"/></inline-formula> when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e058" xlink:type="simple"/></inline-formula> has a non-experimental label <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e059" xlink:type="simple"/></inline-formula>. According to the previous description of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e060" xlink:type="simple"/></inline-formula>, it can be deduced that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e061" xlink:type="simple"/></inline-formula> when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e062" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e063" xlink:type="simple"/></inline-formula>. Therefore, it only needs to estimate the posterior probability for a non-experimental label <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e064" xlink:type="simple"/></inline-formula>. We use the Parzen-window estimation with the Gaussian kernel <xref ref-type="bibr" rid="pone.0067343-Li1">[23]</xref> to estimate the posterior probability of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e065" xlink:type="simple"/></inline-formula> as:<disp-formula id="pone.0067343.e066"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e066" xlink:type="simple"/><label>(6)</label></disp-formula>where, the prior probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e067" xlink:type="simple"/></inline-formula> is the confidence of the event that if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e068" xlink:type="simple"/></inline-formula> then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e069" xlink:type="simple"/></inline-formula>, and it is set as the parameter related to the type of the corresponding non-experimental label <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e070" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e071" xlink:type="simple"/></inline-formula> is the complementary set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e072" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e073" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e074" xlink:type="simple"/></inline-formula> are short for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e075" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e076" xlink:type="simple"/></inline-formula> respectively, which are defined as:<disp-formula id="pone.0067343.e077"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e077" xlink:type="simple"/><label>(7)</label></disp-formula><disp-formula id="pone.0067343.e078"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e078" xlink:type="simple"/><label>(8)</label></disp-formula></p>
<p>where, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e079" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e080" xlink:type="simple"/></inline-formula> consisting of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e081" xlink:type="simple"/></inline-formula> samples is the set of all samples with certain labels; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e082" xlink:type="simple"/></inline-formula> consisting of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e083" xlink:type="simple"/></inline-formula> samples defined as the set of all samples with non-experimental labels because a PNEA sample will get the maximum loss when all the actual label are opposite to the corresponding non-experimental positive label, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e084" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e085" xlink:type="simple"/></inline-formula>.</p>
<p>Plugging <xref ref-type="disp-formula" rid="pone.0067343.e050">equations (3</xref>)-(5) into <xref ref-type="disp-formula" rid="pone.0067343.e035">equation (2</xref>), this active sample selection can be written as following the min-max optimization problem:<disp-formula id="pone.0067343.e086"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e086" xlink:type="simple"/><label>(9)</label></disp-formula></p>
<p>From the derivation, we have:<disp-formula id="pone.0067343.e087"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e087" xlink:type="simple"/><label>(10)</label></disp-formula>where, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e088" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e089" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e090" xlink:type="simple"/></inline-formula> is the kernel matrix of size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e091" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e092" xlink:type="simple"/></inline-formula> is an <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e093" xlink:type="simple"/></inline-formula> identity matrix, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e094" xlink:type="simple"/></inline-formula> is the Kronecker product, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e095" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e096" xlink:type="simple"/></inline-formula>, and<disp-formula id="pone.0067343.e097"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e097" xlink:type="simple"/><label>(11)</label></disp-formula><disp-formula id="pone.0067343.e098"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e098" xlink:type="simple"/><label>(12)</label></disp-formula></p>
<p>Thus, the evaluation function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e099" xlink:type="simple"/></inline-formula> is simplified as:<disp-formula id="pone.0067343.e100"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e100" xlink:type="simple"/><label>(13)</label></disp-formula></p>
<p>Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e101" xlink:type="simple"/></inline-formula>, then<disp-formula id="pone.0067343.e102"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e102" xlink:type="simple"/><label>(14)</label></disp-formula></p>
<p>Except for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e103" xlink:type="simple"/></inline-formula>, all other parts in Eq.(14) can be determined and the min-max optimization problem described as Eq.(8) can be solved through using all feasible values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e104" xlink:type="simple"/></inline-formula> to find the optimal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e105" xlink:type="simple"/></inline-formula> with the smallest <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e106" xlink:type="simple"/></inline-formula>. Similarly, we can pick out other PNEA samples one by one.</p>
<p>Since the usefulness of all the PNEAs are being measured, the algorithm needs to decide how many samples in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e107" xlink:type="simple"/></inline-formula> should be added to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e108" xlink:type="simple"/></inline-formula> to help to retrain the classifier. We observe that there is a high correlation between the usefulness of PNEA samples in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e109" xlink:type="simple"/></inline-formula> and the change rates of the evaluation values. If the change becomes stable, it means the latest added supplementary training samples have little or no effect. Based on this point, this paper presents a simple algorithm, which can output a proper proportion of all samples in the supplementary training sample pool. First, rank all the samples within <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e110" xlink:type="simple"/></inline-formula> in ascending order according to their evaluation to compose a new ordered set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e111" xlink:type="simple"/></inline-formula>. Next, denote the evaluation value of a sample <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e112" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e113" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e114" xlink:type="simple"/></inline-formula>. Then the change rate of its evaluation value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e115" xlink:type="simple"/></inline-formula> can be written as:<disp-formula id="pone.0067343.e116"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e116" xlink:type="simple"/><label>(15)</label></disp-formula></p>
<p>For a given step of proportion <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e117" xlink:type="simple"/></inline-formula> and the corresponding number of intervals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e118" xlink:type="simple"/></inline-formula>, the algorithm needs to decide which proportion is preferred for helping to retrain the basic classifier (e.g. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e119" xlink:type="simple"/></inline-formula>, then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e120" xlink:type="simple"/></inline-formula>, where the preferred proportion is one of following percentages: 10%, 20%, 30%, …, and 100%). Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e121" xlink:type="simple"/></inline-formula> be the number of the samples in the <italic>t</italic>-th interval, and the preferred proportion <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e122" xlink:type="simple"/></inline-formula> can be calculated as:<disp-formula id="pone.0067343.e123"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e123" xlink:type="simple"/><label>(16)</label></disp-formula><disp-formula id="pone.0067343.e124"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e124" xlink:type="simple"/><label>(17)</label></disp-formula></p>
<p>Note that, it is hard to theoretically prove whether the output proportion <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e125" xlink:type="simple"/></inline-formula> is the global optimum or not, but it can be seen that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e126" xlink:type="simple"/></inline-formula> can indeed provide excellent results in subsequent simulation experiments.</p>
<p>After selecting the top <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e127" xlink:type="simple"/></inline-formula> of the samples in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e128" xlink:type="simple"/></inline-formula> and adding them into the original training set, the initial classifier is updated according to the new training set and its performance is improved. An illustration of the work process of the proposed active example selection strategy is shown in <xref ref-type="fig" rid="pone-0067343-g001">Fig. 1</xref>.</p>
<fig id="pone-0067343-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0067343.g001</object-id><label>Figure 1</label><caption>
<title>The work process of the proposed active sample selection strategy.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0067343.g001" position="float" xlink:type="simple"/></fig></sec><sec id="s2c">
<title>Evaluation Metrics</title>
<p>In order to comprehensively evaluate the active sample selection method and compare the classifier performances with/without the proposed approach, some common evaluation metrics are used. Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e129" xlink:type="simple"/></inline-formula> denotes a test set, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e130" xlink:type="simple"/></inline-formula> returns a set of proper labels of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e131" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e132" xlink:type="simple"/></inline-formula> returns a probability indicating the confidence for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e133" xlink:type="simple"/></inline-formula> to be a proper label of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e134" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e135" xlink:type="simple"/></inline-formula> is the rank of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e136" xlink:type="simple"/></inline-formula> derived from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e137" xlink:type="simple"/></inline-formula>. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e138" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e139" xlink:type="simple"/></inline-formula> represent the complementary sets of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e140" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e141" xlink:type="simple"/></inline-formula> , respectively. Therefore, we have:</p>
<list list-type="bullet"><list-item>
<p>True Positives: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e142" xlink:type="simple"/></inline-formula></p>
</list-item><list-item>
<p>False Positives: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e143" xlink:type="simple"/></inline-formula></p>
</list-item><list-item>
<p>True Negatives: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e144" xlink:type="simple"/></inline-formula></p>
</list-item><list-item>
<p>False Negatives: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e145" xlink:type="simple"/></inline-formula></p>
</list-item></list>
<p>Based on the above, three global indices: accuracy (Accu), Matthews correlation coefficient (MCC) and F1-scroe, and three multi-label evaluation metrics: average precision (Avgprec), ranking loss (Rloss) and coverage are computed as follows:<disp-formula id="pone.0067343.e146"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e146" xlink:type="simple"/><label>(18)</label></disp-formula><disp-formula id="pone.0067343.e147"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e147" xlink:type="simple"/><label>(19)</label></disp-formula><disp-formula id="pone.0067343.e148"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e148" xlink:type="simple"/><label>(20)</label></disp-formula><disp-formula id="pone.0067343.e149"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e149" xlink:type="simple"/><label>(21)</label></disp-formula><disp-formula id="pone.0067343.e150"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e150" xlink:type="simple"/><label>(22)</label></disp-formula><disp-formula id="pone.0067343.e151"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0067343.e151" xlink:type="simple"/><label>(23)</label></disp-formula></p>
</sec></sec><sec id="s3">
<title>Results and Discussion</title>
<p>We performed several simulation experiments to evaluate the performance of the proposed approach through both the sub-sampling (10-fold cross validation) and independent dataset test methods using the three groups of datasets mentioned in section “Material and Methods”. In the sub-sampling tests, we performed multiple rounds of randomizations of the original training and testing data on each benchmark dataset. In the independent dataset tests, the benchmark datasets were directly used as the original training sets, and the new independent test sets were adopted for testing. The amphiphilic pseudo amino acid composition <xref ref-type="bibr" rid="pone.0067343-Chou5">[24]</xref> was employed as the feature extraction technology to represent a protein sequence. The protein sequences were formulated with a valid mathematical expression by this method through a public online server named PseAAC at: <ext-link ext-link-type="uri" xlink:href="http://www.csbio.sjtu.edu.cn/bioinf/PseAA/" xlink:type="simple">http://www.csbio.sjtu.edu.cn/bioinf/PseAA/</ext-link>. The details of PseAAC can be found in reference <xref ref-type="bibr" rid="pone.0067343-Shen3">[25]</xref>. In this study, amino acid characters were empirically chosen to be Hydrophobicity, Hydrophilicity and Mass; the weight factor was 0.4, and the lambda parameter was 5. Four different types of multi-label classification models including IMKNN <xref ref-type="bibr" rid="pone.0067343-Cao1">[5]</xref>, SVM <xref ref-type="bibr" rid="pone.0067343-Huang1">[26]</xref>, Gaussian process <xref ref-type="bibr" rid="pone.0067343-He1">[10]</xref> and ML-RBF <xref ref-type="bibr" rid="pone.0067343-Zhang1">[27]</xref>, were used as basic classifiers to test our algorithm. The parameters of these classifiers were assigned the same values as the original papers and all these parameters were fixed in the whole experiments for an objective comparison.</p>
<p>The overall performances of the above classification algorithms following three kinds of conditions were compared. These conditions were: not using the proposed active sample selection (using no supplementary training samples), using the proposed active sample selection with a preferred proportion (top <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e152" xlink:type="simple"/></inline-formula>) of the supplementary training samples, and directly using the whole PNEA samples in the supplementary training sample pool. In the experiments, the kernel function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e153" xlink:type="simple"/></inline-formula> was the same Gaussian kernel used for estimating the posterior probability in Eq.(6). The prior probabilities of the three levels of non-experimental labels were set according to the strength of the evidences of the three non-experimental label types: the prior probability with “Probable” label was set to be the largest, the value of “Potential’ was medium and “By Similarity” was the smallest. We tested several values for the prior probabilities and finally choose a group of values with the best results as: 0.85 for “Probable”, 0.8 for “Potential” and 0.75 for “By Similarity”. The step of proportion <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e154" xlink:type="simple"/></inline-formula> was set to 10% and the number of intervals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e155" xlink:type="simple"/></inline-formula> was 10.</p>
    <p>Through the numerical experiments, we observe the preferred proportions of active sample selection for various datasets are different. The preferred proportion of virus PNEA samples is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e156" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e157" xlink:type="simple"/></inline-formula> for plant, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0067343.e158" xlink:type="simple"/></inline-formula> for Gram-negative bacteria. The comparisons of the performances of these classification models by using none, preferred proportion and all of the samples in the supplementary training sample pool are shown in <xref ref-type="table" rid="pone-0067343-t003"><bold>Table 3</bold></xref><bold>, </bold><xref ref-type="table" rid="pone-0067343-t004"><bold>Table 4</bold></xref><bold>, </bold><xref ref-type="table" rid="pone-0067343-t005"><bold>Table 5</bold></xref><bold>, </bold><xref ref-type="table" rid="pone-0067343-t006"><bold>Table 6</bold></xref>. <xref ref-type="table" rid="pone-0067343-t003"><bold>Table 3</bold></xref><bold>, <xref ref-type="table" rid="pone-0067343-t004"><bold>Table 4</bold></xref> and </bold><xref ref-type="table" rid="pone-0067343-t005"><bold>Table 5</bold></xref> show the average values of 10 randomizations, 10-fold cross-validation measures and their standard deviations, and <xref ref-type="table" rid="pone-0067343-t006"><bold>Table 6</bold></xref> shows the results of the independent dataset test. For each evaluation metric, “↑” means the bigger the metric value the better the performance, and “↓” means the smaller the metric value the better the performance. It can be seen, for each case, the classifier using the supplementary training data selected by the proposed approach always performs better than the basic classifier using no supplementary training sample. Additionally, the results under the proposed approach are superior to that of indiscriminately using the whole data in the supplementary training data pool. From the simulation results, it can be concluded that, on one hand, the improvements to the original prediction indicates that the selected PNEA samples are useful and indeed provide helpful information for prediction; on the other hand, the better performance of the active sample selection over directly using all the samples in the supplementary training sample pool indicates that a part of the PNEA samples disrupts the prediction because they may have some inaccurate information. Therefore, an effective active sample selection is important to select a proper amount of valuable PNEA samples and reduce the possibility of prediction disturbance brought by the redundant supplementary training data. We also observed that the performance improvements of all the classification models are related to the size of the original training set. For the virus cases with the least original training data, each classifier’s performance improvement is superior to that of the other two datasets. On the contrary, for the Gneg cases with the most original training data, the improvement effect is the smallest. We attribute this fact to the original training set with less data having a greater data shortage, so the basic classifiers are better improved by incrementally adding useful supplementary training data. Without dependence on the original classification model, the experiment results show that the proposed active sample selection strategy provides a generic approach for the existing prediction algorithms.</p>
<table-wrap id="pone-0067343-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0067343.t003</object-id><label>Table 3</label><caption>
<title>Results for different basic classifiers (mean±SD) by using varied numbers of supplementary training data, trained and tested in 10-fold cross-validation on the virus dataset.</title>
</caption><alternatives><graphic id="pone-0067343-t003-3" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0067343.t003" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Classifier</td>
<td align="left" rowspan="1" colspan="1">Ealuation metrics</td>
<td colspan="3" align="left" rowspan="1">Number of supplementary training data</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">None</td>
<td align="left" rowspan="1" colspan="1">Top 40%</td>
<td align="left" rowspan="1" colspan="1">All</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">IMKNN</td>
<td align="left" rowspan="1" colspan="1">Accu↑</td>
<td align="left" rowspan="1" colspan="1">0.7753±0.0257</td>
<td align="left" rowspan="1" colspan="1"><bold>0.8041±0.0252</bold></td>
<td align="left" rowspan="1" colspan="1">0.7944±0.0369</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">MCC↑</td>
<td align="left" rowspan="1" colspan="1">0.2796±0.0515</td>
<td align="left" rowspan="1" colspan="1"><bold>0.3889±0.0478</bold></td>
<td align="left" rowspan="1" colspan="1">0.3600±0.0484</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">F1-score↑</td>
<td align="left" rowspan="1" colspan="1">0.4131±0.0674</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5179±0.0539</bold></td>
<td align="left" rowspan="1" colspan="1">0.4886±0.0584</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Avgprec↑</td>
<td align="left" rowspan="1" colspan="1">0.5978±0.0596</td>
<td align="left" rowspan="1" colspan="1"><bold>0.6559±0.0507</bold></td>
<td align="left" rowspan="1" colspan="1">0.6502±0.0541</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Rloss↓</td>
<td align="left" rowspan="1" colspan="1">0.6126±0.0147</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5036±0.0149</bold></td>
<td align="left" rowspan="1" colspan="1">0.5276±0.0161</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Coverage↓</td>
<td align="left" rowspan="1" colspan="1">1.6591±0.3007</td>
<td align="left" rowspan="1" colspan="1"><bold>1.5269±0.2550</bold></td>
<td align="left" rowspan="1" colspan="1">1.5555±0.2919</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">SVM</td>
<td align="left" rowspan="1" colspan="1">Accu↑</td>
<td align="left" rowspan="1" colspan="1">0.7855±0.0199</td>
<td align="left" rowspan="1" colspan="1"><bold>0.8059±0.0218</bold></td>
<td align="left" rowspan="1" colspan="1">0.7887±0.0381</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">MCC↑</td>
<td align="left" rowspan="1" colspan="1">0.3432±0.0581</td>
<td align="left" rowspan="1" colspan="1"><bold>0.3952±0.0471</bold></td>
<td align="left" rowspan="1" colspan="1">0.3739±0.0426</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">F1-score↑</td>
<td align="left" rowspan="1" colspan="1">0.4758±0.0457</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5160±0.0526</bold></td>
<td align="left" rowspan="1" colspan="1">0.5070±0.0676</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Avgprec↑</td>
<td align="left" rowspan="1" colspan="1">0.6385±0.0436</td>
<td align="left" rowspan="1" colspan="1"><bold>0.6752±0.0481</bold></td>
<td align="left" rowspan="1" colspan="1">0.6553±0.0419</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Rloss↓</td>
<td align="left" rowspan="1" colspan="1">0.5376±0.0222</td>
<td align="left" rowspan="1" colspan="1"><bold>0.4915±0.0268</bold></td>
<td align="left" rowspan="1" colspan="1">0.5112±0.0224</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Coverage↓</td>
<td align="left" rowspan="1" colspan="1">1.5376±0.1366</td>
<td align="left" rowspan="1" colspan="1"><bold>1.4795±0.2242</bold></td>
<td align="left" rowspan="1" colspan="1">1.5384±0.2237</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Gaussian process</td>
<td align="left" rowspan="1" colspan="1">Accu↑</td>
<td align="left" rowspan="1" colspan="1">0.7979±0.0224</td>
<td align="left" rowspan="1" colspan="1"><bold>0.8220±0.0127</bold></td>
<td align="left" rowspan="1" colspan="1">0.7991±0.0286</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">MCC↑</td>
<td align="left" rowspan="1" colspan="1">0.3382±0.0520</td>
<td align="left" rowspan="1" colspan="1"><bold>0.4026±0.0359</bold></td>
<td align="left" rowspan="1" colspan="1">0.3430±0.0437</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">F1-score↑</td>
<td align="left" rowspan="1" colspan="1">0.4543±0.0548</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5816±00390</bold></td>
<td align="left" rowspan="1" colspan="1">0.4616±0.0481</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Avgprec↑</td>
<td align="left" rowspan="1" colspan="1">0.6147±0.0228</td>
<td align="left" rowspan="1" colspan="1"><bold>0.6477±0.0230</bold></td>
<td align="left" rowspan="1" colspan="1">0.6332±0.0233</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Rloss↓</td>
<td align="left" rowspan="1" colspan="1">0.5989±0.0298</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5508±0.0295</bold></td>
<td align="left" rowspan="1" colspan="1">0.5688±0.0201</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Coverage↓</td>
<td align="left" rowspan="1" colspan="1">1.5917±0.1892</td>
<td align="left" rowspan="1" colspan="1"><bold>1.5404±0.1588</bold></td>
<td align="left" rowspan="1" colspan="1">1.5651±0.1946</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">ML-RBF</td>
<td align="left" rowspan="1" colspan="1">Accu↑</td>
<td align="left" rowspan="1" colspan="1">0.6783±0.0224</td>
<td align="left" rowspan="1" colspan="1"><bold>0.7517±0.0213</bold></td>
<td align="left" rowspan="1" colspan="1">0.7421±0.0208</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">MCC↑</td>
<td align="left" rowspan="1" colspan="1">0.2749±0.0269</td>
<td align="left" rowspan="1" colspan="1"><bold>0.3505±0.0161</bold></td>
<td align="left" rowspan="1" colspan="1">0.3378±0.0239</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">F1-score↑</td>
<td align="left" rowspan="1" colspan="1">0.3720±0.0400</td>
<td align="left" rowspan="1" colspan="1"><bold>0.4436±0.0203</bold></td>
<td align="left" rowspan="1" colspan="1">0.4103±0.0369</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Avgprec↑</td>
<td align="left" rowspan="1" colspan="1">0.5751±0.0595</td>
<td align="left" rowspan="1" colspan="1"><bold>0.6215±0.0453</bold></td>
<td align="left" rowspan="1" colspan="1">0.5938±0.0469</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Rloss↓</td>
<td align="left" rowspan="1" colspan="1">0.3968±0.0135</td>
<td align="left" rowspan="1" colspan="1"><bold>0.3194±0.0208</bold></td>
<td align="left" rowspan="1" colspan="1">0.3760±0.0189</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Coverage↓</td>
<td align="left" rowspan="1" colspan="1">2.2487±0.3568</td>
<td align="left" rowspan="1" colspan="1"><bold>1.8906±0.2886</bold></td>
<td align="left" rowspan="1" colspan="1">2.1721±0.3477</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap><table-wrap id="pone-0067343-t004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0067343.t004</object-id><label>Table 4</label><caption>
<title>Results for different basic classifiers (mean±SD) by using varied numbers of supplementary training data, trained and tested in 10-fold cross-validation on the plant dataset.</title>
</caption><alternatives><graphic id="pone-0067343-t004-4" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0067343.t004" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Classifier</td>
<td align="left" rowspan="1" colspan="1">Ealuation metrics</td>
<td colspan="3" align="left" rowspan="1">Number of supplementary training data</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">None</td>
<td align="left" rowspan="1" colspan="1">Top 50%</td>
<td align="left" rowspan="1" colspan="1">All</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">IMKNN</td>
<td align="left" rowspan="1" colspan="1">Accu↑</td>
<td align="left" rowspan="1" colspan="1">0.8557±0.0058</td>
<td align="left" rowspan="1" colspan="1"><bold>0.8917±0.0035</bold></td>
<td align="left" rowspan="1" colspan="1">0.8881±0.0045</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">MCC↑</td>
<td align="left" rowspan="1" colspan="1">0.1362±0.0254</td>
<td align="left" rowspan="1" colspan="1"><bold>0.1636±0.0277</bold></td>
<td align="left" rowspan="1" colspan="1">0.1498±0.0285</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">F1-score↑</td>
<td align="left" rowspan="1" colspan="1">0.1858±0.0210</td>
<td align="left" rowspan="1" colspan="1"><bold>0.2124±0.0245</bold></td>
<td align="left" rowspan="1" colspan="1">0.1903±0.0253</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Avgprec↑</td>
<td align="left" rowspan="1" colspan="1">0.2943±0.0153</td>
<td align="left" rowspan="1" colspan="1"><bold>0.3103±0.0168</bold></td>
<td align="left" rowspan="1" colspan="1">0.2934±0.0175</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Rloss↓</td>
<td align="left" rowspan="1" colspan="1">0.8523±0.0178</td>
<td align="left" rowspan="1" colspan="1"><bold>0.8333±0.0152</bold></td>
<td align="left" rowspan="1" colspan="1">0.8546±0.0123</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Coverage↓</td>
<td align="left" rowspan="1" colspan="1">5.9920±0.2188</td>
<td align="left" rowspan="1" colspan="1"><bold>5.7544±0.2598</bold></td>
<td align="left" rowspan="1" colspan="1">5.9703±0.2037</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">SVM</td>
<td align="left" rowspan="1" colspan="1">Accu↑</td>
<td align="left" rowspan="1" colspan="1">0.8742±0.0050</td>
<td align="left" rowspan="1" colspan="1"><bold>0.8820±0.0057</bold></td>
<td align="left" rowspan="1" colspan="1">0.8804±0.0081</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">MCC↑</td>
<td align="left" rowspan="1" colspan="1">0.2215±0.0288</td>
<td align="left" rowspan="1" colspan="1"><bold>0.2649±0.0221</bold></td>
<td align="left" rowspan="1" colspan="1">0.2529±0.0232</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">F1-score↑</td>
<td align="left" rowspan="1" colspan="1">0.2904±0.0261</td>
<td align="left" rowspan="1" colspan="1"><bold>0.3294±0.0292</bold></td>
<td align="left" rowspan="1" colspan="1">0.3183±0.0288</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Avgprec↑</td>
<td align="left" rowspan="1" colspan="1">0.4049±0.0151</td>
<td align="left" rowspan="1" colspan="1"><bold>0.4331±0.0246</bold></td>
<td align="left" rowspan="1" colspan="1">0.4271±0.0439</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Rloss↓</td>
<td align="left" rowspan="1" colspan="1">0.7114±0.0262</td>
<td align="left" rowspan="1" colspan="1"><bold>0.6777±0.0336</bold></td>
<td align="left" rowspan="1" colspan="1">0.6871±0.0383</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Coverage↓</td>
<td align="left" rowspan="1" colspan="1">4.9945±0.2491</td>
<td align="left" rowspan="1" colspan="1"><bold>4.7985±0.2170</bold></td>
<td align="left" rowspan="1" colspan="1">4.8574±0.2372</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Gaussian process</td>
<td align="left" rowspan="1" colspan="1">Accu↑</td>
<td align="left" rowspan="1" colspan="1">0.8909±0.0013</td>
<td align="left" rowspan="1" colspan="1"><bold>0.9116±0.0045</bold></td>
<td align="left" rowspan="1" colspan="1">0.9096±0.0031</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">MCC↑</td>
<td align="left" rowspan="1" colspan="1">0.2084±0.0287</td>
<td align="left" rowspan="1" colspan="1"><bold>0.2421±0.0178</bold></td>
<td align="left" rowspan="1" colspan="1">0.2132±0.0135</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">F1-score↑</td>
<td align="left" rowspan="1" colspan="1">0.1796±0.0284</td>
<td align="left" rowspan="1" colspan="1"><bold>0.2218±0.0153</bold></td>
<td align="left" rowspan="1" colspan="1">0.1963±0.0211</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Avgprec↑</td>
<td align="left" rowspan="1" colspan="1">0.2884±0.0227</td>
<td align="left" rowspan="1" colspan="1"><bold>0.3125±0.0184</bold></td>
<td align="left" rowspan="1" colspan="1">0.2934±0.0135</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Rloss↓</td>
<td align="left" rowspan="1" colspan="1">0.8878±0.0216</td>
<td align="left" rowspan="1" colspan="1"><bold>0.8559±0.0279</bold></td>
<td align="left" rowspan="1" colspan="1">0.8769±0.0141</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Coverage↓</td>
<td align="left" rowspan="1" colspan="1">5.8800±0.2047</td>
<td align="left" rowspan="1" colspan="1"><bold>5.7248±0.2623</bold></td>
<td align="left" rowspan="1" colspan="1">5.8947±0.2497</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">ML-RBF</td>
<td align="left" rowspan="1" colspan="1">Accu↑</td>
<td align="left" rowspan="1" colspan="1">0.8803±0.0084</td>
<td align="left" rowspan="1" colspan="1"><bold>0.8994±0.0046</bold></td>
<td align="left" rowspan="1" colspan="1">0.8898±0.0031</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">MCC↑</td>
<td align="left" rowspan="1" colspan="1">0.2663±0.0177</td>
<td align="left" rowspan="1" colspan="1"><bold>0.2705±0.0200</bold></td>
<td align="left" rowspan="1" colspan="1">0.2656±0.0234</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">F1-score↑</td>
<td align="left" rowspan="1" colspan="1">0.3211±0.0161</td>
<td align="left" rowspan="1" colspan="1"><bold>0.3332±0.0162</bold></td>
<td align="left" rowspan="1" colspan="1">0.3230±0.0230</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Avgprec↑</td>
<td align="left" rowspan="1" colspan="1">0.5511±0.0261</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5682±0.0159</bold></td>
<td align="left" rowspan="1" colspan="1">0.5526±0.0192</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Rloss↓</td>
<td align="left" rowspan="1" colspan="1">0.2356±0.0160</td>
<td align="left" rowspan="1" colspan="1"><bold>0.2211±0.0216</bold></td>
<td align="left" rowspan="1" colspan="1">0.2301±0.0164</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Coverage↓</td>
<td align="left" rowspan="1" colspan="1">2.7591±0.1611</td>
<td align="left" rowspan="1" colspan="1"><bold>2.5926±0.2111</bold></td>
<td align="left" rowspan="1" colspan="1">2.6839±0.1984</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap><table-wrap id="pone-0067343-t005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0067343.t005</object-id><label>Table 5</label><caption>
<title>Results for different basic classifiers (mean±SD) by using varied numbers of supplementary training data, trained and tested in 10-fold cross-validation on the Gram-negative bacteria dataset.</title>
</caption><alternatives><graphic id="pone-0067343-t005-5" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0067343.t005" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Classifier</td>
<td align="left" rowspan="1" colspan="1">Ealuation metrics</td>
<td colspan="3" align="left" rowspan="1">Number of supplementary training data</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">None</td>
<td align="left" rowspan="1" colspan="1">Top 70%</td>
<td align="left" rowspan="1" colspan="1">All</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">IMKNN</td>
<td align="left" rowspan="1" colspan="1">Accu↑</td>
<td align="left" rowspan="1" colspan="1">0.8699±0.0073</td>
<td align="left" rowspan="1" colspan="1"><bold>0.8819±0.0063</bold></td>
<td align="left" rowspan="1" colspan="1">0.8688±0.0073</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">MCC↑</td>
<td align="left" rowspan="1" colspan="1">0.5437±0.0233</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5527±0.0290</bold></td>
<td align="left" rowspan="1" colspan="1">0.5452±0.0225</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">F1-score↑</td>
<td align="left" rowspan="1" colspan="1">0.6092±0.0193</td>
<td align="left" rowspan="1" colspan="1"><bold>0.6170±0.0184</bold></td>
<td align="left" rowspan="1" colspan="1">0.6079±0.0227</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Avgprec↑</td>
<td align="left" rowspan="1" colspan="1">0.6894±0.0152</td>
<td align="left" rowspan="1" colspan="1"><bold>0.7269±0.0183</bold></td>
<td align="left" rowspan="1" colspan="1">0.7152±0.0198</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Rloss↓</td>
<td align="left" rowspan="1" colspan="1">0.2910±0.0243</td>
<td align="left" rowspan="1" colspan="1"><bold>0.2792±0.0299</bold></td>
<td align="left" rowspan="1" colspan="1">0.3055±0.0254</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Coverage↓</td>
<td align="left" rowspan="1" colspan="1">1.4717±0.1886</td>
<td align="left" rowspan="1" colspan="1"><bold>1.4345±0.1331</bold></td>
<td align="left" rowspan="1" colspan="1">1.4828±0.1173</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">SVM</td>
<td align="left" rowspan="1" colspan="1">Accu↑</td>
<td align="left" rowspan="1" colspan="1">0.9026±0.0067</td>
<td align="left" rowspan="1" colspan="1"><bold>0.9062±0.0013</bold></td>
<td align="left" rowspan="1" colspan="1">0.9056±0.0034</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">MCC↑</td>
<td align="left" rowspan="1" colspan="1">0.5698±0.0281</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5847±0.0147</bold></td>
<td align="left" rowspan="1" colspan="1">0.5828±0.0119</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">F1-score↑</td>
<td align="left" rowspan="1" colspan="1">0.6258±0.0242</td>
<td align="left" rowspan="1" colspan="1"><bold>0.6390±0.0128</bold></td>
<td align="left" rowspan="1" colspan="1">0.6366±0.0156</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Avgprec↑</td>
<td align="left" rowspan="1" colspan="1">0.7193±0.0157</td>
<td align="left" rowspan="1" colspan="1"><bold>0.7210±0.0143</bold></td>
<td align="left" rowspan="1" colspan="1">0.7199±0.0117</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Rloss↓</td>
<td align="left" rowspan="1" colspan="1">0.3700±0.0191</td>
<td align="left" rowspan="1" colspan="1"><bold>0.3575±0.0132</bold></td>
<td align="left" rowspan="1" colspan="1">0.3631±0.0164</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Coverage↓</td>
<td align="left" rowspan="1" colspan="1">1.7672±0.0698</td>
<td align="left" rowspan="1" colspan="1"><bold>1.7003±0.0665</bold></td>
<td align="left" rowspan="1" colspan="1">1.7115±0.0672</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Gaussian process</td>
<td align="left" rowspan="1" colspan="1">Accu↑</td>
<td align="left" rowspan="1" colspan="1">0.9332±0.0035</td>
<td align="left" rowspan="1" colspan="1"><bold>0.9384±0.0042</bold></td>
<td align="left" rowspan="1" colspan="1">0.9300±0.0057</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">MCC↑</td>
<td align="left" rowspan="1" colspan="1">0.6678±0.0106</td>
<td align="left" rowspan="1" colspan="1"><bold>0.6878±0.0142</bold></td>
<td align="left" rowspan="1" colspan="1">0.6666±0.0171</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">F1-score↑</td>
<td align="left" rowspan="1" colspan="1">0.6990±0.0263</td>
<td align="left" rowspan="1" colspan="1"><bold>0.7096±0.0203</bold></td>
<td align="left" rowspan="1" colspan="1">0.6984±0.0156</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Avgprec↑</td>
<td align="left" rowspan="1" colspan="1">0.7307±0.0256</td>
<td align="left" rowspan="1" colspan="1"><bold>0.7417±0.0220</bold></td>
<td align="left" rowspan="1" colspan="1">0.7264±0.0189</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Rloss↓</td>
<td align="left" rowspan="1" colspan="1">0.3722±0.0248</td>
<td align="left" rowspan="1" colspan="1"><bold>0.3682±0.0238</bold></td>
<td align="left" rowspan="1" colspan="1">0.3728±0.0207</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Coverage↓</td>
<td align="left" rowspan="1" colspan="1">1.7689±0.0581</td>
<td align="left" rowspan="1" colspan="1"><bold>1.7191±0.0635</bold></td>
<td align="left" rowspan="1" colspan="1">1.7989±0.0732</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">ML-RBF</td>
<td align="left" rowspan="1" colspan="1">Accu↑</td>
<td align="left" rowspan="1" colspan="1">0.9159±0.0035</td>
<td align="left" rowspan="1" colspan="1"><bold>0.9319±0.0071</bold></td>
<td align="left" rowspan="1" colspan="1">0.9127±0.0015</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">MCC↑</td>
<td align="left" rowspan="1" colspan="1">0.6144±0.0187</td>
<td align="left" rowspan="1" colspan="1"><bold>0.6328±0.0183</bold></td>
<td align="left" rowspan="1" colspan="1">0.6020±0.0117</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">F1-score↑</td>
<td align="left" rowspan="1" colspan="1">0.6672±0.0168</td>
<td align="left" rowspan="1" colspan="1"><bold>0.6945±0.0128</bold></td>
<td align="left" rowspan="1" colspan="1">0.6507±0.0113</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Avgprec↑</td>
<td align="left" rowspan="1" colspan="1">0.8057±0.0145</td>
<td align="left" rowspan="1" colspan="1"><bold>0.8295±0.0153</bold></td>
<td align="left" rowspan="1" colspan="1">0.7984±0.0086</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Rloss↓</td>
<td align="left" rowspan="1" colspan="1">0.1147±0.0120</td>
<td align="left" rowspan="1" colspan="1"><bold>0.1044±0.0178</bold></td>
<td align="left" rowspan="1" colspan="1">0.1110±0.0067</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Coverage↓</td>
<td align="left" rowspan="1" colspan="1">0.8786±0.0503</td>
<td align="left" rowspan="1" colspan="1"><bold>0.8499±0.0580</bold></td>
<td align="left" rowspan="1" colspan="1">0.8687±0.0538</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap><table-wrap id="pone-0067343-t006" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0067343.t006</object-id><label>Table 6</label><caption>
<title>Comparison of the prediction results of different basic classifiers by using varied numbers of supplementary training data.</title>
</caption><alternatives><graphic id="pone-0067343-t006-6" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0067343.t006" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Dataset</td>
<td align="left" rowspan="1" colspan="1">Ealuation metrics</td>
<td colspan="13" align="left" rowspan="1">Number of supplementary training data</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td colspan="3" align="left" rowspan="1">IMKNN</td>
<td colspan="3" align="left" rowspan="1">SVM</td>
<td colspan="3" align="left" rowspan="1">Gaussian process</td>
<td colspan="4" align="left" rowspan="1">ML-RBF</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">None</td>
<td align="left" rowspan="1" colspan="1">Top <italic>θ</italic></td>
<td align="left" rowspan="1" colspan="1">All</td>
<td align="left" rowspan="1" colspan="1">None</td>
<td align="left" rowspan="1" colspan="1">Top <italic>θ</italic></td>
<td align="left" rowspan="1" colspan="1">All</td>
<td align="left" rowspan="1" colspan="1">None</td>
<td align="left" rowspan="1" colspan="1">Top <italic>θ</italic></td>
<td align="left" rowspan="1" colspan="1">All</td>
<td align="left" rowspan="1" colspan="1">None</td>
<td align="left" rowspan="1" colspan="1">Top <italic>θ</italic></td>
<td colspan="2" align="left" rowspan="1">All</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Virus</td>
<td align="left" rowspan="1" colspan="1">Accu↑</td>
<td align="left" rowspan="1" colspan="1">0.7476</td>
<td align="left" rowspan="1" colspan="1"><bold>0.7696</bold></td>
<td align="left" rowspan="1" colspan="1">0.7427</td>
<td align="left" rowspan="1" colspan="1">0.7476</td>
<td align="left" rowspan="1" colspan="1"><bold>0.7672</bold></td>
<td align="left" rowspan="1" colspan="1">0.7451</td>
<td align="left" rowspan="1" colspan="1">0.7525</td>
<td align="left" rowspan="1" colspan="1"><bold>0.7784</bold></td>
<td align="left" rowspan="1" colspan="1">0.7672</td>
<td align="left" rowspan="1" colspan="1">0.6397</td>
<td align="left" rowspan="1" colspan="1"><bold>0.7574</bold></td>
<td colspan="2" align="left" rowspan="1">0.7328</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">MCC↑</td>
<td align="left" rowspan="1" colspan="1">0.2518</td>
<td align="left" rowspan="1" colspan="1"><bold>0.3353</bold></td>
<td align="left" rowspan="1" colspan="1">0.2604</td>
<td align="left" rowspan="1" colspan="1">0.2518</td>
<td align="left" rowspan="1" colspan="1"><bold>0.3257</bold></td>
<td align="left" rowspan="1" colspan="1">0.2589</td>
<td align="left" rowspan="1" colspan="1">0.1318</td>
<td align="left" rowspan="1" colspan="1"><bold>0.2634</bold></td>
<td align="left" rowspan="1" colspan="1">0.2095</td>
<td align="left" rowspan="1" colspan="1">0.1255</td>
<td align="left" rowspan="1" colspan="1"><bold>0.2919</bold></td>
<td colspan="2" align="left" rowspan="1">0.1957</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">F1-score↑</td>
<td align="left" rowspan="1" colspan="1">0.4114</td>
<td align="left" rowspan="1" colspan="1"><bold>0.4835</bold></td>
<td align="left" rowspan="1" colspan="1">0.4262</td>
<td align="left" rowspan="1" colspan="1">0.4114</td>
<td align="left" rowspan="1" colspan="1"><bold>0.4751</bold></td>
<td align="left" rowspan="1" colspan="1">0.4222</td>
<td align="left" rowspan="1" colspan="1">0.2406</td>
<td align="left" rowspan="1" colspan="1"><bold>0.3662</bold></td>
<td align="left" rowspan="1" colspan="1">0.3166</td>
<td align="left" rowspan="1" colspan="1">0.3581</td>
<td align="left" rowspan="1" colspan="1"><bold>0.4469</bold></td>
<td colspan="2" align="left" rowspan="1">0.3626</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Avgprec↑</td>
<td align="left" rowspan="1" colspan="1">0.5572</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5982</bold></td>
<td align="left" rowspan="1" colspan="1">0.5438</td>
<td align="left" rowspan="1" colspan="1">0.5572</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5902</bold></td>
<td align="left" rowspan="1" colspan="1">0.5431</td>
<td align="left" rowspan="1" colspan="1">0.4480</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5217</bold></td>
<td align="left" rowspan="1" colspan="1">0.5008</td>
<td align="left" rowspan="1" colspan="1">0.4994</td>
<td align="left" rowspan="1" colspan="1"><bold>0.6061</bold></td>
<td colspan="2" align="left" rowspan="1">0.5757</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Rloss↓</td>
<td align="left" rowspan="1" colspan="1">0.6380</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5654</bold></td>
<td align="left" rowspan="1" colspan="1">0.6390</td>
<td align="left" rowspan="1" colspan="1">0.6380</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5713</bold></td>
<td align="left" rowspan="1" colspan="1">0.6419</td>
<td align="left" rowspan="1" colspan="1">0.8415</td>
<td align="left" rowspan="1" colspan="1"><bold>0.7245</bold></td>
<td align="left" rowspan="1" colspan="1">0.7485</td>
<td align="left" rowspan="1" colspan="1">0.4931</td>
<td align="left" rowspan="1" colspan="1"><bold>0.3326</bold></td>
<td colspan="2" align="left" rowspan="1">0.3622</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Coverage↓</td>
<td align="left" rowspan="1" colspan="1">2.2059</td>
<td align="left" rowspan="1" colspan="1"><bold>2.0441</bold></td>
<td align="left" rowspan="1" colspan="1">2.2941</td>
<td align="left" rowspan="1" colspan="1">2.2059</td>
<td align="left" rowspan="1" colspan="1"><bold>2.0735</bold></td>
<td align="left" rowspan="1" colspan="1">2.3088</td>
<td align="left" rowspan="1" colspan="1">2.5735</td>
<td align="left" rowspan="1" colspan="1"><bold>2.5294</bold></td>
<td align="left" rowspan="1" colspan="1">2.5441</td>
<td align="left" rowspan="1" colspan="1">3.0000</td>
<td align="left" rowspan="1" colspan="1"><bold>2.0147</bold></td>
<td colspan="2" align="left" rowspan="1">2.2941</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Plant</td>
<td align="left" rowspan="1" colspan="1">Accu↑</td>
<td align="left" rowspan="1" colspan="1">0.9061</td>
<td align="left" rowspan="1" colspan="1"><bold>0.9103</bold></td>
<td align="left" rowspan="1" colspan="1">0.9074</td>
<td align="left" rowspan="1" colspan="1">0.9042</td>
<td align="left" rowspan="1" colspan="1"><bold>0.9087</bold></td>
<td align="left" rowspan="1" colspan="1">0.9068</td>
<td align="left" rowspan="1" colspan="1">0.9081</td>
<td align="left" rowspan="1" colspan="1"><bold>0.9112</bold></td>
<td align="left" rowspan="1" colspan="1">0.9090</td>
<td align="left" rowspan="1" colspan="1">0.7350</td>
<td align="left" rowspan="1" colspan="1"><bold>0.7982</bold></td>
<td colspan="2" align="left" rowspan="1">0.7816</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">MCC↑</td>
<td align="left" rowspan="1" colspan="1">0.3906</td>
<td align="left" rowspan="1" colspan="1"><bold>0.4107</bold></td>
<td align="left" rowspan="1" colspan="1">0.3991</td>
<td align="left" rowspan="1" colspan="1">0.4220</td>
<td align="left" rowspan="1" colspan="1"><bold>0.4410</bold></td>
<td align="left" rowspan="1" colspan="1">0.4325</td>
<td align="left" rowspan="1" colspan="1">0.2344</td>
<td align="left" rowspan="1" colspan="1"><bold>0.2924</bold></td>
<td align="left" rowspan="1" colspan="1">0.2636</td>
<td align="left" rowspan="1" colspan="1">0.1428</td>
<td align="left" rowspan="1" colspan="1"><bold>0.1908</bold></td>
<td colspan="2" align="left" rowspan="1">0.1531</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">F1-score↑</td>
<td align="left" rowspan="1" colspan="1">0.4346</td>
<td align="left" rowspan="1" colspan="1"><bold>0.4501</bold></td>
<td align="left" rowspan="1" colspan="1">0.4423</td>
<td align="left" rowspan="1" colspan="1">0.4737</td>
<td align="left" rowspan="1" colspan="1"><bold>0.4893</bold></td>
<td align="left" rowspan="1" colspan="1">0.4823</td>
<td align="left" rowspan="1" colspan="1">0.1864</td>
<td align="left" rowspan="1" colspan="1"><bold>0.2527</bold></td>
<td align="left" rowspan="1" colspan="1">0.2276</td>
<td align="left" rowspan="1" colspan="1">0.2441</td>
<td align="left" rowspan="1" colspan="1"><bold>0.2818</bold></td>
<td colspan="2" align="left" rowspan="1">0.2516</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Avgprec↑</td>
<td align="left" rowspan="1" colspan="1">0.5099</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5142</bold></td>
<td align="left" rowspan="1" colspan="1">0.5121</td>
<td align="left" rowspan="1" colspan="1">0.5762</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5819</bold></td>
<td align="left" rowspan="1" colspan="1">0.5784</td>
<td align="left" rowspan="1" colspan="1">0.2971</td>
<td align="left" rowspan="1" colspan="1"><bold>0.3359</bold></td>
<td align="left" rowspan="1" colspan="1">0.3222</td>
<td align="left" rowspan="1" colspan="1">0.4367</td>
<td align="left" rowspan="1" colspan="1"><bold>0.4622</bold></td>
<td colspan="2" align="left" rowspan="1">0.4266</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Rloss↓</td>
<td align="left" rowspan="1" colspan="1">0.6040</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5975</bold></td>
<td align="left" rowspan="1" colspan="1">0.6001</td>
<td align="left" rowspan="1" colspan="1">0.5212</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5136</bold></td>
<td align="left" rowspan="1" colspan="1">0.5200</td>
<td align="left" rowspan="1" colspan="1">0.8819</td>
<td align="left" rowspan="1" colspan="1"><bold>0.8309</bold></td>
<td align="left" rowspan="1" colspan="1">0.8478</td>
<td align="left" rowspan="1" colspan="1">0.3670</td>
<td align="left" rowspan="1" colspan="1"><bold>0.3372</bold></td>
<td colspan="2" align="left" rowspan="1">0.3722</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Coverage↓</td>
<td align="left" rowspan="1" colspan="1">4.4636</td>
<td align="left" rowspan="1" colspan="1"><bold>4.4291</bold></td>
<td align="left" rowspan="1" colspan="1">4.4674</td>
<td align="left" rowspan="1" colspan="1">3.8867</td>
<td align="left" rowspan="1" colspan="1"><bold>3.8314</bold></td>
<td align="left" rowspan="1" colspan="1">3.8812</td>
<td align="left" rowspan="1" colspan="1">6.2222</td>
<td align="left" rowspan="1" colspan="1"><bold>6.0153</bold></td>
<td align="left" rowspan="1" colspan="1">6.0498</td>
<td align="left" rowspan="1" colspan="1">4.4598</td>
<td align="left" rowspan="1" colspan="1"><bold>4.0728</bold></td>
<td colspan="2" align="left" rowspan="1">4.5402</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Gneg</td>
<td align="left" rowspan="1" colspan="1">Accu↑</td>
<td align="left" rowspan="1" colspan="1">0.8374</td>
<td align="left" rowspan="1" colspan="1"><bold>0.8441</bold></td>
<td align="left" rowspan="1" colspan="1">0.8386</td>
<td align="left" rowspan="1" colspan="1">0.8823</td>
<td align="left" rowspan="1" colspan="1"><bold>0.8829</bold></td>
<td align="left" rowspan="1" colspan="1">0.8811</td>
<td align="left" rowspan="1" colspan="1">0.8950</td>
<td align="left" rowspan="1" colspan="1"><bold>0.8981</bold></td>
<td align="left" rowspan="1" colspan="1">0.8932</td>
<td align="left" rowspan="1" colspan="1">0.8811</td>
<td align="left" rowspan="1" colspan="1"><bold>0.8932</bold></td>
<td colspan="2" align="left" rowspan="1">0.8732</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">MCC↑</td>
<td align="left" rowspan="1" colspan="1">0.4542</td>
<td align="left" rowspan="1" colspan="1"><bold>0.4683</bold></td>
<td align="left" rowspan="1" colspan="1">0.4607</td>
<td align="left" rowspan="1" colspan="1">0.4913</td>
<td align="left" rowspan="1" colspan="1"><bold>0.4970</bold></td>
<td align="left" rowspan="1" colspan="1">0.4918</td>
<td align="left" rowspan="1" colspan="1">0.4924</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5100</bold></td>
<td align="left" rowspan="1" colspan="1">0.4777</td>
<td align="left" rowspan="1" colspan="1">0.4399</td>
<td align="left" rowspan="1" colspan="1"><bold>0.4839</bold></td>
<td colspan="2" align="left" rowspan="1">0.4008</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">F1-score↑</td>
<td align="left" rowspan="1" colspan="1">0.5315</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5429</bold></td>
<td align="left" rowspan="1" colspan="1">0.5366</td>
<td align="left" rowspan="1" colspan="1">0.5591</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5650</bold></td>
<td align="left" rowspan="1" colspan="1">0.5605</td>
<td align="left" rowspan="1" colspan="1">0.5362</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5532</bold></td>
<td align="left" rowspan="1" colspan="1">0.5191</td>
<td align="left" rowspan="1" colspan="1">0.5000</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5294</bold></td>
<td colspan="2" align="left" rowspan="1">0.4655</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Avgprec↑</td>
<td align="left" rowspan="1" colspan="1">0.6291</td>
<td align="left" rowspan="1" colspan="1"><bold>0.6340</bold></td>
<td align="left" rowspan="1" colspan="1">0.6218</td>
<td align="left" rowspan="1" colspan="1">0.6331</td>
<td align="left" rowspan="1" colspan="1"><bold>0.6402</bold></td>
<td align="left" rowspan="1" colspan="1">0.6361</td>
<td align="left" rowspan="1" colspan="1">0.5722</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5847</bold></td>
<td align="left" rowspan="1" colspan="1">0.5572</td>
<td align="left" rowspan="1" colspan="1">0.6935</td>
<td align="left" rowspan="1" colspan="1"><bold>0.7267</bold></td>
<td colspan="2" align="left" rowspan="1">0.7011</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Rloss↓</td>
<td align="left" rowspan="1" colspan="1">0.3891</td>
<td align="left" rowspan="1" colspan="1"><bold>0.3743</bold></td>
<td align="left" rowspan="1" colspan="1">0.3812</td>
<td align="left" rowspan="1" colspan="1">0.4481</td>
<td align="left" rowspan="1" colspan="1"><bold>0.4342</bold></td>
<td align="left" rowspan="1" colspan="1">0.4391</td>
<td align="left" rowspan="1" colspan="1">0.5503</td>
<td align="left" rowspan="1" colspan="1"><bold>0.5343</bold></td>
<td align="left" rowspan="1" colspan="1">0.5707</td>
<td align="left" rowspan="1" colspan="1">0.1715</td>
<td align="left" rowspan="1" colspan="1"><bold>0.1555</bold></td>
<td colspan="2" align="left" rowspan="1">0.1700</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Coverage↓</td>
<td align="left" rowspan="1" colspan="1">2.3010</td>
<td align="left" rowspan="1" colspan="1"><bold>2.1893</bold></td>
<td align="left" rowspan="1" colspan="1">2.2476</td>
<td align="left" rowspan="1" colspan="1">2.6165</td>
<td align="left" rowspan="1" colspan="1"><bold>2.5388</bold></td>
<td align="left" rowspan="1" colspan="1">2.5680</td>
<td align="left" rowspan="1" colspan="1">3.0291</td>
<td align="left" rowspan="1" colspan="1"><bold>2.9417</bold></td>
<td align="left" rowspan="1" colspan="1">3.1214</td>
<td align="left" rowspan="1" colspan="1">1.3204</td>
<td align="left" rowspan="1" colspan="1"><bold>1.2136</bold></td>
<td colspan="2" align="left" rowspan="1">1.3155</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap>
<p>It is worth noting that, the inherent problem with PNEAs is that they can only be experimentally validated. To validate that the proposed strategy is more useful than conventional analysis based simply on PEA, it is better to test it via additional biological experiments. If we can show on PNEA data that the strategy finds true positives and rejects true negatives validated against biological observation of the characteristics of these proteins, the effectiveness of this approach will be further verified. However, in our work, it is difficult to directly conduct biological experiments to validate PNEAs. In a different way, we tried to find true positive and true negative PNEA samples which can be validated against a biological observation in the Swiss-Prot databank. Unfortunately, we found few true positives (e.g. the non-experimental annotation “Golgi apparatus” of the plant protein with entry number “Q9M2T1” has been verified experimentally) and no true negatives. Although the true positives can be successfully found using this strategy, we think the amount of samples identified is too small to provide enough support for this study. Therefore, the related results of these few protein samples are not included in this paper. Moreover, the objective of this study is not to identify true positive proteins, but to make protein subcellular localization prediction tools with better performance in accuracy with the help of non-experimental proteins. According to the results in Table <xref ref-type="table" rid="pone-0067343-t003"><bold>Table 3</bold></xref><bold>, </bold><xref ref-type="table" rid="pone-0067343-t004"><bold>Table 4</bold></xref><bold>, </bold><xref ref-type="table" rid="pone-0067343-t005"><bold>Table 5</bold></xref><bold>, </bold><xref ref-type="table" rid="pone-0067343-t006"><bold>Table 6</bold></xref>, the increase in accuracy over the conventional algorithms after training with these PNEAs indicates the proposed strategy works. Therefore, the proposed method could be thought of as potentially significant, even without the experimental biological validation. However, it is still worth to perform a biological validation for our algorithm, and we hope to cooperate with biochemists to improve this method in the future.</p>
<p>In summary, in order to overcome the shortage of experimental training data in the prediction of protein subcellular location, we mined the proteins with non-experimental annotations and designed a novel active sample selection strategy to find useful PNEA samples. As supplementary training data, these selected samples helped retrain and improve the original basic classifiers. This approach based on the min-max view provides a systematic way for measuring the usefulness of a sample with multiple labels. From the results, it can be clearly seen that the proposed algorithm is significant and valid to increase the predicting performance of all four types of classifiers. We believe that active sample selection techniques in machine learning can be used as a powerful and useful tool to alleviate the data shortage problem and it could be extended to other real-world data mining applications. We also expect that the information of a huge number of proteins with non-experimental annotations can be applied to other biological problems. Furthermore, in order to make the presented method available to compare with the predictors by other interested users, we will make efforts to provide an online prediction web-server with practical value in our future work.</p>
</sec><sec id="s4">
<title>Supporting Information</title>
<supplementary-material id="pone.0067343.s001" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pone.0067343.s001" position="float" xlink:type="simple"><label>Material S1</label><caption>
<p><bold>This includes Tables S1–S3.</bold></p>
<p>(PDF)</p>
</caption></supplementary-material><supplementary-material id="pone.0067343.s002" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pone.0067343.s002" position="float" xlink:type="simple"><label>Material S2</label><caption>
<p><bold>This includes Datasets S1-S6.</bold></p>
<p>(PDF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>The authors wish to thank the editor and anonymous reviewers for their helpful comments and suggestions.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0067343-Nakai1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nakai</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Kanehisa</surname><given-names>M</given-names></name> (<year>1991</year>) <article-title>Expert system for predicting protein localization sites in gram-negative bacteria</article-title>. <source>Proteins: Struct Funct Bioinf</source> <volume>11</volume>: <fpage>95</fpage>–<lpage>110</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Chou1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chou</surname><given-names>KC</given-names></name>, <name name-style="western"><surname>Shen</surname><given-names>HB</given-names></name> (<year>2008</year>) <article-title>Cell-PLoc: a package of web servers for predicting subcellular localization of proteins in various organisms</article-title>. <source>Nat Protocols</source> <volume>2</volume>: <fpage>153</fpage>–<lpage>162</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Chou2"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chou</surname><given-names>KC</given-names></name>, <name name-style="western"><surname>Shen</surname><given-names>HB</given-names></name> (<year>2010</year>) <article-title>Cell-PLoc 2.0: an improved package of web-servers for predicting subcellular localization of proteins in various organisms</article-title>. <source>Natural Science</source> <volume>2</volume>: <fpage>1090</fpage>–<lpage>1103</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Horton1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Horton</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Park</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Obayashi</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Fujita</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Harada</surname><given-names>H</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>WoLF PSORT: protein localization predictor</article-title>. <source>Nucleic Acids Res</source> <volume>35</volume>: <fpage>W585</fpage>–<lpage>W587</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Cao1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cao</surname><given-names>JZ</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>WQ</given-names></name>, <name name-style="western"><surname>Gu</surname><given-names>H</given-names></name> (<year>2012</year>) <article-title>Predicting viral protein subcellular localization with Chou’s pseudo amino acid composition and imbalance-weighted multi-label k-nearest neighbor algorithm</article-title>. <source>Protein Pept Lett</source> <volume>19</volume>: <fpage>1163</fpage>–<lpage>1169</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Gray1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gray</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Bhasin</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Raghava</surname><given-names>GPS</given-names></name> (<year>2005</year>) <article-title>Support vector machine-based method for subcellular localization of human proteins using amino acid compositions, their order, and similarity search</article-title>. <source>J Biol Chem</source> <volume>280</volume>: <fpage>14427</fpage>–<lpage>14432</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Shatkay1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shatkay</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Höglund</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Brady</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Blum</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Dönnes</surname><given-names>P</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>SherLoc: high-accuracy prediction of protein subcellular localization by integrating text and protein sequence data</article-title>. <source>Bioinformatics</source> <volume>23</volume>: <fpage>1410</fpage>–<lpage>1417</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Briesemeister1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Briesemeister</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Blum</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Brady</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Lam</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Kohlbacher</surname><given-names>O</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>SherLoc2: a high-accuracy hybrid method for predicting subcellular localization of proteins</article-title>. <source>J Proteome Res</source> <volume>8</volume>: <fpage>5393</fpage>–<lpage>5366</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Bulashevska1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bulashevska</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Eils</surname><given-names>R</given-names></name> (<year>2006</year>) <article-title>Predicting protein subcellular locations using hierarchical ensemble of Bayesian classifiers based on Markov chains</article-title>. <source>BMC Bioinformatics</source> <volume>7</volume>: <fpage>298</fpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-He1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>He</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Gu</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>WQ</given-names></name> (<year>2012</year>) <article-title>Imbalanced multi-modal multi-label learning for subcellular localization prediction of human proteins with both single and multiple sites</article-title>. <source>PLoS ONE</source> <volume>7</volume>: <fpage>e37155</fpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Emanuelsson1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Emanuelsson</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Nielsen</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Brunak</surname><given-names>S</given-names></name>, <name name-style="western"><surname>von Heijne</surname><given-names>G</given-names></name> (<year>2000</year>) <article-title>Predicting subcellular localization of proteins based on their N-terminal amino acid sequence</article-title>. <source>J Mol Biol</source> <volume>300</volume>: <fpage>1005</fpage>–<lpage>1016</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Ma1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ma</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>WQ</given-names></name>, <name name-style="western"><surname>Gu</surname><given-names>H</given-names></name> (<year>2010</year>) <article-title>Using elman networks ensemble for protein subnuclear location prediction</article-title>. <source>Int J Innov Comput I</source> <volume>6</volume>: <fpage>5093</fpage>–<lpage>5103</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Chou3"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chou</surname><given-names>KC</given-names></name> (<year>2011</year>) <article-title>Some remarks on protein attribute prediction and pseudo amino acid composition</article-title>. <source>J Theor Biol</source> <volume>273</volume>: <fpage>236</fpage>–<lpage>247</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Shen1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shen</surname><given-names>HB</given-names></name>, <name name-style="western"><surname>Chou</surname><given-names>KC</given-names></name> (<year>2010</year>) <article-title>Virus-mPLoc: a fusion classifier for viral protein subcellular location prediction by incorporating multiple sites</article-title>. <source>J Biomol Struct Dyn</source> <volume>28</volume>: <fpage>175</fpage>–<lpage>186</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Xu1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Xu</surname><given-names>Q</given-names></name>, <name name-style="western"><surname>Pan</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Xue</surname><given-names>HH</given-names></name>, <name name-style="western"><surname>Yang</surname><given-names>Q</given-names></name> (<year>2011</year>) <article-title>Multitask learning for protein subcellular location prediction</article-title>. <source>IEEE/ACM Trans Comput Biol Bioinform</source> <volume>8</volume>: <fpage>748</fpage>–<lpage>759</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Junker1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Junker</surname><given-names>VL</given-names></name>, <name name-style="western"><surname>Apweiler</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Bairoch</surname><given-names>A</given-names></name> (<year>1999</year>) <article-title>Representation of functional information in the SWISS-PROT Data Bank</article-title>. <source>Bioinformatics</source> <volume>15</volume>: <fpage>1066</fpage>–<lpage>1067</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Boutet1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boutet</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Lieberherr</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Tognolli</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Schneider</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Bairoch</surname><given-names>A</given-names></name> (<year>2007</year>) <article-title>UniProtKB/Swiss-Prot</article-title>. <source>Methods Mol Biol</source> <volume>406</volume>: <fpage>89</fpage>–<lpage>112</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Settles1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Settles</surname><given-names>B</given-names></name> (<year>2009</year>) <article-title>Active Learning Literature Survey</article-title>. <source>Computer Sciences Technical Report</source> <volume>2009</volume>: <fpage>1648</fpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Hoi1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hoi</surname><given-names>SCH</given-names></name>, <name name-style="western"><surname>Jin</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Zhu</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Lyu</surname><given-names>MR</given-names></name> (<year>2007</year>) <article-title>Semi-supervised SVM batch mode active learning with applications to image retrieval</article-title>. <source>ACM T Inform Syst</source> <volume>27</volume>: <fpage>1</fpage>–<lpage>29</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Chou4"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chou</surname><given-names>KC</given-names></name>, <name name-style="western"><surname>Shen</surname><given-names>HB</given-names></name> (<year>2010</year>) <article-title>Plant-mPLoc: A top-down strategy to augment the power for predicting plant protein subcellular localization</article-title>. <source>PLoS ONE</source> <volume>5</volume>: <fpage>e11335</fpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Shen2"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shen</surname><given-names>HB</given-names></name>, <name name-style="western"><surname>Chou</surname><given-names>KC</given-names></name> (<year>2010</year>) <article-title>Gneg-mPLoc: a top-down strategy to enhance the quality of predicting subcellular localization of Gram-negative bacterial proteins</article-title>. <source>J Theor Biol</source> <volume>264</volume>: <fpage>326</fpage>–<lpage>333</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Wang1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname><given-names>GL</given-names></name>, <name name-style="western"><surname>Dunbrack</surname><given-names>RL</given-names></name> (<year>2003</year>) <article-title>PISCES: a protein sequence culling server</article-title>. <source>Bioinformatics</source> <volume>19</volume>: <fpage>1589</fpage>–<lpage>1591</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Li1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>YW</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>YQ</given-names></name> (<year>2008</year>) <article-title>The nearest neighbor algorithm of local probability centers</article-title>. <source>IEEE T Syst Man Cy B</source> <volume>38</volume>: <fpage>141</fpage>–<lpage>154</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Chou5"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chou</surname><given-names>KC</given-names></name> (<year>2005</year>) <article-title>Using amphiphilic pseudo amino acid composition to predict enzyme subfamily classes</article-title>. <source>Bioinformatics</source> <volume>21</volume>: <fpage>10</fpage>–<lpage>19</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Shen3"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shen</surname><given-names>HB</given-names></name>, <name name-style="western"><surname>Chou</surname><given-names>KC</given-names></name> (<year>2008</year>) <article-title>PseAAC: a flexible web server for generating various kinds of protein pseudo amino acid composition</article-title>. <source>Anal Biochem</source> <volume>373</volume>: <fpage>386</fpage>–<lpage>388</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Huang1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huang</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Shi</surname><given-names>F</given-names></name> (<year>2005</year>) <article-title>Support vector machines for predicting apoptosis proteins types</article-title>. <source>Acta Biotheor</source> <volume>53</volume>: <fpage>39</fpage>–<lpage>47</lpage>.</mixed-citation>
</ref>
<ref id="pone.0067343-Zhang1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname><given-names>ML</given-names></name> (<year>2009</year>) <article-title>ML-RBF : RBF neural networks for multi-label learning</article-title>. <source>Neural Process Lett</source> <volume>29</volume>: <fpage>61</fpage>–<lpage>74</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>