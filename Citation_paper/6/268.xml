<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group>
<journal-title>PLoS ONE</journal-title></journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-13-06640</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0082146</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories>
<title-group>
<article-title>Low-Rank Regularization for Learning Gene Expression Programs</article-title>
<alt-title alt-title-type="running-head">Low-Rank for Learning Gene Expression Programs</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Ye</surname><given-names>Guibo</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Tang</surname><given-names>Mengfan</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Cai</surname><given-names>Jian-Feng</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Nie</surname><given-names>Qing</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Xie</surname><given-names>Xiaohui</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Department of Computer Science, University of California Irvine, Irvine, California, United States of America</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Department of Mathematics, University of California Irvine, Irvine, California, United States of America</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Department of Mathematics, University of Iowa, Iowa City, Iowa, United States of America</addr-line></aff>
<aff id="aff4"><label>4</label><addr-line>Center for Complex Biological Systems, University of California Irvine, Irvine, California, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Muldoon</surname><given-names>Mark R.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Manchester University, United Kingdom</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">xhx@ics.uci.edu</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: GY XX. Performed the experiments: GY MT JC QN XX. Analyzed the data: GY MT JC QN XX. Wrote the paper: GY QN XX.</p></fn>
</author-notes>
<pub-date pub-type="collection"><year>2013</year></pub-date>
<pub-date pub-type="epub"><day>17</day><month>12</month><year>2013</year></pub-date>
<volume>8</volume>
<issue>12</issue>
<elocation-id>e82146</elocation-id>
<history>
<date date-type="received"><day>1</day><month>2</month><year>2013</year></date>
<date date-type="accepted"><day>30</day><month>10</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2013</copyright-year>
<copyright-holder>Ye et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Learning gene expression programs directly from a set of observations is challenging due to the complexity of gene regulation, high noise of experimental measurements, and insufficient number of experimental measurements. Imposing additional constraints with strong and biologically motivated regularizations is critical in developing reliable and effective algorithms for inferring gene expression programs. Here we propose a new form of regulation that constrains the number of independent connectivity patterns between regulators and targets, motivated by the modular design of gene regulatory programs and the belief that the total number of independent regulatory modules should be small. We formulate a multi-target linear regression framework to incorporate this type of regulation, in which the number of independent connectivity patterns is expressed as the rank of the connectivity matrix between regulators and targets. We then generalize the linear framework to nonlinear cases, and prove that the generalized low-rank regularization model is still convex. Efficient algorithms are derived to solve both the linear and nonlinear low-rank regularized problems. Finally, we test the algorithms on three gene expression datasets, and show that the low-rank regularization improves the accuracy of gene expression prediction in these three datasets.</p>
</abstract>
<funding-group><funding-statement>The work was partly supported by a grant from National Institute of Health and a grant from National Science Foundation. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="9"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Systematically discovering gene expression programs within cells is a fundamental goal in both basic and applied biomedical researches, and is crucial for elucidating factors determining cell types, controlling cellular states, or switching cells from healthy states to diseased ones. Although the total number of genes within an organism is usually large (e.g., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e001" xlink:type="simple"/></inline-formula> in humans), most of these genes are believed to be regulated by a much smaller subset of genes called regulators (e.g., transcription factors, signalling molecules, growth factors, etc.) A challenge in computational biology is how to use machine learning methods to automatically discover the mapping from regulators to target genes, thereby inferring the underlying regulatory programs, from a given set of observations <xref ref-type="bibr" rid="pone.0082146-Bansal1">[1]</xref>.</p>
<p>More specifically, suppose we are given a set of observations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e002" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e003" xlink:type="simple"/></inline-formula> denotes the expression of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e004" xlink:type="simple"/></inline-formula> regulators and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e005" xlink:type="simple"/></inline-formula> denotes the expression of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e006" xlink:type="simple"/></inline-formula> target genes in sample <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e007" xlink:type="simple"/></inline-formula>. The goal of learning gene expression programs is to infer the mapping <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e008" xlink:type="simple"/></inline-formula> that fits the observation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e009" xlink:type="simple"/></inline-formula>, and to provide biological interpretations of the inferred mapping.</p>
<p>In addition to the purpose of uncovering gene regulatory mechanisms, the gene expression program learning problem arises recently also in a practical and applied setting in biotechnology development. High-throughput gene expression profiling using Affymetrix arrays typically costs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e010" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e011" xlink:type="simple"/></inline-formula> for human and mouse, respectively, which is still too expensive to be used in large-scale perturbation, drug or small molecule screening, which typically requires tens of thousands of or even millions of expression profiles <xref ref-type="bibr" rid="pone.0082146-Lamb1">[2]</xref>. This constraint has motivated the development and adoption of the Luminex bead technology, which is able to measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e012" xlink:type="simple"/></inline-formula> genes at a much lower cost (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e013" xlink:type="simple"/></inline-formula> per profile). Because the gene expression is so highly correlated, scientists are proposing to use Luminex bead to measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e014" xlink:type="simple"/></inline-formula> carefully chosen “landmark” genes and to computationally extrapolate all remaining ones. This strategy will be able to significantly cut the cost of expression profiling; however, it also calls for better and more efficient methods for target gene expression prediction.</p>
<p>Although the gene expression program learning problem formulated above fits into standard supervised learning, solving the problem is difficult for a number of reasons. First, the total number of parameters determining the mapping from regulators to target genes is typically much greater than the total number of observations. Secondly, the gene expression measurements based on high-throughput techniques are known to be highly noisy. These factors make the gene expression program inference highly challenging. A number of methods have been proposed for gene expression program learning, including methods based on probabilistic graphical models <xref ref-type="bibr" rid="pone.0082146-Friedman1">[3]</xref>–<xref ref-type="bibr" rid="pone.0082146-Grzegorczyk1">[5]</xref>, information-theoretic approaches <xref ref-type="bibr" rid="pone.0082146-Faith1">[6]</xref>, <xref ref-type="bibr" rid="pone.0082146-Margolin1">[7]</xref>, and ordinary differential equations (ODEs) <xref ref-type="bibr" rid="pone.0082146-Gustafsson1">[8]</xref>, <xref ref-type="bibr" rid="pone.0082146-Gardner1">[9]</xref>. See <xref ref-type="bibr" rid="pone.0082146-Bansal1">[1]</xref>, <xref ref-type="bibr" rid="pone.0082146-DeJong1">[10]</xref> for reviews of these and other approaches. However, the performance of these methods tend to be modest.</p>
<p>In this work, we formulate the gene expression program learning as a multi-target (more specifically <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e015" xlink:type="simple"/></inline-formula>-target) regression problem, and use Tikhonov regularization to constrain the space of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e016" xlink:type="simple"/></inline-formula>-target mapping. Two main forms of regularization with biological motivations have been proposed in the literature: a) <italic>sparsity</italic> - each target is likely regulated by only a few regulators instead of all, and b) <italic>modularity</italic> - the expression program is organized into modules, each consisting of a certain combination of regulators, and the total number of independent modules should be small. The sparsity regularization is well recognized and widely used in gene expression program inference <xref ref-type="bibr" rid="pone.0082146-Gardner1">[9]</xref>, <xref ref-type="bibr" rid="pone.0082146-Christley1">[11]</xref>, <xref ref-type="bibr" rid="pone.0082146-Lee1">[12]</xref>. Some of these previous work are based on graphical models <xref ref-type="bibr" rid="pone.0082146-Friedman2">[4]</xref>, <xref ref-type="bibr" rid="pone.0082146-Ye1">[13]</xref>, while others are based on regression. Within the regression framework, a common strategy of imposing sparsity is to use <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e017" xlink:type="simple"/></inline-formula> norm regularization on regression coefficients, similar to the framework of Lasso (least absolute shrinkage and selection operator) <xref ref-type="bibr" rid="pone.0082146-Christley1">[11]</xref>, <xref ref-type="bibr" rid="pone.0082146-Tibshirani1">[14]</xref>.</p>
<p>The modularity regularization is much more difficult to handle and is the focus of this work. A popular approach is the probabilistic graphic model proposed by Segal et al. <xref ref-type="bibr" rid="pone.0082146-Segal1">[15]</xref>, which assigns target genes into different modules and constrains the genes within each module to be identically distributed, and models the regulatory program associated with each module using a rule-based decision tree. However, the Segal model is difficult to train, requiring long running time and only being able to find locally optimal solutions. In addition, the Segal model only captures the qualitative relationship between the regulators and targets since its main purpose is not on predicting the expression of target genes. Another approach taking the modularity structure into account is the SIMoNe proposed by Chiquet et al. <xref ref-type="bibr" rid="pone.0082146-Chiquet1">[16]</xref>. SIMoNe models gene expression data using Gaussian graphical models, and imposes sparsity constraints on the inverse covariance matrix and introduces hidden nodes to the Gaussian graph to learn the network modularity. The primary goal of SIMoNe is to infer the gene regulatory networks in an unsupervised way without distinguishing regulator and target genes, which is different from our main objective.</p>
<p>Here we propose a new approach to incorporating the modularity constraint. We use the rank of the connectivity matrix between regulators and targets to represent the number of independent regulatory modules between them. The modularity regularization is then formulated as a low-rank constraint within a multi-target linear regression framework. The resulting model is convex, and we describe an efficient algorithm to find its globally optimal solution. We further show that the low-rank regularized regression problem can also be generalized to nonlinear cases, where we regularize the dimension of the hypothesis space of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e018" xlink:type="simple"/></inline-formula>-target regression function. We prove that the resulting nonlinear low-rank model is still convex and derive an efficient algorithm to solve it. Finally, we benchmark the performance of the low-rank regulation models on two real biological datasets, and show that the low-rank regulation technique consistently improve prediction accuracy in both cases when compared to the Lasso model.</p>
</sec><sec id="s2" sec-type="methods">
<title>Methods</title>
<sec id="s2a">
<title>Learning gene expression programs in linear space</title>
<p>We begin by introducing some notations. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e019" xlink:type="simple"/></inline-formula> be the set of real numbers and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e020" xlink:type="simple"/></inline-formula> the subset of non-negative ones. Denote <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e021" xlink:type="simple"/></inline-formula> to be the inner product of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e022" xlink:type="simple"/></inline-formula>. We assume that we have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e023" xlink:type="simple"/></inline-formula> target genes and define <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e024" xlink:type="simple"/></inline-formula>. We further assume that for the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e025" xlink:type="simple"/></inline-formula>-th target gene (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e026" xlink:type="simple"/></inline-formula>), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e027" xlink:type="simple"/></inline-formula> samples <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e028" xlink:type="simple"/></inline-formula> are available, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e029" xlink:type="simple"/></inline-formula> denotes the expression of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e030" xlink:type="simple"/></inline-formula> regulators and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e031" xlink:type="simple"/></inline-formula> denotes the expression of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e032" xlink:type="simple"/></inline-formula>-th target gene in sample <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e033" xlink:type="simple"/></inline-formula>. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e034" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e035" xlink:type="simple"/></inline-formula>. The goal of learning gene expression programs is to infer the mapping <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e036" xlink:type="simple"/></inline-formula> that fits the observation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e037" xlink:type="simple"/></inline-formula>, and to provide biological interpretations of the inferred mapping.</p>
<p>In this section, we assume that each target gene is linearly regulated by the regulators. That is, for each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e038" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e039" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e040" xlink:type="simple"/></inline-formula> is a fixed vector of coefficients.</p>
<p>Next we describe two types of regularization that can be used for gene expression program learning: one is the <italic>sparsity</italic> regularization, which has already been widely used in the literature, and the second is the <italic>low-rank</italic> regularization, which has not been used in the gene expression program learning, although having recently become popular in other problem domains such as matrix completion, covariance matrix estimation, metric learning, etc <xref ref-type="bibr" rid="pone.0082146-Cai1">[17]</xref>–<xref ref-type="bibr" rid="pone.0082146-Ying1">[20]</xref>.</p>
</sec><sec id="s2b">
<title>Sparsity regularization</title>
<p>Given the observation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e041" xlink:type="simple"/></inline-formula>, a natural way to infer <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e042" xlink:type="simple"/></inline-formula> is to solve a least-square minimization problem:<disp-formula id="pone.0082146.e043"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e043" xlink:type="simple"/><label>(1)</label></disp-formula>where the norm is the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e044" xlink:type="simple"/></inline-formula> norm by default. However, for the gene expression program learning problem, the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e045" xlink:type="simple"/></inline-formula> inferred by least-square minimization tends to be poor for a number of reasons: 1) the observations as measured by microarrays are usually very noisy, and 2) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e046" xlink:type="simple"/></inline-formula> is usually much larger than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e047" xlink:type="simple"/></inline-formula>, which can lead to overfitting. Various regularization techniques have been introduced to prevent overfitting including ridge regression <xref ref-type="bibr" rid="pone.0082146-Hoerl1">[21]</xref> and Lasso <xref ref-type="bibr" rid="pone.0082146-Tibshirani1">[14]</xref>. Since each target gene is likely regulated by only a few regulators instead of all, a commonly used regularization technique in gene expression program learning is to impose an <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e048" xlink:type="simple"/></inline-formula>-norm based sparsity regularization on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e049" xlink:type="simple"/></inline-formula> as in Lasso <xref ref-type="bibr" rid="pone.0082146-Gardner1">[9]</xref>, <xref ref-type="bibr" rid="pone.0082146-Christley1">[11]</xref>, <xref ref-type="bibr" rid="pone.0082146-Lee1">[12]</xref>, <xref ref-type="bibr" rid="pone.0082146-Tibshirani1">[14]</xref>:<disp-formula id="pone.0082146.e050"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e050" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e051" xlink:type="simple"/></inline-formula> is a regularization parameter. We will call (2) the <italic>Lasso model</italic> in the following.</p>
</sec><sec id="s2c">
<title>Low-rank regularization</title>
<p>In the Lasso model, we treat each regulation function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e052" xlink:type="simple"/></inline-formula> separately and learn them independently. However, it is well-known that the expression values of target genes are often highly correlated, and biologists believe that this high correlation is caused by sharing of regulatory programs among different genes. In addition, although there exist <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e053" xlink:type="simple"/></inline-formula> regulators in an organism, the number regulatory programs (called modules by biologists) active in a particular experimental setting is often much lower than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e054" xlink:type="simple"/></inline-formula>. These considerations suggest that instead of learning each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e055" xlink:type="simple"/></inline-formula> separately for each gene, we should learn all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e056" xlink:type="simple"/></inline-formula>'s jointly, and impose a new regularization on the dimension of the span of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e057" xlink:type="simple"/></inline-formula>'s.</p>
<p>Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e058" xlink:type="simple"/></inline-formula> be a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e059" xlink:type="simple"/></inline-formula> matrix with each column corresponding to one <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e060" xlink:type="simple"/></inline-formula>. Constraining the dimension of the span of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e061" xlink:type="simple"/></inline-formula>'s is equivalent to regularizing the rank of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e062" xlink:type="simple"/></inline-formula>, which motivates us to propose the following model to learn gene expression programs<disp-formula id="pone.0082146.e063"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e063" xlink:type="simple"/><label>(3)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e064" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e065" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e066" xlink:type="simple"/></inline-formula> denotes the Frobenius norm for matrix. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e067" xlink:type="simple"/></inline-formula> is the nuclear norm of matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e068" xlink:type="simple"/></inline-formula>, defined to be the sum of the singular values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e069" xlink:type="simple"/></inline-formula>. The nuclear norm is a convex function and is often used as a convex relaxation of the rank of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e070" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pone.0082146-Cands2">[22]</xref>, <xref ref-type="bibr" rid="pone.0082146-Donoho1">[23]</xref>. Since nuclear norm is convex, model (3) is a convex optimization problem. We will call (3) the <italic>linear low-rank model</italic> in the following. The linear low-rank model has not been proposed for gene expression analysis, although it has appeared in other problem domains such as matrix completion, covariance matrix estimation, metric learning, etc <xref ref-type="bibr" rid="pone.0082146-Cai1">[17]</xref>–<xref ref-type="bibr" rid="pone.0082146-Ying1">[20]</xref>.</p>
</sec><sec id="s2d">
<title>Low-rank regularization for learning gene expression programs in nonlinear space</title>
<p>Next we show that the low rank regularization can also be extended to learn nonlinear gene expression program. We start by proposing a low-rank regularization model in the Hilbert space, then prove that the model is a well-defined convex problem, and finally provide an algorithm to solve the model in the reproducing kernel Hilbert space (RKHS).</p>
<sec id="s2d1">
<title>Low-rank model in Hilbert Space</title>
<p>We assume that each target gene is nonlinearly regulated by its regulators and the mapping <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e071" xlink:type="simple"/></inline-formula>, which is a Hilbert space. Furthermore, we assume that the mappings of different target genes are related to each other in such a way that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e072" xlink:type="simple"/></inline-formula> lies in a common low-dimensional subspace of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e073" xlink:type="simple"/></inline-formula>. Note that the assumption of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e074" xlink:type="simple"/></inline-formula> sharing a common subspace in Hilbert space is a natural generalization of the low-rank constraint in the linear case, where the weighting vectors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e075" xlink:type="simple"/></inline-formula> share a low-dimensional subspace in Euclidean space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e076" xlink:type="simple"/></inline-formula>.</p>
<p>Under the above assumption, the space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e077" xlink:type="simple"/></inline-formula>, consisting of all linear combinations of functions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e078" xlink:type="simple"/></inline-formula>, is a low-dimensional subspace in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e079" xlink:type="simple"/></inline-formula>. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e080" xlink:type="simple"/></inline-formula>. Denote an operator <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e081" xlink:type="simple"/></inline-formula>,<disp-formula id="pone.0082146.e082"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e082" xlink:type="simple"/><label>(4)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e083" xlink:type="simple"/></inline-formula> Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e084" xlink:type="simple"/></inline-formula> be the range of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e085" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e086" xlink:type="simple"/></inline-formula> be the adjoint operator of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e087" xlink:type="simple"/></inline-formula>. Then, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e088" xlink:type="simple"/></inline-formula>. It is easy to see that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e089" xlink:type="simple"/></inline-formula> is a compact operator, and the dimension of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e090" xlink:type="simple"/></inline-formula> is finite and determined by the number of nonzero singular values of the operator <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e091" xlink:type="simple"/></inline-formula>. In order to enforce <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e092" xlink:type="simple"/></inline-formula> lying in a low-dimensional subspace in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e093" xlink:type="simple"/></inline-formula>, we can choose the following regularization term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e094" xlink:type="simple"/></inline-formula>, which equals to the number of nonzero eigenvalues of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e095" xlink:type="simple"/></inline-formula>, and regularizes the dimension of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e096" xlink:type="simple"/></inline-formula>. However, this regularization term is difficult to calculate as it is both nonconvex and nonsmooth. Motivated by the theory of compressed sensing and matrix completion <xref ref-type="bibr" rid="pone.0082146-Cands2">[22]</xref>, <xref ref-type="bibr" rid="pone.0082146-Donoho1">[23]</xref>, we use a convex relaxation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e097" xlink:type="simple"/></inline-formula> by taking the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e098" xlink:type="simple"/></inline-formula> norm of all eigenvalues of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e099" xlink:type="simple"/></inline-formula> as the regularization term, that is,<disp-formula id="pone.0082146.e100"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e100" xlink:type="simple"/><label>(5)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e101" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e102" xlink:type="simple"/></inline-formula> being the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e103" xlink:type="simple"/></inline-formula>-th singular value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e104" xlink:type="simple"/></inline-formula>.</p>
<p>We prove Theorem 1 in <xref ref-type="supplementary-material" rid="pone.0082146.s001">Material S1</xref>, which shows that the regularization term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e105" xlink:type="simple"/></inline-formula> is convex, and can be rewritten as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e106" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e107" xlink:type="simple"/></inline-formula> is an <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e108" xlink:type="simple"/></inline-formula> square matrix with the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e109" xlink:type="simple"/></inline-formula> entry being <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e110" xlink:type="simple"/></inline-formula>, the inner product between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e111" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e112" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e113" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s2d2">
<title>Theorem 1</title>
<p><italic>Let </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e114" xlink:type="simple"/></inline-formula><italic> and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e115" xlink:type="simple"/></inline-formula><italic> be the inner product in </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e116" xlink:type="simple"/></inline-formula><italic>. The operator </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e117" xlink:type="simple"/></inline-formula><italic> is defined by (4). Then</italic></p>
<list list-type="order"><list-item>
<p><italic>for any</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e118" xlink:type="simple"/></inline-formula>, <italic>we have</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e119" xlink:type="simple"/></inline-formula></p>
</list-item><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e120" xlink:type="simple"/></inline-formula> <italic>is a linear operator from</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e121" xlink:type="simple"/></inline-formula> <italic>and</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e122" xlink:type="simple"/></inline-formula></p>
</list-item><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e123" xlink:type="simple"/></inline-formula> <italic>is convex. That is, for any</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e124" xlink:type="simple"/></inline-formula> <italic>and</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e125" xlink:type="simple"/></inline-formula>, <italic>we have</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e126" xlink:type="simple"/></inline-formula>.</p>
</list-item></list>
<p>Based on the above formulation and using least square error for the data fitting term, we therefore propose to learn gene expression programs in Hilbert space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e127" xlink:type="simple"/></inline-formula> by minimizing the following objective function<disp-formula id="pone.0082146.e128"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e128" xlink:type="simple"/><label>(6)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e129" xlink:type="simple"/></inline-formula> is a regularization parameter. We will refer to this model as <italic>nonlinear low-rank model</italic>.</p>
</sec><sec id="s2d3">
<title>Linear case</title>
<p>Next we will show that model (6) can be viewed as a generalization of the low-rank model (3) from linear setting to nonlinear setting. We assume that each target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e130" xlink:type="simple"/></inline-formula> is well described by a linear function defined, for every <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e131" xlink:type="simple"/></inline-formula>, as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e132" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e133" xlink:type="simple"/></inline-formula> is a fixed vector of coefficients. As these linear functions are uniquely determined by those coefficients <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e134" xlink:type="simple"/></inline-formula>, we can define the inner product for linear functions as<disp-formula id="pone.0082146.e135"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e135" xlink:type="simple"/><label>(7)</label></disp-formula>That is, we have implicitly chosen the Hilbert space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e136" xlink:type="simple"/></inline-formula>. Denote <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e137" xlink:type="simple"/></inline-formula>. Using (7), we have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e138" xlink:type="simple"/></inline-formula>. Note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e139" xlink:type="simple"/></inline-formula>. Therefore, (6) can be reformulated as<disp-formula id="pone.0082146.e140"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e140" xlink:type="simple"/><label>(8)</label></disp-formula></p>
</sec><sec id="s2d4">
<title>Kernel case</title>
<p>Reproducing kernel Hilbert space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e141" xlink:type="simple"/></inline-formula> is widely used in statistical inference and machine learning <xref ref-type="bibr" rid="pone.0082146-Cristianini1">[24]</xref>–<xref ref-type="bibr" rid="pone.0082146-Vapnik1">[26]</xref>. It is associated with a Mercer kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e142" xlink:type="simple"/></inline-formula> which is a continuous, symmetric and positive semidefinite function <xref ref-type="bibr" rid="pone.0082146-Aronszajn1">[27]</xref>. We denote its inner product as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e143" xlink:type="simple"/></inline-formula>. The reproducing property of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e144" xlink:type="simple"/></inline-formula> states that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e145" xlink:type="simple"/></inline-formula>, for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e146" xlink:type="simple"/></inline-formula>.</p>
<p>The nonlinear low-rank model (6) can be much simplified when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e147" xlink:type="simple"/></inline-formula>. We prove the following representer theorem in <xref ref-type="supplementary-material" rid="pone.0082146.s001">Material S1</xref>.</p>
</sec><sec id="s2d5">
<title>Theorem 2</title>
<p><italic>Given a data set </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e148" xlink:type="simple"/></inline-formula><italic>, then the minimizer</italic><disp-formula id="pone.0082146.e149"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e149" xlink:type="simple"/><label>(9)</label></disp-formula><italic>exists and each component </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e150" xlink:type="simple"/></inline-formula><italic> takes the following form</italic><disp-formula id="pone.0082146.e151"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e151" xlink:type="simple"/></disp-formula><italic>where </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e152" xlink:type="simple"/></inline-formula><italic> for </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e153" xlink:type="simple"/></inline-formula>.</p>
<p>As a consequence, the minimizer of (6) exists, and each component of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e154" xlink:type="simple"/></inline-formula> lies in the finite dimensional space spanned by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e155" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e156" xlink:type="simple"/></inline-formula>. More specifically, we can show that the solution to the nonlinear low-rank model (6) is<disp-formula id="pone.0082146.e157"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e157" xlink:type="simple"/></disp-formula>for each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e158" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e159" xlink:type="simple"/></inline-formula>'s are the coefficients. Furthermore, it can be shown that the coefficients are determined as the optimal solution that minimizes the following convex function<disp-formula id="pone.0082146.e160"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e160" xlink:type="simple"/><label>(10)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e161" xlink:type="simple"/></inline-formula> is a column vector, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e162" xlink:type="simple"/></inline-formula> is an <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e163" xlink:type="simple"/></inline-formula> matrix, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e164" xlink:type="simple"/></inline-formula> is an <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e165" xlink:type="simple"/></inline-formula> with the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e166" xlink:type="simple"/></inline-formula>-th column denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e167" xlink:type="simple"/></inline-formula>. The problem (10) is of finite dimension, and next we describe an algorithm to solve it.</p>
</sec></sec><sec id="s2e">
<title>Algorithms</title>
<p>In this section, we derive computational algorithms to solve low-rank regularized linear model (3) and nonlinear model (10).</p>
<sec id="s2e1">
<title>Low-rank regularized linear model</title>
<p>Decompose the objective function (3), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e168" xlink:type="simple"/></inline-formula> into two parts with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e169" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e170" xlink:type="simple"/></inline-formula>. The first component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e171" xlink:type="simple"/></inline-formula> is both convex and differentiable, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e172" xlink:type="simple"/></inline-formula>. However, the second component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e173" xlink:type="simple"/></inline-formula> is not differentiable, although it is still convex.</p>
<p>Define <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e174" xlink:type="simple"/></inline-formula> to be<disp-formula id="pone.0082146.e175"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e175" xlink:type="simple"/><label>(11)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e176" xlink:type="simple"/></inline-formula> is a given matrix and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e177" xlink:type="simple"/></inline-formula> is a positive scalar. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e178" xlink:type="simple"/></inline-formula> can be viewed as an approximation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e179" xlink:type="simple"/></inline-formula> around <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e180" xlink:type="simple"/></inline-formula>, and the approximation is accurate when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e181" xlink:type="simple"/></inline-formula> is sufficiently close to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e182" xlink:type="simple"/></inline-formula>. Although it is still a non-differentiable function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e183" xlink:type="simple"/></inline-formula>, there exists a unique minimizer of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e184" xlink:type="simple"/></inline-formula> for a given <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e185" xlink:type="simple"/></inline-formula>, and the solution can be written down explicitly. Denote the minimizer of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e186" xlink:type="simple"/></inline-formula> for a given <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e187" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e188" xlink:type="simple"/></inline-formula>. Next we write down an explicit formula for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e189" xlink:type="simple"/></inline-formula>.</p>
<p>Given a matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e190" xlink:type="simple"/></inline-formula> of rank <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e191" xlink:type="simple"/></inline-formula>, denote the singular value decomposition of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e192" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e193" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e194" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e195" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e196" xlink:type="simple"/></inline-formula> being singular values. For any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e197" xlink:type="simple"/></inline-formula>, define the following soft-thresholding operator of matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e198" xlink:type="simple"/></inline-formula><disp-formula id="pone.0082146.e199"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e199" xlink:type="simple"/><label>(12)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e200" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e201" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e202" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e203" xlink:type="simple"/></inline-formula> otherwise. With this definition, it can be shown that <xref ref-type="bibr" rid="pone.0082146-Cai1">[17]</xref><disp-formula id="pone.0082146.e204"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e204" xlink:type="simple"/><label>(13)</label></disp-formula></p>
<p>Using the above-mentioned notations and definitions, a Nesterov's algorithm <xref ref-type="bibr" rid="pone.0082146-Nesterov1">[28]</xref> can be derived to solve the low-rank regularized linear model (3). The detailed steps are shown in Algorithm 1.</p>
</sec><sec id="s2e2">
<title>Algorithm 1</title>
<p>Nesterov's algorithm for solving (3) with backtracking Initialize <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e205" xlink:type="simple"/></inline-formula>. Set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e206" xlink:type="simple"/></inline-formula></p>
</sec></sec><sec id="s2f">
<title>repeat</title>
<p>Step k, 1) Find the smallest nonnegative integers <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e207" xlink:type="simple"/></inline-formula> such that with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e208" xlink:type="simple"/></inline-formula><disp-formula id="pone.0082146.e209"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e209" xlink:type="simple"/></disp-formula></p>
<p>Step k, 2) Set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e210" xlink:type="simple"/></inline-formula> and compute<disp-formula id="pone.0082146.e211"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e211" xlink:type="simple"/></disp-formula><disp-formula id="pone.0082146.e212"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e212" xlink:type="simple"/></disp-formula><disp-formula id="pone.0082146.e213"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e213" xlink:type="simple"/></disp-formula></p>
</sec><sec id="s2g">
<title>until</title>
<p>Convergence</p>
<sec id="s2g1">
<title>Low-rank regularized nonlinear model</title>
<p>We first convert the problem (10) into a more compact form by changing the optimization variables. Then we derive an algorithm to solve the problem based on the Nesterov's method <xref ref-type="bibr" rid="pone.0082146-Beck1">[29]</xref>, <xref ref-type="bibr" rid="pone.0082146-Nesterov2">[30]</xref>. Note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e214" xlink:type="simple"/></inline-formula> is symmetric and positive semidefinite, so is its square root <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e215" xlink:type="simple"/></inline-formula>. Denote the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e216" xlink:type="simple"/></inline-formula>-th column of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e217" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e218" xlink:type="simple"/></inline-formula>, i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e219" xlink:type="simple"/></inline-formula>. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e220" xlink:type="simple"/></inline-formula> and write <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e221" xlink:type="simple"/></inline-formula> Then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e222" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pone.0082146.e160">equation (10)</xref> can be rewritten as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e223" xlink:type="simple"/></inline-formula><disp-formula id="pone.0082146.e224"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e224" xlink:type="simple"/><label>(14)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e225" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e226" xlink:type="simple"/></inline-formula>. Thus finding a solution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e227" xlink:type="simple"/></inline-formula> of <xref ref-type="disp-formula" rid="pone.0082146.e160">equation (10)</xref> is equivalent to identifying,<disp-formula id="pone.0082146.e228"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e228" xlink:type="simple"/><label>(15)</label></disp-formula>followed by setting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e229" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e230" xlink:type="simple"/></inline-formula> is the (pseudo) inverse of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e231" xlink:type="simple"/></inline-formula> when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e232" xlink:type="simple"/></inline-formula> is (not) invertible.</p>
<p>Similar to the linear case, we first decompose <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e233" xlink:type="simple"/></inline-formula> into two parts with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e234" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e235" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e236" xlink:type="simple"/></inline-formula> is differentiable and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e237" xlink:type="simple"/></inline-formula>.</p>
<p>Define <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e238" xlink:type="simple"/></inline-formula> as the following form,<disp-formula id="pone.0082146.e239"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e239" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e240" xlink:type="simple"/></inline-formula> is a given matrix and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e241" xlink:type="simple"/></inline-formula>.</p>
<p>The unique minimizer of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e242" xlink:type="simple"/></inline-formula> is denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e243" xlink:type="simple"/></inline-formula>, and we apply soft-thresholding operator (12) to give the explicit form of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e244" xlink:type="simple"/></inline-formula>,<disp-formula id="pone.0082146.e245"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e245" xlink:type="simple"/><label>(16)</label></disp-formula>With the above-mentioned notations and definitions, we derive Algorithm 3(′)@ to solve problem (10).</p>
</sec><sec id="s2g2">
<title>Algorithm 2</title>
<p>Nesterov's algorithm for solving (10) with backtracking Initialize <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e246" xlink:type="simple"/></inline-formula>. Set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e247" xlink:type="simple"/></inline-formula></p>
</sec></sec><sec id="s2h">
<title>repeat</title>
<p>Step k, 1) Find the smallest nonnegative integers <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e248" xlink:type="simple"/></inline-formula> such that with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e249" xlink:type="simple"/></inline-formula><disp-formula id="pone.0082146.e250"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e250" xlink:type="simple"/></disp-formula></p>
<p>Step k, 2) Set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e251" xlink:type="simple"/></inline-formula> and compute<disp-formula id="pone.0082146.e252"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e252" xlink:type="simple"/></disp-formula><disp-formula id="pone.0082146.e253"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e253" xlink:type="simple"/></disp-formula><disp-formula id="pone.0082146.e254"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0082146.e254" xlink:type="simple"/></disp-formula></p>
</sec><sec id="s2i">
<title>until</title>
<p>Convergence</p>
<p>Return <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e255" xlink:type="simple"/></inline-formula></p>
</sec></sec><sec id="s3">
<title>Results</title>
<p>Next we test the performance of the low-rank regularization models (both linear and nonlinear) described above on three real biological datasets. We will compare the performance of our models to the Lasso model (2), which imposes a sparsity constraint within a linear regression framework, and the SiMoNe model <xref ref-type="bibr" rid="pone.0082146-Chiquet1">[16]</xref>, which models the modularity structure with a Gaussian graphical model framework. In each of the three experiments, we divide the data into training and test datasets. The models are trained based on training data, and the performance of the resulting models are then evaluated based on test data. We use root-mean-square error (RMSE) to measure the differences between values predicted by each model and the the values actually observed. The average RMSE over all target genes measured on the training and test data will be called training and testing error, respectively. The regularization parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e256" xlink:type="simple"/></inline-formula> of our models is automatically tuned through ten-fold cross-validation based only on the training data, and is set to be the value that gives rise to the best cross-validation performance.</p>
<sec id="s3a">
<title>Yeast gene expression data</title>
<p>We tested our models on a yeast gene expression dataset <xref ref-type="bibr" rid="pone.0082146-Gasch1">[31]</xref>, which contains mRNA measurements of 2,355 genes of <italic>Saccharomyces cerevisiae</italic> responding to diverse environmental transitions including temperature shocks, amino acid starvation, hydrogen peroxide, etc. Overall the dataset contains microarray measurements of yeast genes in 173 environmental transitions (will be referred to as samples). The dataset was rescaled to make the expression values of each gene to be mean 0 and variance 1 across the 173 samples. We used a list of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e257" xlink:type="simple"/></inline-formula> candidate regulators manually compiled in <xref ref-type="bibr" rid="pone.0082146-Segal1">[15]</xref> based on biological annotations (including transcription factors and signaling molecules) as our regulator genes. We used this dataset to learn the regulatory relationship between these 321 regulators and the other 2,034 genes, which will be called targets. We benchmarked the performance of our and control models using ten-fold cross-validation. More specially, we randomly partitioned the 173 samples into 10 nonoverlapping subsets. Each model was trained using nine of the ten subsets, the regularization parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e258" xlink:type="simple"/></inline-formula> in each model was tuned via cross validation on 10 dimensional logarithmically spaced vector ranging between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e259" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e260" xlink:type="simple"/></inline-formula>. After choosing the lambda, the test performance of the learned model was then measured using the remaining subset. We used root-mean-square error (RMSE) to measure the differences between values predicted by each model and the the values actually observed. The average RMSE over all target genes measured on the training and test data will be called training and testing error, respectively.</p>
<p>The training and test performance of four models - SiMoNe <xref ref-type="bibr" rid="pone.0082146-Chiquet1">[16]</xref>, linear low-rank (3), nonlinear low-rank (10), and Lasso (2), on the yeast gene expression dataset is summarized in <xref ref-type="table" rid="pone-0082146-t001">Table 1</xref>. The linear low-rank model (3) reduces training error by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e261" xlink:type="simple"/></inline-formula> and testing error by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e262" xlink:type="simple"/></inline-formula> when compared to the Lasso model (2). The regression-based models, including ours and Lasso, significantly outperform SiMoNe in both training and test performance. If we look specifically at the prediction accuracy of each target gene, we note that 84% of the targets are predicted more accurately by the linear low-rank model than by Lasso (<xref ref-type="fig" rid="pone-0082146-g001">Figure 1</xref>). We used an ANOVA kernel to train the nonlinear low-rank model <xref ref-type="bibr" rid="pone.0082146-ThomasHofmann1">[32]</xref>. Although the training error of the nonlinear model is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e263" xlink:type="simple"/></inline-formula> smaller than that of the linear low-rank model, the testing performance of the two models is similar. The optimal rank returned by the linear model is 78 and the one returned by the nonlinear model is 88, suggesting the existence of approximately 78–88 regulatory modules that are active in this dataset.</p>
<fig id="pone-0082146-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0082146.g001</object-id><label>Figure 1</label><caption>
<title>Comparison of the testing performance of the linear low-rank regularization model vs. Lasso on the yeast gene expression dataset.</title>
<p>Each * indicates one target gene. X-axis represents the test RMSE of the Lasso model, whereas Y-axis represents the test RMSE of the linear low-rank model. The figure shows that the low-rank model yields lower testing error than Lasso for most target genes.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0082146.g001" position="float" xlink:type="simple"/></fig><table-wrap id="pone-0082146-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0082146.t001</object-id><label>Table 1</label><caption>
<title>Root-mean-squared error (RMSE) comparison among different models on the yeast gene expression data.</title>
</caption><alternatives><graphic id="pone-0082146-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0082146.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Model</td>
<td align="left" rowspan="1" colspan="1">Training error</td>
<td align="left" rowspan="1" colspan="1">Testing error</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">SiMoNe</td>
<td align="left" rowspan="1" colspan="1">—</td>
<td align="left" rowspan="1" colspan="1">1.0074±0.0650</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Lasso</td>
<td align="left" rowspan="1" colspan="1">0.3897±0.0045</td>
<td align="left" rowspan="1" colspan="1">0.6124±0.0583</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Linear low-rank</td>
<td align="left" rowspan="1" colspan="1">0.3488±0.0014</td>
<td align="left" rowspan="1" colspan="1">0.5750±0.0053</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Nonlinear low-rank</td>
<td align="left" rowspan="1" colspan="1">0.3249±0.0063</td>
<td align="left" rowspan="1" colspan="1">0.5752±0.0054</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt101"><p>“Lasso” represents model (2), “Linear low-rank” represents model (3), and “Nonlinear low-rank” represents model (6) with ANOVA kernel. SiMoNe is the model described by Chiquet et al. <xref ref-type="bibr" rid="pone.0082146-Chiquet1">[16]</xref>. Both training and testing errors are measured in terms of RMSE averaged over all target genes. Shown here are mean ± standard deviation values of RMSEs in ten different runs.<sup/></p></fn></table-wrap-foot></table-wrap></sec><sec id="s3b">
<title>Human hematopoietic gene expression data</title>
<p>We also tested our models on a human hematopoietic gene expression dataset <xref ref-type="bibr" rid="pone.0082146-Novershtern1">[33]</xref>, which measures mRNA expression values of human genes during hematopoietic differentiation. The dataset contains expression profiles of 8,968 genes in 38 hematopoietic states with a total of 211 experiment conditions (will also be referred to as samples). We used a list of 523 candidate regulators manually complied in <xref ref-type="bibr" rid="pone.0082146-Novershtern1">[33]</xref> based on biological annotations (including important transcriptional regulators or signalling molecules previously implicated in hematopoietic differentiation) as our regulator genes. Among the remaining non-regulator genes, we removed genes with low variance across the samples, and kept only the top 1000 genes with highest variance. These 1000 genes will be called target genes, and our goal is to learn the regulatory relationship between the 523 regulators and the 1000 target genes. We rescaled the expression of each gene (both regulators and targets) to be mean 0 and variance 1 across the samples. Similar to the yeast dataset, we benchmarked the performance of our and control models on this dataset using ten-fold cross-validation, and RMSE was used to measure both training and testing errors.</p>
<p>The training and test performance of four models - SiMoNe <xref ref-type="bibr" rid="pone.0082146-Chiquet1">[16]</xref>, linear low-rank (3), nonlinear low-rank (10), and Lasso (2), on the human hematopoietic gene expression dataset is summarized in <xref ref-type="table" rid="pone-0082146-t002">Table 2</xref>. The linear low-rank model (3) reduces training error by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e264" xlink:type="simple"/></inline-formula> and testing error by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e265" xlink:type="simple"/></inline-formula> when compared to the Lasso model. Similar to the yeast dataset, the regression-based models outperform SiMoNe by a large margin. If we look specifically at the prediction accuracy of each target gene, we find that 70% of the targets are predicted more accurately by the linear low-rank model than by Lasso (<xref ref-type="fig" rid="pone-0082146-g002">Figure 2</xref>). Similar to the yeast dataset, we used an ANOVA kernel to train the nonlinear low-rank model. The training error of the nonlinear model is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e266" xlink:type="simple"/></inline-formula> smaller than that of the linear low-rank model, but their testing performance is similar. The optimal rank returned by the linear model is 112 and the one returned by the nonlinear model is 109, suggesting that approximately 109–112 regulatory modules are active in this hematopoietic gene expression dataset.</p>
<fig id="pone-0082146-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0082146.g002</object-id><label>Figure 2</label><caption>
<title>Comparison of the testing performance of the linear low-rank regularization model vs. Lasso on the human hematopoietic gene expression dataset.</title>
<p>Each * indicates one target gene. X-axis represents the test RMSE of the Lasso model, whereas Y-axis represents the test RMSE of the linear low-rank model. The figure shows that the low-rank model yields lower testing error than Lasso for most target genes.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0082146.g002" position="float" xlink:type="simple"/></fig><table-wrap id="pone-0082146-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0082146.t002</object-id><label>Table 2</label><caption>
<title>Root-mean-squared error (RMSE) comparison among different models on the human hematopoietic gene expression data.</title>
</caption><alternatives><graphic id="pone-0082146-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0082146.t002" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Model</td>
<td align="left" rowspan="1" colspan="1">Training error</td>
<td align="left" rowspan="1" colspan="1">Testing error</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">SiMoNe</td>
<td align="left" rowspan="1" colspan="1">—</td>
<td align="left" rowspan="1" colspan="1">0.9987±0.0400</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Lasso</td>
<td align="left" rowspan="1" colspan="1">0.2345±0.0024</td>
<td align="left" rowspan="1" colspan="1">0.3881±0.0265</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Linear low-rank</td>
<td align="left" rowspan="1" colspan="1">0.1877±0.0030</td>
<td align="left" rowspan="1" colspan="1">0.3758±0.0265</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Nonlinear low-rank</td>
<td align="left" rowspan="1" colspan="1">0.1783±0.0005</td>
<td align="left" rowspan="1" colspan="1">0.3767±0.0265</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt102"><label/><p>“Lasso” represents model (2), “Linear low-rank” represents model (3), and “Nonlinear low-rank” represents model (6) with ANOVA kernel. SiMoNe is the model described by Chiquet et al. <xref ref-type="bibr" rid="pone.0082146-Chiquet1">[16]</xref>. Both training and testing errors are measured in terms of RMSE averaged over all target genes. Shown here are mean ± standard deviation values of RMSEs in ten different runs.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s3c">
<title>Connectivity map data</title>
<p>The third dataset we have experimented with is the connectivity map data provided by Lamb et al. <xref ref-type="bibr" rid="pone.0082146-Lamb1">[2]</xref>, which contains gene expression measurements of human cells responding to diverse treatments with chemical compounds and genetic reagents. The connectivity map data contains microarray measurements of human genes in thousands of profiles (will be referred to as samples). For regulator genes, we used 978 “landmark” genes determined by the connectivity map project as the set of genes that are most predictive of the expression of the other genes (Aravind Subramanian, personal communication). Among the remaining non-regulator genes, we used the top 10,000 genes with highest variances across samples as our target genes. We randomly selected 1000 samples from this dataset to benchmark the performance of our and other models (we were unable to use all samples due to computational constraints.) Our goal is to learn the regulatory relationship between the 10,000 target genes and the 978 landmark genes based on these 1000 samples.</p>
<p>We benchmarked the performance of our and control models using ten-fold cross-validation. More specifically, we randomly partitioned the samples into ten nonoverlapping subsets. Each model was trained using nine of the ten subsets, and the test performance of the learned model was then measured using the remaining subset. We used root-mean-square error (RMSE) to measure the differences between values predicted by each model and the the values actually observed. The average RMSE over all target genes measured on the training and test data will be called training and testing error, respectively.</p>
<p>We were unable to obtain SiMoNe results after hours of running the program, which might be due to the large number of genes contained in this dataset, and the fact that the inverse covariance matrix between these genes is too large to be handled by SiMoNe. So next we will focus on comparing the performance of our models to Lasso. The performance of the linear low-rank (3), nonlinear low-rank (10), and Lasso (2), on the connectivity map data is summarized in <xref ref-type="table" rid="pone-0082146-t003">Table 3</xref>. The nonlinear low-rank model achieves the lowest testing error in this dataset. The testing performances of linear low-rank model and Lasso are similar, with the testing errors of both models <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e267" xlink:type="simple"/></inline-formula> higher than the nonlinear low-rank model. If we compare the prediction performance of each target gene, 88% of the target genes are predicted more accurately by the nonlinear low-rank model than by Lasso.</p>
<table-wrap id="pone-0082146-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0082146.t003</object-id><label>Table 3</label><caption>
<title>Root-mean-squared error (RMSE) comparison among different models on the connectivity map gene expression data.</title>
</caption><alternatives><graphic id="pone-0082146-t003-3" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0082146.t003" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Model</td>
<td align="left" rowspan="1" colspan="1">Training error</td>
<td align="left" rowspan="1" colspan="1">Testing error</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Lasso</td>
<td align="left" rowspan="1" colspan="1">0.4943±0.0008</td>
<td align="left" rowspan="1" colspan="1">0.7077±0.0134</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Linear low-rank</td>
<td align="left" rowspan="1" colspan="1">0.5157±0.0004</td>
<td align="left" rowspan="1" colspan="1">0.7000±0.0123</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Nonlinear low-rank</td>
<td align="left" rowspan="1" colspan="1">0.4025±0.0005</td>
<td align="left" rowspan="1" colspan="1">0.6772±0.0125</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt103"><label/><p>“Lasso” represents model (2), “Linear low-rank” represents model (3), and “Nonlinear low-rank” represents model (6) with ANOVA kernel. Both training and testing errors are measured in terms of RMSE averaged over all target genes. Shown here are mean ± standard deviation values of RMSEs in ten different runs.</p></fn></table-wrap-foot></table-wrap>
<p>Our algorithms were implemented in Matlab and run on the platform of Intel Xeon E5-4617 - 2.9 GHz 1-Core CPU with 128 GB memory. The CPU times of running our algorithms on the three datasets are shown in <xref ref-type="table" rid="pone-0082146-t004">Table 4</xref>. The time complexity of Algorithm 1 and 2 is mainly determined by the singular value decomposition step. Exact singular value decomposition of a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e268" xlink:type="simple"/></inline-formula> matrix has the time complexity of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e269" xlink:type="simple"/></inline-formula>. In our algorithms, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e270" xlink:type="simple"/></inline-formula> corresponds to the number of targets. However, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e271" xlink:type="simple"/></inline-formula> corresponds to the number of regulators in the linear model, and the number of samples in the nonlinear model. So when the number of samples is smaller than the number of regulators, the nonlinear model actually runs fasters than the linear model (See <xref ref-type="table" rid="pone-0082146-t004">Table 4</xref>).</p>
<table-wrap id="pone-0082146-t004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0082146.t004</object-id><label>Table 4</label><caption>
<title>CPU time of running the linear and nonlinear low-rank models.</title>
</caption><alternatives><graphic id="pone-0082146-t004-4" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0082146.t004" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Dataset</td>
<td align="left" rowspan="1" colspan="1">Linear low-rank (min)</td>
<td align="left" rowspan="1" colspan="1">Nonlinear low-rank (min)</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Yeast gene expression</td>
<td align="left" rowspan="1" colspan="1">1.6</td>
<td align="left" rowspan="1" colspan="1">0.8</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Human hematopoietic gene expression</td>
<td align="left" rowspan="1" colspan="1">1.0</td>
<td align="left" rowspan="1" colspan="1">0.4</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Connectivity map gene expression</td>
<td align="left" rowspan="1" colspan="1">1067</td>
<td align="left" rowspan="1" colspan="1">869</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap></sec></sec><sec id="s4">
<title>Discussion</title>
<p>Gene expression program learning is an important problem in both basic research as well as practical and applied settings of biotechnology development. In this paper, we formulate the gene expression program learning as a multi-target (more specifically <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e272" xlink:type="simple"/></inline-formula>-target) regression problem and use Tikhonov regularization to constrain the space of the L-target mapping. We propose a new form of regularization that constrains the number of independent connectivity patterns between regulator genes and target genes. We use the rank of the connectivity matrix from regulators to targets to represent the number of independent connectivity patterns, and approximate the rank of the matrix using its nuclear norm. The resulting low-rank regularization problem is convex, and we provide an efficient algorithm to find its globally optimal solution.</p>
<p>Previously, in gene expression program learning each target gene is usually treated separately. Because the expression of many genes are highly correlated, it would be beneficial to learn their expression regulation jointly instead of separately. However, it was unclear before on how to model the regulatory relationship from regulators to target genes jointly such that the resulting model is both computationally efficient and able to take the constraints between targets into account. The low-rank regularization provides an effective and yet computationally efficient framework for considering all target genes simultaneously. Experiments on two real gene expression datasets demonstrate that the low-rank model outperforms the Lasso model, one of the most widely used regularization method in gene expression program learning, in terms of prediction accuracy in both datasets.</p>
<p>We showed that the low-rank model can also be generalized to nonlinear settings, where we constrain the dimension of the hypothesis space of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0082146.e273" xlink:type="simple"/></inline-formula>-target regression function. We proved that the resulting problem is still convex and derived an efficient algorithm to find its globally optimal solution. We tested the nonlinear low-rank model on the gene expression datasets. The nonlinear low-rank model produces better testing performance than the linear low-rank model in some datasets, but is comparable to the linear model in other datasets. The lack of improvement comparing to the linear one in some datasets might be due to a) the fact that the number of samples used in these two datasets might be too small to fit a more complex model, and b) the kernel we have tried (ANOVA, Gaussian, and polynomial) might not be a good fit for the gene expression program learning. We expect that the nonlinear model will improve when a larger number of samples become available. So finding or designing the right kernel specifically for gene expression program learning will be the key to improving the nonlinear model.</p>
<p>The two forms of regularization, low-rank and sparsity, described in this paper are complementary to each other, considering two different aspects of gene regulation. A future direction is to combine these two regularizations into a single framework to constrain the connectivity matrix to be simultaneously sparse and low rank.</p>
</sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pone.0082146.s001" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pone.0082146.s001" position="float" xlink:type="simple"><label>Material S1</label><caption>
<p>For proving Theorems 1 and 2.</p>
<p>(PDF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We thank A Subramanian for providing us the connectivity map data and for helpful discussions.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0082146-Bansal1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bansal</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Belcastro</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Ambesi-Impiombato</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Di Bernardo</surname><given-names>D</given-names></name> (<year>2007</year>) <article-title>How to infer gene networks from expression profiles</article-title>. <source>Molecular systems biology</source> <volume>3</volume>.</mixed-citation>
</ref>
<ref id="pone.0082146-Lamb1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lamb</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Crawford</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Peck</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Modell</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Blat</surname><given-names>I</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>The connectivity map: using gene-expression signatures to connect small molecules, genes, and disease</article-title>. <source>Science</source> <volume>313</volume>: <fpage>1929</fpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Friedman1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friedman</surname><given-names>N</given-names></name> (<year>2004</year>) <article-title>Inferring cellular networks using probabilistic graphical models</article-title>. <source>Science</source> <volume>303</volume>: <fpage>799</fpage>–<lpage>805</lpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Friedman2"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friedman</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Hastie</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Tibshirani</surname><given-names>R</given-names></name> (<year>2008</year>) <article-title>Sparse inverse covariance estimation with the graphical lasso</article-title>. <source>Biostatistics</source> <volume>9</volume>: <fpage>432</fpage>–<lpage>441</lpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Grzegorczyk1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grzegorczyk</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Husmeier</surname><given-names>D</given-names></name> (<year>2011</year>) <article-title>Improvements in the reconstruction of time-varying gene regulatory networks: dynamic programming and regularization by information sharing among genes</article-title>. <source>Bioinformatics</source> <volume>27</volume>: <fpage>693</fpage>–<lpage>699</lpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Faith1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Faith</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Hayete</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Thaden</surname><given-names>JT</given-names></name>, <name name-style="western"><surname>Mogno</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Wierzbowski</surname><given-names>J</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>Large-scale mapping and validation of escherichia coli transcriptional regulation from a compendium of expression profiles</article-title>. <source>PLoS biology</source> <volume>5</volume>: <fpage>e8</fpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Margolin1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Margolin</surname><given-names>AA</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Lim</surname><given-names>WK</given-names></name>, <name name-style="western"><surname>Kustagi</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Nemenman</surname><given-names>I</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>Reverse engineering cellular networks</article-title>. <source>Nature Protocols</source> <volume>1</volume>: <fpage>662</fpage>–<lpage>671</lpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Gustafsson1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gustafsson</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Hornquist</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Lombardi</surname><given-names>A</given-names></name> (<year>2005</year>) <article-title>Constructing and analyzing a large-scale geneto-gene regulatory network lasso-constrained inference and biological validation</article-title>. <source>Computational Biology and Bioinformatics, IEEE/ACM Transactions on</source> <volume>2</volume>: <fpage>254</fpage>–<lpage>261</lpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Gardner1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gardner</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Di Bernardo</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Lorenz</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Collins</surname><given-names>J</given-names></name> (<year>2003</year>) <article-title>Inferring genetic networks and identifying compound mode of action via expression profiling</article-title>. <source>Science</source> <volume>301</volume>: <fpage>102</fpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-DeJong1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>De Jong</surname><given-names>H</given-names></name> (<year>2002</year>) <article-title>Modeling and simulation of genetic regulatory systems: a literature review</article-title>. <source>Journal of computational biology</source> <volume>9</volume>: <fpage>67</fpage>–<lpage>103</lpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Christley1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Christley</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Nie</surname><given-names>Q</given-names></name>, <name name-style="western"><surname>Xie</surname><given-names>X</given-names></name> (<year>2009</year>) <article-title>Incorporating existing network information into gene network inference</article-title>. <source>PloS one</source> <volume>4</volume>: <fpage>e6799</fpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Lee1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Dudley</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Drubin</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Silver</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Krogan</surname><given-names>N</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Learning a prior on regulatory potential from eqtl data</article-title>. <source>PLoS genetics</source> <volume>5</volume>: <fpage>e1000358</fpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Ye1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ye</surname><given-names>GB</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Xie</surname><given-names>X</given-names></name> (<year>2011</year>) <article-title>Efficient latent variable graphical model selection via split bregman method</article-title>. <source>arXiv preprint arXiv</source> <fpage>11103076</fpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Tibshirani1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tibshirani</surname><given-names>R</given-names></name> (<year>1996</year>) <article-title>Regression shrinkage and selection via the lasso</article-title>. <source>Journal of the Royal Statistical Society Series B (Methodological)</source> <fpage>267</fpage>–<lpage>288</lpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Segal1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Segal</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Shapira</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Regev</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Pe'er</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Botstein</surname><given-names>D</given-names></name>, <etal>et al</etal>. (<year>2003</year>) <article-title>Module networks: identifying regulatory modules and their condition-specific regulators from gene expression data</article-title>. <source>Nature genetics</source> <volume>34</volume>: <fpage>166</fpage>–<lpage>176</lpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Chiquet1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chiquet</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Smith</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Grasseau</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Matias</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Ambroise</surname><given-names>C</given-names></name> (<year>2009</year>) <article-title>Simone: Statistical inference for modular networks</article-title>. <source>Bioinformatics</source> <volume>25</volume>: <fpage>417</fpage>–<lpage>418</lpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Cai1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cai</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Candès</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Shen</surname><given-names>Z</given-names></name> (<year>2010</year>) <article-title>A singular value thresholding algorithm for matrix completion</article-title>. <source>SIAM Journal on Optimization</source> <volume>20</volume>: <fpage>1956</fpage>–<lpage>1982</lpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Chen1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chen</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Ye</surname><given-names>J</given-names></name> (<year>2012</year>) <article-title>Learning incoherent sparse and low-rank patterns from multiple tasks</article-title>. <source>ACM Transactions on Knowledge Discovery from Data (TKDD)</source> <volume>5</volume>: <fpage>22</fpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Cands1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Candès</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Ma</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Wright</surname><given-names>J</given-names></name> (<year>2011</year>) <article-title>Robust principal component analysis?</article-title> <source>Journal of the Association for Computing Machinery</source> <volume>58</volume>: <fpage>1</fpage>–<lpage>37</lpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Ying1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ying</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Huang</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Campbell</surname><given-names>C</given-names></name> (<year>2009</year>) <article-title>Sparse metric learning via smooth optimization</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>22</volume>.</mixed-citation>
</ref>
<ref id="pone.0082146-Hoerl1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hoerl</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Kennard</surname><given-names>R</given-names></name> (<year>1970</year>) <article-title>Ridge regression: Biased estimation for nonorthogonal problems</article-title>. <source>Technometrics</source> <fpage>55</fpage>–<lpage>67</lpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Cands2"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Candès</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Recht</surname><given-names>B</given-names></name> (<year>2009</year>) <article-title>Exact matrix completion via convex optimization</article-title>. <source>Foundations of Computational Mathematics</source> <volume>9</volume>: <fpage>717</fpage>–<lpage>772</lpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Donoho1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Donoho</surname><given-names>D</given-names></name> (<year>2006</year>) <article-title>Compressed sensing</article-title>. <source>IEEE Transactions on Information Theory</source> <volume>52</volume>: <fpage>1289</fpage>–<lpage>1306</lpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Cristianini1"><label>24</label>
<mixed-citation publication-type="book" xlink:type="simple">Cristianini N, Shawe-Taylor J (2004) An introduction to support Vector Machines: and other kernel-based learning methods. Cambridge university press.</mixed-citation>
</ref>
<ref id="pone.0082146-Schlkopf1"><label>25</label>
<mixed-citation publication-type="book" xlink:type="simple">Schölkopf B, Smola A (2002) Learning with kernels: Support vector machines, regularization, optimization, and beyond. MIT press.</mixed-citation>
</ref>
<ref id="pone.0082146-Vapnik1"><label>26</label>
<mixed-citation publication-type="book" xlink:type="simple">Vapnik VN (1998) Statistical learning theory. Adaptive and Learning Systems for Signal Processing, Communications, and Control. New York: John Wiley &amp; Sons Inc., xxvi+736 pp. A Wiley-Interscience Publication.</mixed-citation>
</ref>
<ref id="pone.0082146-Aronszajn1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Aronszajn</surname><given-names>N</given-names></name> (<year>1950</year>) <article-title>Theory of reproducing kernels</article-title>. <source>Trans Amer Math Soc</source> <volume>68</volume>: <fpage>337</fpage>–<lpage>404</lpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Nesterov1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nesterov</surname><given-names>Y</given-names></name> (<year>2005</year>) <article-title>Smooth minimization of non-smooth functions</article-title>. <source>Mathematical Programming</source> <volume>103</volume>: <fpage>127</fpage>–<lpage>152</lpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Beck1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beck</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Teboulle</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>A fast iterative shrinkage-thresholding algorithm for linear inverse problems</article-title>. <source>SIAM J Imaging Sci</source> <volume>2</volume>: <fpage>183</fpage>–<lpage>202</lpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Nesterov2"><label>30</label>
<mixed-citation publication-type="other" xlink:type="simple">Nesterov Y (2007) Gradient methods for minimizing composite objective function. Center for Operations Research and Econometrics (CORE), Catholic University of Louvain, Tech Rep 76.</mixed-citation>
</ref>
<ref id="pone.0082146-Gasch1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gasch</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Spellman</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Kao</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Carmel-Harel</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Eisen</surname><given-names>M</given-names></name>, <etal>et al</etal>. (<year>2000</year>) <article-title>Genomic expression programs in the response of yeast cells to environmental changes</article-title>. <source>Science</source> <volume>11</volume>: <fpage>4241</fpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-ThomasHofmann1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thomas Hofmann</surname><given-names>BS</given-names></name>, <name name-style="western"><surname>Smola</surname><given-names>AJ</given-names></name> (<year>2008</year>) <article-title>Kernel method in machine learning</article-title>. <source>The Annals of Statistics</source> <volume>36</volume>: <fpage>1171</fpage>.</mixed-citation>
</ref>
<ref id="pone.0082146-Novershtern1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Novershtern</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Subramanian</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Lawton</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Mak</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Haining</surname><given-names>W</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Densely interconnected transcriptional circuits control cell states in human hematopoiesis</article-title>. <source>Cell</source> <volume>144</volume>: <fpage>296</fpage>–<lpage>309</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>