<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group>
<journal-title>PLoS ONE</journal-title></journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-13-39122</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0097446</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Molecular cell biology</subject></subj-group></subj-group><subj-group><subject>Computational biology</subject></subj-group><subj-group><subject>Genetics</subject><subj-group><subject>Epigenetics</subject><subj-group><subject>RNA interference</subject></subj-group></subj-group><subj-group><subject>Gene expression</subject></subj-group></subj-group><subj-group><subject>Microbiology</subject><subj-group><subject>Plant microbiology</subject></subj-group></subj-group><subj-group><subject>Molecular biology</subject><subj-group><subject>Molecular biology techniques</subject><subj-group><subject>Sequencing techniques</subject><subj-group><subject>Sequence analysis</subject></subj-group></subj-group></subj-group></subj-group><subj-group><subject>Plant science</subject><subj-group><subject>Plant pathology</subject><subj-group><subject>Plant pathogens</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Supervised Learning Classification Models for Prediction of Plant Virus Encoded RNA Silencing Suppressors</article-title>
<alt-title alt-title-type="running-head">Prediction of Plant Virus Encoded RNA Silencing Suppressors</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Jagga</surname><given-names>Zeenia</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Gupta</surname><given-names>Dinesh</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><addr-line>Bioinformatics Facility, Structural and Computational Biology Group, International Centre for Genetic Engineering and Biotechnology, Aruna Asaf Ali Marg, New Delhi, India</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Raghava</surname><given-names>Gajendra P. S.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>CSIR-Institute of Microbial Technology, India</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">dinesh@icgeb.res.in</email></corresp>
<fn fn-type="conflict"><p>Dinesh Gupta is a PLOS ONE Editorial Board member. This does not alter the authors' adherence to PLOS ONE Editorial policies and criteria.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: ZJ DG. Performed the experiments: ZJ. Analyzed the data: ZJ DG. Contributed reagents/materials/analysis tools: ZJ DG. Wrote the paper: ZJ DG.</p></fn>
</author-notes>
<pub-date pub-type="collection"><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>14</day><month>5</month><year>2014</year></pub-date>
<volume>9</volume>
<issue>5</issue>
<elocation-id>e97446</elocation-id>
<history>
<date date-type="received"><day>23</day><month>9</month><year>2013</year></date>
<date date-type="accepted"><day>21</day><month>4</month><year>2014</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Jagga, Gupta</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Viral encoded RNA silencing suppressor proteins interfere with the host RNA silencing machinery, facilitating viral infection by evading host immunity. In plant hosts, the viral proteins have several basic science implications and biotechnology applications. However <italic>in silico</italic> identification of these proteins is limited by their high sequence diversity. In this study we developed supervised learning based classification models for plant viral RNA silencing suppressor proteins in plant viruses. We developed four classifiers based on supervised learning algorithms: J48, Random Forest, LibSVM and Na√Øve Bayes algorithms, with enriched model learning by correlation based feature selection. Structural and physicochemical features calculated for experimentally verified primary protein sequences were used to train the classifiers. The training features include amino acid composition; auto correlation coefficients; composition, transition, and distribution of various physicochemical properties; and pseudo amino acid composition. Performance analysis of predictive models based on 10 fold cross-validation and independent data testing revealed that the Random Forest based model was the best and achieved 86.11% overall accuracy and 86.22% balanced accuracy with a remarkably high area under the Receivers Operating Characteristic curve of 0.95 to predict viral RNA silencing suppressor proteins. The prediction models for plant viral RNA silencing suppressors can potentially aid identification of novel viral RNA silencing suppressors, which will provide valuable insights into the mechanism of RNA silencing and could be further explored as potential targets for designing novel antiviral therapeutics. Also, the key subset of identified optimal features may help in determining compositional patterns in the viral proteins which are important determinants for RNA silencing suppressor activities. The best prediction model developed in the study is available as a freely accessible web server pVsupPred at <ext-link ext-link-type="uri" xlink:href="http://bioinfo.icgeb.res.in/pvsup/" xlink:type="simple">http://bioinfo.icgeb.res.in/pvsup/</ext-link>.</p>
</abstract>
<funding-group><funding-statement>The authors thank the Department of Biotechnology (DBT, India) for the grant for "Bioinformatics Infrastructure Facility" at ICGEB, New Delhi. ZJ acknowledges the University Grants Commission (UGC, India) for the Senior Research Fellowship. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="9"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>RNA silencing is an evolutionary conserved sequence specific mechanism of post transcriptional gene silencing in eukaryotes which confers innate immunity against viruses <xref ref-type="bibr" rid="pone.0097446-Sioud1">[1]</xref>. In the evolutionary arms race, viruses have adopted various strategies to escape host immune systems. RNA silencing suppressors are the viral encoded proteins evolved with the capability to block host RNA silencing response <xref ref-type="bibr" rid="pone.0097446-Anandalakshmi1">[2]</xref>, <xref ref-type="bibr" rid="pone.0097446-Voinnet1">[3]</xref>. To disrupt RNA silencing machinery, virus encoded RNA silencing suppressors target several effectors of RNAi silencing pathway, such as viral RNA recognition, dicing, RISC assembly, RNA targeting and amplification <xref ref-type="bibr" rid="pone.0097446-Burgyan1">[4]</xref>, <xref ref-type="bibr" rid="pone.0097446-Jiang1">[5]</xref> e.g. P14 of <italic>Pothos latent aureusvirus</italic> and P38 of <italic>Turnip crinkle virus</italic> have been shown to inhibit the processing of dsRNA to siRNA <xref ref-type="bibr" rid="pone.0097446-Mrai1">[6]</xref>, <xref ref-type="bibr" rid="pone.0097446-Mrai2">[7]</xref>. In another case, the P19 protein of <italic>tombusviruses</italic> prevents RNA silencing by siRNA sequestration through binding ds siRNA with a high affinity <xref ref-type="bibr" rid="pone.0097446-Silhavy1">[8]</xref>. Other RNA silencing suppressors, such as the <italic>Tomato aspermy cucumovirus</italic> 2b protein or B2 of the insect-infecting <italic>Flock House virus</italic>, also bind ds siRNA in a size-specific manner <xref ref-type="bibr" rid="pone.0097446-Chen1">[9]</xref>, <xref ref-type="bibr" rid="pone.0097446-Chao1">[10]</xref>. P0 of <italic>Beet Western Yellow virus</italic> directly interacts with AGO1 promoting its degradation <xref ref-type="bibr" rid="pone.0097446-Bortolamiol1">[11]</xref>. P6 protein of <italic>Cauliflower Mosaic virus</italic> directly interacting with dsRNA binding protein 4 required for DCL4 functioning <xref ref-type="bibr" rid="pone.0097446-Haas1">[12]</xref>. The viral proteins identified mostly in plants and some insects or mammalian viruses, show diversity within and across the taxonomic kingdoms <xref ref-type="bibr" rid="pone.0097446-Song1">[13]</xref>.</p>
<p>Along with the suppression activity, these viral proteins have multifunctional and indispensable role in viruses as coat proteins, movement proteins, replicases, proteases, helper components for viral transmission and transcription regulation <xref ref-type="bibr" rid="pone.0097446-Csorba1">[14]</xref>. This makes exploration of these proteins difficult, as inactivation of these proteins risks viability of a given virus in virus-targeted RNA silencing mediated gene knock down experiments. Experimental screening is performed by agrobacterium mediated <italic>in planta</italic> assay systems based on reversal of silencing and enhancement of rolling circle replication of geminivirus replicon <xref ref-type="bibr" rid="pone.0097446-Karjee1">[15]</xref>. Non availability of quick screening method is a major limitation for identification of viral suppressors of RNA silencing. Hence, not many such proteins have been reported till date. Thus, apriori knowledge about endowment of RNA silencing suppression capability of a new pathogenic viral protein would guide the future drug/vaccine strategies against the new viruses <xref ref-type="bibr" rid="pone.0097446-BivalkarMehla1">[16]</xref>, <xref ref-type="bibr" rid="pone.0097446-deVries1">[17]</xref>, other applications like molecular biofarming <xref ref-type="bibr" rid="pone.0097446-Rahman1">[18]</xref>, si/miRNA based antiviral gene therapy <xref ref-type="bibr" rid="pone.0097446-Rauschhuber1">[19]</xref> and as biosensors for miRNA <xref ref-type="bibr" rid="pone.0097446-Ramnani1">[20]</xref>.</p>
<p>Identification of RNA silencing suppressor activity in viral proteins has important implications in studying various facets of viral infection and pathogenesis and understanding the mechanism and function of RNA silencing machinery <xref ref-type="bibr" rid="pone.0097446-Wu1">[21]</xref>. Since, the experimentally validated viral suppressors exhibit considerable diversity in its primary sequences, structures, origins, mode of suppressor action and evolution <xref ref-type="bibr" rid="pone.0097446-Burgyan1">[4]</xref>, <xref ref-type="bibr" rid="pone.0097446-Jiang1">[5]</xref>. Owing to its inherent diversity and complexity, identification of novel RNA silencing suppressor sequences in virus proteomes using conventional similarity based bioinformatics approaches is difficult. Existing approaches to identify RNA silencing suppressor activities investigate the presence of RNA binding motif and/or motifs responsible for binding to the components of silencing machinery ‚Äì for example GW/WG motif for Argonaute binding <xref ref-type="bibr" rid="pone.0097446-BivalkarMehla1">[16]</xref>. However, the presence of these motifs <italic>per se</italic> is not a confirmatory evidence to annotate a viral protein as viral RNA silencing suppressor.</p>
<p>Machine learning algorithms have been extensively implemented to detect cryptic patterns in disparate biological domains. In particular, supervised machine learning algorithms, have been very effective to investigate biological classification problems <xref ref-type="bibr" rid="pone.0097446-Larraaga1">[22]</xref>, <xref ref-type="bibr" rid="pone.0097446-Tarca1">[23]</xref>. The generated classification models have served as valuable tools to further predict the new cases of the same class. Absence of any computational algorithm to identify viral RNA silencing suppressor sequences motivated us to undertake this study. We have developed prediction models for viral RNA silencing suppressors protein sequences and evaluated these by implementing four supervised machine-learning algorithms, namely- Na√Øve Bayes, J48, Random Forest and LibSVM. Feature vector profiles based on various structural and physicochemical features of experimentally verified viral suppressor proteins of RNA silencing were used for algorithm trainings. Our present study marks an effort, to predict unexplored viral RNA silencing suppressors, which could be potential targets for designing novel antiviral therapeutics, and enhance our understanding of RNA silencing. In this direction, we found that among the four machine learning techniques implemented by us, the classifier model based on Random Forest algorithm was the best prediction classifier.</p>
</sec><sec id="s2" sec-type="materials|methods">
<title>Materials and Methods</title>
<p>The general flow of methodology adopted in this study for data mining, feature calculation, feature selection, analysis, model building and validation has been schematically represented in <xref ref-type="fig" rid="pone-0097446-g001">Figure 1</xref>.</p>
<fig id="pone-0097446-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0097446.g001</object-id><label>Figure 1</label><caption>
<title>Methodology flow.</title>
<p>Framework of the computational method used for development of predictions models for plant virus encoded RNA silencing suppressors.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0097446.g001" position="float" xlink:type="simple"/></fig><sec id="s2a">
<title>Data Mining</title>
<p>We generated the training dataset by obtaining protein sequences by searching UniProtKB database release 2013_06 <xref ref-type="bibr" rid="pone.0097446-1">[24]</xref> using combination of different query terms and logical variables. For example, we obtained positive dataset (X) of ‚Äòviral protein sequences which have Kingdom Plantae as hosts and are viral suppressor for RNA silencing‚Äô using the query: ‚Äòtaxonomy:Viruses reviewed:yes fragment:no host:33090 keyword:KW-0941‚Äô. Similarly, we obtained the negative dataset (X‚Ä≤) as ‚Äòviral protein sequences which have natural host plant kingdom hosts but are not viral suppressor for RNA silencing‚Äô by using the query ‚Äòtaxonomy:Viruses reviewed:yes fragment:no host:33090 NOT keyword:KW-0941‚Äô. We limited our selection criterion to include only experimentally verified and complete sequences. We removed the sequences annotated as ‚Äòpolyprotein‚Äô in sequence headers information of fasta formatted positive and negative datasets sequences downloaded from UniProtKB. Further, we manually analyzed the positive dataset polyprotein sequences to process fragments into positive (A) and negative (B) polyprotein subsets. We discarded the polyprotein sequences from the negative dataset (Y‚Ä≤). Hence, we obtained the final positive dataset Z, as X‚àíY+A, and similarly the final negative dataset ‚ÄòZ‚Äô as X‚Ä≤‚àíY‚Ä≤+B. We retrieved the final positive dataset (<xref ref-type="supplementary-material" rid="pone.0097446.s001">Dataset S1</xref>) and final negative dataset (<xref ref-type="supplementary-material" rid="pone.0097446.s002">Dataset S2</xref>) in fasta format by batch retrieval system of the UniProt database.</p>
</sec><sec id="s2b">
<title>PSI-BLAST similarity based Search</title>
<p>We used standalone PSI-BLAST version 2.2.27+ with a threshold E-value 0.001, number of iterations three over final positive dataset i.e. 208 viral RNA suppressor sequences by Leave One Out Cross-Validation (LOO-CV). Each sequence of the dataset was iterated as the query sequence once against rest sequences as reference database.</p>
</sec><sec id="s2c">
<title>Preparing Non-Redundant Dataset</title>
<p>To prepare a non redundant dataset to avoid over-fitting problem, we used CD-HIT (Cluster Database at High Identity with Tolerance) version 4.5.7 <xref ref-type="bibr" rid="pone.0097446-Li1">[25]</xref>, <xref ref-type="bibr" rid="pone.0097446-Li2">[26]</xref>. CD-HIT works on 'the longest sequence first' list removal algorithm for removing redundant sequences from the dataset. We utilized redundancy threshold parameter to generate datasets of redundancy 90%, 70% and 40%. As dataset removing redundancy to 40% reduced dataset size to less than half (<xref ref-type="table" rid="pone-0097446-t001">Table 1</xref>). Thereby we proceeded to calculate feature vectors for two redundancy levels ‚àí90% and 70% datasets. <xref ref-type="fig" rid="pone-0097446-g002">Figure 2</xref> summarizes the dataset generation and filtering methodology followed in the study.</p>
<fig id="pone-0097446-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0097446.g002</object-id><label>Figure 2</label><caption>
<title>Schematic representation of the dataset generation and filtration steps.</title>
<p>The figure shows flowchart for the steps followed for sequence collection, filtering and redundancy removal for training dataset generation.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0097446.g002" position="float" xlink:type="simple"/></fig><table-wrap id="pone-0097446-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0097446.t001</object-id><label>Table 1</label><caption>
<title>Number of protein sequences after removing redundant proteins at thresholds of 90%, 70% and 40% using CD-HIT.</title>
</caption><alternatives><graphic id="pone-0097446-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0097446.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Redundancy cut off</td>
<td align="left" rowspan="1" colspan="1">Positive Dataset (208)</td>
<td align="left" rowspan="1" colspan="1">Negative Dataset (1321)</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>90%</bold></td>
<td align="left" rowspan="1" colspan="1">142</td>
<td align="left" rowspan="1" colspan="1">949</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>70%</bold></td>
<td align="left" rowspan="1" colspan="1">118</td>
<td align="left" rowspan="1" colspan="1">815</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>40%</bold></td>
<td align="left" rowspan="1" colspan="1">66</td>
<td align="left" rowspan="1" colspan="1">555</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap></sec><sec id="s2d">
<title>Calculation of feature vectors</title>
<p>Performing feature selection is a useful measure to avoid over-fitting of the models, which also improves model performance, cost-effectiveness of models, and to understand which features would be predictive in nature <xref ref-type="bibr" rid="pone.0097446-Guyon1">[27]</xref>, <xref ref-type="bibr" rid="pone.0097446-Saeys1">[28]</xref>. In this study, we calculated the feature vectors from the dataset sequences using propy 1.0 <xref ref-type="bibr" rid="pone.0097446-Cao1">[29]</xref>. Propy is a python based package which calculates comprehensive structural and physicochemical features from protein sequences, in five feature groups ‚Äì Amino Acid Composition (AAC), Composition Transition and Distribution (CTD) of various physico-chemical properties, Auto-correlation Coefficients (AC), Pseudo Amino Acid Composition (PAAC) and Qausi Sequence Order structure (QSOD) <xref ref-type="bibr" rid="pone.0097446-Cao1">[29]</xref>. AAC includes percentage amino acid composition of each amino acid, i.e. 20 feature vectors along with dipeptide composition of protein represented by 400 feature vectors. AC incorporates three subgroups, i.e. normalized Moreau-Broto autocorrelation, Moran autocorrelation and Geary autocorrelation with feature vector dimension of 240 each, describing correlation based on specific structural or physicochemical properties. CTD group represent amino acid distribution pattern of the following physicochemical properties: hydrophobicity, normalized van der Waals volume, polarity, charge, secondary structure, solvent accessibility with feature vector dimension of 147. QSOD has two sequence-order sets namely, quasi-sequence-order with 100 feature vectors and sequence-order-coupling number with 90 feature vectors derived from Schneider‚ÄìWrede physicochemical and Grantham chemical distance matrix. PAAC consists of 2 subgroups, type I PAAC and type II PAAC (i.e. amphiphilic PAAC), with feature vector dimension of 60. Use of features based on Chou's pseudo-amino acid compositions have been widely used for development of machine learning models for predicting protein structural and functional classes <xref ref-type="bibr" rid="pone.0097446-Chou1">[30]</xref>‚Äì<xref ref-type="bibr" rid="pone.0097446-Chou3">[32]</xref>. We converted the propy outputs in dictionary format to comma separated value format with IDs in rows and feature vectors in corresponding columns. Additionally, we added a class label ‚ÄòOutcome‚Äô as ‚Äòactive‚Äô for positive dataset and as ‚Äòinactive‚Äô for negative dataset. Before calculating descriptors, propy also validates the protein sequences by Procheck python module to eliminate protein sequences having non-conventional amino acids. Further, we performed a random-stratified splitting of benchmark dataset to generate training-cum-validation dataset (80%) and independent testing dataset (20%).</p>
</sec><sec id="s2e">
<title>Machine Learning</title>
<p>Waikato Environment for Knowledge Analysis (WEKA) version 3.6.10 was used for the data pre-processing, feature selection and classification experiments <xref ref-type="bibr" rid="pone.0097446-Hall1">[33]</xref>. WEKA is a popular suite for various machine learning algorithms, having various tools for data pre-processing, classification, regression, clustering, association rules and visualization.</p>
<sec id="s2e1">
<title>Data Pre-processing</title>
<p>The testing and training dataset in comma separated value (CSV) format were converted to attribute-relation file format (ARFF) files using weka.core.converters, to convert input files to WEKA readable format.</p>
</sec><sec id="s2e2">
<title>Feature Selection</title>
<p>To reduce the feature space dimensions, we combined the filter method ‚ÄúCorrelation based feature selection‚Äù with ‚ÄúBest First search‚Äù strategy. Correlation based feature selection is a multivariate analysis to determines the feature subset that are highly correlated with the class, yet uncorrelated with each other <xref ref-type="bibr" rid="pone.0097446-Hall2">[34]</xref>. ‚ÄúInfoGainAttributeEval‚Äù with ‚ÄúRanker‚Äù strategy was implemented to determine contribution of each feature vector towards class label. We performed optimization of feature vector space only upon training-cum-validation datasets.</p>
</sec><sec id="s2e3">
<title>Algorithms for Classification</title>
<p>In the present study, four different state-of-art supervised machine learning algorithms namely J48, LibSVM, Na√Øve Bayes and Random Forest are used for model generation and comparison. J48 implements C4.5 decision tree learning algorithm <xref ref-type="bibr" rid="pone.0097446-Quinlan1">[35]</xref>. LibSVM is a library of Support Vector Machines (SVM) implemented as a wrapper within WEKA <xref ref-type="bibr" rid="pone.0097446-Chang1">[36]</xref>, <xref ref-type="bibr" rid="pone.0097446-Yasser1">[37]</xref>. SVM algorithm generates a hyperplane in the feature space with maximum margin distinguishing positive instances from negative <xref ref-type="bibr" rid="pone.0097446-Vapnik1">[38]</xref>. Na√Øve Bayes is a classifier based on Bayes theorem and assumes that evidence based on attributes which are statistically independent <xref ref-type="bibr" rid="pone.0097446-Friedman1">[39]</xref>. Random Forest is an ensemble classifier based on independent decision trees <xref ref-type="bibr" rid="pone.0097446-Breiman1">[40]</xref>.</p>
<p>Initially we employed standard error base classifiers on the training data. As the dataset is unbalanced, and the base classifiers assume equal weighting of classes, we trained the models with cost sensitive learning algorithms optimizing mis-classification costs <xref ref-type="bibr" rid="pone.0097446-Japkowicz1">[41]</xref>. Cost sensitive meta-learning algorithm introduces cost sensitivity to its base classifier by two ways. This is achieved either by reweighting the training instances according to the cost assigned to each class or using minimum expected misclassification cost for predicting the class <xref ref-type="bibr" rid="pone.0097446-Witten1">[42]</xref>. We used the former strategy by setting the parameter ‚Äú<italic>Minimize Expected Cost</italic>‚Äù as false in the cost sensitive classifier.</p>
</sec><sec id="s2e4">
<title>Training-cum-validation</title>
<p>We developed the prediction models by training the chosen classifiers on the optimized features from feature selection and validating by 10 fold cross validation technique. We added arbitrary costs on the false negative rates to overcome class imbalance problem in the dataset, with the costs starting from 2 such that false positive rate would not exceed threshold of 20%. Further, we compared the model performances by standard statistical measures.</p>
</sec><sec id="s2e5">
<title>Independent dataset test</title>
<p>It is recommended to perform independent data testing to exclude the ‚Äúmemory‚Äù effect or bias in the predictive modelling <xref ref-type="bibr" rid="pone.0097446-Chou2">[31]</xref>. In this study, we re-evaluated the best predictive models by 10 fold cross validation study on independent dataset. We analyzed and compared the performances of the proposed models using standard statistical measures.</p>
</sec></sec><sec id="s2f">
<title>Classifier Evaluation</title>
<p>We used different evaluation metrics normally recommended for evaluating the classifier's performances- accuracy, sensitivity or recall, specificity, Balanced Classification Rate (BCR), F-value, Matthews Correlation Coefficient (MCC) and Area Under Recievers Operating Characteristic (auROC) curve <xref ref-type="bibr" rid="pone.0097446-Sokolova1">[43]</xref>, <xref ref-type="bibr" rid="pone.0097446-Sun1">[44]</xref>. Accuracy provides the overall effectiveness of the classifier (1). Sensitivity or Recall determines classifier effectiveness to identify positive class labels (2). Specificity calculates the classier effectiveness to identify negative class labels (3). BCR, F-measure and MCC are the standard evaluation metric, in the case of class imbalance. BCR, also called as balanced accuracy, is the average of sensitivity and specificity (4). F measure combines recall and precision by harmonic mean (5). MCC value ranges from 0 to 1 where 1 is the perfect prediction and 0 is random prediction (6). Mathematical representation of these expressions is given below, where TP is the number of True Positive, TN is the number of True Negatives, FN is the number of False Negatives, and FP is the number of False Positives for a prediction method. Receiver Operating Characteristic (ROC) curve is a 2 dimensional graphical plot of FP rate on X-axis vs. TP rate on Y axis. The auROC provides a single measure in the case of comparing performance of several classifiers. <disp-formula id="pone.0097446.e001"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0097446.e001" xlink:type="simple"/><label>(1)</label></disp-formula><disp-formula id="pone.0097446.e002"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0097446.e002" xlink:type="simple"/><label>(2)</label></disp-formula><disp-formula id="pone.0097446.e003"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0097446.e003" xlink:type="simple"/><label>(3)</label></disp-formula><disp-formula id="pone.0097446.e004"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0097446.e004" xlink:type="simple"/><label>(4)</label></disp-formula><disp-formula id="pone.0097446.e005"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0097446.e005" xlink:type="simple"/><label>(5)</label></disp-formula><disp-formula id="pone.0097446.e006"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0097446.e006" xlink:type="simple"/><label>(6)</label></disp-formula></p>
</sec></sec><sec id="s3">
<title>Results and Discussion</title>
<sec id="s3a">
<title>PSI-BLAST similarity based Search</title>
<p>Similarity based searches with annotated datasets is the first step for functional annotation of novel protein sequences. PSI-BLAST is a remote homology similarity and preferred algorithm for functional annotation projects. We employed PSI-BLAST, by leave-one out strategy on final 208 proteins positive dataset and obtained no significant hits for 20 sequences. This reinforced the need to develop alternative prediction tools for viral suppressor proteins.</p>
</sec><sec id="s3b">
<title>Feature Selection</title>
<p>The python tool propy yielded 1537 feature vectors from five different feature groups i.e. AAC, AC, CTD, PAAC, QSOD for each instance. The dataset was then divided into training-cum-validation and test dataset (<xref ref-type="table" rid="pone-0097446-t002">Table 2</xref>). Feature vector optimization performed on training-cum-validation data by correlation based feature selection reduced the feature vectors from 1537 to 73 in dataset of 90% redundancy levels and 77 in dataset with 70% redundancy levels. To have an understanding of which feature vectors contributed to the optimal subset, we investigated the distribution of each type of feature for feature group category (<xref ref-type="table" rid="pone-0097446-t003">Table 3</xref>). Intriguingly, we observed that the optimal feature subset had representation from all feature groups except QSOD. We have provided the description of the features calculated in each feature group category, the selected optimal features and the information gain values as <xref ref-type="supplementary-material" rid="pone.0097446.s003">Dataset S3</xref>.</p>
<table-wrap id="pone-0097446-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0097446.t002</object-id><label>Table 2</label><caption>
<title>Number of positive and negative instances in testing and training dataset.</title>
</caption><alternatives><graphic id="pone-0097446-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0097446.t002" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Redundancy (%)</td>
<td align="left" rowspan="1" colspan="1">Class</td>
<td align="left" rowspan="1" colspan="1">Training Data</td>
<td align="left" rowspan="1" colspan="1">Testing Data</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>90</bold></td>
<td align="left" rowspan="1" colspan="1">Positive</td>
<td align="left" rowspan="1" colspan="1">107</td>
<td align="left" rowspan="1" colspan="1">26</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>90</bold></td>
<td align="left" rowspan="1" colspan="1">Negative</td>
<td align="left" rowspan="1" colspan="1">739</td>
<td align="left" rowspan="1" colspan="1">185</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>70</bold></td>
<td align="left" rowspan="1" colspan="1">Positive</td>
<td align="left" rowspan="1" colspan="1">89</td>
<td align="left" rowspan="1" colspan="1">22</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>70</bold></td>
<td align="left" rowspan="1" colspan="1">Negative</td>
<td align="left" rowspan="1" colspan="1">633</td>
<td align="left" rowspan="1" colspan="1">158</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap><table-wrap id="pone-0097446-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0097446.t003</object-id><label>Table 3</label><caption>
<title>Feature distribution of optimal feature subset generated by correlation based feature selection.</title>
</caption><alternatives><graphic id="pone-0097446-t003-3" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0097446.t003" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Feature Group</td>
<td align="left" rowspan="1" colspan="1">Feature</td>
<td align="left" rowspan="1" colspan="1">Feature vectors calculated</td>
<td colspan="2" align="left" rowspan="1">Feature vectors selected</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Redundancy level</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">90%</td>
<td align="left" rowspan="1" colspan="1">70%</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>AAC</bold></td>
<td align="left" rowspan="1" colspan="1">Amino acid composition</td>
<td align="left" rowspan="1" colspan="1">20</td>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">3</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>AAC</bold></td>
<td align="left" rowspan="1" colspan="1">Dipeptide composition</td>
<td align="left" rowspan="1" colspan="1">400</td>
<td align="left" rowspan="1" colspan="1">39</td>
<td align="left" rowspan="1" colspan="1">43</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>CTD</bold></td>
<td align="left" rowspan="1" colspan="1">Composition</td>
<td align="left" rowspan="1" colspan="1">21</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>CTD</bold></td>
<td align="left" rowspan="1" colspan="1">Transition</td>
<td align="left" rowspan="1" colspan="1">21</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">NA</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>CTD</bold></td>
<td align="left" rowspan="1" colspan="1">Distribution</td>
<td align="left" rowspan="1" colspan="1">105</td>
<td align="left" rowspan="1" colspan="1">10</td>
<td align="left" rowspan="1" colspan="1">10</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>AC</bold></td>
<td align="left" rowspan="1" colspan="1">Geary autocorrelation</td>
<td align="left" rowspan="1" colspan="1">240</td>
<td align="left" rowspan="1" colspan="1">6</td>
<td align="left" rowspan="1" colspan="1">6</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>AC</bold></td>
<td align="left" rowspan="1" colspan="1">Moran autocorrelation</td>
<td align="left" rowspan="1" colspan="1">240</td>
<td align="left" rowspan="1" colspan="1">5</td>
<td align="left" rowspan="1" colspan="1">3</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>AC</bold></td>
<td align="left" rowspan="1" colspan="1">Normalized Moreau-Broto autocorrelation</td>
<td align="left" rowspan="1" colspan="1">240</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">NA</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>PAAC</bold></td>
<td align="left" rowspan="1" colspan="1">Type 1 pseudo-amino acid composition</td>
<td align="left" rowspan="1" colspan="1">20</td>
<td align="left" rowspan="1" colspan="1">5</td>
<td align="left" rowspan="1" colspan="1">5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>PAAC</bold></td>
<td align="left" rowspan="1" colspan="1">Type 2 pseudo-amino acid composition</td>
<td align="left" rowspan="1" colspan="1">40</td>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>QSOD</bold></td>
<td align="left" rowspan="1" colspan="1">Quasi-sequence-order desciptors</td>
<td align="left" rowspan="1" colspan="1">100</td>
<td align="left" rowspan="1" colspan="1">NA</td>
<td align="left" rowspan="1" colspan="1">1</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>QSOD</bold></td>
<td align="left" rowspan="1" colspan="1">Sequence-order-coupling number</td>
<td align="left" rowspan="1" colspan="1">90</td>
<td align="left" rowspan="1" colspan="1">NA</td>
<td align="left" rowspan="1" colspan="1">NA</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">1537</td>
<td align="left" rowspan="1" colspan="1">73</td>
<td align="left" rowspan="1" colspan="1">77</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap></sec><sec id="s3c">
<title>Training-cum-validation</title>
<p>To come up with the best prediction model, we compared the prediction performance of the four machine learning approaches, namely LibSVM, Random Forest, J48 and Na√Øve Bayes. We generated seven training-cum-validation dataset and independent testing dataset for 90% and 70% redundancy i.e. AAC, AC, CTD, PAAC, QSOD, all descriptors and optimized feature set. We performed the initial experiments with standard error base classifiers and later standardized with meta-learner cost sensitive classifiers. We introduced misclassification cost on false negatives and augmented to a threshold of 20% for FP rate. Thereby, we generated a large number of models with different cost settings. As expected we observed that introducing cost for FN, decreased the number of FN and increased FP (data not shown). Hence, cost sensitive classifiers lead to more robust models as compared to standard classifiers. We generated all the training models by 10 fold cross validations, the detailed statistical evaluation for 57 generated models is given in <xref ref-type="supplementary-material" rid="pone.0097446.s004">Dataset S4</xref>. We observed that prediction models generated with dataset redundancy 70% performed better than prediction models generated from dataset redundancy 90%. Within training models generated from dataset with redundancy 70%, prediction models with optimized feature vectors performed better than classifiers generated by individual feature class. Henceforth, we will mainly discuss the training models generated with 70% redundancy threshold, using 77 feature vectors optimized by correlation based feature selection.</p>
<p><xref ref-type="table" rid="pone-0097446-t004">Table 4</xref> shows the misclassification cost, accuracy, sensitivity, specificity, BCR, F-value, MCC and auROCs of the best prediction models generated with dataset of 70% redundancy. We observed that LibSVM required minimum misclassification cost settings of 18 and Na√Øve Bayes required a maximum of 4200. All the classifiers had their accuracies approximately around 80%. The auROC values determined that the models were predictive in nature and not random in their performance. Based on the highest value of calculated statistical evaluators, Random Forest trained prediction model performed the best amongst the four implemented machine learning techniques.</p>
<table-wrap id="pone-0097446-t004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0097446.t004</object-id><label>Table 4</label><caption>
<title>Statistics of best predictive models generated by 10 fold cross-validation of training dataset.</title>
</caption><alternatives><graphic id="pone-0097446-t004-4" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0097446.t004" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Classifier<xref ref-type="table-fn" rid="nt101">*</xref></td>
<td align="left" rowspan="1" colspan="1">CSC J48</td>
<td align="left" rowspan="1" colspan="1">CSC LibSVM</td>
<td align="left" rowspan="1" colspan="1">CSC Na√Øve Bayes</td>
<td align="left" rowspan="1" colspan="1">CSC Random Forest</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Misclassification Cost</bold></td>
<td align="left" rowspan="1" colspan="1">42</td>
<td align="left" rowspan="1" colspan="1">18</td>
<td align="left" rowspan="1" colspan="1">4200</td>
<td align="left" rowspan="1" colspan="1">55.4</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Accuracy</bold></td>
<td align="left" rowspan="1" colspan="1">79.50</td>
<td align="left" rowspan="1" colspan="1">78.25</td>
<td align="left" rowspan="1" colspan="1">78.25</td>
<td align="left" rowspan="1" colspan="1"><bold>80.61</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Sensitivity</bold></td>
<td align="left" rowspan="1" colspan="1">73.03</td>
<td align="left" rowspan="1" colspan="1">62.92</td>
<td align="left" rowspan="1" colspan="1">65.17</td>
<td align="left" rowspan="1" colspan="1"><bold>80.90</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Specificity</bold></td>
<td align="left" rowspan="1" colspan="1">80.41</td>
<td align="left" rowspan="1" colspan="1">80.41</td>
<td align="left" rowspan="1" colspan="1">80.09</td>
<td align="left" rowspan="1" colspan="1"><bold>80.57</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>BCR<sup>$</sup></bold></td>
<td align="left" rowspan="1" colspan="1">76.72</td>
<td align="left" rowspan="1" colspan="1">71.67</td>
<td align="left" rowspan="1" colspan="1">72.63</td>
<td align="left" rowspan="1" colspan="1"><bold>80.73</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>F-value</bold></td>
<td align="left" rowspan="1" colspan="1">0.47</td>
<td align="left" rowspan="1" colspan="1">0.42</td>
<td align="left" rowspan="1" colspan="1">0.42</td>
<td align="left" rowspan="1" colspan="1"><bold>0.51</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>MCC<sup>‚àß</sup></bold></td>
<td align="left" rowspan="1" colspan="1">0.40</td>
<td align="left" rowspan="1" colspan="1">0.33</td>
<td align="left" rowspan="1" colspan="1">0.34</td>
<td align="left" rowspan="1" colspan="1"><bold>0.46</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>auROC<sup>#</sup></bold></td>
<td align="left" rowspan="1" colspan="1">0.79</td>
<td align="left" rowspan="1" colspan="1">0.72</td>
<td align="left" rowspan="1" colspan="1">0.79</td>
<td align="left" rowspan="1" colspan="1"><bold>0.91</bold></td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt101"><label/><p>*CSC denotes Cost Sensitive Classifier; BCR<sup>$</sup> is Balanced Classification Rate; MCC<sup>‚àß</sup> is Matthews Correlation Coefficient; auROC<sup>#</sup> is area under Receiver Operating characteristic curve. Highest numerical value in each row is highlighted as bold.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s3d">
<title>Independent-dataset Testing</title>
<p>We performed an independent data testing to further assess the performances of the best prediction models obtained from cross validation studies on unseen data. The evaluation of comparative performance of the classifiers has done using standard statistical measures i.e. accuracy, sensitivity, specificity, BCR, F-measure, and auROC plot. <xref ref-type="table" rid="pone-0097446-t005">Table 5</xref> summarizes the results from these statistical measures. We found the performance of the models with independent dataset in terms of accuracy and auROC in coherence with the cross validation results.</p>
<table-wrap id="pone-0097446-t005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0097446.t005</object-id><label>Table 5</label><caption>
<title>Summary of statistical measures of the best classifiers on re-evaluation with independent dataset.</title>
</caption><alternatives><graphic id="pone-0097446-t005-5" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0097446.t005" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Classifier<xref ref-type="table-fn" rid="nt102">*</xref></td>
<td align="left" rowspan="1" colspan="1">CSC J48</td>
<td align="left" rowspan="1" colspan="1">CSC LibSVM</td>
<td align="left" rowspan="1" colspan="1">CSC Na√Øve Bayes</td>
<td align="left" rowspan="1" colspan="1">CSC Random Forest</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Accuracy</bold></td>
<td align="left" rowspan="1" colspan="1">85.0</td>
<td align="left" rowspan="1" colspan="1">78.33</td>
<td align="left" rowspan="1" colspan="1">78.33</td>
<td align="left" rowspan="1" colspan="1"><bold>86.11</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Sensitivity</bold></td>
<td align="left" rowspan="1" colspan="1">81.82</td>
<td align="left" rowspan="1" colspan="1">77.27</td>
<td align="left" rowspan="1" colspan="1">68.18</td>
<td align="left" rowspan="1" colspan="1"><bold>86.36</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Specificity</bold></td>
<td align="left" rowspan="1" colspan="1">85.44</td>
<td align="left" rowspan="1" colspan="1">78.48</td>
<td align="left" rowspan="1" colspan="1">79.75</td>
<td align="left" rowspan="1" colspan="1"><bold>86.08</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>BCR<sup>$</sup></bold></td>
<td align="left" rowspan="1" colspan="1">83.63</td>
<td align="left" rowspan="1" colspan="1">77.88</td>
<td align="left" rowspan="1" colspan="1">73.96</td>
<td align="left" rowspan="1" colspan="1"><bold>86.22</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>F-value</bold></td>
<td align="left" rowspan="1" colspan="1">0.57</td>
<td align="left" rowspan="1" colspan="1">0.47</td>
<td align="left" rowspan="1" colspan="1">0.43</td>
<td align="left" rowspan="1" colspan="1"><bold>0.60</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>MCC<sup>‚àß</sup></bold></td>
<td align="left" rowspan="1" colspan="1">0.53</td>
<td align="left" rowspan="1" colspan="1">0.41</td>
<td align="left" rowspan="1" colspan="1">0.36</td>
<td align="left" rowspan="1" colspan="1"><bold>0.57</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>auROC<sup>#</sup></bold></td>
<td align="left" rowspan="1" colspan="1">0.84</td>
<td align="left" rowspan="1" colspan="1">0.78</td>
<td align="left" rowspan="1" colspan="1">0.83</td>
<td align="left" rowspan="1" colspan="1"><bold>0.95</bold></td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt102"><label/><p>*CSC denotes Cost Sensitive classifier; BCR<sup>$</sup> is Balanced Classification Rate; MCC<sup>‚àß</sup> is Matthews Correlation Coefficient; auROC<sup>#</sup> is area under Receiver Operating Characteristic (ROC) curve. Highest numerical value in each row is highlighted as bold letters.</p></fn></table-wrap-foot></table-wrap>
<p>Sensitivity-specificity plots help identification of the best model which correctly classifies positive and negative labelled instances (<xref ref-type="fig" rid="pone-0097446-g003">Figure 3</xref>). Specificity of all the models was in the range of 78‚Äì86%, however the prediction model sensitivities varied in a wide range from a minimum of 68.19% for Na√Øve Bayes, 77.27% for LibSVM and 81.82% for J48 to highest 86.36% for Random Forest algorithm based model.</p>
<fig id="pone-0097446-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0097446.g003</object-id><label>Figure 3</label><caption>
<title>Sensitivity and Specificity plot.</title>
<p>The plot compares sensitivity and specificity of the developed predictive models, to determine effective classifier for identifying positive and negative instances. Random Forest classifier has the highest sensitivity and specificity values as compared to J48, LibSVM and Na√Øve Bayes.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0097446.g003" position="float" xlink:type="simple"/></fig>
<p>LibSVM and Na√Øve Bayes had minimum predictive accuracies of 78.33%, with 85% of J48 classifiers and maximum accuracy 86.11% for Random Forest (<xref ref-type="table" rid="pone-0097446-t005">Table 5</xref>). Owing to the class imbalance problem in the training dataset, we also calculated BCR, F-measure and MCC to precisely evaluate model effectiveness. F-value of the models was in the range of 0.43‚Äì0.60. Although the models may not have high F value, the remarkably high sensitivity or recall (92.31%) of the model based on Random Forest algorithm is noteworthy.</p>
<p>ROC analysis is an established approach for classifier evaluation in the Machine learning approaches, as it shows the trade-off between true positive rate and false positive rate. The ROC plot of the classifiers in <xref ref-type="fig" rid="pone-0097446-g004">Figure 4</xref> and auROC values (<xref ref-type="table" rid="pone-0097446-t005">Table 5</xref>) suggest that the performance of the Random Forest was the best, followed by J48, Na√Øve Bayes and LibSVM. The observed auROC values (<xref ref-type="table" rid="pone-0097446-t005">Table 5</xref>) were significantly higher than threshold of 0.5 i.e. random guess as prediction. Thus, ROC analysis assured for the optimal and robust performance of Random Forest.</p>
<fig id="pone-0097446-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0097446.g004</object-id><label>Figure 4</label><caption>
<title>ROC curves for the predictive performance of different cost sensitive classifiers.</title>
<p>ROC plot depicts a relative trade-off between true positive rate and false positive rate of the predictions. The diagonal value represents a completely random guess. The corresponding scalar values of area under curve are given as auROC in <xref ref-type="table" rid="pone-0097446-t004">table 4</xref>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0097446.g004" position="float" xlink:type="simple"/></fig>
<p>Random Forest model ranks above the J48, Na√Øve Bayes and LibSVM, as evident from the calculated statistical evaluators. Hence, the Random Forest algorithm based model is the most efficient model to classify viral sequences as viral RNA silencing suppressors and silencing non suppressors with an accuracy of 86.22%, BCR of 86.22%, MCC of 0.57 and remarkably high auROC of 0.95.</p>
<p>During the preparation of this manuscript, a new plant virus encoded viral supressor was included in UniProtKB. The protein, CLINK_BBTVA is encoded in Banana Bunchy top virus, annotated as viral suppresor of RNA silencing. The sequence when tested as a new blind independent dataset sequence for the Random Forest algorithm based prediction models, is correctly predicted as a suppressor of RNA silencing. The result further confirms the reliability of the Random Forest based classifier.</p>
</sec><sec id="s3e">
<title>Implementation</title>
<p>The best performing Random Forest based prediction model is implemented as a freely accessible web server pVsupPred (<xref ref-type="fig" rid="pone-0097446-g005">Figure 5</xref>, <ext-link ext-link-type="uri" xlink:href="http://bioinfo.icgeb.res.in/pvsup/" xlink:type="simple">http://bioinfo.icgeb.res.in/pvsup/</ext-link>). Scripting is done in HTML, PHP, PERL and shell to develop the user friendly interface. The server accepts input protein sequences in FASTA format. The VsupPred web server results are generated in simple tabular format which includes sequence ID, prediction score and decision of the model regarding the sequence. The higher prediction scores indicate better confidence level of prediction.</p>
<fig id="pone-0097446-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0097446.g005</object-id><label>Figure 5</label><caption>
<title>Snapshot of pVsupPred web server.</title>
<p>The web server predicts viral suppressor of RNA silencing based on Random Forest classifier generated in this study. <xref ref-type="fig" rid="pone-0097446-g005">Figure 5a</xref> is snapshot of prediction page of pVsupPred webserver. The webserver accepts FASTA formatted protein sequences. <xref ref-type="fig" rid="pone-0097446-g005">Figure 5b</xref> is snapshot of sample output of pVsupPred web server. The prediction threshold range of the developed method is 0 to 1.0.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0097446.g005" position="float" xlink:type="simple"/></fig></sec><sec id="s3f">
<title>Perspective</title>
<p>pVsupPred can be potentially used for screening RNA silencing suppressors in viral proteomes. Since the development and affordability of next generation sequencing, there lies a huge gap between sequencing and annotation projects. pVsupPred is one such endeavor which can be helpful in the annotation projects of genomic data of viruses. We anticipate that our implemented methodology would be useful for experimental biologists and research community as a whole. In future, with availability of additional experimentally validated viral suppressor's sequences, performance of the models could be further improved.</p>
</sec></sec><sec id="s4">
<title>Conclusion</title>
<p>In this study, we employed a empirical approach to develop a prediction method for plant viral RNA silencing suppressors by developing statistical prediction models based on features of the experimentally verified viral suppressor protein sequences. We have generated classification models with four supervised learning classifiers i.e. Na√Øve Bayes, Random Forest, J48 and LibSVM. Feature selection by correlation based feature selection increased the robustness of the models and helped identification of optimal features which reflects compositional patterns in the viral proteins responsible for RNA silencing suppressor activities. Further introducing meta-learning by cost sensitive classifiers led to enhanced and robust performance of models. Random Forest predictive model achieved the best performance when compared with Na√Øve Bayes, J48 and LibSVM models. We expect that these prediction models can aid screening of plant viral ORFs for potential suppressor activity for RNA silencing. Currently, the best prediction model developed in this study is available as web server pVsupPred. To the best of our knowledge, this is the first exclusive prediction method for plant viral suppressors of host RNA silencing.</p>
</sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pone.0097446.s001" mimetype="application/x-fasta" xlink:href="info:doi/10.1371/journal.pone.0097446.s001" position="float" xlink:type="simple"><label>Dataset S1</label><caption>
<p><bold>Positive Benchmark dataset.</bold> This consists of 208 viral RNA silencing suppressor protein sequences in fasta format. The file can be viewed using any text editor like wordpad or Notepad.</p>
<p>(FASTA)</p>
</caption></supplementary-material><supplementary-material id="pone.0097446.s002" mimetype="application/x-fasta" xlink:href="info:doi/10.1371/journal.pone.0097446.s002" position="float" xlink:type="simple"><label>Dataset S2</label><caption>
<p><bold>Negative Benchmark dataset.</bold> This consists of 1321 viral protein sequences which are RNA silencing suppressor in fasta format. The file can be viewed using any text editor like wordpad or Notepad.</p>
<p>(FASTA)</p>
</caption></supplementary-material><supplementary-material id="pone.0097446.s003" mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet" xlink:href="info:doi/10.1371/journal.pone.0097446.s003" position="float" xlink:type="simple"><label>Dataset S3</label><caption>
<p><bold>Feature Vectors.</bold> This contains the information about the description of total feature vectors calculates and the ones selected by feature selection in an excel worksheet.</p>
<p>(XLSX)</p>
</caption></supplementary-material><supplementary-material id="pone.0097446.s004" mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet" xlink:href="info:doi/10.1371/journal.pone.0097446.s004" position="float" xlink:type="simple"><label>Dataset S4</label><caption>
<p><bold>Statistical Evaluation of Classifiers.</bold> This contains the information about the statistical evaluators and parameter details used for generating classifiers by 10 fold cross-validation and re-evaluation of the classifiers by independent dataset at redundancy level of 90% and 70%.</p>
<p>(XLSX)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>The authors thank the Department of Biotechnology (DBT, India) for the grant for "Bioinformatics Infrastructure Facility" at ICGEB, New Delhi. ZJ acknowledges the University Grants Commission (UGC, India) for the Senior Research Fellowship.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0097446-Sioud1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sioud</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>RNA interference and innate immunity</article-title>. <source>Advanced Drug Delivery Reviews</source> <volume>59</volume>: <fpage>153</fpage>‚Äì<lpage>163</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Anandalakshmi1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anandalakshmi</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Pruss</surname><given-names>GJ</given-names></name>, <name name-style="western"><surname>Ge</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Marathe</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Mallory</surname><given-names>AC</given-names></name>, <etal>et al</etal>. (<year>1998</year>) <article-title>A viral suppressor of gene silencing in plants</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>95</volume>: <fpage>13079</fpage>‚Äì<lpage>13084</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Voinnet1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Voinnet</surname><given-names>O</given-names></name> (<year>2005</year>) <article-title>Induction and suppression of RNA silencing: insights from viral infections</article-title>. <source>Nat Rev Genet</source> <volume>6</volume>: <fpage>206</fpage>‚Äì<lpage>220</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Burgyan1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burgyan</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Havelda</surname><given-names>Z</given-names></name> (<year>2011</year>) <article-title>Viral suppressors of RNA silencing</article-title>. <source>Trends Plant Sci</source> <volume>16</volume>: <fpage>265</fpage>‚Äì<lpage>272</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Jiang1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jiang</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Wei</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>Y</given-names></name> (<year>2012</year>) <article-title>Viral suppression of RNA silencing</article-title>. <source>Science China Life Sciences</source> <volume>55</volume>: <fpage>109</fpage>‚Äì<lpage>118</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Mrai1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>M√©rai</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Ker√©nyi</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Kert√©sz</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Magna</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Lakatos</surname><given-names>L</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>Double-stranded RNA binding may be a general plant RNA viral strategy to suppress RNA silencing</article-title>. <source>Journal of Virology</source> <volume>80</volume>: <fpage>5747</fpage>‚Äì<lpage>5756</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Mrai2"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>M√©rai</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Ker√©nyi</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Moln√°r</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Barta</surname><given-names>E</given-names></name>, <name name-style="western"><surname>V√°l√≥czi</surname><given-names>A</given-names></name>, <etal>et al</etal>. (<year>2005</year>) <article-title>Aureusvirus P14 is an efficient RNA silencing suppressor that binds double-stranded RNAs without size specificity</article-title>. <source>Journal of Virology</source> <volume>79</volume>: <fpage>7217</fpage>‚Äì<lpage>7226</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Silhavy1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Silhavy</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Moln√°r</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Lucioli</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Szittya</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Hornyik</surname><given-names>C</given-names></name>, <etal>et al</etal>. (<year>2002</year>) <article-title>A viral protein suppresses RNA silencing and binds silencing-generated, 21-to 25-nucleotide double-stranded RNAs</article-title>. <source>The EMBO Journal</source> <volume>21</volume>: <fpage>3070</fpage>‚Äì<lpage>3080</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Chen1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chen</surname><given-names>HY</given-names></name>, <name name-style="western"><surname>Yang</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Lin</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Yuan</surname><given-names>YA</given-names></name> (<year>2008</year>) <article-title>Structural basis for RNA-silencing suppression by Tomato aspermy virus protein 2b</article-title>. <source>EMBO reports</source> <volume>9</volume>: <fpage>754</fpage>‚Äì<lpage>760</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Chao1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chao</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>Chapados</surname><given-names>BR</given-names></name>, <name name-style="western"><surname>Debler</surname><given-names>EW</given-names></name>, <name name-style="western"><surname>Schneemann</surname><given-names>A</given-names></name>, <etal>et al</etal>. (<year>2005</year>) <article-title>Dual modes of RNA-silencing suppression by Flock House virus protein B2</article-title>. <source>Nature structural &amp; molecular biology</source> <volume>12</volume>: <fpage>952</fpage>‚Äì<lpage>957</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Bortolamiol1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bortolamiol</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Pazhouhandeh</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ziegler-Graff</surname><given-names>V</given-names></name> (<year>2008</year>) <article-title>Viral suppression of RNA silencing by destabilisation of ARGONAUTE 1</article-title>. <source>Plant signaling &amp; behavior</source> <volume>3</volume>: <fpage>657</fpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Haas1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Haas</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Azevedo</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Moissiard</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Geldreich</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Himber</surname><given-names>C</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Nuclear import of CaMV P6 is required for infection and suppression of the RNA silencing factor DRB4</article-title>. <source>The EMBO Journal</source> <volume>27</volume>: <fpage>2102</fpage>‚Äì<lpage>2112</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Song1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Song</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Gao</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Jiang</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>Y</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Silencing suppressors: viral weapons for countering host cell defenses</article-title>. <source>Protein &amp; Cell</source> <volume>2</volume>: <fpage>273</fpage>‚Äì<lpage>281</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Csorba1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Csorba</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Pantaleo</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Burgyan</surname><given-names>J</given-names></name> (<year>2009</year>) <article-title>RNA silencing: an antiviral mechanism</article-title>. <source>Adv Virus Res</source> <volume>75</volume>: <fpage>35</fpage>‚Äì<lpage>71</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Karjee1"><label>15</label>
<mixed-citation publication-type="other" xlink:type="simple">Karjee S, Islam MN, Mukherjee SK (2008) Screening and identification of virus-encoded RNA silencing suppressors. RNAi: Springer. pp. 187‚Äì203.</mixed-citation>
</ref>
<ref id="pone.0097446-BivalkarMehla1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bivalkar-Mehla</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Vakharia</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Mehla</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Abreha</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Kanwar</surname><given-names>JR</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Viral RNA silencing suppressors (RSS): novel strategy of viruses to ablate the host RNA interference (RNAi) defense system</article-title>. <source>Virus Res</source> <volume>155</volume>: <fpage>1</fpage>‚Äì<lpage>9</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-deVries1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>de Vries</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Berkhout</surname><given-names>B</given-names></name> (<year>2008</year>) <article-title>RNAi suppressors encoded by pathogenic human viruses</article-title>. <source>The international journal of biochemistry &amp; cell biology</source> <volume>40</volume>: <fpage>2007</fpage>‚Äì<lpage>2012</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Rahman1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rahman</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Karjee</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Mukherjee</surname><given-names>SK</given-names></name> (<year>2012</year>) <article-title>MYMIV-AC2, a Geminiviral RNAi Suppressor Protein, Has Potential to Increase the Transgene Expression</article-title>. <source>Applied biochemistry and biotechnology</source> <volume>167</volume>: <fpage>758</fpage>‚Äì<lpage>775</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Rauschhuber1"><label>19</label>
<mixed-citation publication-type="other" xlink:type="simple">Rauschhuber C, Mueck-Haeusl M, Zhang W, Nettelbeck DM, Ehrhardt A (2013) RNAi suppressor P19 can be broadly exploited for enhanced adenovirus replication and microRNA knockdown experiments. Scientific reports <volume>3</volume>..</mixed-citation>
</ref>
<ref id="pone.0097446-Ramnani1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ramnani</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Gao</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Ozsoz</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Mulchandani</surname><given-names>A</given-names></name> (<year>2013</year>) <article-title>Electronic Detection of MicroRNA at Attomolar Level with High Specificity</article-title>. <source>Anal Chem</source> <volume>85</volume>: <fpage>8061</fpage>‚Äì<lpage>4</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Wu1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wu</surname><given-names>Q</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Ding</surname><given-names>SW</given-names></name> (<year>2010</year>) <article-title>Viral suppressors of RNA-based viral immunity: host targets</article-title>. <source>Cell Host Microbe</source> <volume>8</volume>: <fpage>12</fpage>‚Äì<lpage>15</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Larraaga1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Larra√±aga</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Calvo</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Santana</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Bielza</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Galdiano</surname><given-names>J</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>Machine learning in bioinformatics</article-title>. <source>Briefings in Bioinformatics</source> <volume>7</volume>: <fpage>86</fpage>‚Äì<lpage>112</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Tarca1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tarca</surname><given-names>AL</given-names></name>, <name name-style="western"><surname>Carey</surname><given-names>VJ</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>X-w</given-names></name>, <name name-style="western"><surname>Romero</surname><given-names>R</given-names></name>, <name name-style="western"><surname>DrƒÉghici</surname><given-names>S</given-names></name> (<year>2007</year>) <article-title>Machine Learning and Its Applications to Biology</article-title>. <source>PLoS Comput Biol</source> <volume>3</volume>: <fpage>e116</fpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><article-title>Update on activities at the Universal Protein Resource (UniProt) in 2013</article-title>. <source>Nucleic Acids Res</source> <volume>41</volume>: <fpage>D43</fpage>‚Äì<lpage>47</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Li1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Jaroszewski</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Godzik</surname><given-names>A</given-names></name> (<year>2001</year>) <article-title>Clustering of highly homologous sequences to reduce the size of large protein databases</article-title>. <source>Bioinformatics</source> <volume>17</volume>: <fpage>282</fpage>‚Äì<lpage>283</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Li2"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Jaroszewski</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Godzik</surname><given-names>A</given-names></name> (<year>2002</year>) <article-title>Tolerating some redundancy significantly speeds up clustering of large protein databases</article-title>. <source>Bioinformatics</source> <volume>18</volume>: <fpage>77</fpage>‚Äì<lpage>82</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Guyon1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Guyon</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Elisseeff</surname><given-names>A</given-names></name> (<year>2003</year>) <article-title>An introduction to variable and feature selection</article-title>. <source>J Mach Learn Res</source> <volume>3</volume>: <fpage>1157</fpage>‚Äì<lpage>1182</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Saeys1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Saeys</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Inza</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Larra√±aga</surname><given-names>P</given-names></name> (<year>2007</year>) <article-title>A review of feature selection techniques in bioinformatics</article-title>. <source>Bioinformatics</source> <volume>23</volume>: <fpage>2507</fpage>‚Äì<lpage>2517</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Cao1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cao</surname><given-names>DS</given-names></name>, <name name-style="western"><surname>Xu</surname><given-names>QS</given-names></name>, <name name-style="western"><surname>Liang</surname><given-names>YZ</given-names></name> (<year>2013</year>) <article-title>propy: a tool to generate various modes of Chou's PseAAC</article-title>. <source>Bioinformatics</source> <volume>29</volume>: <fpage>960</fpage>‚Äì<lpage>962</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Chou1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chou</surname><given-names>K-C</given-names></name> (<year>2009</year>) <article-title>Pseudo amino acid composition and its applications in bioinformatics, proteomics and system biology</article-title>. <source>Current Proteomics</source> <volume>6</volume>: <fpage>262</fpage>‚Äì<lpage>274</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Chou2"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chou</surname><given-names>K-C</given-names></name> (<year>2011</year>) <article-title>Some remarks on protein attribute prediction and pseudo amino acid composition</article-title>. <source>Journal of theoretical biology</source> <volume>273</volume>: <fpage>236</fpage>‚Äì<lpage>247</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Chou3"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chou</surname><given-names>KC</given-names></name> (<year>2001</year>) <article-title>Prediction of protein cellular attributes using pseudo-amino acid composition</article-title>. <source>Proteins: Structure, Function, and Bioinformatics</source> <volume>43</volume>: <fpage>246</fpage>‚Äì<lpage>255</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Hall1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hall</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Frank</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Holmes</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Pfahringer</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Reutemann</surname><given-names>P</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>The WEKA data mining software: an update</article-title>. <source>ACM SIGKDD Explorations Newsletter</source> <volume>11</volume>: <fpage>10</fpage>‚Äì<lpage>18</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Hall2"><label>34</label>
<mixed-citation publication-type="other" xlink:type="simple">Hall MA (1999) Correlation-based feature selection for machine learning: The University of Waikato.</mixed-citation>
</ref>
<ref id="pone.0097446-Quinlan1"><label>35</label>
<mixed-citation publication-type="book" xlink:type="simple">Quinlan JR (1993) C4.5:programs for machine learning: Morgan Kaufmann Publishers Inc. 302 p.</mixed-citation>
</ref>
<ref id="pone.0097446-Chang1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chang</surname><given-names>C-C</given-names></name>, <name name-style="western"><surname>Lin</surname><given-names>C-J</given-names></name> (<year>2011</year>) <article-title>LIBSVM: a library for support vector machines</article-title>. <source>ACM Transactions on Intelligent Systems and Technology (TIST)</source> <volume>2</volume>: <fpage>27</fpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Yasser1"><label>37</label>
<mixed-citation publication-type="other" xlink:type="simple">Yasser E-M, Honavar V (2005) WLSVM: Integrating LibSVM into Weka Environment. Software available at <ext-link ext-link-type="uri" xlink:href="http://www.cs.iastate.edu/~yasser/wlsvm/" xlink:type="simple">http://www.cs.iastate.edu/~yasser/wlsvm/</ext-link>. Accessed 2014 April 26.</mixed-citation>
</ref>
<ref id="pone.0097446-Vapnik1"><label>38</label>
<mixed-citation publication-type="other" xlink:type="simple">Vapnik V (1998) Statistical learning theory: Wiley.</mixed-citation>
</ref>
<ref id="pone.0097446-Friedman1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friedman</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Geiger</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Goldszmidt</surname><given-names>M</given-names></name> (<year>1997</year>) <article-title>Bayesian network classifiers</article-title>. <source>Machine learning</source> <volume>29</volume>: <fpage>131</fpage>‚Äì<lpage>163</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Breiman1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Breiman</surname><given-names>L</given-names></name> (<year>2001</year>) <article-title>Random Forests</article-title>. <source>Machine learning</source> <volume>45</volume>: <fpage>5</fpage>‚Äì<lpage>32</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Japkowicz1"><label>41</label>
<mixed-citation publication-type="other" xlink:type="simple">Japkowicz N (2000) The class imbalance problem: Significance and strategies. Proceedings of the 2000 International Conference on Artificial Intelligence 111‚Äì117.</mixed-citation>
</ref>
<ref id="pone.0097446-Witten1"><label>42</label>
<mixed-citation publication-type="other" xlink:type="simple">Witten IH, Frank E (2005) Data Mining: Practical machine learning tools and techniques: Morgan Kaufmann.</mixed-citation>
</ref>
<ref id="pone.0097446-Sokolova1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sokolova</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Lapalme</surname><given-names>G</given-names></name> (<year>2009</year>) <article-title>A systematic analysis of performance measures for classification tasks</article-title>. <source>Information Processing &amp; Management</source> <volume>45</volume>: <fpage>427</fpage>‚Äì<lpage>437</lpage>.</mixed-citation>
</ref>
<ref id="pone.0097446-Sun1"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sun</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Kamel</surname><given-names>MS</given-names></name>, <name name-style="western"><surname>Wong</surname><given-names>AKC</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>Y</given-names></name> (<year>2007</year>) <article-title>Cost-sensitive boosting for classification of imbalanced data</article-title>. <source>Pattern Recognition</source> <volume>40</volume>: <fpage>3358</fpage>‚Äì<lpage>3378</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>