<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group>
<journal-title>PLoS ONE</journal-title></journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-13-38485</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0086696</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Computer science</subject><subj-group><subject>Computerized simulations</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Materials science</subject><subj-group><subject>Material by attribute</subject><subj-group><subject>Intelligent materials</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Complex systems</subject><subject>Control theory</subject></subj-group></subj-group><subj-group><subject>Calculus</subject><subj-group><subject>Differential equations</subject></subj-group></subj-group><subj-group><subject>Nonlinear dynamics</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Physics</subject><subj-group><subject>Classical mechanics</subject><subj-group><subject>Mechanics</subject><subj-group><subject>Dynamics (mechanics)</subject></subj-group></subj-group></subj-group><subj-group><subject>Physical laws and principles</subject><subj-group><subject>Mechanics</subject><subj-group><subject>Dynamics (mechanics)</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Automated Design of Complex Dynamic Systems</article-title>
<alt-title alt-title-type="running-head">Designing Complex Dynamic Systems</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Hermans</surname><given-names>Michiel</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Schrauwen</surname><given-names>Benjamin</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Bienstman</surname><given-names>Peter</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Dambre</surname><given-names>Joni</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>ELIS department, Ghent University, Ghent, Belgium</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>INTEC department, Ghent University, Ghent, Belgium</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Gomez-Gardenes</surname><given-names>Jesus</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Universidad de Zarazoga, Spain</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">michiel.hermans@ugent.be</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: MH BS PB JD. Performed the experiments: MH BS PB JD. Analyzed the data: MH BS PB JD. Wrote the paper: MH BS PB JD.</p></fn>
</author-notes>
<pub-date pub-type="collection"><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>31</day><month>1</month><year>2014</year></pub-date>
<volume>9</volume>
<issue>1</issue>
<elocation-id>e86696</elocation-id>
<history>
<date date-type="received"><day>18</day><month>9</month><year>2013</year></date>
<date date-type="accepted"><day>11</day><month>12</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Hermans et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Several fields of study are concerned with uniting the concept of computation with that of the design of physical systems. For example, a recent trend in robotics is to design robots in such a way that they require a minimal control effort. Another example is found in the domain of photonics, where recent efforts try to benefit directly from the complex nonlinear dynamics to achieve more efficient signal processing. The underlying goal of these and similar research efforts is to internalize a large part of the necessary computations within the physical system itself by exploiting its inherent non-linear dynamics. This, however, often requires the optimization of large numbers of system parameters, related to both the system's structure as well as its material properties. In addition, many of these parameters are subject to fabrication variability or to variations through time. In this paper we apply a machine learning algorithm to optimize physical dynamic systems. We show that such algorithms, which are normally applied on abstract computational entities, can be extended to the field of differential equations and used to optimize an associated set of parameters which determine their behavior. We show that machine learning training methodologies are highly useful in designing robust systems, and we provide a set of both simple and complex examples using models of physical dynamical systems. Interestingly, the derived optimization method is intimately related to <italic>direct collocation</italic> a method known in the field of optimal control. Our work suggests that the application domains of both machine learning and optimal control have a largely unexplored overlapping area which envelopes a novel design methodology of smart and highly complex physical systems.</p>
</abstract>
<funding-group><funding-statement>This work is supported by the interuniversity attraction pole (IAP) Photonics@be of the Belgian Science Policy Office and the ERC NaResCo Starting grant. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="11"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>The digital computation paradigm has become so dominant, that in the minds of many, the word digital is implicitly assumed whenever computation is mentioned. This is mainly due to the fact that digital computation is extremely robust against variability and noise. This greatly facilitates the design process and is one of the main reasons why we can keep on designing ever more complex computers. However, the digital paradigm doesn't map well to the natural computation that occurs in many physical media and the quenching of their often rich dynamical spectrum to two-valued attractor dynamics comes at a huge efficiency cost. In contrast, analogue computers carry the potential to directly exploit the way the dynamics of physical systems respond to external stimuli, continuously transforming their real-valued state. This requires the selection of a physical system with natural dynamics that roughly match the computational requirements of a given task. Some of the pioneers of current computer science have investigated more generally applicable analogue computing models. For instance, in Von Neumann's original discussion of cellular automata <xref ref-type="bibr" rid="pone.0086696-VonNeumann1">[1]</xref>, five types were proposed, most of which were analogue. Turing's description of the role of reaction-diffusion in morphogenesis <xref ref-type="bibr" rid="pone.0086696-Turing1">[2]</xref> is another example. This work was originally adopted mainly by the biological community (to study morphogenesis), but it later became the basis for, e.g., Adamatzki's recent work on reaction-diffusion computers <xref ref-type="bibr" rid="pone.0086696-Adamatzky1">[3]</xref>.</p>
<p>In practice, even coming close to the exploitation of this potential for complex behaviors that are not easily partitioned into small building blocks requires an economically unacceptable design effort. Besides the inherent complexity of tuning a complex nonlinear dynamical system, designers need to ensure robustness under uncertain conditions. Many design parameters are not very well controlled during fabrication. Additionally, they may vary in time in random or systematic ways (e.g., due to thermal effects). Finally, the exact desired behavior of the system can usually not be described analytically, because it needs to perform its task under variable conditions and in the presence of what is usually termed <italic>noise</italic>.</p>
<p>Yet, robust and highly complex analogue computing occurs within all living systems, from single cells to complex organisms. As the brain (human or animal) is exposed to stimuli from its senses, muscles and pain receptors, it auto-rewires, using its adaptation mechanisms, first in order to structure, correlate and organize the vast amounts of incoming data and then to control its actions in order to achieve increasingly complex goals. Less well known is the fact that animal bodies are constructed in such a way that their movements require as little energy as possible and can mostly be controlled by relatively simple periodic central pattern generators (CPGs - <xref ref-type="bibr" rid="pone.0086696-Arena1">[4]</xref>, <xref ref-type="bibr" rid="pone.0086696-Ijspeert1">[5]</xref>). This is being exploited in a recent trend in robotics called morphological computing <xref ref-type="bibr" rid="pone.0086696-Paul1">[6]</xref>–<xref ref-type="bibr" rid="pone.0086696-Pfeifer2">[8]</xref>, in which robot designers focus on the design of robots rather than their control. The central claim of this line of research is that a large part of the control complexity can be internalized into the robot's morphology by clever design.</p>
<p>Many efforts to solve such problems have been made in the past. One often used design strategy, inspired by the evolution that led to biological systems, is to use metaheuristic optimization. This term refers to algorithms that treat the system as a black box, only sample the solution space and use some heuristic search algorithm to maximize an associated fitness function, e.g., evolutionary techniques. One issue with this approach is that truly large-scale systems with thousands of parameters offer a too large search space, and the time needed for optimizing grows prohibitively large.</p>
<p>In this paper we introduce a design methodology that allows for a more efficient design of robust physical systems. Our approach is applicable whenever an approximate parametric model of the system's dynamics exists and sufficient examples of the desired dynamical behavior are available. Essentially, we revert to machine learning algorithms, which have proven their merit in creating remarkably powerful systems, and apply them to physical dynamic systems, operating in continuous time. In particular we extend the gradient descent training algorithms known as <italic>Real-Time Recurrent Learning</italic> (RTRL), and <italic>Backpropagation through time</italic> (BPTT), respectively. These are commonly used for training recurrent neural networks (RNNs), which are discrete-time dynamical systems. Historically, BPTT was introduced first <xref ref-type="bibr" rid="pone.0086696-Mozer1">[9]</xref>–<xref ref-type="bibr" rid="pone.0086696-Werbos1">[12]</xref>, and it was developed by eliminating the time-aspect of recurrent neural networks and considering them a special form of multi-layered perceptron. RTRL was introduced later <xref ref-type="bibr" rid="pone.0086696-Williams1">[13]</xref> as an online alternative to BPTT. BPTT has proven to be a highly successful method for training recurrent networks, leading to remarkably complex and powerful systems <xref ref-type="bibr" rid="pone.0086696-Sutskever1">[14]</xref>, <xref ref-type="bibr" rid="pone.0086696-Graves1">[15]</xref>, often with several millions of parameters. As we show, our extensions of BPTT and RTRL are capable of taking into account and exploiting the long-term dynamic effects of the systems under consideration.</p>
<p>The resulting equations that describe how the gradients w.r.t. the parameter values are computed are identical to equations that are used within numerical optimal control, more specifically the computation of the costates of the system, which stem from the Pontyagrin maximum principle <xref ref-type="bibr" rid="pone.0086696-Boltyanskii1">[16]</xref>.</p>
<p>Optimal control primarily deals with a different problem: given a certain dynamic system, how can we create an input (or control) signal for this system in order to get it to operate in a certain way. This problem has a wide variety of applications in chemical plants, economy, robotics, spaceflight, etc. one example is the minimization of fuel expenditure in a rocket leaving the earth's atmosphere <xref ref-type="bibr" rid="pone.0086696-Vinh1">[17]</xref>. Even though the mathematical formalism also allows for the optimization of system parameters, not much work actually considers optimal control as a useful tool for system design, and much more commonly systems are designed first, and later optimal control algorithms are applied to control them. Machine learning starts from an information processing perspective. Here, the input signals of the system need to be processed (filtered, classified,…). Samples of desired input-output behavior are provided, and the <italic>system parameters</italic> (usually static values) need to be optimized in order to optimally approximate this desired behavior.</p>
<p>The design problems presented in this paper lie within the overlapping area of machine learning and optimal control: optimizing the <italic>design</italic> rather than control, but working with physical dynamical systems. We provide a set of three examples of which one leans to control theory (locomotion), one to optimal design (magnetic beam focusing), and one to machine learning (optimizing a photonic chip using examples of input/output signals).</p>
</sec><sec id="s2">
<title>Results</title>
<p>In this section, we first provide the equations needed for obtaining gradients in continuous time dynamical systems, and we explain how they can be used in an online and offline optimization fashion. Second, we illustrate the applicability of our approach by optimizing three different dynamical systems.</p>
<sec id="s2a">
<title>Gradient Descent</title>
<p>Here we briefly present the main mathematical results of which the proof can be found in <xref ref-type="supplementary-material" rid="pone.0086696.s005">derivation S1</xref>. We formally extend BPTT to continuous time for a number of types of dynamical systems. Continuous time variants of BPTT have been considered before (e.g., <xref ref-type="bibr" rid="pone.0086696-Pearlmutter1">[18]</xref>), but these derivations focus only on neural network-like systems and start from an Euler approximation of differential equations. We derive continuous time BPTT directly, without the need for approximations, and in a generic form which is applicable to a much wider variety of dynamical systems.</p>
<sec id="s2a1">
<title>Instantaneous gradient</title>
<p>We consider continuous-time dynamical systems characterized by a state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e001" xlink:type="simple"/></inline-formula> and a set of parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e002" xlink:type="simple"/></inline-formula>, that is excited by an external input signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e003" xlink:type="simple"/></inline-formula>. For brevity, the following summary focuses on ordinary differential equations (ODEs), i.e.:<disp-formula id="pone.0086696.e004"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0086696.e004" xlink:type="simple"/><label>(1)</label></disp-formula></p>
<p>The derivations in the supplementary material are also given for the cases of delayed differential equations and for (delay) differential algebraic equations (one of our example systems, the photonic network, is described by a delayed differential algebraic equation).</p>
<p>Suppose that, at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e005" xlink:type="simple"/></inline-formula>, we wish to minimize the cost function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e006" xlink:type="simple"/></inline-formula>. The gradient of this cost function w.r.t. the parameters, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e007" xlink:type="simple"/></inline-formula>, provides the direction in which the system parameters need to be changed to decrease the cost function at each point in time. It is given by<disp-formula id="pone.0086696.e008"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0086696.e008" xlink:type="simple"/></disp-formula><disp-formula id="pone.0086696.e009"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0086696.e009" xlink:type="simple"/></disp-formula><disp-formula id="pone.0086696.e010"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0086696.e010" xlink:type="simple"/></disp-formula></p>
<p>Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e011" xlink:type="simple"/></inline-formula> is the output error, i.e. the gradient of the cost function w.r.t. the system state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e012" xlink:type="simple"/></inline-formula>), and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e013" xlink:type="simple"/></inline-formula> is the total derivative of the state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e014" xlink:type="simple"/></inline-formula> w.r.t. the parameters. For notational reasons we also introduce <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e015" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e016" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e017" xlink:type="simple"/></inline-formula>, the partial derivatives of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e018" xlink:type="simple"/></inline-formula> w.r.t. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e019" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e020" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e021" xlink:type="simple"/></inline-formula>, respectively. If we take the derivative of <xref ref-type="disp-formula" rid="pone.0086696.e004">equation 1</xref> w.r.t. the parameters, we can write:<disp-formula id="pone.0086696.e022"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0086696.e022" xlink:type="simple"/><label>(2)</label></disp-formula>Which is a differential equation that defines the evolution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e023" xlink:type="simple"/></inline-formula>, provided that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e024" xlink:type="simple"/></inline-formula> is an invertible matrix. This is for instance the case for explicit ODEs, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e025" xlink:type="simple"/></inline-formula> is the identity matrix. As <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e026" xlink:type="simple"/></inline-formula> evolves according to an ODE, it can be computed in parallel with the system itself, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e027" xlink:type="simple"/></inline-formula> can also be updated continuously. This is known as <italic>online learning</italic>, as the system optimization happens while the simulation runs.</p>
<p>One downside of the above approach is that the matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e028" xlink:type="simple"/></inline-formula> can be very large, as its number of elements equals the product of the number of state variables and the number of parameters in the system. Even for modestly sized dynamical systems, this number can grow into the several thousands, leading to a high computational cost.</p>
</sec><sec id="s2a2">
<title>Gradient time integral</title>
<p>One approach to drastically reducing the computational cost is called batch learning. Instead of continuously updating the parameters, one now considers a finite time interval of duration <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e029" xlink:type="simple"/></inline-formula> and defines the total cost as the time integral of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e030" xlink:type="simple"/></inline-formula> over this interval. We obtain the following expression for the gradient w.r.t. the parameters.<disp-formula id="pone.0086696.e031"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0086696.e031" xlink:type="simple"/><label>(3)</label></disp-formula></p>
<p>In this case, it is possible to avoid explicitly computing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e032" xlink:type="simple"/></inline-formula> (see <xref ref-type="supplementary-material" rid="pone.0086696.s005">derivation S1</xref>). Instead, we need to solve a second system of differential equations that expresses the evolution of the error <italic>backwards in time</italic>:<disp-formula id="pone.0086696.e033"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0086696.e033" xlink:type="simple"/><label>(4)</label></disp-formula>in which <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e034" xlink:type="simple"/></inline-formula>. The gradient can then be replaced by</p>
<p><disp-formula id="pone.0086696.e035"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0086696.e035" xlink:type="simple"/><label>(5)</label></disp-formula>This expression has two important advantages over the previous one. First of all, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e036" xlink:type="simple"/></inline-formula> has the same dimensionality as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e037" xlink:type="simple"/></inline-formula>, and hence the cost of computing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e038" xlink:type="simple"/></inline-formula> is roughly the same as that of solving the system itself. The second advantage is that for many systems, the evolution of individual state variables only depends on a small fraction of the parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e039" xlink:type="simple"/></inline-formula>. This is particularly the case in systems associated with sparse networks of interacting parts. As a result, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e040" xlink:type="simple"/></inline-formula> is often a very sparse matrix, and the multiplication in the integral can be solved efficiently. One important downside of this approach is the fact that it requires storing the time traces of the system state over the full interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e041" xlink:type="simple"/></inline-formula>. In general, it is advisable to keep the batches short. However, in some cases long batches are required, which may cause memory problems.</p>
<p>The above equations for computing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e042" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e043" xlink:type="simple"/></inline-formula> are the continuous-time equivalents of the machine learning techniques known as <italic>Real-Time Recurrent Learning</italic> (RTRL), and <italic>Backpropagation through time</italic> (BPTT), respectively. As stated in the introduction, equations ?? and 5 and the associated differential equations for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e044" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e045" xlink:type="simple"/></inline-formula> also appear in the theory of optimal control (for a comprehensive overview, see, e.g., <xref ref-type="bibr" rid="pone.0086696-Bertsekas1">[19]</xref>), in particular in modern, large scale applications which can only be solved numerically (e.g., as presented on the second SADCO industrial workshop 2012). In order to obtain good control solutions, a common approach is to write the input signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e046" xlink:type="simple"/></inline-formula> as a function of a finite set of parameters (often by means of interpolation or splines), and to optimize this discrete set of parameters in a way that is largely equivalent to the previously presented method to compute the gradient w.r.t. the system parameters. Here too, in order to find this gradient, additional differential equations need to be solved (known as costates), and they are essentially identical to those for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e047" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e048" xlink:type="simple"/></inline-formula> defined above. This intimate connection between backpropagation through time and optimal control has been described before <xref ref-type="bibr" rid="pone.0086696-LeCun1">[20]</xref>, but has received little attention in further research.</p>
</sec></sec><sec id="s2b">
<title>Numerical Simulations</title>
<p>We demonstrate our approach by embedding certain desired behaviors into simplified models of three different physical dynamical systems. Ordered according to increasing system complexity, they are: two-dimensional mass-spring-damper (MSD) networks, a beam of charged particles influenced by magnetic fields, and a photonic network of optical amplifiers. All details concerning the physical models and optimization methods can be found in the materials and methods. The presented optimization strategy can readily be used on more complicated and realistic simulation models, such as those used in the emerging field of simulation-based engineering <xref ref-type="bibr" rid="pone.0086696-Oden1">[21]</xref>.</p>
<sec id="s2b1">
<title>Mass-spring-damper systems</title>
<p>We optimize MSD networks consisting of point masses connected with massless linear springs and dampers. The state of this system consist of the positions and velocities of the point masses. Possible optimisation parameters in this system are the rest lengths of the springs, the spring constants and the spring damping constants, as well as any parameters of possible control signals driving the springs.</p>
<p>First, we embed a specific trajectory: when the network evolves dynamically from a predefined initial condition, one of the nodes has to trace a pentagram shape. In order to achieve this, no external control signal is applied and we only optimize the rest lengths and spring constants of the springs. The optimized MSD network tracks the pentagram with nearly perfect accuracy, demonstrating BPTT's ability to find a solution for such tasks. The simplicity of this task (no noise; a single, well defined objective function) allows for a direct comparison with the so-called <italic>Covariance Matrix Adaptation Evolution Strategy</italic> (CMA-ES) <xref ref-type="bibr" rid="pone.0086696-Hansen1">[22]</xref>, one of the most widely used evolutionary algorithms. The shape of the setup and the resulting trajectory are shown in <xref ref-type="fig" rid="pone-0086696-g001">Fig. 1a and 1b</xref>, respectively. <xref ref-type="fig" rid="pone-0086696-g001">Fig. 1c</xref> compares the convergence speeds of CMA-ES and gradient descent for this task, and shows that gradient descent converges substantially faster. This is to be expected, as CMA-ES makes an estimate of the gradient from multiple samples, whereas we compute it directly. This particular problem only has 160 parameters, which is still feasible for CMA-ES. As dimensionality increases, evolutionary algorithms will face increasing difficulty, as the cost of sampling the search space increases exponentially.</p>
<fig id="pone-0086696-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0086696.g001</object-id><label>Figure 1</label><caption>
<title>Embedding a trajectory in an MSD-network.</title>
<p><bold>A:</bold> Depiction of the initial condition of the MSD-system. The grey circles have fixed positions, the black circles are point masses that are non-fixed, and the red circle is the point mass of which we wish to control the trajectory. The connecting lines represent massless linear spring-dampers. <bold>B:</bold> Illustration of the trajectory of the selected node after optimisation. The full blue line is the actual trajectory, (including the trajectory after completing the pentagram) and the dashed line is the target. <bold>C:</bold> Comparison of the convergence speed of gradient descent and CMA-ES for the same initial parameter set.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0086696.g001" position="float" xlink:type="simple"/></fig>
<p>Second, we optimize an MSD model for locomotion, a problem which has often been studied in the context of evolutionary algorithms <xref ref-type="bibr" rid="pone.0086696-Paul1">[6]</xref>, <xref ref-type="bibr" rid="pone.0086696-Lipson1">[23]</xref> and matches well with the concept of morphological computation <xref ref-type="bibr" rid="pone.0086696-Paul1">[6]</xref>–<xref ref-type="bibr" rid="pone.0086696-Pfeifer2">[8]</xref>. Our ‘robots’ are MSD-networks of which the spring resting lengths are modulated periodically. They exist in a 2D environment with gravity and a ground contact model. Initially, the robot is a worm-shaped set of springs of which each spring's rest length is periodically modulated with a random phase and amplitude. In this application, we optimize both the robot's shape (the spring rest lengths) and its control (the phases and amplitudes of the modulation). As the number of parameters for this problem is still manageable (162 in total), we use the online learning approach, which allows us to gradually optimize the robot while the simulation runs. The cost function consists of two contributions. The first is simply the squared difference between the average horizontal speed and a target value. The second is the sum of squares of the rest lengths and amplitudes in the cost function. This is added to avoid the trivial solution of ever increasing the modulation amplitudes and the rest lengths of the springs (which leads to larger contractions and a greater speed). Note that this second cost term is equivalent to the ubiquitous L2 regularization strategy in machine learning, where it is used to avoid extreme parameter sensitivity, and hence overfitting. We show a schematic depiction of the initial robot and an example of a trained robot in <xref ref-type="fig" rid="pone-0086696-g002">Fig. 2</xref>.</p>
<fig id="pone-0086696-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0086696.g002</object-id><label>Figure 2</label><caption>
<title>MSD robots.</title>
<p>We have represented the springs that constitute the MSD robots with lines, where we visualized their modulation amplitude by making springs with large modulation amplitudes thicker and redder. The blue line represents the ground. <bold>A:</bold> Shape of the robot at the initialization of training. All springs have an equally large modulation amplitude. <bold>B:</bold> Snapshot of an example robot after finishing training. Only 4 of its springs still have a non-negligible modulation amplitude, and they provide virtually all locomotive power.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0086696.g002" position="float" xlink:type="simple"/></fig>
<p>Videos of an initial (unoptimized) MSD robot (<xref ref-type="supplementary-material" rid="pone.0086696.s001">movie S1</xref>) and of some resulting optimized locomotions (<xref ref-type="supplementary-material" rid="pone.0086696.s002">movies S2</xref>–<xref ref-type="supplementary-material" rid="pone.0086696.s004">S4</xref>) are available in the supplementary material. The robots develop highly dynamic gaits in which usually only the front and rear extremities touch the ground. Even though the initial shape of the robot is identical for all experiments, the initial parameters that determine the control (in particular the phase of the periodic modulation), are chosen randomly at the beginning of each simulation. These small differences do lead to strongly differing final robot morphologies and gates, indicating that this problem has a high number of local optima.</p>
<p>Interestingly, when examining the parameter values for the modulation amplitudes, it appears that, due to the imposed restrictions on size and strength, the robots tend to end up with only a few springs with a large modulation amplitude, providing the bulk of the locomotive power, whereas the other springs exhibit small to virtually zero amplitudes. This poses an interesting possibility: when one would actually design and build a physical robot, it would be desirable to have as little actuated parts as possible. One could then use L1 regularization <xref ref-type="bibr" rid="pone.0086696-Tibshirani1">[24]</xref>, which leads to a sparse solution in which a large part of the amplitudes are zero (which would greatly simplify the eventual robot construction).</p>
</sec><sec id="s2b2">
<title>Magnetic focusing</title>
<p>The MSD networks from the previous section are highly simplified, and cannot directly be constructed physically without taking into account a range of more realistic effects such as nonlinear springs, contacts and collisions. In the next example we have trained a spatial configuration of magnets to focus a beam of charged particles, which is a more practically applicable physical design problem. This problem has a well-known solution, consisting of two ideal quadrupole fields placed behind each other at a 90<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e049" xlink:type="simple"/></inline-formula> angle <xref ref-type="bibr" rid="pone.0086696-Hawkes1">[25]</xref>. Producing an ideal quadrupole field, however, requires a precisely manufactured geometry of magnetic cores. We use a discrete set of 200 point dipole magnets (leading to 1,200 trainable parameters: all magnet positions and orientations, having 3 coordinates each), which cannot produce such a field exactly. As such, gradient descent needs to find an approximate solution to the problem. In order to use the presented framework, we simulate an incoming ‘beam’ of particles (a discrete number of them), and use their positions and velocities as the state of the dynamic system. The resulting magnet configuration, beam, and distribution of particles crossing the focal plane are shown in <xref ref-type="fig" rid="pone-0086696-g003">Fig. 3</xref>, as well as a set of cross-sections of the beam within the lens, showing the lateral magnetic field lines within. The configuration manages to focus the beam with slightly better focus than the quadrupole set we compare against. Interestingly, the shape of the beam and the cross-sections show that the magnet configuration has found a solution that is qualitatively similar to that of an ideal quadrupole lens, in which the beam is first focused in one direction and then in the other.</p>
<fig id="pone-0086696-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0086696.g003</object-id><label>Figure 3</label><caption>
<title>Illustration of the magnetic beam focuser.</title>
<p><bold>A:</bold> Particle distribution in a cross-section of the focal plane, shown for 50,000 particles, both for a set of two ideal quadrupoles and our configuration (CT-RTRL results). The cost function, the root mean squared distance (RMSD) of the particles w.r.t. the focal point as they pass through the focal plane, is shown underneath the panels. On the right we show the scale compared to the original beam width (red circle). Note that the spread of the particles for the quadrupoles is largely due to a relatively large spread in particle velocities. <bold>B:</bold> Illustration of the particle beam envelope (light green) and the spatial configuration of magnets (blue red cones), indicating position, direction and magnitude. <bold>C:</bold> Cut-through illustrations of the particle beam envelope and the lateral magnetic field at different positions throughout the beam focuser.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0086696.g003" position="float" xlink:type="simple"/></fig></sec><sec id="s2b3">
<title>Photonic networks</title>
<p>The third dynamical system we consider is an integrated network of semiconductor optical amplifiers (SOAs). Nonlinear photonic networks have been considered as promising candidates for information processing <xref ref-type="bibr" rid="pone.0086696-Paquot1">[26]</xref>–<xref ref-type="bibr" rid="pone.0086696-Brunner1">[29]</xref>. Essentially, the dynamics of networks of photonic components can show parallels with those of recurrent neural networks, making them an interesting platform for integrating high-bandwidth neural-network-like systems in physical hardware. SOA networks like the ones in <xref ref-type="bibr" rid="pone.0086696-Vandoorne1">[27]</xref>, <xref ref-type="bibr" rid="pone.0086696-Vandoorne2">[30]</xref> are an interesting example of our technique because there exist non-negligible interconnection delays between the different amplifiers. Due to the finite speed of light and the fact that the internal dynamics of the SOAs are extremely fast, these need to be taken into account explicitly, and influence the system dynamics in a meaningful fashion. The model that describes this SOA network is a delayed differential algebraic equation.</p>
<p>In this example, we show that our approach can handle uncertainty due to manufacturing variations and noise, yielding robust and manufacturable designs. We use gradient descent to optimize the parameters of a 4<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e050" xlink:type="simple"/></inline-formula>4 network of SOAs, interconnected with optical waveguides, inspired by earlier work <xref ref-type="bibr" rid="pone.0086696-Vandoorne2">[30]</xref>. The dynamical model we use for the individual SOAs has been shown to be an excellent approximation of reality <xref ref-type="bibr" rid="pone.0086696-Fiers1">[31]</xref>. The optimisation parameters for this system are the bias currents of the SOAs the losses and phase changes of all input and inter-SOA connections, as well as the delays of the inter-SOA connections. The desired output is realised by linearly combining a fraction of the light coming out of the SOAs and converting this to the electrical domain using a photodetector. Hence, the losses and phase changes in the readout connections are also optimised. On-chip photonic interconnections are etched from a silicon substrate with a finite resolution. This causes small variations on the exact length of the connections, and hence the phase of the light arriving at each SOA. In addition, each SOA produces a certain amount of noise in the form of amplified stimulated emission. Again, we need to include this noise in the optimization algorithm in order to obtain a robust solution. Both phase variability and noise were included in our optimizations.</p>
<p>We have optimized the network twice, once to behave like a photonic D flip-flop and once to realise a 5-bit delayed one-hot detector (a device which should produce an output spike when exactly one of the past five bits of the input stream was equal to one). In order to train the networks we use input/output example time traces, and an associated cost. In order to obtain robust designs, we took care to provide enough and sufficiently diverse training examples. In particular, the D flip-flop was not trained with a periodic clock to avoid solutions that internalize the clock period in the internal delays and do not work properly for other clock periods. The one-hot detector was trained to operate at a single clock frequency, but it did not receive a clock input, which means that it had to extract the clock phase itself. Due to the fact that we optimize this system by randomly sampling input/output examples, we essentially train this system using <italic>stochastic gradient descent</italic>, which is currently one of the most popular training methods in machine learning problems that involve large amounts of data <xref ref-type="bibr" rid="pone.0086696-Bottou1">[32]</xref>. A schematic depiction of the SOA network and example time traces for the trained networks are shown in <xref ref-type="fig" rid="pone-0086696-g004">Fig. 4</xref>, showing that despite substantial levels of noise and the included manufacturing variations, gradient descent is capable of training the networks to nearly perfect accuracy for both tasks (see supplementary methods for more detailed performance measures). Note that a photonic flip-flop can in fact be constructed with significantly fewer components <xref ref-type="bibr" rid="pone.0086696-Shinya1">[33]</xref>, <xref ref-type="bibr" rid="pone.0086696-Liu1">[34]</xref>. We use the example here to show that the presented method is capable to create a working solution automatically from nothing more than input/output examples and an associated cost function, and indeed that the SOA network is generic enough to embed several different behaviors within its parameters.</p>
<fig id="pone-0086696-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0086696.g004</object-id><label>Figure 4</label><caption>
<title>Results for optimised SOA networks.</title>
<p><bold>A:</bold> Schematic representation of the SOA-network. The black lines are photonic waveguides. The circles are the SOAs, with the arrows indicating in which direction the light passes through. Each SOA receives an external input signal, and at its output, a channel goes to the output (represented by the dotted lines). These channels are optimally combined optically and the output signal is the optical power of this combination. <bold>B:</bold> Illustration of the photonic D flip-flop. The top two time traces show the two input channels, the clock (set) signal and the data signal. The red time trace is the desired output power of the SOA network, and the blue one is a superposition of the measured output power for 10 different instances, each with different noise and internal phase variations. <bold>C:</bold> Illustration of the photonic 5-bit one-hot detector. The black time trace is the input signal, a random bit stream, and the red time trace is the desired output power of the SOA network. The blue one is a superposition of the measured output power for 10 different instances, each with different noise and internal phase variations.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0086696.g004" position="float" xlink:type="simple"/></fig></sec></sec></sec><sec id="s3">
<title>Discussion</title>
<p>In this work, we have shown that it is possible to optimize surprisingly complex dynamics within a range of physical systems, by extending backpropagation through time to continuous time. Our examples illustrate that energy-efficient, robust and manufacturable solutions can be achieved by applying some of the knowledge that has been built up in the machine learning community.</p>
<p>First we have considered mechanical MSD systems. Using an online variant of the BPTT algorithm, we were able to automatically find energy efficient solutions for locomotion in simulated MSD robots. Despite the fact that the obtained gaits are fast, natural and efficient, the control of the optimized robots is extremely simple (periodically modulated rest lengths of the involved springs), and the full motion emerges synergistically between the control parameters and the robot's shape. This shows that our extension of BPTT can be very useful in the field of morphological computation and embodiment <xref ref-type="bibr" rid="pone.0086696-Pfeifer1">[7]</xref>.</p>
<p>Second, we have configured a set of dipole magnets in order to focus a beam of charged particles, obtaining a solution that is in many ways equivalent to a known solution to this problem: a double quadrupole field. This example shows that BPTT can be useful in finding non-trivial solutions for problems that occur in designing electromagnetic devices, and perhaps even plasma physics.</p>
<p>Finally we have applied BPTT to a realistic example from the domain of photonics. We have optimized the internal parameters of a network of integrated SOAs and waveguides in order to make it perform two digital operations on input streams. In this case, the model for the system dynamics was more complex than in the previous systems, as it required delayed differential algebraic equations. In addition, to obtain robust and manufacturable solutions, we included realistic levels of parametric variations and system noise directly into the training process. Yet, despite the increased complexity of these examples, our approach has succeeded in automatically finding highly performant and robust solutions for both tasks.</p>
<p>In all these instances it is clear that BPTT is able to truly exploit the dynamic part of the system, and automatically link events that are separated in time. For instance in the case of the magnetic beam focuser: information of the objective function is only available when particles reach the focal plain, yet at that point in time, their interaction with the magnets has already happened. Due to the way BPTT takes into account the state history, it still provides a solution to the given problem. Similar in the case of the photonic flip flop: the on/off state is remembered indefinitely long, and that means that internally, the network has produced two stable point attractors between which it can toggle only when a clock pulse arrives. This indicates that BPTT can manipulate the dynamics of the system in a profound way.</p>
<p>The applicability of gradient descent to real-world dynamical systems greatly depends on the accuracy of existing models and on the understanding of associated stochastic phenomena such as noise and process variation. In the fields of electronics and photonics, highly accurate models exist, but variability can be considerable. Other domains, such as, e.g., robotics, are known to suffer from a lack of accurate models. Applying BPTT for such problems poses a challenge. Conceivably, in some cases even an approximate model could provide a useful gradient and help to identify approximate parameter values which can subsequently be fine-tuned using more accurate simulations. When an analytical model is not available, an approximate model can be built from measurements or simulations, e.g. using self-modeling approaches <xref ref-type="bibr" rid="pone.0086696-Bongard1">[35]</xref>. Further research will need to be conducted to find if such a strategy is feasible or not.</p>
<p>The ability to embed specified dynamic behaviors into a generic physical platform opens a broad range of possible applications. For example, it may provide a large jump in research concerned with bringing computation outside the silicon domain. Designing physical devices that robustly and efficiently perform non-digital computations now become feasible. The proposed methodology can also be used by engineers to design machines and robots that exploit their inherent non-linear behavior in ways that were previously too difficult to explore. It could even be extended to embed non-trivial dynamical behaviors into passive or active continuous media which are characterized by partial differential equations, since the mathematics for controlling such systems already exists <xref ref-type="bibr" rid="pone.0086696-Becker1">[36]</xref>. This could lead to e.g., systems that facilitate the task of sensors, chemical controllers based on reaction-diffusion systems <xref ref-type="bibr" rid="pone.0086696-Dale1">[37]</xref>. Our technique could also help to gain more insight into the cost functions that were optimized by nature in complex biological systems by emulating them.</p>
<p>One important contribution of this work is that it fades the boundaries between several disciplines and provides part of a roadmap towards integrating optimal design, machine learning, and optimal control into one discipline. Researchers working on designing complex physical systems would not easily consider machine learning as a potential optimization strategy. On the other hand, the machine learning community generally considers the computer the only platform on which to implement their models. Paul Werbos, who is often considered as the originator of the backpropagation algorithm <xref ref-type="bibr" rid="pone.0086696-Werbos1">[12]</xref>, <xref ref-type="bibr" rid="pone.0086696-Werbos2">[38]</xref> through his framework of <italic>ordered derivatives</italic>, has already pleaded for a better cooperation between scientists and engineers in these fields and others (most noticeably <italic>automated differentiation</italic>) <xref ref-type="bibr" rid="pone.0086696-Werbos3">[39]</xref>. The main difficulty in the application of our technique lies in the derivation of the matrices <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e051" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e052" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e053" xlink:type="simple"/></inline-formula>, which can be tedious and needs to be done for each new system. The potential of such an interdisciplinary collaboration lies in the creation of machine-learning based automated design tools for generic dynamical systems, in which only the system equations and the examples need to be provided by the user. We hope that this paper can pave the way for the realization of this vision by combining the necessary mathematical and machine learning background and by providing convincing design examples.</p>
</sec><sec id="s4" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Notes on Gradient Descent</title>
<p>Dynamic systems can exhibit bifurcations, which are associated with very rapid changes in dynamics as a function of the parameters <xref ref-type="bibr" rid="pone.0086696-Doya1">[40]</xref>. This translates to extremely steep parts in the cost function and as a result, very large gradients. Simply updating the system using this gradient will lead to a very large and unpredictable change of the parameters, and may in fact break down the training process altogether. In order to deal with this, we normalize the gradient before using it for parameter updates, essentially only the direction of the gradient and not its magnitude.</p>
<p>We use two strategies: online (CT-RTRL) and batched training (CT-BPTT). In the online case, parameter updates happen continuously. In other words, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e054" xlink:type="simple"/></inline-formula> will depend on time, and evolves according to:<disp-formula id="pone.0086696.e055"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0086696.e055" xlink:type="simple"/></disp-formula>Here we need to make sure that the time scale at which <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e056" xlink:type="simple"/></inline-formula> changes is much slower than the timescales of the DS, otherwise the training process will interfere with the actual dynamics of the system. In the batched training, parameters are updated offline in between discrete simulation instances of a fixed length. The update equation can be written as:<disp-formula id="pone.0086696.e057"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0086696.e057" xlink:type="simple"/></disp-formula></p>
<p>In all our experiments we use dimensionless units for simplicity. In the final example of photonic networks, however, we will use values that reflect realistic physical values of SOA parameters. Often, the numerical scaling of different kinds of parameters (e.g., spring constants vs. their rest lengths) may differ orders of magnitude. If this is the case, we will normalise their respective gradients separately and set separate learning rates.</p>
<p>In order to ensure convergence we let the learning rate decay over the course of the experiments (in either a linear or an exponential fashion).</p>
</sec><sec id="s4b">
<title>Notes on Implementation</title>
<p>All experiments shown in this paper were performed on a single laptop computer with 8 GB RAM and a 2.3 GHz Intel Core i7 processor. We used Matlab for our experiments and made the code for generating the results available on <ext-link ext-link-type="uri" xlink:href="http://users.elis.ugent.be/~mhermans/code.zip" xlink:type="simple">http://users.elis.ugent.be/~mhermans/code.zip</ext-link>.</p>
</sec><sec id="s4c">
<title>Experimental Details</title>
<sec id="s4c1">
<title>Embedding a trajectory in an MSD-system</title>
<p>The state of the system is made up of the mobile node positions and velocities. The force exerted on the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e058" xlink:type="simple"/></inline-formula> -th node, exhibited by a spring connecting the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e059" xlink:type="simple"/></inline-formula>-th and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e060" xlink:type="simple"/></inline-formula>-th node is equal to<disp-formula id="pone.0086696.e061"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0086696.e061" xlink:type="simple"/></disp-formula></p>
<p>Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e062" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e063" xlink:type="simple"/></inline-formula> represent the position and velocity of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e064" xlink:type="simple"/></inline-formula>-th node respectively. The parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e065" xlink:type="simple"/></inline-formula> is the spring constant for this particular spring, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e066" xlink:type="simple"/></inline-formula> is its damping constant, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e067" xlink:type="simple"/></inline-formula> its rest length. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e068" xlink:type="simple"/></inline-formula> is the euclidean distance between the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e069" xlink:type="simple"/></inline-formula>-th and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e070" xlink:type="simple"/></inline-formula>-th node.</p>
<p>At <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e071" xlink:type="simple"/></inline-formula>, the springs are allowed to relax, and the centre mass will follow a certain trajectory that is determined by the system parameters. We have optimized the rest lengths and spring constants (where we made sure these could not grow smaller than zero by truncating their values).</p>
<p>The target trajectory has the shape of a pentagram. To avoid instantaneous changes in velocity on the corners of the star we made sure that the velocity of the desired trajectory goes to zero at the turning points. In practice: if one straight segment of the pentagram is traced over a time interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e072" xlink:type="simple"/></inline-formula> then its speed evolves as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e073" xlink:type="simple"/></inline-formula> Once we have constructed the target position as a function of time, we derive the accompanying velocity by taking its derivative, and use this velocity as an additional target for the central node state.</p>
<list list-type="bullet"><list-item>
<p><bold>Cost function</bold> The cost function is the sum of the mean square errors of the position and velocity of the target node.</p>
</list-item><list-item>
<p><bold>CMA-ES details</bold> The comparison with CMA-ES was made using a standardized implementation, available at <ext-link ext-link-type="uri" xlink:href="https://www.lri.fr/~hansen/cmaes_inmatlab.html" xlink:type="simple">https://www.lri.fr/~hansen/cmaes_inmatlab.html</ext-link>. The only parameters which need to be set by the user are the population size and the initial standard deviations of the parameters. Since we know that the relative scaling of spring constants vs. rest lengths is about a factor 25, we also set the initial values for parameter standard deviations accordingly (25 times greater for the spring constants than the rest lengths). We found that a tradeoff between good performance and speed of convergence was found with a quite small populations (20 individuals, small compared to the dimensionality of the problem), and small initial standard deviations (0.04 for the rest lengths and 1 for the spring constants). Using these parameters, we ran 5 experiments and chose the best end result to compare with gradient descent. Note that gradient descent has no stochastic element in this case, such that we only needed to run one experiment to obtain the result.</p>
</list-item><list-item>
<p><bold>Gradient descent details</bold> We used CT-BPTT with batches of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e074" xlink:type="simple"/></inline-formula>, the time needed to complete the target trajectory. We optimized the learning meta-parameters; the initial learning rate and the rate at which it decreases after each training iteration. We normalized the full parameter gradient (not separately for the two parameter sets), and chose an initial learning rate of 5 for the spring constants and 0.05 for the rest lengths. Each training iteration both learning speeds were multiplied with a factor 0.999.</p>
</list-item><list-item>
<p><bold>Implementation details</bold> We used leapfrog integration for the forward simulation and Euler integration for the error backpropagation. The step size was chosen at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e075" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p><bold>Initial parameters</bold> Initial spring constants were set to 25. Damping constants were all equal to 0.1, and the rest lengths were chosen as the distances between the nodes when the system is in its initial condition, but with the target mass in the centre.</p>
</list-item></list>
</sec><sec id="s4c2">
<title>MSD robots</title>
<p>For the locomotion experiment, each spring's instantaneous rest length is given by<disp-formula id="pone.0086696.e076"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0086696.e076" xlink:type="simple"/></disp-formula>in which <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e077" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e078" xlink:type="simple"/></inline-formula> are amplitude and phase respectively, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e079" xlink:type="simple"/></inline-formula> is the rest length without modulation. The exponential function assures that the modulation signal cannot become negative, yet reach high peak values if desired. We use a highly simplified model for the ground, with an upward force <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e080" xlink:type="simple"/></inline-formula>, such that it is nearly zero above ground (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e081" xlink:type="simple"/></inline-formula>), but increases very rapidly below ground. Ground friction only acts in the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e082" xlink:type="simple"/></inline-formula>-direction, and is modeled as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e083" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e084" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e085" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e086" xlink:type="simple"/></inline-formula> As such, the harder a node is pressed down, the stronger lateral friction will be.</p>
<list list-type="bullet"><list-item>
<p><bold>Cost function</bold> The cost function is the mean square error between the target velocity and the mean robot velocity in the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e087" xlink:type="simple"/></inline-formula>-direction. Additionally, we add the sum of squares of the amplitudes and the rest lengths, scaled with 0.2 and 0.001, respectively. The target speed increases slowly over time as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e088" xlink:type="simple"/></inline-formula> such that the robot slowly speeds up during training.</p>
</list-item><list-item>
<p><bold>Training method</bold> We used online training, where the initial learning rates for amplitudes, phases, and rest lengths are 0.05, 0.2, and 0.2, respectively, and the corresponding gradients are normalized separately. Due to the constantly changing target velocity, learning rates are kept constant over time.</p>
</list-item><list-item>
<p><bold>Implementation details</bold> We used leapfrog integration for the forward simulation and Euler integration for updating <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e089" xlink:type="simple"/></inline-formula>. The step size was chosen at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e090" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p><bold>Initial parameters</bold> Spring constants were all equal to 100. Damping constants were all equal to 1, and the rest lengths were chosen as the distances between the nodes when the system is in its initial shape. Amplitudes were initialised at 0.2, and all phases were picked randomly between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e091" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e092" xlink:type="simple"/></inline-formula></p>
</list-item></list>
</sec><sec id="s4c3">
<title>Magnetic lens</title>
<p>In order to simulate the beam, we used a set of 200 discrete, non-interacting particles passing through the lens. All particles are initialized on a uniformly sampled position in a circle with radius one, (the source), and their initial velocity is always aligned with the beam, and has mean value 4 and standard deviation 0.1. If a particle crosses the focal plane, or its distance to the beam axis is greater than 2, it is reinitialized at the beam source (and its corresponding <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e093" xlink:type="simple"/></inline-formula> is reset to zero). This way, the beam remains constantly present during the training phase.</p>
<p>The beam is lying along the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e094" xlink:type="simple"/></inline-formula>-axis, the source is at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e095" xlink:type="simple"/></inline-formula>, and the focal point at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e096" xlink:type="simple"/></inline-formula>. All magnets positions are bound such that their distance to the beam axis (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e097" xlink:type="simple"/></inline-formula>-axis) is no smaller than 2, and their <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e098" xlink:type="simple"/></inline-formula>-coordinates lie between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e099" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e100" xlink:type="simple"/></inline-formula>. Their magnetic moment <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e101" xlink:type="simple"/></inline-formula> has a magnitude capped at 10, but in practice none even come close to this bound.</p>
<p>Each particle has unit charge and mass, and feels the magnetic Lorentz force: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e102" xlink:type="simple"/></inline-formula>. The magnetic field at each location is the sum of the fields of each magnetic dipole. A single magnetic dipole with magnetic moment <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e103" xlink:type="simple"/></inline-formula>, located at the origin, has a magnetic field given by<disp-formula id="pone.0086696.e104"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0086696.e104" xlink:type="simple"/></disp-formula>where we omitted the scaling factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e105" xlink:type="simple"/></inline-formula></p>
<p>We used 200 dipole magnets, leading to a total of 1200 parameters (3 coordinates for magnet moment and 3 for position).</p>
<list list-type="bullet"><list-item>
<p><bold>Cost function</bold> The cost function is equal to the mean square error between the particles velocity and that pointed towards the focal point. The magnitude of the target velocity is that equal to the initial particle's velocity (static magnetic fields cannot change the velocity of a charged particle, only its direction.) The resulting output error <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e106" xlink:type="simple"/></inline-formula> is next scaled for each particle with a factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e107" xlink:type="simple"/></inline-formula>, such that only particles close to the focal point actively contribute to the overall cost.</p>
</list-item><list-item>
<p><bold>Quadrupole lens</bold> In order to make the comparison we implemented a simulation in which two ideal quadrupole fields were placed at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e108" xlink:type="simple"/></inline-formula> and at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e109" xlink:type="simple"/></inline-formula> In these regions the magnetic field components are given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e110" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e111" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e112" xlink:type="simple"/></inline-formula> in which <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e113" xlink:type="simple"/></inline-formula> for the two fields. Everywhere else the magnetic field was equal to zero. Parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e114" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e115" xlink:type="simple"/></inline-formula> were optimised using the mean square distance to the focal point when the particles cross the focal plane. We used a brute force search for optimisation.</p>
</list-item><list-item>
<p><bold>Training method</bold> Training is performed online. We used a learning rate that linearly decays over the course of the experiment, which runs for a time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e116" xlink:type="simple"/></inline-formula>, with an initial value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e117" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p><bold>Implementation details</bold> We used leapfrog integration for the forward simulation and Euler integration for updating <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e118" xlink:type="simple"/></inline-formula>. The step size was chosen at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e119" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p><bold>Initial parameters</bold> All initial dipole <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e120" xlink:type="simple"/></inline-formula>-coordinates were uniformly sampled between the bounds described above. Their distances to the beam axis were chosen between 2 and 4, and their angle w.r.t. the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e121" xlink:type="simple"/></inline-formula>-axis randomly sampled between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e122" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e123" xlink:type="simple"/></inline-formula>. Magnetic moment coordinates were chosen from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e124" xlink:type="simple"/></inline-formula></p>
</list-item></list>
</sec><sec id="s4c4">
<title>Photonic SOA networks</title>
<p>Each SOA has an internal state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e125" xlink:type="simple"/></inline-formula> which describes the SOA gain. It evolves according to<disp-formula id="pone.0086696.e126"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0086696.e126" xlink:type="simple"/></disp-formula></p>
<p>Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e127" xlink:type="simple"/></inline-formula> is the free carrier lifetime, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e128" xlink:type="simple"/></inline-formula> is the rest value for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e129" xlink:type="simple"/></inline-formula>, which is determined by an external bias current, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e130" xlink:type="simple"/></inline-formula> is the complex field at the SOA input side and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e131" xlink:type="simple"/></inline-formula> is the saturation power of the SOA. We chose <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e132" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e133" xlink:type="simple"/></inline-formula>. The field that exits the SOA is described by<disp-formula id="pone.0086696.e134"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0086696.e134" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e135" xlink:type="simple"/></inline-formula> is a factor corresponding to internal losses (which we chose fixed at 0.5), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e136" xlink:type="simple"/></inline-formula> is the imaginary unit and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e137" xlink:type="simple"/></inline-formula> a constant depending on the SOA which we chose at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e138" xlink:type="simple"/></inline-formula>. For further details and the derivation of these formulas we refer to <xref ref-type="bibr" rid="pone.0086696-Agrawal1">[41]</xref>.</p>
<p>The incoming field for a given SOA within the network is given by<disp-formula id="pone.0086696.e139"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0086696.e139" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e140" xlink:type="simple"/></inline-formula> is the complex weight associated with a single connection (described in more detail further in this section), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e141" xlink:type="simple"/></inline-formula> are the complex fields of all other SOAs and input channels that connect to this SOA, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e142" xlink:type="simple"/></inline-formula> are the associated connection delays. We optimised the following parameters: the complex weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e143" xlink:type="simple"/></inline-formula> of each SOA, the delays <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e144" xlink:type="simple"/></inline-formula>, and the bias current <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e145" xlink:type="simple"/></inline-formula> for each SOA, leading to a total of 152 parameters for the one-hot detector, and 184 for the flip-flop (counting the real and imaginary parts of the complex weights as separate parameters).</p>
<p>The delayed 5-bit one-hot task input consisted of a random bitstream with period <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e146" xlink:type="simple"/></inline-formula>. The flip-flop input existed of one random bitstream (the data), with period <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e147" xlink:type="simple"/></inline-formula>, and a clock input, where at each time a new data bit enters the network, there's a one in ten chance of a clock pulse. The clock pulse has length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e148" xlink:type="simple"/></inline-formula>.</p>
<p>Each interconnection will introduce a fixed phase shift and a decrease in amplitude, which can be combined to a single complex weight with which the complex amplitude is multiplied. Additionally, each interconnection will have a certain delay value. If we assume that at the output side of each SOA half the power goes to the output connection, and the remaining fraction is split in two as it connects to at most two other SOAs in our particular network architecture, we can state that only at most a quarter of the output power from one SOA reaches another SOA, which means that the moduli of the complex weights are truncated at 0.5. The values of the delays were bounded between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e149" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e150" xlink:type="simple"/></inline-formula> for the one-hot task, and between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e151" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e152" xlink:type="simple"/></inline-formula> for the flip flop task. Note that in principle the delay and the phase shift of a connection are codetermined by the physical length of the connection. In practice, however, the wavelength is much shorter than an interconnection, such that even a tiny shift in length will cause a very large phase shift. Therefore we consider these two parameters as independent.</p>
<p>The precision at which the phase shift of an interconnection is manufactured is determined by the precision at which connection lengths can be made, and hence also dependent on the wavelength of the light. In order to model these variations, each training iteration we perturb the phases of all interconnections by adding them with a random phase, sampled from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e153" xlink:type="simple"/></inline-formula> (based on manufacturing precision with standard deviation of 10 nm and an optical wavelength of 1550 nm), and use the according weights to compute the gradients on. This assures that the found solution is not extremely sensitive to small variations in phase.</p>
<p>Finally, we superpose noise on each SOA's output field, modeled as a mixture of frequencies near that of the signal frequency based on <xref ref-type="bibr" rid="pone.0086696-Olsson1">[42]</xref>. The noise has the following time dependence:<disp-formula id="pone.0086696.e154"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0086696.e154" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e155" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e156" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e157" xlink:type="simple"/></inline-formula> are values which are sampled randomly at each training iteration between from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e158" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e159" xlink:type="simple"/></inline-formula>, respectively. This noise takes on the form of a quickly fluctuating signal with a low amplitude (its power being roughly 50–100 times lower than the average output power of the SOAs after training). Each SOA node receives both network feedback and an external input signal, which consists of a weighted sum of input data channels and a constant bias signal. The corresponding connections we have modeled to have zero delay. The output light signal is constructed by combining the output light of each SOA. Here again, each of these output connections has an associated complex weight, and we again assumed they have no delay.</p>
<list list-type="bullet"><list-item>
<p><bold>Cost function</bold> In the considered tasks we are only concerned with output power, not phase. Therefore the cost function was the mean square error between the desired and actual output power (which for both tasks vary between zero and one).</p>
</list-item><list-item>
<p><bold>Training method</bold> Training is performed in batches of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e160" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e161" xlink:type="simple"/></inline-formula> for the one-hot and flip-flop task, respectively. Each training example was made from a randomly sampled bitstream, such that optimization of the network is performed with stochastic gradient descent. The learning rate was started at 0.1 and reduced with a factor 0.9999 per training iteration. As soon as performance became adequate (by visually inspecting the results), we set this factor to 0.999 for quicker convergence. The trained parameters include all weights, the rest gain <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e162" xlink:type="simple"/></inline-formula> of all SOAs, and all delays. The weights and rest carrier densities gradients were normalized separately.</p>
<p>Training delays posed a practical difficulty, as in our simulation delays are still represented by a discrete number of time steps. Therefore, we only considered the sign of the respective gradients and either increased or decreased them with one time step each update. We are not certain whether training the delays poses a significant help in this case, especially since the delay gradient depends on the time derivatives of the SOA outputs, which have been polluted with amplified spontaneous emission noise.</p>
</list-item></list>
<list list-type="bullet"><list-item>
<p><bold>Implementation details</bold> We used Euler integration with time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e163" xlink:type="simple"/></inline-formula>. Delays were implemented by picking corresponding SOA output values from recorded time traces at a discrete number of time steps in the past.</p>
</list-item><list-item>
<p><bold>Initial parameters</bold> All weights of all connections were initialized with phases sampled uniformly from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e164" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e165" xlink:type="simple"/></inline-formula>. Initial moduli of internal connections were sampled uniformly between 0 and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e166" xlink:type="simple"/></inline-formula>, those of input data between 0 and 0.25 for the flip flop task and between 0 and 0.5 for the one-hot task. Input bias weights were initialized between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e167" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0086696.e168" xlink:type="simple"/></inline-formula>, and initial output weights all had modulo zero. Delays were uniformly sampled between their minimal and maximal value. The rest gains were initially all chosen at 3.</p>
</list-item><list-item>
<p><bold>Validation</bold> To measure how well the trained networks performed, we measured ROC curves in two ways: by taking the average output power over the duration of an entire bit, or by sampling output power at the last time step of a bit. For each task we measured this for 10 sequences of 50000 time steps, each with different input sequences, phase variations and noise. We omitted the first 10 bits as the networks may still be in a transient state from initial conditions. The ROC curves themselves were nearly perfectly square, so we do not show them. Instead we measured the area-under-curve (AUC) (equivalent to bit error rate), which was exactly equal to one for the photonic flip flop, for both methods of measurement (perfect performance for the given number of test instances). The one-hot detector had an AUC equal to exactly one when taking the average output power over each bit, and 0.9999 when using the last sample of each bit.</p>
</list-item></list>
</sec></sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pone.0086696.s001" mimetype="video/quicktime" xlink:href="info:doi/10.1371/journal.pone.0086696.s001" position="float" xlink:type="simple"><label>Movie S1</label><caption>
<p><bold>Animation of the MSD robot before optimisation commences.</bold> The blue horizontal line represents the ground, black lines are massless springs and the coloured dots are point masses (coloured for distinction).</p>
<p>(MOV)</p>
</caption></supplementary-material><supplementary-material id="pone.0086696.s002" mimetype="video/quicktime" xlink:href="info:doi/10.1371/journal.pone.0086696.s002" position="float" xlink:type="simple"><label>Movie S2</label><caption>
<p><bold>Animation of an example outcome of an MSD robot after optimisation.</bold> The blue horizontal line represents the ground, black lines are massless springs and the coloured dots are point masses (coloured for distinction).</p>
<p>(MOV)</p>
</caption></supplementary-material><supplementary-material id="pone.0086696.s003" mimetype="video/quicktime" xlink:href="info:doi/10.1371/journal.pone.0086696.s003" position="float" xlink:type="simple"><label>Movie S3</label><caption>
<p><bold>Animation of an example outcome of an MSD robot after optimisation.</bold> The blue horizontal line represents the ground, black lines are massless springs and the coloured dots are point masses (coloured for distinction).</p>
<p>(MOV)</p>
</caption></supplementary-material><supplementary-material id="pone.0086696.s004" mimetype="video/quicktime" xlink:href="info:doi/10.1371/journal.pone.0086696.s004" position="float" xlink:type="simple"><label>Movie S4</label><caption>
<p><bold>Animation of an example outcome of an MSD robot after optimisation.</bold> The blue horizontal line represents the ground, black lines are massless springs and the coloured dots are point masses (coloured for distinction).</p>
<p>(MOV)</p>
</caption></supplementary-material><supplementary-material id="pone.0086696.s005" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pone.0086696.s005" position="float" xlink:type="simple"><label>Derivation S1</label><caption>
<p>Mathematical derivation of the BPTT algorithm for continuous-time systems.</p>
<p>(PDF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We would like to thank Juan Pablo Carbajal, Martin Fiers, and Thomas Van Vaerenbergh for helpful insights and comments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0086696-VonNeumann1"><label>1</label>
<mixed-citation publication-type="other" xlink:type="simple">Von Neumann J (1951) The general and logical theory of automata. Cerebral mechanisms in behavior : 1–41.</mixed-citation>
</ref>
<ref id="pone.0086696-Turing1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Turing</surname><given-names>A</given-names></name> (<year>1952</year>) <article-title>The chemical basis of morphogenesis</article-title>. <source>Philosophical Transactions of the Royal Society of London Series B, Biological Sciences</source> <volume>237</volume>: <fpage>37</fpage>–<lpage>72</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Adamatzky1"><label>3</label>
<mixed-citation publication-type="other" xlink:type="simple">Adamatzky A, Costello BDL, Asai T (2005) Reaction-diffusion computers. Access Online via Elsevier.</mixed-citation>
</ref>
<ref id="pone.0086696-Arena1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Arena</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Fortuna</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Frasca</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Sicurella</surname><given-names>G</given-names></name> (<year>2004</year>) <article-title>An adaptive, self-organizing dynamical system for hierarchical control of bio-inspired locomotion</article-title>. <source>Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions</source> <volume>34</volume>: <fpage>1823</fpage>–<lpage>1837</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Ijspeert1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ijspeert</surname><given-names>AJ</given-names></name> (<year>2008</year>) <article-title>Central pattern generators for locomotion control in animals and robots: a review</article-title>. <source>Neural Networks</source> <volume>21</volume>: <fpage>642</fpage><lpage>653</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Paul1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paul</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Valero-Cuevas</surname><given-names>FJ</given-names></name>, <name name-style="western"><surname>Lipson</surname><given-names>H</given-names></name> (<year>2006</year>) <article-title>Design and control of tensegrity robots for locomotion</article-title>. <source>Robotics, IEEE Transactions</source> <volume>22</volume>: <fpage>944</fpage>–<lpage>957</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Pfeifer1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pfeifer</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Lungarella</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Iida</surname><given-names>F</given-names></name> (<year>2007</year>) <article-title>Self-organization, embodiment, and biologically inspired robotics</article-title>. <source>Science</source> <volume>318</volume>: <fpage>1088</fpage>–<lpage>1093</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Pfeifer2"><label>8</label>
<mixed-citation publication-type="other" xlink:type="simple">Pfeifer R, Bongard J, Grand S (2007) How the body shapes the way we think: a new view of intelligence. Cambridge, MA: The MIT Press.</mixed-citation>
</ref>
<ref id="pone.0086696-Mozer1"><label>9</label>
<mixed-citation publication-type="other" xlink:type="simple">Mozer MC (1995) A focused backpropagation algorithm for temporal pattern recognition. In: Chauvin Y, Rumelhart DE, editors, Backpropagation: Theory, Architectures and Applications, Hillsdale, NJ, USA: L. Erlbaum Associates Inc. 137–169.</mixed-citation>
</ref>
<ref id="pone.0086696-Rumelhart1"><label>10</label>
<mixed-citation publication-type="other" xlink:type="simple">Rumelhart D, Hinton G, Williams R (1986) Learning internal representations by error propagation. Cambridge, MA: MIT Press.</mixed-citation>
</ref>
<ref id="pone.0086696-Robinson1"><label>11</label>
<mixed-citation publication-type="other" xlink:type="simple">Robinson AJ, Fallside F (1987) The utility driven dynamic error propagation network. Technical Report CUED/F-INFENG/TR.1, Cambridge University Engineering Department, Cambridge.</mixed-citation>
</ref>
<ref id="pone.0086696-Werbos1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Werbos</surname><given-names>P</given-names></name> (<year>1988</year>) <article-title>Generalization of backpropagation with application to a recurrent gas market model</article-title>. <source>Neural Networks</source> <volume>1</volume>: <fpage>339</fpage>–<lpage>356</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Williams1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Williams</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Zipser</surname><given-names>D</given-names></name> (<year>1989</year>) <article-title>A learning algorithm for continually running fully recurrent neural networks</article-title>. <source>Neural Computation</source> <volume>1</volume>: <fpage>270</fpage>–<lpage>280</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Sutskever1"><label>14</label>
<mixed-citation publication-type="other" xlink:type="simple">Sutskever I, Martens J, Hinton G (2011) Generating text with recurrent neural networks. In: Proceedings of the 28th International Conference on Machine Learning. 1017–1024.</mixed-citation>
</ref>
<ref id="pone.0086696-Graves1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Graves</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Schmidhuber</surname><given-names>J</given-names></name> (<year>2005</year>) <article-title>Framewise phoneme classification with bidirectional lstm and other neural network architectures</article-title>. <source>Neural Networks</source> <volume>18</volume>: <fpage>602</fpage>–<lpage>610</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Boltyanskii1"><label>16</label>
<mixed-citation publication-type="other" xlink:type="simple">Boltyanskii VG, Gamkrelidze RV, Pontryagin LS (1960) The theory of optimal processes. i. the maximum principle. Technical report, DTIC Document.</mixed-citation>
</ref>
<ref id="pone.0086696-Vinh1"><label>17</label>
<mixed-citation publication-type="other" xlink:type="simple">Vinh N (1981) Optimal trajectories in atmospheric flight. Elsevier.</mixed-citation>
</ref>
<ref id="pone.0086696-Pearlmutter1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pearlmutter</surname><given-names>BA</given-names></name> (<year>1989</year>) <article-title>Learning state space trajectories in recurrent neural networks</article-title>. <source>Neural Computation</source> <volume>1</volume>: <fpage>263</fpage>–<lpage>269</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Bertsekas1"><label>19</label>
<mixed-citation publication-type="other" xlink:type="simple">Bertsekas DP (1995) Dynamic programming and optimal control. Athena Scientific Belmont.</mixed-citation>
</ref>
<ref id="pone.0086696-LeCun1"><label>20</label>
<mixed-citation publication-type="other" xlink:type="simple">LeCun Y (1988) A theoretical framework for back-propagation. In: Proceedings of the 1988 Connectionist Models Summer School. 21–28.</mixed-citation>
</ref>
<ref id="pone.0086696-Oden1"><label>21</label>
<mixed-citation publication-type="other" xlink:type="simple">Oden J, Belytschko T, Hughes T, Johnson C, Keyes D, <etal>et al</etal>.. (2006) Revolutionizing engineering science through simulation: A report of the national science foundation blue ribbon panel on simulation-based engineering science. Arlington, VA: National Science Foundation.</mixed-citation>
</ref>
<ref id="pone.0086696-Hansen1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hansen</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Ostermeier</surname><given-names>A</given-names></name> (<year>2001</year>) <article-title>Completely derandomized self-adaptation in evolution strategies</article-title>. <source>Evolutionary computation</source> <volume>9</volume>: <fpage>159</fpage>–<lpage>195</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Lipson1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lipson</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Pollack</surname><given-names>JB</given-names></name> (<year>2000</year>) <article-title>Automatic design and manufacture of robotic lifeforms</article-title>. <source>Nature</source> <volume>406</volume>: <fpage>974</fpage>–<lpage>978</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Tibshirani1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tibshirani</surname><given-names>R</given-names></name> (<year>1994</year>) <article-title>Regression shrinkage and selection via the lasso</article-title>. <source>Journal of the Royal Statistical Society, Series B</source> <volume>58</volume>: <fpage>267</fpage>–<lpage>288</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Hawkes1"><label>25</label>
<mixed-citation publication-type="other" xlink:type="simple">Hawkes PW (1966) Quadrupole optics. Springer.</mixed-citation>
</ref>
<ref id="pone.0086696-Paquot1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paquot</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Duport</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Smerieri</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Dambre</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Schrauwen</surname><given-names>B</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Optoelectronic reservoir computing</article-title>. <source>Scientific Reports</source> <volume>2</volume>: <fpage>1</fpage>–<lpage>6</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Vandoorne1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vandoorne</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Dierckx</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Schrauwen</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Verstraeten</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Baets</surname><given-names>R</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Toward optical signal processing using photonic reservoir computing</article-title>. <source>Optics Express</source> <volume>16</volume>: <fpage>11182</fpage>–<lpage>11192</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Larger1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Larger</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Soriano</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Brunner</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Appeltant</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Gutierrez</surname><given-names>J</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Photonic information processing beyond turing: an optoelectronic implementation of reservoir computing</article-title>. <source>Optics express</source> <volume>3</volume>: <fpage>20</fpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Brunner1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunner</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Soriano</surname><given-names>MC</given-names></name>, <name name-style="western"><surname>Mirasso</surname><given-names>CR</given-names></name>, <name name-style="western"><surname>Fischer</surname><given-names>I</given-names></name> (<year>2013</year>) <article-title>Parallel photonic information processing at gigabyte per second data rates using transient states</article-title>. <source>Nature communications</source> <volume>4</volume>: <fpage>1364</fpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Vandoorne2"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vandoorne</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Dambre</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Verstraeten</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Schrauwen</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Bienstman</surname><given-names>P</given-names></name> (<year>2011</year>) <article-title>Parallel reservoir computing using optical amplifiers</article-title>. <source>Neural Networks, IEEE Transactions on</source> <volume>22</volume>: <fpage>1469</fpage>–<lpage>1481</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Fiers1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fiers</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Van Vaerenbergh</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Caluwaerts</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Vande Ginste</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Schrauwen</surname><given-names>B</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Time-domain and frequency-domain modeling of nonlinear optical components at the circuit-level using a node-based approach</article-title>. <source>JOSA B</source> <volume>29</volume>: <fpage>896</fpage>–<lpage>900</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Bottou1"><label>32</label>
<mixed-citation publication-type="other" xlink:type="simple">Bottou L (2010) Large-scale machine learning with stochastic gradient descent. In: Compstat. 177–186.</mixed-citation>
</ref>
<ref id="pone.0086696-Shinya1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shinya</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Mitsugi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Tanabe</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Notomi</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Yokohama</surname><given-names>I</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>All-optical flip-flop circuit composed of coupled two-port resonant tunneling filter in two-dimensional photonic crystal slab</article-title>. <source>Optics Express</source> <volume>14</volume>: <fpage>1230</fpage>–<lpage>1235</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Liu1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liu</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Kumar</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Huybrechts</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Spuesens</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Roelkens</surname><given-names>G</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>An ultra-small, low-power, all-optical flip-flop memory on a silicon chip</article-title>. <source>Nature Photonics</source> <volume>4</volume>: <fpage>182</fpage>–<lpage>187</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Bongard1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bongard</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Zykov</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Lipson</surname><given-names>H</given-names></name> (<year>2006</year>) <article-title>Resilient machines through continuous self-modeling</article-title>. <source>Science</source> <volume>314</volume>: <fpage>1118</fpage><lpage>1121</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Becker1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Becker</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Kapp</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Rannacher</surname><given-names>R</given-names></name> (<year>2000</year>) <article-title>Adaptive finite element methods for optimal control of partial differential equations: Basic concept</article-title>. <source>SIAM Journal on Control and Optimization</source> <volume>39</volume>: <fpage>113</fpage>–<lpage>132</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Dale1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dale</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Husbands</surname><given-names>P</given-names></name> (<year>2010</year>) <article-title>The evolution of reaction-diffusion controllers for minimally cognitive agents</article-title>. <source>Artificial Life</source> <volume>16</volume>: <fpage>1</fpage>–<lpage>19</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Werbos2"><label>38</label>
<mixed-citation publication-type="other" xlink:type="simple">Werbos PJ (1994) The roots of backpropagation: from ordered derivatives to neural networks and political forecasting, volume 1.</mixed-citation>
</ref>
<ref id="pone.0086696-Werbos3"><label>39</label>
<mixed-citation publication-type="other" xlink:type="simple">Werbos PJ (2006) Backwards differentiation in ad and neural nets: Past links and new opportunities. In: Automatic Differentiation: Applications, Theory, and Implementations, Springer. 15–34.</mixed-citation>
</ref>
<ref id="pone.0086696-Doya1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doya</surname><given-names>K</given-names></name> (<year>1992</year>) <article-title>Bifurcations in the learning of recurrent neural networks</article-title>. <source>In: 1992 IEEE International Symposium on Circuits and Systems</source>. <volume>6</volume>: <fpage>2777</fpage>–<lpage>2780</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Agrawal1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Agrawal</surname><given-names>GP</given-names></name>, <name name-style="western"><surname>Olsson</surname><given-names>NA</given-names></name> (<year>1989</year>) <article-title>Self-phase modulation and spectral broadening of optical pulses in semiconductor laser amplifiers</article-title>. <source>Quantum Electronics, IEEE</source> <volume>25</volume>: <fpage>2297</fpage>–<lpage>2306</lpage>.</mixed-citation>
</ref>
<ref id="pone.0086696-Olsson1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Olsson</surname><given-names>NA</given-names></name> (<year>1989</year>) <article-title>Lightwave systems with optical amplifiers</article-title>. <source>Lightwave Technology</source> <volume>7</volume>: <fpage>1071</fpage>–<lpage>1082</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>