<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-15-02589</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0130316</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>On the Distribution of Salient Objects in Web Images and Its Influence on Salient Object Detection</article-title>
<alt-title alt-title-type="running-head">How Spatial Biases in Web Images Influence Salient Object Detection</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Schauerte</surname> <given-names>Boris</given-names></name>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Stiefelhagen</surname> <given-names>Rainer</given-names></name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
</contrib-group>
<aff id="aff001">
<addr-line>Institute for Anthropomatics and Robotics, Karlsruhe Institute of Technology, Karlsruhe, Baden–Württemberg, Germany</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Cristani</surname> <given-names>Marco</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University of Verona, ITALY</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: BS. Performed the experiments: BS. Analyzed the data: BS. Contributed reagents/materials/analysis tools: BS. Wrote the paper: BS RS.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">boris.schauerte@kit.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<year>2015</year>
</pub-date>
<pub-date pub-type="epub">
<day>22</day>
<month>7</month>
<year>2015</year>
</pub-date>
<volume>10</volume>
<issue>7</issue>
<elocation-id>e0130316</elocation-id>
<history>
<date date-type="received">
<day>19</day>
<month>1</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>19</day>
<month>5</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Schauerte, Stiefelhagen</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0130316" xlink:type="simple"/>
<abstract>
<p>In recent years it has become apparent that a Gaussian center bias can serve as an important prior for visual saliency detection, which has been demonstrated for predicting human eye fixations and salient object detection. Tseng et al. have shown that the photographer’s tendency to place interesting objects in the center is a likely cause for the center bias of eye fixations. We investigate the influence of the photographer’s center bias on salient object detection, extending our previous work. We show that the centroid locations of salient objects in photographs of Achanta and Liu’s data set in fact correlate strongly with a Gaussian model. This is an important insight, because it provides an empirical motivation and justification for the integration of such a center bias in salient object detection algorithms and helps to understand why Gaussian models are so effective. To assess the influence of the center bias on salient object detection, we integrate an explicit Gaussian center bias model into two state-of-the-art salient object detection algorithms. This way, first, we quantify the influence of the Gaussian center bias on pixel- and segment-based salient object detection. Second, we improve the performance in terms of <italic>F</italic><sub>1</sub> score, <italic>F</italic><sub><italic>β</italic></sub> score, area under the recall-precision curve, area under the receiver operating characteristic curve, and hit-rate on the well-known data set by Achanta and Liu. Third, by debiasing Cheng et al.’s region contrast model, we exemplarily demonstrate that implicit center biases are partially responsible for the outstanding performance of state-of-the-art algorithms. Last but not least, we introduce a non-biased salient object detection method, which is of interest for applications in which the image data is not likely to have a photographer’s center bias (e.g., image data of surveillance cameras or autonomous robots).</p>
</abstract>
<funding-group>
<funding-statement>The authors have no support or funding to report.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="3"/>
<page-count count="16"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>The original datasets (MSRA / Achanta) on which our work is based are publicly available on the respective author’s website and mirrored on GitHub (<ext-link ext-link-type="uri" xlink:type="simple" xlink:href="https://github.com/bschauerte/SalientObjectsAchanta">https://github.com/bschauerte/SalientObjectsAchanta</ext-link>). These datasets are common evaluation benchmark datasets in the field of salient object detection. Our additional code and tools are available on GitHub: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="https://github.com/bschauerte/region_contrast_saliency">https://github.com/bschauerte/region_contrast_saliency</ext-link>; <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="https://github.com/bschauerte/salient_object_detection_evaluation">https://github.com/bschauerte/salient_object_detection_evaluation</ext-link>; <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="https://github.com/bschauerte/salient_object_distribution">https://github.com/bschauerte/salient_object_distribution</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>1 Introduction</title>
<p>Among other influences such as task-specific factors, human attention is attracted to salient stimuli. In this context, saliency describes the subjective, perceptual quality that lets some items in the world stand out from their neighbors and immediately grab our attention. Accordingly, the goal of visual saliency detection is to determine what parts of an image are likely to grab the human attention. The task of “traditional” <italic>visual saliency detection</italic> is to predict where human observers look when presented with a scene, which can be recorded using eye tracking equipment (e.g., [<xref ref-type="bibr" rid="pone.0130316.ref001">1</xref>–<xref ref-type="bibr" rid="pone.0130316.ref004">4</xref>]). Liu et al. adapted the traditional definition of visual saliency by incorporating the high level concept of a salient object into the process of visual attention computation [<xref ref-type="bibr" rid="pone.0130316.ref005">5</xref>]. Here, a <italic>salient object</italic> is defined as being the object in an image that attracts most of the user’s interest such as, for example, the man, the cross, the baseball players and the flowers in <xref ref-type="fig" rid="pone.0130316.g001">Fig 1</xref>. Accordingly, Liu et al. [<xref ref-type="bibr" rid="pone.0130316.ref005">5</xref>] defined the task of <italic>salient object detection</italic> as the binary labeling problem of separating the salient object from the background. Thus, in contrast to traditional visual saliency detection, salient object detection does not just comprise of the task to calculate the saliency of image regions, but it also incorporates the task to determine and segment the most salient object in the image. Here, it is important to note that the selection of a salient object happens consciously by the user whereas the gaze trajectories that are recorded using eye trackers are the result of mostly unconscious processes. Consequently, also taking into account that salient objects attract the human gaze (see, e.g., [<xref ref-type="bibr" rid="pone.0130316.ref001">1</xref>]), salient object detection and predicting where people look are very closely related yet substantially different tasks.</p>
<fig id="pone.0130316.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0130316.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Illustration of the Achanta/Liu data set: example images (a), the corresponding segmentation masks (c), the mean over all segmentation masks (d), and the scatter plot of the centroid locations across all images (b).</title>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pone.0130316.g001"/>
</fig>
<p>The photographer’s center bias, i.e. the natural tendency of photographers to place the objects of interest near the center of their composition in order to enhance their focus and size relative to the background (see Tseng et al. [<xref ref-type="bibr" rid="pone.0130316.ref006">6</xref>]; we would like to note that Tseng et al.—due to their methodology—did not investigate the exact spatial distribution of the objects that attract the gaze, since they hired five persons who provided subjective scores from 1 to 5 in terms of how interesting things were biased toward the image center), has been identified as one cause for the often reported center bias in eye-tracking data during eye-gaze studies [<xref ref-type="bibr" rid="pone.0130316.ref007">7</xref>–<xref ref-type="bibr" rid="pone.0130316.ref009">9</xref>]. As a consequence, the integration of a center bias has become an increasingly important aspect in visual saliency models that focus on gaze prediction (e.g., [<xref ref-type="bibr" rid="pone.0130316.ref002">2</xref>, <xref ref-type="bibr" rid="pone.0130316.ref003">3</xref>, <xref ref-type="bibr" rid="pone.0130316.ref010">10</xref>]). In contrast, most recently proposed salient object detection algorithms do not incorporate an explicit model of the photographer’s center bias (see, e.g., [<xref ref-type="bibr" rid="pone.0130316.ref011">11</xref>–<xref ref-type="bibr" rid="pone.0130316.ref014">14</xref>]). A notable exception and closely related to our work is the work by Jiang et al. [<xref ref-type="bibr" rid="pone.0130316.ref015">15</xref>], in which one of the three main criteria that characterize a salient object is that “it is most probably placed near the center of the image” [<xref ref-type="bibr" rid="pone.0130316.ref015">15</xref>]. The authors justify this characterization with the “rule of thirds”, which is one of the most well-known principles of photographic composition (see, e.g., [<xref ref-type="bibr" rid="pone.0130316.ref016">16</xref>]), and use a Gaussian distance metric as a model. However, Jiang et al. do neither justify why the rule of thirds would be well represented by a Gaussian distance metric nor do they investigate the quantitative influence of such a Gaussian center bias. We go beyond following the rule of third and show that the distribution of the objects’ centroids correlates strongly positively with a 2-dimensional Gaussian distribution. This means nothing less than that we provide a strong empirical justification for integrating Gaussian center bias models into salient object detection algorithms. To demonstrate the importance, we adapt two state-of-the-art salient object detection methods to quantify the influence of the photographer’s center bias on salient object detection.</p>
<p>The contribution of this paper is twofold: First, we use the salient object data set by Achanta et al. [<xref ref-type="bibr" rid="pone.0130316.ref011">11</xref>] to investigate the spatial distribution of salient objects in images. This way, in Sec. 3, we show that it is likely that salient objects in photographs are distributed around the image center in such a way that the radii are half-Gaussian distributed and the angles are uniformly distributed. Second, in Sec. 4, we explicitly integrate Gaussian center bias models in two recently proposed salient object detection methods: The pixel-based maximum symmetric surround salient object detection by Achanta et al. [<xref ref-type="bibr" rid="pone.0130316.ref012">12</xref>] and the segment-based region contrast method by Cheng et al. [<xref ref-type="bibr" rid="pone.0130316.ref014">14</xref>]. In order to measure the influence, we use the following evaluation measures: The maximum <italic>F</italic><sub>1</sub> score, the maximum <italic>F</italic><sub><italic>β</italic></sub> score with <inline-formula id="pone.0130316.e001"><alternatives><graphic id="pone.0130316.e001g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0130316.e001"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:mi>β</mml:mi> <mml:mo>=</mml:mo> <mml:msqrt><mml:mrow><mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>3</mml:mn></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula> [<xref ref-type="bibr" rid="pone.0130316.ref011">11</xref>], the area-under-curve of the precision-recall curve, the AUC of the receiver operating characteristic (ROC AUC), and the hit-rate. In summary, the integration of the center bias model increases the ROC AUC by 2% and the performance with respect to all remaining measures by roughly 5%. Thus, we further advance the state-of-the-art of pixel-based as well as segment-based salient object detection. By modifying Cheng et al.’s region contrast model [<xref ref-type="bibr" rid="pone.0130316.ref014">14</xref>], first, we obtained a non-biased salient object detection algorithm that is based on region contrast and, second, we exemplarily demonstrate that implicit center biases can already be found in well-performing, state-of-the-art salient object detection algorithms and substantially influence the performance. This is important to consider when comparing and selecting algorithms for applications in which the data is not necessarily biased towards the center.</p>
<p>The remainder of this paper is organized as follows: In Sec. 2, we provide an overview of related work. Subsequently, in Sec. 3, we introduce and investigate our hypotheses about the spatial distribution of salient objects. Then, in Sec. 4, we integrate our hypotheses into two recently proposed salient object detection methods and evaluate the influence on the salient object detection performance. We conclude with a short summary and discussion in Sec. 5. Furthermore, please feel free to check the supplemental material for additional information such as, e.g., further evaluation results.</p>
</sec>
<sec id="sec002">
<title>2 Related Work</title>
<p>We focus on the most recent related work that addresses bottom-up saliency detection with an emphasis on salient object detection (see, e.g., [<xref ref-type="bibr" rid="pone.0130316.ref017">17</xref>] for a more general overview of computational attention models). Such methods may be biologically motivated, or purely computational, or involve both aspects. In 2009, Achanta et al. [<xref ref-type="bibr" rid="pone.0130316.ref011">11</xref>, <xref ref-type="bibr" rid="pone.0130316.ref012">12</xref>] introduced a salient object detection approach that basically relies on the difference of pixels to the average color and intensity value. In order to evaluate their approach, they selected a sub-set of 1000 images of the image data set that was collected from the web by Liu et al. [<xref ref-type="bibr" rid="pone.0130316.ref005">5</xref>] and calculated segmentation masks of the salient objects that were marked by 9 participants using (rough) rectangle annotations [<xref ref-type="bibr" rid="pone.0130316.ref005">5</xref>]. Please note that this procedure also means that during the manual data set annotation the selection of the salient object happens mostly conscious whereas gaze trajectories that are recorded using eye trackers are a result of a mostly unconscious process. Since it was created, the salient object data set by Achanta et al. serves as reference data set to evaluate methods for salient object detection (see, e.g., [<xref ref-type="bibr" rid="pone.0130316.ref011">11</xref>–<xref ref-type="bibr" rid="pone.0130316.ref014">14</xref>]). Liu et al. [<xref ref-type="bibr" rid="pone.0130316.ref005">5</xref>] and Alexe et al. [<xref ref-type="bibr" rid="pone.0130316.ref018">18</xref>] approach salient object detection using machine learning. To this end, Liu et al. [<xref ref-type="bibr" rid="pone.0130316.ref005">5</xref>] combine multi-scale contrast, center-surround histograms, and color spatial-distributions with conditional random fields. Similarly, Alexe et al. [<xref ref-type="bibr" rid="pone.0130316.ref018">18</xref>] combine multi-scale saliency, color contrast, edge density, and superpixels in a Bayesian framework. Closely related to Bayesian surprise [<xref ref-type="bibr" rid="pone.0130316.ref019">19</xref>], Klein et al. [<xref ref-type="bibr" rid="pone.0130316.ref013">13</xref>] use the Kullback-Leibler Divergence of the center and surround image patch histograms to calculate the saliency. Cheng et al. [<xref ref-type="bibr" rid="pone.0130316.ref014">14</xref>] use segmentation to define a regional contrast-based method, which simultaneously evaluates global contrast differences and spatial coherence. Here, we can differentiate between algorithms that rely on segmentation-based (e.g., [<xref ref-type="bibr" rid="pone.0130316.ref014">14</xref>, <xref ref-type="bibr" rid="pone.0130316.ref018">18</xref>]) and pixel-based contrast measures (e.g., [<xref ref-type="bibr" rid="pone.0130316.ref011">11</xref>–<xref ref-type="bibr" rid="pone.0130316.ref013">13</xref>]). Closely related to our work on the quantitative influence of the center bias on salient object detection is the work by Jiang et al. [<xref ref-type="bibr" rid="pone.0130316.ref015">15</xref>] and most recently Borji et al. [<xref ref-type="bibr" rid="pone.0130316.ref020">20</xref>]. In Jiang et al.’s work [<xref ref-type="bibr" rid="pone.0130316.ref015">15</xref>] one of the main criteria that characterize a salient object is that “it is most probably placed near the center of the image”, which is justified with the “rule of thirds”. Most recently, Borji et al. [<xref ref-type="bibr" rid="pone.0130316.ref020">20</xref>] evaluated several salient object detection models and also performed tests with an additive Gaussian center bias and conclude that the resulting “change in accuracy is not significant and does not alter model rankings”. But, this neglects the possibility that well-performing models already have an integrated, implicit center bias, which—as one part of our work—we demonstrate exemplarily to be the case for Cheng et al.’s region contrast algorithm [<xref ref-type="bibr" rid="pone.0130316.ref014">14</xref>]. Furthermore, there exist several approaches that explicitly integrate a center bias, but do not provide a quantitative evaluation of its influence nor an empirical justification of the chosen model (e.g., [<xref ref-type="bibr" rid="pone.0130316.ref021">21</xref>]). In this paper, we adapt the pixel-based method by Achanta et al. [<xref ref-type="bibr" rid="pone.0130316.ref012">12</xref>] and the segmentation-based method by Cheng et al. [<xref ref-type="bibr" rid="pone.0130316.ref014">14</xref>] to incorporate a model of the photographer-related center bias and quantify the influence of the center bias on the performance. Furthermore, Borji et al. [<xref ref-type="bibr" rid="pone.0130316.ref020">20</xref>] do not provide an empirical justification why a Gaussian distribution is an appropriate center bias model, which is another part of the work described in this paper.</p>
<p>It has been observed in several studies that the visual attention of human participants in natural scenes is biased toward the center of static images and videos (see, e.g., [<xref ref-type="bibr" rid="pone.0130316.ref008">8</xref>, <xref ref-type="bibr" rid="pone.0130316.ref009">9</xref>, <xref ref-type="bibr" rid="pone.0130316.ref022">22</xref>]). One possible bottom-up cause of the bias is intrinsic bottom-up visual saliency as predicted by computational saliency models. One possible top-down cause of the center bias is known as photographer bias (see, e.g., [<xref ref-type="bibr" rid="pone.0130316.ref007">7</xref>–<xref ref-type="bibr" rid="pone.0130316.ref009">9</xref>]), which describes the natural tendency of photographers to place objects of interest near the center of their composition. In fact, what the photographer considers interesting may also be highly bottom-up salient. Additionally, the photographer bias may lead to a viewing strategy bias [<xref ref-type="bibr" rid="pone.0130316.ref023">23</xref>], which means that viewers may orient their attention more often toward the center of the scene, because they expect salient or interesting objects to be placed there. Thus, since in natural images and videos the distribution of objects of interest and thus saliency is usually biased toward the center, it is often unclear how much the saliency actually contributes in guiding attention. It is possible that people look at the center for reasons other than saliency, but their gaze happens to fall on salient locations. Therefore, this center bias may result in overestimating the influence of saliency computed by the model and contaminate the evaluation of how visual saliency may guide orienting behavior. Recently, Tseng et al. [<xref ref-type="bibr" rid="pone.0130316.ref006">6</xref>] were able to demonstrate quantitatively that center bias is correlated strongly with photographer bias and is influenced by viewing strategy at scene onset. Furthermore, e.g., they were able to show that motor bias had no effect. However, they did not evaluate and computationally model how specifically the objects that attract the gaze are distributed spatially in the image. Instead, Tseng et al. hired five naive participants to provide subjective scores from 1 to 5 in terms of how interesting things were biased toward the image center [<xref ref-type="bibr" rid="pone.0130316.ref006">6</xref>]. In this paper, we use the data set by Achanta et al. [<xref ref-type="bibr" rid="pone.0130316.ref011">11</xref>] to investigate the distribution of salient objects in photographs and then evaluate the influence on two state-of-the-art salient object detection models.</p>
</sec>
<sec id="sec003">
<title>3 Center Bias Model</title>
<p>To investigate the spatial distribution of salient objects in photographs collected from the web, we use the manually annotated segmentation masks by Achanta et al. [<xref ref-type="bibr" rid="pone.0130316.ref011">11</xref>, <xref ref-type="bibr" rid="pone.0130316.ref012">12</xref>] that mark the salient objects in 1000 images of the salient object data set by Liu et al. [<xref ref-type="bibr" rid="pone.0130316.ref005">5</xref>]. More specifically, we use the segmentation masks to determine the centroids of all salient objects in data set and analyze the centroids’ spatial distribution. The images in the data set by Liu et al. [<xref ref-type="bibr" rid="pone.0130316.ref005">5</xref>] have been collected from a variety of sources, mostly from image forums and image search engines. Liu et al. collected more than 60,000 images and subsequently selected an image subset in which all images contain a salient object or a distinctive foreground object [<xref ref-type="bibr" rid="pone.0130316.ref005">5</xref>]. Nine users marked the salient objects using (rough) bounding boxes and the salient objects in the image database have been defined based on the “majority agreement”. However, as a consequence of the selection process, the data set does not include images without distinct salient objects. This is an important aspect to consider when trying to generalize the results reported on Achanta et al.’s and Liu et al.’s data set to other data sets or application areas.</p>
<p>In order to statistically analyze the 2-dimensional spatial distribution of the salient objects’ centroids, we first identify the center of the spatial distribution. Then, given the distribution’s center, we can use a polar coordinate system to independently analyze the distribution of the angles and distances between the center and the salient objects.</p>
<sec id="sec004">
<title>3.1 The Center</title>
<p>Our model is based on a polar coordinate system that has its pole at the image center. Since the images in Achanta’s data set have varying widths and heights, we use in the following normalized Cartesian image coordinates in the range [0, 1] × [0, 1]. The mean salient object centroid location is [0.5021, 0.5024]<sup>T</sup> and the corresponding covariance matrix is [0.0223, −0.0008; −0.0008, 0.0214]. Thus, we can motivate the use of a polar coordinate system that has its pole at [0.5, 0.5]<sup>T</sup> to represent all locations relative to the expected distribution’s mode.</p>
</sec>
<sec id="sec005">
<title>3.2 The Angles are Distributed Uniformly</title>
<p>Our first model hypothesis is that the centroids’ angles in the specified polar coordinate system are uniformly distributed in [−<italic>π</italic>, <italic>π</italic>].</p>
<p>In order to investigate the hypothesis, we use a Quantile-Quantile (Q-Q) plot as a graphical method to compare probability distributions (see [<xref ref-type="bibr" rid="pone.0130316.ref024">24</xref>]). In Q-Q plots the quantiles of the samples of two distributions are plotted against each other. Thus, the more similar the two distributions are, the better the points in the Q-Q plot will approximate the line <italic>f</italic>(<italic>x</italic>) = <italic>x</italic>. We calculate the Q-Q plot of the salient object location angles in our polar coordinate system versus uniformly drawn samples in [−<italic>π</italic>, <italic>π</italic>], see <xref ref-type="fig" rid="pone.0130316.g002">Fig 2</xref> (left). The apparent linearity of the plotted Q-Q points supports the hypothesis that the angles are distributed uniformly.</p>
<fig id="pone.0130316.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0130316.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Quantile-Quantile (Q-Q) plots of the angles versus a uniform distribution (left), radii versus a half-Gaussian distribution (middle), transformed radii (see Sec. 3.3) versus a normal distribution (right).</title>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pone.0130316.g002"/>
</fig>
<p>We can quantify the observed linearity, see <xref ref-type="fig" rid="pone.0130316.g002">Fig 2</xref> (left), to analyze the correlation between the model distribution and the data samples using probability plot correlation coefficients (PPCC) [<xref ref-type="bibr" rid="pone.0130316.ref024">24</xref>]. The PPCC is the correlation coefficient between the paired quantiles and measures the agreement of the fitted distribution with the observed data (i.e., goodness-of-fit). The closer the correlation coefficient is to one, the higher the positive correlation and the more likely the distributions are shifted and/or scaled versions of each other. Furthermore, by comparing against critical values of the PPCC (see [<xref ref-type="bibr" rid="pone.0130316.ref025">25</xref>] and [<xref ref-type="bibr" rid="pone.0130316.ref024">24</xref>]), we can use the PPCC as a statistical test, which is closely related to the Shapiro-Wilk test [<xref ref-type="bibr" rid="pone.0130316.ref026">26</xref>] and can reject the hypothesis that the data samples match the assumed model distribution. Furthermore, we can use the correlation to test the hypothesis of no correlation by transforming the correlation to create a t-statistic.</p>
<p>The obvious linearity of the Q-Q plot, see <xref ref-type="fig" rid="pone.0130316.g002">Fig 2</xref> (left), is reflected by a PPCC of 0.9988 (mean of several runs with <italic>N</italic> = 1000 uniform randomly selected samples), which is substantially higher than the critical value of 0.8880 (see [<xref ref-type="bibr" rid="pone.0130316.ref025">25</xref>]) and thus the hypothesis of identical distributions can not be rejected. Furthermore, the hypothesis of no correlation is rejected at <italic>α</italic> = 0.05 (<italic>p</italic> = 0).</p>
</sec>
<sec id="sec006">
<title>3.3 The Radii follow a Half-Gaussian Distribution</title>
<p>Our second model hypothesis is that the radii of the salient object locations follow a half-Gaussian distribution. We have to consider a half-Gaussian distribution in the interval [0,∞], because the radius—as a length—is by definition positive. If we consider the image borders, we could assume a two-sided truncated distribution, but we have three reasons to work with a one-sided model: The variance of the radii seems sufficiently small, the “true” centroid of the salient object may be outside the image borders (i.e., parts of the salient object can be truncated by the image borders), and it facilitates the use of various, well-known statistical tests (see [<xref ref-type="bibr" rid="pone.0130316.ref027">27</xref>]).</p>
<p>We can use a Q-Q plot against a half-Gaussian distribution to graphically assess the hypothesis, see <xref ref-type="fig" rid="pone.0130316.g002">Fig 2</xref> (middle). The linearity of the points suggests that the radii are distributed according to a half-Gaussian distribution. The visible outliers in the upper-right are caused by less than 30 centroids that are highly likely to be disturbed by the image borders. Please be aware of the fact that it is not necessary to know the exact distribution parameters when working with Q-Q plots as long as the distributions are linearly related (see [<xref ref-type="bibr" rid="pone.0130316.ref024">24</xref>]). Furthermore, we transform the polar coordinates in such a way that they represent the same point with a combination of positive angles in [0, <italic>π</italic>] and radii in [−∞,∞]. This way, we can compare the distribution of the transformed radii against a normal distribution with its mode and mean at 0, see <xref ref-type="fig" rid="pone.0130316.g002">Fig 2</xref> (right).</p>
<p>The obvious correlation that is visible in the Q-Q plots, see <xref ref-type="fig" rid="pone.0130316.g002">Fig 2</xref> (middle and right), is reflected by a PPCC of 0.9987, which is above the critical value of 0.9984 (see [<xref ref-type="bibr" rid="pone.0130316.ref024">24</xref>]). The hypothesis of no correlation is rejected at <italic>α</italic> = 0.05 (<italic>p</italic> = 0).</p>
</sec>
</sec>
<sec id="sec007">
<title>4 Quantifying the Influence on Salient Object Detection</title>
<p>To assess the influence of the center bias on pixel- and object-based salient object detection, we integrate a Gaussian center bias into the algorithms by Achanta et al. [<xref ref-type="bibr" rid="pone.0130316.ref012">12</xref>] and Cheng at al. [<xref ref-type="bibr" rid="pone.0130316.ref014">14</xref>].</p>
<sec id="sec008">
<title>4.1 Center Biased Saliency Models</title>
<sec id="sec009">
<title>Pixel-based</title>
<p>As a pixel-based model, we use maximum symmetric surround saliency detection by Achanta et al. [<xref ref-type="bibr" rid="pone.0130316.ref012">12</xref>] in combination with a Gaussian center bias map (cf., e.g., [<xref ref-type="bibr" rid="pone.0130316.ref003">3</xref>, <xref ref-type="bibr" rid="pone.0130316.ref010">10</xref>]). To this end, we define the center bias saliency map <italic>S</italic><sub>C</sub> ∈ <bold>R</bold><sup><italic>M</italic>×<italic>N</italic></sup>
<disp-formula id="pone.0130316.e002"><alternatives><graphic id="pone.0130316.e002g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0130316.e002"/><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi> <mml:mi mathvariant="normal">C</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo><mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>y</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="1.em"/><mml:mtext>with</mml:mtext></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula>
<disp-formula id="pone.0130316.e003"><alternatives><graphic id="pone.0130316.e003g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0130316.e003"/><mml:math id="M3" display="block" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:msqrt><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfrac><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd/><mml:mtd><mml:mo>*</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:msqrt><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfrac><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where (<italic>x</italic>, <italic>y</italic>) is the pixel coordinate, <italic>μ</italic> = (<italic>μ</italic><sub><italic>x</italic></sub>, <italic>μ</italic><sub><italic>y</italic></sub>) is the image center’s coordinate, and <italic>σ</italic><sub><italic>x</italic></sub> and <italic>σ</italic><sub><italic>y</italic></sub> are the standard deviation in x- and y-direction depending on the image width and height, respectively.</p>
<p>In order to investigate the influence of the center bias, we investigate different, plausible strategies to investigate the combination of the bottom-up and center bias saliency maps <italic>S</italic><sub>B</sub> and <italic>S</italic><sub>C</sub>, respectively:
<disp-formula id="pone.0130316.e004"><alternatives><graphic id="pone.0130316.e004g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0130316.e004"/><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi> <mml:mi mathvariant="normal">P</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mi mathvariant="normal">C</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mi mathvariant="normal">B</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
where <italic>f</italic> is the chosen center bias integration scheme.</p>
<p>We consider the following schemes, cf. [<xref ref-type="bibr" rid="pone.0130316.ref004">4</xref>]: First, a convex, linear integration, i.e. <italic>f</italic><sub>+</sub>(<italic>S</italic><sub>C</sub>, <italic>S</italic><sub>B</sub>) = <italic>w</italic><sub>C</sub><italic>S</italic><sub>C</sub> + <italic>w</italic><sub>B</sub><italic>S</italic><sub>B</sub> with <inline-formula id="pone.0130316.e005"><alternatives><graphic id="pone.0130316.e005g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0130316.e005"/><mml:math id="M5" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mtext>B</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mtext>C</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mspace width="1pt"/><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mtext>B</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mtext>C</mml:mtext></mml:msub><mml:mo>∈</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>R</mml:mi></mml:mstyle><mml:mn>0</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Second, multiplicative integration as a supra-linear combination method, i.e. <italic>f</italic><sub>°</sub>(<italic>S</italic><sub>C</sub>, <italic>S</italic><sub>B</sub>) = <italic>S</italic><sub>C°</sub><italic>S</italic><sub>B</sub>, where <sub>°</sub> denotes the Hadamard product. Third, the minimum as a further, alternative supra-linear combination, i.e. <italic>f</italic><sub>↓</sub>(<italic>S</italic><sub>C</sub>, <italic>S</italic><sub>B</sub>) = min(<italic>S</italic><sub>C</sub>, <italic>S</italic><sub>B</sub>). Fourth, the maximum to realize a late, sub-linear combination scheme, i.e. <italic>f</italic><sub>↑</sub>(<italic>S</italic><sub>C</sub>, <italic>S</italic><sub>B</sub>) = max(<italic>S</italic><sub>C</sub>, <italic>S</italic><sub>B</sub>). All these schemes are also related to different Fuzzy logic interpretations, which might provide a common theoretical framework and interpretation throughout later applications (e.g., [<xref ref-type="bibr" rid="pone.0130316.ref028">28</xref>]). To improve the readability, we refer to the linear combination for explicit center bias integration—unless stated otherwise, of course—in the following.</p>
</sec>
<sec id="sec010">
<title>Segmentation-based</title>
<p>As a segmentation-based model, we adapt Cheng et al.’s region contrast model [<xref ref-type="bibr" rid="pone.0130316.ref014">14</xref>]. This model is particularly interesting, because it already provides state-of-the-art performance, which is partially caused by an implicit (i.e., unmotivated, undiscussed and potentially unknowingly introduced by the authors) center bias as we will show in the following. This way, we can observe how the model behaves if we remove the implicit center bias—which was neither motivated nor explained by the authors—and add an explicit Gaussian center bias. The spatially weighted region contrast saliency equation is defined as follows
<disp-formula id="pone.0130316.e006"><alternatives><graphic id="pone.0130316.e006g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0130316.e006"/><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi> <mml:mi mathvariant="normal">S</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>≠</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder> <mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>s</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>D</mml:mi> <mml:mi>r</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="2.em"/><mml:mtext>with</mml:mtext></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula>
<disp-formula id="pone.0130316.e007"><alternatives><graphic id="pone.0130316.e007g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0130316.e007"/><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>s</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mi>D</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>s</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula> <italic>w</italic>(<italic>r</italic><sub><italic>i</italic></sub>) is the weight of region <italic>r</italic><sub><italic>i</italic></sub>, which equals the number of pixels in <italic>r</italic><sub><italic>i</italic></sub>—i.e., <italic>w</italic>(<italic>r</italic><sub><italic>i</italic></sub>) = ∣<italic>r</italic><sub><italic>i</italic></sub>∣—to emphasize color contrast to bigger regions. <italic>D</italic><sub><italic>r</italic></sub>(⋅;⋅) is the color distance metric between the two regions
<disp-formula id="pone.0130316.e008"><alternatives><graphic id="pone.0130316.e008g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0130316.e008"/><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>D</mml:mi> <mml:mi>r</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:munder> <mml:munder><mml:mo>∑</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:munder> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>;</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>;</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>D</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>c</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula>
where <italic>f</italic>(<italic>c</italic><sub><italic>k</italic>;<italic>i</italic></sub>) is the (frequentist) probability of the i-th color <italic>c</italic><sub><italic>k</italic>;<italic>i</italic></sub> among all <italic>n</italic><sub><italic>k</italic></sub> colors in the k-th region <italic>r</italic><sub><italic>k</italic></sub>, which is determined using a color histogram. The probability of the color inside the regions <italic>f</italic>(<italic>c</italic><sub><italic>k</italic>;<italic>i</italic></sub>) is used as weight to emphasize color differences between dominant colors. <italic>D</italic>(<italic>c</italic><sub><italic>i</italic></sub>;<italic>c</italic><sub><italic>j</italic></sub>) measures the distance between the colors and in the following it is defined as being the Euclidean distance in the CIE Lab color space. Finally, <inline-formula id="pone.0130316.e009"><alternatives><graphic id="pone.0130316.e009g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0130316.e009"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover><mml:mi>D</mml:mi> <mml:mo accent="true">^</mml:mo></mml:mover> <mml:mi>s</mml:mi></mml:msub> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is the spatial distance between regions <italic>r</italic><sub><italic>k</italic></sub> and <italic>r</italic><sub><italic>i</italic></sub>, where <italic>σ</italic><sub><italic>s</italic></sub> controls the spatial weighting. The spatial distance between two regions is defined as the Euclidean distance between the centroids of the respective regions using pixel coordinates that are normalized to the range [0, 1] × [0, 1]. Smaller values of <italic>σ</italic><sub><italic>s</italic></sub> influence the spatial weighting in such a way that the contrast to regions that are farther away contributes less to the saliency of the current region.</p>
<p>It is this unnormalized Gaussian weighted Euclidean distance <inline-formula id="pone.0130316.e010"><alternatives><graphic id="pone.0130316.e010g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0130316.e010"/><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover><mml:mi>D</mml:mi> <mml:mo accent="true">^</mml:mo></mml:mover> <mml:mi>s</mml:mi></mml:msub> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> that causes an implicit Gaussian-like center bias (see Figs <xref ref-type="fig" rid="pone.0130316.g003">3</xref> and <xref ref-type="fig" rid="pone.0130316.g004">4</xref>), because it favors regions whose distances to the other neighbors are smaller, which is—in general—the case for segments at the center of the image. Although this biased distance function has a significant impact on the performance, its choice has not been clearly motivated, discussed, or evaluated by Cheng et al. To remove this implicit bias, we introduce a normalized, i.e. locally debiased, distance function <inline-formula id="pone.0130316.e011"><alternatives><graphic id="pone.0130316.e011g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0130316.e011"/><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mover><mml:mi>D</mml:mi> <mml:mo accent="true">ˇ</mml:mo></mml:mover> <mml:mi>s</mml:mi></mml:msub> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> that still weights close-by regions higher than further away regions, but does not lead to an implicit center bias
<disp-formula id="pone.0130316.e012"><alternatives><graphic id="pone.0130316.e012g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0130316.e012"/><mml:math id="M12" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mover><mml:mi>D</mml:mi> <mml:mo accent="true">ˇ</mml:mo></mml:mover> <mml:mi>s</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>s</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub> <mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>s</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mspace width="4pt"/></mml:mrow></mml:math></alternatives> <label>(7)</label></disp-formula>
<disp-formula id="pone.0130316.e013"><alternatives><graphic id="pone.0130316.e013g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0130316.e013"/><mml:math id="M13" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">i</mml:mi> <mml:mo>.</mml:mo> <mml:mi mathvariant="normal">e</mml:mi> <mml:mo>.</mml:mo> <mml:mspace width="4pt"/><mml:mo>∀</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>:</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:munder> <mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi> <mml:mo>ˇ</mml:mo></mml:mover> <mml:mi>s</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives> <label>(8)</label></disp-formula></p>
<fig id="pone.0130316.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0130316.g003</object-id>
<label>Fig 3</label>
<caption>
<title>An example illustrating the influence of the implicit center bias in the region contrast method by Cheng et al. [<xref ref-type="bibr" rid="pone.0130316.ref014">14</xref>].</title>
<p>Left-to-right: Image, region contrast (RC) saliency map, and locally debiased region contrast (LDRC) saliency map. As can be seen, RC tends to assign a comparatively high saliency to regions in the center of the image even if these regions exhibit an apparently low perceptual saliency such as, for example, the space between the hand cart’s wheels in the bottom row.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pone.0130316.g003"/>
</fig>
<fig id="pone.0130316.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0130316.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Illustration of the implicit center bias in the method by Cheng et al. [<xref ref-type="bibr" rid="pone.0130316.ref014">14</xref>].</title>
<p>Left: Each pixel shows the distance weight sum, i.e. <inline-formula id="pone.0130316.e015"><alternatives><graphic id="pone.0130316.e015g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0130316.e015"/><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:msub> <mml:msub><mml:mover><mml:mi>D</mml:mi> <mml:mo accent="true">^</mml:mo></mml:mover> <mml:mi>s</mml:mi></mml:msub> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, to all other pixels in a regular grid. Right: The average weight sum depending on the centroid location calculated on the Achanta/Liu data set using Felzenszwalb’s segmentation method.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pone.0130316.g004"/>
</fig>
<p>Similar to the pixel-based model (see Sec. 4.1), we can now integrate an explicit center bias into the segmentation-based model
<disp-formula id="pone.0130316.e014"><alternatives><graphic id="pone.0130316.e014g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0130316.e014"/><mml:math id="M14" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi> <mml:mi mathvariant="normal">S</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo><mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>≠</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:munder> <mml:msub><mml:mover><mml:mi>D</mml:mi> <mml:mo accent="true">ˇ</mml:mo></mml:mover> <mml:mi>s</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>w</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>D</mml:mi> <mml:mi>r</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mspace width="0.166667em"/><mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>;</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>x</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>y</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(9)</label></disp-formula>
Here, <italic>f</italic> is the chosen center bias integration function as in <xref ref-type="disp-formula" rid="pone.0130316.e004">Eq 3</xref>. Furthermore, <italic>C</italic>(<italic>r</italic><sub><italic>k</italic></sub>) denotes the centroid of region <italic>r</italic><sub><italic>k</italic></sub> and <italic>g</italic> is defined as in <xref ref-type="disp-formula" rid="pone.0130316.e003">Eq 2</xref>.</p>
</sec>
</sec>
<sec id="sec011">
<title>4.2 Evaluation Procedure</title>
<sec id="sec012">
<title>Dataset</title>
<p>As for the graphical investigation of our hypotheses using Q-Q plots (see <xref ref-type="fig" rid="pone.0130316.g002">Fig 2</xref>), we use the manually annotated segmentation masks by Achanta et al. [<xref ref-type="bibr" rid="pone.0130316.ref011">11</xref>, <xref ref-type="bibr" rid="pone.0130316.ref012">12</xref>], see Sec. 3, to quantify the influence of the Gaussian center bias on salient object detection.</p>
</sec>
<sec id="sec013">
<title>Baseline algorithms</title>
<p>In order to compare our results, we use a set of saliency detection algorithms that we group into two coarse categories: First, algorithms that were specifically proposed for salient object detection and, second, algorithms that have been proposed and evaluated in other contexts. From the second category, we use: The well-known saliency model by Itti and Koch [<xref ref-type="bibr" rid="pone.0130316.ref029">29</xref>], Graph-Based Visual Saliency (GBVS) by Harel at al. [<xref ref-type="bibr" rid="pone.0130316.ref030">30</xref>], Context-Aware Saliency (CAS) by Goferman et al. [<xref ref-type="bibr" rid="pone.0130316.ref031">31</xref>, <xref ref-type="bibr" rid="pone.0130316.ref032">32</xref>], and the FFT’s spectral residuals (FFT) and DCT image signatures (DCT) by Hou et al. [<xref ref-type="bibr" rid="pone.0130316.ref033">33</xref>, <xref ref-type="bibr" rid="pone.0130316.ref034">34</xref>]. For FFT and DCT, we optimized the resolution at which the saliency maps are calculated, which is the most important algorithm parameter and has a significant influence on the performance. As baseline for salient object detection algorithms (first category), we use: The Frequency-Tuned model (FT) by Achanta et al. [<xref ref-type="bibr" rid="pone.0130316.ref011">11</xref>] (please note that an erratum regarding their reported results has been published at <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://ivrg.epfl.ch/supplementary_material/RK_CVPR09">http://ivrg.epfl.ch/supplementary_material/RK_CVPR09</ext-link>), the Bonn Information-Theoretic Saliency model (BITS) by Klein et al. [<xref ref-type="bibr" rid="pone.0130316.ref013">13</xref>], the Maximum Symmetric Surround Saliency (MSSS) model by Achanta et al. [<xref ref-type="bibr" rid="pone.0130316.ref012">12</xref>], and the Region Contrast (RC) model by Cheng et al. [<xref ref-type="bibr" rid="pone.0130316.ref014">14</xref>] that uses Felzenszwalb’s image segmentation method [<xref ref-type="bibr" rid="pone.0130316.ref035">35</xref>]. The latter two are the original algorithms we adapted.</p>
<p>Of course, we evaluate our adapted, center biased models: The maximum symmetric surround saliency with center bias (MSSS+CB; see Sec. 4.1) and the region contrast model with explicit center bias (RC+CB; see Sec. 4.1). In order to investigate the influence of the implicit center bias in the region contrast model (see Sec. 4.1), we calculate the performance of the locally debiased region contrast model without and with explicit center bias (LDRC and LDRC+CB, respectively; see Sec. 4.1). Additionally, as a reference we provide the results for the standalone segment-based and pixel-based center bias models, i.e. <italic>w</italic><sub>C</sub> = 1 (CB<sub>S</sub> and CB<sub>P</sub>, respectively).</p>
<p>If available, we used the reference implementations that have been provided by the authors. For MSSS we use the C++ implementation by Achanta, because it provides a better performance than the basic Matlab implementation. For Itti we use the iLab Neuromorphic Vision Toolkit (iNVT). We integrated the methods directly into Matlab (mex) in order to avoid quantization and/or compression artifacts that may occur due to saving and loading them as images. For DCT and FFT, we used the implementations in our publicly available Matlab toolbox [<xref ref-type="bibr" rid="pone.0130316.ref036">36</xref>]. All calculations have been made using double precision arithmetic. To make our results as reproducible as possible (we have observed that the precision-recall curves of different authors vary), we will make our implementations and evaluation scripts open source. We would like to note that our evaluation measure implementations follow the implementations of Weka and LingPipe. The corresponding precision-recall curves and results of further baseline algorithms can be seen in <xref ref-type="fig" rid="pone.0130316.g005">Fig 5</xref>.</p>
<fig id="pone.0130316.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0130316.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Precision-recall curves for all evaluated models with full (top) and limited range of the precision (bottom).</title>
<p>This graphic is best viewed in color.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pone.0130316.g005"/>
</fig>
</sec>
<sec id="sec014">
<title>Measures</title>
<p>We can use the binary segmentation masks for saliency evaluation by treating the saliency maps as binary classifiers. At a specific threshold <italic>t</italic> we regard all pixels that have a saliency value above the thresholds as positives and all pixels with values below the thresholds as negatives. By sweeping over all thresholds min(<italic>S</italic>) ≤ <italic>t</italic> ≤ max(<italic>S</italic>), we can evaluate the performance using common binary classifier evaluation measures.</p>
<p>Most commonly, precision-recall curves are used—e.g., by Achanta et al. [<xref ref-type="bibr" rid="pone.0130316.ref011">11</xref>, <xref ref-type="bibr" rid="pone.0130316.ref012">12</xref>], Cheng et al. [<xref ref-type="bibr" rid="pone.0130316.ref014">14</xref>], and Klein et al. [<xref ref-type="bibr" rid="pone.0130316.ref013">13</xref>]—to evaluate the salient object detection performance. We use five evaluation measures to quantify the performance of the algorithms. We calculate the area under curve (AUC) of the (interpolated) precision-recall curve (PR) and the receiver operating characteristic (ROC) curve [<xref ref-type="bibr" rid="pone.0130316.ref037">37</xref>]. Complementary to the PR AUC, we calculate the maximum <italic>F</italic><sub>1</sub> and <inline-formula id="pone.0130316.e016"><alternatives><graphic id="pone.0130316.e016g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0130316.e016"/><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi> <mml:msqrt><mml:mrow><mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>3</mml:mn></mml:mrow></mml:msqrt></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> scores with
<disp-formula id="pone.0130316.e017"><alternatives><graphic id="pone.0130316.e017g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0130316.e017"/><mml:math id="M17" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>F</mml:mi> <mml:mi>β</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mrow><mml:mtext>precision</mml:mtext> <mml:mo>·</mml:mo> <mml:mtext>recall</mml:mtext></mml:mrow> <mml:mrow><mml:msup><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>·</mml:mo> <mml:mtext>precision</mml:mtext> <mml:mo>+</mml:mo> <mml:mtext>recall</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(10)</label></disp-formula> <italic>F</italic><sub><italic>β</italic></sub> with <inline-formula id="pone.0130316.e018"><alternatives><graphic id="pone.0130316.e018g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0130316.e018"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:mi>β</mml:mi> <mml:mo>=</mml:mo> <mml:msqrt><mml:mrow><mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>3</mml:mn></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula> has been proposed by Achanta et al. to weight precision more than recall for salient object detection [<xref ref-type="bibr" rid="pone.0130316.ref011">11</xref>]. Additionally, we calculate the hit-rate (HR) that measures how often the pixel with the maximum saliency belongs to the salient object.</p>
</sec>
</sec>
<sec id="sec015">
<title>4.3 Quantitative Evaluation Results and Discussion</title>
<sec id="sec016">
<title>Explicit center bias integration type</title>
<p>How does the performance depend on the chosen center bias integration? To investigate this question, we tested the minimum, maximum, and product as alternative combinations. To account for the influence of different value distributions within the normalized value range, we also weighted the input of the min and max operation (e.g., <inline-formula id="pone.0130316.e019"><alternatives><graphic id="pone.0130316.e019g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0130316.e019"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi> <mml:mtext mathvariant="normal">P</mml:mtext> <mml:mrow><mml:mtext>min</mml:mtext></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mtext>min</mml:mtext> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mtext mathvariant="normal">C</mml:mtext></mml:msub> <mml:msub><mml:mi>S</mml:mi> <mml:mtext mathvariant="normal">C</mml:mtext></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mtext mathvariant="normal">B</mml:mtext></mml:msub> <mml:msub><mml:mi>S</mml:mi> <mml:mtext mathvariant="normal">B</mml:mtext></mml:msub> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>). The results of the algorithms using different combination types are shown in <xref ref-type="table" rid="pone.0130316.t001">Table 1</xref>. The presented results are the results that we achieve with the center bias weight that results in the highest <italic>F</italic><sub>1</sub> score.</p>
<table-wrap id="pone.0130316.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0130316.t001</object-id>
<label>Table 1</label>
<caption>
<title>The maximum <italic>F</italic><sub>1</sub> score, maximum <italic>F</italic><sub><italic>β</italic></sub> score, PR AUC (∫PR), ROC AUC (∫ROC), and Hit-Rate (HR) that we obtain using different combination types.</title>
</caption>
<alternatives>
<graphic id="pone.0130316.t001g" mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pone.0130316.t001"/>
<table frame="box" rules="all" border="0">
<colgroup span="1">
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1">Method</th>
<th align="left" rowspan="1" colspan="1">Combination</th>
<th align="left" rowspan="1" colspan="1"><italic>F</italic><sub>1</sub></th>
<th align="left" rowspan="1" colspan="1"><italic>F</italic><sub><italic>β</italic></sub></th>
<th align="left" rowspan="1" colspan="1">∫PR</th>
<th align="left" rowspan="1" colspan="1">∫ROC</th>
<th align="left" rowspan="1" colspan="1">HR</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">LDRC+CB</td>
<td align="left" rowspan="1" colspan="1">Linear/Convex</td>
<td char="." align="char" rowspan="1" colspan="1"><underline>0.8034</underline></td>
<td char="." align="char" rowspan="1" colspan="1"><underline>0.8183</underline></td>
<td char="." align="char" rowspan="1" colspan="1">0.8800</td>
<td char="." align="char" rowspan="1" colspan="1"><underline>0.9624</underline></td>
<td char="." align="char" rowspan="1" colspan="1">0.9240</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">LDRC+CB</td>
<td align="left" rowspan="1" colspan="1">Max</td>
<td char="." align="char" rowspan="1" colspan="1">0.7504</td>
<td char="." align="char" rowspan="1" colspan="1">0.7561</td>
<td char="." align="char" rowspan="1" colspan="1">0.8108</td>
<td char="." align="char" rowspan="1" colspan="1">0.9422</td>
<td char="." align="char" rowspan="1" colspan="1">0.8630</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">LDRC+CB</td>
<td align="left" rowspan="1" colspan="1">Min</td>
<td char="." align="char" rowspan="1" colspan="1">0.7897</td>
<td char="." align="char" rowspan="1" colspan="1">0.8049</td>
<td char="." align="char" rowspan="1" colspan="1">0.8584</td>
<td char="." align="char" rowspan="1" colspan="1">0.9535</td>
<td char="." align="char" rowspan="1" colspan="1">0.8880</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">LDRC+CB</td>
<td align="left" rowspan="1" colspan="1">Product</td>
<td char="." align="char" rowspan="1" colspan="1">0.7883</td>
<td char="." align="char" rowspan="1" colspan="1">0.8024</td>
<td char="." align="char" rowspan="1" colspan="1">0.8704</td>
<td char="." align="char" rowspan="1" colspan="1">0.9578</td>
<td char="." align="char" rowspan="1" colspan="1">0.9130</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">LDRC</td>
<td align="left" rowspan="1" colspan="1">–</td>
<td char="." align="char" rowspan="1" colspan="1">0.7574</td>
<td char="." align="char" rowspan="1" colspan="1">0.7675</td>
<td char="." align="char" rowspan="1" colspan="1">0.8302</td>
<td char="." align="char" rowspan="1" colspan="1">0.9430</td>
<td char="." align="char" rowspan="1" colspan="1">0.8680</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">RC+CB</td>
<td align="left" rowspan="1" colspan="1">Linear/Convex</td>
<td char="." align="char" rowspan="1" colspan="1">0.7973</td>
<td char="." align="char" rowspan="1" colspan="1">0.8120</td>
<td char="." align="char" rowspan="1" colspan="1">0.8833</td>
<td char="." align="char" rowspan="1" colspan="1">0.9620</td>
<td char="." align="char" rowspan="1" colspan="1">0.9340</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">RC+CB</td>
<td align="left" rowspan="1" colspan="1">Max</td>
<td char="." align="char" rowspan="1" colspan="1">0.7855</td>
<td char="." align="char" rowspan="1" colspan="1">0.7993</td>
<td char="." align="char" rowspan="1" colspan="1">0.8710</td>
<td char="." align="char" rowspan="1" colspan="1">0.9568</td>
<td char="." align="char" rowspan="1" colspan="1">0.9140</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">RC+CB</td>
<td align="left" rowspan="1" colspan="1">Min</td>
<td char="." align="char" rowspan="1" colspan="1">0.7962</td>
<td char="." align="char" rowspan="1" colspan="1">0.8150</td>
<td char="." align="char" rowspan="1" colspan="1">0.8807</td>
<td char="." align="char" rowspan="1" colspan="1">0.9603</td>
<td char="." align="char" rowspan="1" colspan="1">0.9180</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">RC+CB</td>
<td align="left" rowspan="1" colspan="1">Product</td>
<td char="." align="char" rowspan="1" colspan="1">0.7974</td>
<td char="." align="char" rowspan="1" colspan="1">0.8136</td>
<td char="." align="char" rowspan="1" colspan="1"><underline>0.8878</underline></td>
<td char="." align="char" rowspan="1" colspan="1"><underline>0.9623</underline></td>
<td char="." align="char" rowspan="1" colspan="1"><underline>0.9460</underline></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">RC</td>
<td align="left" rowspan="1" colspan="1">–</td>
<td char="." align="char" rowspan="1" colspan="1">0.7855</td>
<td char="." align="char" rowspan="1" colspan="1">0.7993</td>
<td char="." align="char" rowspan="1" colspan="1">0.8710</td>
<td char="." align="char" rowspan="1" colspan="1">0.9568</td>
<td char="." align="char" rowspan="1" colspan="1">0.9140</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">MSSS+CB</td>
<td align="left" rowspan="1" colspan="1">Linear/Convex</td>
<td char="." align="char" rowspan="1" colspan="1">0.7490</td>
<td char="." align="char" rowspan="1" colspan="1">0.7678</td>
<td char="." align="char" rowspan="1" colspan="1">0.8265</td>
<td char="." align="char" rowspan="1" colspan="1">0.9495</td>
<td char="." align="char" rowspan="1" colspan="1">0.8900</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">MSSS+CB</td>
<td align="left" rowspan="1" colspan="1">Max</td>
<td char="." align="char" rowspan="1" colspan="1">0.7165</td>
<td char="." align="char" rowspan="1" colspan="1">0.7337</td>
<td char="." align="char" rowspan="1" colspan="1">0.7849</td>
<td char="." align="char" rowspan="1" colspan="1">0.9270</td>
<td char="." align="char" rowspan="1" colspan="1">0.8420</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">MSSS+CB</td>
<td align="left" rowspan="1" colspan="1">Min</td>
<td char="." align="char" rowspan="1" colspan="1">0.7373</td>
<td char="." align="char" rowspan="1" colspan="1">0.7606</td>
<td char="." align="char" rowspan="1" colspan="1">0.8211</td>
<td char="." align="char" rowspan="1" colspan="1">0.9339</td>
<td char="." align="char" rowspan="1" colspan="1">0.9140</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">MSSS+CB</td>
<td align="left" rowspan="1" colspan="1">Product</td>
<td char="." align="char" rowspan="1" colspan="1">0.7523</td>
<td char="." align="char" rowspan="1" colspan="1">0.7748</td>
<td char="." align="char" rowspan="1" colspan="1">0.8398</td>
<td char="." align="char" rowspan="1" colspan="1">0.9445</td>
<td char="." align="char" rowspan="1" colspan="1">0.9350</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">MSSS</td>
<td align="left" rowspan="1" colspan="1">–</td>
<td char="." align="char" rowspan="1" colspan="1">0.7165</td>
<td char="." align="char" rowspan="1" colspan="1">0.7337</td>
<td char="." align="char" rowspan="1" colspan="1">0.7849</td>
<td char="." align="char" rowspan="1" colspan="1">0.9270</td>
<td char="." align="char" rowspan="1" colspan="1">0.8420</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">CB<sub>S</sub></td>
<td align="left" rowspan="1" colspan="1">–</td>
<td char="." align="char" rowspan="1" colspan="1">0.5793</td>
<td char="." align="char" rowspan="1" colspan="1">0.5764</td>
<td char="." align="char" rowspan="1" colspan="1">0.5920</td>
<td char="." align="char" rowspan="1" colspan="1">0.8623</td>
<td char="." align="char" rowspan="1" colspan="1">0.6980</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">CB<sub>P</sub></td>
<td align="left" rowspan="1" colspan="1">–</td>
<td char="." align="char" rowspan="1" colspan="1">0.5604</td>
<td char="." align="char" rowspan="1" colspan="1">0.5452</td>
<td char="." align="char" rowspan="1" colspan="1">0.5638</td>
<td char="." align="char" rowspan="1" colspan="1">0.8673</td>
<td char="." align="char" rowspan="1" colspan="1">0.7120</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>In <xref ref-type="table" rid="pone.0130316.t001">Table 1</xref>, we can see that the linear combination is the best choice for LDRC+CB. However, for MSSS+CB and RC+CB the product seems to be the combination that provides the best performance. Apparently MSSS+CB benefits more from using the product as combination type than RC+CB. Also interesting to note is that LDRC+CB with the product as combination achieves similar results to RC. However, LDRC+CB remains the algorithm that provides the best performance in terms of <italic>F</italic><sub>1</sub> score and <italic>F</italic><sub><italic>β</italic></sub> score whereas RC+CB provides the best performance in terms of PR AUC and HR. Interestingly, LDRC+CB and RC+CB achieve a nearly identical ROC AUC.</p>
</sec>
<sec id="sec017">
<title>Convex center bias weight</title>
<p>How does the weight of the center bias influence the performance? To answer this question, we calculated the performance of LDRC+CB, RC+RB, and MSSS+CB with <italic>w</italic><sub>C</sub> ∈ [0, 1] in 0.025 steps. The resulting curves of the <italic>F</italic><sub>1</sub> score, <italic>F</italic><sub><italic>β</italic></sub> score, PR AUC, ROC AUC, and hit-rate are shown in <xref ref-type="fig" rid="pone.0130316.g006">Fig 6(c), 6(a) and 6(b)</xref>, respectively.</p>
<fig id="pone.0130316.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0130316.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Illustration of the influence of the weight <italic>w</italic><sub>C</sub> on the performance of RC+CB, LDRC+CB, and MSSS+CB (convex combination).</title>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pone.0130316.g006"/>
</fig>
<p>For each of the three algorithms the values of <italic>w</italic><sub>C</sub> that lead to the optimal <italic>F</italic><sub>1</sub> score, <italic>F</italic><sub><italic>β</italic></sub> score, PR AUC, and ROC AUC lie within a small interval. In contrast, for all algorithms the value of <italic>w</italic><sub>C</sub> that achieves the highest hit-rate is outside these intervals and substantially higher. Furthermore, the best weight for each measure depends on the algorithm and varies substantially. It is interesting to see that small weights only have a minor (yet positive) influence on RC+CB until a point is reached (roughly at <italic>w</italic><sub>C</sub> = 0.55) where the performance begins to drop significantly. This becomes especially apparent when comparing the curves of RC+CB, see <xref ref-type="fig" rid="pone.0130316.g006">Fig 6(a)</xref>, with the curves of LDRC+CB, see <xref ref-type="fig" rid="pone.0130316.g006">Fig 6(c)</xref>.</p>
</sec>
<sec id="sec018">
<title>Quantitative comparison</title>
<p>The center bias itself already has a considerable predictive power, see <xref ref-type="table" rid="pone.0130316.t002">Table 2</xref>, and is relatively close to the performance of FT. However, there is a substantial performance gap between the standalone center bias models (CB<sub>S</sub> and CB<sub>P</sub>) and good non-biased methods such as, e.g., MSSS and LDRC.</p>
<table-wrap id="pone.0130316.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0130316.t002</object-id>
<label>Table 2</label>
<caption>
<title>The maximum <italic>F</italic><sub>1</sub> score, maximum <italic>F</italic><sub><italic>β</italic></sub> score, PR AUC (∫PR), ROC AUC (∫ROC), and Hit-Rate (HR) of the evaluated algorithms (sorted ascending by <italic>F</italic><sub><italic>β</italic></sub>).</title>
</caption>
<alternatives>
<graphic id="pone.0130316.t002g" mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pone.0130316.t002"/>
<table frame="box" rules="all" border="0">
<colgroup span="1">
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1">Method</th>
<th align="left" rowspan="1" colspan="1"><italic>F</italic><sub>1</sub></th>
<th align="left" rowspan="1" colspan="1"><italic>F</italic><sub><italic>β</italic></sub></th>
<th align="left" rowspan="1" colspan="1">∫PR</th>
<th align="left" rowspan="1" colspan="1">∫ROC</th>
<th align="left" rowspan="1" colspan="1">HR</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">LDRC+CB</td>
<td char="." align="char" rowspan="1" colspan="1"><underline>0.8034</underline></td>
<td char="." align="char" rowspan="1" colspan="1"><underline>0.8183</underline></td>
<td char="." align="char" rowspan="1" colspan="1">0.8800</td>
<td char="." align="char" rowspan="1" colspan="1"><underline>0.9624</underline></td>
<td char="." align="char" rowspan="1" colspan="1">0.9240</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">RC+CB</td>
<td char="." align="char" rowspan="1" colspan="1">0.7973</td>
<td char="." align="char" rowspan="1" colspan="1">0.8120</td>
<td char="." align="char" rowspan="1" colspan="1"><underline>0.8833</underline></td>
<td char="." align="char" rowspan="1" colspan="1">0.9620</td>
<td char="." align="char" rowspan="1" colspan="1"><underline>0.9340</underline></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">RC</td>
<td char="." align="char" rowspan="1" colspan="1">0.7855</td>
<td char="." align="char" rowspan="1" colspan="1">0.7993</td>
<td char="." align="char" rowspan="1" colspan="1">0.8710</td>
<td char="." align="char" rowspan="1" colspan="1">0.9568</td>
<td char="." align="char" rowspan="1" colspan="1">0.9140</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">MSSS+CB</td>
<td char="." align="char" rowspan="1" colspan="1">0.7490</td>
<td char="." align="char" rowspan="1" colspan="1">0.7678</td>
<td char="." align="char" rowspan="1" colspan="1">0.8265</td>
<td char="." align="char" rowspan="1" colspan="1">0.9495</td>
<td char="." align="char" rowspan="1" colspan="1">0.8900</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">LDRC</td>
<td char="." align="char" rowspan="1" colspan="1">0.7574</td>
<td char="." align="char" rowspan="1" colspan="1">0.7675</td>
<td char="." align="char" rowspan="1" colspan="1">0.8302</td>
<td char="." align="char" rowspan="1" colspan="1">0.9430</td>
<td char="." align="char" rowspan="1" colspan="1">0.8680</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">BITS</td>
<td char="." align="char" rowspan="1" colspan="1">0.7342</td>
<td char="." align="char" rowspan="1" colspan="1">0.7582</td>
<td char="." align="char" rowspan="1" colspan="1">0.7589</td>
<td char="." align="char" rowspan="1" colspan="1">0.9316</td>
<td char="." align="char" rowspan="1" colspan="1">0.7540</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">MSSS</td>
<td char="." align="char" rowspan="1" colspan="1">0.7165</td>
<td char="." align="char" rowspan="1" colspan="1">0.7337</td>
<td char="." align="char" rowspan="1" colspan="1">0.7849</td>
<td char="." align="char" rowspan="1" colspan="1">0.9270</td>
<td char="." align="char" rowspan="1" colspan="1">0.8420</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">FFT</td>
<td char="." align="char" rowspan="1" colspan="1">0.6455</td>
<td char="." align="char" rowspan="1" colspan="1">0.6375</td>
<td char="." align="char" rowspan="1" colspan="1">0.6593</td>
<td char="." align="char" rowspan="1" colspan="1">0.8926</td>
<td char="." align="char" rowspan="1" colspan="1">0.8080</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">DCT</td>
<td char="." align="char" rowspan="1" colspan="1">0.6472</td>
<td char="." align="char" rowspan="1" colspan="1">0.6368</td>
<td char="." align="char" rowspan="1" colspan="1">0.6612</td>
<td char="." align="char" rowspan="1" colspan="1">0.8962</td>
<td char="." align="char" rowspan="1" colspan="1">0.8270</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">GBVS</td>
<td char="." align="char" rowspan="1" colspan="1">0.6403</td>
<td char="." align="char" rowspan="1" colspan="1">0.6242</td>
<td char="." align="char" rowspan="1" colspan="1">0.6970</td>
<td char="." align="char" rowspan="1" colspan="1">0.9088</td>
<td char="." align="char" rowspan="1" colspan="1">0.8480</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">FT</td>
<td char="." align="char" rowspan="1" colspan="1">0.5995</td>
<td char="." align="char" rowspan="1" colspan="1">0.6009</td>
<td char="." align="char" rowspan="1" colspan="1">0.6261</td>
<td char="." align="char" rowspan="1" colspan="1">0.8392</td>
<td char="." align="char" rowspan="1" colspan="1">0.7100</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">CB<sub>S</sub></td>
<td char="." align="char" rowspan="1" colspan="1">0.5793</td>
<td char="." align="char" rowspan="1" colspan="1">0.5764</td>
<td char="." align="char" rowspan="1" colspan="1">0.5920</td>
<td char="." align="char" rowspan="1" colspan="1">0.8623</td>
<td char="." align="char" rowspan="1" colspan="1">0.6980</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">CAS</td>
<td char="." align="char" rowspan="1" colspan="1">0.5857</td>
<td char="." align="char" rowspan="1" colspan="1">0.5615</td>
<td char="." align="char" rowspan="1" colspan="1">0.5888</td>
<td char="." align="char" rowspan="1" colspan="1">0.8741</td>
<td char="." align="char" rowspan="1" colspan="1">0.6920</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">CB<sub>P</sub></td>
<td char="." align="char" rowspan="1" colspan="1">0.5604</td>
<td char="." align="char" rowspan="1" colspan="1">0.5452</td>
<td char="." align="char" rowspan="1" colspan="1">0.5638</td>
<td char="." align="char" rowspan="1" colspan="1">0.8673</td>
<td char="." align="char" rowspan="1" colspan="1">0.7120</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">iNVT</td>
<td char="." align="char" rowspan="1" colspan="1">0.3383</td>
<td char="." align="char" rowspan="1" colspan="1">0.4012</td>
<td char="." align="char" rowspan="1" colspan="1">0.4396</td>
<td char="." align="char" rowspan="1" colspan="1">0.5768</td>
<td char="." align="char" rowspan="1" colspan="1">0.6870</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>As could be expected, the performance of RC drops substantially if we remove the implicit center bias as is done by LDRC (see Sec. 4.1), which can best be seen in <xref ref-type="table" rid="pone.0130316.t003">Table 3</xref>. What happens if we add our explicit center bias model to unbiased models? As can be seen in the performance difference between MSSS and MSSS+CB as well as the performance difference between LDRC to LDRC+CB, the performance is substantially increased with respect to all evaluation measures, see Tables <xref ref-type="table" rid="pone.0130316.t002">2</xref> and <xref ref-type="table" rid="pone.0130316.t003">3</xref>. Interestingly, the relative performance improvement from pixel-based MSSS to MSSS+CB and segment-based LDRC to LDRC+CB is comparable, see <xref ref-type="table" rid="pone.0130316.t003">Table 3</xref>. Furthermore, with the exception of HR, the performance of LDRC+CB and RC+CB is nearly identical with a slight advantage for LDRC+CB (see Tables <xref ref-type="table" rid="pone.0130316.t002">2</xref> and <xref ref-type="table" rid="pone.0130316.t003">3</xref>). This indicates that we did not lose important information by debiasing the distance metric (LDRC+CB vs RC+CB) and that the explicit Gaussian center bias model is advantageous compared to the implicit weight bias (LDRC+CB and RC+CB vs RC).</p>
<table-wrap id="pone.0130316.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0130316.t003</object-id>
<label>Table 3</label>
<caption>
<title>Relative performance (in %) of our adapted algorithms with respect to their baseline.</title>
</caption>
<alternatives>
<graphic id="pone.0130316.t003g" mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pone.0130316.t003"/>
<table frame="box" rules="all" border="0">
<colgroup span="1">
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1">Method</th>
<th align="left" rowspan="1" colspan="1">Baseline</th>
<th align="left" rowspan="1" colspan="1"><italic>F</italic><sub>1</sub></th>
<th align="left" rowspan="1" colspan="1"><italic>F</italic><sub><italic>β</italic></sub></th>
<th align="left" rowspan="1" colspan="1">∫PR</th>
<th align="left" rowspan="1" colspan="1">∫ROC</th>
<th align="left" rowspan="1" colspan="1">HR</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">LDRC</td>
<td align="left" rowspan="1" colspan="1">RC</td>
<td char="." align="char" rowspan="1" colspan="1">96.4</td>
<td char="." align="char" rowspan="1" colspan="1">96.0</td>
<td char="." align="char" rowspan="1" colspan="1">95.3</td>
<td char="." align="char" rowspan="1" colspan="1">98.6</td>
<td char="." align="char" rowspan="1" colspan="1">95.0</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">RC+CB</td>
<td align="left" rowspan="1" colspan="1">RC</td>
<td char="." align="char" rowspan="1" colspan="1">101.5</td>
<td char="." align="char" rowspan="1" colspan="1">101.6</td>
<td char="." align="char" rowspan="1" colspan="1">101.4</td>
<td char="." align="char" rowspan="1" colspan="1">100.5</td>
<td char="." align="char" rowspan="1" colspan="1">102.2</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">LDRC+CB</td>
<td align="left" rowspan="1" colspan="1">RC</td>
<td char="." align="char" rowspan="1" colspan="1">102.3</td>
<td char="." align="char" rowspan="1" colspan="1">102.4</td>
<td char="." align="char" rowspan="1" colspan="1">101.0</td>
<td char="." align="char" rowspan="1" colspan="1">100.6</td>
<td char="." align="char" rowspan="1" colspan="1">101.1</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">LDRC+CB</td>
<td align="left" rowspan="1" colspan="1">LDRC</td>
<td char="." align="char" rowspan="1" colspan="1">106.1</td>
<td char="." align="char" rowspan="1" colspan="1">106.6</td>
<td char="." align="char" rowspan="1" colspan="1">106.0</td>
<td char="." align="char" rowspan="1" colspan="1">102.1</td>
<td char="." align="char" rowspan="1" colspan="1">106.5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">MSSS+CB</td>
<td align="left" rowspan="1" colspan="1">MSSS</td>
<td char="." align="char" rowspan="1" colspan="1">104.5</td>
<td char="." align="char" rowspan="1" colspan="1">104.7</td>
<td char="." align="char" rowspan="1" colspan="1">105.3</td>
<td char="." align="char" rowspan="1" colspan="1">102.4</td>
<td char="." align="char" rowspan="1" colspan="1">105.7</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>In summary, MSSS+CB provides a substantially higher performance than MSSS and outperforms, e.g., FT and BITS. RC+CB and LDRC+CB provide a better performance than their unbiased counterparts RC and LDRC, respectively. Furthermore, their performance is very similar and both outperform all other models. Interestingly, LDRC is the best model without center bias in our evaluation on Achanta’s data set. This makes LDRC an interesting candidate for applications in which the image data can not be expected to have a photographer’s center bias (e.g., image data of surveillance cameras, autonomous robots, or human-robot interaction [<xref ref-type="bibr" rid="pone.0130316.ref038">38</xref>]).</p>
</sec>
<sec id="sec019">
<title>Statistical significance</title>
<p>One question remains: Does the integration of an explicit center bias result in a statistically significant performance improvement? To address this question, we test the performance (i.e., <italic>F</italic><sub>1</sub>, <italic>F</italic><sub><italic>β</italic></sub>, ∫PR, and ∫ROC) of LDRC and MSSS with and without an explicit center bias. For this purpose, we rely on two pairwise, two-sample t-tests: First, we perform a two-tailed test to check whether the compared performances with and without an integrated center bias come from distributions with equal means (i.e., ℋ<sub>=</sub>: “means are equal”). Second, we perform a one-tailed test to check whether the performance with an integrated center bias is worse than without an integrated center bias, i.e. the center biased performance distribution’s mode is lower (i.e., ℋ<sub>&lt;</sub>: “mean is lower”). If we can reject both hypotheses, then it is clear that the performance of the algorithm has significantly improved due to the integrated center bias. All tests are performed at a confidence level of 95%, i.e., <italic>α</italic> = 5%.</p>
<p>For MSSS, we can reject the hypothesis of equal mean for <italic>F</italic><sub>1</sub>, <italic>F</italic><sub><italic>β</italic></sub>, ∫PR, and ∫ROC with <italic>p</italic><sub><italic>F</italic><sub>1</sub></sub> = 0.0285, <italic>p</italic><sub><italic>F</italic><sub><italic>β</italic></sub></sub> = 0.0031, <italic>p</italic><sub>∫PR</sub> = 5.252 × 10<sup>−7</sup>, and <italic>p</italic><sub>∫ROC</sub> = 2.618 × 10<sup>−16</sup>, respectively. Additionally, we can reject the hypothesis that an integrated center bias has a negative influence on the performance with <italic>p</italic><sub><italic>F</italic><sub>1</sub></sub> = 0.0142, <italic>p</italic><sub><italic>F</italic><sub><italic>β</italic></sub></sub> = 0.0015, <italic>p</italic><sub>∫PR</sub> = 2.626 × 10<sup>−7</sup>, and <italic>p</italic><sub>∫ROC</sub> = 1.309 × 10<sup>−16</sup>.</p>
<p>Similarly, we can reject the hypothesis that the performance of LDRC with and without center bias has an equal mean for <italic>F</italic><sub>1</sub>, <italic>F</italic><sub><italic>β</italic></sub>, ∫PR, and ∫ROC with <italic>p</italic><sub><italic>F</italic><sub>1</sub></sub> = 0.0018, <italic>p</italic><sub><italic>F</italic><sub><italic>β</italic></sub></sub> = 2.426 × 10<sup>−5</sup>, <italic>p</italic><sub>∫PR</sub> = 1.118 × 10<sup>−7</sup>, and <italic>p</italic><sub>∫ROC</sub> = 1.555 × 10<sup>−5</sup>, respectively. And, we can reject the hypothesis that an integrated center bias has a negative influence on the performance with <italic>p</italic><sub><italic>F</italic><sub>1</sub></sub> = 9.071 × 10<sup>−4</sup>, <italic>p</italic><sub><italic>F</italic><sub><italic>β</italic></sub></sub> = 1.213 × 10<sup>−5</sup>, <italic>p</italic><sub>∫PR</sub> = 5.590 × 10<sup>−8</sup>, and <italic>p</italic><sub>∫ROC</sub> = 7.773 × 10<sup>−6</sup>.</p>
<p>Consequently, it is apparent that the integration of a center bias can lead to statistically significant performance improvements for pixel-based as well as segmentation-based algorithms.</p>
</sec>
</sec>
</sec>
<sec id="sec020" sec-type="conclusions">
<title>5 Conclusion</title>
<p>We formulated and investigated two hypotheses about the location of salient objects in photographs: First, the radial centroid distribution around the image center is uniform. Second, the distances between their centroids and the image center follow a normal distribution. We investigated these hypotheses using graphical methods, which indicate that our hypotheses are true. This is an important insight, because it provides a strong empirical motivation and justification for the widely applied Gaussian center bias models. To investigate the influence of the center bias on salient object detection, we explicitly integrated the center bias model in two state-of-the-art salient object detection algorithms. We have shown that the explicitly modeled center bias has a significant, positive influence on the performance (in terms of hit-rate, the area under the precision-recall curve, the area under the receiver operating characteristic curve, the <italic>F</italic><sub>1</sub> score, and the <italic>F</italic><sub><italic>β</italic></sub> score). Last but not least, by debiasing Cheng et al.’s region contrast model, we have exemplarily shown that implicit center biases might at least partially be responsible for the performance of state-of-the-art salient object detection algorithms and as a consequence we introduced an adapted, non-biased salient object detection algorithm.</p>
</sec>
</body>
<back>
<ack>
<p>We acknowledge support by Deutsche Forschungsgemeinschaft and Open Access Publishing Fund of Karlsruhe Institute of Technology.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0130316.ref001">
<label>1</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Einhäuser</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Spain</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Perona</surname> <given-names>P</given-names></name>. <article-title>Objects predict fixations better than early saliency</article-title>. <source>Journal of Vision</source>. <year>2008</year>;<volume>8</volume>(<issue>14</issue>).</mixed-citation>
</ref>
<ref id="pone.0130316.ref002">
<label>2</label>
<mixed-citation xlink:type="simple" publication-type="other">Yang, Y, Song M, Li N, Bu J, Chen C. What is the chance of happening: a new way to predict where people look. In: Proc. European Conf. Comp. Vis.; 2010.</mixed-citation>
</ref>
<ref id="pone.0130316.ref003">
<label>3</label>
<mixed-citation xlink:type="simple" publication-type="other">Judd T, Ehinger K, Durand F, Torralba A. Learning to Predict Where Humans Look. In: Proc. Int. Conf. Comp. Vis.; 2009.</mixed-citation>
</ref>
<ref id="pone.0130316.ref004">
<label>4</label>
<mixed-citation xlink:type="simple" publication-type="other">Schauerte B, Stiefelhagen R. Predicting Human Gaze using Quaternion DCT Image Signature Saliency and Face Detection. In: Proc. Workshop on the Applications of Computer Vision; 2012.</mixed-citation>
</ref>
<ref id="pone.0130316.ref005">
<label>5</label>
<mixed-citation xlink:type="simple" publication-type="other">Liu T, Sun J, et al. Learning to Detect A Salient Object. In: Proc. Int. Conf. Comp. Vis. Pat. Rec.; 2007.</mixed-citation>
</ref>
<ref id="pone.0130316.ref006">
<label>6</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tseng</surname> <given-names>PH</given-names></name>, <name name-style="western"><surname>Carmi</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Cameron</surname> <given-names>IGM</given-names></name>, <name name-style="western"><surname>Munoz</surname> <given-names>DP</given-names></name>, <name name-style="western"><surname>Itti</surname> <given-names>L</given-names></name>. <article-title>Quantifying center bias of observers in free viewing of dynamic natural scenes</article-title>. <source>Journal of Vision</source>. <year>2009</year>;<volume>9</volume>(<issue>7</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/9.7.4" xlink:type="simple">10.1167/9.7.4</ext-link></comment> <object-id pub-id-type="pmid">19761319</object-id></mixed-citation>
</ref>
<ref id="pone.0130316.ref007">
<label>7</label>
<mixed-citation xlink:type="simple" publication-type="other">Reinagel P, Zador AM. Natural Scene Statistics At the Centre of Gaze. In: Network: Computation in Neural Systems; 1999. p. 341–350.</mixed-citation>
</ref>
<ref id="pone.0130316.ref008">
<label>8</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Parkhurst</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Niebur</surname> <given-names>E</given-names></name>. <article-title>Scene content selected by active vision</article-title>. <source>Spatial Vision</source>. <year>2003</year>;<volume>16</volume>(<issue>2</issue>):<fpage>125</fpage>–<lpage>154</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1163/15685680360511645" xlink:type="simple">10.1163/15685680360511645</ext-link></comment> <object-id pub-id-type="pmid">12696858</object-id></mixed-citation>
</ref>
<ref id="pone.0130316.ref009">
<label>9</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tatler</surname> <given-names>BW</given-names></name>. <article-title>The central fixation bias in scene viewing: Selecting an optimal viewing position independently of motor biases and image feature distributions</article-title>. <source>Journal of Vision</source>. <year>2007</year>;<volume>7</volume>(<issue>14</issue>). Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://www.journalofvision.org/content/7/14/4.abstract">http://www.journalofvision.org/content/7/14/4.abstract</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/7.14.4" xlink:type="simple">10.1167/7.14.4</ext-link></comment> <object-id pub-id-type="pmid">18217799</object-id></mixed-citation>
</ref>
<ref id="pone.0130316.ref010">
<label>10</label>
<mixed-citation xlink:type="simple" publication-type="other">Borji A, Sihite DN, Itti L. Probabilistic Learning of Task-Specific Visual Attention. In: Proc. Int. Conf. Comp. Vis. Pat. Rec.; 2012.</mixed-citation>
</ref>
<ref id="pone.0130316.ref011">
<label>11</label>
<mixed-citation xlink:type="simple" publication-type="other">Achanta R, Hemami S, Estrada F, Süsstrunk S. Frequency-tuned Salient Region Detection. In: Proc. Int. Conf. Comp. Vis. Pat. Rec.; 2009.</mixed-citation>
</ref>
<ref id="pone.0130316.ref012">
<label>12</label>
<mixed-citation xlink:type="simple" publication-type="other">Achanta R, Süsstrunk S. Saliency detection using maximum symmetric surround. In: Proc. Int. Conf. Image Process.; 2010.</mixed-citation>
</ref>
<ref id="pone.0130316.ref013">
<label>13</label>
<mixed-citation xlink:type="simple" publication-type="other">Klein DA, Frintrop S. Center-surround Divergence of Feature Statistics for Salient Object Detection. In: Proc. Int. Conf. Comp. Vis.; 2011.</mixed-citation>
</ref>
<ref id="pone.0130316.ref014">
<label>14</label>
<mixed-citation xlink:type="simple" publication-type="other">Cheng MM, Zhang GX, Mitra NJ, Huang X, Hu SM. Global Contrast based Salient Region Detection. In: Proc. Int. Conf. Comp. Vis. Pat. Rec.; 2011.</mixed-citation>
</ref>
<ref id="pone.0130316.ref015">
<label>15</label>
<mixed-citation xlink:type="simple" publication-type="other">Jiang H, Wang J, Yuan Z, Liu T, Zheng N. Automatic salient object segmentation based on context and shape prior. In: Proc. British Mach. Vis. Conf.; 2011.</mixed-citation>
</ref>
<ref id="pone.0130316.ref016">
<label>16</label>
<mixed-citation xlink:type="simple" publication-type="other">Luo Y, Tang X. Photo and Video Quality Evaluation: Focusing on the Subject. In: Proc. European Conf. Comp. Vis.; 2008.</mixed-citation>
</ref>
<ref id="pone.0130316.ref017">
<label>17</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Tsotsos</surname> <given-names>JK</given-names></name>. <source>A Computational Perspective on Visual Attention</source>. <publisher-name>The MIT Press</publisher-name>; <year>2011</year>.</mixed-citation>
</ref>
<ref id="pone.0130316.ref018">
<label>18</label>
<mixed-citation xlink:type="simple" publication-type="other">Alexe B, Deselaers T, Ferrari V. “What is an object?”. In: Proc. Int. Conf. Comp. Vis. Pat. Rec.; 2010.</mixed-citation>
</ref>
<ref id="pone.0130316.ref019">
<label>19</label>
<mixed-citation xlink:type="simple" publication-type="other">Itti L, Baldi PF. Bayesian Surprise Attracts Human Attention. In: Advances in Neural Information Processing Systems; 2006.</mixed-citation>
</ref>
<ref id="pone.0130316.ref020">
<label>20</label>
<mixed-citation xlink:type="simple" publication-type="other">Borji A, Sihite DN, Itti L. Salient Object Detection: A Benchmark. In: Proc. European Conf. Comp. Vis.; 2012.</mixed-citation>
</ref>
<ref id="pone.0130316.ref021">
<label>21</label>
<mixed-citation xlink:type="simple" publication-type="other">Scharfenberger C, Wong A, Fergani K, Zelek JS, Clausi DA. Statistical Textural Distinctiveness for Salient Region Detection in Natural Images. In: Proc. Int. Conf. Comp. Vis. Pat. Rec.; 2013.</mixed-citation>
</ref>
<ref id="pone.0130316.ref022">
<label>22</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Busswell</surname> <given-names>GT</given-names></name>. <source>How people look at pictures: A study of the psychology of perception in art</source>. <publisher-name>University of Chicago Press</publisher-name>; <year>1935</year>.</mixed-citation>
</ref>
<ref id="pone.0130316.ref023">
<label>23</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Parkhurst</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Law</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Niebur</surname> <given-names>E</given-names></name>. <article-title>Modeling the role of salience in the allocation of overt visual attention</article-title>. <source>Vision Research</source>. <year>2002</year>;<volume>42</volume>(<issue>1</issue>):<fpage>107</fpage>–<lpage>123</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0042-6989(01)00250-4" xlink:type="simple">10.1016/S0042-6989(01)00250-4</ext-link></comment> <object-id pub-id-type="pmid">11804636</object-id></mixed-citation>
</ref>
<ref id="pone.0130316.ref024">
<label>24</label>
<mixed-citation xlink:type="simple" publication-type="other">NIST/SEMATECH. Engineering Statistics Handbook; 2012.</mixed-citation>
</ref>
<ref id="pone.0130316.ref025">
<label>25</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Vogel</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Kroll</surname> <given-names>CN</given-names></name>. <article-title>Low-Flow Frequency Analysis Using Probability-Plot Correlation Coefficients</article-title>. <source>Journal of Water Resources Planning and Management</source>. <year>1989</year>;<volume>115</volume>(<issue>3</issue>):<fpage>338</fpage>–<lpage>357</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1061/(ASCE)0733-9496(1989)115:3(338)" xlink:type="simple">10.1061/(ASCE)0733-9496(1989)115:3(338)</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0130316.ref026">
<label>26</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Shapiro</surname> <given-names>SS</given-names></name>, <name name-style="western"><surname>Wilk</surname> <given-names>MB</given-names></name>. <article-title>An analysis of variance test for normality (complete samples)</article-title>. <source>Biometrika</source>. <year>1965</year>;<volume>52</volume>:<fpage>591</fpage>–<lpage>611</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2307/2333709" xlink:type="simple">10.2307/2333709</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0130316.ref027">
<label>27</label>
<mixed-citation xlink:type="simple" publication-type="other">Schauerte B, Stiefelhagen R. How the Distribution of Salient Objects in Images Influences Salient Object Detection. In: Proc. Int. Conf. Image Process.; 2013.</mixed-citation>
</ref>
<ref id="pone.0130316.ref028">
<label>28</label>
<mixed-citation xlink:type="simple" publication-type="other">Schauerte B, Richarz J, Plötz T, Thurau C, Fink GA. Multi-modal and multi-camera attention in smart environments. In: Proc. Int. Conf. Multimodal Interfaces; 2009.</mixed-citation>
</ref>
<ref id="pone.0130316.ref029">
<label>29</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Itti</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Niebur</surname> <given-names>E</given-names></name>. <article-title>A Model of Saliency-Based Visual Attention for Rapid Scene Analysis</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>1998</year>;<volume>20</volume>(<issue>11</issue>):<fpage>1254</fpage>–<lpage>1259</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/34.730558" xlink:type="simple">10.1109/34.730558</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0130316.ref030">
<label>30</label>
<mixed-citation xlink:type="simple" publication-type="other">Harel J, Koch C, Perona P. Graph-based visual saliency. In: Advances in Neural Information Processing Systems; 2007.</mixed-citation>
</ref>
<ref id="pone.0130316.ref031">
<label>31</label>
<mixed-citation xlink:type="simple" publication-type="other">Goferman S, Zelnik-Manor L, Tal A. Context-aware saliency detection. In: Proc. Int. Conf. Comp. Vis. Pat. Rec.; 2010.</mixed-citation>
</ref>
<ref id="pone.0130316.ref032">
<label>32</label>
<mixed-citation xlink:type="simple" publication-type="other">Goferman S, Zelnik-Manor L, Tal A. Context-Aware Saliency Detection. IEEE Trans Pattern Anal Mach Intell. 2012;.</mixed-citation>
</ref>
<ref id="pone.0130316.ref033">
<label>33</label>
<mixed-citation xlink:type="simple" publication-type="other">Hou X, Zhang L. Saliency Detection: A Spectral Residual Approach. In: Proc. Int. Conf. Comp. Vis. Pat. Rec.; 2007.</mixed-citation>
</ref>
<ref id="pone.0130316.ref034">
<label>34</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hou</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Harel</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>. <article-title>Image Signature: Highlighting Sparse Salient Regions</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2012</year>;<volume>34</volume>(<issue>1</issue>):<fpage>194</fpage>–<lpage>201</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TPAMI.2011.146" xlink:type="simple">10.1109/TPAMI.2011.146</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0130316.ref035">
<label>35</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Felzenszwalb</surname> <given-names>PF</given-names></name>, <name name-style="western"><surname>Huttenlocher</surname> <given-names>DP</given-names></name>. <article-title>Efficient Graph-Based Image Segmentation</article-title>. <source>Int J Comput Vision</source>. <year>2004</year>;<volume>59</volume>:<fpage>167</fpage>–<lpage>181</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/B:VISI.0000022288.19776.77" xlink:type="simple">10.1023/B:VISI.0000022288.19776.77</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0130316.ref036">
<label>36</label>
<mixed-citation xlink:type="simple" publication-type="other">Schauerte B. Spectral Visual Saliency Toolbox (SViST); 2011. Available from: <ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://bit.ly/RAPmMk">http://bit.ly/RAPmMk</ext-link>.</mixed-citation>
</ref>
<ref id="pone.0130316.ref037">
<label>37</label>
<mixed-citation xlink:type="simple" publication-type="other">Davis J, Goadrich M. The relationship between Precision-Recall and ROC curves. In: Proc. Int. Conf. Machine Learning; 2006.</mixed-citation>
</ref>
<ref id="pone.0130316.ref038">
<label>38</label>
<mixed-citation xlink:type="simple" publication-type="other">Schauerte B, Stiefelhagen R. Look at this! Learning to Guide Visual Saliency in Human-Robot Interaction. In: Proc. Int. Conf. Intell. Robots Syst.; 2014.</mixed-citation>
</ref>
</ref-list>
</back>
</article>