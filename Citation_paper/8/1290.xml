<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-17-16094</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0184683</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Data visualization</subject><subj-group><subject>Phase diagrams</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Recall (memory)</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Recall (memory)</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject><subj-group><subject>Scale-free networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Artificial neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Artificial neural networks</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Artificial neural networks</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Enhanced storage capacity with errors in scale-free Hopfield neural networks: An analytical study</article-title>
<alt-title alt-title-type="running-head">Enhanced storage capacity with errors in scale-free Hopfield neural networks</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8384-668X</contrib-id>
<name name-style="western">
<surname>Kim</surname> <given-names>Do-Hyun</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Park</surname> <given-names>Jinha</given-names></name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Kahng</surname> <given-names>Byungnam</given-names></name>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Department of Physics, Sogang University, Seoul, Korea</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>CCSS, CTP and Department of Physics and Astronomy, Seoul National University, Seoul, Korea</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Dovrolis</surname> <given-names>Constantine</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Georgia Institute of Technology, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">dohyunkim@sogang.ac.kr</email> (D-HK); <email xlink:type="simple">bkahng@snu.ac.kr</email> (BK)</corresp>
</author-notes>
<pub-date pub-type="collection">
<year>2017</year>
</pub-date>
<pub-date pub-type="epub">
<day>27</day>
<month>10</month>
<year>2017</year>
</pub-date>
<volume>12</volume>
<issue>10</issue>
<elocation-id>e0184683</elocation-id>
<history>
<date date-type="received">
<day>11</day>
<month>5</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>29</day>
<month>8</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Kim et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0184683"/>
<abstract>
<p>The Hopfield model is a pioneering neural network model with associative memory retrieval. The analytical solution of the model in mean field limit revealed that memories can be retrieved without any error up to a finite storage capacity of <italic>O</italic>(<italic>N</italic>), where <italic>N</italic> is the system size. Beyond the threshold, they are completely lost. Since the introduction of the Hopfield model, the theory of neural networks has been further developed toward realistic neural networks using analog neurons, spiking neurons, etc. Nevertheless, those advances are based on fully connected networks, which are inconsistent with recent experimental discovery that the number of connections of each neuron seems to be heterogeneous, following a heavy-tailed distribution. Motivated by this observation, we consider the Hopfield model on scale-free networks and obtain a different pattern of associative memory retrieval from that obtained on the fully connected network: the storage capacity becomes tremendously enhanced but with some error in the memory retrieval, which appears as the heterogeneity of the connections is increased. Moreover, the error rates are also obtained on several real neural networks and are indeed similar to that on scale-free model networks.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100003725</institution-id>
<institution>National Research Foundation of Korea</institution>
</institution-wrap>
</funding-source>
<award-id>2014R1A3A2069005</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Kahng</surname> <given-names>Byungnam</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100003725</institution-id>
<institution>National Research Foundation of Korea</institution>
</institution-wrap>
</funding-source>
<award-id>2015R1A5A7037676</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Kahng</surname> <given-names>Byungnam</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100002646</institution-id>
<institution>Sogang University</institution>
</institution-wrap>
</funding-source>
<award-id>201610033.01</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8384-668X</contrib-id>
<name name-style="western">
<surname>Kim</surname> <given-names>Do-Hyun</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100002646</institution-id>
<institution>Sogang University</institution>
</institution-wrap>
</funding-source>
<award-id>201710066.01</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8384-668X</contrib-id>
<name name-style="western">
<surname>Kim</surname> <given-names>Do-Hyun</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was supported by the National Research Foundation of Korea (<ext-link ext-link-type="uri" xlink:href="http://www.nrf.re.kr" xlink:type="simple">http://www.nrf.re.kr</ext-link>): Grant Nos. 2014R1A3A2069005 and 2015R1A5A7037676 to B. Kahng and by Sogang University (<ext-link ext-link-type="uri" xlink:href="http://www.sogang.ac.kr" xlink:type="simple">http://www.sogang.ac.kr</ext-link>): Grant No. 201610033.01 and 201710066.01 to D.-H. Kim. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="2"/>
<table-count count="0"/>
<page-count count="12"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>We construct several small-size neural networks based on the connectivity network data sets (<ext-link ext-link-type="uri" xlink:href="https://sites.google.com/site/bctnet/datasets" xlink:type="simple">https://sites.google.com/site/bctnet/datasets</ext-link>), on which numerical simulations of the Hopfield model have been performed.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Human neuroscience has attracted increasing attention through various studies. Among such activities, the retrieval or recall of associative memory in neural networks is a historically noticeable issue [<xref ref-type="bibr" rid="pone.0184683.ref001">1</xref>, <xref ref-type="bibr" rid="pone.0184683.ref002">2</xref>]. Associative memory means the ability to link, learn and remember the relationship between independent and unrelated items such as one man’s name (e.g., Albert Einstein) and his previous achievement (e.g., <italic>E</italic> = <italic>mc</italic><sup>2</sup>). Neural network models of associative memory have been used to explain how the brain stores and recalls long-term memories. These models incorporate the so-called Hebbian rule for a cell assembly, a group of excitatory neurons mutually coupled by strong synapses [<xref ref-type="bibr" rid="pone.0184683.ref003">3</xref>]: Memory storage occurs when a cell assembly is created by Hebbian synaptic plasticity, and memory retrieval or recall occurs when the neurons in the cell assembly are activated by a stimulus. Neural network models of associative memory assume that information exists alternatively as neural activity or as synaptic connections. When novel information first enters into the brain, it is encoded in a pattern of neural activity. If this information is stored as memory, the neural activity leaves synaptic connections modified. The stored information can be retrieved when the modified connections again become active [<xref ref-type="bibr" rid="pone.0184683.ref004">4</xref>].</p>
<p>A Hopfield model was introduced in the year 1982 [<xref ref-type="bibr" rid="pone.0184683.ref005">5</xref>], following which there have been enormous researches based on the premise that retrieval of associative memory occurs by way of pattern recognition in collective excitations of associated neurons. The Hopfield model has been thus accepted as a paradigmatic neural network model for associative memory retrieval. A Hopfield network is composed of Ising-type neurons with two discrete states, that is, an excitation pattern of each neuron is in a state either +1 or −1, representing excited and rest states for transmitting or not transmitting a signal, respectively. Each neuron is supposed to be connected to all other neurons and then the synaptic weight is updated by the Hebbian rule. An associative memory was introduced as an activity pattern by collective excitations of the associated neurons. A simple example of excitation updated by the Hebbian rule is presented in Section 1 of the supporting information (<xref ref-type="supplementary-material" rid="pone.0184683.s001">S1 File</xref>).</p>
<p>In this model study, a mathematical quantity called <italic>energy</italic> was introduced to each memory pattern. This quantity decreases as the retrieval activity proceeeds until the system reaches a stable state, at which the retrieved pattern is consistent to a stored pattern. We will show why such behavior occurs in the Hopfield model in Section 2 of the <xref ref-type="supplementary-material" rid="pone.0184683.s001">S1 File</xref>. This behavior is similar to the dynamic process of a thermodynamic system toward an equilibrium state, at which a free energy becomes minimum.</p>
<p>In associative neural networks, the number of patterns that can be stored in synaptic connections is a central quantity to calibrate their performance. The maximal number of patterns that can be stored before having total confusion divided by the total number of neurons in the network is called the <italic>storage capacity</italic> of the network. If the cell assemblies share no neurons in common, the number of patterns that can be stored is as many as the total number of neurons in the network. If the cell assemblies share some neurons, however, interference may occur among those cell assemblies. If too many patterns are stored in the network, the stored patterns can be interfered and the quality of memory recall can be lowered. Therefore, interference effect induces errors in memory retrieval and reduces the retrievability [<xref ref-type="bibr" rid="pone.0184683.ref004">4</xref>]. Furthermore, stochastic processes in any real network system can occur. We thus consider a temperature <italic>T</italic>, which is not to be understood as a physical temperature, but as a noise strength for the stochastic process. In the limit of zero temperature (<italic>T</italic> = 0), the deterministic model is recovered. The Hopfield model treated such noise effect in terms of temperature, and invoke the formalism of Boltzmann statistical mechanics to obtain various properties of the retrievability as a function of both the number of stored patterns and temperature, i.e., noise strength. Amit <italic>et al</italic>. calculated the storage capacity of the Hopfield model on a fully connected network at finite temperature using a method in statistical physics [<xref ref-type="bibr" rid="pone.0184683.ref006">6</xref>, <xref ref-type="bibr" rid="pone.0184683.ref007">7</xref>].</p>
<p>Recent experimental results using the functional magnetic resonance imaging (fMRI) technique have revealed that functional neural networks in resting state are not fully connected networks but the number of connections of each coarse-grained neuron are heterogeneous following a heavy-tailed distribution [<xref ref-type="bibr" rid="pone.0184683.ref008">8</xref>, <xref ref-type="bibr" rid="pone.0184683.ref009">9</xref>]. For further discussion, the number of connections of a neuron is referred to as degree using a term in graph theory. Even though it is not clear yet how functional connections of an individual neuron is related to its anatomical connections, it becomes more acceptable that neurons with similar patterns of connection tend to exhibit similar functions [<xref ref-type="bibr" rid="pone.0184683.ref008">8</xref>]. This suggests that neural networks need not necessarily be a fully connected network. Indeed, there exists supporting evidences: a structural neural network of the worm <italic>Caenorhabditis elegans</italic> has a power-law tail in the degree distribution [<xref ref-type="bibr" rid="pone.0184683.ref010">10</xref>]. Mathematically, this is expressed as <italic>P</italic><sub><italic>d</italic></sub>(<italic>k</italic>) ∼ <italic>k</italic><sup>−<italic>γ</italic></sup>, where <italic>P</italic><sub><italic>d</italic></sub>(<italic>k</italic>) is the degree distribution, <italic>k</italic> denotes degree and <italic>γ</italic> is the degree exponent. Such networks are referred to as scale-free (SF) networks. Recent electrophysiological data [<xref ref-type="bibr" rid="pone.0184683.ref011">11</xref>] also revealed that the distribution of synaptic connections strength follows a log-normal distribution, which has a heavy tail similar to that in the power-law distribution. Moreover, it is known that the brain damages of schizophrenia [<xref ref-type="bibr" rid="pone.0184683.ref012">12</xref>, <xref ref-type="bibr" rid="pone.0184683.ref013">13</xref>] or comatose patients [<xref ref-type="bibr" rid="pone.0184683.ref014">14</xref>] are caused by the malfunctioning of hub neurons. Thus, hub neurons that have many connections is likely to exist in a brain network.</p>
<p>This paper aims to investigate the properties of the retrieval patterns created by the Hopfield model on SF networks at finite temperature analytically. We also compare obtained results with previous numerical results obtained from fully connected networks and diverse SF networks such as the Barabási-Albert model with <italic>γ</italic> = 3 [<xref ref-type="bibr" rid="pone.0184683.ref015">15</xref>] and the Molloy-Reed model with several values of <italic>γ</italic> [<xref ref-type="bibr" rid="pone.0184683.ref016">16</xref>]. We also use the Chung-Lu model [<xref ref-type="bibr" rid="pone.0184683.ref017">17</xref>, <xref ref-type="bibr" rid="pone.0184683.ref018">18</xref>] to construct uncorrelated SF networks over the entire range of <italic>γ</italic>. The details of the Chung-Lu model are presented in the Method section. Particularly we consider the limit <italic>γ</italic> → 2, which is the case observed in the fMRI data [<xref ref-type="bibr" rid="pone.0184683.ref019">19</xref>–<xref ref-type="bibr" rid="pone.0184683.ref021">21</xref>].</p>
<p>The main results of our studies are presented in Figs <xref ref-type="fig" rid="pone.0184683.g001">1</xref> and <xref ref-type="fig" rid="pone.0184683.g002">2</xref>. In <xref ref-type="fig" rid="pone.0184683.g001">Fig 1</xref>, we present properties of the retrieval pattern for various degree exponents <italic>γ</italic> as a fuction of temperature <italic>T</italic> and storage rate <italic>a</italic>. When <italic>γ</italic> &lt; 2.04 (<xref ref-type="fig" rid="pone.0184683.g001">Fig 1(e) and 1(f)</xref>), the retrieval phase spans most of the low-temperature (noise) region; thus memory retrieval in the system is appreciably enhanced compared with the one of the original Hopfield model [<xref ref-type="bibr" rid="pone.0184683.ref006">6</xref>, <xref ref-type="bibr" rid="pone.0184683.ref007">7</xref>]. In <xref ref-type="fig" rid="pone.0184683.g002">Fig 2</xref>, the error rate, a fraction of neurons which fails memory retrieval, is obtained as a function of storage rate <italic>a</italic> ≡ <italic>p</italic>/<italic>N</italic>, the number of stored patterns per neuron, at zero temperature <italic>T</italic> = 0. Remarkably when <italic>γ</italic> = 2.01, the error rate is almost zero when the number of stored patterns <italic>p</italic> is small, and gradually increases but is less than 0.3 even when <italic>p</italic> is increased to the total number of neurons <italic>N</italic>. This implies that the storage capacity becomes tremendously enhanced as the SF network becomes extremely heterogeneous in structure, but there occur some errors. We remark that the previous solution of the Hopfield model on fully connected networks [<xref ref-type="bibr" rid="pone.0184683.ref006">6</xref>, <xref ref-type="bibr" rid="pone.0184683.ref007">7</xref>] revealed that the error rate is zero up to a certain threshold, but beyond which it becomes one-half and the system falls into a total confusion state. Based on these results, we think that the solution of the Hopfield model on SF networks reflects more a normal brain in the point that it provides the case of imperfect memory retrieval with some error even when its storage capacity is small. Moreover, the result is timely in accord with a recent experimental discovery that storage capacity of brain is in the petabyte range, as much as entire web, ten times more than previously thought [<xref ref-type="bibr" rid="pone.0184683.ref022">22</xref>]. We hope that our result will provide some theoretical development for modeling associative memory networks in neuroscience.</p>
<fig id="pone.0184683.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0184683.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Phase diagram of the Hopfield model in the plane of (<italic>T</italic>, <italic>a</italic>).</title>
<p>Here <italic>T</italic> and <italic>a</italic> denote temperature and storage rate, respectively. Degree exponent <italic>γ</italic> is infinity in <bold>a</bold>, 5.0 in <bold>b</bold>, 4.0 in <bold>c</bold>, 3.0 in <bold>d</bold>, 2.04 in <bold>e</bold>, and 2.01 in <bold>f</bold>. <bold>P</bold> represents the paramagnetic phase, in which <italic>m</italic> = 0, <italic>q</italic> = 0, and <italic>r</italic> = 0 because of thermal fluctuations. Here, <italic>m</italic>, <italic>q</italic>, and <italic>r</italic> are given by Eqs (29-31) of the <xref ref-type="supplementary-material" rid="pone.0184683.s001">S1 File</xref>, respectively. <bold>SG</bold> does the spin-glass phase, in which <italic>m</italic> = 0, <italic>q</italic> &gt; 0, and <italic>r</italic> &gt; 0. In the P and SG phases, the retrieval of stored patterns is impossible. Thus, they are often referred to as the confusion phase. The retrieval phase is denoted as <bold>R</bold>, in which <italic>m</italic> &gt; 0, <italic>q</italic> &gt; 0, and <italic>r</italic> &gt; 0. The retrieval of stored memory is possible. Finally, <bold>M</bold> does the mixed phase, in which the features of both the retrieval and the spin-glass phases coexist. As the degree exponent <italic>γ</italic> is decreased from infinity in <bold>a</bold> through <italic>γ</italic> = 2.01 in <bold>f</bold>, the retrieval phase not only intrudes into the region of the SG phase, but also raises the boundary of the phase <bold>P</bold> to a higher temperature region. Eventually the SG phase remains on the <italic>T</italic> = 0 axis when <italic>γ</italic> = <italic>γ</italic><sub><italic>c</italic></sub> ≃ 2.04, in which the phase <bold>R</bold> spans most of the low-temperature region. Thus, memory retrieval is enhanced. The phase boundary was obtained by performing numerical calculations for the Chung-Lu SF networks with the system size <italic>N</italic> = 1000 and mean degree <italic>K</italic> = 5.0. Solid and dotted lines or curves indicate the second-order and first-order transitions, respectively. We note that the case <bold>a</bold> on ER network is nearly the same as that in mean field limit obtained in the original Hopfield model.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0184683.g001" xlink:type="simple"/>
</fig>
<fig id="pone.0184683.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0184683.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Conceptual figures of the storage capacities and the error rates.</title>
<p><bold>a</bold> for an ER random network and <bold>c</bold> for a SF network. <bold>b</bold> and <bold>d</bold> Plot of the error rate <italic>n</italic><sub><italic>e</italic></sub> ≡ (1 − <italic>m</italic>)/2 vs storage rate <italic>a</italic> for several <italic>γ</italic> values of the Chung-Lu model at <italic>T</italic> = 0. Here, numerical values are obtained using <italic>N</italic> = 1000 and <italic>K</italic> = 5.0. The dotted lines for <italic>γ</italic> ≫ 2.0 indicate the sudden jumps from small error rates to the state of <italic>n</italic><sub><italic>e</italic></sub> = 0.5. (<bold>a</bold> and <bold>c</bold>, Figure courtesy of Joonwon Lee.)</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0184683.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Model development</title>
<sec id="sec004">
<title>Hopfield model on a given network</title>
<p>In the Hopfield model, each neuron at node <italic>i</italic> of a given neural network (denoted as <italic>G</italic>) has an Ising spin <italic>S</italic><sub><italic>i</italic></sub> with two states, <italic>S</italic><sub><italic>i</italic></sub> = +1 and <italic>S</italic><sub><italic>i</italic></sub> = −1 representing excited and rest states for transmitting or not transmitting a signal, respectively [<xref ref-type="bibr" rid="pone.0184683.ref005">5</xref>]. The Hamiltonian (corresponding to energy of the system) is introduced as
<disp-formula id="pone.0184683.e001"><alternatives><graphic id="pone.0184683.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="script">H</mml:mi> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo> <mml:mo>∈</mml:mo> <mml:mi>G</mml:mi></mml:mrow></mml:munder> <mml:msub><mml:mi>J</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>S</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>S</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where the coupling strength <italic>J</italic><sub><italic>ij</italic></sub> between two connected nodes (<italic>i</italic>, <italic>j</italic>) ∈ <italic>G</italic>, known as the synapse efficacy, takes the Hebbian form,
<disp-formula id="pone.0184683.e002"><alternatives><graphic id="pone.0184683.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>J</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>K</mml:mi></mml:mfrac> <mml:mspace width="1pt"/><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>μ</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>p</mml:mi></mml:munderover> <mml:mspace width="1pt"/><mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula> <italic>K</italic> is the mean degree, i.e., the average number of edges of the network <italic>G</italic> of size <italic>N</italic>. <inline-formula id="pone.0184683.e003"><alternatives><graphic id="pone.0184683.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> is another quantity assigned to node <italic>i</italic>, which also has either +1 or −1. A collective quantity <inline-formula id="pone.0184683.e004"><alternatives><graphic id="pone.0184683.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:mo>{</mml:mo> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> represents a memory pattern denoted by <italic>μ</italic> that is stored in the system. The index <italic>μ</italic> runs <italic>μ</italic> = 1, …, <italic>p</italic>, which means that the number of memory patterns stored is <italic>p</italic>. Whereas <inline-formula id="pone.0184683.e005"><alternatives><graphic id="pone.0184683.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> is fixed throughout the dynamics. Starting from some initial values of {<italic>S</italic><sub><italic>i</italic></sub>(<italic>t</italic> = 0)}, the state of each spin is updated asynchronously as
<disp-formula id="pone.0184683.e006"><alternatives><graphic id="pone.0184683.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>S</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mtext>sgn</mml:mtext> <mml:mo>(</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:munder> <mml:mspace width="1pt"/><mml:msub><mml:mi>J</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>S</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
When ∑<sub><italic>j</italic></sub> <italic>J</italic><sub><italic>ij</italic></sub><italic>S</italic><sub><italic>j</italic></sub>(<italic>t</italic>) becomes zero, <italic>S</italic><sub><italic>i</italic></sub>(<italic>t</italic> + 1) = +1 is assigned definitely.</p>
<p>As the updating is repeated, the energy given by (<xref ref-type="disp-formula" rid="pone.0184683.e001">1</xref>) reduces and the system reaches a local minumum state. In this state, each spin <italic>S</italic><sub><italic>i</italic></sub> becomes equivalent to <inline-formula id="pone.0184683.e007"><alternatives><graphic id="pone.0184683.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msubsup><mml:mi>ξ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>μ</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> for a given pattern <italic>μ</italic>, and the stored memory is retrieved. The underlying mechanism for such converging behavior is explained in Section 2 of the <xref ref-type="supplementary-material" rid="pone.0184683.s001">S1 File</xref>. In real-world systems, however, the repeated dynamics may not be deterministic as <xref ref-type="disp-formula" rid="pone.0184683.e006">Eq 3</xref>, but it may include some noise. To take into account of noise effect, the original study [<xref ref-type="bibr" rid="pone.0184683.ref006">6</xref>] of Hopfield model invoked the formalism developed in equilibrium statistical mechanics at finite temperature, in which temperature represents noise strength.</p>
</sec>
<sec id="sec005">
<title>Ensemble average of the Hopfield model over different network configurations and stored patterns</title>
<p>Thus far, we consider the Hopfield model on a given network. However, connection profiles of SF networks can be different from sample to sample even though they follow the same degree distribution. Thus, we need to take average of physical quantities over the ensemble of different network configurations. To proceed, we consider the probability that a given SF network <italic>G</italic> exists in the ensemble. That is given as
<disp-formula id="pone.0184683.e008"><alternatives><graphic id="pone.0184683.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mi>K</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:munder><mml:mo>∏</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo> <mml:mo>∈</mml:mo> <mml:mi>G</mml:mi></mml:mrow></mml:munder> <mml:msub><mml:mi>f</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:munder><mml:mo>∏</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo> <mml:mo>∉</mml:mo> <mml:mi>G</mml:mi></mml:mrow></mml:munder> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
where <italic>f</italic><sub><italic>ij</italic></sub> is the probability to connect a link between two nodes <italic>i</italic> and <italic>j</italic>. It was derived that <italic>f</italic><sub><italic>ij</italic></sub> = 1 − exp(−<italic>NKw</italic><sub><italic>i</italic></sub><italic>w</italic><sub><italic>j</italic></sub>) [<xref ref-type="bibr" rid="pone.0184683.ref017">17</xref>, <xref ref-type="bibr" rid="pone.0184683.ref018">18</xref>]. The factor <italic>w</italic><sub><italic>i</italic></sub> is a weight of node <italic>i</italic> reflecting heterogeneous degrees of a SF network. The explicit form of the weight factor is presented in the Method section. Then the ensemble average over different networks for any given physical quantity <italic>A</italic> is taken as
<disp-formula id="pone.0184683.e009"><alternatives><graphic id="pone.0184683.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:mi>A</mml:mi> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>K</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>G</mml:mi></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mi>K</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>A</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
where 〈⋯〉<sub><italic>K</italic></sub> denotes the average over different graph configurations. <italic>A</italic>(<italic>G</italic>) represents any physical quantity obtained in a graph <italic>G</italic>.</p>
<p>Next, we consider a situation in which stored patterns are not deterministic, but stochastically generated. This case is considered for the purpose of testing the efficiency of the algoritumc. Specifically, each pattern <italic>μ</italic> is created with the population of <inline-formula id="pone.0184683.e010"><alternatives><graphic id="pone.0184683.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mrow><mml:msubsup><mml:mi>ξ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> with probability 1/2 and that of <inline-formula id="pone.0184683.e011"><alternatives><graphic id="pone.0184683.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:msubsup><mml:mi>ξ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> with probability 1/2 for each node <italic>i</italic>. Then, the probability that a pattern <italic>μ</italic> is created is given as
<disp-formula id="pone.0184683.e012"><alternatives><graphic id="pone.0184683.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mo>{</mml:mo> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mo>[</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula></p>
<p>We take the average of any given physical quantity <italic>A</italic> over the ensemble of different stored patterns as
<disp-formula id="pone.0184683.e013"><alternatives><graphic id="pone.0184683.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e013" xlink:type="simple"/><mml:math display="block" id="M13"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:mi>A</mml:mi> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mi>ξ</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo>∫</mml:mo> <mml:mi>d</mml:mi> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mo>{</mml:mo> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>A</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mo>{</mml:mo> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
where 〈⋯〉<sub><italic>ξ</italic></sub> is an average over the quenched disorder of <inline-formula id="pone.0184683.e014"><alternatives><graphic id="pone.0184683.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>.</p>
</sec>
</sec>
<sec id="sec006">
<title>Analytic solutions</title>
<sec id="sec007">
<title>The order parameters</title>
<p>We characterize the phases of the Hopfield model by three quantities, which are often called the order parameters in statistical physics as follows: i) The overlap parameter defined as <inline-formula id="pone.0184683.e015"><alternatives><graphic id="pone.0184683.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mrow><mml:msubsup><mml:mi>m</mml:mi> <mml:mrow><mml:mi>α</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>≡</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>〈</mml:mo> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup> <mml:msubsup><mml:mi>S</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>α</mml:mi></mml:msubsup> <mml:mo>〉</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, which represents the extent of which the <italic>μ</italic>-th pattern of memory <italic>ξ</italic><sup><italic>μ</italic></sup> and the <italic>α</italic>-th state of the system <italic>S</italic><sup><italic>α</italic></sup> overlap with each other. Thus, this quantity measures the retrieval success rate of the <italic>μ</italic>-th memory. We remark that the factor <italic>w</italic><sub><italic>i</italic></sub> is required to take into account heterogeneous degrees. <italic>w</italic><sub><italic>i</italic></sub> is proportional to the degree <italic>k</italic><sub><italic>i</italic></sub> of neuron <italic>i</italic> [<xref ref-type="bibr" rid="pone.0184683.ref023">23</xref>–<xref ref-type="bibr" rid="pone.0184683.ref026">26</xref>]. Actually this weight <italic>w</italic><sub><italic>i</italic></sub> is the same as that used in the Chung-Lu model introduced in Methods. ii) The so-called spin glass order parameter <inline-formula id="pone.0184683.e016"><alternatives><graphic id="pone.0184683.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:msub><mml:mi>q</mml:mi> <mml:mrow><mml:mi>α</mml:mi> <mml:mi>β</mml:mi></mml:mrow></mml:msub> <mml:mo>≡</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>〈</mml:mo> <mml:msubsup><mml:mi>S</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>α</mml:mi></mml:msubsup> <mml:msubsup><mml:mi>S</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>β</mml:mi></mml:msubsup> <mml:mo>〉</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> representing the extent of which the two states <italic>α</italic> and <italic>β</italic> of the replica overlap each other. The replica is a quantity introduced in the spin-glass theory to resolve the technical difficulties in taking the ensemble average 〈⋯〉<sub><italic>K</italic></sub> and 〈⋯〉<sub><italic>ξ</italic></sub> introduced in Eqs (<xref ref-type="disp-formula" rid="pone.0184683.e009">5</xref>) and (<xref ref-type="disp-formula" rid="pone.0184683.e013">7</xref>). iii) A new quantity is introduced as <inline-formula id="pone.0184683.e017"><alternatives><graphic id="pone.0184683.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>α</mml:mi> <mml:mi>β</mml:mi></mml:mrow></mml:msub> <mml:mo>≡</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>/</mml:mo> <mml:mi>p</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>μ</mml:mi> <mml:mo>=</mml:mo> <mml:mn>2</mml:mn></mml:mrow> <mml:mi>p</mml:mi></mml:msubsup> <mml:msubsup><mml:mi>m</mml:mi> <mml:mrow><mml:mi>α</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup> <mml:msubsup><mml:mi>m</mml:mi> <mml:mrow><mml:mi>β</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>. This quantity is necessary to derive the first two order parameters, and is interpreted as the sum of the effects of each non-retrieval pattern.</p>
<p>Next, we take the replica-symmetric solution by setting <italic>q</italic><sub><italic>αβ</italic></sub> = <italic>q</italic> and <italic>r</italic><sub><italic>αβ</italic></sub> = <italic>r</italic> for all <italic>α</italic> ≠ <italic>β</italic> and <inline-formula id="pone.0184683.e018"><alternatives><graphic id="pone.0184683.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:msubsup><mml:mi>m</mml:mi> <mml:mrow><mml:mi>α</mml:mi></mml:mrow> <mml:mn>1</mml:mn></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>m</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> for all <italic>α</italic>. Then the order parameters <italic>m</italic>, <italic>q</italic> and <italic>r</italic> at zero temperature are obtained as
<disp-formula id="pone.0184683.e019"><alternatives><graphic id="pone.0184683.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e019" xlink:type="simple"/><mml:math display="block" id="M19"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>m</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mspace width="1pt"/><mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="3.33333pt"/><mml:mtext>erf</mml:mtext> <mml:mo>(</mml:mo> <mml:msqrt><mml:mfrac><mml:mrow><mml:mi>N</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:mfrac></mml:msqrt> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula> <disp-formula id="pone.0184683.e020"><alternatives><graphic id="pone.0184683.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>q</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mi>N</mml:mi> <mml:mi>K</mml:mi></mml:mrow></mml:mfrac> <mml:mspace width="1pt"/><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mspace width="1pt"/><mml:msqrt><mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:mi>N</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>π</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:mfrac></mml:msqrt> <mml:mspace width="3.33333pt"/><mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mi>N</mml:mi> <mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:mfrac> <mml:msup><mml:mi>m</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula> <disp-formula id="pone.0184683.e021"><alternatives><graphic id="pone.0184683.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>r</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mi>q</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>K</mml:mi> <mml:mo>+</mml:mo> <mml:mi>K</mml:mi> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
which are used for obtaining the error rate. Here, storage rate <italic>a</italic> is defined as the number of existing memory patterns <italic>p</italic> divided by the total number of neurons in a given network, i.e., the network size <italic>N</italic>. Therefore, the storage capacity <italic>a</italic><sub><italic>c</italic></sub> is the maximum value of storage rate <italic>a</italic>. Detailed calculations for those parameters are presented in Section 4 of the <xref ref-type="supplementary-material" rid="pone.0184683.s001">S1 File</xref>.</p>
</sec>
<sec id="sec008">
<title>Phase diagram and error rates</title>
<p>The phase diagram we obtained are shown in <xref ref-type="fig" rid="pone.0184683.g001">Fig 1</xref> in the <italic>T</italic> − <italic>a</italic> plane for different degree exponents <italic>γ</italic> but with fixed <italic>N</italic> = 1000 and <italic>K</italic> = 5.0. First, in <xref ref-type="fig" rid="pone.0184683.g001">Fig 1(a)</xref>, we consider the case of the Erdős and Rényi (ER) network, equivalent to the limit <italic>γ</italic> → ∞. This phase diagram is nearly the same as that obtained on fully connected network in Ref. [<xref ref-type="bibr" rid="pone.0184683.ref007">7</xref>]. There exist four different phases: i) The paramagnetic phase denoted as P, in which <italic>m</italic> = 0, <italic>q</italic> = 0, and <italic>r</italic> = 0 because of thermal fluctuations. ii) The spin-glass phase denoted as SG, in which <italic>m</italic> = 0, <italic>q</italic> &gt; 0, and <italic>r</italic> &gt; 0. In those phases P and SG, the retrieval of stored patterns is impossible. Thus, they are referred to as the confusion phases. iii) The retrieval phase denoted as R, in which <italic>m</italic> &gt; 0, <italic>q</italic> &gt; 0, and <italic>r</italic> &gt; 0, and in which the retrieval of stored memory is possible. Finally, the mixed phase denoted as M, in which the properties of both the retrieval and the spin-glass phases coexist.</p>
<p>The free energy of the system in each phase has also been investigated in Ref. [<xref ref-type="bibr" rid="pone.0184683.ref007">7</xref>]. In phase R, a stored pattern, for instance, <inline-formula id="pone.0184683.e022"><alternatives><graphic id="pone.0184683.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:msub><mml:mrow><mml:mo>{</mml:mo> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>}</mml:mo></mml:mrow> <mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula> is matched to a state of the system <inline-formula id="pone.0184683.e023"><alternatives><graphic id="pone.0184683.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:msub><mml:mrow><mml:mo>{</mml:mo> <mml:msubsup><mml:mi>S</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>α</mml:mi></mml:msubsup> <mml:mo>}</mml:mo></mml:mrow> <mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula> at the global minimum of free energy. However, in phase M, a stored pattern is matched to a state of the system in a metastable state, while the SG state lies at the global minimum.</p>
<p>For <italic>a</italic> = 0, which occurs when <inline-formula id="pone.0184683.e024"><alternatives><graphic id="pone.0184683.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> in the limit <italic>N</italic> → ∞, there exist two phases, P and R. As <italic>a</italic> is increased slightly from <italic>a</italic> = 0, the SG and M phases appear between the phases P and R. The transition between the P and the SG phases is a second-order transition, whereas that between SG and M is a first-order transition. Likewise, the transition between M and R is also first order. At <italic>T</italic> = 0, the transition between R and M occurs at <italic>a</italic><sub><italic>m</italic></sub> ≃ 0.114, and the transition between M and SG occurs at <italic>a</italic><sub><italic>c</italic></sub> ≃ 0.143. Therefore, as temperature is lowered from a sufficiently high value, successive transitions occur following the steps P → SG → M → R for <italic>a</italic> &lt; <italic>a</italic><sub><italic>m</italic></sub>. For <italic>a</italic><sub><italic>m</italic></sub> &lt; <italic>a</italic> &lt; <italic>a</italic><sub><italic>c</italic></sub>, successive transitions occur following the steps P → SG → M, and for <italic>a</italic> &gt; <italic>a</italic><sub><italic>c</italic></sub>, a transition from P → SG occurs.</p>
<p>Second, we obtain the phase diagram for finite <italic>γ</italic> values in <xref ref-type="fig" rid="pone.0184683.g001">Fig 1(b)–1(e)</xref>. That undergoes drastic changes depending on <italic>γ</italic>. Remarkably, the R phase intrudes into the region of the SG phase, but it also raises the boundary of the P phase to a higher-temperature region. As <italic>γ</italic> approaches 2.04 in <xref ref-type="fig" rid="pone.0184683.g001">Fig 1(e)</xref>, we observe that the R phase prevails, whereas the SG phase shrinks until it only exists at <italic>T</italic> = 0 in the region <italic>a</italic> &gt; 0.6. Such changes in the <italic>T</italic> − <italic>a</italic> diagrams can be understood analytically by examining the phase boundary between the P and SG phases. The details are presented in Section 5 of the <xref ref-type="supplementary-material" rid="pone.0184683.s001">S1 File</xref>.</p>
<p>Third, we consider a particular case, the noiseless case <italic>T</italic> = 0 in <xref ref-type="fig" rid="pone.0184683.g002">Fig 2</xref>. The order parameters <italic>m</italic>, <italic>q</italic> and <italic>r</italic> are given in Eqs (<xref ref-type="disp-formula" rid="pone.0184683.e019">8</xref>–<xref ref-type="disp-formula" rid="pone.0184683.e021">10</xref>). The figures show the behavior of the error rates <italic>n</italic><sub><italic>e</italic></sub> as a function of <italic>a</italic> with <italic>N</italic> = 1000 and <italic>K</italic> = 5.0. The error rate means the relative error of the neural networks and is defined as <italic>n</italic><sub><italic>e</italic></sub> ≡ (1 − <italic>m</italic>)/2. This figure is obtained analytically for various degree exponent values including <italic>γ</italic> ≃ 2.0. We first consider the case in which <italic>γ</italic> → ∞, i.e., the ER limit. The result of this case is almost the same as that obtained in Ref. [<xref ref-type="bibr" rid="pone.0184683.ref007">7</xref>]. When <italic>a</italic> is less than <italic>a</italic><sub><italic>c</italic></sub> ≃ 0.143, <italic>n</italic><sub><italic>e</italic></sub> is very small, such that the error rate is negligible and the system is almost in the error-free state. The obtained value <italic>a</italic><sub><italic>c</italic></sub> is close to <italic>a</italic><sub><italic>c</italic></sub> ≃ 0.138, which was obtained on the fully connected network in Ref. [<xref ref-type="bibr" rid="pone.0184683.ref007">7</xref>]. As <italic>a</italic> reaches <italic>a</italic><sub><italic>c</italic></sub>, <italic>n</italic><sub><italic>e</italic></sub> suddenly jumps to 0.5 as shown in <xref ref-type="fig" rid="pone.0184683.g002">Fig 2(b)</xref>. This means that for <italic>a</italic> &gt; <italic>a</italic><sub><italic>c</italic></sub>, the system is in the error-full state (i.e., the state of complete confusion). As <italic>γ</italic> decreases, this behavior persists up to <italic>γ</italic><sub><italic>c</italic></sub> ≃ 2.7, and no longer holds for <italic>γ</italic> ≤ <italic>γ</italic><sub><italic>c</italic></sub>. Note that the value of <italic>a</italic><sub><italic>c</italic></sub> slightly decreases with decreasing <italic>γ</italic>, but their dependences are almost negligible.</p>
<p>Next, when <italic>γ</italic> approaches 2.0, <italic>n</italic><sub><italic>e</italic></sub> noticeably changes from the step-function-like shape to a monotonously increasing one as shown in <xref ref-type="fig" rid="pone.0184683.g002">Fig 2(d)</xref>. As <italic>γ</italic> is lowered further and approaches 2.0, the range of <italic>a</italic> for the state of complete confusion, with <italic>n</italic><sub><italic>e</italic></sub> = 0.5, disappears and <italic>a</italic><sub><italic>c</italic></sub> cannot be defined anymore. For instance, at <italic>γ</italic> = 2.01, the error rate becomes less than 0.3 for the entire range of <italic>a</italic>. Therefore, when <italic>γ</italic> is lowered to as small as 2.0, while the range of <italic>a</italic> for the state of <italic>n</italic><sub><italic>e</italic></sub> = 0 (the state of perfect retrieval without error) is reduced, the range of <italic>a</italic> for the state of <italic>n</italic><sub><italic>e</italic></sub> = 0.5, the region of the state of complete confusion disappears. The details are presented in Section 6 of the <xref ref-type="supplementary-material" rid="pone.0184683.s001">S1 File</xref>. These behaviors have never been observed yet in previous studies [<xref ref-type="bibr" rid="pone.0184683.ref015">15</xref>, <xref ref-type="bibr" rid="pone.0184683.ref016">16</xref>, <xref ref-type="bibr" rid="pone.0184683.ref027">27</xref>]. <xref ref-type="fig" rid="pone.0184683.g002">Fig 2</xref> thus implies that as the network changes from the ER network to the extremely heterogeneous SF network with degree exponent two, the storage capacity becomes tremendously enhanced, but some error occurs. This result suggests that hubs play central roles in memory retrieval.</p>
</sec>
</sec>
</sec>
<sec id="sec009">
<title>Conclusion and discussion</title>
<p>We obtained the results that as the network changes from a hub-absent network to a SF network with degree exponent just above two, the storage capacity becomes tremendously enhanced, but some error occurs. These features seem to be in accordance with what we experience in everyday life and with a recent discovery of enormously high storage capability in the human brain. Thus hubs, i.e., neurons with a large number of synapses, and other nodes with heterogeneous degrees in neural networks play a central role in enhancing the storage capacity. It is interesting that a normal human brain has such a structure, even though detailed structural properties such as modularity and degree correlation are not yet known. We consider the effect of degree correlation near <italic>γ</italic> = 2.0 by performing a similar analysis of the static model [<xref ref-type="bibr" rid="pone.0184683.ref023">23</xref>–<xref ref-type="bibr" rid="pone.0184683.ref026">26</xref>], the degree correlation of which is disassortative between 2 &lt; <italic>γ</italic> &lt; 3. We find that correlated SF networks near <italic>γ</italic> = 2.0<sup>+</sup> have lower values of <italic>n</italic><sub><italic>e</italic></sub> than the uncorrelated ones. The details are presented in Section 8 of the <xref ref-type="supplementary-material" rid="pone.0184683.s001">S1 File</xref>. We also checked the error rate <italic>n</italic><sub><italic>e</italic></sub> on several real neural networks, having heavy-tailed degree distributions but with undetermined degree exponents due to the small network size. The obtained patterns of the error rates are indeed similar to the theoretical prediction. The details are presented in Section 9 of the <xref ref-type="supplementary-material" rid="pone.0184683.s001">S1 File</xref>.</p>
<p>Our results might provide some guidelines for constructing an artificial neural network, provided that it is constructed on the basis of the Hopfield model: If we want to construct an artificial neural network that is capable of perfect memory recall with a large value of <italic>a</italic><sub><italic>c</italic></sub>, as a basic topology of the artificial neural network, it may be more appropriate to choose an ER-type random network or a fully connected one. However, if we prefer to construct an artificial neural network that supports an extended range of storage rate and tolerate a small range of errors, it may be more appropriate to choose an SF-type network with the degree exponent <italic>γ</italic> ≃ 2.0. In this case, we can construct an artificial neural network with relatively low cost compared with a fully-connected one which needs very many synapses of <italic>O</italic>(<italic>N</italic><sup>2</sup>).</p>
</sec>
<sec id="sec010" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec011">
<title>Construction of scale-free networks</title>
<p>To construct uncorrelated SF networks, we use the Chung-Lu (CL) model [<xref ref-type="bibr" rid="pone.0184683.ref017">17</xref>, <xref ref-type="bibr" rid="pone.0184683.ref018">18</xref>]: We start with a fixed number of <italic>N</italic> vertices. Each vertex <italic>i</italic> (<italic>i</italic> = 1, 2, …, <italic>N</italic>) is assigned a weight
<disp-formula id="pone.0184683.e025"><alternatives><graphic id="pone.0184683.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>i</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>ν</mml:mi></mml:mrow></mml:msup> <mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>i</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>ν</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
where <italic>ν</italic> is a control parameter in the range [0, 1), and <italic>i</italic><sub>0</sub> is constant given by
<disp-formula id="pone.0184683.e026"><alternatives><graphic id="pone.0184683.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e026" xlink:type="simple"/><mml:math display="block" id="M26"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>i</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mn>10</mml:mn> <mml:msqrt><mml:mn>2</mml:mn></mml:msqrt> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>ν</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mi>ν</mml:mi></mml:mrow></mml:msup> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn> <mml:mi>ν</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn> <mml:mo>&lt;</mml:mo> <mml:mi>ν</mml:mi> <mml:mo>&lt;</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mspace width="3.33333pt"/><mml:mspace width="3.33333pt"/><mml:mspace width="3.33333pt"/></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>≤</mml:mo> <mml:mi>ν</mml:mi> <mml:mo>&lt;</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula></p>
<p>A pair of vertices (<italic>i</italic>, <italic>j</italic>) is chosen with the probabilities <italic>w</italic><sub><italic>i</italic></sub> and <italic>w</italic><sub><italic>j</italic></sub>, respectively, and they are connected with an edge, unless the pair is already connected. This process is repeated <italic>NK</italic>/2 times. Then, the resulting network becomes an uncorrelated SF network following a power-law degree distribution, <italic>P</italic><sub><italic>d</italic></sub>(<italic>k</italic>) ∼ <italic>k</italic><sup>−<italic>γ</italic></sup>, where <italic>k</italic> denotes the degree and <italic>γ</italic> is the degree exponent with <italic>γ</italic> = 1 + 1/<italic>ν</italic>. In such random networks, the probability that a given pair of vertices (<italic>i</italic>, <italic>j</italic>) (<italic>i</italic> ≠ <italic>j</italic>) is not connected by an edge, as denoted by 1 − <italic>f</italic><sub><italic>ij</italic></sub>, is given by (1 − 2<italic>w</italic><sub><italic>i</italic></sub><italic>w</italic><sub><italic>j</italic></sub>)<sup><italic>NK</italic>/2</sup> ≃ exp(−<italic>NKw</italic><sub><italic>i</italic></sub><italic>w</italic><sub><italic>j</italic></sub>), while the connection probability <italic>f</italic><sub><italic>ij</italic></sub> = 1 − exp(−<italic>NKw</italic><sub><italic>i</italic></sub><italic>w</italic><sub><italic>j</italic></sub>).</p>
<p>As a particular case, when we choose <italic>i</italic><sub>0</sub> as 1 for all the values of <italic>ν</italic>, the CL model reduces to the static model [<xref ref-type="bibr" rid="pone.0184683.ref023">23</xref>–<xref ref-type="bibr" rid="pone.0184683.ref026">26</xref>], which has correlations for the range of 1/2 &lt; <italic>ν</italic> &lt; 1 [<xref ref-type="bibr" rid="pone.0184683.ref023">23</xref>, <xref ref-type="bibr" rid="pone.0184683.ref024">24</xref>]. Therefore, the weights for the static model are given by
<disp-formula id="pone.0184683.e027"><alternatives><graphic id="pone.0184683.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e027" xlink:type="simple"/><mml:math display="block" id="M27"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:msup><mml:mi>i</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>ν</mml:mi></mml:mrow></mml:msup> <mml:mrow><mml:msub><mml:mi>ζ</mml:mi> <mml:mi>N</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ν</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula>
where <italic>ν</italic> is a control parameter in the range [0, 1), and <inline-formula id="pone.0184683.e028"><alternatives><graphic id="pone.0184683.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:msub><mml:mi>ζ</mml:mi> <mml:mi>N</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ν</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msup><mml:mi>j</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>ν</mml:mi></mml:mrow></mml:msup> <mml:mo>≃</mml:mo> <mml:msup><mml:mi>N</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>ν</mml:mi></mml:mrow></mml:msup> <mml:mo>/</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>ν</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Note that <italic>f</italic><sub><italic>ij</italic></sub> ≃ <italic>NKw</italic><sub><italic>i</italic></sub><italic>w</italic><sub><italic>j</italic></sub> for finite <italic>K</italic>, however, <italic>f</italic><sub><italic>ij</italic></sub> ≃ 1 for 2 &lt; <italic>γ</italic> &lt; 3 and <italic>ij</italic> ≪ <italic>N</italic><sup>3−<italic>γ</italic></sup>.</p>
<p>For the Erdős-Rényi (ER) graph [<xref ref-type="bibr" rid="pone.0184683.ref028">28</xref>–<xref ref-type="bibr" rid="pone.0184683.ref030">30</xref>], <italic>ν</italic> becomes 0 and the weights of both models become <italic>w</italic><sub><italic>i</italic></sub> = 1/<italic>N</italic>, independent of the index <italic>i</italic>. Since <italic>w</italic><sub><italic>i</italic></sub><italic>w</italic><sub><italic>j</italic></sub> = 1/<italic>N</italic><sup>2</sup>, the fraction of bonds present becomes <italic>f</italic><sub><italic>ij</italic></sub> ≃ <italic>K</italic>/<italic>N</italic> and the total number of the connected edges <italic>L</italic> is <italic>NK</italic>/2. So <italic>K</italic> becomes the mean degree in the ER graph.</p>
<p>When <italic>K</italic> approaches <italic>N</italic>, this network becomes a fully-connected one or a regular lattice with infinite-range interaction. Note that previous studies on the Hopfield model focused mainly on such extreme cases of <italic>K</italic> → <italic>N</italic> [<xref ref-type="bibr" rid="pone.0184683.ref002">2</xref>, <xref ref-type="bibr" rid="pone.0184683.ref006">6</xref>, <xref ref-type="bibr" rid="pone.0184683.ref007">7</xref>, <xref ref-type="bibr" rid="pone.0184683.ref031">31</xref>, <xref ref-type="bibr" rid="pone.0184683.ref032">32</xref>].</p>
</sec>
</sec>
<sec id="sec012">
<title>Supporting information</title>
<supplementary-material id="pone.0184683.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pone.0184683.s001" xlink:type="simple">
<label>S1 File</label>
<caption>
<title>Supporting information for the main paper.</title>
<p>This supporting information contains the detailed calculations of the free energy and order parameters using the replica analysis.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0184683.s002" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pone.0184683.s002" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>A simple example of simulation of the Hopfield model.</title>
<p>(a) A network of size <italic>N</italic> = 9 and mean degree <inline-formula id="pone.0184683.e029"><alternatives><graphic id="pone.0184683.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0184683.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mrow><mml:mi>K</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>5</mml:mn> <mml:mn>3</mml:mn></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>. (b) Two patterns <italic>ξ</italic><sup>1</sup> and <italic>ξ</italic><sup>2</sup> are encoded by the Hebbian rule. In (c) and (d), we start from two random patterns. After some updates using Eq (1) of the <xref ref-type="supplementary-material" rid="pone.0184683.s001">S1 File</xref>, the patterns <italic>ξ</italic><sup>1</sup> and <italic>ξ</italic><sup>2</sup> are retrieved.</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0184683.s003" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pone.0184683.s003" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Phase diagrams shown in <xref ref-type="fig" rid="pone.0184683.g001">Fig 1</xref> of the main paper are redrawn with the Almeida-Thouless(AT) line (Eq (32) of the <xref ref-type="supplementary-material" rid="pone.0184683.s001">S1 File</xref>).</title>
<p>Note that the dotted black line near zero temperature in each panel represents the AT line. Thus, we check that the replica-symmetric solution is valid over almost the entire region in the plane of (<italic>T</italic>, <italic>a</italic>).</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0184683.s004" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pone.0184683.s004" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Phase diagrams of the Hopfield model in the plane of (<italic>T</italic>, <italic>N</italic>) on the Chung-Lu model of SF networks with (a) <italic>γ</italic> = 5.00 and (b) <italic>γ</italic> = 3.00.</title>
<p>Here, <italic>a</italic> is fixed as 0.1.</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0184683.s005" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pone.0184683.s005" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Phase diagrams of the Hopfield model in the plane of (<italic>T</italic>, <italic>a</italic>) on the static model of SF networks with (a) <italic>γ</italic> = 2.35 and (b) <italic>γ</italic> = 2.01.</title>
<p>Those phase diagrams correspond to <xref ref-type="fig" rid="pone.0184683.g001">Fig 1(e) and 1(f)</xref> of the main paper for the CL model, respectively. <bold>P</bold> represents paramagnetic phase, <bold>SG</bold> spin glass phase, and <bold>R</bold> retrieval phase. The SG phase remains only on the axis <italic>T</italic> = 0 when <italic>γ</italic> = <italic>γ</italic><sub><italic>c</italic></sub> ≃ 2.35, then the R phase spans the entire region of <italic>a</italic> at the lower temperatures. Thus, the static model shows better memory retrieval than the CL model. To obtain the phase boundary, numerical calculations were performed for the static model with plugging <italic>N</italic> = 1000 and <italic>K</italic> = 5.0 into the formulas. Solid and dotted curves indicate the second-order and the first-order transitions, respectively. Note that black dotted line in each panel near the <italic>T</italic> = 0 line represents the AT line (Eq (32) of the <xref ref-type="supplementary-material" rid="pone.0184683.s001">S1 File</xref>). Thus, replica-symmetric solution is valid over almost the entire region of the phase space.</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0184683.s006" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pone.0184683.s006" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Plot of the error rate <italic>n</italic><sub><italic>e</italic></sub> ≡ (1 − <italic>m</italic>)/2 vs storage rate <italic>a</italic> for <italic>γ</italic> = 2.01 and 2.35 for the static model at <italic>T</italic> = 0, which corresponds to <xref ref-type="fig" rid="pone.0184683.g002">Fig 2(b)</xref> of the main paper for the CL model.</title>
<p>Here, numerical values are obtained using <italic>N</italic> = 1000 and <italic>K</italic> = 5.0.</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0184683.s007" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pone.0184683.s007" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Plot of the error rate <italic>n</italic><sub><italic>e</italic></sub> vs storage rate <italic>a</italic> for (a) <italic>γ</italic> = 2.01 and (b) <italic>γ</italic> = 2.04 for the CL and the static model at <italic>T</italic> = 0.</title>
<p>These figures provide the comparison of the error rate between for the CL model and for the static model. Here, numerical values are obtained using <italic>N</italic> = 1000 and <italic>K</italic> = 5.0.</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0184683.s008" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pone.0184683.s008" xlink:type="simple">
<label>S7 Fig</label>
<caption>
<title/>
<p>Plots of the number of neurons (nodes) with degree larger than <italic>k</italic> vs degree <italic>k</italic> of the real neural networks (left column) and of their error rates <italic>n</italic><sub><italic>e</italic></sub> vs storage rate <italic>a</italic> (right column): (a) and (b) for the visual cortex network of macaque monkey, with <italic>N</italic> = 30 and <italic>L</italic> = 311. (c) and (d) for the corticocortical connectivity network in the visual and sensorimotor area of macaque monkey, with <italic>N</italic> = 47 and <italic>L</italic> = 505. (e) and (f) for the cortex network of cat, with <italic>N</italic> = 52 and <italic>L</italic> = 818. The red lines of (a), (c) and (e) are guidelines with slope −1 for eye drawn to compare the data points with the scale-freeness of <italic>γ</italic> = 2.0. The red curves of (b), (d) and (f) are drawn to compare the simulation data (•) with the analytic solution (red curve) using Eq (40) of the <xref ref-type="supplementary-material" rid="pone.0184683.s001">S1 File</xref> under the same conditions of <italic>N</italic> and <italic>L</italic>. Here, the <italic>γ</italic> values we used for <italic>w</italic><sub><italic>i</italic></sub> in Eq (40) of the <xref ref-type="supplementary-material" rid="pone.0184683.s001">S1 File</xref> were 2.0035 (b), 2.0050 (d), and 2.0035 (f), respectively.</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0184683.s009" mimetype="application/vnd.ms-excel" position="float" xlink:href="info:doi/10.1371/journal.pone.0184683.s009" xlink:type="simple">
<label>S1 Data Set</label>
<caption>
<title>Supporting information for the “<xref ref-type="supplementary-material" rid="pone.0184683.s008">S7 Fig</xref>”.</title>
<p>This is another supporting information which contains the data set necessary to replicate the “<xref ref-type="supplementary-material" rid="pone.0184683.s008">S7 Fig</xref>”.</p>
<p>(XLS)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>This work was supported by the NRF of Korea (Grant Nos. 2014R1A3A2069005 and 2015R1A5A7037676) and Sogang University (Grant Nos. 201610033.01 and 201710066.01). We would like to express our appreciation toward S.-H. Lee and Joonwon Lee.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0184683.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Josselyn</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Köhler</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Frankland</surname> <given-names>PW</given-names></name>. <article-title>Finding the engram</article-title>. <source>Nat Rev Neurosci</source>. <year>2015</year> <month>Aug</month>;<volume>16</volume>(<issue>9</issue>):<fpage>521</fpage>–<lpage>534</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn4000" xlink:type="simple">10.1038/nrn4000</ext-link></comment> <object-id pub-id-type="pmid">26289572</object-id></mixed-citation>
</ref>
<ref id="pone.0184683.ref002">
<label>2</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Amit</surname> <given-names>DJ</given-names></name>. <source>Modeling brain function: The world of attractor neural networks</source>. <publisher-name>Cambridge University Press</publisher-name>; <year>1989</year>.</mixed-citation>
</ref>
<ref id="pone.0184683.ref003">
<label>3</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Hebb</surname> <given-names>DO</given-names></name>. <source>The Organization of Behavior</source>. <publisher-name>Wiley &amp; Sons</publisher-name>; <year>1949</year>.</mixed-citation>
</ref>
<ref id="pone.0184683.ref004">
<label>4</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Kandel</surname> <given-names>ER</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Jessell</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Siegelbaum</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Hudspeth</surname> <given-names>AJ</given-names></name>. <source>Principles of Neural Science</source>. <volume>vol. 5</volume>. <edition>5th ed</edition>. <publisher-name>McGraw-hill</publisher-name> <publisher-loc>New York</publisher-loc>; <year>2013</year>.</mixed-citation>
</ref>
<ref id="pone.0184683.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hopfield</surname> <given-names>JJ</given-names></name>. <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>1982</year> <month>Apr</month>;<volume>79</volume>(<issue>8</issue>):<fpage>2554</fpage>–<lpage>2558</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.79.8.2554" xlink:type="simple">10.1073/pnas.79.8.2554</ext-link></comment> <object-id pub-id-type="pmid">6953413</object-id></mixed-citation>
</ref>
<ref id="pone.0184683.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Amit</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Gutfreund</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>. <article-title>Storing infinite numbers of patterns in a spin-glass model of neural networks</article-title>. <source>Phys Rev Lett</source>. <year>1985</year> <month>Sep</month>;<volume>55</volume>(<issue>14</issue>):<fpage>1530</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevLett.55.1530" xlink:type="simple">10.1103/PhysRevLett.55.1530</ext-link></comment> <object-id pub-id-type="pmid">10031847</object-id></mixed-citation>
</ref>
<ref id="pone.0184683.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Amit</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Gutfreund</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>. <article-title>Statistical mechanics of neural networks near saturation</article-title>. <source>Ann Phys (NY)</source>. <year>1987</year> <month>Jan</month>;<volume>173</volume>(<issue>1</issue>):<fpage>30</fpage>–<lpage>67</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0003-4916(87)90092-3" xlink:type="simple">10.1016/0003-4916(87)90092-3</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0184683.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bullmore</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Sporns</surname> <given-names>O</given-names></name>. <article-title>Complex brain networks: graph theoretical analysis of structural and functional systems</article-title>. <source>Nat Rev Neurosci</source>. <year>2009</year> <month>Mar</month>;<volume>10</volume>(<issue>3</issue>):<fpage>186</fpage>–<lpage>198</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn2575" xlink:type="simple">10.1038/nrn2575</ext-link></comment> <object-id pub-id-type="pmid">19190637</object-id></mixed-citation>
</ref>
<ref id="pone.0184683.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>van den Heuvel</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Sporns</surname> <given-names>O</given-names></name>. <article-title>Network hubs in the human brain</article-title>. <source>Trends Cogn Sci</source>. <year>2013</year> <month>Dec</month>;<volume>17</volume>(<issue>12</issue>):<fpage>683</fpage>–<lpage>696</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2013.09.012" xlink:type="simple">10.1016/j.tics.2013.09.012</ext-link></comment> <object-id pub-id-type="pmid">24231140</object-id></mixed-citation>
</ref>
<ref id="pone.0184683.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Barabási</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Albert</surname> <given-names>R</given-names></name>. <article-title>Emergence of scaling in random networks</article-title>. <source>Science</source>. <year>1999</year> <month>Oct</month>;<volume>286</volume>(<issue>5439</issue>):<fpage>509</fpage>–<lpage>512</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.286.5439.509" xlink:type="simple">10.1126/science.286.5439.509</ext-link></comment> <object-id pub-id-type="pmid">10521342</object-id></mixed-citation>
</ref>
<ref id="pone.0184683.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Song</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sjöström</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Reigl</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name>. <article-title>Highly nonrandom features of synaptic connectivity in local cortical circuits</article-title>. <source>PLoS Biol</source>. <year>2005</year> <month>Mar</month>;<volume>3</volume>(<issue>3</issue>):<fpage>e68</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.0030068" xlink:type="simple">10.1371/journal.pbio.0030068</ext-link></comment> <object-id pub-id-type="pmid">15737062</object-id></mixed-citation>
</ref>
<ref id="pone.0184683.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rubinov</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bullmore</surname> <given-names>E</given-names></name>. <article-title>Schizophrenia and abnormal brain network hubs</article-title>. <source>Dialogues Clin Neurosci</source>. <year>2013</year> <month>Sep</month>;<volume>15</volume>(<issue>3</issue>):<fpage>339</fpage>–<lpage>349</lpage>. <object-id pub-id-type="pmid">24174905</object-id></mixed-citation>
</ref>
<ref id="pone.0184683.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Steullet</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Cabungcal</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Monin</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Dwir</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>O’Donnell</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Cuenod</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Redox dysregulation, neuroinflammation, and NMDA receptor hypofunction: A “central hub” in schizophrenia pathophysiology?</article-title> <source>Schizophr Res</source>. <year>2014</year> <month>Jul</month> <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.schres.2014.06.021" xlink:type="simple">10.1016/j.schres.2014.06.021</ext-link></comment> <object-id pub-id-type="pmid">25000913</object-id></mixed-citation>
</ref>
<ref id="pone.0184683.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Achard</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Delon-Martin</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Vértes</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Renard</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Schenck</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schneider</surname> <given-names>F</given-names></name>, <etal>et al</etal>. <article-title>Hubs of brain functional networks are radically reorganized in comatose patients</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2012</year> <month>Oct</month>;<volume>109</volume>(<issue>50</issue>):<fpage>20608</fpage>–<lpage>20613</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1208933109" xlink:type="simple">10.1073/pnas.1208933109</ext-link></comment> <object-id pub-id-type="pmid">23185007</object-id></mixed-citation>
</ref>
<ref id="pone.0184683.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Stauffer</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Aharony</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>da Fontoura Costa</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Adler</surname> <given-names>J</given-names></name>. <article-title>Efficient Hopfield pattern recognition on a scale-free neural network</article-title>. <source>Eur Phys J B</source>. <year>2003</year> <month>Apr</month>;<volume>32</volume>(<issue>3</issue>):<fpage>395</fpage>–<lpage>399</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1140/epjb/e2003-00114-7" xlink:type="simple">10.1140/epjb/e2003-00114-7</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0184683.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Torres</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Munoz</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Marro</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Garrido</surname> <given-names>P</given-names></name>. <article-title>Influence of topology on the performance of a neural network</article-title>. <source>Neurocomputing</source>. <year>2004</year> <month>Mar</month>;<volume>58</volume>:<fpage>229</fpage>–<lpage>234</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neucom.2004.01.048" xlink:type="simple">10.1016/j.neucom.2004.01.048</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0184683.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chung</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>L</given-names></name>. <article-title>Connected components in random graphs with given expected degree sequences</article-title>. <source>Ann Comb</source>. <year>2002</year> <month>Nov</month>;<volume>6</volume>(<issue>2</issue>):<fpage>125</fpage>–<lpage>145</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/PL00012580" xlink:type="simple">10.1007/PL00012580</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0184683.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cho</surname> <given-names>YS</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Park</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kahng</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>D</given-names></name>. <article-title>Percolation transitions in scale-free networks under the Achlioptas process</article-title>. <source>Phys Rev Lett</source>. <year>2009</year> <month>Sep</month>;<volume>103</volume>(<issue>13</issue>):<fpage>135702</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevLett.103.135702" xlink:type="simple">10.1103/PhysRevLett.103.135702</ext-link></comment> <object-id pub-id-type="pmid">19905523</object-id></mixed-citation>
</ref>
<ref id="pone.0184683.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Eguiluz</surname> <given-names>VM</given-names></name>, <name name-style="western"><surname>Chialvo</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Cecchi</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Baliki</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Apkarian</surname> <given-names>AV</given-names></name>. <article-title>Scale-free brain functional networks</article-title>. <source>Phys Rev Lett</source>. <year>2005</year> <month>Jan</month>;<volume>94</volume>(<issue>1</issue>):<fpage>018102</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevLett.94.018102" xlink:type="simple">10.1103/PhysRevLett.94.018102</ext-link></comment> <object-id pub-id-type="pmid">15698136</object-id></mixed-citation>
</ref>
<ref id="pone.0184683.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>van den Heuvel</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Stam</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Boersma</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Pol</surname> <given-names>HH</given-names></name>. <article-title>Small-world and scale-free organization of voxel-based resting-state functional connectivity in the human brain</article-title>. <source>NeuroImage</source>. <year>2008</year> <month>Aug</month>;<volume>43</volume>(<issue>3</issue>):<fpage>528</fpage>–<lpage>539</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2008.08.010" xlink:type="simple">10.1016/j.neuroimage.2008.08.010</ext-link></comment> <object-id pub-id-type="pmid">18786642</object-id></mixed-citation>
</ref>
<ref id="pone.0184683.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gallos</surname> <given-names>LK</given-names></name>, <name name-style="western"><surname>Makse</surname> <given-names>HA</given-names></name>, <name name-style="western"><surname>Sigman</surname> <given-names>M</given-names></name>. <article-title>A small world of weak ties provides optimal global integration of self-similar modules in functional brain networks</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2012</year> <month>Feb</month>;<volume>109</volume>(<issue>8</issue>):<fpage>2825</fpage>–<lpage>2830</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1106612109" xlink:type="simple">10.1073/pnas.1106612109</ext-link></comment> <object-id pub-id-type="pmid">22308319</object-id></mixed-citation>
</ref>
<ref id="pone.0184683.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bartol</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Bromer</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Kinney</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Chirillo</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Bourne</surname> <given-names>JN</given-names></name>, <name name-style="western"><surname>Harris</surname> <given-names>KM</given-names></name>, <etal>et al</etal>. <article-title>Nanoconnectomic upper bound on the variability of synaptic plasticity</article-title>. <source>eLife</source>. <year>2015</year> <month>Nov</month>;<volume>4</volume>:<fpage>e10778</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/eLife.10778" xlink:type="simple">10.7554/eLife.10778</ext-link></comment> <object-id pub-id-type="pmid">26618907</object-id></mixed-citation>
</ref>
<ref id="pone.0184683.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Goh</surname> <given-names>KI</given-names></name>, <name name-style="western"><surname>Kahng</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>D</given-names></name>. <article-title>Universal behavior of load distribution in scale-free networks</article-title>. <source>Phys Rev Lett</source>. <year>2001</year> <month>Dec</month>;<volume>87</volume>(<issue>27</issue>):<fpage>278701</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevLett.87.278701" xlink:type="simple">10.1103/PhysRevLett.87.278701</ext-link></comment> <object-id pub-id-type="pmid">11800921</object-id></mixed-citation>
</ref>
<ref id="pone.0184683.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lee</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Goh</surname> <given-names>KI</given-names></name>, <name name-style="western"><surname>Kahng</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>D</given-names></name>. <article-title>Intrinsic degree-correlations in the static model of scale-free networks</article-title>. <source>Eur Phys J B</source>. <year>2006</year> <month>Feb</month>;<volume>49</volume>(<issue>2</issue>):<fpage>231</fpage>–<lpage>238</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1140/epjb/e2006-00051-y" xlink:type="simple">10.1140/epjb/e2006-00051-y</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0184683.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kim</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Rodgers</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Kahng</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>D</given-names></name>. <article-title>Spin-glass phase transition on scale-free networks</article-title>. <source>Phys Rev E</source>. <year>2005</year> <month>May</month>;<volume>71</volume>(<issue>5</issue>):<fpage>056115</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.71.056115" xlink:type="simple">10.1103/PhysRevE.71.056115</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0184683.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kim</surname> <given-names>DH</given-names></name>. <article-title>Inverse transitions in a spin-glass model on a scale-free network</article-title>. <source>Phys Rev E</source>. <year>2014</year> <month>Feb</month>;<volume>89</volume>(<issue>2</issue>):<fpage>022803</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevE.89.022803" xlink:type="simple">10.1103/PhysRevE.89.022803</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0184683.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Castillo</surname> <given-names>IP</given-names></name>, <name name-style="western"><surname>Wemmenhove</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Hatchett</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Coolen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Skantzos</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Nikoletopoulos</surname> <given-names>T</given-names></name>. <article-title>Analytic solution of attractor neural networks on scale-free graphs</article-title>. <source>J Phys A: Math Gen</source>. <year>2004</year> <month>Sep</month>;<volume>37</volume>(<issue>37</issue>):<fpage>8789</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/0305-4470/37/37/002" xlink:type="simple">10.1088/0305-4470/37/37/002</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0184683.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Erdös</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Rényi</surname> <given-names>A</given-names></name>. <article-title>On random graphs, I</article-title>. <source>Publ Math Debr</source>. <year>1959</year>;<volume>6</volume>:<fpage>290</fpage>–<lpage>297</lpage>.</mixed-citation>
</ref>
<ref id="pone.0184683.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Erdös</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Rényi</surname> <given-names>A</given-names></name>. <article-title>On the evolution of random graphs</article-title>. <source>Publ Math Inst Hung Acad Sci</source>. <year>1960</year>;<volume>5</volume>(<issue>17-61</issue>):<fpage>43</fpage>.</mixed-citation>
</ref>
<ref id="pone.0184683.ref030">
<label>30</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Bollobás</surname> <given-names>B</given-names></name>. <source>Random graphs</source>. 2001. <publisher-name>Cambridge University Press</publisher-name>; <year>2001</year>.</mixed-citation>
</ref>
<ref id="pone.0184683.ref031">
<label>31</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Mézard</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Parisi</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Virasoro</surname> <given-names>MA</given-names></name>. <source>Spin glass theory and beyond</source>. <publisher-name>World Scientific</publisher-name>; <year>1987</year>.</mixed-citation>
</ref>
<ref id="pone.0184683.ref032">
<label>32</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Nishimori</surname> <given-names>H</given-names></name>. <source>Statistical physics of spin glasses and information processing: an introduction</source>. <publisher-name>Oxford University Press</publisher-name>; <year>2001</year>.</mixed-citation>
</ref>
</ref-list>
</back>
</article>