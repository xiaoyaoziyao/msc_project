<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-01070</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003408</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subject>Neural networks</subject><subject>Sensory systems</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Physics</subject><subj-group><subject>Statistical mechanics</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>Searching for Collective Behavior in a Large Network of Sensory Neurons</article-title>
<alt-title alt-title-type="running-head">Collective Behavior in a Network of Real Neurons</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Tkačik</surname><given-names>Gašper</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Marre</surname><given-names>Olivier</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Amodei</surname><given-names>Dario</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Schneidman</surname><given-names>Elad</given-names></name><xref ref-type="aff" rid="aff5"><sup>5</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Bialek</surname><given-names>William</given-names></name><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="aff" rid="aff6"><sup>6</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Berry</surname><given-names>Michael J.</given-names><suffix>II</suffix></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Institute of Science and Technology Austria, Klosterneuburg, Austria</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Institut de la Vision, INSERM U968, UPMC, CNRS U7210, CHNO Quinze-Vingts, Paris, France</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Department of Molecular Biology, Princeton Neuroscience Institute, Princeton University, Princeton, New Jersey, United States of America</addr-line></aff>
<aff id="aff4"><label>4</label><addr-line>Joseph Henry Laboratories of Physics, Princeton University, Princeton, New Jersey, United States of America</addr-line></aff>
<aff id="aff5"><label>5</label><addr-line>Department of Neurobiology, Weizmann Institute of Science, Rehovot, Israel</addr-line></aff>
<aff id="aff6"><label>6</label><addr-line>Lewis–Sigler Institute for Integrative Genomics, Princeton University, Princeton, New Jersey, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Sporns</surname><given-names>Olaf</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Indiana University, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">gtkacik@ist.ac.at</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: GT OM DA ES WB MJB. Performed the experiments: OM DA MJB. Analyzed the data: GT OM DA WB MJB. Wrote the paper: GT OM WB MJB.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>1</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>2</day><month>1</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>1</issue>
<elocation-id>e1003408</elocation-id>
<history>
<date date-type="received"><day>14</day><month>6</month><year>2013</year></date>
<date date-type="accepted"><day>5</day><month>11</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Tkačik et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Maximum entropy models are the least structured probability distributions that exactly reproduce a chosen set of statistics measured in an interacting network. Here we use this principle to construct probabilistic models which describe the correlated spiking activity of populations of up to 120 neurons in the salamander retina as it responds to natural movies. Already in groups as small as 10 neurons, interactions between spikes can no longer be regarded as small perturbations in an otherwise independent system; for 40 or more neurons pairwise interactions need to be supplemented by a global interaction that controls the distribution of synchrony in the population. Here we show that such “K-pairwise” models—being systematic extensions of the previously used pairwise Ising models—provide an excellent account of the data. We explore the properties of the neural vocabulary by: 1) estimating its entropy, which constrains the population's capacity to represent visual information; 2) classifying activity patterns into a small set of metastable collective modes; 3) showing that the neural codeword ensembles are extremely inhomogenous; 4) demonstrating that the state of individual neurons is highly predictable from the rest of the population, allowing the capacity for error correction.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>Sensory neurons encode information about the world into sequences of spiking and silence. Multi-electrode array recordings have enabled us to move from single units to measuring the responses of many neurons simultaneously, and thus to ask questions about how populations of neurons as a whole represent their input signals. Here we build on previous work that has shown that in the salamander retina, pairs of retinal ganglion cells are only weakly correlated, yet the population spiking activity exhibits large departures from a model where the neurons would be independent. We analyze data from more than a hundred salamander retinal ganglion cells and characterize their collective response using maximum entropy models of statistical physics. With these models in hand, we can put bounds on the amount of information encoded by the neural population, constructively demonstrate that the code has error correcting redundancy, and advance two hypotheses about the neural code: that collective states of the network could carry stimulus information, and that the distribution of neural activity patterns has very nontrivial statistical properties, possibly related to critical systems in statistical physics.</p>
</abstract>
<funding-group><funding-statement>This work was funded by NSF grant IIS-0613435, NSF grant PHY-0957573, NSF grant CCF-0939370, NIH grant R01 EY14196, NIH grant P50 GM071508, the Fannie and John Hertz Foundation, the Swartz Foundation, the WM Keck Foundation, ANR Optima and the French State program “Investissements d'Avenir” [LIFESENSES: ANR-10-LABX-65], and the Austrian Research Foundation FWF P25651. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="23"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Physicists have long hoped that the functional behavior of large, highly interconnected neural networks could be described by statistical mechanics <xref ref-type="bibr" rid="pcbi.1003408-Hopfield1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1003408-Hertz1">[3]</xref>. The goal of this effort has been not to simulate the details of particular networks, but to understand how interesting functions can emerge, collectively, from large populations of neurons. The hope, inspired by our quantitative understanding of collective behavior in systems near thermal equilibrium, is that such emergent phenomena will have some degree of universality, and hence that one can make progress without knowing all of the microscopic details of each system. A classic example of work in this spirit is the Hopfield model of associative or content–addressable memory <xref ref-type="bibr" rid="pcbi.1003408-Hopfield1">[1]</xref>, which is able to recover the correct memory from any of its subparts of sufficient size. Because the computational substrate of neural states in these models are binary “spins,” and the memories are realized as locally stable states of the network dynamics, methods of statistical physics could be brought to bear on theoretically challenging issues such as the storage capacity of the network or its reliability in the presence of noise <xref ref-type="bibr" rid="pcbi.1003408-Amit1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Hertz1">[3]</xref>. On the other hand, precisely because of these abstractions, it has not always been clear how to bring the predictions of the models into contact with experiment.</p>
<p>Recently it has been suggested that the analogy between statistical physics models and neural networks can be turned into a precise mapping, and connected to experimental data, using the maximum entropy framework <xref ref-type="bibr" rid="pcbi.1003408-Schneidman1">[4]</xref>. In a sense, the maximum entropy approach is the opposite of what we usually do in making models or theories. The conventional approach is to hypothesize some dynamics for the network we are studying, and then calculate the consequences of these assumptions; inevitably, the assumptions we make will be wrong in detail. In the maximum entropy method, however, we are trying to strip away all our assumptions, and find models of the system that have <italic>as little structure as possible</italic> while still reproducing some set of experimental observations.</p>
<p>The starting point of the maximum entropy method for neural networks is that the network could, if we don't know anything about its function, wander at random among all possible states. We then take measured, average properties of the network activity as constraints, and each constraint defines some minimal level of structure. Thus, in a completely random system neurons would generate action potentials (spikes) or remain silent with equal probability, but once we measure the mean spike rate for each neuron we know that there must be some departure from such complete randomness. Similarly, absent any data beyond the mean spike rates, the maximum entropy model of the network is one in which each neuron spikes independently of all the others, but once we measure the correlations in spiking between pairs of neurons, an additional layer of structure is required to account for these data. The central idea of the maximum entropy method is that, for each experimental observation that we want to reproduce, we add only the minimum amount of structure required.</p>
<p>An important feature of the maximum entropy approach is that the mathematical form of a maximum entropy model is exactly equivalent to a problem in statistical mechanics. That is, the maximum entropy construction defines an “effective energy” for every possible state of the network, and the probability that the system will be found in a particular state is given by the Boltzmann distribution in this energy landscape. Further, the energy function is built out of terms that are related to the experimental observables that we are trying to reproduce. Thus, for example, if we try to reproduce the correlations among spiking in pairs of neurons, the energy function will have terms describing effective interactions among pairs of neurons. As explained in more detail below, these connections are not analogies or metaphors, but precise mathematical equivalencies.</p>
<p>Minimally structured models are attractive, both because of the connection to statistical mechanics and because they represent the absence of modeling assumptions about data beyond the choice of experimental constraints. Of course, these features do not guarantee that such models will provide an accurate description of a real system. They do, however, give us a framework for starting with simple models and systematically increasing their complexity without worrying that the choice of model class itself has excluded the “correct” model or biased our results. Interest in maximum entropy approaches to networks of real neurons was triggered by the observation that, for groups of up to 10 ganglion cells in the vertebrate retina, maximum entropy models based on the mean spike probabilities of individual neurons and correlations between pairs of cells indeed generate successful predictions for the probabilities of all the combinatorial patterns of spiking and silence in the network as it responds to naturalistic sensory inputs <xref ref-type="bibr" rid="pcbi.1003408-Schneidman1">[4]</xref>. In particular, the maximum entropy approach made clear that genuinely collective behavior in the network can be consistent with relatively weak correlations among pairs of neurons, so long as these correlations are widespread, shared among most pairs of cells in the system. This approach has now been used to analyze the activity in a variety of neural systems <xref ref-type="bibr" rid="pcbi.1003408-Shlens1">[5]</xref>–<xref ref-type="bibr" rid="pcbi.1003408-Ganmor2">[15]</xref>, the statistics of natural visual scenes <xref ref-type="bibr" rid="pcbi.1003408-Tkaik3">[16]</xref>–<xref ref-type="bibr" rid="pcbi.1003408-Saremi1">[18]</xref>, the structure and activity of biochemical and genetic networks <xref ref-type="bibr" rid="pcbi.1003408-Lezon1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Tkaik4">[20]</xref>, the statistics of amino acid substitutions in protein families <xref ref-type="bibr" rid="pcbi.1003408-Bialek1">[21]</xref>–<xref ref-type="bibr" rid="pcbi.1003408-Sulkowska1">[27]</xref>, the rules of spelling in English words <xref ref-type="bibr" rid="pcbi.1003408-Stephens2">[28]</xref>, the directional ordering in flocks of birds <xref ref-type="bibr" rid="pcbi.1003408-Bialek2">[29]</xref>, and configurations of groups of mice in naturalistic habitats <xref ref-type="bibr" rid="pcbi.1003408-Shemesh1">[30]</xref>.</p>
<p>One of the lessons of statistical mechanics is that systems with many degrees of freedom can behave in qualitatively different ways from systems with just a few degrees of freedom. If we can study only a handful of neurons (e.g., <italic>N</italic>∼10 as in Ref <xref ref-type="bibr" rid="pcbi.1003408-Schneidman1">[4]</xref>), we can try to extrapolate based on the hypothesis that the group of neurons that we analyze is typical of a larger population. These extrapolations can be made more convincing by looking at a population of <italic>N</italic> = 40 neurons, and within such larger groups one can also try to test more explicitly whether the hypothesis of homogeneity or typicality is reliable <xref ref-type="bibr" rid="pcbi.1003408-Tkaik1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Tkaik2">[9]</xref>. All these analyses suggest that, in the salamander retina, the roughly 200 interconnected neurons that represent a small patch of the visual world should exhibit dramatically collective behavior. In particular, the states of these large networks should cluster around local minima of the energy landscape, much as for the attractors in the Hopfield model of associative memory <xref ref-type="bibr" rid="pcbi.1003408-Hopfield1">[1]</xref>. Further, this collective behavior means that responses will be substantially redundant, with the behavior of one neuron largely predictable from the state of other neurons in the network; stated more positively, this collective response allows for pattern completion and error correction. Finally, the collective behavior suggested by these extrapolations is a very special one, in which the probability of particular network states, or equivalently the degree to which we should be surprised by the occurrence of any particular state, has an anomalously large dynamic range <xref ref-type="bibr" rid="pcbi.1003408-Mora2">[31]</xref>. If correct, these predictions would have a substantial impact on how we think about coding in the retina, and about neural network function more generally. Correspondingly, there is some controversy about all these issues <xref ref-type="bibr" rid="pcbi.1003408-Nirenberg1">[32]</xref>–<xref ref-type="bibr" rid="pcbi.1003408-Roudi3">[35]</xref>.</p>
<p>Here we return to the salamander retina, in experiments that exploit a new generation of multi–electrode arrays and associated spike–sorting algorithms <xref ref-type="bibr" rid="pcbi.1003408-Marre1">[36]</xref>. As schematized in <xref ref-type="fig" rid="pcbi-1003408-g001">Figure 1</xref>, these methods make it possible to record from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e001" xlink:type="simple"/></inline-formula> ganglion cells in the relevant densely interconnected patch, while projecting natural movies onto the retina. Access to these large populations poses new problems for the inference of maximum entropy models, both in principle and in practice. What we find is that, with extensions of algorithms developed previously <xref ref-type="bibr" rid="pcbi.1003408-Broderick1">[37]</xref>, it is possible to infer maximum entropy models for more than one hundred neurons, and that with nearly two hours of data there are no signs of “overfitting” (cf. <xref ref-type="bibr" rid="pcbi.1003408-Ganmor2">[15]</xref>). We have built models that match the mean probability of spiking for individual neurons, the correlations between spiking in pairs of neurons, and the distribution of summed activity in the network (i.e., the probability that <italic>K</italic> out of the <italic>N</italic> neurons spike in the same small window of time <xref ref-type="bibr" rid="pcbi.1003408-Treves1">[38]</xref>–<xref ref-type="bibr" rid="pcbi.1003408-Okun1">[40]</xref>). We will see that models which satisfy all these experimental constraints provide a strikingly accurate description of the states taken on by the network as a whole, that these states <italic>are</italic> collective, and that the collective behavior predicted by our models has implications for how the retina encodes visual information.</p>
<fig id="pcbi-1003408-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003408.g001</object-id><label>Figure 1</label><caption>
<title>A schematic of the experiment.</title>
<p>(<bold>A</bold>) Four frames from the natural movie stimulus showing swimming fish and water plants. (<bold>B</bold>) The responses of a set of 120 neurons to a single stimulus repeat, black dots designate spikes. (<bold>C</bold>) The raster for a zoomed-in region designated by a red square in (B), showing the responses discretized into Δ<italic>τ</italic> = 20 ms time bins, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e002" xlink:type="simple"/></inline-formula> represents a silence (absence of spike) of neuron i, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e003" xlink:type="simple"/></inline-formula> represents a spike.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003408.g001" position="float" xlink:type="simple"/></fig><sec id="s1a">
<title>Maximum entropy</title>
<p>The idea of maximizing entropy has its origin in thermodynamics and statistical mechanics. The idea that we can use this principle to build models of systems that are not in thermal equilibrium is more recent, but still more than fifty years old <xref ref-type="bibr" rid="pcbi.1003408-Jaynes1">[41]</xref>; in the past few years, there has been a new surge of interest in the formal aspects of maximum entropy constructions for (out-of-equilibrium) spike rasters (see, e.g., <xref ref-type="bibr" rid="pcbi.1003408-Cessac1">[42]</xref>). Here we provide a description of this approach which we hope makes the ideas accessible to a broad audience.</p>
<p>We imagine a neural system exposed to a stationary stimulus ensemble, in which simultaneous recordings from <italic>N</italic> neurons can be made. In small windows of time, as we see in <xref ref-type="fig" rid="pcbi-1003408-g001">Figure 1</xref>, a single neuron i either does (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e004" xlink:type="simple"/></inline-formula>) or does not (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e005" xlink:type="simple"/></inline-formula>) generate an action potential or spike <xref ref-type="bibr" rid="pcbi.1003408-Rieke1">[43]</xref>; the state of the entire network in that time bin is therefore described by a “binary word” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e006" xlink:type="simple"/></inline-formula>. As the system responds to its inputs, it visits each of these states with some probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e007" xlink:type="simple"/></inline-formula>. Even before we ask what the different states mean, for example as codewords in a representation of the sensory world, specifying this distribution requires us to determine the probability of each of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e008" xlink:type="simple"/></inline-formula> possible states. Once <italic>N</italic> increases beyond ∼20, brute force sampling from data is no longer a general strategy for “measuring” the underlying distribution.</p>
<p>Even when there are many, many possible states of the network, experiments of reasonable size can be sufficient to estimate the averages or expectation values of various functions of the state of the system, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e009" xlink:type="simple"/></inline-formula>, where the averages are taken across data collected over the course of the experiment. The goal of the maximum entropy construction is to search for the probability distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e010" xlink:type="simple"/></inline-formula> that matches these experimental measurements but otherwise is as unstructured as possible. Minimizing structure means maximizing entropy <xref ref-type="bibr" rid="pcbi.1003408-Jaynes1">[41]</xref>, and for any set of moments or statistics that we want to match, the form of the maximum entropy distribution can be found analytically:<disp-formula id="pcbi.1003408.e011"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e011" xlink:type="simple"/><label>(1)</label></disp-formula><disp-formula id="pcbi.1003408.e012"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e012" xlink:type="simple"/><label>(2)</label></disp-formula><disp-formula id="pcbi.1003408.e013"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e013" xlink:type="simple"/><label>(3)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e014" xlink:type="simple"/></inline-formula> is the effective “energy” function or the Hamiltonian of the system, and the partition function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e015" xlink:type="simple"/></inline-formula> ensures that the distribution is normalized. The couplings <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e016" xlink:type="simple"/></inline-formula> must be set such that the expectation values of all constraint functions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e017" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e018" xlink:type="simple"/></inline-formula>, over the distribution <italic>P</italic> match those measured in the experiment:<disp-formula id="pcbi.1003408.e019"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e019" xlink:type="simple"/><label>(4)</label></disp-formula>These equations might be hard to solve, but they are guaranteed to have exactly one solution for the couplings <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e020" xlink:type="simple"/></inline-formula> given any set of measured expectation values <xref ref-type="bibr" rid="pcbi.1003408-Bazaraa1">[44]</xref>.</p>
<p>Why should we study the neural vocabulary, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e021" xlink:type="simple"/></inline-formula>, at all? In much previous work on neural coding, the focus has been on constructing models for a “codebook” which can predict the response of the neurons to arbitrary stimuli, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e022" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003408-GranotAtedgi1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Pillow1">[45]</xref>, or on building a “dictionary” that describes the stimuli consistent with particular patterns of activity, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e023" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003408-Rieke1">[43]</xref>. In a natural setting, stimuli are drawn from a space of very high dimensionality, so constructing these “encoding” and “decoding” mappings between the stimuli and responses is very challenging and often involves making strong assumptions about how stimuli drive neural spiking (e.g. through linear filtering of the stimulus) <xref ref-type="bibr" rid="pcbi.1003408-Pillow1">[45]</xref>–<xref ref-type="bibr" rid="pcbi.1003408-Sadeghi1">[48]</xref>. While the maximum entropy framework itself can be extended to build stimulus-dependent maximum entropy models for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e024" xlink:type="simple"/></inline-formula> and study detailed encoding and decoding mappings <xref ref-type="bibr" rid="pcbi.1003408-GranotAtedgi1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Tkaik6">[49]</xref>–<xref ref-type="bibr" rid="pcbi.1003408-Ganmor3">[51]</xref>, we choose to focus here directly on the total distribution of responses, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e025" xlink:type="simple"/></inline-formula>, thus taking a very different approach.</p>
<p>Already when we study the smallest possible network, i.e., a pair of interacting neurons, the usual approach is to measure the correlation between spikes generated in the two cells, and to dissect this correlation into contributions which are intrinsic to the network and those which are ascribed to common, stimulus driven inputs. The idea of decomposing correlations dates back to a time when it was hoped that correlations among spikes could be used to map the synaptic connections between neurons <xref ref-type="bibr" rid="pcbi.1003408-Perkel1">[52]</xref>. In fact, in a highly interconnected system, the dominant source of correlations between two neurons—even if they are entirely intrinsic to the network—will always be through the multitude of indirect paths involving other neurons <xref ref-type="bibr" rid="pcbi.1003408-Ginzburg1">[53]</xref>. Regardless of the source of these correlations, however, the question of whether they are driven by the stimulus or are intrinsic to the network is unlikely a question that the brain could answer. We, as external observers, can repeat the stimulus exactly, and search for correlations conditional on the stimulus, but this is not accessible to the organism, unless the brain could build a “noise model” of spontaneous activity of the retina in the absence of any stimuli <italic>and</italic> this model also generalized to stimulus-driven activity. The brain has access only to the output of the retina: the patterns of activity which are drawn from the distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e026" xlink:type="simple"/></inline-formula>, rather than activity conditional on the stimulus, so the neural mechanism by which the correlations could be split into signal and noise components is unclear. If the responses <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e027" xlink:type="simple"/></inline-formula> are codewords for the visual stimulus, then the entropy of this distribution sets the capacity of the code to carry information. Word by word, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e028" xlink:type="simple"/></inline-formula> determines how surprised the brain should be by each particular pattern of response, including the possibility that the response was corrupted by noise in the retinal circuit and thus should be corrected or ignored <xref ref-type="bibr" rid="pcbi.1003408-Hopfield2">[54]</xref>. In a very real sense, what the brain “sees” are sequences of states drawn from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e029" xlink:type="simple"/></inline-formula>. In the same spirit that many groups have studied the statistical structures of natural scenes <xref ref-type="bibr" rid="pcbi.1003408-Field1">[55]</xref>–<xref ref-type="bibr" rid="pcbi.1003408-Stephens3">[60]</xref>, we would like to understand the statistical structure of the codewords that represent these scenes.</p>
<p>The maximum entropy method is not a model for network activity. Rather it is a framework for building models, and to implement this framework we have to choose which functions of the network state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e030" xlink:type="simple"/></inline-formula> we think are interesting. The hope is that while there are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e031" xlink:type="simple"/></inline-formula> states of the system as a whole, there is a much smaller number of measurements, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e032" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e033" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e034" xlink:type="simple"/></inline-formula>, which will be sufficient to capture the essential structure of the collective behavior in the system. We emphasize that this is a hypothesis, and must be tested. How should we choose the functions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e035" xlink:type="simple"/></inline-formula>? In this work we consider three classes of possibilities:</p>
<list list-type="alpha-upper"><list-item>
<p>We expect that networks have very different behaviors depending on the overall probability that neurons generate spikes as opposed to remaining silent. Thus, our first choice of functions to constrain in our models is the set of mean spike probabilities or firing rates, which is equivalent to constraining <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e036" xlink:type="simple"/></inline-formula>, for each neuron i. These constraints contribute a term to the energy function<disp-formula id="pcbi.1003408.e037"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e037" xlink:type="simple"/><label>(5)</label></disp-formula>Note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e038" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e039" xlink:type="simple"/></inline-formula> is the mean spike rate of neuron i, and Δ<italic>τ</italic> is the size of the time slices that we use in our analysis, as in <xref ref-type="fig" rid="pcbi-1003408-g001">Figure 1</xref>. <italic>Maximum entropy models that constrain only the firing rates of all the neurons (i.e. </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e040" xlink:type="simple"/></inline-formula><italic>) are called</italic> “<italic>independent models”; we denote their distribution functions by </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e041" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p>As a second constraint we take the correlations between neurons, two by two. This corresponds to measuring<disp-formula id="pcbi.1003408.e042"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e042" xlink:type="simple"/><label>(6)</label></disp-formula>for every pair of cells ij. These constraints contribute a term to the energy function<disp-formula id="pcbi.1003408.e043"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e043" xlink:type="simple"/><label>(7)</label></disp-formula>It is more conventional to think about correlations between two neurons in terms of their spike trains. If we define<disp-formula id="pcbi.1003408.e044"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e044" xlink:type="simple"/><label>(8)</label></disp-formula>where neuron i spikes at times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e045" xlink:type="simple"/></inline-formula>, then the spike–spike correlation function is <xref ref-type="bibr" rid="pcbi.1003408-Rieke1">[43]</xref><disp-formula id="pcbi.1003408.e046"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e046" xlink:type="simple"/><label>(9)</label></disp-formula>and we also have the average spike rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e047" xlink:type="simple"/></inline-formula>. The correlations among the discrete spike/silence variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e048" xlink:type="simple"/></inline-formula> then can be written as<disp-formula id="pcbi.1003408.e049"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e049" xlink:type="simple"/><label>(10)</label></disp-formula><italic>Maximum entropy models that constrain average firing rates and correlations (i.e.</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e050" xlink:type="simple"/></inline-formula><italic>) are called</italic> “<italic>pairwise models”; we denote their distribution functions by</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e051" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p>Firing rates and pairwise correlations focus on the properties of particular neurons. As an alternative, we can consider quantities that refer to the network as a whole, independent of the identity of the individual neurons. A simple example is the “distribution of synchrony” (also called “population firing rate”), that is, the probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e052" xlink:type="simple"/></inline-formula> that <italic>K</italic> out of the <italic>N</italic> neurons spike in the same small slice of time. We can count the number of neurons that spike by summing all of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e053" xlink:type="simple"/></inline-formula>, remembering that we have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e054" xlink:type="simple"/></inline-formula> for spikes and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e055" xlink:type="simple"/></inline-formula> for silences. Then<disp-formula id="pcbi.1003408.e056"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e056" xlink:type="simple"/><label>(11)</label></disp-formula>where<disp-formula id="pcbi.1003408.e057"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e057" xlink:type="simple"/><label>(12)</label></disp-formula><disp-formula id="pcbi.1003408.e058"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e058" xlink:type="simple"/><label>(13)</label></disp-formula>If we know the distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e059" xlink:type="simple"/></inline-formula>, then we know all its moments, and hence we can think of the functions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e060" xlink:type="simple"/></inline-formula> that we are constraining as being<disp-formula id="pcbi.1003408.e061"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e061" xlink:type="simple"/><label>(14)</label></disp-formula><disp-formula id="pcbi.1003408.e062"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e062" xlink:type="simple"/><label>(15)</label></disp-formula><disp-formula id="pcbi.1003408.e063"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e063" xlink:type="simple"/><label>(16)</label></disp-formula>and so on. Because there are only <italic>N</italic> neurons, there are only <italic>N</italic>+1 possible values of <italic>K</italic>, and hence only <italic>N</italic> unique moments. Constraining all of these moments contributes a term to the energy function<disp-formula id="pcbi.1003408.e064"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e064" xlink:type="simple"/><label>(17)</label></disp-formula>where V is an effective potential <xref ref-type="bibr" rid="pcbi.1003408-Tkaik5">[39]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Okun1">[40]</xref><italic>. Maximum entropy models that constrain average firing rates, correlations, and the distribution of synchrony (i.e. </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e065" xlink:type="simple"/></inline-formula><italic>) are called</italic> “<italic>K-pairwise models”; we denote their distribution functions by </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e066" xlink:type="simple"/></inline-formula>.</p>
</list-item></list>
<p>It is important that the mapping between maximum entropy models and a Boltzmann distribution with some effective energy function is <italic>not</italic> an analogy, but rather a mathematical equivalence. In using the maximum entropy approach we are <italic>not</italic> assuming that the system of interest is in some thermal equilibrium state (note that there is no explicit temperature in <xref ref-type="disp-formula" rid="pcbi.1003408.e011">Eq (1)</xref>), nor are we assuming that there is some mysterious force which drives the system to a state of maximum entropy. We are also not assuming that the temporal dynamics of the network is described by Newton's laws or Brownian motion on the energy landscape. What we are doing is making models that are consistent with certain measured quantities, but otherwise have as little structure as possible. As noted above, this is the opposite of what we usually do in building models or theories—rather than trying to impose some hypothesized structure on the world, we are trying to remove all structures that are not explicitly contained within the chosen set of experimental constraints.</p>
<p>The mapping to a Boltzmann distribution is not an analogy, but if we take the energy function more literally we are making use of analogies. Thus, the term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e067" xlink:type="simple"/></inline-formula> that emerges from constraining the mean spike probabilities of every neuron is analogous to a magnetic field being applied to each spin, where spin “up” (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e068" xlink:type="simple"/></inline-formula>) marks a spike and spin “down” (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e069" xlink:type="simple"/></inline-formula>) denotes silence. Similarly, the term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e070" xlink:type="simple"/></inline-formula> that emerges from constraining the pairwise correlations among neurons corresponds to a “spin–spin” interaction which tends to favor neurons firing together (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e071" xlink:type="simple"/></inline-formula>) or not (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e072" xlink:type="simple"/></inline-formula>). Finally, the constraint on the overall distribution of activity generates a term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e073" xlink:type="simple"/></inline-formula> which we can interpret as resulting from the interaction between all the spins/neurons in the system and one other, hidden degree of freedom, such as an inhibitory interneuron. These analogies can be useful, but need not be taken literally.</p>
</sec></sec><sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Can we learn the model?</title>
<p>We have applied the maximum entropy framework to the analysis of one large experimental data set on the responses of ganglion cells in the salamander retina to a repeated, naturalistic movie. These data are collected using a new generation of multi–electrode arrays that allow us to record from a large fraction of the neurons in a 450×450 µm patch, which contains a total of ∼200 ganglion cells <xref ref-type="bibr" rid="pcbi.1003408-Marre1">[36]</xref>, as in <xref ref-type="fig" rid="pcbi-1003408-g001">Figure 1</xref>. In the present data set, we have selected 160 neurons that pass standard tests for the stability of spike waveforms, the lack of refractory period violations, and the stability of firing across the duration of the experiment (see <xref ref-type="sec" rid="s4">Methods</xref> and Ref <xref ref-type="bibr" rid="pcbi.1003408-Marre1">[36]</xref>). The visual stimulus is a greyscale movie of swimming fish and swaying water plants in a tank; the analyzed chunk of movie is 19 s long, and the recording was stable through 297 repeats, for a total of more than 1.5 hrs of data. As has been found in previous experiments in the retinas of multiple species <xref ref-type="bibr" rid="pcbi.1003408-Schneidman1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Mastronarde1">[61]</xref>–<xref ref-type="bibr" rid="pcbi.1003408-Trong1">[64]</xref>, we found that correlations among neurons are most prominent on the ∼20 ms time scale, and so we chose to discretize the spike train into Δ<italic>τ</italic> = 20 ms bins.</p>
<p>Maximum entropy models have a simple form [<xref ref-type="disp-formula" rid="pcbi.1003408.e011">Eq (1)</xref>] that connects precisely with statistical physics. But to complete the construction of a maximum entropy model, we need to impose the condition that averages in the maximum entropy distribution match the experimental measurements, as in <xref ref-type="disp-formula" rid="pcbi.1003408.e019">Eq (4)</xref>. This amounts to finding all the coupling constants <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e074" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003408.e012">Eq (2)</xref>. This is, in general, a hard problem. We need not only to solve this problem, but also to convince ourselves that our solution is meaningful, and that it does not reflect overfitting to the limited set of data at our disposal. A detailed account of the numerical solution to this inverse problem is given in <xref ref-type="sec" rid="s4"><italic>Methods</italic></xref><italic>: Learning maximum entropy models from data</italic>.</p>
<p>In <xref ref-type="fig" rid="pcbi-1003408-g002">Figure 2</xref> we show an example of <italic>N</italic> = 100 neurons from a small patch of the salamander retina, responding to naturalistic movies. We notice that correlations are weak, but widespread, as in previous experiments on smaller groups of neurons <xref ref-type="bibr" rid="pcbi.1003408-Schneidman1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Tkaik1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Tkaik2">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Puchalla1">[65]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Segev1">[66]</xref>. Because the data set is very large, the threshold for reliable detection of correlations is very low; if we shuffle the data completely by permuting time and repeat indices independently for each neuron, the standard deviation of correlation coefficients,<disp-formula id="pcbi.1003408.e075"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e075" xlink:type="simple"/><label>(18)</label></disp-formula>is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e076" xlink:type="simple"/></inline-formula>, as shown in <xref ref-type="fig" rid="pcbi-1003408-g002">Figure 2C</xref>, vastly smaller than the typical correlations that we observe (median <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e077" xlink:type="simple"/></inline-formula>, 90% of values between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e078" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e079" xlink:type="simple"/></inline-formula>). More subtly, this means that only ∼6.3% percent of the correlation coefficients are within error bars of zero, and there is no sign that there is a large excess fraction of pairs that have truly zero correlation—the distribution of correlations across the population seems continuous. Note that, as customary, we report normalized correlation coefficients (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e080" xlink:type="simple"/></inline-formula>, between −1 and 1), while maximum entropy formally constrains an equivalent set of unnormalized second order moments, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e081" xlink:type="simple"/></inline-formula> [<xref ref-type="disp-formula" rid="pcbi.1003408.e042">Eq (6)</xref>].</p>
<fig id="pcbi-1003408-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003408.g002</object-id><label>Figure 2</label><caption>
<title>Learning the pairwise maximum entropy model for a 100 neuron subset.</title>
<p>A subgroup of 100 neurons from our set of 160 has been sorted by the firing rate. At left, the statistics of the neural activity: (<bold>A</bold>) correlations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e082" xlink:type="simple"/></inline-formula>, (<bold>B</bold>) firing rates (equivalent to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e083" xlink:type="simple"/></inline-formula>), and (<bold>C</bold>) the distribution of correlation coefficients <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e084" xlink:type="simple"/></inline-formula>. The red distribution is the distribution of differences between two halves of the experiment, and the small red error bar marks the standard deviation of correlation coefficients in fully shuffled data (1.8×10<sup>−3</sup>). At right, the parameters of a pairwise maximum entropy model [<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e085" xlink:type="simple"/></inline-formula> from <xref ref-type="disp-formula" rid="pcbi.1003408.e088">Eq (19)</xref>] that reproduces these data: (<bold>D</bold>) coupling constants <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e086" xlink:type="simple"/></inline-formula>, (<bold>E</bold>) fields <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e087" xlink:type="simple"/></inline-formula>, and (<bold>F</bold>) the distribution of couplings in this group of neurons.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003408.g002" position="float" xlink:type="simple"/></fig>
<p>We began by constructing maximum entropy models that match the mean spike rates and pairwise correlations, i.e. “pairwise models,” whose distribution is, from <xref ref-type="disp-formula" rid="pcbi.1003408.e037">Eqs (5</xref>, <xref ref-type="disp-formula" rid="pcbi.1003408.e043">7)</xref>,<disp-formula id="pcbi.1003408.e088"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e088" xlink:type="simple"/><label>(19)</label></disp-formula>When we reconstruct the coupling constants of the maximum entropy model, we see that the “interactions” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e089" xlink:type="simple"/></inline-formula> among neurons are widespread, and almost symmetrically divided between positive and negative values; for more details see <xref ref-type="sec" rid="s4"><italic>Methods</italic></xref><italic>: Learning maximum entropy models from data</italic>. <xref ref-type="fig" rid="pcbi-1003408-g003">Figure 3</xref> shows that the model we construct really does satisfy the constraints, so that the differences, for example, between the measured and predicted correlations among pairs of neurons are within the experimental errors in the measurements.</p>
<fig id="pcbi-1003408-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003408.g003</object-id><label>Figure 3</label><caption>
<title>Reconstruction precision for a 100 neuron subset.</title>
<p>Given the reconstructed Hamiltonian of the pairwise model, we used an independent Metropolis Monte Carlo (MC) sampler to assess how well the constrained model statistics (mean firing rates (<bold>A</bold>), covariances (<bold>B</bold>), plotted on y-axes) match the measured statistics (corresponding x-axes). Error bars on data computed by bootstrapping; error bars on MC estimates obtained by repeated MC runs generating a number of samples that is equal to the original data size. (<bold>C</bold>) The distribution of the difference between true and model values for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e090" xlink:type="simple"/></inline-formula> covariance matrix elements, normalized by the estimated error bar in the data; red overlay is a Gaussian with zero mean and unit variance. The distribution has nearly Gaussian shape with a width of ≈1.1, showing that the learning algorithm reconstructs the covariance statistics to within measurement precision.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003408.g003" position="float" xlink:type="simple"/></fig>
<p>With <italic>N</italic> = 100 neurons, measuring the mean spike probabilities and all the pairwise correlations means that we estimate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e091" xlink:type="simple"/></inline-formula> separate quantities. This is a large number, and it is not clear that we are safe in taking all these measurements at face value. It is possible, for example, that with a finite data set the errors in the different elements of the correlation matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e092" xlink:type="simple"/></inline-formula> are sufficiently strongly correlated that we don't really know the matrix as a whole with high precision, even though the individual elements are measured very accurately. This is a question about overfitting: is it possible that the parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e093" xlink:type="simple"/></inline-formula> are being finely tuned to match even the statistical errors in our data?</p>
<p>To test for overfitting (<xref ref-type="fig" rid="pcbi-1003408-g004">Figure 4</xref>), we exploit the fact that the stimuli consist of a short movie repeated many times. We can choose a random 90% of these repeats from which to learn the parameters of the maximum entropy model, and then check that the probability of the data in the other 10% of the experiment is predicted to be the same, within errors. We see in <xref ref-type="fig" rid="pcbi-1003408-g004">Figure 4</xref> that this is true, and that it remains true as we expand from <italic>N</italic> = 10 neurons (for which we surely have enough data) out to <italic>N</italic> = 120, where we might have started to worry. Taken together, <xref ref-type="fig" rid="pcbi-1003408-g002">Figures 2</xref>, <xref ref-type="fig" rid="pcbi-1003408-g003">3</xref>, and <xref ref-type="fig" rid="pcbi-1003408-g004">4</xref> suggest strongly that our data and algorithms are sufficient to construct maximum entropy models, reliably, for networks of more than one hundred neurons.</p>
<fig id="pcbi-1003408-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003408.g004</object-id><label>Figure 4</label><caption>
<title>A test for overfitting.</title>
<p>(<bold>A</bold>) The per-neuron average log-probability of data (log-likelihood, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e094" xlink:type="simple"/></inline-formula>) under the pairwise model of <xref ref-type="disp-formula" rid="pcbi.1003408.e088">Eq (19)</xref>, computed on the training repeats (black dots) and on the testing repeats (red dots), for the same group of <italic>N</italic> = 100 neurons shown in <xref ref-type="fig" rid="pcbi-1003408-g001">Figure 1</xref> and <xref ref-type="fig" rid="pcbi-1003408-g002">2</xref>. Here the repeats have been reordered so that the training repeats precede testing repeats; in fact, the choice of test repeats is random. (<bold>B</bold>) The ratio of the log-likelihoods on test vs training data, shown as a function of the network size <italic>N</italic>. Error bars are the standard deviation across 30 subgroups at each value of <italic>N</italic>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003408.g004" position="float" xlink:type="simple"/></fig></sec><sec id="s2b">
<title>Do the models work?</title>
<p>How well do our maximum entropy models describe the behavior of large networks of neurons? The models predict the probability of occurrence for all possible combinations of spiking and silence in the network, and it seems natural to use this huge predictive power to test the models. In small networks, this is a useful approach. Indeed, much of the interest in the maximum entropy approach derives from the success of models based on mean spike rates and pairwise correlations, as in <xref ref-type="disp-formula" rid="pcbi.1003408.e088">Eq (19)</xref>, in reproducing the probability distribution over states in networks of size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e095" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003408-Schneidman1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Shlens1">[5]</xref>. With <italic>N</italic> = 10, there are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e096" xlink:type="simple"/></inline-formula> possible combinations of spiking and silence, and reasonable experiments are sufficiently long to estimate the probabilities of all of these individual states. But with <italic>N</italic> = 100, there are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e097" xlink:type="simple"/></inline-formula> possible states, and so it is not possible to “just measure” all the probabilities. Thus, we need another strategy for testing our models.</p>
<p>Striking (and model–independent) evidence for nontrivial collective behavior in these networks is obtained by asking for the probability that <italic>K</italic> out of the <italic>N</italic> neurons generate a spike in the same small window of time, as shown in <xref ref-type="fig" rid="pcbi-1003408-g005">Figure 5</xref>. This distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e098" xlink:type="simple"/></inline-formula>, should become Gaussian at large <italic>N</italic> if the neurons are independent, or nearly so, and we have noted that the correlations between pairs of cells are weak. Thus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e099" xlink:type="simple"/></inline-formula> is very well approximated by an independent model, with fractional errors on the order of the correlation coefficients, typically less than ∼10%. But, even in groups of <italic>N</italic> = 10 cells, there are substantial departures from the predictions of an independent model (<xref ref-type="fig" rid="pcbi-1003408-g005">Figure 5A</xref>). In groups of <italic>N</italic> = 40 cells, we see <italic>K</italic> = 10 cells spiking synchronously with probability ∼10<sup>4</sup> times larger than expected from an independent model (<xref ref-type="fig" rid="pcbi-1003408-g005">Figure 5B</xref>), and the departure from independence is even larger at <italic>N</italic> = 100 (<xref ref-type="fig" rid="pcbi-1003408-g005">Figure 5C</xref>) <xref ref-type="bibr" rid="pcbi.1003408-Ganmor1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Ganmor2">[15]</xref>.</p>
<fig id="pcbi-1003408-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003408.g005</object-id><label>Figure 5</label><caption>
<title>Predicted vs measured probability of <italic>K</italic> simultaneous spikes (spike synchrony).</title>
<p>(<bold>A–C</bold>) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e100" xlink:type="simple"/></inline-formula> for subnetworks of size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e101" xlink:type="simple"/></inline-formula>; error bars are s.d. across random halves of the duration of the experiment. For <italic>N</italic> = 10 we already see large deviations from an independent model, but these are captured by the pairwise model. At <italic>N</italic> = 40 (B), the pairwise models miss the tail of the distribution, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e102" xlink:type="simple"/></inline-formula>. At <italic>N</italic> = 100 (C), the deviations between the pairwise model and the data are more substantial. (<bold>D</bold>) The probability of silence in the network, as a function of population size; error bars are s.d. across 30 subgroups of a given size <italic>N</italic>. Throughout, red shows the data, grey the independent model, and black the pairwise model.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003408.g005" position="float" xlink:type="simple"/></fig>
<p>Maximum entropy models that match the mean spike rate and pairwise correlations in a network make an unambiguous, quantitative prediction for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e103" xlink:type="simple"/></inline-formula>, with no adjustable parameters. In smaller groups of neurons, certainly for <italic>N</italic> = 10, this prediction is quite accurate, and accounts for most of the difference between the data and the expectations from an independent model, as shown in <xref ref-type="fig" rid="pcbi-1003408-g005">Figure 5</xref>. But even at <italic>N</italic> = 40 we see small deviations between the data and the predictions of the pairwise model. Because the silent state is highly probable, we can measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e104" xlink:type="simple"/></inline-formula> very accurately, and the pairwise models make errors of nearly a factor of three at <italic>N</italic> = 100, and independent models are off by a factor of about twenty. The pairwise model errors in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e105" xlink:type="simple"/></inline-formula> are negligible when compared to the many orders of magnitude differences from an independent model, but they are highly significant. The pattern of errors also is important, since in the real networks silence persists as being highly probable even at <italic>N</italic> = 120—with indications that this surprising trend might continue towards larger <italic>N</italic> <xref ref-type="bibr" rid="pcbi.1003408-Tkaik5">[39]</xref> —and the pairwise model doesn't quite capture this.</p>
<p>If a model based on pairwise correlations doesn't quite account for the data, it is tempting to try and include correlations among triplets of neurons. But at <italic>N</italic> = 100 there are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e106" xlink:type="simple"/></inline-formula> of these triplets, so a model that includes these correlations is much more complex than one that stops with pairs. An alternative is to use <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e107" xlink:type="simple"/></inline-formula> itself as a constraint on our models, as explained above in relation to <xref ref-type="disp-formula" rid="pcbi.1003408.e064">Eq (17)</xref>. This defines the “K-pairwise model,”<disp-formula id="pcbi.1003408.e108"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e108" xlink:type="simple"/><label>(20)</label></disp-formula>where the “potential” <italic>V</italic> is chosen to match the observed distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e109" xlink:type="simple"/></inline-formula>. As noted above, we can think of this potential as providing a global regulation of the network activity, such as might be implemented by inhibitory interneurons with (near) global connectivity. Whatever the mechanistic interpretation of this model, it is important that it is <italic>not</italic> much more complex than the pairwise model: matching <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e110" xlink:type="simple"/></inline-formula> adds only ∼<italic>N</italic> parameters to our model, while the pairwise model already has <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e111" xlink:type="simple"/></inline-formula> parameters. All of the tests given in the previous section can be redone in this case, and again we find that we can learn the K-pairwise models from the available data with no signs of overfitting. <xref ref-type="fig" rid="pcbi-1003408-g006">Figure 6</xref> shows the parameters of the K-pairwise model for the same group of <italic>N</italic> = 100 neurons shown in <xref ref-type="fig" rid="pcbi-1003408-g002">Figure 2</xref>. Notice that the pairwise interaction terms <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e112" xlink:type="simple"/></inline-formula> remain roughly the same; the local fields <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e113" xlink:type="simple"/></inline-formula> are also similar but have a shift towards more negative values.</p>
<fig id="pcbi-1003408-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003408.g006</object-id><label>Figure 6</label><caption>
<title>K-pairwise model for a the same group of <italic>N</italic> = 100 cells shown in <xref ref-type="fig" rid="pcbi-1003408-g001">Figure 1</xref>.</title>
<p>The neurons are again sorted in the order of decreasing firing rates. (<bold>A</bold>) Pairwise interactions, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e114" xlink:type="simple"/></inline-formula>, and the comparison with the interactions of the pairwise model, (<bold>B</bold>). (<bold>C</bold>) Single-neuron fields, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e115" xlink:type="simple"/></inline-formula>, and the comparison with the fields of the pairwise model, (<bold>D</bold>). (<bold>E</bold>) The global potential, <italic>V</italic>(<italic>K</italic>), where <italic>K</italic> is the number of synchronous spikes. See <xref ref-type="sec" rid="s4"><italic>Methods</italic></xref><italic>: Parametrization of the K-pairwise model</italic> for details.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003408.g006" position="float" xlink:type="simple"/></fig>
<p>Since we didn't make explicit use of the triplet correlations in constructing the K-pairwise model, we can test the model by predicting these correlations. In <xref ref-type="fig" rid="pcbi-1003408-g007">Figure 7A</xref> we show<disp-formula id="pcbi.1003408.e116"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e116" xlink:type="simple"/><label>(21)</label></disp-formula>as computed from the real data and from the models, for a single group of <italic>N</italic> = 100 neurons. We see that pairwise models capture the rankings of the different triplets, so that more strongly correlated triplets are predicted to be more strongly correlated, but these models miss quantitatively, overestimating the positive correlations and failing to predict significantly negative correlations. These errors are largely corrected in the K-pairwise model, despite the fact that adding a constraint on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e117" xlink:type="simple"/></inline-formula> doesn't add any information about the identity of the neurons in the different triplets. Specifically, <xref ref-type="fig" rid="pcbi-1003408-g007">Figure 7A</xref> shows that the biases of the pairwise model in the prediction of three-point correlations have been largely removed (with some residual deviations at large absolute values of the three-point correlation) by adding the K-spike constraint; on the other hand, the variance of predictions across bins containing three-point correlations of approximately the same magnitude did not decrease substantially. It is also interesting that this improvement in our predictions (as well as that in <xref ref-type="fig" rid="pcbi-1003408-g008">Figure 8</xref> below) occurs even though the numerical value of the effective potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e118" xlink:type="simple"/></inline-formula> is quite small, as shown in <xref ref-type="fig" rid="pcbi-1003408-g006">Figure 6E</xref> (quantitatively, in an example group of <italic>N</italic> = 100 neurons, the variance in energy associated with the <italic>V</italic>(<italic>K</italic>) potential accounts roughly for only 5% of the total variance in energy). Fixing the distribution of global activity thus seems to capture something about the network that individual spike probabilities and pairwise correlations have missed.</p>
<fig id="pcbi-1003408-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003408.g007</object-id><label>Figure 7</label><caption>
<title>Predicted vs real connected three–point correlations, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e119" xlink:type="simple"/></inline-formula> from <xref ref-type="disp-formula" rid="pcbi.1003408.e116">Eq (21)</xref>.</title>
<p>(<bold>A</bold>) Measured <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e120" xlink:type="simple"/></inline-formula> (x-axis) vs predicted by the model (y-axis), shown for an example 100 neuron subnetwork. The ∼1.6×10<sup>5</sup> triplets are binned into 1000 equally populated bins; error bars in x are s.d. across the bin. The corresponding values for the predictions are grouped together, yielding the mean and the s.d. of the prediction (y-axis). Inset shows a zoom-in of the central region, for the K-pairwise model. (<bold>B</bold>) Error in predicted three-point correlation functions as a function of subnetwork size <italic>N</italic>. Shown are mean absolute deviations of the model prediction from the data, for pairwise (black) and K-pairwise (red) models; error bars are s.d. across 30 subnetworks at each <italic>N</italic>, and the dashed line shows the mean absolute difference between two halves of the experiment. Inset shows the distribution of three–point correlations (grey filled region) and the distribution of differences between two halves of the experiment (dashed line); note the logarithmic scale.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003408.g007" position="float" xlink:type="simple"/></fig><fig id="pcbi-1003408-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003408.g008</object-id><label>Figure 8</label><caption>
<title>Predicted vs real distributions of energy, <italic>E</italic>.</title>
<p>(<bold>A</bold>) The cumulative distribution of energies, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e121" xlink:type="simple"/></inline-formula> from <xref ref-type="disp-formula" rid="pcbi.1003408.e128">Eq (22)</xref>, for the K-pairwise models (red) and the data (black), in a population of 120 neurons. Inset shows the high energy tails of the distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e122" xlink:type="simple"/></inline-formula> from <xref ref-type="disp-formula" rid="pcbi.1003408.e131">Eq (24)</xref>; dashed line denotes the energy that corresponds to the probability of seeing the pattern once in an experiment. See <xref ref-type="supplementary-material" rid="pcbi.1003408.s005">Figure S5</xref> for an analogous plot for the pairwise model. (<bold>B</bold>) Relative difference in the first two moments (mean, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e123" xlink:type="simple"/></inline-formula>, dashed; standard deviation, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e124" xlink:type="simple"/></inline-formula>, solid) of the distribution of energies evaluated over real data and a sample from the corresponding model (black = pairwise; red = K-pairwise). Error bars are s.d. over 30 subnetworks at a given size <italic>N</italic>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003408.g008" position="float" xlink:type="simple"/></fig>
<p>An interesting effect is shown in <xref ref-type="fig" rid="pcbi-1003408-g007">Figure 7B</xref>, where we look at the average absolute deviation between predicted and measured <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e125" xlink:type="simple"/></inline-formula>, as a function of the group size <italic>N</italic>. With increasing <italic>N</italic> the ratio between the total number of (predicted) three-point correlations and (fitted) model parameters is increasing (from ≈2 at <italic>N</italic> = 10 to ≈40 for <italic>N</italic> = 120), leading us to believe that predictions will grow progressively worse. Nevertheless, the average error in three-point prediction stays constant with network size, for both pairwise and K-pairwise models. An attractive explanation is that, as <italic>N</italic> increases, the models encompass larger and larger fractions of the interacting neural patch and thus decrease the effects of “hidden” units, neurons that are present but not included in the model; such unobserved units, even if they only interacted with other units in a pairwise fashion, could introduce effective higher-order interactions between observed units, thereby causing three-point correlation predictions to deviate from those of the pairwise model <xref ref-type="bibr" rid="pcbi.1003408-Schneidman2">[67]</xref>. The accuracy of the K-pairwise predictions is not quite as good as the errors in our measurements (dashed line in <xref ref-type="fig" rid="pcbi-1003408-g007">Figure 7B</xref>), but still very good, improving by a factor of ∼2 relative to the pairwise model to well below 10<sup>−3</sup>.</p>
<p>Maximum entropy models assign an effective energy to every possible combination of spiking and silence in the network, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e126" xlink:type="simple"/></inline-formula> from <xref ref-type="disp-formula" rid="pcbi.1003408.e108">Eq (20)</xref>. Learning the model means specifying all the parameters in this expression, so that the mapping from states to energies is completely determined. The energy determines the probability of the state, and while we can't estimate the probabilities of all possible states, we can ask whether the distribution of energies that we see in the data agrees with the predictions of the model. Thus, if we have a set of states drawn out of a distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e127" xlink:type="simple"/></inline-formula>, we can count the number of states that have energies lower than <italic>E</italic>,<disp-formula id="pcbi.1003408.e128"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e128" xlink:type="simple"/><label>(22)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e129" xlink:type="simple"/></inline-formula> is the Heaviside step function,<disp-formula id="pcbi.1003408.e130"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e130" xlink:type="simple"/><label>(23)</label></disp-formula>Similarly, we can count the number of states that have energy larger than <italic>E</italic>,<disp-formula id="pcbi.1003408.e131"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e131" xlink:type="simple"/><label>(24)</label></disp-formula>Now we can take the distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e132" xlink:type="simple"/></inline-formula> to be the distribution of states that we actually see in the experiment, or we can take it to be the distribution predicted by the model, and if the model is accurate we should find that the cumulative distributions are similar in these two cases. Results are shown in <xref ref-type="fig" rid="pcbi-1003408-g008">Figure 8A</xref> (analogous results for the pairwise model are shown in <xref ref-type="supplementary-material" rid="pcbi.1003408.s005">Figure S5</xref>). <xref ref-type="fig" rid="pcbi-1003408-g008">Figure 8B</xref> focuses on the agreement between the first two moments of the distribution of energies, i.e., the mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e133" xlink:type="simple"/></inline-formula> and variance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e134" xlink:type="simple"/></inline-formula>, as a function of the network size <italic>N</italic>, showing that the K-pairwise model is significantly better at matching the variance of the energies relative to the pairwise model.</p>
<p>We see that the distributions of energies in the data and the model are very similar. There is an excellent match in the “low energy” (high probability) region, and then as we look at the high energy tail (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e135" xlink:type="simple"/></inline-formula>) we see that theory and experiment match out to probabilities of better than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e136" xlink:type="simple"/></inline-formula>. Thus the distribution of energies, which is an essential construct of the model, seems to match the data across &gt;90% of the states that we see.</p>
<p>The successful prediction of the cumulative distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e137" xlink:type="simple"/></inline-formula> is especially striking because it extends to <italic>E</italic>∼25. At these energies, the probability of any single state is predicted to be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e138" xlink:type="simple"/></inline-formula>, which means that these states should occur roughly once per fifty years (!). This seems ridiculous—what are such rare states doing in our analysis, much less as part of the claim that theory and experiment are in quantitative agreement? The key is that there are many, many of these rare states—so many, in fact, that the theory is predicting that ∼10% of the all the states we observe will be (at least) this rare: individually surprising events are, as a group, quite common. In fact, of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e139" xlink:type="simple"/></inline-formula> combinations of spiking and silence (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e140" xlink:type="simple"/></inline-formula> distinct ones) that we see in subnetworks of <italic>N</italic> = 120 neurons, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e141" xlink:type="simple"/></inline-formula> of these occur only once, which means we really don't know anything about their probability of occurrence. We can't say that the probability of any one of these rare states is being predicted correctly by the model, since we can't measure it, but we can say that the distribution of (log) probabilities—that is, the distribution of energies—across the set of observed states is correct, down to the ∼10% level. The model thus is predicting things far beyond what can be inferred directly from the frequencies with which common patterns are observed to occur in realistic experiments.</p>
<p>Finally, the structure of the models we are considering is that the state of each neuron—an Ising spin—experiences an “effective field” from all the other spins, determining the probability of spiking vs. silence. This effective field consists of an intrinsic bias for each neuron, plus the effects of interactions with all the other neurons:<disp-formula id="pcbi.1003408.e142"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e142" xlink:type="simple"/><label>(25)</label></disp-formula>If the model is correct, then the probability of spiking is simply related to the effective field,<disp-formula id="pcbi.1003408.e143"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e143" xlink:type="simple"/><label>(26)</label></disp-formula>To test this relationship, we can choose one neuron, compute the effective field from the states of all the other neurons, at every moment in time, then collect all those moments when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e144" xlink:type="simple"/></inline-formula> is in some narrow range, and see how often the neuron spikes. We can then repeat this for every neuron, in turn. If the model is correct, spiking probability should depend on the effective field according to <xref ref-type="disp-formula" rid="pcbi.1003408.e143">Eq (26)</xref>. We emphasize that there are no new parameters to be fit, but rather a parameter–free relationship to be tested. The results are shown in <xref ref-type="fig" rid="pcbi-1003408-g009">Figure 9</xref>. We see that, throughout the range of fields that are well sampled in the experiment, there is good agreement between the data and <xref ref-type="disp-formula" rid="pcbi.1003408.e143">Eq (26)</xref>. As we go into the tails of the distribution, we see some deviations, but error bars also are (much) larger.</p>
<fig id="pcbi-1003408-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003408.g009</object-id><label>Figure 9</label><caption>
<title>Effective field and spiking probabilities in a network of <italic>N</italic> = 120 neurons.</title>
<p>Given any configuration of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e145" xlink:type="simple"/></inline-formula> neurons, the K-pairwise model predicts the probability of firing of the <italic>N</italic>-th neuron by <xref ref-type="disp-formula" rid="pcbi.1003408.e142">Eqs (25</xref>,<xref ref-type="disp-formula" rid="pcbi.1003408.e143">26)</xref>; the effective field <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e146" xlink:type="simple"/></inline-formula> is fully determined by the parameters of the maximum entropy model and the state of the network. For each activity pattern in recorded data we computed the effective field, and binned these values (shown on x-axis). For every bin we estimated from data the probability that the <italic>N</italic>-th neuron spiked (black circles; error bars are s.d. across 120 cells). This is compared with a parameter-free prediction (red line) from <xref ref-type="disp-formula" rid="pcbi.1003408.e143">Eq (26)</xref>. For comparison, gray squares show the analogous analysis for the pairwise model (error bars omitted for clarity, comparable to K-pairwise models). Inset: same curves shown on the logarithmic plot emphasizing the low range of effective fields. The gray shaded region shows the distribution of the values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e147" xlink:type="simple"/></inline-formula> over all 120 neurons and all patterns in the data.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003408.g009" position="float" xlink:type="simple"/></fig></sec><sec id="s2c">
<title>What do the models teach us?</title>
<p>We have seen that it is possible to construct maximum entropy models which match the mean spike probabilities of each cell, the pairwise correlations, and the distribution of summed activity in the network, and that our data are sufficient to insure that all the parameters of these models are well determined, even when we consider groups of <italic>N</italic> = 100 neurons or more. <xref ref-type="fig" rid="pcbi-1003408-g007">Figures 7</xref> through <xref ref-type="fig" rid="pcbi-1003408-g009">9</xref> indicate that these models give a fairly accurate description of the distribution of states—the myriad combinations of spiking and silence—taken on by the network as a whole. In effect we have constructed a statistical mechanics for these networks, not by analogy or metaphor but in quantitative detail. We now have to ask what we can learn about neural function from this description.</p>
<sec id="s2c1">
<title>Basins of attraction</title>
<p>In the Hopfield model, dynamics of the neural network corresponds to motion on an energy surface. Simple learning rules can sculpt the energy surface to generate multiple local minima, or attractors, into which the system can settle. These local minima can represent stored memories, or the solutions to various computational problems <xref ref-type="bibr" rid="pcbi.1003408-Hopfield3">[68]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Hopfield4">[69]</xref>. If we imagine monitoring a Hopfield network over a long time, the distribution of states that it visits will be dominated by the local minima of the energy function. Thus, even if we can't take the details of the dynamical model seriously, it still should be true that the energy landscape determines the probability distribution over states in a Boltzmann–like fashion, with multiple energy minima translating into multiple peaks of the distribution.</p>
<p>In our maximum entropy models, we find a range of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e148" xlink:type="simple"/></inline-formula> values encompassing both signs (<xref ref-type="fig" rid="pcbi-1003408-g002">Figures 2D and F</xref>), as in spin glasses <xref ref-type="bibr" rid="pcbi.1003408-Mezard1">[70]</xref>. The presence of such competing interactions generates “frustration,” where (for example) triplets of neurons cannot find a combination of spiking and silence that simultaneously minimizes all the terms in the energy function <xref ref-type="bibr" rid="pcbi.1003408-Schneidman1">[4]</xref>. In the simplest model of spin glasses, these frustration effects, distributed throughout the system, give rise to a very complex energy landscape, with a proliferation of local minima <xref ref-type="bibr" rid="pcbi.1003408-Mezard1">[70]</xref>. Our models are not precisely Hopfield models, nor are they instances of the standard (more random) spin glass models. Nonetheless, by looking at the pairwise <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e149" xlink:type="simple"/></inline-formula> terms in the energy function of our models, 48±2% of all interacting triplets of neurons are frustrated across different subnetworks of various sizes (<italic>N</italic>≥40), and it is reasonable to expect that we will find many local minima in the energy function of the network.</p>
<p>To search for local minima of the energy landscape, we take every combination of spiking and silence observed in the data and move “downhill” on the function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e150" xlink:type="simple"/></inline-formula> from <xref ref-type="disp-formula" rid="pcbi.1003408.e108">Eq (20)</xref> (see <xref ref-type="sec" rid="s4"><italic>Methods</italic></xref><italic>: Exploring the energy landscape</italic>). When we can no longer move downhill, we have identified a locally stable pattern of activity, or a “metastable state,” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e151" xlink:type="simple"/></inline-formula>, such that a flip of any single spin—switching the state of any one neuron from spiking to silent, or vice versa—increases the energy or decreases the predicted probability of the new state. This procedure also partitions the space of all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e152" xlink:type="simple"/></inline-formula> possible patterns into domains, or basins of attraction, centered on the metastable states, and compresses the microscopic description of the retinal state to a number <italic>α</italic> identifying the basin to which that state belongs.</p>
<p><xref ref-type="fig" rid="pcbi-1003408-g010">Figure 10</xref> shows how the number of metastable states that we identify in the data grows with the size <italic>N</italic> of the network. At very small <italic>N</italic>, the only stable configuration is the all-silent state, but for <italic>N</italic>&gt;30 the metastable states start to proliferate. Indeed, we see no sign that the number of metastable states is saturating, and the growth is certainly faster than linear in the number of neurons. Moreover, the total numbers of possible metastable states in the models' energy landscapes could be substantially higher than shown, because we only count those states that are accessible by descending from patterns <italic>observed in the experiment</italic>. It thus is possible that these real networks exceed the “capacity” of model networks <xref ref-type="bibr" rid="pcbi.1003408-Amit1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Hertz1">[3]</xref>.</p>
<fig id="pcbi-1003408-g010" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003408.g010</object-id><label>Figure 10</label><caption>
<title>The number of identified metastable patterns.</title>
<p>Every recorded pattern is assigned to its basin of attraction by descending on the energy landscape. The number of distinct basins is shown as a function of the network size, <italic>N</italic>, for K-pairwise models (black line). Gray lines show the subsets of those basins that are encountered multiple times in the recording (more than 10 times, dark gray; more than 100 times, light gray). Error bars are s.d. over 30 subnetworks at every <italic>N</italic>. Note the logarithmic scale for the number of MS states.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003408.g010" position="float" xlink:type="simple"/></fig>
<p><xref ref-type="fig" rid="pcbi-1003408-g011">Figure 11A</xref> provides a more detailed view of the most prominent metastable states, and the “energy valleys” that surround them. The structure of the energy valleys can be thought of as clustering the patterns of neural activity, although in contrast to the usual formulation of clustering we don't need to make an arbitrary choice of metric for similarity among patterns. Nonetheless, we can measure the overlap <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e153" xlink:type="simple"/></inline-formula> between all pairs of patterns <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e154" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e155" xlink:type="simple"/></inline-formula> that we see in the experiment,<disp-formula id="pcbi.1003408.e156"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e156" xlink:type="simple"/><label>(27)</label></disp-formula>and we find that patterns which fall into the same valley are much more correlated with one another than they are with patterns that fall into other valleys (<xref ref-type="fig" rid="pcbi-1003408-g011">Figure 11B</xref>). If we start at one of the metastable states and take a random “uphill” walk in the energy landscape (<xref ref-type="sec" rid="s4"><italic>Methods</italic></xref><italic>: Exploring the energy landscape</italic>), we eventually reach a transition state where there is a downhill path into other metastable states, and a selection of these trajectories is shown in <xref ref-type="fig" rid="pcbi-1003408-g011">Figure 11C</xref>. Importantly, the transition states are at energies quite high relative to the metastable states (<xref ref-type="fig" rid="pcbi-1003408-g011">Figure 11D</xref>), so the peaks of the probability distribution are well resolved from one another. In many cases it takes a large number of steps to find the transition state, so that the metastable states are substantially separated in Hamming distance.</p>
<fig id="pcbi-1003408-g011" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003408.g011</object-id><label>Figure 11</label><caption>
<title>Energy landscape in a <italic>N</italic> = 120 neuron K-pairwise model.</title>
<p>(<bold>A</bold>) The 10 most frequently occurring metastable (MS) states (active neurons for each in red), and 50 randomly chosen activity patterns for each MS state (black dots represent spikes). MS 1 is the all-silent basin. (<bold>B</bold>) The overlaps, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e157" xlink:type="simple"/></inline-formula>, between all pairs of identified patterns belonging to basins 2,…,10 (MS 1 left out due to its large size). Patterns within the same basin are much more similar between themselves than to patterns belonging to other basins. (<bold>C</bold>) The structure of the energy landscape explored with Monte Carlo. Starting in the all-silent state, single spin-flip steps are taken until the configuration crosses the energy barrier into another basin. Here, two such paths are depicted (green, ultimately landing in the basin of MS 9; purple, landing in basin of MS 5) as projections into 3D space of scalar products (overlaps) with the MS 1, 5, and 9. (<bold>D</bold>) The detailed structure of the energy landscape. 10 MS patterns from (A) are shown in the energy (y-axis) vs log basin size (x-axis) diagram (silent state at lower right corner). At left, transitions frequently observed in MC simulations starting in each of the 10 MS states, as in (C). The most frequent transitions are decays to the silent state. Other frequent transitions (and their probabilities) shown using vertical arrows between respective states. Typical transition statistics (for MS 3 decaying into the silent state) shown in the inset: the distribution of spin-flip attempts needed, <italic>P</italic>(<italic>L</italic>), and the distribution of energy barriers, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e158" xlink:type="simple"/></inline-formula>, over 1000 observed transitions.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003408.g011" position="float" xlink:type="simple"/></fig>
<p>Individual neurons in the retina are known to generate rather reproducible responses to naturalistic stimuli <xref ref-type="bibr" rid="pcbi.1003408-Marre1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Puchalla1">[65]</xref>, but even a small amount of noise in the response of single cells is enough to ensure that groups of <italic>N</italic> = 100 neurons almost never generate the same response to two repetitions of the same visual stimulus. It is striking, then, that when we show the same movie again, the retina revisits the same basin of attraction with very high probability, as shown in <xref ref-type="fig" rid="pcbi-1003408-g012">Figure 12</xref>. The same metastable states and corresponding valleys are identifiable from different subsets of the full population, providing a measure of redundancy that we explore more fully below. Further, the transitions into and out of these valleys are very rapid, with a time scale of just ∼2.5Δ<italic>τ</italic>. In summary, the neural code for visual signals seems to have a structure in which repeated presentations of the stimulus produce patterns of response that fall within the same basins of our model's landscape, despite the fact that the energy landscape is constructed without explicitly incorporating any stimulus dependence or prior knowledge of its repeated trial.</p>
<fig id="pcbi-1003408-g012" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003408.g012</object-id><label>Figure 12</label><caption>
<title>Basin assignments are reproducible across stimulus repeats and across subnetworks.</title>
<p>(<bold>A</bold>) Most frequently occurring MS patterns collected from 30 subnetworks of size <italic>N</italic> = 120 out of a total population of 160 neurons; patterns have been clustered into 12 clusters (colors). (<bold>B</bold>) The probability (across stimulus repeats) that the population is in a particular basin of attraction at any given time. Each line corresponds to one pattern from (A); patterns belonging to the same cluster are depicted in the same color. Inset shows the detailed structure of several transitions out of the all-silent state; overlapping lines of the same color show that the same transition is identified robustly across different subnetwork choices of 120 neurons out of 160. (<bold>C</bold>) On about half of the time bins, the population is in the all-silent basin; on the remaining time bins, the coherence (the probability of being in the dominant basin divided by the probability of being in every possible non-silent basin) is high. (<bold>D</bold>) The average autocorrelation function of traces in (B), showing the typical time the population stays within a basin (dashed red line is best exponential fit with <italic>τ</italic> = 48 ms, or about 2.5 time bins).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003408.g012" position="float" xlink:type="simple"/></fig></sec><sec id="s2c2">
<title>Entropy</title>
<p>Central to our understanding of neural coding is the entropy of the responses <xref ref-type="bibr" rid="pcbi.1003408-Rieke1">[43]</xref>. Conceptually, the entropy measures the size of the neural vocabulary: with <italic>N</italic> neurons there are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e159" xlink:type="simple"/></inline-formula> <italic>possible</italic> configurations of spiking and silence, but since not all of these have equal probabilities—some, like the all-silent pattern, may occur orders of magnitude more frequently than others, such as the all-spikes pattern—the <italic>effective</italic> number of configurations is reduced to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e160" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e161" xlink:type="simple"/></inline-formula> is the entropy of the vocabulary for the network of <italic>N</italic> neurons. Furthermore, if the patterns of spiking and silence really are codewords for the stimulus, then the mutual information between the stimulus and response, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e162" xlink:type="simple"/></inline-formula>, can be at most the entropy of the codewords, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e163" xlink:type="simple"/></inline-formula>. Thus, the entropy of the system's output bounds the information transmission. This is true even if the output words are correlated in time; temporal correlations imply that the entropy of state sequences is smaller than expected from the entropy of single snapshots, as studied here, and hence the limits on information transmission are even more stringent <xref ref-type="bibr" rid="pcbi.1003408-GranotAtedgi1">[14]</xref>.</p>
<p>We cannot sample the distribution—and thus estimate the entropy directly—for large sets of neurons, but we know that maximum entropy models with constraints <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e164" xlink:type="simple"/></inline-formula> put an upper bound to the true entropy, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e165" xlink:type="simple"/></inline-formula>. Unfortunately, even computing the entropy of our model distribution is not simple. Naively, we could draw samples out of the model via Monte Carlo, and since simulations can run longer than experiments, we could hope to accumulate enough samples to make a direct estimate of the entropy, perhaps using more sophisticated methods for dealing with sample size dependences <xref ref-type="bibr" rid="pcbi.1003408-Nemenman1">[71]</xref>. But this is terribly inefficient (see <xref ref-type="sec" rid="s4"><italic>Methods</italic></xref><italic>: Computing the entropy and the partition function</italic>). An alternative is to make more thorough use of the mathematical equivalence between maximum entropy models and statistical mechanics.</p>
<p>The first approach to entropy estimation involves extending our maximum entropy models of <xref ref-type="disp-formula" rid="pcbi.1003408.e012">Eq (2)</xref> by introducing a parameter analogous to the temperature <italic>T</italic> in statistical physics:<disp-formula id="pcbi.1003408.e166"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e166" xlink:type="simple"/><label>(28)</label></disp-formula>Thus, for <italic>T</italic> = 1, the distribution in <xref ref-type="disp-formula" rid="pcbi.1003408.e166">Eq (28)</xref> is exactly equal to the maximum entropy model with parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e167" xlink:type="simple"/></inline-formula>, but by varying <italic>T</italic> and keeping the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e168" xlink:type="simple"/></inline-formula> constant, we access a one-parameter family of distributions. Unlike in statistical physics, <italic>T</italic> here is purely a mathematical device, and we do not consider the distributions at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e169" xlink:type="simple"/></inline-formula> as describing any real network of neurons. One can nevertheless compute, for each of these distributions at temperature <italic>T</italic>, the heat capacity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e170" xlink:type="simple"/></inline-formula>, and then thermodynamics teaches us that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e171" xlink:type="simple"/></inline-formula>; we can thus invert this relation to compute the entropy:<disp-formula id="pcbi.1003408.e172"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e172" xlink:type="simple"/><label>(29)</label></disp-formula></p>
<p>The heat capacity might seem irrelevant since there is no “heat” in our problem, but this quantity is directly related to the variance of energy, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e173" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e174" xlink:type="simple"/></inline-formula> as in <xref ref-type="fig" rid="pcbi-1003408-g008">Figure 8</xref>. The energy, in turn, is related to the logarithm of the probability, and hence the heat capacity is the variance in how surprised we should be by any state drawn out of the distribution. In practice, we can draw sample states from a Monte Carlo simulation, compute the energy of each such state, and estimate the variance over a long simulation. Importantly, it is well known that such estimates stabilize long before we have collected enough samples to visit every state of the system <xref ref-type="bibr" rid="pcbi.1003408-Landau1">[72]</xref>. Thus, we start with the inferred maximum entropy model, generate a dense family of distributions at different <italic>T</italic> spanning the values from 0 to 1, and, from each distribution, generate enough samples to estimate the variance of energy and thus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e175" xlink:type="simple"/></inline-formula>; finally, we do the integral in <xref ref-type="disp-formula" rid="pcbi.1003408.e172">Eq (29)</xref>.</p>
<p>Interestingly, the mapping to statistical physics gives us other, independent ways of estimating the entropy. The most likely state of the network, in all the cases we have explored, is complete silence. Further, in the K-pairwise models, this probability is reproduced exactly, since it is just <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e176" xlink:type="simple"/></inline-formula>. Mathematically, this probability is given by<disp-formula id="pcbi.1003408.e177"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e177" xlink:type="simple"/><label>(30)</label></disp-formula>where the energy of the silent state is easily computed from the model just by plugging in to the Hamiltonian in <xref ref-type="disp-formula" rid="pcbi.1003408.e108">Eq (20)</xref>; in fact we could choose our units so that the silent state has precisely zero energy, making this even easier. But then we see that, in this model, estimating the probability of silence (which we can do directly from the data) is the same as estimating the partition function <italic>Z</italic>, which usually is very difficult since it involves summing over all possible states. Once we have <italic>Z</italic>, we know from statistical mechanics that<disp-formula id="pcbi.1003408.e178"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e178" xlink:type="simple"/><label>(31)</label></disp-formula>and we can estimate the average energy from a single Monte Carlo simulation of the model at the “real” <italic>T</italic> = 1 (cf. <xref ref-type="fig" rid="pcbi-1003408-g008">Figure 8</xref>).</p>
<p>Finally, there are more sophisticated Monte Carlo resampling methods that generate an estimate of the “density of states” <xref ref-type="bibr" rid="pcbi.1003408-Wang1">[73]</xref>, related to the cumulative distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e179" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e180" xlink:type="simple"/></inline-formula> discussed above, and from this density we can compute the partition function directly. As explained in <xref ref-type="sec" rid="s4"><italic>Methods</italic></xref><italic>: Computing the entropy and the partition function</italic>, the three different methods of entropy estimation agree to better than 1% on groups of <italic>N</italic> = 120 neurons.</p>
<p><xref ref-type="fig" rid="pcbi-1003408-g013">Figure 13A</xref> shows the entropy per neuron of the K-pairwise model as a function of network size, <italic>N</italic>. For comparison, we also plot the independent entropy, i.e. the entropy of the non-interacting maximum entropy model that matches the mean firing rate of every neuron defined in <xref ref-type="disp-formula" rid="pcbi.1003408.e037">Eq (5)</xref>. It is worth noting that despite the diversity of firing rates for individual neurons, and the broad distribution of correlations in pairs of neurons, the entropy per neuron varies hardly at all as we look at different groups of neurons chosen out of the larger group from which we can record. This suggests that collective, “thermodynamic” properties of the network may be robust to some details of the neural population, as discussed in the <xref ref-type="sec" rid="s1">Introduction</xref>. These entropy differences between the independent and correlated models may not seem large, but losing Δ<italic>S</italic> = 0.05 bits of entropy per neuron means that for <italic>N</italic> = 200 neurons the vocabulary of neural responses is restricted <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e181" xlink:type="simple"/></inline-formula>–fold.</p>
<fig id="pcbi-1003408-g013" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003408.g013</object-id><label>Figure 13</label><caption>
<title>Entropy and multi-information from the K-pairwise model.</title>
<p>(<bold>A</bold>) Independent entropy per neuron, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e182" xlink:type="simple"/></inline-formula>, in black, and the entropy of the K-pairwise models per neuron, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e183" xlink:type="simple"/></inline-formula>, in red, as a function of <italic>N</italic>. Dashed lines are fits from (B). (<bold>B</bold>) Independent entropy scales linearly with <italic>N</italic> (black dashed line). Multi-information <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e184" xlink:type="simple"/></inline-formula> of the K-pairwise models is shown in dark red. Dashed red line is a best quadratic fit for dependence of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e185" xlink:type="simple"/></inline-formula> on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e186" xlink:type="simple"/></inline-formula>; this can be rewritten as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e187" xlink:type="simple"/></inline-formula>, where <italic>γ</italic>(<italic>N</italic>) (shown in inset) is the effective scaling of multi-information with system size <italic>N</italic>. In both panels, error bars are s.d. over 30 subnetworks at each size <italic>N</italic>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003408.g013" position="float" xlink:type="simple"/></fig>
<p>The difference between the entropy of the model (an upper bound to the true entropy of the system) and the independent entropy, also known as the <italic>multi</italic>–<italic>information</italic>,<disp-formula id="pcbi.1003408.e188"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003408.e188" xlink:type="simple"/><label>(32)</label></disp-formula>measures the amount of statistical structure between <italic>N</italic> neurons due to pairwise interactions and the K-spike constraint. As we see in <xref ref-type="fig" rid="pcbi-1003408-g013">Figure 13B</xref>, the multi-information initially grows quadratically (<italic>γ</italic> = 2) as a function of <italic>N</italic>. While this growth is slowing as <italic>N</italic> increases, it is still faster than linear (<italic>γ</italic>&gt;1), and correspondingly the entropy per neuron keeps decreasing, so that even with <italic>N</italic> = 120 neurons we have not yet reached the extensive scaling regime where the entropy per neuron would be constant. These results are consistent with earlier suggestions in Ref <xref ref-type="bibr" rid="pcbi.1003408-Schneidman1">[4]</xref>. There the multi-information increased proportionally to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e189" xlink:type="simple"/></inline-formula> for small populations (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e190" xlink:type="simple"/></inline-formula> cells), which we also see. The previous paper also suggested from very general theoretical grounds that this scaling would break down at larger network sizes, as we now observe. Truly extensive scaling should emerge for populations much larger than the “correlated patch size” of <italic>N</italic>∼200 cells, because then many pairs of neurons would lack any correlation. Our current data suggest such a transition, but do not provide an accurate estimate of the system size at which extensive behavior emerges.</p>
</sec><sec id="s2c3">
<title>Coincidences and surprises</title>
<p>Usually we expect that, as the number of elements <italic>N</italic> in a system becomes large, the entropy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e191" xlink:type="simple"/></inline-formula> becomes proportional to <italic>N</italic> and the distribution becomes nearly uniform over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e192" xlink:type="simple"/></inline-formula> states. This is the concept of “typicality” in information theory <xref ref-type="bibr" rid="pcbi.1003408-Cover1">[74]</xref> and the “equivalence of ensembles” in statistical physics <xref ref-type="bibr" rid="pcbi.1003408-Lanford1">[75]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Landau2">[76]</xref>. At <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e193" xlink:type="simple"/></inline-formula>, we have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e194" xlink:type="simple"/></inline-formula> bits, so that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e195" xlink:type="simple"/></inline-formula>, and for the full <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e196" xlink:type="simple"/></inline-formula> neurons in our experiment the number of states is even larger. In a uniform distribution, if we pick two states at random then the probability that these states are the same is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e197" xlink:type="simple"/></inline-formula>. On the hypothesis of uniformity, this probability is sufficiently small that large groups of neurons should <italic>never</italic> visit the same state twice during the course of a one hour experiment. In fact, if we choose two moments in time at random from the experiment, the probability that even the full 160–neuron state that we observe will be the same is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e198" xlink:type="simple"/></inline-formula>.</p>
<p>We can make these considerations a bit more precise by exploring the dependence of coincidence probabilities on <italic>N</italic>. We expect that the negative logarithm of the coincidence probability, like the entropy itself, will grow linearly with <italic>N</italic>; equivalently we should see an exponential decay of coincidence probability as we increase the size of the system. This is exactly true if the neurons are independent, even if different cells have different probabilities of spiking, provided that we average over possible choices of <italic>N</italic> neurons out of the population. But the real networks are far from this prediction, as we can see in <xref ref-type="fig" rid="pcbi-1003408-g014">Figure 14A</xref>.</p>
<fig id="pcbi-1003408-g014" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003408.g014</object-id><label>Figure 14</label><caption>
<title>Coincidence probabilities.</title>
<p>(<bold>A</bold>) The probability that the combination of spikes and silences is exactly the same at two randomly chosen moments of time, as a function of the size of the population. The real networks are orders of magnitude away from the predictions of an independent model, and this behavior is captured precisely by the K-pairwise model. (<bold>B</bold>) Extrapolating the <italic>N</italic> dependence of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e199" xlink:type="simple"/></inline-formula> to large <italic>N</italic>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003408.g014" position="float" xlink:type="simple"/></fig>
<p>K-pairwise models reproduce the coincidence probability very well, with the fractional error in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e200" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e201" xlink:type="simple"/></inline-formula> of 0.3%. To assess how important various statistical features of the distribution are to the success of this prediction, we compared this with an error that a pairwise model would make (88%); this is most likely because pairwise models fail to capture the probability of the all-silent state which recurs most often. If one constructs a model that reproduces exactly the silent state probability, while in the non-silent patterns the neurons are assumed to spike independently, all with the identical firing rate equal to the population mean, the error in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e202" xlink:type="simple"/></inline-formula> prediction is 7.5%. This decreases to 1.8% for a model that, in addition to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e203" xlink:type="simple"/></inline-formula>, reproduces the complete probability of seeing <italic>K</italic> spikes simultaneously, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e204" xlink:type="simple"/></inline-formula> (but doesn't reproduce either the individual firing rates or the correlations between the neurons); the form of this model is given by <xref ref-type="disp-formula" rid="pcbi.1003408.e064">Eq (17)</xref>. In sum, (i) the observed coincidence probability cannot be explained simply by the recurrent all-silent state; (ii) including the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e205" xlink:type="simple"/></inline-formula> constraint is essential; (iii) a further 6-fold decrease in error is achieved by including the pairwise and single-neuron constraints.</p>
<p>Larger and larger groups of neurons do seem to approach a “thermodynamic limit” in which <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e206" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003408-g014">Figure 14B</xref>), but the limiting ratio <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e207" xlink:type="simple"/></inline-formula> is an order of magnitude smaller than our estimates of the entropy per neuron (0.166 bits, or 0.115 nats, per neuron for <italic>N</italic> = 120 in K-pairwise models; <xref ref-type="fig" rid="pcbi-1003408-g013">Figure 13A</xref>). Thus, the correlations among neurons make the recurrence of combinatorial patterns thousands of times more likely than would be expected from independent neurons, and this effect is even larger than simply the reduction in entropy. This suggests that the true distribution over states is extremely inhomogeneous, not just because total silence is anomalously probable but because the dynamic range of probabilities for the different active states also is very large. Importantly, as seen in <xref ref-type="fig" rid="pcbi-1003408-g014">Figure 14</xref>, this effect is captured with very high precision by our maximum entropy model.</p>
</sec><sec id="s2c4">
<title>Redundancy and predictability</title>
<p>In the retina we usually think of neurons as responding to the visual stimulus, and so it is natural to summarize their response as spike rate vs. time in a (repeated) movie, the post–stimulus time histogram (PSTH). We can do this for each of the cells in the population that we study; one example is in the top row of <xref ref-type="fig" rid="pcbi-1003408-g015">Figure 15A</xref>. This example illustrates common features of neural responses to naturalistic sensory inputs—long epochs of near zero spike probability, interrupted by brief transients containing a small number of spikes <xref ref-type="bibr" rid="pcbi.1003408-Berry1">[77]</xref>. Can our models predict this behavior, despite the fact that they make no explicit reference to the visual input?</p>
<fig id="pcbi-1003408-g015" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003408.g015</object-id><label>Figure 15</label><caption>
<title>Predicting the firing probability of a neuron from the rest of the network.</title>
<p>(<bold>A</bold>) Probability per unit time (spike rate) of a single neuron. Top, in red, experimental data. Lower traces, in black, predictions based on states of other neurons in an <italic>N</italic>–cell group, as described in the text. Solid lines are the mean prediction across all trials, and thin lines are the envelope ± one standard deviation. (<bold>B</bold>) Cross–correlation (CC) between predicted and observed spike rates vs. time, for each neuron in the <italic>N</italic> = 120 group. Green empty circles are averages of CC computed from every trial, whereas blue solid circles are the CC computed from average predictions. (<bold>C</bold>) Dependence of CC on the population size <italic>N</italic>. Thin blue lines follow single neurons as predictions are based on increasing population sizes; red line is the cell illustrated in (A), and the line with error bars shows mean ± s.d. across all cells. Green line shows the equivalent mean behavior computed for the green empty circles in (B).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003408.g015" position="float" xlink:type="simple"/></fig>
<p>The maximum entropy models that we have constructed predict the distribution of states taken on by the network as a whole, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e208" xlink:type="simple"/></inline-formula>. From this we can construct the conditional distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e209" xlink:type="simple"/></inline-formula>, which tells us the probability of spiking in one cell given the current state of all the other cells, and hence we have a prediction for the spike probability in one neuron at each moment in time. Further, we can repeat this construction using not all the neurons in the network, but only a group of <italic>N</italic>, with variable <italic>N</italic>.</p>
<p>As the stimulus movie proceeds, all of the cells in the network are spiking, dynamically, so that the state of the system varies. Through the conditional distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e210" xlink:type="simple"/></inline-formula>, this varying state predicts a varying spike probability for the one cell in the network on which we are focusing, and we can plot this predicted probability vs. time in the same way that we would plot a conventional PSTH. On each repeat of the movie, the states of the network are slightly different, and hence the predicted PSTH is slightly different. What we see in <xref ref-type="fig" rid="pcbi-1003408-g015">Figure 15A</xref> is that, as we use more and more neurons in the network to make the prediction, the PSTH based on collective effects alone, trial by trial, starts to look more and more like the real PSTH obtained by averaging over trials. In particular, the predicted PSTH has near zero spike probability over most of the time, the short epochs of spiking are at the correct moments, and these epochs have the sharp onsets observed experimentally. These are features of the data which are very difficult to reproduce in models that, for example, start by linearly filtering the visual stimulus through a receptive field <xref ref-type="bibr" rid="pcbi.1003408-vanHateren1">[78]</xref>–<xref ref-type="bibr" rid="pcbi.1003408-Chen1">[82]</xref>. In contrast, the predictions in <xref ref-type="fig" rid="pcbi-1003408-g015">Figure 15</xref> make no reference to the visual stimulus, only to the outputs of other neurons in the network.</p>
<p>We can evaluate the predictions of spike probability vs. time by computing the correlation coefficient between our predicted PSTH and the experimental PSTH, as has been done in many other contexts <xref ref-type="bibr" rid="pcbi.1003408-vanHateren1">[78]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Dan1">[83]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Theunissen1">[84]</xref>. Since we generate a prediction for the PSTH on every presentation of the movie, we can compute the correlation from these raw predictions, and then average, or average the predictions and then compute the correlation; results are shown in <xref ref-type="fig" rid="pcbi-1003408-g015">Figures 15B and C</xref>. We see that correlation coefficients can reach ∼0.8, on average, or even higher for particular cells. Predictions seem of more variable quality for cells with lower average spike rate, but this is a small effect. The quality of average predictions, as well as the quality of single trial predictions, still seems to grow gradually as we include more neurons even at <italic>N</italic>∼100, so it may be that we have not seen the best possible performance yet.</p>
<p>Our ability to predict the state of individual neurons by reference to the network, but not the visual input, means that the representation of the sensory input in this population is substantially redundant. Stated more positively, the full information carried by this population of neurons—indeed, the full information available to the brain about this small patch of the visual world—is accessible to downstream cells and areas that receive inputs from only a fraction of the neurons.</p>
</sec></sec></sec><sec id="s3">
<title>Discussion</title>
<p>It is widely agreed that neural activity in the brain is more than the sum of its parts—coherent percepts, thoughts, and actions require the coordinated activity of many neurons in a network, not the independent activity of many individual neurons. It is not so clear, however, how to build bridges between this intuition about collective behavior and the activity of individual neurons.</p>
<p>One set of ideas is that the activity of the network as a whole may be confined to some very low dimensional trajectory, such as a global, coherent oscillation. Such oscillatory activity is observable in the summed electrical activity of large numbers of neurons—the EEG—and should be reflected as oscillations in the (auto–)correlation functions of spike trains from individual neurons. On a more refined level, dimensionality reduction techniques like PCA allow the activity patterns of a neural network to be viewed on a low-dimensional manifold, facilitating visualization and intuition <xref ref-type="bibr" rid="pcbi.1003408-Stopfer1">[85]</xref>–<xref ref-type="bibr" rid="pcbi.1003408-Rutishauser1">[88]</xref>. A very different idea is provided by the Hopfield model, in which collective behavior is expressed in the stabilization of many discrete patterns of activity, combinations of spiking and silence across the entire network <xref ref-type="bibr" rid="pcbi.1003408-Hopfield1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Amit1">[2]</xref>. Taken together, these many patterns can span a large fraction of the full space of possibilities, so that there need be no dramatic dimensionality reduction in the usual sense of this term.</p>
<p>The claim that a network of neurons exhibits collective behavior is really the claim that the distribution of states taken on by the network has some nontrivial structure that cannot be factorized into contributions from individual cells or perhaps even smaller subnetworks. Our goal in this work has been to build a model of this distribution, and to explore the structure of that model. We emphasize that building a model is, in this view, the first step rather than the last step. But building a model is challenging, because the space of states is very large and data are limited.</p>
<p>An essential step in searching for collective behavior has been to develop experimental techniques that allow us to record not just from a large number of neurons, but from a large fraction of the neurons in a densely interconnected region of the retina <xref ref-type="bibr" rid="pcbi.1003408-Marre1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Segev2">[89]</xref>. In large networks, even measuring the correlations among pairs of neurons can become problematic: individual elements of the correlation matrix might be well determined from small data sets, but much larger data sets are required to be confident that the matrix as a whole is well determined. Thus, long, stable recordings are even more crucial than usual.</p>
<p>To use the maximum entropy approach, we have to be sure that we can actually find the models that reproduce the observed expectation values (<xref ref-type="fig" rid="pcbi-1003408-g002">Figure 2</xref>, <xref ref-type="fig" rid="pcbi-1003408-g003">3</xref>) and that we have not, in the process, fit to spurious correlations that arise from the finite size of our data set (<xref ref-type="fig" rid="pcbi-1003408-g004">Figure 4</xref>). Once these tests are passed, we can start to assess the accuracy of the model as a description of the network as a whole. In particular, we found that the pairwise model began to break down at a network size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e211" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003408-g005">Figure 5</xref>). However, by adding the K-spike constraint that reproduces the probability of <italic>K</italic> out of <italic>N</italic> neurons spiking synchronously (<xref ref-type="fig" rid="pcbi-1003408-g006">Figure 6</xref>), which is a statistic that is well-sampled and does not greatly increase the model's complexity, we could again recover good performance (<xref ref-type="fig" rid="pcbi-1003408-g007">Figures 7</xref>–<xref ref-type="fig" rid="pcbi-1003408-g009">9</xref>). Although the primary goal of this work was to examine the responses of the retina under naturalistic stimulation, we also checked that the K-pairwise models are able to capture the joint behavior of retinal ganglion cells under a very different, random checkerboard stimulation (<xref ref-type="supplementary-material" rid="pcbi.1003408.s007">Figure S7</xref>). Despite a significantly smaller amount of total correlation (and a complete lack of long-range correlation) in the checkerboard stimuli compared to natural scenes, the pairwise model still deviated significantly from data at large <italic>N</italic> and the K-spike constraint proved necessary; this happened even though the total amount of correlation in the codewords is smaller for the checkerboard stimulus. Characterizing more completely the dependence of the statistical properties of the neural code on the stimulus type therefore seems like one of the interesting avenues for future work.</p>
<p>How can we interpret the meaning of the K-spike constraint and its biological relevance? One possibility would be to view it as a global modulatory effect of, e.g., inhibitory interneurons with dense connectivity. Alternatively, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e212" xlink:type="simple"/></inline-formula> might be an important feature of the neural code for downstream neurons. For example, if a downstream neuron sums over its inputs and fires whenever the sum exceeds a threshold, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e213" xlink:type="simple"/></inline-formula> will be informative about the rate of such threshold crossings. We note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e214" xlink:type="simple"/></inline-formula> is a special case of a more general linear function, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e215" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e216" xlink:type="simple"/></inline-formula> are arbitrary weights and <italic>θ</italic> is a threshold (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e217" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e218" xlink:type="simple"/></inline-formula> for <italic>K</italic>). An interesting question to explore in the future would be to ask if the <italic>K</italic>-statistic really is the most informative choice that maximally reduces the entropy of the K-pairwise model relative to the pairwise model, or whether additional modeling power could be gained by optimizing the weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e219" xlink:type="simple"/></inline-formula>, perhaps even by adding several such projection vectors as constraints. In any case, the K-pairwise model should not be regarded as “the ultimate model” of the retinal code: it is a model that emerged from pairwise constructions in a data-driven attempt to account for systematic deficiencies of Ising-like models on large populations. Similarly, systematic deviations that remain, e.g., at the low and high ends of the effective field <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e220" xlink:type="simple"/></inline-formula> as in <xref ref-type="fig" rid="pcbi-1003408-g009">Figure 9</xref>, might inform us about further useful constraints that could improve the model. These could include either new higher-order interactions, global constraints, or couplings across time bins, as suggested by Refs <xref ref-type="bibr" rid="pcbi.1003408-Ganmor1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Marre2">[90]</xref>.</p>
<p>Perhaps the most useful global test of our models is to ask about the distribution of state probabilities: how often should we see combinations of spiking and silence that occur with probability <italic>P</italic>? This has the same flavor as asking for the probability of every state, but does not suffer from the curse of dimensionality. Since maximum entropy models are mathematically identical to the Boltzmann distribution in statistical mechanics, this question about the frequency of states with probability <italic>P</italic> is the same as asking how many states have a given energy <italic>E</italic>; we can avoid binning along the <italic>E</italic> axis by asking for the number of states with energies smaller (higher probability) or larger (lower probability) than <italic>E</italic>. <xref ref-type="fig" rid="pcbi-1003408-g008">Figure 8</xref> shows that these cumulative distributions computed from the model agree with experiment far into the tail of low probability states. These states are so rare that, individually, they almost never occur, but there are so many of these rare states that, in aggregate, they make a measurable contribution to the distribution of energies. Indeed, most of the states that we see in the data are rare in this sense, and their statistical weight is correctly predicted by the model.</p>
<p>The maximum entropy models that we construct from the data do not appear to simplify along any conventional axes. The matrix of correlations among spikes in different cells (<xref ref-type="fig" rid="pcbi-1003408-g001">Figure 1A</xref>) is of full rank, so that principal component analysis does not yield significant dimensionality reduction. The matrix of “interactions” in the model (<xref ref-type="fig" rid="pcbi-1003408-g001">Figure 1D</xref>) is neither very sparse nor of low rank, perhaps because we are considering a group of neurons all located (approximately) within the radius of the typical dendritic arbor, so that all cells have a chance to interact with one another. Most importantly, the interactions that we find are not weak (<xref ref-type="fig" rid="pcbi-1003408-g001">Figure 1F</xref>), and together with being widespread this means that their impact is strong. Technically, we cannot capture the impact within low orders of perturbation theory (<xref ref-type="sec" rid="s4"><italic>Methods</italic></xref><italic>: Are the networks in the perturbative regime?</italic>), but qualitatively this means that the behavior of the network as a whole is not in any sense “close” to the behavior of non–interacting neurons. Thus, the reason that our models work is likely not because the correlations are weak, as had been suggested <xref ref-type="bibr" rid="pcbi.1003408-Roudi2">[34]</xref>.</p>
<p>Having convinced ourselves that we can build models which give an accurate description of the probability distribution over the states of spiking and silence in the network, we can ask what these models teach us about function. As emphasized in Ref <xref ref-type="bibr" rid="pcbi.1003408-Schneidman1">[4]</xref>, one corollary of collective behavior is the possibility of error correction or pattern completion—we can predict the spiking or silence of one neuron by knowing the activity of all the other neurons. With a population of <italic>N</italic> = 100 cells, the quality of these predictions becomes quite high (<xref ref-type="fig" rid="pcbi-1003408-g015">Figure 15</xref>). The natural way of testing these predictions is to look at the probability of spiking vs. time in the stimulus movie. Although we make no reference to the stimulus, we reproduce the sharp peaks of activity and extended silences that are so characteristic of the response to naturalistic inputs, and so difficult to capture in conventional models where each individual neuron responds to the visual stimulus as seen through its receptive field <xref ref-type="bibr" rid="pcbi.1003408-vanHateren1">[78]</xref>.</p>
<p>One of the dominant concepts in thinking about the retina has been the idea that the structure of receptive fields serves to reduce the redundancy of natural images and enhance the efficiency of information transmission to the brain <xref ref-type="bibr" rid="pcbi.1003408-Barlow1">[91]</xref>–<xref ref-type="bibr" rid="pcbi.1003408-vanHdateren1">[94]</xref> (but see <xref ref-type="bibr" rid="pcbi.1003408-Puchalla1">[65]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Barlow2">[95]</xref>). While one could argue that the observed redundancy among neurons is less than expected from the structure of natural images or movies, none of what we have described here would happen if the retina truly “decorrelated” its inputs. Far from being almost independent, the activity of single neurons is predicted very well by the state of the remaining network, and the combinations of spiking and silence in different cells cluster into basins of attraction defined by the local minima of energy in our models. While it is intriguing to think about the neural code as being organized around the “collective metastable states,” some of which we have identified using the maximum entropy model, further work is necessary to explore this idea in detail. Unlike our other results, where we could either compare parameter-free predictions to data, or put a bound on the entropy of the code, it is harder to compare the model's energy landscape (and its local minima) to the true energy landscape, for which we would need to be able to estimate all pattern probabilities directly from data. It is therefore difficult to assess how dependent the identified collective states are on the form of the model. Nevertheless, for any particular model assignment of activity patterns to collective states, one could ask how well those collective modes capture the information about the stimuli, and use that as a direct measure of model performance. We believe this to be a promising avenue for future research.</p>
<p>With <italic>N</italic> = 120 neurons, our best estimate of the entropy corresponds to significant occupancy of roughly one million distinct combinations of spiking and silence. Each state could occur with a different probability, and (aside from normalization) there are no constraints—each of these probabilities could be seen as a separate parameter describing the network activity. It is appealing to think that there must be some simplification, that we won't need a million parameters, but it is not obvious that any particular simplification strategy will work. Indeed, there has been the explicit claim that maximum entropy approach has been successful on small (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e221" xlink:type="simple"/></inline-formula>) groups of neurons simply because a low-order maximum model will generically approximate well any probability distribution that is sufficiently sparse, and that we should thus not expect it to work for large networks <xref ref-type="bibr" rid="pcbi.1003408-Roudi2">[34]</xref>. Thus, it may seem surprising that we can write down a relatively simple model, with parameters that number less than a percent of the number of effectively occupied states (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e222" xlink:type="simple"/></inline-formula> parameters for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e223" xlink:type="simple"/></inline-formula> effective states at <italic>N</italic> = 120) and whose values are directly determined by measurable observables, and have this model predict so much of the structure in the distribution of states. Surprising or not, it certainly is important that, as the community contemplates monitoring the activity of ever larger number of neurons <xref ref-type="bibr" rid="pcbi.1003408-Alivisatos1">[96]</xref>, we can identify theoretical approaches that have the potential to tame the complexity of these large systems.</p>
<p>Some cautionary remarks about the interpretation of our models seem in order. Using the maximum entropy method does not mean there is some hidden force maximizing the entropy of neural activity, or that we are describing neural activity as being in something like thermal equilibrium; all we are doing is building maximally agnostic models of the probability distribution over states. Even in the context of statistical mechanics, there are infinitely many models for the dynamics of the system that will be consistent with the equilibrium distribution, so we should not take the success of our models to mean that the dynamics of the network corresponds to something like Monte Carlo dynamics on the energy landscape. It is tempting to look at the couplings <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e224" xlink:type="simple"/></inline-formula> between different neurons as reflecting genuine, mechanistic interactions, but even in the context of statistical physics we know that this interpretation need not be so precise: we can achieve a very accurate description of the collective behavior in large systems even if we do not capture every microscopic detail, and the interactions that we do describe in the most successful of models often are effective interactions mediated by degrees of freedom that we need not treat explicitly. Finally, the fact that a maximum entropy model which matches a particular set of experimental observations is successful does not mean that this choice of observables (e.g., pairwise correlations) is unique or minimal. For all these reasons, we do not think about our models in terms of their parameters, but rather as a description of the probability distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e225" xlink:type="simple"/></inline-formula> itself, which encodes the collective behavior of the system.</p>
<p>The striking feature of the distribution over states is its extreme inhomogeneity. The entropy of the distribution is not that much smaller than it would be if the neurons made independent decisions to spike or be silent, but the shape of the distribution is very different; the network builds considerable structure into the space of states, without sacrificing much capacity. The probability of the same state repeating is many orders of magnitude larger than expected for independent neurons, and this really is quite startling (<xref ref-type="fig" rid="pcbi-1003408-g014">Figure 14</xref>). If we extrapolate to the full population of ∼250 neurons in this correlated, interconnected patch of the retina, the probability that two randomly chosen states of the system are the same is roughly one percent. Thus, some combination of spiking and silence across this huge population should repeat exactly every few seconds. This is true despite the fact that we are looking at the entire visual representation of a small patch of the world, and the visual stimuli are fully naturalistic. Although complete silence repeats more frequently, a wide range of other states also recur, so that many different combinations of spikes and silence occur often enough that we (or the brain) can simply count them to estimate their probability. This would be absolutely impossible in a population of nearly independent neurons, and it has been suggested that these repeated patterns provide an anchor for learning <xref ref-type="bibr" rid="pcbi.1003408-Ganmor1">[12]</xref>. It is also possible that the detailed structure of the distribution, including its inhomogeneity, is matched to the statistical structure of visual inputs in a way that goes beyond the idea of redundancy reduction, occupying a regime in which strongly correlated activity is an optimal code <xref ref-type="bibr" rid="pcbi.1003408-Stephens1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Saremi1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Tkaik6">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Sourlas1">[97]</xref>.</p>
<p>Building a precise model of activity patterns required us to match the statistics of global activity (the probability that <italic>K</italic> out of <italic>N</italic> neurons spike in the same small window of time). Several recent works suggested alternative means of capturing the higher-order correlations <xref ref-type="bibr" rid="pcbi.1003408-Ganmor1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Bohte1">[98]</xref>–<xref ref-type="bibr" rid="pcbi.1003408-Kster1">[103]</xref>. Particularly promising and computationally tractable amongst these models is the dichotomized Gaussian (DG) model <xref ref-type="bibr" rid="pcbi.1003408-Macke1">[100]</xref> that could explain correctly the distribution of synchrony in the monkey cortex <xref ref-type="bibr" rid="pcbi.1003408-Yu2">[104]</xref>. While DG does well when compared with pairwise models on our data, it is significantly less successful than the full K-pairwise models that we have explored here. In particular, the DG predictions of three-neuron correlations are much less accurate than in our model, and the probability of coincidences is underestimated by an amount that grows with increasing <italic>N</italic> (<xref ref-type="supplementary-material" rid="pcbi.1003408.s006">Figure S6</xref>). Elsewhere we have explored a very simple model in which we ignore the identity of the neurons and match only the global behavior <xref ref-type="bibr" rid="pcbi.1003408-Tkaik5">[39]</xref>. This model already has a lot of structure, including the extreme inhomogeneity that we have emphasized here. In the simpler model we can exploit the equivalence between maximum entropy models and statistical mechanics to argue that this inhomogeneity is equivalent to the statement that the population of neurons is poised near a critical surface in its parameter space, and we have seen hints of this from analyses of smaller populations as well <xref ref-type="bibr" rid="pcbi.1003408-Tkaik1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Tkaik2">[9]</xref>. The idea that biological networks might organize themselves to critical points has a long history, and several different notions of criticality have been suggested <xref ref-type="bibr" rid="pcbi.1003408-Mora2">[31]</xref>. A sharp question, then, is whether the full probability distributions that we have described here correspond to a critical system in the sense of statistical physics, and whether we can find more direct evidence for criticality in the data, perhaps without the models as intermediaries.</p>
<p>Finally, we note that our approach to building models for the activity of the retinal ganglion cell population is entirely unsupervised: we are making use only of structure in the spike trains themselves, with no reference to the visual stimulus. In this sense, the structures that we discover here are structures that could be discovered by the brain, which has no access to the visual stimulus beyond that provided by these neurons. While there are more structures that we could use—notably, the correlations across time—we find it remarkable that so much is learnable from just an afternoon's worth of data. As it becomes more routine to record the activity of such (nearly) complete sensory representations, it will be interesting to take the organism's point of view <xref ref-type="bibr" rid="pcbi.1003408-Rieke1">[43]</xref> more fully, and try to extract meaning from the spike trains in an unsupervised fashion.</p>
</sec><sec id="s4" sec-type="methods">
<title>Methods</title>
<sec id="s4a">
<title>Ethics statement</title>
<p>This study was performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health. The protocol was approved by the Institutional Animal Care and Use Committee (IACUC) of Princeton University (Protocol 1827 for guinea pigs and 1828 for salamanders).</p>
</sec><sec id="s4b">
<title>Electrophysiology</title>
<p>We analyzed the recordings from the tiger salamander (<italic>Ambystoma tigrinum</italic>) retinal ganglion cells responding to naturalistic movie clips, as in the experiments of Refs. <xref ref-type="bibr" rid="pcbi.1003408-Schneidman1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Marre1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Puchalla1">[65]</xref>. In brief, animals were euthanized according to institutional animal care standards. The retina was isolated from the eye under dim illumination and transferred as quickly as possible into oxygenated Ringer's medium, in order to optimize the long-term stability of recordings. Tissue was flattened and attached to a dialysis membrane using polylysine. The retina was then lowered with the ganglion cell side against a multi-electrode array. Arrays were first fabricated in university cleanroom facilities <xref ref-type="bibr" rid="pcbi.1003408-Amodei1">[105]</xref>. Subsequently, production was contracted out to a commercial MEMS foundry for higher volume production (Innovative Micro Technologies, Santa Barbara, CA). Raw voltage traces were digitized and stored for off-line analysis using a 252-channel preamplifier (MultiChannel Systems, Germany). The recordings were sorted using custom spike sorting software developed specifically for the new dense array <xref ref-type="bibr" rid="pcbi.1003408-Marre1">[36]</xref>. 234 neurons passed the standard tests for the waveform stability and the lack of refractory period violations. Of those, 160 cells whose firing rates were most stable across stimulus repeats were selected for further analysis. Within this group, the mean fraction of interspike intervals (ISI) shorter than 2 ms (i.e., possible refractory violations) was <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e226" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4c">
<title>Stimulus display</title>
<p>The stimulus consisted of a short (<italic>t</italic> = 19 s) grayscale movie clip of swimming fish and water plants in a fish tank, which was repeated 297 times. The stimulus was presented using standard optics, at a rate of 30 frames per second, and gamma corrected for the display.</p>
</sec><sec id="s4d">
<title>Data preparation</title>
<p>We randomly selected 30 subgroups of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e227" xlink:type="simple"/></inline-formula> cells for analysis from the total of 160 sorted cells. In sum, we analyzed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e228" xlink:type="simple"/></inline-formula> groups of neurons, which we denote by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e229" xlink:type="simple"/></inline-formula>, where <italic>N</italic> denotes the subgroup size, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e230" xlink:type="simple"/></inline-formula> indexes the chosen subgroup of that size. Time was discretized into Δ<italic>τ</italic> = 20 ms time bins, as in our previous work <xref ref-type="bibr" rid="pcbi.1003408-Schneidman1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Tkaik1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Tkaik2">[9]</xref>. The state of the retina was represented by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e231" xlink:type="simple"/></inline-formula> if the neuron i spiked at least once (was silent) in a given time bin <italic>t</italic>. This binary description is incomplete only in ∼0.5% of the time bins that contain more than one spike; we treat these bins as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e232" xlink:type="simple"/></inline-formula>. Across the entire experiment, the mean probability for a single neuron to make a spike in a timebin (that is, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e233" xlink:type="simple"/></inline-formula>) is ∼3.1%. Time discretization resulted in 953 time bins per stimulus repeat; 297 presented repeats yielded a total of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e234" xlink:type="simple"/></inline-formula> <italic>N</italic>-bit binary samples during the course of the experiment for each subgroup.</p>
</sec><sec id="s4e">
<title>Learning maximum entropy models from data</title>
<p>We used a modified version of our previously published learning procedure to compute the maximum entropy models given measured constraints <xref ref-type="bibr" rid="pcbi.1003408-Broderick1">[37]</xref>; the proof of convergence for the core of this L1-regularized maximum entropy algorithm is given in Ref. <xref ref-type="bibr" rid="pcbi.1003408-Dudik1">[106]</xref>. Our new algorithm can use as constraints arbitrary functions, not only single and pairwise marginals as before. Parameters of the Hamiltonian are learned sequentially in an order which greedily optimizes a bound on the log likelihood, and we use a variant of histogram Monte Carlo to estimate the values of constrained statistics during learning steps <xref ref-type="bibr" rid="pcbi.1003408-Ferrenberg1">[107]</xref>. Monte Carlo induces sampling errors on our estimates of these statistics, which provide an implicit regularization for the parameters of the Hamiltonian <xref ref-type="bibr" rid="pcbi.1003408-Dudik1">[106]</xref>. We verified the correctness of the algorithm explicitly for groups of 10 and 20 neurons where exact numerical solutions are feasible. We also verified that our MC sampling had a long enough “burn-in” time to equilibrate, even for groups of maximal size (<italic>N</italic> = 120), by starting the sampling repeatedly from same vs different random initial conditions (100 runs each) and comparing the constrained statistics, as well as the average and variance of the energy and magnetization, across these runs; all statistics were not significantly dependent on the initial state (two–sample Kolmogorov-Smirnov test at significance level 0.05).</p>
<p>Supplementary <xref ref-type="supplementary-material" rid="pcbi.1003408.s001">Figure S1</xref> provides a summary of the models we have learned for populations of different sizes. In small networks there is a systematic bias to the distribution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e235" xlink:type="simple"/></inline-formula> parameters, but as we look to larger networks this vanishes and the distribution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e236" xlink:type="simple"/></inline-formula> becomes symmetric. Importantly, the distribution remains quite broad, with the standard deviation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e237" xlink:type="simple"/></inline-formula> across all pairs declining only slightly. In particular, the typical coupling does not decline as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e238" xlink:type="simple"/></inline-formula>, as would be expected in conventional spin glass models <xref ref-type="bibr" rid="pcbi.1003408-Mezard1">[70]</xref>. This implies, as emphasized previously <xref ref-type="bibr" rid="pcbi.1003408-Tkaik2">[9]</xref>, that the “thermodynamic limit” (very large <italic>N</italic>) for these systems will be different from what we might expect based on traditional physics examples.</p>
<p>We withheld a random selection of 20 stimulus repeats (test set) for model validation, while training the model on the remaining 277 repeats. On training data, we computed the constrained statistics (mean firing rates, covariances, and the K-spike distribution), and used bootstrapping to estimate the error bars on each of these quantities; the constraints were the only input to the learning algorithm. <xref ref-type="fig" rid="pcbi-1003408-g001">Figure 1</xref> shows an example reconstruction for a pairwise model for <italic>N</italic> = 100 neurons; the precision of the learning algorithm is shown in <xref ref-type="fig" rid="pcbi-1003408-g002">Figure 2</xref>.</p>
<p>The dataset consists of a total of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e239" xlink:type="simple"/></inline-formula> binary pattern samples, but he number of statistically independent samples must be smaller: while the repeats are plausibly statistically independent, the samples within each repeat are not. The variance for a binary variable given its mean, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e240" xlink:type="simple"/></inline-formula>, is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e241" xlink:type="simple"/></inline-formula>; with <italic>R</italic> independent repeats, the error on the estimate in the average should decrease as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e242" xlink:type="simple"/></inline-formula>. By repeatedly estimating the statistical errors with different subsets of repeats and comparing the expected scaling of the error in the original data set with the data set where we shuffle time bins randomly, thereby destroying the repeat structure, we can estimate the effective number of independent samples; we find this to be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e243" xlink:type="simple"/></inline-formula>, about 37% of the total number of samples, <italic>T</italic>.</p>
<p>We note that our largest models have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e244" xlink:type="simple"/></inline-formula> constrained statistics that are estimated from at least 15× as many statistically independent samples. Moreover, the vast majority of these statistics are pairwise correlation coefficients that can be estimated extremely tightly from the data, often with relative errors below 1%, so we do not expect overfitting on general grounds. Nevertheless, we explicitly checked that there is no overfitting by comparing the log likelihood of the data under the learned maximum entropy model, for each of the 360 subgroups <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e245" xlink:type="simple"/></inline-formula>, on the training and testing set, as shown in <xref ref-type="fig" rid="pcbi-1003408-g003">Figure 3</xref>.</p>
</sec><sec id="s4f">
<title>Parametrization of the K-pairwise model</title>
<p>The parametrization of the K-pairwise Hamiltonian of <xref ref-type="disp-formula" rid="pcbi.1003408.e108">Eq (20)</xref> is degenerate, that is, there are multiple sets of coupling constants <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e246" xlink:type="simple"/></inline-formula> that specify mathematically identical models. This is because adjusting all local fields <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e247" xlink:type="simple"/></inline-formula> by a constant offset adds a term linear in <italic>K</italic> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e248" xlink:type="simple"/></inline-formula>; similarly, adjusting all pairwise couplings <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e249" xlink:type="simple"/></inline-formula> by a constant offset adds a quadratic term to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e250" xlink:type="simple"/></inline-formula>. For comparing model predictions (i.e., observables, entropy, the structure of the energy landscape etc) this is inconsequential, but when model parameters are compared directly in <xref ref-type="fig" rid="pcbi-1003408-g005">Figure 5</xref>, one must choose a gauge that will make the comparison of the pairwise and K-pairwise parameters unbiased. Since there is no <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e251" xlink:type="simple"/></inline-formula> in the pairwise model, we extract from the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e252" xlink:type="simple"/></inline-formula> of the K-pairwise model all those components that can be equivalently parametrized by offsets to local fields and pairwise couplings. In detail, we subtract best linear and quadratic fits from the reconstructed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e253" xlink:type="simple"/></inline-formula>, such that the remaining <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e254" xlink:type="simple"/></inline-formula> only constrains multi-point correlations that cannot be accounted for by a choice of fields and pairwise interactions; the linear and quadratic fit then give us adjustments to local fields and pairwise interactions.</p>
</sec><sec id="s4g">
<title>Exploring the energy landscape</title>
<p>To find the metastable (MS) states, we start with a pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e255" xlink:type="simple"/></inline-formula> that appears in the data, and attempt to flip spins <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e256" xlink:type="simple"/></inline-formula> from their current state into <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e257" xlink:type="simple"/></inline-formula>, in order of increasing i. A flip is retained if the energy of the new configuration is smaller than before the flip. When none of the spins can be flipped, the resulting pattern is recorded as the MS state. The set of MS states found can depend on the manner in which descent is performed, in particular when some of the states visited during descent are on the “ridges” between multiple basins of attraction. Note that whether a pattern is a MS state or not is independent of the descent method; what depends on the method is which MS states are found by starting from the data patterns. To explore the structure of the energy landscape in <xref ref-type="fig" rid="pcbi-1003408-g011">Figure 11</xref>, we started 1000 Metropolis MC simulations repeatedly in each of the 10 most common MS states of the model; after each attempted spin-flip, we checked whether the resulting state is still in the basin of attraction of the starting MS state (by invoking the descent method above), or whether it has crossed the energy barrier into another basin. We histogrammed the transition probabilities into other MS basins of attraction and, for particular transitions, we tracked the transition paths to extract the number of spin-flip attempts and the energy barriers. The “basin size” of a given MS state is the number of patterns <italic>in the recorded data</italic> from which the given MS state is reached by descending on the energy landscape. The results presented in <xref ref-type="fig" rid="pcbi-1003408-g011">Figure 11</xref> are typical of the transitions we observe across multiple subnetworks of 120 neurons.</p>
</sec><sec id="s4h">
<title>Computing the entropy and partition function of the maximum entropy distributions</title>
<p>Entropy estimation is a challenging problem. As explained in the text, the usual approach of counting samples and identifying frequencies with probabilities will fail catastrophically in all the cases of interest here, even if we are free to draw samples from our model rather than from real data. Within the framework of maximum entropy models, however, the equivalence to statistical mechanics gives us several tools. Here we summarize the evidence that these multiple tools lead to consistent answers, so that we can be confident in our estimates.</p>
<p>Our first try at entropy estimation is based on the heat capacity integration in <xref ref-type="disp-formula" rid="pcbi.1003408.e172">Eq. (29)</xref>. To begin, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e258" xlink:type="simple"/></inline-formula> neurons, we <italic>can</italic> enumerate all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e259" xlink:type="simple"/></inline-formula> states of the network and hence we can find the maximum entropy distributions exactly (with no Monte Carlo sampling). From these distributions we can also compute the entropy exactly, and it agrees with the results of the heat capacity integration. Indeed, there is good agreement for the entire distribution, with Jensen-Shannon divergence between exact maximum entropy solutions and solutions using our reconstruction procedure at ∼10<sup>−6</sup>. As a second check, now usable for all <italic>N</italic>, we note that the entropy is zero at <italic>T</italic> = 0, but <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e260" xlink:type="simple"/></inline-formula> bits at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e261" xlink:type="simple"/></inline-formula>. Thus we can do the heat capacity integration from <italic>T</italic> = 1 to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e262" xlink:type="simple"/></inline-formula> instead of <italic>T</italic> = 0 to <italic>T</italic> = 1, and we get essentially the same result for the entropy (mean relative difference of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e263" xlink:type="simple"/></inline-formula> across 30 networks at <italic>N</italic> = 100 and <italic>N</italic> = 120).</p>
<p>Leaning further on the mapping to statistical physics, we realize that the heat capacity is a summary statistic for the density of states. There are Monte Carlo sampling methods, due to Wang and Landau <xref ref-type="bibr" rid="pcbi.1003408-Wang1">[73]</xref> (WL), that aim specifically at estimating this density, and those allow us to compute the entropy from a single simulation run. Based on the benchmarks of the WL method that we performed (convergence of the result with histogram refinement) we believe that the entropy estimate from the WL MC has a fractional bias that is at or below <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e264" xlink:type="simple"/></inline-formula>. The results, in <xref ref-type="supplementary-material" rid="pcbi.1003408.s002">Figure S2A</xref>, are in excellent agreement with the heat capacity integration.</p>
<p>K-pairwise models have the attractive feature that, by construction, they match exactly the probability of the all-silent pattern, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e265" xlink:type="simple"/></inline-formula>, seen in the data. As explained in the main text, this means that we can “measure” the partition function, <italic>Z</italic>, of our model directly from the probability of silence. Then we can compute the average energy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e266" xlink:type="simple"/></inline-formula> from a single MC sampling run, and find the entropy for each network. As shown in Figures S2B and C, the results agree both with the heat capacity integration and with the Wang–Landau method, to an accuracy of better than 1%.</p>
<p>The error on entropy estimation from the probability of silence has two contributions: the first has to do with the error in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e267" xlink:type="simple"/></inline-formula> that contributes to error in <italic>Z</italic> by <xref ref-type="disp-formula" rid="pcbi.1003408.e177">Eq (30)</xref>, and the second with the estimate of the mean energy, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e268" xlink:type="simple"/></inline-formula>, of the model. By construction of the model, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e269" xlink:type="simple"/></inline-formula> needs to be matched to data, but in fact that match is limited by the error bar on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e270" xlink:type="simple"/></inline-formula> itself estimated from data, and on how well the model reproduces this observable; these two errors combine to give a fractional error of a few tenths of a percent. From this error one may then compute the fractional error in Z; for <italic>N</italic> = 120 groups of neurons, this is on average <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e271" xlink:type="simple"/></inline-formula>. For the entropy estimation, we also need the average energy; this itself can be estimated through a long Metropolis MC sampling. The sampling is unbiased, but with an error of typically between half and a percent, for <italic>N</italic> = 120 sets. Together, these errors combine into a conservative error estimate of ∼1% for the entropy computed from the silence and from the average energy, although the true error might in fact be smaller.</p>
<p>Finally, there are methods that allow us to estimate entropy by counting samples even in cases where the number of samples is much smaller than the number of states <xref ref-type="bibr" rid="pcbi.1003408-Nemenman1">[71]</xref> (NSB). The NSB method is not guaranteed to work in all cases, but the comparison with the entropy estimates from heat capacity integration (<xref ref-type="supplementary-material" rid="pcbi.1003408.s003">Figure S3A</xref>) suggests that so long as <italic>N</italic>&lt;50, NSB estimates are reliable (see also <xref ref-type="bibr" rid="pcbi.1003408-Berry2">[108]</xref>). Supplementary <xref ref-type="supplementary-material" rid="pcbi.1003408.s003">Figure S3B</xref> shows that the NSB estimate of the entropy does not depend on the sample size for <italic>N</italic>&lt;50; if we draw from our models a number of samples equal to the number found in the data, and then ten times more, we see that the estimated entropy changes by just a few percent, within the error bars. This is another signature of the accuracy of the NSB estimator for <italic>N</italic>&lt;50. As <italic>N</italic> increases, these direct estimates of entropy become significantly dependent on the sample size, and start to disagree with the heat capacity integration. The magnitude of these systematic errors depends on the structure of the underlying distribution, and it is thus interesting that NSB estimates of the entropy from our model and from the real data agree with one another up to <italic>N</italic> = 120, as shown in <xref ref-type="supplementary-material" rid="pcbi.1003408.s003">Figure S3C</xref>.</p>
</sec><sec id="s4i">
<title>Are real networks in the perturbative regime?</title>
<p>The pairwise correlations between neurons in this system are quite weak. Thus, if we make a model for the activity of just two neurons, treating them as independent is a very good approximation. It might seem that this statement is invariant to the number of neurons that we consider—either correlations are weak, or they are strong. But this misses the fact that weak but widespread correlations can have a non–perturbative effect on the structure of the probability distribution. Nonetheless, it has been suggested that maximum entropy methods are successful only because correlations are weak, and hence that we can't really capture non–trivial collective behaviors with this approach <xref ref-type="bibr" rid="pcbi.1003408-Roudi2">[34]</xref>.</p>
<p>While independent models fail to explain the behavior of even small groups of neurons <xref ref-type="bibr" rid="pcbi.1003408-Schneidman1">[4]</xref>, it is possible that groups of neurons might be in a weak perturbative regime, where the contribution of pairwise interactions could be treated as a small perturbation to the independent Hamiltonian, if the expansion was carried out in the correct representation <xref ref-type="bibr" rid="pcbi.1003408-Roudi2">[34]</xref>. Of course, with finite <italic>N</italic>, all quantities must be analytic functions of the coupling constants, and so we expect that, if carried to sufficiently high order, any perturbative scheme will converge—although this convergence may become much slower at larger <italic>N</italic>, signaling genuinely collective behavior in large networks.</p>
<p>To make the question of whether correlations are weak or strong precise, we ask whether we can approximate the maximum entropy distribution with the leading orders of perturbation theory. There are a number of reasons to think that this won't work <xref ref-type="bibr" rid="pcbi.1003408-Cocco1">[109]</xref>–<xref ref-type="bibr" rid="pcbi.1003408-Azhar2">[112]</xref>, but in light of the suggestion from Ref <xref ref-type="bibr" rid="pcbi.1003408-Roudi2">[34]</xref> we wanted to explore this explicitly. If correlations are weak, there is a simple relationship between the correlations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e272" xlink:type="simple"/></inline-formula> and the corresponding interactions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e273" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003408-Roudi2">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Sessak1">[113]</xref>. We see in <xref ref-type="supplementary-material" rid="pcbi.1003408.s004">Figure S4A</xref> that this relationship is violated, and the consequence is that models built by assuming this perturbative relationship are easily distinguishable from the data even at <italic>N</italic> = 15 (<xref ref-type="supplementary-material" rid="pcbi.1003408.s004">Figure S4B</xref>). We conclude that treating correlations as a small perturbation is inconsistent with the data. Indeed, if we try to compute the entropy itself, it can be shown that even going out to fourth order in perturbation theory is not enough once <italic>N</italic>&gt;10 <xref ref-type="bibr" rid="pcbi.1003408-Azhar1">[111]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Azhar2">[112]</xref>.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003408.s001" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003408.s001" position="float" xlink:type="simple"><label>Figure S1</label><caption>
<p><bold>Interactions in the (K-)pairwise model.</bold> (<bold>A</bold>) The distributions of pairwise couplings, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e274" xlink:type="simple"/></inline-formula>, in pairwise models of <xref ref-type="disp-formula" rid="pcbi.1003408.e088">Eq (19)</xref>, for different network sizes (<italic>N</italic>). The distribution is pooled over 30 networks at each <italic>N</italic>. (<bold>B</bold>) The mean (solid) and s.d. (dashed) of the distributions in (A) as a function of network size (black); the mean and s.d. of the corresponding distributions for K-pairwise models as a function of network size (red).</p>
<p>(PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003408.s002" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003408.s002" position="float" xlink:type="simple"><label>Figure S2</label><caption>
<p><bold>Precision of entropy estimates.</bold> (<bold>A</bold>) Entropy estimation using heat capacity integration (x-axis) from <xref ref-type="disp-formula" rid="pcbi.1003408.e172">Eq (29)</xref> versus entropy estimation using the Wang-Landau sampling method (y-axis) <xref ref-type="bibr" rid="pcbi.1003408-Wang1">[73]</xref>. Each plot symbol is one subnetwork of either <italic>N</italic> = 100 or <italic>N</italic> = 120 neurons (circles = pairwise models, crosses = K-pairwise models). The two sampling methods yield results that agree to within ∼1%. (<bold>B</bold>) Fractional difference between the heat capacity method and the entropy determined from the all-silent pattern. The histogram is over 30 networks at <italic>N</italic> = 100 and 30 at <italic>N</italic> = 120, for the K-pairwise model. (<bold>C</bold>) Fractional difference between the Wang-Landau sampling method and the entropy determined from the all-silent pattern. Same convention as in (B).</p>
<p>(PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003408.s003" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003408.s003" position="float" xlink:type="simple"><label>Figure S3</label><caption>
<p><bold>Sample-based entropy estimation.</bold> (<bold>A</bold>) The bias in entropy estimates computed directly from samples drawn from K-pairwise models. The NSB entropy estimate <xref ref-type="bibr" rid="pcbi.1003408-Nemenman1">[71]</xref> in bits per neuron computed using <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e275" xlink:type="simple"/></inline-formula> samples from the model (same size as the experimental data set) on y-axis; the true entropy (using heat capacity integration) method on x-axis. Each dot represents one subnetwork of a particular size (<italic>N</italic>, different colors). For small networks (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e276" xlink:type="simple"/></inline-formula>) the bias is negligible, but estimation from samples significantly underestimates the entropy for larger networks. (<bold>B</bold>) The fractional bias of the estimator as a function of <italic>N</italic> (black dots = data from (A), gray dots = using 10 fold more samples). Red line shows the mean ± s.d. over 30 subnetworks at each size. (<bold>C</bold>) The NSB estimation of entropy from samples drawn from the model (x-axis) vs the samples from real experiment (y-axis); each dot is a subnetwork of a given size (color as in (A)). The data entropy estimate is slightly smaller than that of the model, as is expected for true entropy; for estimates from finite data this would only be expected if the biases on data vs MC samples were the same.</p>
<p>(PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003408.s004" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003408.s004" position="float" xlink:type="simple"><label>Figure S4</label><caption>
<p><bold>Perturbative vs exact solution for the pairwise maximum entropy models.</bold> (<bold>A</bold>) The comparison of couplings <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e277" xlink:type="simple"/></inline-formula> for a group of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e278" xlink:type="simple"/></inline-formula> neurons, computed using the exact maximum entropy reconstruction algorithm, with the lowest order perturbation theory result, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e279" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e280" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e281" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003408-Roudi2">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1003408-Sessak1">[113]</xref>. In the case of larger networks, the perturbative <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e282" xlink:type="simple"/></inline-formula> deviate more and more from equality (black line). Inset: the average absolute difference between the true and perturbative coupling, normalized by the average true coupling. (<bold>B</bold>) The exact pairwise model, <xref ref-type="disp-formula" rid="pcbi.1003408.e088">Eq (19)</xref>, can be compared to the distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e283" xlink:type="simple"/></inline-formula>, sampled from data; the olive line (circles) shows the Jensen-Shannon divergence (corrected for finite sample size) between the two distributions, for four example networks of size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e284" xlink:type="simple"/></inline-formula>. The turquoise line (squares) shows the same comparison in which the pairwise model parameters, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e285" xlink:type="simple"/></inline-formula>, were calculated perturbatively. The black line shows the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e286" xlink:type="simple"/></inline-formula> between two halves of the data for the four selected networks.</p>
<p>(PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003408.s005" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003408.s005" position="float" xlink:type="simple"><label>Figure S5</label><caption>
<p><bold>Predicted vs real distributions of energy, </bold><bold><italic>E</italic></bold><bold>, for the pairwise model.</bold> The cumulative distribution of energies, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e287" xlink:type="simple"/></inline-formula> from <xref ref-type="disp-formula" rid="pcbi.1003408.e128">Eq (22)</xref>, for the patterns generated by the pairwise models (red) and the data (black), in a population of 120 neurons. Inset shows the high energy tails of the distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e288" xlink:type="simple"/></inline-formula> from <xref ref-type="disp-formula" rid="pcbi.1003408.e131">Eq (24)</xref>; dashed line denotes the energy that corresponds to the probability of seeing the pattern once in an experiment. This figure is analogous to <xref ref-type="fig" rid="pcbi-1003408-g008">Figure 8</xref>; the same group of neurons is used here.</p>
<p>(PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003408.s006" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003408.s006" position="float" xlink:type="simple"><label>Figure S6</label><caption>
<p><bold>Dichotomized Gaussian model performance for a group of </bold><bold><italic>N</italic></bold><bold> = 120 neurons.</bold> (<bold>A</bold>) The distribution of synchronous spikes, <italic>P</italic>(<italic>K</italic>), in the data (black) and in the DG model fit to data (red). For this network, DG predicts <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e289" xlink:type="simple"/></inline-formula>; the true value is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003408.e290" xlink:type="simple"/></inline-formula>. (<bold>B</bold>) The comparison of three-point correlations estimated from data (x-axis) and predicted by the two models (y-axis; red = DG, black = K-pairwise). As in <xref ref-type="fig" rid="pcbi-1003408-g007">Figure 7</xref>, three-point correlations are binned; shown are the means for the predictions in a given bin, error-bars are omitted for clarity. DG underperforms the K-pairwise model specifically for negative correlations. (<bold>C</bold>) The probability of coincidences, analogous to <xref ref-type="fig" rid="pcbi-1003408-g014">Figure 14</xref>, computed for the DG model (red) and compared to data (black); gray line is the independent model.</p>
<p>(PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003408.s007" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003408.s007" position="float" xlink:type="simple"><label>Figure S7</label><caption>
<p><bold>Maximum entropy models for the checkerboard stimulation.</bold> We stimulated a separate retina with a checkerboard stimulus. The square check size was 69 µm, smaller than the typical size of the ganglion cell receptive fields. Each check was randomly selected to be either black or white on each frame displayed at a rate of 30 Hz. The entire stimulus consisted of 69 repeats of 30 seconds each, and subgroups of up to 120 neurons were analyzed. (<bold>A</bold>) Distribution of synchrony, <italic>P</italic>(<italic>K</italic>), for a group of 120 neurons, in the data (red), as predicted by the pairwise model (black), and by the independent model (gray). (<bold>B</bold>) As the network of <italic>N</italic> neurons gets larger, the discrepancy in the prediction of the probability of silence, <italic>P</italic>(0), grows in a qualitatively similar way as under naturalistic stimulation. (<bold>C</bold>) K-pairwise models capture the distribution of energies very well even at <italic>N</italic> = 120 (cf. <xref ref-type="fig" rid="pcbi-1003408-g008">Figure 8</xref> for an analogous plot for natural stimulation). (<bold>D</bold>) Under checkerboard stimulation, the distribution of codewords is less correlated than under the natural stimulation, as quantified by the ratio of the entropy to the independent entropy, shown as a function of subgroup size <italic>N</italic>.</p>
<p>(PDF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We thank A Cavagna, I Giardina, T Mora, SE Palmer, GJ Stephens, and A Walczak for many helpful discussions.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1003408-Hopfield1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name> (<year>1982</year>) <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proc Natl Acad Sci (USA)</source> <volume>79</volume>: <fpage>2554</fpage>–<lpage>8</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Amit1"><label>2</label>
<mixed-citation publication-type="book" xlink:type="simple">Amit DJ (1989) Modeling Brain Function: The World of Attractor Neural Networks. Cambridge: Cambridge University Press.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Hertz1"><label>3</label>
<mixed-citation publication-type="book" xlink:type="simple">Hertz J, Krogh A &amp; Palmer RG (1991) Introduction to the Theory of Neural Computation. Redwood City: Addison Wesley.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Schneidman1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name>, <name name-style="western"><surname>Segev</surname><given-names>R</given-names></name> (<year>2006</year>) <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name> (<year>2006</year>) <article-title>Weak pairwise correlations imply strongly correlated network states in a neural population</article-title>. <source>Nature</source> <volume>440</volume>: <fpage>1007</fpage>–<lpage>1012</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Shlens1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shlens</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Field</surname><given-names>GD</given-names></name>, <name name-style="western"><surname>Gaulthier</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Grivich</surname><given-names>MI</given-names></name>, <name name-style="western"><surname>Petrusca</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Sher</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Litke</surname><given-names>AM</given-names></name> (<year>2006</year>) <name name-style="western"><surname>Chichilnisky</surname><given-names>EJ</given-names></name> (<year>2006</year>) <article-title>The structure of multi-neuron firing patterns in primate retina</article-title>. <source>J Neurosci</source> <volume>26</volume>: <fpage>8254</fpage>–<lpage>8266</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Tkaik1"><label>6</label>
<mixed-citation publication-type="other" xlink:type="simple">Tkačik G, Schneidman E, Berry MJ II &amp; Bialek W (2006) Ising models for networks of real neurons. <italic>arXiv.org</italic>: q-bio/0611072.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Yu1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yu</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Huang</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Singer</surname><given-names>W</given-names></name> (<year>2008</year>) <name name-style="western"><surname>Nikolic</surname><given-names>D</given-names></name> (<year>2008</year>) <article-title>A small world of neuronal synchrony</article-title>. <source>Cereb Cortex</source> <volume>18</volume>: <fpage>2891</fpage>–<lpage>2901</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Tang1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tang</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Jackson</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Hobbs</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Smith</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Patel</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Prieto</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Petruscam</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Grivich</surname><given-names>MI</given-names></name>, <name name-style="western"><surname>Sher</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Hottowy</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Dabrowski</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Litke</surname><given-names>AM</given-names></name> (<year>2008</year>) <name name-style="western"><surname>Beggs</surname><given-names>JM</given-names></name> (<year>2008</year>) <article-title>A maximum entropy model applied to spatial and temporal correlations from cortical networks in vitro</article-title>. <source>J Neurosci</source> <volume>28</volume>: <fpage>505</fpage>–<lpage>518</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Tkaik2"><label>9</label>
<mixed-citation publication-type="other" xlink:type="simple">Tkačik G, Schneidman E, Berry MJ II &amp; Bialek W (2009) Spin–glass models for a network of real neurons. <italic>arXiv.org</italic>: 0912.5409.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Shlens2"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shlens</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Field</surname><given-names>GD</given-names></name>, <name name-style="western"><surname>Gaulthier</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Greschner</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Sher</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Litke</surname><given-names>AM</given-names></name> (<year>2009</year>) <name name-style="western"><surname>Chichilnisky</surname><given-names>EJ</given-names></name> (<year>2009</year>) <article-title>The structure of large–scale synchronized firing in primate retina</article-title>. <source>J Neurosci</source> <volume>29</volume>: <fpage>5022</fpage>–<lpage>5031</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Ohiorhenuan1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ohiorhenuan</surname><given-names>IE</given-names></name>, <name name-style="western"><surname>Mechler</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Purpura</surname><given-names>KP</given-names></name>, <name name-style="western"><surname>Schmid</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Hu</surname><given-names>Q</given-names></name> (<year>2010</year>) <name name-style="western"><surname>Victor</surname><given-names>JD</given-names></name> (<year>2010</year>) <article-title>Sparse coding and higher–order correlations in fine–scale cortical networks</article-title>. <source>Nature</source> <volume>466</volume>: <fpage>617</fpage>–<lpage>621</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Ganmor1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ganmor</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Segev</surname><given-names>R</given-names></name> (<year>2011</year>) <name name-style="western"><surname>Schniedman</surname><given-names>E</given-names></name> (<year>2011</year>) <article-title>Sparse low–order interaction network underlies a highly correlated and learnable neural population code</article-title>. <source>Proc Natl Acad Sci (USA)</source> <volume>108</volume>: <fpage>9679</fpage>–<lpage>9684</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Vasquez1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vasquez</surname><given-names>JC</given-names></name>, <name name-style="western"><surname>Marre</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Palacios</surname><given-names>AG</given-names></name>, <name name-style="western"><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name> (<year>2012</year>) <name name-style="western"><surname>Cessac</surname><given-names>B</given-names></name> (<year>2012</year>) <article-title>Gibbs distribution analysis of temporal correlations structure in retina ganglion cells</article-title>. <source>J Physiol Paris</source> <volume>3–4</volume>: <fpage>120</fpage>–<lpage>127</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-GranotAtedgi1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Granot-Atedgi</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Tkačik</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Segev</surname><given-names>R</given-names></name> (<year>2013</year>) <name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name> (<year>2013</year>) <article-title>Stimulus-dependent maximum entropy models of neural population codes</article-title>. <source>PLoS Comput Biol</source> <volume>9</volume>: <fpage>e1002922</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Ganmor2"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ganmor</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Segev</surname><given-names>R</given-names></name> (<year>2011</year>) <name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name> (<year>2011</year>) <article-title>The architecture of functional interaction networks in the retina</article-title>. <source>J Neurosci</source> <volume>31</volume>: <fpage>3044</fpage>–<lpage>3054</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Tkaik3"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tkačik</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Prentice</surname><given-names>JS</given-names></name>, <name name-style="western"><surname>Victor</surname><given-names>JD</given-names></name> (<year>2010</year>) <name name-style="western"><surname>Balasubramanian</surname><given-names>V</given-names></name> (<year>2010</year>) <article-title>Local statistics in natural scenes predict the saliency of synthetic textures</article-title>. <source>Proc Natl Acad Sci (USA)</source> <volume>107</volume>: <fpage>18149</fpage>–<lpage>18154</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Stephens1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stephens</surname><given-names>GJ</given-names></name>, <name name-style="western"><surname>Mora</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Tkačik</surname><given-names>G</given-names></name> (<year>2013</year>) <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name> (<year>2013</year>) <article-title>Statistical thermodynamics of natural images</article-title>. <source>Phys Rev Lett</source> <volume>110</volume>: <fpage>018701</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Saremi1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Saremi</surname><given-names>S</given-names></name> (<year>2013</year>) <name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name> (<year>2013</year>) <article-title>Hierarchical model of natural images and the origin of scale invariance</article-title>. <source>Proc Natl Acad Sci (USA)</source> <volume>110</volume>: <fpage>3071</fpage>–<lpage>3076</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Lezon1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lezon</surname><given-names>TR</given-names></name>, <name name-style="western"><surname>Banavar</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Cieplak</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Maritan</surname><given-names>A</given-names></name> (<year>2006</year>) <name name-style="western"><surname>Federoff</surname><given-names>NV</given-names></name> (<year>2006</year>) <article-title>Using the principle of entropy maximization to infer genetic interaction networks from gene expression patterns</article-title>. <source>Proc Natl Acad Sci (USA)</source> <volume>103</volume>: <fpage>19033</fpage>–<lpage>19038</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Tkaik4"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tkačik</surname><given-names>G</given-names></name> (<year>2007</year>) <article-title>Information Flow in Biological Networks</article-title>. <source>Dissertation, Princeton University</source></mixed-citation>
</ref>
<ref id="pcbi.1003408-Bialek1"><label>21</label>
<mixed-citation publication-type="other" xlink:type="simple">Bialek W &amp; Ranganathan R (2007) Rediscovering the power of pairwise interactions. <italic>arXiv.org</italic>: 0712.4397.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Seno1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seno</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Trovato</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Banavar</surname><given-names>JR</given-names></name> (<year>2008</year>) <name name-style="western"><surname>Maritan</surname><given-names>A</given-names></name> (<year>2008</year>) <article-title>Maximum entropy approach for deducing amino acid interactions in proteins</article-title>. <source>Phys Rev Lett</source> <volume>100</volume>: <fpage>078102</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Weigt1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Weigt</surname><given-names>M</given-names></name>, <name name-style="western"><surname>White</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Szurmant</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Hoch</surname><given-names>JA</given-names></name> (<year>2009</year>) <name name-style="western"><surname>Hwa</surname><given-names>T</given-names></name> (<year>2009</year>) <article-title>Identification of direct residue contacts in protein–protein interaction by message passing</article-title>. <source>Proc Natl Acad Sci (USA)</source> <volume>106</volume>: <fpage>67</fpage>–<lpage>72</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Halabi1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Halabi</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Rivoire</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Leibler</surname><given-names>S</given-names></name> (<year>2009</year>) <name name-style="western"><surname>Ranganathan</surname><given-names>R</given-names></name> (<year>2009</year>) <article-title>Protein sectors: Evolutionary units of three–dimensional structure</article-title>. <source>Cell</source> <volume>138</volume>: <fpage>774</fpage>–<lpage>786</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Mora1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mora</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Walczak</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name> (<year>2010</year>) <name name-style="western"><surname>Callan</surname><given-names>CG</given-names><suffix>Jr</suffix></name> (<year>2010</year>) <article-title>Maximum entropy models for antibody diversity</article-title>. <source>Proc Natl Acad Sci (USA)</source> <volume>107</volume>: <fpage>5405</fpage>–<lpage>5410</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Marks1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marks</surname><given-names>DS</given-names></name>, <name name-style="western"><surname>Colwell</surname><given-names>LJ</given-names></name>, <name name-style="western"><surname>Sheridan</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Hopf</surname><given-names>TA</given-names></name>, <name name-style="western"><surname>Pagnani</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Zecchina</surname><given-names>R</given-names></name> (<year>2011</year>) <name name-style="western"><surname>Sander</surname><given-names>C</given-names></name> (<year>2011</year>) <article-title>Protein 3D structure computed from evolutionary sequence variation</article-title>. <source>PLoS One</source> <volume>6</volume>: <fpage>e28766</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Sulkowska1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sulkowska</surname><given-names>JI</given-names></name>, <name name-style="western"><surname>Morocos</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Weigt</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Hwa</surname><given-names>T</given-names></name> (<year>2012</year>) <name name-style="western"><surname>Onuchic</surname><given-names>JN</given-names></name> (<year>2012</year>) <article-title>Genomics–aided structure prediction</article-title>. <source>Proc Natl Acad Sci (USA)</source> <volume>109</volume>: <fpage>10340</fpage>–<lpage>10345</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Stephens2"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stephens</surname><given-names>GJ</given-names></name> (<year>2010</year>) <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name> (<year>2010</year>) <article-title>Statistical mechanics of letters in words</article-title>. <source>Phys Rev E</source> <volume>81</volume>: <fpage>066119</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Bialek2"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Cavagna</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Giardina</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Mora</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Silvestri</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Viale</surname><given-names>M</given-names></name> (<year>2012</year>) <name name-style="western"><surname>Walczak</surname><given-names>A</given-names></name> (<year>2012</year>) <article-title>Statistical mechanics for natural flocks of birds</article-title>. <source>Proc Natl Acad Sci (USA)</source> <volume>109</volume>: <fpage>4786</fpage>–<lpage>4791</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Shemesh1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shemesh</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Sztainberg</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Forkosh</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Shlapobersky</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>A</given-names></name> (<year>2013</year>) <name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name> (<year>2013</year>) <article-title>High-order social interactions in groups of mice</article-title>. <source>Elife</source> <volume>2</volume>: <fpage>e00759</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Mora2"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mora</surname><given-names>T</given-names></name> (<year>2010</year>) <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name> (<year>2010</year>) <article-title>Are biological systems poised at criticality?</article-title> <source>J Stat Phys</source> <volume>144</volume>: <fpage>268</fpage>–<lpage>302</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Nirenberg1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nirenberg</surname><given-names>S</given-names></name> (<year>2007</year>) <name name-style="western"><surname>Victor</surname><given-names>J</given-names></name> (<year>2007</year>) <article-title>Analyzing the activity of large populations of neurons: How tractable is the problem</article-title>. <source>Curr Opin Neurobiol</source> <volume>17</volume>: <fpage>397</fpage>–<lpage>400</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Roudi1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roudi</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Aurell</surname><given-names>E</given-names></name> (<year>2009</year>) <name name-style="western"><surname>Hertz</surname><given-names>JA</given-names></name> (<year>2009</year>) <article-title>Statistical physics of pairwise probability models</article-title>. <source>Front Comput Neurosci</source> <volume>3</volume>: <fpage>22</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Roudi2"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roudi</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Nirenberg</surname><given-names>S</given-names></name> (<year>2009</year>) <name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name> (<year>2009</year>) <article-title>Pairwise maximum entropy models for studying large biological systems: when they can work and when they can't</article-title>. <source>PLoS Comput Biol</source> <volume>5</volume>: <fpage>e1000380</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Roudi3"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roudi</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Trycha</surname><given-names>J</given-names></name> (<year>2009</year>) <name name-style="western"><surname>Hertz</surname><given-names>J</given-names></name> (<year>2009</year>) <article-title>The Ising model for neural data: model quality and approximate methods for extracting functional connectivity</article-title>. <source>Phys Rev E</source> <volume>79</volume>: <fpage>051915</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Marre1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marre</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Amodei</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Deshmukh</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Sadeghi</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Soo</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Holy</surname><given-names>TE</given-names></name> (<year>2012</year>) <name name-style="western"><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name> (<year>2012</year>) <article-title>Mapping a complete neural population in the retina</article-title>. <source>J Neurosci</source> <volume>32</volume>: <fpage>14859</fpage>–<lpage>14873</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Broderick1"><label>37</label>
<mixed-citation publication-type="other" xlink:type="simple">Broderick T, Dudik M, Tkačik, G Schapire RE &amp; Bialek W (2007) Faster solutions of the inverse pairwise Ising problem. <italic>arXiv.org</italic>: 0712.2437.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Treves1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Treves</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name>, <name name-style="western"><surname>Booth</surname><given-names>M</given-names></name> (<year>1999</year>) <name name-style="western"><surname>Wakeman</surname><given-names>EA</given-names></name> (<year>1999</year>) <article-title>Firing rate distributions and efficiency of information transmission in inferior temporal cortex neurons to natural visual stimuli</article-title>. <source>Neural Comput</source> <volume>11</volume>: <fpage>601</fpage>–<lpage>631</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Tkaik5"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tkačik</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Marre</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Mora</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Amodei</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name> (<year>2013</year>) <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name> (<year>2013</year>) <article-title>The simplest maximum entropy model for collective behavior in a neural network</article-title>. <source>J Stat Mech</source> <fpage>P03011</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Okun1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Okun</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Yger</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Marguet</surname><given-names>SL</given-names></name>, <name name-style="western"><surname>Gerard-Mercier</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Benucci</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Katzner</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Busse</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Carandini</surname><given-names>M</given-names></name> (<year>2012</year>) <name name-style="western"><surname>Harris</surname><given-names>KD</given-names></name> (<year>2012</year>) <article-title>Population rate dynamics and multi neuron firing patterns in sensory cortex</article-title>. <source>J Neurosci</source> <volume>32</volume>: <fpage>17108</fpage>–<lpage>17119</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Jaynes1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jaynes</surname><given-names>ET</given-names></name> (<year>1957</year>) <article-title>Information theory and statistical mechanics</article-title>. <source>Phys Rev</source> <volume>106</volume>: <fpage>620</fpage>–<lpage>630</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Cessac1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cessac</surname><given-names>B</given-names></name> (<year>2013</year>) <name name-style="western"><surname>Cofré</surname><given-names>R</given-names></name> (<year>2013</year>) <article-title>Spike train statistics and Gibbs distributions</article-title>. <source>J Physiol Paris</source> <bold>pii:</bold> S0928-4257. <italic>arXiv.org:1302.5007</italic>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Rieke1"><label>43</label>
<mixed-citation publication-type="book" xlink:type="simple">Rieke F, Warland D, de Ruyter van Steveninck RR &amp; Bialek W (1997) Spikes: Exploring the Neural Code. Cambridge: MIT Press.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Bazaraa1"><label>44</label>
<mixed-citation publication-type="book" xlink:type="simple">Bazaraa MS, Sherali HD &amp; Shetty CM (2005) Nonlinear programming: Theory and algorithms. Hoboken NJ, USA: Wiley &amp; Sons.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Pillow1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pillow</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Shlens</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Sher</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Litke</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Chichilnisky</surname><given-names>EJ</given-names></name> (<year>2008</year>) <name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name> (<year>2008</year>) <article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title>. <source>Nature</source> <volume>454</volume>: <fpage>995</fpage>–<lpage>999</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Fairhall1"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fairhall</surname><given-names>AL</given-names></name>, <name name-style="western"><surname>Burlingame</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Narasimhan</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Puchalla</surname><given-names>JL</given-names></name> (<year>2006</year>) <name name-style="western"><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name> (<year>2006</year>) <article-title>Selectivity for multiple stimulus features in retinal ganglion cells</article-title>. <source>J Neurophysiol</source> <volume>96</volume>: <fpage>2724</fpage>–<lpage>2738</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Keat1"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keat</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Reinagel</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Reid</surname><given-names>RC</given-names></name> (<year>2001</year>) <name name-style="western"><surname>Meister</surname><given-names>M</given-names></name> (<year>2001</year>) <article-title>Predicting every spike: a model for the responses of visual neurons</article-title>. <source>Neuron</source> <volume>30</volume>: <fpage>803</fpage>–<lpage>817</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Sadeghi1"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sadeghi</surname><given-names>KS</given-names></name> (<year>2009</year>) <article-title>Progress on Deciphering the Retinal Code</article-title>. <source>Dissertation, Princeton University</source></mixed-citation>
</ref>
<ref id="pcbi.1003408-Tkaik6"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tkačik</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Prentice</surname><given-names>JS</given-names></name>, <name name-style="western"><surname>Balasubramanian</surname><given-names>V</given-names></name> (<year>2010</year>) <name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name> (<year>2010</year>) <article-title>Optimal population coding by noisy spiking neurons</article-title>. <source>Proc Natl Acad Sci (USA)</source> <volume>107</volume>: <fpage>14419</fpage>–<lpage>14424</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Tkaik7"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tkačik</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Granot-Atedgi</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Segev</surname><given-names>R</given-names></name> (<year>2013</year>) <name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name> (<year>2013</year>) <article-title>Retinal metric: a stimulus distance measure derived from population neural responses</article-title>. <source>Phys Rev Lett</source> <volume>110</volume>: <fpage>058104</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Ganmor3"><label>51</label>
<mixed-citation publication-type="other" xlink:type="simple">Ganmor E (2013) Noise, structure and adaptation in neural population codes. Thesis. Weizmann Institute of Science.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Perkel1"><label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perkel</surname><given-names>D</given-names></name> (<year>1968</year>) <name name-style="western"><surname>Bullock</surname><given-names>T</given-names></name> (<year>1968</year>) <article-title>Neural coding</article-title>. <source>Neurosci Res Program Bull</source> <volume>6</volume>: <fpage>221</fpage>–<lpage>343</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Ginzburg1"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ginzburg</surname><given-names>I</given-names></name> (<year>1994</year>) <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>1994</year>) <article-title>Theory of correlations in stochastic neural networks</article-title>. <source>Phys Rev E</source> <volume>50</volume>: <fpage>3171</fpage>–<lpage>3191</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Hopfield2"><label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name> (<year>2008</year>) <article-title>Searching for memories, sudoku, implicit check bits, and the iterative use of not–always–correct rapid neural computation</article-title>. <source>Neural Comp</source> <volume>20</volume>: <fpage>1119</fpage>–<lpage>1164</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Field1"><label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Field</surname><given-names>D</given-names></name> (<year>1987</year>) <article-title>Relations between the statistics of natural images and the response properties of cortical cells</article-title>. <source>J Opt Soc A</source> <volume>4</volume>: <fpage>2379</fpage>–<lpage>2394</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Dong1"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dong</surname><given-names>DW</given-names></name> (<year>1995</year>) <name name-style="western"><surname>Atick</surname><given-names>JJ</given-names></name> (<year>1995</year>) <article-title>Statistics of natural time-varying images</article-title>. <source>Network</source> <volume>6</volume>: <fpage>345</fpage>–<lpage>358</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Ruderman1"><label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ruderman</surname><given-names>DL</given-names></name> (<year>1994</year>) <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name> (<year>1994</year>) <article-title>Statistics of natural images: scaling in the woods</article-title>. <source>Phys Rev Lett</source> <volume>73</volume>: <fpage>814</fpage>–<lpage>817</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Schwartz1"><label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schwartz</surname><given-names>O</given-names></name> (<year>2001</year>) <name name-style="western"><surname>Simoncelli</surname><given-names>EP</given-names></name> (<year>2001</year>) <article-title>Natural signal statistics and sensory gain control</article-title>. <source>Nat Neurosci</source> <volume>4</volume>: <fpage>819</fpage>–<lpage>825</lpage> (2001).</mixed-citation>
</ref>
<ref id="pcbi.1003408-Bethge1"><label>59</label>
<mixed-citation publication-type="book" xlink:type="simple">Bethge M &amp; Berens P (2008) Near-maximum entropy models for binary neural representations of natural images. In: Platt J <etal>et al</etal>. eds. Adv Neural Info Proc Sys <volume>20</volume>: : 97–104. Cambridge, MA: MIT Press.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Stephens3"><label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stephens</surname><given-names>GJ</given-names></name>, <name name-style="western"><surname>Mora</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Tkačik</surname><given-names>G</given-names></name> (<year>2013</year>) <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name> (<year>2013</year>) <article-title>Statistical thermodynamics of natural images</article-title>. <source>Phys Rev Lett</source> <volume>110</volume>: <fpage>018701</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Mastronarde1"><label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mastronarde</surname><given-names>DN</given-names></name> (<year>1983</year>) <article-title>Interactions between ganglion cells in cat retina</article-title>. <source>J Neurophysiol</source> <volume>49</volume>: <fpage>350</fpage>–<lpage>365</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-DeVries1"><label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DeVries</surname><given-names>SH</given-names></name> (<year>1999</year>) <article-title>Correlated firing in rabbit retinal ganglion cells</article-title>. <source>J Neurophysiol</source> <volume>81</volume>: <fpage>908</fpage>–<lpage>920</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Brivanlou1"><label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brivanlou</surname><given-names>IH</given-names></name>, <name name-style="western"><surname>Warland</surname><given-names>DK</given-names></name> (<year>1998</year>) <name name-style="western"><surname>Meister</surname><given-names>M</given-names></name> (<year>1998</year>) <article-title>Mechanisms of concerted firing among retinal ganglion cells</article-title>. <source>Neuron</source> <volume>20</volume>: <fpage>527</fpage>–<lpage>539</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Trong1"><label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Trong</surname><given-names>PK</given-names></name> (<year>2008</year>) <name name-style="western"><surname>Rieke</surname><given-names>F</given-names></name> (<year>2008</year>) <article-title>Origin of correlated activity between parasol retinal ganglion cells</article-title>. <source>Nat Neurosci</source> <volume>11</volume>: <fpage>1343</fpage>–<lpage>1351</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Puchalla1"><label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Puchalla</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>RA</given-names></name> (<year>2005</year>) <name name-style="western"><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name> (<year>2005</year>) <article-title>Redundancy in the population code of the retina</article-title>. <source>Neuron</source> <volume>46</volume>: <fpage>493</fpage>–<lpage>504</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Segev1"><label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Segev</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Puchalla</surname><given-names>J</given-names></name> (<year>2006</year>) <name name-style="western"><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name> (<year>2006</year>) <article-title>Functional organization of retinal ganglion cells in the salamander retina</article-title>. <source>J Neurophysiol</source> <volume>95</volume>: <fpage>2277</fpage>–<lpage>2292</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Schneidman2"><label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Still</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name> (<year>2003</year>) <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name> (<year>2003</year>) <article-title>Network information and connected correlations</article-title>. <source>Phys Rev Lett</source> <volume>91</volume>: <fpage>238701</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Hopfield3"><label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name> (<year>1985</year>) <name name-style="western"><surname>Tank</surname><given-names>DW</given-names></name> (<year>1985</year>) <article-title>“Neural” computation of decisions in optimization problems</article-title>. <source>Biol Cybern</source> <volume>52</volume>: <fpage>141</fpage>–<lpage>152</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Hopfield4"><label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name> (<year>1986</year>) <name name-style="western"><surname>Tank</surname><given-names>DW</given-names></name> (<year>1986</year>) <article-title>Computing with neural circuits: a model</article-title>. <source>Science</source> <volume>233</volume>: <fpage>625</fpage>–<lpage>633</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Mezard1"><label>70</label>
<mixed-citation publication-type="book" xlink:type="simple">Mezard M, Parisi G &amp; Virasoro MA (1987) Spin Glass Theory and Beyond. Singapore: World Scientific</mixed-citation>
</ref>
<ref id="pcbi.1003408-Nemenman1"><label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nemenman</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Shafee</surname><given-names>F</given-names></name> (<year>2001</year>) <name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name> (<year>2001</year>) <article-title>Entropy and inference, revisited</article-title>. <source>Adv Neural Info Proc Syst</source> <volume>14</volume>: <fpage>471</fpage>–<lpage>478</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Landau1"><label>72</label>
<mixed-citation publication-type="book" xlink:type="simple">Landau DP &amp; Binder K (2000) Monte Carlo Simulations in Statistical Physics. Cambridge, UK: Cambridge University Press.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Wang1"><label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname><given-names>F</given-names></name> (<year>2001</year>) <name name-style="western"><surname>Landau</surname><given-names>DP</given-names></name> (<year>2001</year>) <article-title>Efficient, multiple-range random walk algorithm to calculate the density of states</article-title>. <source>Phys Rev Lett</source> <volume>86</volume>: <fpage>2050</fpage>–<lpage>2053</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Cover1"><label>74</label>
<mixed-citation publication-type="book" xlink:type="simple">Cover TM &amp; Thomas JA (1991) Elements of Information Theory. New York: Wiley.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Lanford1"><label>75</label>
<mixed-citation publication-type="book" xlink:type="simple">Lanford OE III (1973) Entropy and equilibrium states in classical statistical mechanics. Statistical Mechanics and Mathematical Problems, Lenard A ed., pp. 1–113. Berlin: Springer Verlag.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Landau2"><label>76</label>
<mixed-citation publication-type="book" xlink:type="simple">Landau LD and Lifshitz EM (1977) Statistical Physics. Oxford: Pergamon.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Berry1"><label>77</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name>, <name name-style="western"><surname>Warland</surname><given-names>DK</given-names></name> (<year>1997</year>) <name name-style="western"><surname>Meister</surname><given-names>M</given-names></name> (<year>1997</year>) <article-title>The structure and precision of retinal spike train</article-title>. <source>Proc Natl Acad Sci (USA)</source> <volume>94</volume>: <fpage>5411</fpage>–<lpage>5416</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-vanHateren1"><label>78</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Hateren</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>Rüttiger</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Sun</surname><given-names>H</given-names></name> (<year>2002</year>) <name name-style="western"><surname>Lee</surname><given-names>BB</given-names></name> (<year>2002</year>) <article-title>Processing of natural temporal stimuli by macaque retinal ganglion cells</article-title>. <source>J Neurosci</source> <volume>22</volume>: <fpage>9945</fpage>–<lpage>9960</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Shapley1"><label>79</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shapley</surname><given-names>RM</given-names></name> (<year>1978</year>) <name name-style="western"><surname>Victor</surname><given-names>JD</given-names></name> (<year>1978</year>) <article-title>The effect of contrast on the transfer properties of cat retinal ganglion cells</article-title>. <source>J Physiol</source> <volume>285</volume>: <fpage>275</fpage>–<lpage>298</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Victor1"><label>80</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Victor</surname><given-names>JD</given-names></name> (<year>1979</year>) <name name-style="western"><surname>Shapley</surname><given-names>RM</given-names></name> (<year>1979</year>) <article-title>Receptive field mechanisms of cat X and Y retinal ganglion cells</article-title>. <source>J Gen Physiol</source> <volume>74</volume>: <fpage>275</fpage>–<lpage>298</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Mante1"><label>81</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mante</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Bonin</surname><given-names>V</given-names></name> (<year>2008</year>) <name name-style="western"><surname>Carandini</surname><given-names>M</given-names></name> (<year>2008</year>) <article-title>Functional mechanisms shaping lateral geniculate responses to artificial and natural stimuli</article-title>. <source>Neuron</source> <volume>58</volume>: <fpage>625</fpage>–<lpage>638</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Chen1"><label>82</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chen</surname><given-names>EY</given-names></name>, <name name-style="western"><surname>Marre</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Fisher</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Schwartz</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Levy</surname><given-names>J</given-names></name>, <name name-style="western"><surname>da Silveira</surname><given-names>RA</given-names></name> (<year>2013</year>) <name name-style="western"><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name> (<year>2013</year>) <article-title>Alert response to motion onset in the retina</article-title>. <source>J Neurosci</source> <volume>33</volume>: <fpage>120</fpage>–<lpage>132</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Dan1"><label>83</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Atick</surname><given-names>JJ</given-names></name> (<year>1996</year>) <name name-style="western"><surname>Reid</surname><given-names>RC</given-names></name> (<year>1996</year>) <article-title>Efficient coding of natural scenes in lateral geniculate nucleus: experimental test of a computational theory</article-title>. <source>J Neurosci</source> <volume>16</volume>: <fpage>3351</fpage>–<lpage>3362</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Theunissen1"><label>84</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name>, <name name-style="western"><surname>Sen</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Doupe</surname><given-names>AJ</given-names></name> (<year>2000</year>) <article-title>Spectral-temporal receptive fields of nonlinear auditory neurons obtained using natural sounds</article-title>. <source>J Neurosci</source> <volume>20</volume>: <fpage>2135</fpage>–<lpage>2331</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Stopfer1"><label>85</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stopfer</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Jayaraman</surname><given-names>V</given-names></name> (<year>2003</year>) <name name-style="western"><surname>Laurent</surname><given-names>G</given-names></name> (<year>2003</year>) <article-title>Intensity versus identity coding in an olfactory system</article-title>. <source>Neuron</source> <volume>39</volume>: <fpage>991</fpage>–<lpage>1004</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Machens1"><label>86</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Machens</surname><given-names>CK</given-names></name>, <name name-style="western"><surname>Romo</surname><given-names>R</given-names></name> (<year>2010</year>) <name name-style="western"><surname>Brody</surname><given-names>CD</given-names></name> (<year>2010</year>) <article-title>Functional, but not anatomical, separation of “what” and “when” in prefrontal cortex</article-title>. <source>J Neurosci</source> <volume>30</volume>: <fpage>350</fpage>–<lpage>360</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Churchland1"><label>87</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Churchland</surname><given-names>MM</given-names></name>, <name name-style="western"><surname>Cunningham</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Kaufman</surname><given-names>MT</given-names></name>, <name name-style="western"><surname>Ryu</surname><given-names>SI</given-names></name> (<year>2010</year>) <name name-style="western"><surname>Shenoy</surname><given-names>KV</given-names></name> (<year>2010</year>) <article-title>Cortical preparatory activity: representation of movement or first cog in a dynamical machine?</article-title> <source>Neuron</source> <volume>68</volume>: <fpage>387</fpage>–<lpage>400</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Rutishauser1"><label>88</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rutishauser</surname><given-names>U</given-names></name>, <name name-style="western"><surname>Kotowicz</surname><given-names>A</given-names></name> (<year>2013</year>) <name name-style="western"><surname>Laurent</surname><given-names>G</given-names></name> (<year>2013</year>) <article-title>A method for closed-loop presentation of sensory stimuli conditional on the internal brain-state of awake animals</article-title>. <source>J Neurosci Methods</source> <volume>215</volume>: <fpage>139</fpage>–<lpage>155</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Segev2"><label>89</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Segev</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Goodhouse</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Puchalla</surname><given-names>JL</given-names></name> (<year>2004</year>) <name name-style="western"><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name> (<year>2004</year>) <article-title>Recording spikes from a large fraction of the ganglion cells in a retinal patch</article-title>. <source>Nat Neurosci</source> <volume>7</volume>: <fpage>1155</fpage>–<lpage>1162</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Marre2"><label>90</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marre</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Boustani</surname><given-names>SE</given-names></name>, <name name-style="western"><surname>Fregnac</surname><given-names>Y</given-names></name> (<year>2009</year>) <name name-style="western"><surname>Destexhe</surname><given-names>A</given-names></name> (<year>2009</year>) <article-title>Prediction of spatio–temporal patterns of neural activity from pairwise correlations</article-title>. <source>Phys Rev Lett</source> <volume>102</volume>: <fpage>138101</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Barlow1"><label>91</label>
<mixed-citation publication-type="book" xlink:type="simple">Barlow HB (1961) Possible principles underlying the transformation of sensory messages. In: Rosenblith W, ed. Sensory communication, pp 217–234. Cambridge, USA: MIT Press.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Attneave1"><label>92</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Attneave</surname><given-names>F</given-names></name> (<year>1954</year>) <article-title>Some informational aspects of visual perception</article-title>. <source>Psychol Rev</source> <volume>61</volume>: <fpage>183</fpage>–<lpage>193</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Atick1"><label>93</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Atick</surname><given-names>JJ</given-names></name> (<year>1990</year>) <name name-style="western"><surname>Redlich</surname><given-names>AN</given-names></name> (<year>1990</year>) <article-title>Towards a theory of early visual processing</article-title>. <source>Neural Comput</source> <volume>2</volume>: <fpage>308</fpage>–<lpage>320</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-vanHdateren1"><label>94</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Hdateren</surname><given-names>JH</given-names></name> (<year>1992</year>) <article-title>Real and optimal neural images in early vision</article-title>. <source>Nature</source> <volume>360</volume>: <fpage>68</fpage>–<lpage>70</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Barlow2"><label>95</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barlow</surname><given-names>H</given-names></name> (<year>2001</year>) <article-title>Redundancy reduction revisited</article-title>. <source>Network</source> <volume>12</volume>: <fpage>241</fpage>–<lpage>253</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Alivisatos1"><label>96</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alivisatos</surname><given-names>AP</given-names></name>, <name name-style="western"><surname>Chun</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Church</surname><given-names>GM</given-names></name>, <name name-style="western"><surname>Greenspan</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Roukes</surname><given-names>ML</given-names></name> (<year>2012</year>) <name name-style="western"><surname>Yuste</surname><given-names>R</given-names></name> (<year>2012</year>) <article-title>The brain activity map project and the challenge of functional connectomics</article-title>. <source>Neuron</source> <volume>74</volume>: <fpage>970</fpage>–<lpage>974</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Sourlas1"><label>97</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sourlas</surname><given-names>N</given-names></name> (<year>1989</year>) <article-title>Spin-glass models as error-correcting codes</article-title>. <source>Nature</source> <volume>339</volume>: <fpage>693</fpage>–<lpage>695</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Bohte1"><label>98</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bohte</surname><given-names>SM</given-names></name>, <name name-style="western"><surname>Spekreijse</surname></name> (<year>2000</year>) <name name-style="western"><surname>Roelfsema</surname><given-names>PR</given-names></name> (<year>2000</year>) <article-title>The effects of pairwise and higher-order correlations in the firing rate of a postsynaptic neuron</article-title>. <source>Neural Comput</source> <volume>12</volume>: <fpage>153</fpage>–<lpage>179</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Amari1"><label>99</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amari</surname><given-names>S-I</given-names></name>, <name name-style="western"><surname>Nakahara</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>S</given-names></name> (<year>2003</year>) <name name-style="western"><surname>Sakai</surname><given-names>Y</given-names></name> (<year>2003</year>) <article-title>Synchronous firing and higher-order interactions in neuron pool</article-title>. <source>Neural Comput</source> <volume>15</volume>: <fpage>127</fpage>–<lpage>142</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Macke1"><label>100</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Macke</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>Opper</surname><given-names>M</given-names></name> (<year>2011</year>) <name name-style="western"><surname>Bethge</surname><given-names>M</given-names></name> (<year>2011</year>) <article-title>Common input explains higher-order correlations and entropy in a simple model of neural population activity</article-title>. <source>Phys Rev Lett</source> <volume>106</volume>: <fpage>1</fpage>–<lpage>4</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Santos1"><label>101</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Santos</surname><given-names>GS</given-names></name>, <name name-style="western"><surname>Gireesh</surname><given-names>ED</given-names></name>, <name name-style="western"><surname>Plenz</surname><given-names>D</given-names></name> (<year>2010</year>) <name name-style="western"><surname>Nakahara</surname><given-names>H</given-names></name> (<year>2010</year>) <article-title>Hierarchical interaction structure of neural activities in cortical slice cultures</article-title>. <source>J Neurosci</source> <volume>30</volume>: <fpage>8720</fpage>–<lpage>8733</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Shimazaki1"><label>102</label>
<mixed-citation publication-type="other" xlink:type="simple">Shimazaki H, Sadeghi K, Ikegaya Y &amp; Toyoizumi T (2012) The simultaneous silence of neurons explains higher-order interactions in ensemble spiking activity. Cosyne 2013, Salt Lake City, USA.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Kster1"><label>103</label>
<mixed-citation publication-type="other" xlink:type="simple">Köster U, Sohl-Dickstein J, Gray CM &amp; Olshausen BA (2013) Higher order correlations within cortical layers dominate functional connectivity in microcolumns. <italic>arXiv.org</italic>:1301.0050.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Yu2"><label>104</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yu</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Yang</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Nakahara</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Santos</surname><given-names>GS</given-names></name>, <name name-style="western"><surname>Nikolič</surname><given-names>D</given-names></name> (<year>2011</year>) <name name-style="western"><surname>Plenz</surname><given-names>D</given-names></name> (<year>2011</year>) <article-title>Higher-order interactions characterized in cortical activity</article-title>. <source>J Neurosci</source> <volume>31</volume>: <fpage>17514</fpage>–<lpage>17526</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Amodei1"><label>105</label>
<mixed-citation publication-type="other" xlink:type="simple">Amodei D, Schwartz G &amp; Berry MJ II (2008) Correlations and the structure of the population code in a dense patch of the retina. In: Stett A ed. Proceedings MEA Meeting. BIOPRO Baden-Württemberg, 2008. pp. 197.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Dudik1"><label>106</label>
<mixed-citation publication-type="other" xlink:type="simple">Dudik M, Phillips SJ &amp; Schapire RE (2004) Performance guarantees for regularized maximum entropy density estimation. Proceedings 17th Annual conference on learning theory.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Ferrenberg1"><label>107</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ferrenberg</surname><given-names>AM</given-names></name> (<year>1988</year>) <name name-style="western"><surname>Swendsen</surname><given-names>RH</given-names></name> (<year>1988</year>) <article-title>New Monte Carlo technique for studying phase transitions</article-title>. <source>Phys Rev Lett</source> <volume>61</volume>: <fpage>2635</fpage>–<lpage>2638</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Berry2"><label>108</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name>, <name name-style="western"><surname>Tkačik</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Dubuis</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Marre</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Azerado da Silveira</surname><given-names>R</given-names></name> (<year>2013</year>) <article-title>A simple method for estimating the entropy of neural activity</article-title>. <source>J Stat Mech</source> <fpage>P03015</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Cocco1"><label>109</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cocco</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Leibler</surname><given-names>S</given-names></name> (<year>2009</year>) <name name-style="western"><surname>Monasson</surname><given-names>R</given-names></name> (<year>2009</year>) <article-title>Neuronal couplings between retinal ganglion cells inferred by efficient inverse statistical physics methods</article-title>. <source>Proc Natl Acad Sci (USA)</source> <volume>106</volume>: <fpage>14058</fpage>–<lpage>14062</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Cocco2"><label>110</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cocco</surname><given-names>S</given-names></name> (<year>2011</year>) <name name-style="western"><surname>Monasson</surname><given-names>R</given-names></name> (<year>2011</year>) <article-title>Adaptive cluster expansion for inferring Boltzmann machines with noisy data</article-title>. <source>Phys Rev Lett</source> <volume>106</volume>: <fpage>090601</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Azhar1"><label>111</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Azhar</surname><given-names>F</given-names></name> (<year>2008</year>) <article-title>An Information Theoretic Study of Neural Populations. Dissertation</article-title>. <source>University of California at Santa Barbara</source></mixed-citation>
</ref>
<ref id="pcbi.1003408-Azhar2"><label>112</label>
<mixed-citation publication-type="other" xlink:type="simple">Azhar F &amp; Bialek W (2010) When are correlations strong? <italic>arXiv.org</italic>: 1012.5987.</mixed-citation>
</ref>
<ref id="pcbi.1003408-Sessak1"><label>113</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sessak</surname><given-names>V</given-names></name> (<year>2009</year>) <name name-style="western"><surname>Monasson</surname><given-names>R</given-names></name> (<year>2009</year>) <article-title>Small-correlation expansions for the inverse Ising problem</article-title>. <source>J Phys A</source> <volume>42</volume>: <fpage>055001</fpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>