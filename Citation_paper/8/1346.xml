<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
      <journal-id journal-id-type="publisher-id">plos</journal-id>
      <journal-id journal-id-type="pmc">plosone</journal-id>
      <journal-title-group>
        <journal-title>PLoS ONE</journal-title>
      </journal-title-group>
      <issn pub-type="epub">1932-6203</issn>
      <publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">PONE-D-12-14757</article-id>
      <article-id pub-id-type="doi">10.1371/journal.pone.0050276</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Computational biology</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subj-group>
                <subject>Coding mechanisms</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Signaling networks</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Cognitive neuroscience</subject>
              <subj-group>
                <subject>Working memory</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subject>Learning and memory</subject>
              <subject>Neural networks</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Applied mathematics</subject>
            <subj-group>
              <subject>Complex systems</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Nonlinear dynamics</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Physics</subject>
          <subj-group>
            <subject>Biophysics</subject>
            <subj-group>
              <subject>Biophysics simulations</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Interdisciplinary physics</subject>
          </subj-group>
          <subj-group>
            <subject>Statistical mechanics</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Computational Biology</subject>
          <subject>Neuroscience</subject>
          <subject>Physics</subject>
          <subject>Mathematics</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Robust Short-Term Memory without Synaptic Learning</article-title>
        <alt-title alt-title-type="running-head">Robust Short-Term Memory without Synaptic Learning</alt-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Johnson</surname>
            <given-names>Samuel</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Marro</surname>
            <given-names>J.</given-names>
          </name>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Torres</surname>
            <given-names>Joaquín J.</given-names>
          </name>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
        </contrib>
      </contrib-group>
      <aff id="aff1">
        <label>1</label>
        <addr-line>Department of Mathematics, Imperial College London, London, United Kingdom</addr-line>
      </aff>
      <aff id="aff2">
        <label>2</label>
        <addr-line>Oxford Centre for Integrative Systems Biology, and Department of Physics, University of Oxford, Oxford, United Kingdom</addr-line>
      </aff>
      <aff id="aff3">
        <label>3</label>
        <addr-line>Departamento de Electromagnetismo y Física de la Materia, and Institute Carlos I for Theoretical and Computational Physics, University of Granada, Granada, Spain</addr-line>
      </aff>
      <contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Chialvo</surname>
            <given-names>Dante R.</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group>
      <aff id="edit1">
        <addr-line>National Research &amp; Technology Council, Argentina</addr-line>
      </aff>
      <author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">samuel.johnson@imperial.ac.uk</email></corresp>
        <fn fn-type="conflict">
          <p>The authors have declared that no competing interests exist.</p>
        </fn>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: SJ. Performed the experiments: SJ JJT. Analyzed the data: SJ JM JJT. Wrote the paper: SJ JM JJT.</p>
        </fn>
      </author-notes>
      <pub-date pub-type="collection">
        <year>2013</year>
      </pub-date>
      <pub-date pub-type="epub">
        <day>22</day>
        <month>1</month>
        <year>2013</year>
      </pub-date>
      <volume>8</volume>
      <issue>1</issue>
      <elocation-id>e50276</elocation-id>
      <history>
        <date date-type="received">
          <day>22</day>
          <month>5</month>
          <year>2012</year>
        </date>
        <date date-type="accepted">
          <day>23</day>
          <month>10</month>
          <year>2012</year>
        </date>
      </history>
      <permissions>
        <copyright-year>2013</copyright-year>
        <copyright-holder>Johnson et al</copyright-holder>
        <license xlink:type="simple">
          <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
        </license>
      </permissions>
      <abstract>
        <p>Short-term memory in the brain cannot in general be explained the way long-term memory can – as a gradual modification of synaptic weights – since it takes place too quickly. Theories based on some form of cellular bistability, however, do not seem able to account for the fact that noisy neurons can collectively store information in a robust manner. We show how a sufficiently clustered network of simple model neurons can be instantly induced into metastable states capable of retaining information for a short time (a few seconds). The mechanism is robust to different network topologies and kinds of neural model. This could constitute a viable means available to the brain for sensory and/or short-term memory with no need of synaptic learning. Relevant phenomena described by neurobiology and psychology, such as local synchronization of synaptic inputs and power-law statistics of forgetting avalanches, emerge naturally from this mechanism, and we suggest possible experiments to test its viability in more biological settings.</p>
      </abstract>
      <funding-group>
        <funding-statement>This work was supported by Junta de Andalucía projects FQM-01505 and P09-FQM4682, by the joint Spanish Research Ministry (MEC) and the European Budget for the Regional Development (FEDER) project FIS2009-08451, and by the Granada Research of Excellence Initiative on Bio-Health (GREIB) traslational project GREIB.PT_2011_19 of the Spanish Science and Innovation Ministry (MICINN) “Campus of International Excellence.” S.J. is grateful for financial support from the Oxford Centre for Integrative Systems Biology, and from the European Commission under the Marie Curie Intra-European Fellowship Programme PIEF-GA-2010-276454. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
      </funding-group>
      <counts>
        <page-count count="9"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <sec id="s1a">
        <title>Slow but sure, or fast and fleeting?</title>
        <p>Memory – the storage and retrieval of information by the brain – is probably nowadays one of the best understood of all the collective phenomena to emerge in that most complex of systems. Thanks to a gradual modification of synaptic weights (the interaction strengths with which neurons signal to one other) particular patterns of firing and non-firing cells become energetically favourable and so systems evolve towards these attractors according to a mechanism known as Associative Memory <xref ref-type="bibr" rid="pone.0050276-Hebb1">[1]</xref>–<xref ref-type="bibr" rid="pone.0050276-Amit1">[4]</xref>. In nature, these synaptic modifications occur via the biochemical processes of long-term potentiation (LTP) and depression (LTD) <xref ref-type="bibr" rid="pone.0050276-Gruart1">[5]</xref>, <xref ref-type="bibr" rid="pone.0050276-DeRoo1">[6]</xref>. However, some memory processes take place on timescales of seconds or less and in many instances cannot be accounted for by LTP and LTD <xref ref-type="bibr" rid="pone.0050276-Durstewitz1">[7]</xref>, since these require at least minutes to be effected <xref ref-type="bibr" rid="pone.0050276-Lee1">[8]</xref>, <xref ref-type="bibr" rid="pone.0050276-Klintsova1">[9]</xref>. For example, visual stimuli are recalled in great detail for up to about one second after exposure (iconic memory); similarly, acoustic information seems to linger for three or four seconds (echoic memory) <xref ref-type="bibr" rid="pone.0050276-Sperling1">[10]</xref>, <xref ref-type="bibr" rid="pone.0050276-Cowan1">[11]</xref>. In fact, it appears that the brain actually holds and continually updates a kind of buffer in which sensory information regarding its surroundings is maintained (sensory memory) <xref ref-type="bibr" rid="pone.0050276-Baddeley1">[12]</xref>. This is easily observed by simply closing one's eyes and recalling what was last seen, or thinking about a sound after it has finished. Another instance is the capability referred to as <italic>working</italic> memory <xref ref-type="bibr" rid="pone.0050276-Durstewitz1">[7]</xref>, <xref ref-type="bibr" rid="pone.0050276-Baddeley2">[13]</xref>: just as a computer requires RAM for its calculations despite having a hard drive for long-term storage, the brain must continually store and delete information to perform almost any cognitive task. We shall here use <italic>short-term</italic> memory to describe the brain's ability to store information on a timescale of seconds or less.</p>
        <p>Evidence that short-term memory is related to sensory information while long-term memory is more conceptual can be found in psychology. For instance, a sequence of similar sounding letters is more difficult to retain for a short time than one of phonetically distinct ones, while this has no bearing on long-term memory, for which semantics seems to play the main role <xref ref-type="bibr" rid="pone.0050276-Conrad1">[14]</xref>, <xref ref-type="bibr" rid="pone.0050276-Conrad2">[15]</xref>; and the way many of us think about certain concepts, such as chess, geometry or music, is apparently quite sensorial: we imagine positions, surfaces or notes as they would look or sound. Most theories of short-term memory – which almost always focus on working memory – make use of some form of previously stored information (i.e., of synaptic learning) and so can account for labelling tasks, such as remembering a particular series of digits or a known word, but not for the instant recall of novel information <xref ref-type="bibr" rid="pone.0050276-Barak1">[16]</xref>–<xref ref-type="bibr" rid="pone.0050276-Mongillo1">[18]</xref>. (This method can also be used to represent a continuous variable, such as the value of an angle or the length of an object, because concepts such as <italic>angle</italic> and <italic>length</italic> are in some sense already “known” at the time of the stimulus <xref ref-type="bibr" rid="pone.0050276-Wang1">[19]</xref>.) An interesting exception is the mechanism proposed by Chialvo <italic>et al.</italic> <xref ref-type="bibr" rid="pone.0050276-Chialvo1">[20]</xref> which allows for arbitrary patterns of activity to be temporarily retained thanks to the refractory times of neurons.</p>
        <p>Attempts to deal with novel information have been made by proposing mechanisms of <italic>cellular bistability</italic>: neurons are assumed to retain the state they are placed in (such as firing or not firing) for some period of time thereafter <xref ref-type="bibr" rid="pone.0050276-Camperi1">[21]</xref>–<xref ref-type="bibr" rid="pone.0050276-Tarnow1">[23]</xref>. Although there may indeed be subcellular processes leading to a certain bistability, the main problem with short-term memory depending exclusively on such a mechanism is that if each neuron must act independently of the rest the patterns will not be robust to random fluctuations <xref ref-type="bibr" rid="pone.0050276-Durstewitz1">[7]</xref> – and the behaviour of individual neurons is known to be quite noisy <xref ref-type="bibr" rid="pone.0050276-Compte1">[24]</xref>. It is worth pointing out that one of the strengths of Associative Memory is that the behaviour of a given neuron depends on many neighbours and not just on itself, which means that robust global recall can emerge despite random fluctuations at an individual level.</p>
      </sec>
      <sec id="s1b">
        <title>Harnessing network structure</title>
        <p>Something that, at least until recently, most neural-network models have failed to take into account is the structure of the network – its topology – it often being assumed that synapses are placed among the neurons completely at random, or even that all neurons are connected to all the rest. Although relatively little is yet known about the architecture of the brain at the level of neurons and synapses, experiments have shown that it is heterogeneous (some neurons have very many more synapses than others), clustered (two neurons have a higher chance of being connected if they share neighbours than if not) and highly modular (there are groups, or modules, with neurons forming synapses preferentially to those in the same module) <xref ref-type="bibr" rid="pone.0050276-Sporns1">[25]</xref>, <xref ref-type="bibr" rid="pone.0050276-Johnson1">[26]</xref>. We show here that it suffices to use a more realistic network topology, in particular one that is modular and/or clustered, for a randomly chosen pattern of activity the system is placed in to be metastable. This means that novel information can be instantly stored and retained for a short period of time in the absence of both synaptic learning and cellular bistability. The only requisite is that the patterns be coarse grained versions of the usual patterns – that is, whereas it is often assumed that each neuron in some way represents one bit of information, we shall allocate a bit to a small group or neurons. (This does not, of course, mean that memories are expected to be encoded as bitmaps. In fact, we are not making any assumptions regarding neural coding.)</p>
        <p>The mechanism, which we call Cluster Reverberation (CR), is very simple. If neurons in a group are more densely connected to each other than to the rest of the network, either because they form a module or because the network is significantly clustered, they will tend to retain the activity of the group: when they are all initially firing, they each continue to receive many action potentials and so go on firing, whereas if they start off silent, there is not usually enough input current from the outside to set them off. (This is similar to the ‘re-entrant’ activity exhibited by excitable elements <xref ref-type="bibr" rid="pone.0050276-Lewis1">[27]</xref>.) The fact that each neuron's state depends on its neighbours confers to the mechanism a certain robustness to random fluctuations. This robustness is particularly important for biological neurons, which as mentioned are quite noisy. Furthermore, not only does the limited duration of short-term memory states emerge naturally from this mechanism (even in the absence of interference from new stimuli) but this natural forgetting follows power-law statistics, as has been observed experimentally <xref ref-type="bibr" rid="pone.0050276-Wixted1">[28]</xref>–<xref ref-type="bibr" rid="pone.0050276-Sikstrm1">[30]</xref>. It is also coherent with recent observations of locally synchronized neural activity <italic>in vivo</italic> <xref ref-type="bibr" rid="pone.0050276-Takahashi1">[31]</xref>, and of clustering in both synaptic inputs <xref ref-type="bibr" rid="pone.0050276-Kleindienst1">[32]</xref> and plasticity <xref ref-type="bibr" rid="pone.0050276-Makino1">[33]</xref> during development. The viability of this mechanism in a more realistic setting could perhaps be put to the test by growing modular and/or clustered networks <italic>in vitro</italic> and carrying out similar experiments as we do here in simulation <xref ref-type="bibr" rid="pone.0050276-Kohl1">[34]</xref>, <xref ref-type="bibr" rid="pone.0050276-SheinIdelson1">[35]</xref> (see Discussion).</p>
      </sec>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <sec id="s2a">
        <title>The simplest neurons on modular networks</title>
        <p>Consider a network of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e001" xlink:type="simple"/></inline-formula> model neurons, with activities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e002" xlink:type="simple"/></inline-formula>. The topology is given by the adjacency matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e003" xlink:type="simple"/></inline-formula>, each element representing the existence or absence of a synapse from neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e004" xlink:type="simple"/></inline-formula> to neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e005" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e006" xlink:type="simple"/></inline-formula> need not be symmetric). In this kind of model – a network of what are often referred to as Amari-Hopfield neurons – each edge usually has a <italic>synaptic weight</italic> associated, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e007" xlink:type="simple"/></inline-formula>, which serves to store information <xref ref-type="bibr" rid="pone.0050276-Hebb1">[1]</xref>–<xref ref-type="bibr" rid="pone.0050276-Amit1">[4]</xref>. However, since our objective is to show how this can be achieved without synaptic learning, we shall here consider these to have all the same value: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e008" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e009" xlink:type="simple"/></inline-formula>. Neurons are updated in parallel (Little dynamics) at each time step, according to the stochastic transition rule<disp-formula id="pone.0050276.e010"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0050276.e010" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e011" xlink:type="simple"/></inline-formula> is the <italic>field</italic> at neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e012" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e013" xlink:type="simple"/></inline-formula> is a stochasticity parameter called <italic>temperature</italic>. This dynamics can be derived by considering coupled binary elements in a thermal bath, the transition rule stemming from energy differences between states <xref ref-type="bibr" rid="pone.0050276-Hopfield1">[3]</xref>, <xref ref-type="bibr" rid="pone.0050276-Amit1">[4]</xref>, <xref ref-type="bibr" rid="pone.0050276-Fraiman1">[36]</xref>.</p>
        <p>We shall consider the network defined by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e014" xlink:type="simple"/></inline-formula> to be made up of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e015" xlink:type="simple"/></inline-formula> distinct modules. To achieve this, we can first construct <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e016" xlink:type="simple"/></inline-formula> separate random directed networks, each with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e017" xlink:type="simple"/></inline-formula> nodes and mean degree (mean number of neighbours) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e018" xlink:type="simple"/></inline-formula>. Then we evaluate each edge <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e019" xlink:type="simple"/></inline-formula> and, with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e020" xlink:type="simple"/></inline-formula>, eliminate it (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e021" xlink:type="simple"/></inline-formula>), to be substituted for another edge between the original (postsynaptic) neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e022" xlink:type="simple"/></inline-formula> and a new (presynaptic) neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e023" xlink:type="simple"/></inline-formula> chosen at random from among any of those in other modules (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e024" xlink:type="simple"/></inline-formula>). We do not allow self-edges (although they can occur in reality) since these could be regarded as equivalent to a form of cellular bistability. Note that this protocol does not alter the number of presynaptic neighbours of each node, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e025" xlink:type="simple"/></inline-formula>, although the number of postsynaptic neurons, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e026" xlink:type="simple"/></inline-formula>, can vary. The parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e027" xlink:type="simple"/></inline-formula> can be seen as a measure of <italic>modularity</italic> of the partition considered, since it coincides with the expected value of the proportion of edges that link different modules <xref ref-type="bibr" rid="pone.0050276-Newman1">[37]</xref>. In particular, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e028" xlink:type="simple"/></inline-formula> defines a network of disconnected modules, while <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e029" xlink:type="simple"/></inline-formula> yields a random network in which this partition has no modularity. If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e030" xlink:type="simple"/></inline-formula>, the partition is less than randomly modular – i.e., it is <italic>quasi-multipartite</italic> (or multipartite if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e031" xlink:type="simple"/></inline-formula>).</p>
      </sec>
      <sec id="s2b">
        <title>Cluster reverberation</title>
        <p>A memory pattern, in the form of a given configuration of activities, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e032" xlink:type="simple"/></inline-formula>, can be stored in this system with no need of prior learning. (The system will recall the pattern perfectly when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e033" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e034" xlink:type="simple"/></inline-formula>.) Imagine a pattern such that the activities of all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e035" xlink:type="simple"/></inline-formula> neurons found in any module are the same – i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e036" xlink:type="simple"/></inline-formula>, where the index <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e037" xlink:type="simple"/></inline-formula> denotes the module that neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e038" xlink:type="simple"/></inline-formula> belongs to. The system can be induced into this configuration through the application of an appropriate stimulus: the field of each neuron will be altered for just one time step according to<disp-formula id="pone.0050276.e039"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0050276.e039" xlink:type="simple"/></disp-formula>where the factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e040" xlink:type="simple"/></inline-formula> is the intensity of the stimulus (see <xref ref-type="fig" rid="pone-0050276-g001">Fig. 1</xref>). This mechanism for dynamically storing information will work for values of parameters such that the system is sensitive to the stimulus, acquiring the desired configuration, yet also able to retain it for some interval of time thereafter (a similar setting is considered, for instance, in Ref. <xref ref-type="bibr" rid="pone.0050276-Johnson2">[38]</xref>).</p>
        <fig id="pone-0050276-g001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0050276.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>Diagram of a modular network composed of four five-neuron clusters.</title>
            <p>The four circles enclosed by the dashed line represent the stimulus: each is connected to a particular module, which adopts the input state (red or blue) and retains it after the stimulus has disappeared thanks to Cluster Reverberation.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0050276.g001" position="float" xlink:type="simple"/>
        </fig>
        <p>The two configurations of minimum energy of the system are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e041" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e042" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e043" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e044" xlink:type="simple"/></inline-formula> (see the next section for a more detailed discussion on energy). However, the energy is locally minimized for any configuration in which each module comprises either all active or all inactive neurons (that is, for configurations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e045" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e046" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e047" xlink:type="simple"/></inline-formula> a binary variable specific to the whole module <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e048" xlink:type="simple"/></inline-formula> that neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e049" xlink:type="simple"/></inline-formula> belongs to). These are the configurations that we shall use to store information. We define the mean activity of each module, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e050" xlink:type="simple"/></inline-formula>, which is a mesoscopic variable, as well as the global mean activity, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e051" xlink:type="simple"/></inline-formula> (these magnitudes change with time, but, where possible, we shall avoid writing the time dependence explicitly for clarity; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e052" xlink:type="simple"/></inline-formula> stands for an average over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e053" xlink:type="simple"/></inline-formula>). The mean activity in a neural network model is usually taken to represent the mean firing rate measured in experiments <xref ref-type="bibr" rid="pone.0050276-Torres1">[39]</xref>. The extent to which the network, at a given time, retains the pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e054" xlink:type="simple"/></inline-formula> with which it was stimulated is measured with the <italic>overlap</italic> parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e055" xlink:type="simple"/></inline-formula>. Ideally, the system should be capable of reacting immediately to a stimulus by adopting the right configuration, yet also be able to retain it for long enough to use the information once the stimulus has disappeared. A measure of performance for such a task is therefore<disp-formula id="pone.0050276.e056"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0050276.e056" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e057" xlink:type="simple"/></inline-formula> is the time at which the stimulus is received and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e058" xlink:type="simple"/></inline-formula> is the period of time we are interested in (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e059" xlink:type="simple"/></inline-formula>) <xref ref-type="bibr" rid="pone.0050276-Johnson2">[38]</xref>. If the intensity of the stimulus, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e060" xlink:type="simple"/></inline-formula>, is very large, then the system will always adopt the right pattern perfectly and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e061" xlink:type="simple"/></inline-formula> will only depend on how well it can then be retained. In this case, the best network will be one that is made up of mutually disconnected modules (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e062" xlink:type="simple"/></inline-formula>). However, since the stimulus in a real brain can be expected to arrive via a relatively small number of axons, either from another part of the brain or directly from sensory cells, it might be more realistic to assume that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e063" xlink:type="simple"/></inline-formula> is of a similar order as the input a typical neuron receives from its neighbours, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e064" xlink:type="simple"/></inline-formula>.</p>
        <p><xref ref-type="fig" rid="pone-0050276-g002">Figure 2</xref> shows the mean performance obtained in Monte Carlo (MC) simulations when the network is repeatedly stimulated with different randomly generated patterns. For low enough values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e065" xlink:type="simple"/></inline-formula> and stimuli of intensity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e066" xlink:type="simple"/></inline-formula>, the system can capture and successfully retain any pattern it is “shown” for some period of time, even though this pattern was in no way previously learned. For less intense stimuli (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e067" xlink:type="simple"/></inline-formula>), performance is nonmonotonic with modularity: there exists an optimal value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e068" xlink:type="simple"/></inline-formula> at which the system is sensitive to stimuli yet still able to retain new patterns quite well.</p>
        <fig id="pone-0050276-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0050276.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Performance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e069" xlink:type="simple"/></inline-formula> against <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e070" xlink:type="simple"/></inline-formula> for networks of the sort described in the main text with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e071" xlink:type="simple"/></inline-formula> modules of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e072" xlink:type="simple"/></inline-formula> neurons each, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e073" xlink:type="simple"/></inline-formula>, obtained from Monte Carlo (MC) simulations; patterns are shown with intensities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e074" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e075" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e076" xlink:type="simple"/></inline-formula>, and performance is computed evey <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e077" xlink:type="simple"/></inline-formula> time steps, preceding the next random stimulus; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e078" xlink:type="simple"/></inline-formula> (error bars represent standard deviations; lines – splines – are drawn as a guide to the eye).</title>
            <p>Inset: typical time series of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e079" xlink:type="simple"/></inline-formula> (i.e., the overlap with whichever pattern was last shown) for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e080" xlink:type="simple"/></inline-formula> (bad performance), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e081" xlink:type="simple"/></inline-formula> (intermediate), and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e082" xlink:type="simple"/></inline-formula> (optimal); with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e083" xlink:type="simple"/></inline-formula>.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0050276.g002" position="float" xlink:type="simple"/>
        </fig>
        <p>Just as some degree of structural (quenched) noise, given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e084" xlink:type="simple"/></inline-formula>, can improve performance by increasing sensitivity, so too the dynamical (annealed) noise set by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e085" xlink:type="simple"/></inline-formula> can have a similar effect. This apparent stochastic resonance is looked into below in Analysis.</p>
      </sec>
      <sec id="s2c">
        <title>Energy and topology</title>
        <p>Each pair of neurons contributes a configurational energy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e086" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pone.0050276-Amit1">[4]</xref>; that is, if there is an edge from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e087" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e088" xlink:type="simple"/></inline-formula> and they have opposite activities, the energy is increased in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e089" xlink:type="simple"/></inline-formula>, whereas it is decreased by the same amount if their activities are equal. Given a configuration, we can obtain its associated energy by summing over all pairs. To study how the system relaxes from the metastable states (i.e., how it “forgets” the information stored) we shall be interested in configurations with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e090" xlink:type="simple"/></inline-formula> neurons that have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e091" xlink:type="simple"/></inline-formula> (and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e092" xlink:type="simple"/></inline-formula> neurons with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e093" xlink:type="simple"/></inline-formula>), chosen in such a way that one module at most, say <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e094" xlink:type="simple"/></inline-formula>, has neurons in both states simultaneously. Therefore, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e095" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e096" xlink:type="simple"/></inline-formula> is the number of modules with all their neurons in the positive state and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e097" xlink:type="simple"/></inline-formula> is the number of neurons with positive sign in module <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e098" xlink:type="simple"/></inline-formula>. We can write <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e099" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e100" xlink:type="simple"/></inline-formula>. The total configurational energy of the system will be<disp-formula id="pone.0050276.e101"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0050276.e101" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e102" xlink:type="simple"/></inline-formula> is the number of edges linking nodes with opposite activities. By simply counting over expected numbers of edges, we can obtain the expected value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e103" xlink:type="simple"/></inline-formula> (which amounts to a mean-field approximation), yielding:<disp-formula id="pone.0050276.e104"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0050276.e104" xlink:type="simple"/><label>(2)</label></disp-formula><disp-formula id="pone.0050276.e105"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0050276.e105" xlink:type="simple"/></disp-formula><xref ref-type="fig" rid="pone-0050276-g003">Figure 3</xref> shows the mean-field configurational energy curves for various values of the modularity on a small modular network. The local minima (metastable states) are the configurations used to store patterns. It should be noted that the mapping <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e106" xlink:type="simple"/></inline-formula> is highly degenerate: there are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e107" xlink:type="simple"/></inline-formula> patterns with mean activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e108" xlink:type="simple"/></inline-formula> that all have the same energy.</p>
        <fig id="pone-0050276-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0050276.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Configurational energy of a network made up of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e109" xlink:type="simple"/></inline-formula> modules of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e110" xlink:type="simple"/></inline-formula> neurons each, according to <xref ref-type="disp-formula" rid="pone.0050276.e104">Eq. (2)</xref>, for various values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e111" xlink:type="simple"/></inline-formula> (increasing from bottom to top).</title>
            <p>The minima correspond to situations such that all neurons within any given module have the same sign.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0050276.g003" position="float" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s2d">
        <title>Forgetting avalanches</title>
        <p>In obtaining the energy we have assumed that the number of synapses rewired from a given module is always equal to its expected value: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e112" xlink:type="simple"/></inline-formula>. However, since each edge is evaluated with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e113" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e114" xlink:type="simple"/></inline-formula> will in fact vary somewhat from one module to another, being approximately Poisson distributed with mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e115" xlink:type="simple"/></inline-formula>. Neglecting all but the last term in <xref ref-type="disp-formula" rid="pone.0050276.e104">Eq. (2)</xref> and approximating <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e116" xlink:type="simple"/></inline-formula>, the depth of the energy well corresponding to a given module is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e117" xlink:type="simple"/></inline-formula>. The typical escape time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e118" xlink:type="simple"/></inline-formula> from an energy well of depth <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e119" xlink:type="simple"/></inline-formula> at temperature <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e120" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e121" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pone.0050276-Levine1">[40]</xref>. Using Stirling's approximation [<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e122" xlink:type="simple"/></inline-formula>] in the Poisson distribution over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e123" xlink:type="simple"/></inline-formula> and expressing it in terms of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e124" xlink:type="simple"/></inline-formula>, we find that the escape times are distributed according to<disp-formula id="pone.0050276.e125"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0050276.e125" xlink:type="simple"/><label>(3)</label></disp-formula>where<disp-formula id="pone.0050276.e126"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0050276.e126" xlink:type="simple"/><label>(4)</label></disp-formula>Therefore, at low temperatures, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e127" xlink:type="simple"/></inline-formula> will behave approximately like a power law. Note also that the size of the network, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e128" xlink:type="simple"/></inline-formula>, does not appear in <xref ref-type="disp-formula" rid="pone.0050276.e125">Eqs. (3)</xref> and <xref ref-type="disp-formula" rid="pone.0050276.e126">(4)</xref>. Rather, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e129" xlink:type="simple"/></inline-formula> scales with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e130" xlink:type="simple"/></inline-formula>, which could be small even in the thermodynamic limit (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e131" xlink:type="simple"/></inline-formula>).</p>
        <p>The left panel of <xref ref-type="fig" rid="pone-0050276-g004">Fig. 4</xref> shows the distribution of time intervals between events in which the overlap <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e132" xlink:type="simple"/></inline-formula> of at least one module <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e133" xlink:type="simple"/></inline-formula> changes sign. The power-law-like behaviour is apparent, and justifies talking about <italic>forgetting avalanches</italic> – since there are cascades of many forgetting events interspersed with long periods of metastability. This is very similar to the behaviour observed in other nonequilibrium settings in which power-law statistics arise from the convolution of exponentials, such as demagnetization processes <xref ref-type="bibr" rid="pone.0050276-Hurtado1">[41]</xref> or Griffiths phases on networks <xref ref-type="bibr" rid="pone.0050276-Muoz1">[42]</xref>.</p>
        <fig id="pone-0050276-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0050276.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Left panel: distribution of escape times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e134" xlink:type="simple"/></inline-formula>, as defined in the main text, for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e135" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e136" xlink:type="simple"/></inline-formula>, from MC simulations.</title>
            <p>Slope is for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e137" xlink:type="simple"/></inline-formula>. Other parameters as in <xref ref-type="fig" rid="pone-0050276-g002">Fig. 2</xref>. Right panel: exponent <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e138" xlink:type="simple"/></inline-formula> of the quasi-power-law distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e139" xlink:type="simple"/></inline-formula> as given by <xref ref-type="disp-formula" rid="pone.0050276.e126">Eq. (4)</xref> for temperatures <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e140" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e141" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e142" xlink:type="simple"/></inline-formula> (from bottom to top).</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0050276.g004" position="float" xlink:type="simple"/>
        </fig>
        <p>It is known from experimental psychology that forgetting in humans is indeed quite well described by power laws <xref ref-type="bibr" rid="pone.0050276-Wixted1">[28]</xref>–<xref ref-type="bibr" rid="pone.0050276-Sikstrm1">[30]</xref> – although most experiments to date seem to refer to slightly longer timescales than we are interested in here. The right panel of <xref ref-type="fig" rid="pone-0050276-g004">Fig. 4</xref> shows the value of the exponent <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e143" xlink:type="simple"/></inline-formula> as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e144" xlink:type="simple"/></inline-formula>. Although for low temperatures it is almost constant over many decades of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e145" xlink:type="simple"/></inline-formula> – approximating a pure power law – for any finite <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e146" xlink:type="simple"/></inline-formula> there will always be a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e147" xlink:type="simple"/></inline-formula> such that the denominator in the logarithm of <xref ref-type="disp-formula" rid="pone.0050276.e126">Eq. (4)</xref> approaches zero and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e148" xlink:type="simple"/></inline-formula> diverges, signifying a truncation of the distribution.</p>
        <p>Note that we have considered the information stored in a pattern to be lost once the system evolves to any other energy minimum. However, this new pattern will be highly correlated with the original one, and it might be reasonable to assume that the system has to escape from a large number of energy minima, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e149" xlink:type="simple"/></inline-formula>, before the information can be considered to have been entirely forgotten. The time for this is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e150" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e151" xlink:type="simple"/></inline-formula> are independently drawn from Eq (3). If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e152" xlink:type="simple"/></inline-formula> is sufficiently large, the distribution of times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e153" xlink:type="simple"/></inline-formula> will tend to a Lévy distribution <xref ref-type="bibr" rid="pone.0050276-Gnedenko1">[43]</xref>. In practice, these different broad-tailed distributions [power-law, Lévy, or as given by Eq. (3)] are likely to be indistinguishable experimentally unless it is possible to observe over many orders of magnitude.</p>
      </sec>
      <sec id="s2e">
        <title>Clustered networks</title>
        <p>Although we have illustrated how the mechanism of Cluster Reverberation works on a modular network, it is not actually necessary for the topology to have this characteristic – only for the patterns to be in some way “coarse-grained, ” as described, and that each region of the network encoding one bit have a small enough parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e154" xlink:type="simple"/></inline-formula>, defined as the proportion of synapses to other regions. For instance, for the famous Watts-Strogatz <italic>small-world</italic> model <xref ref-type="bibr" rid="pone.0050276-Watts1">[44]</xref> – a ring of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e155" xlink:type="simple"/></inline-formula> nodes, each initially connected to its <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e156" xlink:type="simple"/></inline-formula> nearest neighbours before a proportion <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e157" xlink:type="simple"/></inline-formula> of the edges are randomly rewired – we have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e158" xlink:type="simple"/></inline-formula> (which is not surprising considering the resemblance between this model and the modular network used above). More precisely, the expected modularity of a randomly imposed box of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e159" xlink:type="simple"/></inline-formula> neurons is<disp-formula id="pone.0050276.e160"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0050276.e160" xlink:type="simple"/></disp-formula>the second term on the right accounting for the edges rewired to the same box, and the third to the edges not rewired but sufficiently close to the border to connect with a different box.</p>
        <p>Perhaps a more realistic model of clustered network would be a random network embedded in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e161" xlink:type="simple"/></inline-formula>-dimensional Euclidean space. For this we shall use the scheme laid out by Rozenfeld <italic>et al.</italic> <xref ref-type="bibr" rid="pone.0050276-Rozenfeld1">[45]</xref>, which consists simply in allocating each node to a site on a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e162" xlink:type="simple"/></inline-formula>-torus and then, given a particular degree sequence, placing edges to the nearest nodes possible – thereby attempting to minimize total edge length. For a scale-free degree sequence [i.e., a set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e163" xlink:type="simple"/></inline-formula> drawn from a degree distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e164" xlink:type="simple"/></inline-formula>] according to some exponent <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e165" xlink:type="simple"/></inline-formula>, then, as shown in Analysis, such a network has a modularity<disp-formula id="pone.0050276.e166"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0050276.e166" xlink:type="simple"/><label>(5)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e167" xlink:type="simple"/></inline-formula> is the linear size of the boxes considered. It is interesting that even in this scenario, where the boxes of neurons which are to receive the same stimulus are chosen at random with no consideration for the underlying topology, these boxes need not have very many neurons for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e168" xlink:type="simple"/></inline-formula> to be quite low (as long as the degree distribution is not too heterogeneous).</p>
        <p>Carrying out the same repeated stimulation test as on the modular networks in <xref ref-type="fig" rid="pone-0050276-g002">Fig. 2</xref>, we find a similar behaviour for the scale-free embedded networks. This is shown in <xref ref-type="fig" rid="pone-0050276-g005">Fig. 5</xref>, where for high enough intensity of stimuli <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e169" xlink:type="simple"/></inline-formula> and scale-free exponent <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e170" xlink:type="simple"/></inline-formula>, performance can, as in the modular case, be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e171" xlink:type="simple"/></inline-formula>. We should point out that for good performance on these networks we require more neurons for each bit of information than on modular networks with the same <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e172" xlink:type="simple"/></inline-formula> (in <xref ref-type="fig" rid="pone-0050276-g005">Fig. 5</xref> we use <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e173" xlink:type="simple"/></inline-formula>, as opposed to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e174" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pone-0050276-g002">Fig. 2</xref>). However, that we should be able to obtain good results for such diverse network topologies underlines that the mechanism of Cluster Reverberation is robust and not dependent on some very specific architecture.</p>
        <fig id="pone-0050276-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0050276.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Performance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e175" xlink:type="simple"/></inline-formula> against exponent <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e176" xlink:type="simple"/></inline-formula> for scale-free networks, embedded on a 2D lattice, with patterns of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e177" xlink:type="simple"/></inline-formula> modules of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e178" xlink:type="simple"/></inline-formula> neurons each, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e179" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e180" xlink:type="simple"/></inline-formula>; patterns are shown with intensities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e181" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e182" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e183" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e184" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e185" xlink:type="simple"/></inline-formula> (error bars represent standard deviations; lines – splines – are drawn as a guide to the eye).</title>
            <p>Inset: typical time series for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e186" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e187" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e188" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e189" xlink:type="simple"/></inline-formula>.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0050276.g005" position="float" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s2f">
        <title>Spiking neurons</title>
        <p>In the usual spirit of determining the minimal ingredients for a mechanism to function we have, up until now, used the simplest model neurons able to exhibit CR. This approach makes for a good illustration of the main idea and allows for a certain amount of analytical understanding of the underlying phenomena. However, before CR can be considered as a plausible candidate for helping to explain short-term memory, we must check that it is compatible with more realistic neural models. For this we examine the behaviour of the popular Integrate-and-Fire (IF) model neurons – often referred to as <italic>spiking neurons</italic> – in the same kind of setting as described above for the simpler Amari-Hopfield neurons. In the IF model, each neuron is characterized at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e190" xlink:type="simple"/></inline-formula> by a <italic>membrane potential</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e191" xlink:type="simple"/></inline-formula>, described by the differential equation<disp-formula id="pone.0050276.e192"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0050276.e192" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e193" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e194" xlink:type="simple"/></inline-formula> are, respectively, the membrane time constant and resistance, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e195" xlink:type="simple"/></inline-formula>; the term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e196" xlink:type="simple"/></inline-formula> is the synaptic current generated by the arrival of Action Potentials (AP) from the neuron's presynaptic neighbours, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e197" xlink:type="simple"/></inline-formula> is the current generated by the presentation of a particular external stimulus to the network and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e198" xlink:type="simple"/></inline-formula> is an additional noisy external current. Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e199" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e200" xlink:type="simple"/></inline-formula> are constants and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e201" xlink:type="simple"/></inline-formula> is a Gaussian noise of mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e202" xlink:type="simple"/></inline-formula> and autocorrelation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e203" xlink:type="simple"/></inline-formula>. Each synaptic contribution to the total synaptic current is modelled as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e204" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e205" xlink:type="simple"/></inline-formula> represents the fraction of neurotransmitters in the synaptic cleft, which follows the dynamics <xref ref-type="bibr" rid="pone.0050276-Tsodyks1">[46]</xref><disp-formula id="pone.0050276.e206"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0050276.e206" xlink:type="simple"/></disp-formula>Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e207" xlink:type="simple"/></inline-formula> is the time at which an AP arrives at synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e208" xlink:type="simple"/></inline-formula>, inducing the release of a fraction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e209" xlink:type="simple"/></inline-formula> of neurotransmitters, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e210" xlink:type="simple"/></inline-formula> is the typical time-scale for neurotransmitter inactivation. Whenever <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e211" xlink:type="simple"/></inline-formula> surpasses a given threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e212" xlink:type="simple"/></inline-formula>, the neuron fires an AP to all its postsynaptic neighbours and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e213" xlink:type="simple"/></inline-formula> is reset to zero, then undergoing a refractory time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e214" xlink:type="simple"/></inline-formula> before again becoming susceptible to input. Because the parameters and variables of this model represent measurable physiological quantities, it is possible to use it to make quantitative – albeit tentative – predictions about the timescales on which CR might be expected to be effective in a real neural system.</p>
        <p><xref ref-type="fig" rid="pone-0050276-g006">Figure 6</xref> is a raster plot of a modular network of IF neurons. The system performs a short-term memory task akin to the one previously described for the Amari-Hopfield neural network: the neurons belonging to clusters that correspond to ones in a random pattern are stimulated, for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e215" xlink:type="simple"/></inline-formula> ms, with an intensity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e216" xlink:type="simple"/></inline-formula>, while the the remaining neurons receive an opposite stimulus, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e217" xlink:type="simple"/></inline-formula>. We then allow the system to evolve for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e218" xlink:type="simple"/></inline-formula> ms, before choosing a new random pattern and stimulating again. In such tests, the neurons in positively stimulated clusters usually begin to oscillate in synchrony, while the rest remain silent (save for occasional individual APs caused by noise). However, since this is a metastable state, with time active clusters can suddenly go mostly silent, or the neurons in silent clusters begin spontaneously to fire in synchrony. Thus, the information is gradually lost, as in the case with simpler neurons.</p>
        <fig id="pone-0050276-g006" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0050276.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>Raster plot, obtained from MC simulations, of a network of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e219" xlink:type="simple"/></inline-formula> integrate-and-fire (IF) neurons wired up (as described in the main text) in groups of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e220" xlink:type="simple"/></inline-formula>, with a rewiring probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e221" xlink:type="simple"/></inline-formula>.</title>
            <p>Every <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e222" xlink:type="simple"/></inline-formula> ms, a new pattern is shown for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e223" xlink:type="simple"/></inline-formula> ms with an intensity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e224" xlink:type="simple"/></inline-formula> pA (plotted in blue). Parameters for the neurons are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e225" xlink:type="simple"/></inline-formula> pA, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e226" xlink:type="simple"/></inline-formula> mV, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e227" xlink:type="simple"/></inline-formula> ms, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e228" xlink:type="simple"/></inline-formula> ms, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e229" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e230" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e231" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e232" xlink:type="simple"/></inline-formula> ms, which are all within the physiological range; and the external noisy current is modelled with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e233" xlink:type="simple"/></inline-formula> pA and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e234" xlink:type="simple"/></inline-formula> pA ms<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e235" xlink:type="simple"/></inline-formula>.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0050276.g006" position="float" xlink:type="simple"/>
        </fig>
        <p>To gauge how well the system is performing the task, we look at each cluster <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e236" xlink:type="simple"/></inline-formula> for the last <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e237" xlink:type="simple"/></inline-formula> ms before the next stimulus and assign a value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e238" xlink:type="simple"/></inline-formula> to its mean activity if it is active, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e239" xlink:type="simple"/></inline-formula> if it is silent. We then define the performance as:<disp-formula id="pone.0050276.e240"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0050276.e240" xlink:type="simple"/><label>(6)</label></disp-formula>In <xref ref-type="fig" rid="pone-0050276-g007">Fig. 7</xref> we show the values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e241" xlink:type="simple"/></inline-formula> obtained in MC simulations against <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e242" xlink:type="simple"/></inline-formula>. Using different values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e243" xlink:type="simple"/></inline-formula> we observe a similar behaviour to that of <xref ref-type="fig" rid="pone-0050276-g002">Fig. 2</xref>. In particular, for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e244" xlink:type="simple"/></inline-formula> pA, we have the interesting nonmonotonic behaviour in which performance benefits from a certain degree of rewiring. While, for the sake of illustration, in <xref ref-type="fig" rid="pone-0050276-g006">Fig. 6</xref> we only show the evolution of the system for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e245" xlink:type="simple"/></inline-formula> ms after stimulation, in <xref ref-type="fig" rid="pone-0050276-g007">Fig. 7</xref> we wait for five seconds. Although the model is too simple, and the network too small, to make quantitative predictions about the brain, it is nevertheless promising that with physiologically realistic parameters we observe high performance (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e246" xlink:type="simple"/></inline-formula>) over several seconds, since this is the timescale on which short-term memory operates in humans.</p>
        <fig id="pone-0050276-g007" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0050276.g007</object-id>
          <label>Figure 7</label>
          <caption>
            <title>Performance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e247" xlink:type="simple"/></inline-formula> against rewiring <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e248" xlink:type="simple"/></inline-formula> for modular networks of IF neurons, as obtained from MC simulations.</title>
            <p>The network is periodically stimulated with a new random pattern for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e249" xlink:type="simple"/></inline-formula> ms with an intensity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e250" xlink:type="simple"/></inline-formula> pA (green squares), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e251" xlink:type="simple"/></inline-formula> pA (red circles) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e252" xlink:type="simple"/></inline-formula> pA (blue triangles) (error bars represent standard deviations; lines – splines – are drawn as a guide to the eye). The system evolves in the absence of stimuli for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e253" xlink:type="simple"/></inline-formula> ms and performance, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e254" xlink:type="simple"/></inline-formula>, is computed according to <xref ref-type="disp-formula" rid="pone.0050276.e240">Eq. (6)</xref>. (An interval of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e255" xlink:type="simple"/></inline-formula> seconds corresponds roughly to the timescale on which short-term memory operates in the brain.) Other parameters are as in <xref ref-type="fig" rid="pone-0050276-g006">Fig. 6</xref>.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0050276.g007" position="float" xlink:type="simple"/>
        </fig>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>Cluster Reverberation may be a means available to neural systems for performing certain short-term tasks, such as sensory memory or working memory. To the best of our knowledge, it is the first mechanism proposed to use network properties with no need of synaptic learning. All that is required is for the underlying network to be highly clustered or modular, and for small groups of neurons in some sense to store one bit of information, as opposed to a conventional view which assumes one bit per neuron. Considering the enormous number of neurons in the brain, and the fact that real neurons are possibly too noisy to store information individually anyway, these hypotheses do not seem far-fetched. The mechanism is furthermore consistent with what is known about the structure of biological neural networks, with experiments that have revealed power-law statistics of forgetting, and with recent observations of locally synchronized synaptic activity.</p>
      <p>For the sake of illustration, we have focused here on the simplest model neurons that are able to exhibit the behaviour of interest. However, we have shown how the mechanism can also work with the slightly more sophisticated Integrate-and-Fire neurons, and there is no reason to believe that it would not also be viable with more realistic models, or even actual cells. Although CR comes about thanks to the high modularity of small groups of neurons, we have shown how robust it is to the details of the topology by carrying out simulations on clustered networks with no explicitly built-in modularity. And this setting suggests an interesting point. If an initially homogeneous (i.e., neither modular nor clustered) area of brain tissue were repeatedly stimulated with different patterns in the same way as we have done in our simulations, then synaptic plasticity mechanisms (LTP and LTD) might be expected to alter the network structure in such a way that synapses within each of the imposed modules would all tend to become strengthened, while inter-module synapses would vary their weights in accordance with the details of the patterns being shown <xref ref-type="bibr" rid="pone.0050276-vanAerde1">[47]</xref>. The result would be a modular structure conducive to efficient CR for arbitrary patterns, with simultaneous Hebbian learning in the inter-synapses of the actual patterns shown. In this way, the same network might be capable of both short-term and long-term memory, explaining, perhaps, why our brains can indeed store completely novel information but usually with a certain bias in favour of what we are expecting to perceive.</p>
      <p>Although we have not gone into the question of neural coding, there would seem to be an intrinsic difference between <italic>semantic</italic> storage of information – used for long-term memory and probably useful for certain working-memory tasks that require the labelling of previously learned information – and <italic>sensory</italic> storage, for which some mechanism such as the one proposed here must store novel information immediately – in a similar but more efficient way to how the retina retains the pigmentation left by an image it was recently exposed to. If novel sensory information were held for long enough in metastable states, Hebbian learning (either in the same or other areas of the brain) could take place and the information be stored thereafter indefinitely. This might constitute the essence of concentrating so as to memorise a recent stimulus.</p>
      <p>Finally, we should mention that CR could work in conjunction with other mechanisms, such as processes leading to cellular bistability, making these more robust to noise and augmenting their efficacy. Whether CR would work for biological neural systems could perhaps be put to the test by growing such modular networks <italic>in vitro</italic>, stimulating appropriately, and observing the duration of the metastable states <xref ref-type="bibr" rid="pone.0050276-Kohl1">[34]</xref>, <xref ref-type="bibr" rid="pone.0050276-SheinIdelson1">[35]</xref>. <italic>In vivo</italic> recordings of neural activity during short-term memory tasks, together with a mapping of the underlying synaptic connections, might be used to ascertain whether the brain could indeed harness this mechanism. For this it must be borne in mind that the neurons forming a module need not find themselves close together in metric space, and that effective modularity might come about via stronger intra- than inter-connexions, instead of simply through a higher density of synapses within the clusters. We hope that observations and experiments such as these will be carried out and eventually reveal something more about the basis of this puzzling emergent property of the brain's known as thought.</p>
    </sec>
    <sec id="s4">
      <title>Analysis</title>
      <sec id="s4a">
        <title>The effect of noise</title>
        <p>On a random network (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e256" xlink:type="simple"/></inline-formula>), the Amari-Hopfield model described in the main text has a second order phase transition with temperature, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e257" xlink:type="simple"/></inline-formula>, at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e258" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pone.0050276-Amit1">[4]</xref>. This can be seen by considering the mean-field equation for the overlap at the steady state, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e259" xlink:type="simple"/></inline-formula>, where we have substituted <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e260" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pone.0050276.e010">Eq. (1)</xref>. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e261" xlink:type="simple"/></inline-formula>, the paramagnetic solution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e262" xlink:type="simple"/></inline-formula> becomes unstable, and ferromagnetic solutions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e263" xlink:type="simple"/></inline-formula> appear <xref ref-type="bibr" rid="pone.0050276-Fraiman1">[36]</xref>. This result also holds for the modular networks described in the main text. However, that the global overlap <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e264" xlink:type="simple"/></inline-formula> is different form zero does not mean that the short-term memory configurations we are interested in are stable. In fact, we know they are metastable for any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e265" xlink:type="simple"/></inline-formula> (see Results: Energy and topology), but we can set an upper bound on the temperature at which these states can be maintained even for a short time by considering again the mean-field equation for such a configuration. For a neuron in module <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e266" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e267" xlink:type="simple"/></inline-formula>. For patterns with mean activity zero (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e268" xlink:type="simple"/></inline-formula>), states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e269" xlink:type="simple"/></inline-formula> will be unstable if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e270" xlink:type="simple"/></inline-formula>.</p>
        <p>As we saw from <xref ref-type="fig" rid="pone-0050276-g002">Fig. 2</xref>, for stimuli <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e271" xlink:type="simple"/></inline-formula>, the system does not always leave whichever meatastable state it is in to go perfectly to the pattern shown. A degree of “structural noise” (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e272" xlink:type="simple"/></inline-formula>) can lead to a better response. In the same way, the dynamical noise set by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e273" xlink:type="simple"/></inline-formula> can improve performance. <xref ref-type="fig" rid="pone-0050276-g008">Figure 8</xref> shows how performance varies with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e274" xlink:type="simple"/></inline-formula> for different values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e275" xlink:type="simple"/></inline-formula>. Due to the trade-off between sensitivity to stimuli and stability of the memory states, there is in general an optimum level of noise at which the system performs best. This dynamics can be interpreted as a kind of stochastic resonance, with the stimuli playing the part of the periodic forcing typically seen in such systems <xref ref-type="bibr" rid="pone.0050276-Benzi1">[48]</xref>. Both the dynamic (annealed) noise, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e276" xlink:type="simple"/></inline-formula>, and the structural (quenched) noise, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e277" xlink:type="simple"/></inline-formula>, serve to increase the sensitivity of the system to stimuli.</p>
        <fig id="pone-0050276-g008" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0050276.g008</object-id>
          <label>Figure 8</label>
          <caption>
            <title>Performance </title>
            <p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e278" xlink:type="simple"/></inline-formula><bold> against </bold><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e279" xlink:type="simple"/></inline-formula><bold> for the Hopfield-Amari networks described in the main text, obtained from MC simulations, for values of the rewiring </bold><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e280" xlink:type="simple"/></inline-formula><bold>, </bold><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e281" xlink:type="simple"/></inline-formula><bold>, </bold><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e282" xlink:type="simple"/></inline-formula><bold> and </bold><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e283" xlink:type="simple"/></inline-formula><bold>, and stimulus </bold><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e284" xlink:type="simple"/></inline-formula><bold>.</bold> All other parameters as in <xref ref-type="fig" rid="pone-0050276-g002">Fig. 2</xref>. (Error bars represent standard deviations; lines – splines – are drawn as a guide to the eye).</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0050276.g008" position="float" xlink:type="simple"/>
        </fig>
        <p>It is interesting to observe in <xref ref-type="fig" rid="pone-0050276-g008">Fig. 8</xref> that whereas highly modular networks (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e285" xlink:type="simple"/></inline-formula>) are most robust to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e286" xlink:type="simple"/></inline-formula>, for no values of parameters do they exhibit as good performance as the less modular networks when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e287" xlink:type="simple"/></inline-formula> is relatively low.</p>
      </sec>
      <sec id="s4b">
        <title>Effective modularity of clustered networks</title>
        <p>We wish to estimate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e288" xlink:type="simple"/></inline-formula>, the proportion of edges that cross the boundaries of a box of linear size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e289" xlink:type="simple"/></inline-formula> placed randomly on a network embedded in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e290" xlink:type="simple"/></inline-formula>-dimensional space according to the scheme laid out in Ref. <xref ref-type="bibr" rid="pone.0050276-Rozenfeld1">[45]</xref>. The number of nodes within a radius <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e291" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e292" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e293" xlink:type="simple"/></inline-formula> a constant. We shall therefore assume a node with degree <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e294" xlink:type="simple"/></inline-formula> to have edges to all nodes up to a distance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e295" xlink:type="simple"/></inline-formula>, and none beyond (note that this is not necessarily always feasible in practice). To estimate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e296" xlink:type="simple"/></inline-formula>, we shall first calculate the probability that a randomly chosen edge have length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e297" xlink:type="simple"/></inline-formula>. The chance that the edge belong to a node with degree <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e298" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e299" xlink:type="simple"/></inline-formula> (where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e300" xlink:type="simple"/></inline-formula> is the degree distribution). The proportion of edges that have length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e301" xlink:type="simple"/></inline-formula> among those belonging to a node with degree <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e302" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e303" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e304" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e305" xlink:type="simple"/></inline-formula> otherwise. Considering, for example, scale-free networks (as in Ref. <xref ref-type="bibr" rid="pone.0050276-Rozenfeld1">[45]</xref>), so that the degree distribution is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e306" xlink:type="simple"/></inline-formula> in some interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e307" xlink:type="simple"/></inline-formula>, and integrating over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e308" xlink:type="simple"/></inline-formula>, we have the distribution of lengths,<disp-formula id="pone.0050276.e309"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0050276.e309" xlink:type="simple"/></disp-formula>where we have assumed, for simplicity, that the network is sufficiently sparse that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e310" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e311" xlink:type="simple"/></inline-formula>, and where we have normalised for the interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e312" xlink:type="simple"/></inline-formula>; strictly, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e313" xlink:type="simple"/></inline-formula>, but we shall also ignore this effect. Next we need the probability that an edge of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e314" xlink:type="simple"/></inline-formula> fall between two compartments of linear size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e315" xlink:type="simple"/></inline-formula>. This depends on the geometry of the situation as well as dimensionality; however, a first approximation which is independent of such considerations is<disp-formula id="pone.0050276.e316"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0050276.e316" xlink:type="simple"/></disp-formula>We can now estimate the modularity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e317" xlink:type="simple"/></inline-formula> as<disp-formula id="pone.0050276.e318"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0050276.e318" xlink:type="simple"/><label>(7)</label></disp-formula><xref ref-type="fig" rid="pone-0050276-g009">Figure 9</xref> compares this expression with the value obtained numerically after averaging over many network realizations, and shows that it is fairly good – considering the approximations used for its derivation.</p>
        <fig id="pone-0050276-g009" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0050276.g009</object-id>
          <label>Figure 9</label>
          <caption>
            <title>Proportion of outgoing edges, </title>
            <p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e319" xlink:type="simple"/></inline-formula><bold>, from boxes of linear size </bold><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e320" xlink:type="simple"/></inline-formula><bold> against exponent </bold><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e321" xlink:type="simple"/></inline-formula><bold> for scale-free networks embedded on </bold><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e322" xlink:type="simple"/></inline-formula><bold> lattices.</bold> Lines from <xref ref-type="disp-formula" rid="pone.0050276.e318">Eq. (7)</xref> and symbols (with error bars representing standard deviations) from simulations with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e323" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0050276.e324" xlink:type="simple"/></inline-formula>.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0050276.g009" position="float" xlink:type="simple"/>
        </fig>
      </sec>
    </sec>
  </body>
  <back>
    <ack>
      <p>Many thanks to Jorge F. Mejias, Sebastiano de Franciscis, Miguel A. Muñoz, Sabine Hilfiker, Peter E. Latham, Ole Paulsen and Nick S. Jones for useful comments and suggestions.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pone.0050276-Hebb1">
        <label>1</label>
        <mixed-citation publication-type="other" xlink:type="simple">Hebb DO (1949) The organization of behavior. New York: Wiley.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Amari1">
        <label>2</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amari</surname><given-names>S</given-names></name> (<year>1972</year>) <article-title>Characteristics of random nets of analog neuron-like elements</article-title>. <source>IEEE Trans Syst Man Cybern</source> <volume>2</volume>: <fpage>643</fpage>–<lpage>657</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Hopfield1">
        <label>3</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name> (<year>1982</year>) <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>79</volume>: <fpage>2554</fpage>–<lpage>2558</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Amit1">
        <label>4</label>
        <mixed-citation publication-type="other" xlink:type="simple">Amit DJ (1989) Modeling brain function. Cambridge: Cambridge Univ Press.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Gruart1">
        <label>5</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gruart</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Muñoz</surname><given-names>MD</given-names></name>, <name name-style="western"><surname>Delgado-García</surname><given-names>JM</given-names></name> (<year>2006</year>) <article-title>Involvement of the CA3-CA1 synapse in the acquisition of associative learning in behaving mice</article-title>. <source>J Neurosci</source> <volume>26</volume>: <fpage>1077</fpage>–<lpage>1087</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-DeRoo1">
        <label>6</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>De Roo</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Klauser</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Mendez</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Poglia</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Muller</surname><given-names>D</given-names></name> (<year>2008</year>) <article-title>Activity-dependent PSD formation and stabilization of newly formed spines in hippocampal slice cultures</article-title>. <source>Cereb Cortex</source> <volume>18</volume>: <fpage>151</fpage>–<lpage>161</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Durstewitz1">
        <label>7</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Seamans</surname><given-names>JK</given-names></name>, <name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name> (<year>2000</year>) <article-title>Neurocomputational models of working memory</article-title>. <source>Nat Neurosci</source> <volume>3</volume>: <fpage>1184</fpage>–<lpage>1191</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Lee1">
        <label>8</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname><given-names>KS</given-names></name>, <name name-style="western"><surname>Schottler</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Oliver</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Lynch</surname><given-names>G</given-names></name> (<year>1980</year>) <article-title>Brief bursts of high-frequency stimulation produce two types of structural change in rat hippocampus</article-title>. <source>J Neurophysiol</source> <volume>44</volume>: <fpage>247</fpage>–<lpage>258</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Klintsova1">
        <label>9</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Klintsova</surname><given-names>AY</given-names></name>, <name name-style="western"><surname>Greenough</surname><given-names>WT</given-names></name> (<year>1999</year>) <article-title>Synaptic plasticity in cortical systems</article-title>. <source>Curr Opin Neurobiol</source> <volume>9</volume>: <fpage>203</fpage>–<lpage>208</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Sperling1">
        <label>10</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sperling</surname><given-names>GA</given-names></name> (<year>1960</year>) <article-title>The information available in brief visual persentation</article-title>. <source>Psychol Monogr</source> <volume>74</volume>: <fpage>1</fpage>–<lpage>30</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Cowan1">
        <label>11</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cowan</surname><given-names>N</given-names></name> (<year>1984</year>) <article-title>On short and long auditory stores</article-title>. <source>Psychol Bull</source> <volume>96</volume>: <fpage>341</fpage>–<lpage>370</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Baddeley1">
        <label>12</label>
        <mixed-citation publication-type="other" xlink:type="simple">Baddeley AD (1999) Essentials of human Memory. London: Psychology Press.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Baddeley2">
        <label>13</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baddeley</surname><given-names>A</given-names></name> (<year>2003</year>) <article-title>Working memory: looking back and looking forward</article-title>. <source>Nat Rev Neurosci</source> <volume>4</volume>: <fpage>829</fpage>–<lpage>839</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Conrad1">
        <label>14</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Conrad</surname><given-names>R</given-names></name> (<year>1964</year>) <article-title>Acoustic confusion in immediate memory</article-title>. <source>B J Psychol</source> <volume>55</volume>: <fpage>75</fpage>–<lpage>84</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Conrad2">
        <label>15</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Conrad</surname><given-names>R</given-names></name> (<year>1964</year>) <article-title>Information, acoustic confusion and memory span</article-title>. <source>B J Psychol</source> <volume>55</volume>: <fpage>429</fpage>–<lpage>432</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Barak1">
        <label>16</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barak</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>Persistent activity in neural networks with dynamic synapses</article-title>. <source>PLoS Comput Biol</source> <volume>3</volume>: <fpage>e35</fpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Roudi1">
        <label>17</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roudi</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name> (<year>2007</year>) <article-title>A balanced memory network</article-title>. <source>PLoS Comput Biol</source> <volume>3</volume>: <fpage>e141</fpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Mongillo1">
        <label>18</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mongillo</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Barak</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name> (<year>2008</year>) <article-title>Synaptic theory of working memory</article-title>. <source>Science</source> <volume>319</volume>: <fpage>1543</fpage>–<lpage>1546</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Wang1">
        <label>19</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname><given-names>XJ</given-names></name> (<year>2001</year>) <article-title>Synaptic reverbaration underlying mnemonic presistent activity</article-title>. <source>Trends Neurosci</source> <volume>24</volume>: <fpage>455</fpage>–<lpage>463</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Chialvo1">
        <label>20</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chialvo</surname><given-names>DR</given-names></name>, <name name-style="western"><surname>Cecchi</surname><given-names>GA</given-names></name>, <name name-style="western"><surname>Magnasco</surname><given-names>MO</given-names></name> (<year>2000</year>) <article-title>Noise-induced memory in extended excitable systems</article-title>. <source>Phys Rev E</source> <volume>61</volume>: <fpage>5654</fpage>–<lpage>5657</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Camperi1">
        <label>21</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Camperi</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>XJ</given-names></name> (<year>1998</year>) <article-title>A model of visuospatial working memory in prefrontal cortex: recurrent network and cellular bistability</article-title>. <source>J Comp Neurosci</source> <volume>5</volume>: <fpage>383</fpage>–<lpage>405</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Teramae1">
        <label>22</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Teramae</surname><given-names>JN</given-names></name>, <name name-style="western"><surname>Fukai</surname><given-names>T</given-names></name> (<year>2005</year>) <article-title>A cellular mechanism for graded persistent activity in a model neuron and its implications for working memory</article-title>. <source>J Comput Neurosci</source> <volume>18</volume>: <fpage>105</fpage>–<lpage>121</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Tarnow1">
        <label>23</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tarnow</surname><given-names>E</given-names></name> (<year>2008</year>) <article-title>Short term memory may be the depletion of the readily releasable pool of presynaptic neurotransmitter vesicles</article-title>. <source>Cogn Neurodyn</source> <volume>3</volume>: <fpage>263</fpage>–<lpage>269</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Compte1">
        <label>24</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Compte</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Constantinidis</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Tegnér</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Raghavachari</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Raghavachari</surname><given-names>S</given-names></name>, <etal>et al</etal>. (<year>2003</year>) <article-title>Temporally irregular mnemonic persistent activity in prefrontal neurons of monkeys during a delayed response task</article-title>. <source>J Neurophysiol</source> <volume>90</volume>: <fpage>3441</fpage>–<lpage>3454</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Sporns1">
        <label>25</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Chialvo</surname><given-names>DR</given-names></name>, <name name-style="western"><surname>Kaiser</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Hilgetag</surname><given-names>CC</given-names></name> (<year>2004</year>) <article-title>Organization, development and function of complex brain networks</article-title>. <source>Trends Cogn Sci</source> <volume>8</volume>: <fpage>418</fpage>–<lpage>425</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Johnson1">
        <label>26</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Johnson</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Marro</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Torres</surname><given-names>JJ</given-names></name> (<year>2010</year>) <article-title>Evolving networks and the development of neural systems</article-title>. <source>J Stat Mech P03003</source></mixed-citation>
      </ref>
      <ref id="pone.0050276-Lewis1">
        <label>27</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lewis</surname><given-names>TJ</given-names></name>, <name name-style="western"><surname>Rinzel</surname><given-names>J</given-names></name> (<year>2000</year>) <article-title>Self-organized synchronous oscillations in a network of excitable cells coupled by gap junctions</article-title>. <source>Comput Neural Syst</source> <volume>11</volume>: <fpage>299</fpage>–<lpage>320</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Wixted1">
        <label>28</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wixted</surname><given-names>JT</given-names></name>, <name name-style="western"><surname>Ebbesen</surname><given-names>EB</given-names></name> (<year>1991</year>) <article-title>On the form of forgetting</article-title>. <source>Psychol Sci</source> <volume>2</volume>: <fpage>409</fpage>–<lpage>415</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Wixted2">
        <label>29</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wixted</surname><given-names>JT</given-names></name>, <name name-style="western"><surname>Ebbesen</surname><given-names>EB</given-names></name> (<year>1997</year>) <article-title>Genuine power curves in forgetting: A quantitative analysis of individual subject forgetting functions</article-title>. <source>Mem Cognition</source> <volume>25</volume>: <fpage>731</fpage>–<lpage>739</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Sikstrm1">
        <label>30</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sikström</surname><given-names>S</given-names></name> (<year>2002</year>) <article-title>Forgetting curves: implications for connectionist models</article-title>. <source>Cognitive Psychol</source> <volume>45</volume>: <fpage>95</fpage>–<lpage>152</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Takahashi1">
        <label>31</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Takahashi</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Kitamura</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Matsuo</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Mayford</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Kano</surname><given-names>M</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Locally synchronized synaptic inputs</article-title>. <source>Science</source> <volume>335</volume>: <fpage>353</fpage>–<lpage>356</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Kleindienst1">
        <label>32</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kleindienst</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Winnubst</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Roth-Alpermann</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Bonhoeffer</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Lohmann</surname><given-names>C</given-names></name> (<year>2011</year>) <article-title>Activitydependent clustering of functional synaptic inputs on developing hippocampal dendrites</article-title>. <source>Neuron</source> <volume>72</volume>: <fpage>1012</fpage>–<lpage>1024</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Makino1">
        <label>33</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Makino</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Malinow</surname><given-names>R</given-names></name> (<year>2011</year>) <article-title>Compartmentalized versus global synaptic plasticity on dendrites controlled by experience</article-title>. <source>Neuron</source> <volume>72</volume>: <fpage>1001</fpage>–<lpage>1011</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Kohl1">
        <label>34</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kohl</surname><given-names>MM</given-names></name>, <name name-style="western"><surname>Shipton</surname><given-names>OA</given-names></name>, <name name-style="western"><surname>Deacon</surname><given-names>RM</given-names></name>, <name name-style="western"><surname>Rawlins</surname><given-names>JN</given-names></name>, <name name-style="western"><surname>Deisseroth</surname><given-names>K</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Hemisphere-specific optogenetic stimulation reveals left-right asymmetry of hippocampal plasticity</article-title>. <source>Nat Neurosci</source> <volume>14</volume>: <fpage>1413</fpage>–<lpage>1415</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-SheinIdelson1">
        <label>35</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shein-Idelson</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ben-Jacob</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Hanein</surname><given-names>Y</given-names></name> (<year>2011</year>) <article-title>Engineered neuronal circuits: A new platform for studying the role of modular topology</article-title>. <source>Front Neuroeng</source> <volume>4</volume>: <fpage>10</fpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Fraiman1">
        <label>36</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fraiman</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Balenzuela</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Foss</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Chialvo</surname><given-names>D</given-names></name> (<year>2009</year>) <article-title>Ising-like dynamics in large-scale functional brain networks</article-title>. <source>Phys Rev E</source> <volume>79</volume>: <fpage>61922</fpage>–<lpage>61931</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Newman1">
        <label>37</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Newman</surname><given-names>MEJ</given-names></name> (<year>2003</year>) <article-title>The structure and function of complex networks. SIAM Rev</article-title>. <volume>45</volume>: <fpage>167</fpage>–<lpage>256</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Johnson2">
        <label>38</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Johnson</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Marro</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Torres</surname><given-names>JJ</given-names></name> (<year>2008</year>) <article-title>Functional optimization in complex excitable networks</article-title>. <source>EPL</source> <volume>83</volume>: <fpage>46006</fpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Torres1">
        <label>39</label>
        <mixed-citation publication-type="other" xlink:type="simple">Torres JJ, Varona P (2011) Modeling biological neural networks, in Handbook of natural computing, Rozenberg G, Bäck THW, Kok JN (Eds). Berlin: Springer-Verlag.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Levine1">
        <label>40</label>
        <mixed-citation publication-type="other" xlink:type="simple">Levine RD (2005) Molecular reaction dynamics. Cambridge: Cambridge University Press.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Hurtado1">
        <label>41</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hurtado</surname><given-names>PI</given-names></name>, <name name-style="western"><surname>Marro</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Garrido</surname><given-names>PL</given-names></name> (<year>2008</year>) <article-title>Demagnetization via nucleation of the nonequilibrium metastable phase in a model of disorder</article-title>. <source>J Stat Phys</source> <volume>133</volume>: <fpage>29</fpage>–<lpage>58</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Muoz1">
        <label>42</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Muñoz</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Juhasz</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Castellano</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Odor</surname><given-names>G</given-names></name> (<year>2010</year>) <article-title>Griffiths phases in complex networks</article-title>. <source>Phys Rev Lett</source> <volume>105</volume>: <fpage>128701</fpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Gnedenko1">
        <label>43</label>
        <mixed-citation publication-type="other" xlink:type="simple">Gnedenko BV, Kolmogorov AN (1954) Limit distributions for sums of independent random variables. Cambridge: Addison-Wesley.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Watts1">
        <label>44</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Watts</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Strogatz</surname><given-names>SH</given-names></name> (<year>1998</year>) <article-title>Collective dynamics of ‘small-world’ networks</article-title>. <source>Nature</source> <volume>395</volume>: <fpage>440</fpage>–<lpage>442</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Rozenfeld1">
        <label>45</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rozenfeld</surname><given-names>AF</given-names></name>, <name name-style="western"><surname>Cohen</surname><given-names>R</given-names></name>, <name name-style="western"><surname>ben-Avraham</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Havlin</surname><given-names>S</given-names></name> (<year>2002</year>) <article-title>Scale-free networks on lattices</article-title>. <source>Phys Rev Lett</source> <volume>89</volume>: <fpage>218701</fpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Tsodyks1">
        <label>46</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsodyks</surname><given-names>MV</given-names></name>, <name name-style="western"><surname>Markram</surname><given-names>H</given-names></name> (<year>1997</year>) <article-title>The neural code between neocortical pyramidal neurons depends on neurotransmitter release probability</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>94</volume>: <fpage>719</fpage>–<lpage>723</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-vanAerde1">
        <label>47</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Aerde</surname><given-names>KI</given-names></name>, <name name-style="western"><surname>Mann</surname><given-names>EO</given-names></name>, <name name-style="western"><surname>Canto</surname><given-names>CB</given-names></name>, <name name-style="western"><surname>Heistek</surname><given-names>TS</given-names></name>, <name name-style="western"><surname>Linkenkaer-Hansen</surname><given-names>K</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Flexible spike timing of layer 5 neurons during dynamic beta-oscillation shifts in rat prefrontal cortex</article-title>. <source>J Physiol</source> <volume>587</volume>: <fpage>5177</fpage>–<lpage>5196</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0050276-Benzi1">
        <label>48</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Benzi</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Sutera</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Vulpiani</surname><given-names>A</given-names></name> (<year>1981</year>) <article-title>The mechanism of stochastic resonance</article-title>. <source>J Phys A</source> <volume>14</volume>: <fpage>453</fpage>–<lpage>457</lpage>.</mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>