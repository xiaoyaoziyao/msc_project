<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article article-type="research-article" dtd-version="3.0" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-01668</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004845</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>People and places</subject><subj-group><subject>Demography</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Genomics statistics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Genomics</subject><subj-group><subject>Genomics statistics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Genomics</subject><subj-group><subject>Animal genomics</subject><subj-group><subject>Invertebrate genomics</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Evolutionary biology</subject><subj-group><subject>Evolutionary processes</subject><subj-group><subject>Natural selection</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Evolutionary biology</subject><subj-group><subject>Population genetics</subject><subj-group><subject>Natural selection</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Population genetics</subject><subj-group><subject>Natural selection</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Population biology</subject><subj-group><subject>Population genetics</subject><subj-group><subject>Natural selection</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Model organisms</subject><subj-group><subject>Animal models</subject><subj-group><subject>Drosophila melanogaster</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Invertebrates</subject><subj-group><subject>Arthropoda</subject><subj-group><subject>Insects</subject><subj-group><subject>Drosophila</subject><subj-group><subject>Drosophila melanogaster</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Evolutionary biology</subject><subj-group><subject>Population genetics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Population genetics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Population biology</subject><subj-group><subject>Population genetics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Population biology</subject><subj-group><subject>Population metrics</subject><subj-group><subject>Population size</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Evolutionary biology</subject><subj-group><subject>Population genetics</subject><subj-group><subject>Effective population size</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Population genetics</subject><subj-group><subject>Effective population size</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Population biology</subject><subj-group><subject>Population genetics</subject><subj-group><subject>Effective population size</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Population biology</subject><subj-group><subject>Population metrics</subject><subj-group><subject>Effective population size</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Population biology</subject><subj-group><subject>Population metrics</subject><subj-group><subject>Population size</subject><subj-group><subject>Effective population size</subject></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Deep Learning for Population Genetic Inference</article-title>
<alt-title alt-title-type="running-head">Deep Learning for Population Genetic Inference</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Sheehan</surname> <given-names>Sara</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Song</surname> <given-names>Yun S.</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
<xref ref-type="aff" rid="aff006"><sup>6</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Department of Computer Science, Smith College, Northampton, Massachusetts, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Computer Science Division, UC Berkeley, Berkeley, California, United States of America</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Department of Statistics, UC Berkeley, Berkeley, California, United States of America</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>Department of Integrative Biology, UC Berkeley, Berkeley, California, United States of America</addr-line>
</aff>
<aff id="aff005">
<label>5</label>
<addr-line>Department of Mathematics, University of Pennsylvania, Philadelphia, Pennsylvania, United States of America</addr-line>
</aff>
<aff id="aff006">
<label>6</label>
<addr-line>Department of Biology, University of Pennsylvania, Philadelphia, Pennsylvania, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Chen</surname> <given-names>Kevin</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Rutgers University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: SS YSS. Performed the experiments: SS. Analyzed the data: SS YSS. Contributed reagents/materials/analysis tools: SS YSS. Wrote the paper: SS YSS. Developed the software used in analysis: SS.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">ssheehan@smith.edu</email> (SS); <email xlink:type="simple">yss@berkeley.edu</email> (YSS)</corresp>
</author-notes>
<pub-date pub-type="collection">
<month>3</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="epub">
<day>28</day>
<month>3</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>3</issue>
<elocation-id>e1004845</elocation-id>
<history>
<date date-type="received">
<day>2</day>
<month>10</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>2</day>
<month>3</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Sheehan, Song</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004845"/>
<abstract>
<p>Given genomic variation data from multiple individuals, computing the likelihood of complex population genetic models is often infeasible. To circumvent this problem, we introduce a novel likelihood-free inference framework by applying deep learning, a powerful modern technique in machine learning. Deep learning makes use of multilayer neural networks to learn a feature-based function from the input (e.g., hundreds of correlated summary statistics of data) to the output (e.g., population genetic parameters of interest). We demonstrate that deep learning can be effectively employed for population genetic inference and learning informative features of data. As a concrete application, we focus on the challenging problem of jointly inferring natural selection and demography (in the form of a population size change history). Our method is able to separate the global nature of demography from the local nature of selection, without sequential steps for these two factors. Studying demography and selection jointly is motivated by <italic>Drosophila</italic>, where pervasive selection confounds demographic analysis. We apply our method to 197 African <italic>Drosophila melanogaster</italic> genomes from Zambia to infer both their overall demography, and regions of their genome under selection. We find many regions of the genome that have experienced hard sweeps, and fewer under selection on standing variation (soft sweep) or balancing selection. Interestingly, we find that soft sweeps and balancing selection occur more frequently closer to the centromere of each chromosome. In addition, our demographic inference suggests that previously estimated bottlenecks for African <italic>Drosophila melanogaster</italic> are too extreme.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Deep learning is an active area of research in machine learning which has been applied to various challenging problems in computer science over the past several years, breaking long-standing records of classification accuracy. Here, we apply deep learning to develop a novel likelihood-free inference framework to estimate population genetic parameters and learn informative features of DNA sequence data. As a concrete example, we focus on the challenging problem of jointly inferring natural selection and demographic history.</p>
</abstract>
<funding-group>
<funding-statement>This research is supported in part by a National Science Foundation Graduate Research Fellowship (SS), a National Institute of Health Grant R01-GM094402 (YSS), and a Packard Fellowship for Science and Engineering (YSS). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="8"/>
<table-count count="9"/>
<page-count count="28"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>With the advent of large-scale whole-genome variation data, population geneticists are currently interested in considering increasingly more complex models. However, statistical inference in this setting is a challenging task, as computing the likelihood of a complex population genetic model is a difficult problem both theoretically and computationally.</p>
<p>In this paper, we introduce a novel likelihood-free inference framework for population genomics by applying <italic>deep learning</italic>, which is an active area of machine learning research. To our knowledge, deep learning has not been employed in population genomics before. A recent survey article [<xref ref-type="bibr" rid="pcbi.1004845.ref001">1</xref>] provides an accessible introduction to deep learning, and we provide a high-level description below. Our general goal in this paper is to demonstrate the potential of deep learning as a powerful framework for population genetic analysis that can allow accurate inference of previously intractable models.</p>
<p>As a concrete example, we consider models of non-equilibrium demography and natural selection, for which multi-locus full-likelihood computation is prohibitive, and apply deep learning to the challenging problem of jointly inferring demographic history and selection (see [<xref ref-type="bibr" rid="pcbi.1004845.ref002">2</xref>] for a recent review of this topic). One reason why this problem is difficult is that demography (for example, a bottleneck in population size) and selection can leave similar signals in the genome. Untangling the two factors directly has rarely been attempted; most methods that estimate selection try to demonstrate robustness to demographic scenarios, rather than estimating demographic parameters jointly with selection. The demographic models considered in this paper are restricted to a single population with time-varying effective population size, but the overall framework presented here can be applied to more general demographic models. We anticipate many algorithmic, modeling, and application directions that could emerge from this proof of concept.</p>
<p>Our focus on the joint inference of demography and selection is motivated by <italic>Drosophila</italic>, where previous demographic estimates [<xref ref-type="bibr" rid="pcbi.1004845.ref003">3</xref>] may have been confounded by pervasive selection. The reverse has occurred as well, with selection estimates being confounded by demography [<xref ref-type="bibr" rid="pcbi.1004845.ref004">4</xref>]. See [<xref ref-type="bibr" rid="pcbi.1004845.ref005">5</xref>] for a more thorough discussion of the role selection plays in the <italic>Drosophila</italic> genome. To test our method, we simulate data under a variety of realistic demographies for an African population of <italic>Drosophila melanogaster</italic>. For each demographic history, we simulate many regions under different selection parameters. We then apply our tailored deep learning method using hundreds of potentially informative summary statistics. From the output of this trained network, we demonstrate that parameters can be accurately inferred for test datasets, and interpret which statistics are making the biggest contributions. After training our deep network, we also apply it to analyze 197 <italic>Drosophila melanogaster</italic> genomes from Zambia, Africa [<xref ref-type="bibr" rid="pcbi.1004845.ref006">6</xref>] to learn about their effective population size change history and selective landscape.</p>
<sec id="sec002">
<title>A summary of related works</title>
<p>Several machine learning methods have been developed for selection only, often focusing on classifying the genome into neutral versus selected regions. Examples include methods based on support vector machines (SVMs) [<xref ref-type="bibr" rid="pcbi.1004845.ref007">7</xref>–<xref ref-type="bibr" rid="pcbi.1004845.ref009">9</xref>] or boosting [<xref ref-type="bibr" rid="pcbi.1004845.ref010">10</xref>–<xref ref-type="bibr" rid="pcbi.1004845.ref012">12</xref>]. Often these methods demonstrate robustness to different demographic scenarios, but do not explicitly infer demography.</p>
<p>Many methods have been developed to infer ancestral population size changes, including PSMC [<xref ref-type="bibr" rid="pcbi.1004845.ref013">13</xref>], diCal [<xref ref-type="bibr" rid="pcbi.1004845.ref014">14</xref>, <xref ref-type="bibr" rid="pcbi.1004845.ref015">15</xref>], and MSMC [<xref ref-type="bibr" rid="pcbi.1004845.ref016">16</xref>]. These methods make the underlying assumption that selection does not significantly bias the results. This is perhaps somewhat true for humans, but is not a valid assumption for many species such as <italic>Drosophila</italic>, for which selection seems ubiquitous throughout the genome.</p>
<p>Few previous works have addressed both population size changes and selection. Galtier <italic>et al.</italic> [<xref ref-type="bibr" rid="pcbi.1004845.ref017">17</xref>] developed a likelihood-based method for distinguishing between a bottleneck and selection. They applied their method to <italic>Drosophila</italic> data to conclude that a series of selective sweeps was more likely than a bottleneck, but did not explicitly infer parameters for both selection and demography. In their method, Galtier <italic>et al.</italic> assumed that demographic events affect the entire genome, whereas selection is a local phenomenon. In contrast, Gossmann <italic>et al.</italic> [<xref ref-type="bibr" rid="pcbi.1004845.ref018">18</xref>] estimated the effective population size locally along the genome, and reported that it is correlated with the density of selected sites. To make our results as easily interpretable as evolutionary events as possible, we estimate global effective population size changes.</p>
<p>Approximate Bayesian Computation (ABC) [<xref ref-type="bibr" rid="pcbi.1004845.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1004845.ref020">20</xref>] is a likelihood-free inference method based on simulating datasets and comparing their summary statistics. (A more detailed description of the framework is provided below.) This approach has been used to study various complex population genetic models (e.g., [<xref ref-type="bibr" rid="pcbi.1004845.ref021">21</xref>–<xref ref-type="bibr" rid="pcbi.1004845.ref023">23</xref>]) for which likelihood computation is prohibitive. Partly due to several influential theoretical works [<xref ref-type="bibr" rid="pcbi.1004845.ref024">24</xref>–<xref ref-type="bibr" rid="pcbi.1004845.ref031">31</xref>], the popularity of ABC has grown rapidly over the past decade. ABC’s main advantages are that it is easy to use and is able to output a posterior distribution. There are a few challenges, however: 1) ABC suffers from the “curse of dimensionality,” with decreasing accuracy and stability as the number of summary statistics grows [<xref ref-type="bibr" rid="pcbi.1004845.ref032">32</xref>]. 2) ABC typically uses a rejection algorithm, so the simulated datasets are not used optimally, and often a very large number are required. 3) It is difficult to interpret the output of ABC in terms of which statistics were the most informative for each parameter. 4) Although ABC can be used very effectively for continuous parameter inference, it would be difficult to use ABC for a complex scenario that involved joint inference of continuous parameters and categorical distributions. To our knowledge, ABC has thus not been previously applied to the problem of jointly inferring demography (using continuous effective population sizes) and selection (using discrete classification). The work of Bazin <italic>et al.</italic> [<xref ref-type="bibr" rid="pcbi.1004845.ref033">33</xref>] estimates demographic and selection parameters using a hierarchical ABC approach, though not in an explicit joint framework.</p>
<p>We see our deep learning method as a complimentary approach to, rather than a direct replacement of, other likelihood-free inference methods such as ABC or SVM. Deep learning has several advantages such as making full use of datasets, elegantly handling correlations between summary statistics, and producing interpretable features of the data. It also produces an “inverse” function from the input summary statistics to the population genetic parameters of interest. However, in contrast to ABC, it does not provide a posterior distribution.</p>
</sec>
<sec id="sec003">
<title>A brief introduction to deep learning</title>
<p>Deep learning has its beginnings in neural networks, which were originally inspired by the way neurons are connected in the brain [<xref ref-type="bibr" rid="pcbi.1004845.ref034">34</xref>]. Neural networks have been studied for over 60 years and a huge body of literature exists on the topic. A neural network is typically used to learn a complex function between inputs (data) and outputs (<italic>response variables</italic>) in the absence of a model, so it can be thought of as a generalized regression framework. While standard regression and classification methods involve fitting linear combinations of <italic>fixed</italic> basis functions, a neural network tries to <italic>learn</italic> basis functions (usually non-linear) appropriate for the data. A neural network architecture consists of multiple layers of <italic>computational units</italic> (nodes), with connections between the layers but not between nodes within a layer. Within a layer, each node computes a transformation (usually non-linear) of the outputs from the previous layer. <xref ref-type="supplementary-material" rid="pcbi.1004845.s005">S1 Fig</xref> illustrates a simple feed-forward neural network with a single hidden layer. The phrase “deep learning” refers to algorithms for learning <italic>deep</italic> neural network architectures with many hidden layers.</p>
<p>The <italic>universal approximation theorem</italic> [<xref ref-type="bibr" rid="pcbi.1004845.ref035">35</xref>, <xref ref-type="bibr" rid="pcbi.1004845.ref036">36</xref>] states that any continuous function on compact subsets of <inline-formula id="pcbi.1004845.e001"><alternatives><graphic id="pcbi.1004845.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mi>n</mml:mi></mml:msup></mml:math></alternatives></inline-formula> can be uniformly approximated by a feed-forward neural network with a single hidden layer, provided that the number of nodes in the hidden layer is sufficiently large and the transformation (called the activation function) associated with each node satisfies some mild conditions. However, it can be challenging to learn the weights of such a network and to interpret the hidden layer. So as learning problems became more complex, it was desirable to train networks with more hidden layers. Since their introduction over 30 years ago, deep architectures have proved adept at modeling multiple levels of abstraction. However, they were notoriously difficult to train since their objective functions are non-convex and highly non-linear, and the level of non-linearity increases with the number of layers in the network.</p>
<p>A major breakthrough was made in 2006 when Hinton and Salakhutdinov [<xref ref-type="bibr" rid="pcbi.1004845.ref037">37</xref>] showed that a deep feed-forward neural network can be trained effectively by first performing unsupervised “pretraining” one layer at a time, followed by supervised fine-tuning using a gradient-descent algorithm called <italic>back-propagation</italic> [<xref ref-type="bibr" rid="pcbi.1004845.ref038">38</xref>]. (Simply put, pretraining provides a good initialization point for non-convex optimization.) They applied their learning algorithm to dimensionality reduction of images and achieved substantially better results than PCA-based methods.</p>
<p>Following the work of Hinton and Salakhutdinov, deep learning has been applied to various challenging problems in computer science over the last several years, making groundbreaking progress. Deep learning broke long-standing records for accuracy that had been set by approaches based on hand-coded rules. Well-known examples include automatic speech recognition (transforming spoken words into typed text) [<xref ref-type="bibr" rid="pcbi.1004845.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1004845.ref040">40</xref>] and computer vision (automatically classifying images into different categories and tagging objects/individuals in photos) [<xref ref-type="bibr" rid="pcbi.1004845.ref041">41</xref>].</p>
<p>Many variations have been developed, including <italic>dropout</italic>, which attempts to learn better and more robust features of the data (see [<xref ref-type="bibr" rid="pcbi.1004845.ref042">42</xref>, <xref ref-type="bibr" rid="pcbi.1004845.ref043">43</xref>]). Deep learning has also been applied to problems in neuroscience [<xref ref-type="bibr" rid="pcbi.1004845.ref044">44</xref>] and computational biology [<xref ref-type="bibr" rid="pcbi.1004845.ref045">45</xref>–<xref ref-type="bibr" rid="pcbi.1004845.ref048">48</xref>], but has not been used for population genetics before. We demonstrate here that deep learning can accommodate more complex models than existing techniques, and can provide a complimentary approach to existing likelihood-free inference in population genetics. For example, deep learning could be used to select optimal statistics for ABC (as demonstrated in [<xref ref-type="bibr" rid="pcbi.1004845.ref049">49</xref>]), or another method.</p>
</sec>
<sec id="sec004">
<title>ABC background</title>
<p>Rejection-based methods have been used since the late 1990’s (see [<xref ref-type="bibr" rid="pcbi.1004845.ref019">19</xref>, <xref ref-type="bibr" rid="pcbi.1004845.ref050">50</xref>]) to estimate population genetic parameters when the likelihood is difficult to compute. Early improvements to ABC quickly helped make it a popular method for a variety of scenarios (see [<xref ref-type="bibr" rid="pcbi.1004845.ref020">20</xref>] for a good introduction). ABC works by simulating many datasets under a prior for the parameters of interest. Then these datasets are reduced to a vector of summary statistics that are ideally informative for the parameters. The summary statistics that are closest to the summary statistics for the target dataset are retained, and the corresponding parameters used to estimate the desired posterior distributions. ABC has many advantages, including ease of use and the output of a posterior distribution. However, there are a few shortcomings, which various works have sought to address.</p>
<p>In its simplest form, ABC does not naturally handle correlated or weakly informative summary statistics, which can add noise to the data. To tackle this problem, many methods for dimensionality reduction or wisely selecting summary statistics have been proposed (see [<xref ref-type="bibr" rid="pcbi.1004845.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1004845.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1004845.ref031">31</xref>, <xref ref-type="bibr" rid="pcbi.1004845.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1004845.ref052">52</xref>], and [<xref ref-type="bibr" rid="pcbi.1004845.ref053">53</xref>] for a dimensionality reduction comparison). However, simple reductions cannot always learn subtle relationships between the data and the parameters. Expert pruning of statistics helps some methods, but given the lack of sufficient statistics, valuable information can be eliminated. Blum and François [<xref ref-type="bibr" rid="pcbi.1004845.ref028">28</xref>] suggested performing a dimensionality reduction step on the summary statistics via a neural network similar to <xref ref-type="supplementary-material" rid="pcbi.1004845.s005">S1 Fig</xref>. This reduction is similar in spirit to the work presented here, although there are many algorithmic and application differences.</p>
<p>A second issue with ABC is the rejection step, which does not make optimal use of the datasets that are not retained. The more statistics and parameters used, the more datasets must be simulated and rejected to properly explore the space, making the interaction between these two issues problematic. To address the problem of rejecting datasets, different weighting approaches have been proposed (see [<xref ref-type="bibr" rid="pcbi.1004845.ref028">28</xref>] for a good example of how the estimation error changes as fewer datasets are rejected). The idea is to keep more datasets, but then weight each retained dataset by its distance to the target dataset. However, few approaches utilize all the datasets in this way, and the most popular implementation of ABC (<italic>ABCtoolbox</italic>, [<xref ref-type="bibr" rid="pcbi.1004845.ref054">54</xref>]) typically still rejects most of the simulated datasets by default.</p>
<p>One final issue with ABC is the black-box nature of the output. Given the distances between the simulated datasets and the target dataset, and the posterior, there is no clear way to tell which statistics were the most informative.</p>
</sec>
</sec>
<sec id="sec005" sec-type="results">
<title>Results</title>
<p>We first describe the simulated dataset we generated to investigate joint inference of demographic history and selection. In the Methods section, we describe how to transform each region (as well as each region of the real data) into a set of 345 summary statistics, and also detail our deep learning algorithm. In what follows, we present the results of our method in a variety of different contexts.</p>
<sec id="sec006">
<title>Simulating data</title>
<p>To create a simulated dataset that is appropriate for our scenario of interest, we first define the response variables we would like to estimate. For simplicity, we consider piecewise-constant effective population size histories with three epochs: a recent effective population size <italic>N</italic><sub>1</sub>, a bottleneck size <italic>N</italic><sub>2</sub>, and an ancient size <italic>N</italic><sub>3</sub>. Further, we define a genomic region as belonging to 4 different selection classes: no selection (neutral), positive directional selection (hard sweep), selection on standing variation (soft sweep), and balancing selection. See [<xref ref-type="bibr" rid="pcbi.1004845.ref055">55</xref>] for a more complete analysis of the different types of selection in <italic>Drosophila</italic>. To accurately reflect that demography affects the entire genome while selection affects a particular region, we simulate many genomic regions under the same demographic history, but the selection class for each region is chosen independently. See <xref ref-type="fig" rid="pcbi.1004845.g001">Fig 1</xref> for a simplified illustration of the data.</p>
<fig id="pcbi.1004845.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004845.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Input data for the demography and selection scenario.</title>
<p>For each demographic history (bottleneck), we simulated many different genomic regions. Each region can either have no selection (neutral), one site with an <italic>de novo</italic> mutation under positive selection (hard sweep), one site under balancing selection, or one standing variant under positive selection (soft sweep).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.g001" xlink:type="simple"/>
</fig>
<p>To simulate data, we used the program <monospace>msms</monospace> [<xref ref-type="bibr" rid="pcbi.1004845.ref056">56</xref>]. To make our simulated data as close to the real data as possible, we simulated <italic>n</italic> = 100 haplotypes, and correspondingly downsampled the Zambia <italic>Drosophila melanogaster</italic> dataset to match. We repeated the following procedure 2500 times. First we selected three population sizes for the demographic model, then simulated 160 regions with these sizes, 40 for each selection scenario. Each region was 100 kb, with the selected site (if present) occurring randomly in the middle 20 kb of the region. We used a baseline effective population size <italic>N</italic><sub>ref</sub> = 100,000, a per-base, per-generation mutation rate <italic>μ</italic> = 8.4 × 10<sup>−9</sup> [<xref ref-type="bibr" rid="pcbi.1004845.ref057">57</xref>], and a per-base, per-generation recombination rate <italic>r</italic> equal to <italic>μ</italic>, as inferred by PSMC [<xref ref-type="bibr" rid="pcbi.1004845.ref013">13</xref>]. This ratio of recombination to mutation rate is somewhat lower than what has been inferred in other works (see [<xref ref-type="bibr" rid="pcbi.1004845.ref058">58</xref>, <xref ref-type="bibr" rid="pcbi.1004845.ref059">59</xref>]), so we also simulated a testing dataset with a higher recombination rate, described below. However, it is likely that there is significant variation in this ratio across the genome [<xref ref-type="bibr" rid="pcbi.1004845.ref060">60</xref>]. We used a generation time of 10 generations per year. Based on the population size change results of PSMC, we chose the time change-points for demographic history to be <italic>t</italic><sub>1</sub> = 0.5 and <italic>t</italic><sub>2</sub> = 5 in coalescent units. Scaled effective population size parameters λ<sub><italic>i</italic></sub> : = <italic>N</italic><sub><italic>i</italic></sub>/<italic>N</italic><sub>ref</sub> and their prior distributions are below:</p>
<list list-type="order">
<list-item>
<p>Recent effective population size scaling factor: λ<sub>1</sub> ∼ Unif(3, 14).</p>
</list-item>
<list-item>
<p>Bottleneck effective population size scaling factor: λ<sub>2</sub> ∼ Unif(0.5, 6).</p>
</list-item>
<list-item>
<p>Ancient effective population size scaling factor: λ<sub>3</sub> ∼ Unif(2, 10).</p>
</list-item>
</list>
<p>For the selection classes, the different types are shown below:</p>
<list list-type="bullet">
<list-item>
<p><bold>Class 0, neutral</bold>: no selection, neutral region.</p>
</list-item>
<list-item>
<p><bold>Class 1, hard sweep</bold>: positive selection on a <italic>de novo</italic> mutation (i.e., hard sweep). For the selection coefficient, we used <italic>s</italic> ∈ {0.01, 0.02, 0.05, 0.1}, with 10 regions for each value of <italic>s</italic>. The onset of selection was chosen to be 0.005 in coalescent units, which provided sweeps at a variety of different stages of completion at the present time. We discarded datasets where the frequency of the selected allele was 0 (early loss of the beneficial mutation due to drift), but for a large fraction of datasets, the beneficial allele had not yet fixed.</p>
</list-item>
<list-item>
<p><bold>Class 2, soft sweep</bold>: positive selection on standing variation (i.e., soft sweep). The selection coefficients and selection start time were chosen as in the hard sweep scenario, but now the initial frequency of the beneficial allele was chosen to be 0.001.</p>
</list-item>
<list-item>
<p><bold>Class 3, balancing</bold>: heterozygote advantage, balancing selection. The selection start time and selection coefficients were chosen in the same fashion as Class 1.</p>
</list-item>
</list>
<p>Using this strategy, we simulated 2500 different demographic histories, with 160 regions for each one, for a total of 400,000 datasets. To build 160 regions for each demography, we simulated 40 datasets for each of the selection classes above.</p>
</sec>
<sec id="sec007">
<title>Results for demography and selection on simulated data</title>
<p>Of the 400,000 total datasets, we used 75% for training and left out 25% for testing. In <xref ref-type="table" rid="pcbi.1004845.t001">Table 1</xref>, we show the effective population size results for a network with 3 hidden layers respectively with 25, 25, and 10 nodes (5 layers total including input and output). The best results were found when we averaged the statistics for all of the datasets (160 “regions”) with the same demography, then ran this average through the trained network.</p>
<table-wrap id="pcbi.1004845.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004845.t001</object-id>
<label>Table 1</label>
<caption>
<title>Deep learning results for population sizes under the scenario with demography and selection, using a network with 3 hidden layers of sizes 25, 25, and 10.</title> <p>We evaluate the results in three ways, using the relative error of the estimates: |<italic>N</italic><sub>est</sub> − <italic>N</italic><sub>true</sub>|/<italic>N</italic><sub>true</sub>. First, for each test demography, we average the statistics for each dataset, and then run these values through the training network. Second, for each test demography, we run the datasets through the network one by one, then average the predictions. Finally, we perform the second procedure, but with only the regions we classified as neutral. We note that the most ancient size (<italic>N</italic><sub>3</sub>) is always the least accurately estimated.</p>
</caption>
<alternatives>
<graphic id="pcbi.1004845.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Deep learning predictions</th>
<th align="center"><italic>N</italic><sub>1</sub> error</th>
<th align="center"><italic>N</italic><sub>2</sub> error</th>
<th align="center"><italic>N</italic><sub>3</sub> error</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Average stat prediction</td>
<td align="char" char=".">0.051</td>
<td align="char" char=".">0.074</td>
<td align="char" char=".">0.487</td>
</tr>
<tr>
<td align="left">Final prediction</td>
<td align="char" char=".">0.098</td>
<td align="char" char=".">0.077</td>
<td align="char" char=".">0.569</td>
</tr>
<tr>
<td align="left">Neutral regions prediction</td>
<td align="char" char=".">0.072</td>
<td align="char" char=".">0.083</td>
<td align="char" char=".">0.566</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>
<xref ref-type="table" rid="pcbi.1004845.t002">Table 2</xref> shows the results of our three different size prediction methods (see <xref ref-type="sec" rid="sec014">Methods</xref>) for an example demography. For each parameter, we include a range between the 2.5th and 97.5th quantiles. To further investigate uncertainty in our estimates, we also include a violin plot of the three population sizes, shown in <xref ref-type="fig" rid="pcbi.1004845.g002">Fig 2A</xref>. In this example (and more generally), our point estimate of the most ancient size <italic>N</italic><sub>3</sub> is the least accurate, but the most recent size <italic>N</italic><sub>1</sub> has the largest uncertainty. When considering a single dataset, there is not always a clear winner among our three prediction methods. Overall, the average statistic method is usually the most accurate, followed by the neutral regions method.</p>
<table-wrap id="pcbi.1004845.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004845.t002</object-id>
<label>Table 2</label>
<caption>
<title>Results for an example demography, with the true population sizes shown in the first row.</title> <p>The rows below each dotted line are the three different size predictions for this dataset. <italic>Average stat</italic>: average the statistics for each region within the same demography, then run these averages through the trained network. <italic>Final</italic>: run the statistics for each region through the trained network, then average the results. <italic>Neutral regions</italic>: only average the results for the regions we predict as neutral.</p>
</caption>
<alternatives>
<graphic id="pcbi.1004845.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center"><italic>N</italic><sub>1</sub></th>
<th align="center"><italic>N</italic><sub>2</sub></th>
<th align="center"><italic>N</italic><sub>3</sub></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">True sizes</td>
<td align="center">878,675</td>
<td align="center">235,199</td>
<td align="center">713,001</td>
</tr>
<tr>
<td align="left">Average stat prediction</td>
<td align="center">872,541</td>
<td align="center">264,817</td>
<td align="center">600,650</td>
</tr>
<tr>
<td align="left">Final prediction</td>
<td align="center">840,784</td>
<td align="center">243,180</td>
<td align="center">622,359</td>
</tr>
<tr>
<td align="left">Quantiles (2.5th, 97.5th)</td>
<td align="center">(659231, 954631)</td>
<td align="center">(209797, 279496)</td>
<td align="center">(597402, 649887)</td>
</tr>
<tr>
<td align="left">Standard deviation</td>
<td align="center">74,477</td>
<td align="center">37,238</td>
<td align="center">54,165</td>
</tr>
<tr>
<td align="left">Neutral regions prediction</td>
<td align="center">873,289</td>
<td align="center">237,686</td>
<td align="center">619,943</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<fig id="pcbi.1004845.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004845.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Violin plots of population size estimates for (A) an example simulated dataset and (B) the real dataset.</title>
<p>In both cases, we have the largest uncertainty for the most recent population size. For the simulated dataset, the true population sizes are shown in blue, and we note that our point estimate of the most ancient population size is the least accurate.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.g002" xlink:type="simple"/>
</fig>
<p>To analyze the selection results, we calculated a confusion matrix in <xref ref-type="table" rid="pcbi.1004845.t003">Table 3</xref> to show which datasets of each class were classified correctly, or misclassified as belonging to a different class. The rows of a confusion matrix represent the true class of each dataset, and the columns represent the predicted or called class. An ideal confusion matrix would have all 1’s (100%) on the diagonal, and 0’s off the diagonal. Our most frequent error is classifying hard sweep datasets as neutral (row 2, column 1 of the confusion matrix). We hypothesized that this was either because selection occurred anciently and quickly, or because selection occurred recently and the sweep was not yet complete. To test this, we examined the results conditional on the frequency of the selected allele at present. The results shown in <xref ref-type="table" rid="pcbi.1004845.t004">Table 4</xref> suggest that in fact many of the sweeps were not complete, which is why the regions sometimes appeared neutral. Regardless, this type of false negative error is arguably preferable to falsely calling many regions to be under selection when they are in fact neutral.</p>
<table-wrap id="pcbi.1004845.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004845.t003</object-id>
<label>Table 3</label>
<caption>
<title>Confusion matrix for the selection predictions, in the demography and selection scenario.</title> <p>Each row represents the datasets that truly belong to each selection class. Each column represents the datasets that were actually classified as each selection class. Ideally we would like all 1’s down the diagonal, and 0’s in the off-diagonal entries. The largest number in each row is shown in boldface. We can see that neutral datasets are the easiest to classify, and sometimes regions under selection (hard sweeps in particular) look neutral as well (first column). The overall percentage of misclassified datasets was 6.2%.</p>
</caption>
<alternatives>
<graphic id="pcbi.1004845.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.t003" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center" colspan="4">Called Class</th>
</tr>
<tr>
<th align="left">True Class</th>
<th align="left">Neutral</th>
<th align="left">Hard Sweep</th>
<th align="left">Soft Sweep</th>
<th align="left">Balancing</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Neutral</td>
<td align="char" char="."><bold>0.9995</bold></td>
<td align="char" char=".">0.0002</td>
<td align="char" char=".">0.0003</td>
<td align="char" char=".">0.0000</td>
</tr>
<tr>
<td align="left">Hard Sweep</td>
<td align="char" char=".">0.1434</td>
<td align="char" char="."><bold>0.8333</bold></td>
<td align="char" char=".">0.0032</td>
<td align="char" char=".">0.0201</td>
</tr>
<tr>
<td align="left">Soft Sweep</td>
<td align="char" char=".">0.0096</td>
<td align="char" char=".">0.0010</td>
<td align="char" char="."><bold>0.9891</bold></td>
<td align="char" char=".">0.0003</td>
</tr>
<tr>
<td align="left">Balancing</td>
<td align="char" char=".">0.0301</td>
<td align="char" char=".">0.0356</td>
<td align="char" char=".">0.0056</td>
<td align="char" char="."><bold>0.9287</bold></td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="pcbi.1004845.t004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004845.t004</object-id>
<label>Table 4</label>
<caption>
<title>Hard sweep results, broken down by the present-day frequency (<italic>f</italic>) of the selected allele (which we would not know for a real dataset).</title> <p>We defined “low” frequency as <italic>f</italic> ∈ [0, 0.3), “moderate” frequency as <italic>f</italic> ∈ [0.3, 0.7), and “high” frequency as <italic>f</italic> ∈ [0.7, 1]. We can see that if the frequency of the selected allele is low, the region is often misclassified as neutral, since the selective sweep is not yet complete. However, if the frequency is moderate or high, the dataset is usually classified correctly as a hard sweep.</p>
</caption>
<alternatives>
<graphic id="pcbi.1004845.t004g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.t004" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="left"/>
<th align="center" colspan="4">Called Class</th>
</tr>
<tr>
<th align="left"><italic>f</italic></th>
<th align="left"/>
<th align="left">Neutral</th>
<th align="left">Hard Sweep</th>
<th align="left">Soft Sweep</th>
<th align="left">Balancing</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Low</td>
<td align="left">(18.4% of datasets)</td>
<td align="char" char="."><bold>0.6820</bold></td>
<td align="char" char=".">0.3137</td>
<td align="char" char=".">0.0009</td>
<td align="char" char=".">0.0035</td>
</tr>
<tr>
<td align="left">Moderate</td>
<td align="left">(12.5% of datasets)</td>
<td align="char" char=".">0.0988</td>
<td align="char" char="."><bold>0.8220</bold></td>
<td align="char" char=".">0.0000</td>
<td align="char" char=".">0.0793</td>
</tr>
<tr>
<td align="left">High</td>
<td align="left">(69.1% of datasets)</td>
<td align="char" char=".">0.0084</td>
<td align="char" char="."><bold>0.9734</bold></td>
<td align="char" char=".">0.0044</td>
<td align="char" char=".">0.0138</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>We also wanted to test the impact of unsupervised pretraining using autoencoders (see <xref ref-type="sec" rid="sec014">Methods</xref>). Tables <xref ref-type="table" rid="pcbi.1004845.t005">5</xref> and <xref ref-type="table" rid="pcbi.1004845.t006">6</xref> compare the results for a randomly initialized network to a network initialized using autoencoders. These results demonstrate that pretraining is very effective. Due to the non-convex nature of the optimization problem, random initialization is most likely finding a poor local minima for the cost function.</p>
<table-wrap id="pcbi.1004845.t005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004845.t005</object-id>
<label>Table 5</label>
<caption>
<title>Relative error on the test dataset, for a deep network with 6 hidden layers.</title> <p>For the results in the first row, the weights of the entire network were initialized randomly, then optimized. In the second row, the weights were initialized using autoencoders for each layer. The positive impact of unsupervised pretraining is clear; random initialization causes the optimization to get stuck in a local minima.</p>
</caption>
<alternatives>
<graphic id="pcbi.1004845.t005g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.t005" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Initialization Type</th>
<th align="center"><italic>N</italic><sub>1</sub> error</th>
<th align="center"><italic>N</italic><sub>2</sub> error</th>
<th align="center"><italic>N</italic><sub>3</sub> error</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Random</td>
<td align="char" char=".">0.429</td>
<td align="char" char=".">0.421</td>
<td align="char" char=".">0.710</td>
</tr>
<tr>
<td align="left">Autoencoder</td>
<td align="char" char=".">0.061</td>
<td align="char" char=".">0.166</td>
<td align="char" char=".">0.577</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="pcbi.1004845.t006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004845.t006</object-id>
<label>Table 6</label>
<caption>
<title>Confusion matrix for the selection predictions, compared between random initialization (top) and autoencoder initialization (bottom), for a deep network with 6 hidden layers.</title> <p>Again, ideally we would like all 1’s down the diagonal, and 0’s in the off-diagonal entries. The largest number in each row is shown in boldface. When the network is initialized randomly, almost every dataset is classified as neutral; the network has not really learned anything meaningful from the input data. The overall percentage of misclassification is 74.8% for random initialization, while it is only 6.1% for autoencoder initialization.</p>
</caption>
<alternatives>
<graphic id="pcbi.1004845.t006g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.t006" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="left"/>
<th align="center" colspan="2">Called Class</th>
<th align="left"/>
</tr>
<tr>
<th align="left">True Class</th>
<th align="center">Neutral</th>
<th align="center">Hard Sweep</th>
<th align="center">Soft Sweep</th>
<th align="center">Balancing</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"/>
<td align="left"/>
<td align="center" colspan="2">Random Initialization</td>
<td align="left"/>
</tr>
<tr>
<td align="left">Neutral</td>
<td align="char" char="."><bold>1.000</bold></td>
<td align="char" char=".">0.000</td>
<td align="char" char=".">0.000</td>
<td align="char" char=".">0.000</td>
</tr>
<tr>
<td align="left">Hard Sweep</td>
<td align="char" char="."><bold>0.978</bold></td>
<td align="char" char=".">0.007</td>
<td align="char" char=".">0.000</td>
<td align="char" char=".">0.015</td>
</tr>
<tr>
<td align="left">Soft Sweep</td>
<td align="char" char="."><bold>1.000</bold></td>
<td align="char" char=".">0.000</td>
<td align="char" char=".">0.000</td>
<td align="char" char=".">0.000</td>
</tr>
<tr>
<td align="left">Balancing</td>
<td align="char" char="."><bold>1.000</bold></td>
<td align="char" char=".">0.000</td>
<td align="char" char=".">0.000</td>
<td align="char" char=".">0.000</td>
</tr>
<tr>
<td align="left"/>
<td align="left"/>
<td align="center" colspan="2">Autoencoder Initialization</td>
<td align="left"/>
</tr>
<tr>
<td align="left">Neutral</td>
<td align="char" char="."><bold>1.000</bold></td>
<td align="char" char=".">0.000</td>
<td align="char" char=".">0.000</td>
<td align="char" char=".">0.000</td>
</tr>
<tr>
<td align="left">Hard Sweep</td>
<td align="char" char=".">0.145</td>
<td align="char" char="."><bold>0.831</bold></td>
<td align="char" char=".">0.004</td>
<td align="char" char=".">0.021</td>
</tr>
<tr>
<td align="left">Soft Sweep</td>
<td align="char" char=".">0.011</td>
<td align="char" char=".">0.001</td>
<td align="char" char="."><bold>0.987</bold></td>
<td align="char" char=".">0.000</td>
</tr>
<tr>
<td align="left">Balancing</td>
<td align="char" char=".">0.030</td>
<td align="char" char=".">0.028</td>
<td align="char" char=".">0.001</td>
<td align="char" char="."><bold>0.941</bold></td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Lastly, we wanted to assess the impact of training the deep network on data that was created under different conditions than the testing data. In particular, we simulated testing data where the ratio of the recombination rate to the mutation rate was equal to 4 (instead of 1 as in our previous results). The results of this experiment are shown in <xref ref-type="supplementary-material" rid="pcbi.1004845.s001">S1 Table</xref>. This rather large difference in the testing data negatively impact the results, but much more so for selection than for the effective population sizes. This is perhaps due to the dependence of selection inference on LD and haplotype statistics, which would be more affected by recombination rate variation.</p>
<p>We also simulated testing data with bottlenecks that were more extreme than the training data, to assess the impact of a misspecified range of demographic models. Specifically, the most recent population size was outside the range of the training data for all the testing datasets. The results are shown in <xref ref-type="supplementary-material" rid="pcbi.1004845.s002">S2 Table</xref>. In contrast to the previous testing scenario, here the population size results are somewhat negatively impacted (largely for the most recent size), but the selection results are only impacted slightly. In general, identifying model misspecification for continuous population genetic parameters can be done as follows: if the predicted value of a <italic>normalized</italic> continuous parameter is outside the range [0,1], then the testing/real data parameter is outside the range of the training/simulated data parameter. This simple criterion can help users identify when their simulated data needs to be modified to provide accurate results for their real data.</p>
</sec>
<sec id="sec008">
<title>Results on real <italic>Drosophila melanogaster</italic> data</title>
<p>We then ran the real <italic>Drosophila melanogaster</italic> data through our trained network just like any other test dataset. We considered only the chromosome arms 2L, 2R, 3L, and 3R in this study. We partitioned each chromosome arm into 20 kb windows and ran our method on five consecutive windows at a time, sliding by 20 kb after each run. Classification was performed on the middle 20 kb window. The population size predictions for some of these regions (roughly 14%) were outside the range of the simulated data, so we discarded them from all further analysis. These regions tended to be near the telomeres and centromeres, where assembly is often poor.</p>
<p>For the demographic history, the population size results are shown in <xref ref-type="table" rid="pcbi.1004845.t007">Table 7</xref>. The first row (average statistic method) is our best estimate, which we also plot and compare with other histories in <xref ref-type="fig" rid="pcbi.1004845.g003">Fig 3</xref>. Our history is close to the PSMC result, although more resolution would be needed for a proper comparison. We based our time change points (<italic>t</italic><sub>1</sub> and <italic>t</italic><sub>2</sub>) on PSMC, so it would be surprising if these histories were notably inconsistent. The expansion after the bottleneck is roughly consistent with previous results citing the range expansion of <italic>Drosophila melanogaster</italic> (out of sub-Saharan Africa) as beginning around 15,000 years ago [<xref ref-type="bibr" rid="pcbi.1004845.ref061">61</xref>]. This range expansion likely led to an effective population size increase like the one we infer. Our expansion time is also consistent with bottleneck estimates in non-African <italic>Drosophila melanogaster</italic> populations, such as the Netherlands population investigated by Thornton and Andolfatto [<xref ref-type="bibr" rid="pcbi.1004845.ref058">58</xref>]. In <xref ref-type="fig" rid="pcbi.1004845.g002">Fig 2B</xref>, we include a violin plot of the population size estimates from each region. We have the largest uncertainty for the most recent population size.</p>
<table-wrap id="pcbi.1004845.t007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004845.t007</object-id>
<label>Table 7</label>
<caption>
<title>Population size results for African <italic>Drosophila</italic> from Zambia, rounded to the nearest hundred.</title> <p>The first row (predictions based on averaging the statistics for each region in the <italic>Drosophila</italic> genome) represents our best estimate of the population sizes.</p>
</caption>
<alternatives>
<graphic id="pcbi.1004845.t007g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.t007" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Prediction</th>
<th align="center"><italic>N</italic><sub>1</sub> (recent)</th>
<th align="center"><italic>N</italic><sub>2</sub> (bottleneck)</th>
<th align="center"><italic>N</italic><sub>3</sub> (ancestral)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Average stat prediction</td>
<td align="center">544,200</td>
<td align="center">145,300</td>
<td align="center">652,700</td>
</tr>
<tr>
<td align="left">Final prediction</td>
<td align="center">694,000</td>
<td align="center">178,400</td>
<td align="center">638,300</td>
</tr>
<tr>
<td align="left">Quantiles (2.5th, 97.5th)</td>
<td align="center">(359300, 1004300)</td>
<td align="center">(63800, 265000)</td>
<td align="center">(593200, 694900)</td>
</tr>
<tr>
<td align="left">Standard deviation</td>
<td align="center">170,400</td>
<td align="center">53,800</td>
<td align="center">27,100</td>
</tr>
<tr>
<td align="left">Neutral regions prediction</td>
<td align="center">635,900</td>
<td align="center">170,700</td>
<td align="center">646,700</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<fig id="pcbi.1004845.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004845.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Comparison between demographic histories. On the left in green is PSMC [<xref ref-type="bibr" rid="pcbi.1004845.ref013">13</xref>], run on the entire genome for a subset of the data (<italic>n</italic> = 20).</title>
<p>In blue is our history from the first line of <xref ref-type="table" rid="pcbi.1004845.t007">Table 7</xref>. On the right in red is the history from Duchen <italic>et al.</italic> [<xref ref-type="bibr" rid="pcbi.1004845.ref003">3</xref>] (note the change in the <italic>y</italic>-axis scale). The difference between our estimates and these previous estimates is likely due in large part to their assumption of a very short (1000 generations) bottleneck. Given this timing constraint, only a very severe bottleneck could fit the data. A more gradual bottleneck seems more realistic, although we do not have a simple explanation for why there was a bottleneck at all.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.g003" xlink:type="simple"/>
</fig>
<p>The number of windows classified as Neutral, Hard Sweep, Soft Sweep, and Balancing Selection are 1101, 2455, 179, 404, respectively. See <xref ref-type="supplementary-material" rid="pcbi.1004845.s003">S3 Table</xref> for further details. If we restrict our analysis to regions classified with probability greater than 0.9999, then we find 42 hard sweeps, 34 soft sweeps, and 17 regions under balancing selection. In <xref ref-type="supplementary-material" rid="pcbi.1004845.s004">S4 Table</xref>, we include a table of these high-confidence windows, along with which genes are found in each window. We also include a plot, <xref ref-type="fig" rid="pcbi.1004845.g004">Fig 4</xref>, of where the selected regions fall throughout the genome (restricted to regions with a probability at least 0.95). Interestingly, soft sweeps and balancing selection seem to occur more frequently closer to the centromere of each chromosome. There is also an excess of hard sweeps on chromosome arm 2L.</p>
<fig id="pcbi.1004845.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004845.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Plot of the selected regions we infer for African <italic>Drosophila melanogaster</italic>.</title>
<p>Each of the 4 subplots is a chromosome arm (2L, 2R, 3L and 3R). We restricted the plot to selected sites with a probability of at least 0.95.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.g004" xlink:type="simple"/>
</fig>
<p>Upon examining the genes in the top regions classified to be under selection, we find several notable results. In the hard sweep category, a few of the top regions harbor many genes involved in chromatin assembly or disassembly. Further, there are two regions each containing exactly one gene, where the gene has a known function. These are <italic>Fic domain-containing protein</italic> (detection of light and visual behavior), and <italic>charybde</italic> (negative regulation of growth).</p>
<p>In the soft sweep category, we find many genes related to transcription, and several related to pheromone detection (chemosensory proteins). We also find four regions each containing exactly one gene, with that gene having a known function. These are <italic>gooseberry</italic> (segment polarity determination), <italic>steppke</italic> (positive regulation of growth), <italic>Krüppel</italic> (proper body formation), and <italic>Accessory gland protein 36DE</italic> (sperm storage, [<xref ref-type="bibr" rid="pcbi.1004845.ref062">62</xref>]). Fly embryos with a mutant <italic>Krüppel</italic> (German for “cripple”) gene have a “gap” phenotype with the middle body segments missing [<xref ref-type="bibr" rid="pcbi.1004845.ref063">63</xref>]. We also identify three interesting genes in regions with other genes: <italic>seminal fluid proteins 36F</italic> and <italic>38D</italic> (see [<xref ref-type="bibr" rid="pcbi.1004845.ref064">64</xref>]), and <italic>deadlock</italic>, which is involved in transposable element silencing, which preserves genome integrity [<xref ref-type="bibr" rid="pcbi.1004845.ref065">65</xref>].</p>
<p>Finally in the balancing selection category, one interesting gene we find is <italic>nervana 3</italic>, which is involved in the sensory perception of sound. We also find balancing selection regions containing the genes <italic>cycle</italic> (circadian rhythms) and <italic>Dihydropterin deaminase</italic> (eye pigment), although there are other genes within these putatively selected regions.</p>
</sec>
<sec id="sec009">
<title>Most informative statistics</title>
<p>One of the advantages of deep learning is that it outputs an optimal weight on each edge connecting two nodes of the network, starting with the input statistics and ending with the output variables. By investigating this network, we are able to determine which statistics are the “best” or most helpful for estimating our parameters of interest. Here we use two different methods, one using permutation testing and the other using a perturbation approach described in <xref ref-type="supplementary-material" rid="pcbi.1004845.s009">S1 Algorithm</xref>.</p>
<p>To perform permutation testing, we randomly permuted the values of each statistic across all the test datasets and then measured the resulting decrease in accuracy. For each output (3 population sizes and selection category), we then retained the 25 statistics that caused the largest accuracy decreases. The results are shown in the 4-way Venn diagram in <xref ref-type="fig" rid="pcbi.1004845.g005">Fig 5</xref>, which highlights statistics common to each subset of outputs. For each statistic name in the diagram, <italic>close</italic>, <italic>mid</italic>, and <italic>far</italic> represent the genomic subregion where the statistic was calculated. The numbers after each colon refer to the position of the statistic within its distribution or order; for the folded site frequency spectrum (SFS) statistics, it is the number of minor alleles. Several statistics are informative for all the parameters, including the raw number of segregating sites (“S”) in each subregion and the number of singletons (“SFS, far: 1”). There is generally more overlap between the statistics for selection and <italic>N</italic><sub>1</sub> or <italic>N</italic><sub>2</sub>, than with the statistics for selection and <italic>N</italic><sub>3</sub>, possibly because selection occurs on a more recent timescale. Tajima’s D is also most important for selection (which it was designed to detect) and <italic>N</italic><sub>1</sub>. Identity-by-state (IBS) statistics seem to play an increasingly more important role as the population sizes become more ancient. Perhaps unsurprisingly, LD statistics are most important for selection. Interestingly, the SFS is not as informative as one might have anticipated. However, the number of singletons is very informative, especially for selection and <italic>N</italic><sub>1</sub>, which could have been foreseen given that recent events shape the terminal branches of a genealogy, and thus the singletons. The distances between segregating sites (“BET” statistics) do not generally seem very helpful. Neither does H2, the frequency of the second most common haplotype. It is comparing H1 to H2 (via the H12 statistic) that is helpful.</p>
<fig id="pcbi.1004845.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004845.g005</object-id>
<label>Fig 5</label>
<caption>
<title>A Venn diagram of most informative statistics for each output variable (<italic>N</italic><sub>1</sub>, <italic>N</italic><sub>2</sub>, <italic>N</italic><sub>3</sub>, and selection).</title>
<p>For each variable, the top 25 statistics were chosen using permutation testing. The Venn diagram captures statistics common to each subset of output variables, with notable less informative statistics shown in the lower right. Close, mid, and far represent the genomic region where the statistic was calculated. The numbers after each colon refer to the position of the statistic within its distribution or order. For the SFS statistics, it is number of minor alleles. For each region, there are 50 SFS statistics, 16 BET statistics (distribution between segregating sites), 30 IBS statistics, and 16 LD statistics.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.g005" xlink:type="simple"/>
</fig>
<p>The results (<xref ref-type="supplementary-material" rid="pcbi.1004845.s006">S2 Fig</xref>) from the perturbation method share some similarities and have some differences from the Venn diagram in <xref ref-type="fig" rid="pcbi.1004845.g005">Fig 5</xref>. An interesting set of four statistics is informative for all response variables, including very long IBS tracts close to the selected site (“IBS, close: 30”) and the H12 statistic. Additionally, “LD, mid: 4” represents low LD between sites close to the selected site and sites mid-range from the selected site (likewise for LD “LD, far: 4”). Many low LD pairs could signal a lack of selection, and vice versa. IBS statistics are generally not as helpful for selection, but extremely informative for the population sizes, especially <italic>N</italic><sub>2</sub> and <italic>N</italic><sub>3</sub>, as also observed in the permutation method. Bottlenecks can have a significant impact on IBS tract distributions, so it makes sense that <italic>N</italic><sub>2</sub> (the bottleneck size) is the most reliant on IBS statistics. The number of segregating sites <italic>S</italic> is generally quite informative, especially for selection.</p>
<p>It is interesting that the list of notable less-informative statistics is very similar for both permutation and perturbation methods. Overall, roughly the same set of informative statistics were identified by each method, but the precise position of each statistic on the Venn diagram shifted.</p>
</sec>
<sec id="sec010">
<title>Comparison with ABCtoolbox</title>
<p>Although ABC is not well suited for our scenario of interest and deep learning is a complementary method, we wanted to find a scenario where we could compare the performance of these two methods. To this effect, we restricted the analysis to estimating (continuous) demographic parameters only. We used the popular ABCtoolbox [<xref ref-type="bibr" rid="pcbi.1004845.ref054">54</xref>], using the same training and testing datasets as for deep learning. For ABC, the training data represents the data simulated under the prior distributions (uniform in our case), and each test dataset was compared with the training data separately. We retained 5% of the training datasets, and used half of these retained datasets for posterior density estimation. Overall, we used 75% of the datasets for training and 25% for testing.</p>
<p>We tested two scenarios, one with the full set of summary statistics (345 total), and the other with a reduced set of summary statistics (100 total). For the reduced set of summary statistics, we chose statistics which seemed to be informative: the number of segregating sites, Tajima’s <italic>D</italic>, the first 15 entries of site frequency spectrum, H1, and the distribution of distances between segregating sites. It is interesting that the informative statistics identified above do not match this set of “intuitively” chosen statistics. The results are shown in <xref ref-type="table" rid="pcbi.1004845.t008">Table 8</xref>, which suggest that deep learning produces more accurate estimates of the recent population size (<italic>N</italic><sub>1</sub>) than does ABCtoolbox, whereas they have comparable accuracies for more ancient sizes.</p>
<table-wrap id="pcbi.1004845.t008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004845.t008</object-id>
<label>Table 8</label>
<caption>
<title>A comparison between ABCtoolbox and deep learning for demography only.</title> <p>Out of 1000 demographies (160,000 datasets total), 75% were used for training and 25% for testing. In this scenario, deep learning generally outperforms ABCtoolbox, as measured by the relative error: |<italic>N</italic><sub>est</sub> − <italic>N</italic><sub>true</sub>|/<italic>N</italic><sub>true</sub>. There is generally more improvement using deep learning when the number of statistics is larger.</p>
</caption>
<alternatives>
<graphic id="pcbi.1004845.t008g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.t008" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Dataset</th>
<th align="left">Method</th>
<th align="center"><italic>N</italic><sub>1</sub> error</th>
<th align="center"><italic>N</italic><sub>2</sub> error</th>
<th align="center"><italic>N</italic><sub>3</sub> error</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Full summary statistics</td>
<td align="left">ABCtoolbox</td>
<td align="char" char=".">0.062</td>
<td align="char" char=".">0.043</td>
<td align="char" char=".">0.218</td>
</tr>
<tr>
<td align="left"/>
<td align="left">Deep learning</td>
<td align="char" char=".">0.044</td>
<td align="char" char=".">0.028</td>
<td align="char" char=".">0.221</td>
</tr>
<tr>
<td align="left">Filtered summary statistics</td>
<td align="left">ABCtoolbox</td>
<td align="char" char=".">0.161</td>
<td align="char" char=".">0.035</td>
<td align="char" char=".">0.311</td>
</tr>
<tr>
<td align="left"/>
<td align="left">Deep learning</td>
<td align="char" char=".">0.065</td>
<td align="char" char=".">0.055</td>
<td align="char" char=".">0.319</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec011">
<title>Regularization of the network weights</title>
<p>In deep learning, one hyper-parameter that should be investigated closely is the regularization parameter λ in the cost function, also called the weight decay parameter. (See <xref ref-type="sec" rid="sec014">Methods</xref> for details). If λ is set to be too high, large weights will be penalized too much, and interesting features of the data cannot be learned well. But if λ is set too low, the weights tend to display runaway behavior. Due to this balance, a validation procedure is typically used to find the right λ. In our case, the additional runtime of simulating more data and performing more training would be too computationally expensive, but we do provide a small validation study in <xref ref-type="supplementary-material" rid="pcbi.1004845.s007">S3 Fig</xref></p>
</sec>
<sec id="sec012">
<title>Runtime</title>
<p>In terms of runtime, the majority is spent simulating the data. During training, most of the runtime is spent fine-tuning the deep network, which requires computing the cost function and derivatives many times. To speed up this computation, our deep learning implementation is parallelized across datasets, since each dataset adds to the cost function independently. This significantly improves the training time for deep learning, which can be run in a few days on our simulated dataset (400,000 regions) with a modest number of hidden nodes/layers. Once the training is completed, an arbitrary number of datasets can be tested more or less instantaneously. In contrast, each of the “training” datasets for ABC must be examined for <italic>each</italic> test dataset. This takes several weeks for a dataset of this size (which is why we tested ABC on a subset of the data), although it could be parallelized across the test datasets. We include a runtime comparison on the reduced dataset in <xref ref-type="table" rid="pcbi.1004845.t009">Table 9</xref>.</p>
<table-wrap id="pcbi.1004845.t009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004845.t009</object-id>
<label>Table 9</label>
<caption>
<title>Approximate runtime results.</title>
<p>Simulating the data (which is the same for both methods) and computing the summary statistics is the most time consuming part of the process, although it is highly parallelized. ABCtoolbox could also be parallelized, but we did not implement that here. For the “filtered statistics” row, the 345 statistics have been downsampled to 100 for all the datasets.</p>
</caption>
<alternatives>
<graphic id="pcbi.1004845.t009g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.t009" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Task</th>
<th align="center">ABCtoolbox</th>
<th align="center">Deep Learning</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Simulating data</td>
<td align="center">370 hrs (10 ∼ 15 cores)</td>
<td align="center">370 hrs (10 ∼ 15 cores)</td>
</tr>
<tr>
<td align="left">Computing summary statistics</td>
<td align="center">1800 hrs (1 core)</td>
<td align="center">1800 hrs (1 core)</td>
</tr>
<tr>
<td align="left"><bold>Demography only</bold> (1000 × 160 datasets)</td>
<td align="center"/>
<td align="center"/>
</tr>
<tr>
<td align="left"> Training and testing (filtered statistics)</td>
<td align="center">114 hrs (1 core)</td>
<td align="center">3.75 hrs (20 cores)</td>
</tr>
<tr>
<td align="left"> Training and testing (unfiltered)</td>
<td align="center">336 hrs (1 core)</td>
<td align="center">11 hrs (20 cores)</td>
</tr>
<tr>
<td align="left"><bold>Demography &amp; selection</bold> (2500 × 160 datasets)</td>
<td align="center"/>
<td align="center"/>
</tr>
<tr>
<td align="left"> Training</td>
<td align="center">N/A</td>
<td align="center">74 hrs (20 cores)</td>
</tr>
<tr>
<td align="left"> Testing</td>
<td align="center">N/A</td>
<td align="center">3 min (1 core)</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
</sec>
<sec id="sec013" sec-type="conclusions">
<title>Discussion</title>
<p>In this paper, we have sought to demonstrate the potential of deep learning as a new inference framework for population genomic analysis. This investigation is still in its infancy, and there are many directions of future work. One important advantage of deep learning is that it provides a way to distinguish informative summary statistics from uninformative ones. Here we have presented two methods for extracting informative statistics given a trained deep network. Other methods are possible, and it is an open question which one would produce the most accurate results. It would be interesting to down-sample statistics using a variety of methods, then compare the results. Overall, learning more about how various summary statistics relate to parameters would be useful for population genetics going forward.</p>
<p>The prospect of using deep learning to classify regions as neutral or selected is very appealing for subsequent demographic inference. There are other machine learning methods that perform such classification, but they are generally limited to two classes (selected or neutral). One exception is a study in humans [<xref ref-type="bibr" rid="pcbi.1004845.ref066">66</xref>], which classifies genomic regions as neutral or under positive, negative, or balancing selection. Although their approach does not jointly infer selection and demography, it would be interesting to see their method used on <italic>Drosophila</italic>.</p>
<p>We infer many hard sweeps in African <italic>Drosophila</italic>, which is perhaps expected given their large effective population size. However, when restricting our analysis to selected regions with high confidence, the numbers of hard sweeps and soft sweeps are comparable. It would be interesting to analyze our results in the context of a simulation study by Schrider <italic>et al.</italic> [<xref ref-type="bibr" rid="pcbi.1004845.ref067">67</xref>], which found that regions classified as soft sweeps are often truly the “shoulders” of hard sweeps. This possibility is worth investigating, as the signatures of soft sweeps and soft shoulders are extremely similar. In our simulations, both hard and soft sweeps are relatively recent, which makes them easier to detect. It would be useful to incorporate datasets with a wider range of selection onset times, which could capture regions of the real data that are currently being classified as neutral. Right now we have 4 different selection classes which are all forms of positive selection. To incorporate background or purifying selection, a 5th class of (weakly) deleterious mutations could be added.</p>
<p>One aspect of the simulations that warrants future study is the effect of the ratio of recombination to mutation rate, <italic>ρ</italic>/<italic>θ</italic>. From running PSMC on 20 haplotypes from Zambia, we inferred <italic>ρ</italic>/<italic>θ</italic> = 1, whereas other studies have found <italic>ρ</italic>/<italic>θ</italic> = 6.8 [<xref ref-type="bibr" rid="pcbi.1004845.ref059">59</xref>] and <italic>ρ</italic>/<italic>θ</italic> = 10 [<xref ref-type="bibr" rid="pcbi.1004845.ref058">58</xref>] for a population from Zimbabwe. Both these works also show significant variation in this ratio across different <italic>Drosophila</italic> populations. Andolfatto and Przeworski [<xref ref-type="bibr" rid="pcbi.1004845.ref060">60</xref>] also find significant variation in this ratio across the genome. As noted by Haddrill <italic>et al.</italic> [<xref ref-type="bibr" rid="pcbi.1004845.ref059">59</xref>], recent bottlenecks can make this ratio appear lower than it actually is, which is possibly the case for our dataset. As shown in the Supporting Information, misspecification of this ratio can lead to poor selection classification results. In general, assuming a lower recombination rate than reality leads to conservative results, since sites are assumed to be less independent [<xref ref-type="bibr" rid="pcbi.1004845.ref058">58</xref>–<xref ref-type="bibr" rid="pcbi.1004845.ref060">60</xref>]. Our takeaway from this analysis is that <italic>ρ</italic>/<italic>θ</italic> should be accurately estimated using a more robust method than PSMC or incorporated as a model parameter, with datasets simulated under a variety of ratios. It would also be interesting to investigate variation in the recombination rate across the genome, which could be incorporated into future simulations.</p>
<p>Deep learning can make efficient use of even a limited number of simulated datasets. In this vein, it would be interesting to use an approach such as ABC MCMC [<xref ref-type="bibr" rid="pcbi.1004845.ref024">24</xref>] to simulate data, and then use deep learning on these simulated datasets. Alternatively, deep learning could be used to select informative statistics for a subsequent method such as ABC [<xref ref-type="bibr" rid="pcbi.1004845.ref049">49</xref>]. Blum and François [<xref ref-type="bibr" rid="pcbi.1004845.ref028">28</xref>] used a single-layer neural network to perform (non-linear) regression to obtain a posterior mean, which they used to adjust the parameters of the posterior sample for a more accurate posterior distribution. It is unclear whether deep networks (where the theory is much more complicated) could be used in the same way, although there is the concept of a residual which could be used for parameter adjustment in an rejection-based framework. Such hybrid approaches could be a fruitful area of further exploration, potentially extending the generality and usefulness of ABC.</p>
<p>We would also like to apply deep learning to a wider variety of scenarios in populations genetics. Population structure and splits would be examples, although these scenarios would most likely require very different sets of summary statistics.</p>
<p>On the computer science side, deep learning has almost exclusively been used for classification, not continuous parameter inference. It would be interesting to see the type of continuous parameter inference presented here used in other fields and applications.</p>
<p>Finally, machine learning methods have been criticized for their “black-box” nature. In some sense they throw away a lot of the coalescent modeling that we know to be realistic, although this is included somewhat in the expert summary statistics of the data. It would be advantageous to somehow combine the strengths of coalescent theory and the strengths of machine learning to create a robust method for population genetic inference.</p>
</sec>
<sec id="sec014" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec015">
<title>Deep learning details</title>
<p>In this section we provide the theory behind training deep networks. The notation in this section follows that of [<xref ref-type="bibr" rid="pcbi.1004845.ref068">68</xref>]. Let <bold><italic>x</italic></bold><sup>(<italic>i</italic>)</sup> be the vector of summary statistics for dataset <italic>i</italic>, and <bold><italic>y</italic></bold><sup>(<italic>i</italic>)</sup> the vector of response variables that dataset <italic>i</italic> was simulated under. If we have <italic>m</italic> such datasets, then together {(<bold><italic>x</italic></bold><sup>(1)</sup>, <bold><italic>y</italic></bold><sup>(1)</sup>), …, (<bold><italic>x</italic></bold><sup>(<italic>m</italic>)</sup>, <bold><italic>y</italic></bold><sup>(<italic>m</italic>)</sup>)} form the training data that will be used to learn the function from summary statistics to response variables. Deep neural networks are a way to express this type of complex, non-linear function. The first layer of the network is the input data, the next layers are the “hidden layers” of the network, and the final layer represents the network’s prediction of the response variable.</p>
<sec id="sec016">
<title>Cost function for a deep network</title>
<p>An example deep network is shown in <xref ref-type="fig" rid="pcbi.1004845.g006">Fig 6</xref>. The collection of weights between layer <italic>ℓ</italic> and layer <italic>ℓ</italic>+1 is denoted <inline-formula id="pcbi.1004845.e002"><alternatives><graphic id="pcbi.1004845.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1004845.e003"><alternatives><graphic id="pcbi.1004845.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> is the weight associated with the connection between node <italic>j</italic> in layer <italic>ℓ</italic> and node <italic>k</italic> in layer <italic>ℓ</italic>+1. The biases for layer <italic>ℓ</italic> is denoted <bold><italic>b</italic></bold><sup>(<italic>ℓ</italic>)</sup>. The total number of layers (including the input and output layers) is denoted <italic>L</italic>, and the number of hidden nodes in layer <italic>ℓ</italic> is denoted <italic>u</italic><sub><italic>ℓ</italic></sub>. The main goal is to learn the weights that best describe the function between the inputs and the outputs.</p>
<fig id="pcbi.1004845.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004845.g006</object-id>
<label>Fig 6</label>
<caption>
<title>An example of a deep neural network with two hidden layers.</title>
<p>The first layer is the input data (each dataset has 5 statistics), and the last layer predicts the 2 response variables. The last node in each input layer (+1) represents the bias term. Here the number of layers <italic>L</italic> = 4, and the number of nodes (computational units) in each layer is <italic>u</italic><sub>1</sub> = 5, <italic>u</italic><sub>2</sub> = 3, <italic>u</italic><sub>3</sub> = 3, and <italic>u</italic><sub>4</sub> = 2 (these counts exclude the biases).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.g006" xlink:type="simple"/>
</fig>
<p>To learn this function, we first describe how the values of the hidden nodes are computed, given a trial weight vector. The value of hidden node <italic>j</italic> in layer <italic>ℓ</italic> ≥ 2 is denoted <inline-formula id="pcbi.1004845.e004"><alternatives><graphic id="pcbi.1004845.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:msubsup><mml:mi>a</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, and is defined by
<disp-formula id="pcbi.1004845.e005"><alternatives><graphic id="pcbi.1004845.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mrow><mml:msubsup><mml:mi>a</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mfenced close=")" open="("><mml:msubsup><mml:mi>z</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced> <mml:mspace width="1.em"/><mml:mtext>and</mml:mtext> <mml:mspace width="1.em"/><mml:msubsup><mml:mi>z</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:msup><mml:mi>a</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>b</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula>
where <inline-formula id="pcbi.1004845.e006"><alternatives><graphic id="pcbi.1004845.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> is the <italic>j</italic><sup>th</sup> <italic>column</italic> of the weight matrix <inline-formula id="pcbi.1004845.e007"><alternatives><graphic id="pcbi.1004845.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msup><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> (i.e., all the weights going into node <italic>j</italic> of layer <italic>ℓ</italic> − 1), <inline-formula id="pcbi.1004845.e008"><alternatives><graphic id="pcbi.1004845.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">a</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>a</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is a vector of the values of all the nodes in layer <italic>ℓ</italic> − 1 (with <inline-formula id="pcbi.1004845.e009"><alternatives><graphic id="pcbi.1004845.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">a</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, the input vector), and
<disp-formula id="pcbi.1004845.e010"><alternatives><graphic id="pcbi.1004845.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mrow><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>:</mml:mo> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mtext>exp</mml:mtext> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mi>z</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula>
is the <italic>activation function</italic>. Here we use a logistic function, but other functions can be used. Another common activation function is the hyperbolic tangent function.</p>
<p>Hence, given the input data and a set of weights, we can <italic>feed forward</italic> to learn the values of all hidden nodes, and a prediction of the response variables. These predictions are usually denoted by <bold><italic>h</italic></bold><sub><bold><italic>W</italic>, <italic>b</italic></bold></sub>(<bold><italic>x</italic></bold><sup>(<italic>i</italic>)</sup>) for our <italic>hypothesis</italic> for dataset <italic>i</italic>, based on all the weights <bold><italic>W</italic></bold> = (<bold><italic>W</italic></bold><sup>(1)</sup>, …,<bold><italic>W</italic></bold><sup>(<italic>L</italic>-1)</sup>) and biases <bold><italic>b</italic></bold> = (<bold><italic>b</italic></bold><sup>(1)</sup>, …,<bold><italic>b</italic></bold><sup>(<italic>L</italic>-1)</sup>). We will discuss different ways to compute the hypothesis function later on. To find the best weights, we define a loss function based on the square of the <italic>l</italic><sub>2</sub>-norm between this hypothesis and the true response variables. This loss function is given by
<disp-formula id="pcbi.1004845.e011"><alternatives><graphic id="pcbi.1004845.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mrow><mml:mi>J</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>m</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>‖</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">h</mml:mi> <mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:msup><mml:mo>‖</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
The goal of deep learning is to find the weights (<bold><italic>W</italic></bold> and <bold><italic>b</italic></bold>) that minimize this loss function. To efficiently find these optimal weights, we can use <italic>backpropagation</italic> to find the gradient. The intuition behind this approach is that once we have found the hypothesis, we then want to see how much each of the weights contributed to any differences between the hypothesis and the truth. Therefore we start at the last hidden layer, see how much each of those weights contributed, then work our way backwards, using the gradient of the previous layer to compute the gradient of the next layer. For this procedure we need to compute the partial derivatives of the cost function with respect to each weight.</p>
<p>Consider a single training example <bold><italic>x</italic></bold> with associated output <bold><italic>y</italic></bold>. The cost for this dataset is a term in <xref ref-type="disp-formula" rid="pcbi.1004845.e011">Eq (3)</xref> and is denoted by
<disp-formula id="pcbi.1004845.e012"><alternatives><graphic id="pcbi.1004845.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mrow><mml:mi>J</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>:</mml:mo> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>‖</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">h</mml:mi> <mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold-italic">y</mml:mi> <mml:msup><mml:mo>‖</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
Then, define
<disp-formula id="pcbi.1004845.e013"><alternatives><graphic id="pcbi.1004845.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e013" xlink:type="simple"/><mml:math display="block" id="M13"><mml:mrow><mml:msubsup><mml:mi>δ</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>:</mml:mo> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mi>J</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <inline-formula id="pcbi.1004845.e014"><alternatives><graphic id="pcbi.1004845.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:msubsup><mml:mi>z</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, defined in <xref ref-type="disp-formula" rid="pcbi.1004845.e005">Eq (1)</xref>, is the input to the activation function for node <italic>j</italic> in layer <italic>ℓ</italic>. We first consider <inline-formula id="pcbi.1004845.e015"><alternatives><graphic id="pcbi.1004845.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:msubsup><mml:mi>δ</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> for the final layer. Noting that the <italic>j</italic><sup>th</sup> entry of <bold><italic>h</italic></bold><sub><bold><italic>W</italic>, <italic>b</italic></bold></sub>(<bold><italic>x</italic></bold>) is <inline-formula id="pcbi.1004845.e016"><alternatives><graphic id="pcbi.1004845.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, we get
<disp-formula id="pcbi.1004845.e017"><alternatives><graphic id="pcbi.1004845.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:mrow><mml:msubsup><mml:mi>δ</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>z</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>]</mml:mo></mml:mrow> <mml:msup><mml:mi>f</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mfenced close=")" open="("><mml:msubsup><mml:mi>z</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
Based on this initialization, we can recursively compute all the <italic>δ</italic> variables:
<disp-formula id="pcbi.1004845.e018"><alternatives><graphic id="pcbi.1004845.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e018" xlink:type="simple"/><mml:math display="block" id="M18"><mml:mrow><mml:msubsup><mml:mi>δ</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:mfenced close="]" open=" [" separators=""><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>u</mml:mi> <mml:mrow><mml:mi>ℓ</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:munderover> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:msubsup><mml:mi>δ</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced> <mml:msup><mml:mi>f</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mfenced close=")" open="("><mml:msubsup><mml:mi>z</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
Now we can use the <italic>δ</italic> variables to recursively compute the partial derivatives for one dataset:
<disp-formula id="pcbi.1004845.e019"><alternatives><graphic id="pcbi.1004845.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e019" xlink:type="simple"/><mml:math display="block" id="M19"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>a</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>δ</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>ℓ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>δ</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ℓ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives></disp-formula>
Finally, putting all the datasets together we get
<disp-formula id="pcbi.1004845.e020"><alternatives><graphic id="pcbi.1004845.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mi>J</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mi>J</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac> <mml:mspace width="1.em"/><mml:mtext>and</mml:mtext> <mml:mspace width="1.em"/><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mi>J</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msubsup><mml:mi>b</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mi>J</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:msubsup><mml:mi>b</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
Since we can compute the derivatives using this backpropagation algorithm, we can use the LBFGS optimization routine (as implemented in [<xref ref-type="bibr" rid="pcbi.1004845.ref069">69</xref>]) to find the weights that minimize the cost function.</p>
</sec>
<sec id="sec017">
<title>Unsupervised pretraining using autoencoders</title>
<p>It is possible to train a deep network by attempting to minimize the cost function described above directly, but in practice, this proved difficult due to the high-dimensionality and non-convexity of the optimization problem. Initializing the weights randomly before training resulted in poor local minima. Hinton and Salakhutdinov [<xref ref-type="bibr" rid="pcbi.1004845.ref037">37</xref>] sought to initialize the weights in a more informed way, using an unsupervised pretraining routine. Unsupervised training ignores the output (often called the “labels”) and attempts to learn as much as possible about the structure of the data on its own. PCA is an example of unsupervised learning. In Hinton and Salakhutdinov, the unsupervised pretraining step uses an <italic>autoencoder</italic> to try to learn the best function from the data to itself, after it has gone through a dimensionality reduction step (this can be thought of as trying to compress the data, then reconstruct it with minimal loss). Autoencoding provides a way to initialize the weights of a deep network that will ideally be close to optimal for the supervised learning step as well. See <xref ref-type="supplementary-material" rid="pcbi.1004845.s008">S4 Fig</xref> for a diagram of an autoencoder.</p>
<p>Training an autoencoder is an optimization procedure in itself. As before, let <bold><italic>W</italic></bold><sup>(1)</sup> be the vector of weights connecting the input <bold><italic>x</italic></bold> to the hidden layer <bold><italic>a</italic></bold>, and <bold><italic>x</italic></bold><sup>(2)</sup> be the vector of weights connecting <bold><italic>a</italic></bold> to the output layer <inline-formula id="pcbi.1004845.e021"><alternatives><graphic id="pcbi.1004845.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, which in this case should be as close as possible to the original input data. We again typically use the logistic function shown in <xref ref-type="disp-formula" rid="pcbi.1004845.e010">Eq (2)</xref> as our activation function <italic>f</italic>, so we can compute the output using:
<disp-formula id="pcbi.1004845.e022"><alternatives><graphic id="pcbi.1004845.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mrow><mml:msub><mml:mi>a</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>b</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="1.em"/><mml:mtext>and</mml:mtext> <mml:mspace width="1.em"/><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold-italic">a</mml:mi> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>b</mml:mi> <mml:mi>k</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
If a linear activation function is used instead of a logistic function, the hidden layer becomes the principle components of the data. This makes dimensionality reduction with an autoencoder similar in spirit to PCA, which has been used frequently in genetic analysis (see [<xref ref-type="bibr" rid="pcbi.1004845.ref070">70</xref>] for an example). However, the non-linear nature of an autoencoder has been shown to reconstruct complex data more accurately than PCA. Using backpropagation as we did before, we can minimize the following autoencoder cost function using all <italic>m</italic> input datasets:
<disp-formula id="pcbi.1004845.e023"><alternatives><graphic id="pcbi.1004845.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mrow><mml:mi>A</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>m</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>‖</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:msup><mml:mo>‖</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
The resulting weights <bold><italic>W</italic></bold>*<sup>(1)</sup> and <bold><italic>b</italic></bold>*<sup>(1)</sup> will then be used to initialize the weights between the first and second layers of our deep network. The weights <bold><italic>W</italic></bold>*<sup>(2)</sup>&gt; and <bold><italic>b</italic></bold>*<sup>(2)</sup> are discarded. To initialize the rest of the weights, we can repeat the autoencoder procedure, but this time we will use the hidden layer <inline-formula id="pcbi.1004845.e024"><alternatives><graphic id="pcbi.1004845.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">a</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>a</mml:mi> <mml:mi>j</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, where <inline-formula id="pcbi.1004845.e025"><alternatives><graphic id="pcbi.1004845.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mrow><mml:msubsup><mml:mi>a</mml:mi> <mml:mi>j</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>*</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>·</mml:mo> <mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mo>+</mml:mo> <mml:msubsup><mml:mrow><mml:mi>b</mml:mi></mml:mrow> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>*</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, as our input data, and feed it through the next hidden layer. In this way we can use “stacked” autoencoders to initialize all the weights of the deep network. Finally, the supervised training procedure described in the previous section can be used to <italic>fine-tune</italic> the weights to obtain the best function from the inputs to the response variables.</p>
<p>When the number of hidden nodes is large, we would like to constrain an autoencoder such that only a fraction of the hidden nodes are “firing” at any given time. This corresponds to the idea that only a subset of the neurons in our brains are firing at once, depending on the input stimulus. To create a similar phenomenon for an autoencoder, we create a <italic>sparsity</italic> constraint that ensures the activation of most of the nodes is close to 0, and the activation of a small fraction, <italic>ρ</italic>, of nodes is close to 1. Let <inline-formula id="pcbi.1004845.e026"><alternatives><graphic id="pcbi.1004845.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:msub><mml:mover accent="true"><mml:mi>ρ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula> be the average activation of the hidden node <italic>j</italic>:
<disp-formula id="pcbi.1004845.e027"><alternatives><graphic id="pcbi.1004845.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e027" xlink:type="simple"/><mml:math display="block" id="M27"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>ρ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>m</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:msub><mml:mi>a</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <inline-formula id="pcbi.1004845.e028"><alternatives><graphic id="pcbi.1004845.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:msub><mml:mi>a</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the value of the <italic>j</italic><sup>th</sup> hidden node when activated with dataset <bold><italic>x</italic></bold><sup>(<italic>i</italic>)</sup>. To ensure sparsity, we would like <inline-formula id="pcbi.1004845.e029"><alternatives><graphic id="pcbi.1004845.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:msub><mml:mover accent="true"><mml:mi>ρ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula> to be close to <italic>ρ</italic>, our desired fraction of active nodes. This can be accomplished by minimizing the KL divergence:
<disp-formula id="pcbi.1004845.e030"><alternatives><graphic id="pcbi.1004845.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e030" xlink:type="simple"/><mml:math display="block" id="M30"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>u</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:munderover> <mml:mtext>KL</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ρ</mml:mi> <mml:mo>‖</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>ρ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>u</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:munderover> <mml:mfenced close="]" open=" [" separators=""><mml:mi>ρ</mml:mi> <mml:mo form="prefix">log</mml:mo> <mml:mfrac><mml:mi>ρ</mml:mi> <mml:msub><mml:mover accent="true"><mml:mi>ρ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub></mml:mfrac> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>ρ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo form="prefix">log</mml:mo> <mml:mfrac><mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>ρ</mml:mi></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>ρ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mfenced> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <italic>u</italic><sub>2</sub> is the number of nodes in the hidden layer of the autoencoder. We multiply this term by a sparsity weight <italic>β</italic>.</p>
<p>In addition, a regularization term is included, which prevents the magnitude of the weights from becoming too large. To accomplish this, we add a penalty to the cost function that is the sum of the squares of all weights (excluding the biases), weighted by a well-chosen constant λ, which is often called the <italic>weight decay parameter</italic>. Including both sparsity and regularization, our final autoencoder cost becomes:
<disp-formula id="pcbi.1004845.e031"><alternatives><graphic id="pcbi.1004845.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e031" xlink:type="simple"/><mml:math display="block" id="M31"><mml:mrow><mml:msub><mml:mi>A</mml:mi> <mml:mo>λ</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>m</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>‖</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>-</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:msup><mml:mo>‖</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>β</mml:mi> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>u</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:munderover> <mml:mtext>KL</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ρ</mml:mi> <mml:mo>‖</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>ρ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mfrac><mml:mo>λ</mml:mo> <mml:mn>2</mml:mn></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>ℓ</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>u</mml:mi> <mml:mi>ℓ</mml:mi></mml:msub></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>u</mml:mi> <mml:mrow><mml:mi>ℓ</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:munderover> <mml:msup><mml:mfenced close="]" open=" [" separators=""><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced> <mml:mn>2</mml:mn></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
<p>We also regularize the weights on the last layer during fine-tuning, so our deep learning cost function becomes:
<disp-formula id="pcbi.1004845.e032"><alternatives><graphic id="pcbi.1004845.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e032" xlink:type="simple"/><mml:math display="block" id="M32"><mml:mrow><mml:msub><mml:mi>J</mml:mi> <mml:mo>λ</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>m</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>‖</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">h</mml:mi> <mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:msup><mml:mo>‖</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mfrac><mml:mo>λ</mml:mo> <mml:mn>2</mml:mn></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>u</mml:mi> <mml:mrow><mml:mi>L</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>u</mml:mi> <mml:mi>L</mml:mi></mml:msub></mml:munderover> <mml:msup><mml:mfenced close="]" open=" ["><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced> <mml:mn>2</mml:mn></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
</sec>
<sec id="sec018">
<title>The final layer: Continuous variable vs. classification</title>
<p>In population genetics, often we want to estimate continuous response variables. To compute our hypothesis for a response variable, based on a set of weights, we could use a logistic activation function, <xref ref-type="disp-formula" rid="pcbi.1004845.e010">Eq (2)</xref>, as we did for the other layers. However, the logistic function is more suitable for binary classification. Instead, we use a linear activation function, so in the case of a single response variable, our hypothesis for dataset <italic>i</italic> becomes 
<disp-formula id="pcbi.1004845.e033"><alternatives><graphic id="pcbi.1004845.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e033" xlink:type="simple"/><mml:math display="block" id="M33"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">h</mml:mi> <mml:mstyle displaystyle="false" scriptlevel="1"><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow></mml:mstyle> <mml:mtext>linear</mml:mtext></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">a</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:msup><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>+</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
In other words, it is the dot product of the activations of the final hidden layer and the weights that connect the final hidden layer to the response variables.</p>
<p>For such classification results, if we had two classes, we could use logistic regression to find the probability a dataset was assigned to each class. With more than two classes, we can extend this concept and use <italic>softmax regression</italic> to assign a probability to each class. If we have <italic>K</italic> classes labeled 1, …, <italic>K</italic>, we can define our hypothesis as follows
<disp-formula id="pcbi.1004845.e034"><alternatives><graphic id="pcbi.1004845.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e034" xlink:type="simple"/><mml:math display="block" id="M34"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">h</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mrow><mml:mtext>softmax</mml:mtext></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>∣</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>∣</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo>∣</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>·</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>b</mml:mi><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow> <mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math>
</alternatives></disp-formula>
where <italic>Z</italic> is the sum of all the entries, so that our probabilities sum to 1. Using this formulation, we can define our classification cost function:
<disp-formula id="pcbi.1004845.e035"><alternatives><graphic id="pcbi.1004845.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e035" xlink:type="simple"/><mml:math display="block" id="M35"><mml:mrow><mml:msubsup><mml:mi>J</mml:mi> <mml:mo>λ</mml:mo> <mml:mtext>softmax</mml:mtext></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>m</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>K</mml:mi></mml:munderover> <mml:mn mathvariant="double-struck">1</mml:mn> <mml:mrow><mml:mo>{</mml:mo> <mml:msup><mml:mi>y</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mi>j</mml:mi> <mml:mo>}</mml:mo></mml:mrow> <mml:mo form="prefix">log</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>y</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mi>j</mml:mi> <mml:mo>|</mml:mo> <mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>;</mml:mo> <mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mfrac><mml:mo>λ</mml:mo> <mml:mn>2</mml:mn></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>u</mml:mi> <mml:mrow><mml:mi>L</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>u</mml:mi> <mml:mi>L</mml:mi></mml:msub></mml:munderover> <mml:msup><mml:mfenced close="]" open=" ["><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>L</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mfenced> <mml:mn>2</mml:mn></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
Intuitively, we can think about this cost function as making the log probability of the correct class as close to 0 as possible.</p>
</sec>
</sec>
<sec id="sec019">
<title>Transforming input data into summary statistics</title>
<p>For many deep learning applications, the raw data can be used directly (the pixels of an image, for example). Unfortunately, we currently cannot input raw genomic data into a deep learning method. Similarly to ABC, we need to transform the data into summary statistics that are potentially informative about the parameters of interest. Unlike ABC, however, deep learning should not be negatively affected by correlated or uninformative summary statistics. Thus we sought to include a large number of potentially informative summary statistics of the data. To account for the impact of selection, we divided each 100 kb region into three smaller regions: 1) close to the selected site (40–60 kb), 2) mid-range from the selected site (20–40 kb and 60–80 kb), and 3) far from the selected site (0–20 kb and 80–100 kb). These regions are based off of the simulation scenario in Peter <italic>et al.</italic> [<xref ref-type="bibr" rid="pcbi.1004845.ref023">23</xref>], and shown more explicitly in <xref ref-type="fig" rid="pcbi.1004845.g007">Fig 7</xref>. Within each of these three regions, we calculated the following statistics, except where noted.</p>
<fig id="pcbi.1004845.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004845.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Regions used for computing the statistics, which are based off of Peter <italic>et al.</italic> [<xref ref-type="bibr" rid="pcbi.1004845.ref023">23</xref>].</title>
<p>Note that the selected site was chosen randomly within region 1.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.g007" xlink:type="simple"/>
</fig>
<p>For all the statistics described below, <italic>n</italic> is the haploid sample size. In the case of simulated data, <italic>n</italic> = 100. For the real data, we had 197 samples; within each 100 kb region we sorted the samples by missing data, then retained the 100 most complete samples, except where noted.</p>
<list list-type="order">
<list-item>
<p>Number of segregating sites within each smaller region, <italic>S</italic>. Since all of the statistics must be in [0, 1], we normalized by <italic>S</italic><sub>max</sub> = 5000 (any <italic>S</italic> &gt; <italic>S</italic><sub>max</sub> was truncated): 3 statistics.</p>
</list-item>
<list-item>
<p>Tajima’s <italic>D</italic> statistic [<xref ref-type="bibr" rid="pcbi.1004845.ref071">71</xref>], computed as follows
<disp-formula id="pcbi.1004845.e036"><alternatives><graphic id="pcbi.1004845.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e036" xlink:type="simple"/><mml:math display="block" id="M36"><mml:mrow><mml:mi>D</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>π</mml:mi> <mml:mo>-</mml:mo> <mml:mi>S</mml:mi> <mml:mo>/</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow> <mml:msqrt><mml:mrow><mml:mover accent="true"><mml:mtext>Var</mml:mtext> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>π</mml:mi> <mml:mo>-</mml:mo> <mml:mi>S</mml:mi> <mml:mo>/</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <italic>π</italic> is the average number of pairwise differences between two samples, and <inline-formula id="pcbi.1004845.e037"><alternatives><graphic id="pcbi.1004845.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:mrow><mml:msub><mml:mi>a</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. We normalized by <italic>D</italic><sub>min</sub> = −3.0 and <italic>D</italic><sub>max</sub> = 3.0, again truncating the rare statistic outside this range: 3 statistics.</p>
</list-item>
<list-item>
<p>Folded site frequency spectrum (SFS): <italic>η</italic><sub><italic>i</italic></sub> is the number of segregating sites where the minor allele occurs <italic>i</italic> times out of <italic>n</italic> samples, for <italic>i</italic> = 1, 2, …, ⌊<italic>n</italic>/2⌋. For the real data, for each segregating site, enough samples were included to obtain 100 with non-missing data. If that was not possible for a given site, the site was not included. To normalize the SFS, we divided each <italic>η</italic><sub><italic>i</italic></sub> by the sum of the entries, which gives us the probability of observing <italic>i</italic> minor alleles, given the site was segregating: 50 ⋅ 3 = 150 statistics.</p>
</list-item>
<list-item>
<p>Length distribution between segregating sites: let <italic>B</italic><sub><italic>k</italic></sub> be the number of bases between the <italic>k</italic> and <italic>k</italic>+1 segregating sites. To compute the distribution of these lengths, we define <italic>J</italic> bins and count the number of lengths that fall into each bin:
<disp-formula id="pcbi.1004845.e038"><alternatives><graphic id="pcbi.1004845.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e038" xlink:type="simple"/><mml:math display="block" id="M38"><mml:mrow><mml:msubsup><mml:mi>d</mml:mi> <mml:mi>j</mml:mi> <mml:mtext>BET</mml:mtext></mml:msubsup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo> <mml:mi>k</mml:mi> <mml:mo>∈</mml:mo> <mml:mrow><mml:mo>{</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>,</mml:mo> <mml:mi>S</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>:</mml:mo></mml:mrow> <mml:msub><mml:mi>B</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>∈</mml:mo> <mml:mtext>bin</mml:mtext> <mml:mspace width="4.pt"/><mml:mi>j</mml:mi> <mml:mo>|</mml:mo></mml:mrow></mml:mrow> <mml:mi>S</mml:mi></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
We choose <italic>J</italic> = 16 equally spaced bins, the first starting at 0 and the last starting at 300: 16 ⋅ 3 = 48 statistics.</p>
</list-item>
<list-item>
<p>Identity-by-state (IBS) tract length distribution: for each pair of samples, and IBS tract is a contiguous genomic region where the samples are identical at every base (delimited by bases where they differ). For all pairs, let <italic>L</italic> be the set of IBS tract lengths. In a similar fashion to the length distribution statistics, we define <italic>M</italic> bins, and count the number of IBS tract lengths that fall into each bin:
<disp-formula id="pcbi.1004845.e039"><alternatives><graphic id="pcbi.1004845.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e039" xlink:type="simple"/><mml:math display="block" id="M39"><mml:mrow><mml:msubsup><mml:mi>d</mml:mi> <mml:mi>m</mml:mi> <mml:mtext>IBS</mml:mtext></mml:msubsup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mo>|</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>L</mml:mi> <mml:mo>:</mml:mo> <mml:mi>ℓ</mml:mi> <mml:mo>∈</mml:mo> <mml:mtext>bin</mml:mtext> <mml:mspace width="4.pt"/><mml:mi>m</mml:mi> <mml:mo>|</mml:mo></mml:mrow> <mml:mrow><mml:mo>|</mml:mo> <mml:mi>L</mml:mi> <mml:mo>|</mml:mo></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
We choose <italic>M</italic> = 30 equally spaced bins, the first starting at 0 and the last starting at 5000: 30 ⋅ 3 = 90 statistics.</p>
</list-item>
<list-item>
<p>Linkage disequilibrium (LD) distribution: LD is a measure of the correlation between two segregating sites. For example, if there was no recombination between two sites, their alleles would be highly correlated and LD would be high in magnitude. If there was infinite recombination between two sites, they would be independent and have LD close to 0. For two loci, let <italic>A</italic>, <italic>a</italic> be the alleles for the first site, and <italic>B</italic>, <italic>b</italic> be the alleles for the second site. Let <italic>p</italic><sub><italic>A</italic></sub> be the frequency of allele <italic>A</italic>, <italic>p</italic><sub><italic>B</italic></sub> be the frequency of allele <italic>B</italic>, and <italic>p</italic><sub><italic>AB</italic></sub> be the frequency of haplotype <italic>AB</italic>. Then the linkage disequilibrium is computed by
<disp-formula id="pcbi.1004845.e040"><alternatives><graphic id="pcbi.1004845.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e040" xlink:type="simple"/><mml:math display="block" id="M40"><mml:mrow><mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>A</mml:mi> <mml:mi>B</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mrow><mml:mi>A</mml:mi> <mml:mi>B</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>A</mml:mi></mml:msub> <mml:msub><mml:mi>p</mml:mi> <mml:mi>B</mml:mi></mml:msub> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
Here we compute the LD between pairs of sites, where one site is in the “selected” region, and the other is in each of the three regions (including the selected region). Then we create an LD distribution similar to the IBS distribution above, using 16 bins, with the first bin ending at −0.05, and the last bin starting at 0.2: 16 ⋅ 3 = 48 statistics.</p>
</list-item>
<list-item>
<p>H1, H12, and H2 statistics, as described in Garud <italic>et al.</italic> [<xref ref-type="bibr" rid="pcbi.1004845.ref072">72</xref>]. These statistics help to distinguish between hard and soft sweeps, and are calculated in the selected (middle) region only: 3 statistics.</p>
</list-item>
</list>
<p>This gives us a total of 345 statistics.</p>
</sec>
<sec id="sec020">
<title>Deep learning for demography and selection</title>
<sec id="sec021">
<title>Deep learning model</title>
<p>To modify our deep learning method to accommodate this type of inference problem, during training we have an outer-loop that changes the demography as necessary, and an inner loop that accounts for differences in selection for each region. During testing, we compare three different methods for predicting the effective population sizes: (1) <italic>Average stat</italic>: we average the statistics for each region within the dataset (corresponding to the same demography), then run this average through the trained network. (2) <italic>Final</italic>: we run the statistics for each region through the trained network separately, then average the results. (3) <italic>Neutral regions</italic>: we use the same method as (2), but instead of using all the regions, we only use the ones we predict as being neutral. We also include a range between the 2.5th and 97.5th quantiles. To predict the selection class, we obtain a probability distribution over the classes for each region, then select the class with the highest probability.</p>
<p>One final complication is that we estimate continuous response variables for the population sizes, but consider selection to be a discrete response variable. This involves a linear activation function for the population sizes and a softmax classifier for selection. A diagram of our deep learning method is shown in <xref ref-type="fig" rid="pcbi.1004845.g008">Fig 8</xref>.</p>
<fig id="pcbi.1004845.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004845.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Our deep learning framework for effective population size changes and selection.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.g008" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec022">
<title>Analysis of informative statistics</title>
<p>One advantage of deep learning is that the weights of the optimal network give us an interpretable link between the summary statistics and the output variables. However, from these weights, it is not immediately obvious which statistics are the most informative or “best” for a given output variable. If we only wanted to use only a small subset of the statistics, which ones would give us the best results? To answer this question, we use both a permutation testing approach and a perturbation method described in <xref ref-type="supplementary-material" rid="pcbi.1004845.s009">S1 Algorithm</xref>.</p>
</sec>
</sec>
<sec id="sec023">
<title>Software availability</title>
<p>An open-source software package (called evoNet) that implements deep learning algorithms for population genetic inference is available at <ext-link ext-link-type="uri" xlink:href="https://sourceforge.net/projects/evonet" xlink:type="simple">https://sourceforge.net/projects/evonet</ext-link>.</p>
</sec>
</sec>
<sec id="sec024">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004845.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.s001" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Results for very different training and testing data.</title>
<p>In this scenario, the testing data is simulated with a recombination rate that is 4 times higher than that of the training data. The effective population size results (top table) are still generally accurate, but selection (bottom table) is harder to predict. Neutral regions are predicted correctly, but soft sweeps are also often classified as neutral. The classifier has difficulty distinguishing between hard sweeps and balancing selection. The top table should be compared to <xref ref-type="table" rid="pcbi.1004845.t001">Table 1</xref> and the bottom table should be compared to <xref ref-type="table" rid="pcbi.1004845.t003">Table 3</xref>.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004845.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.s002" xlink:type="simple">
<label>S2 Table</label>
<caption>
<title>Results for very different training and testing data.</title>
<p>In this scenario, the testing data is simulated with bottleneck parameters that are more severe than the training data. This has a slight negative impact on the population size results (largely on the most recent size which was outside the training range), but has little effect on the selection results. The overall percentage of misclassified regions is 7.8%. The top table should be compared to <xref ref-type="table" rid="pcbi.1004845.t001">Table 1</xref> and the bottom table should be compared to <xref ref-type="table" rid="pcbi.1004845.t003">Table 3</xref>.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004845.s003" mimetype="application/vnd.ms-excel" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.s003" xlink:type="simple">
<label>S3 Table</label>
<caption>
<title>Classification results for all windows, along with which genes are found in each window.</title>
<p>(XLS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004845.s004" mimetype="application/vnd.ms-excel" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.s004" xlink:type="simple">
<label>S4 Table</label>
<caption>
<title>High-confidence windows, along with which genes are found in each window.</title>
<p>(XLS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004845.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.s005" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>An example of a classical neural network.</title>
<p>The single hidden layer serves to learn informative combinations of the inputs, remove correlations, and typically reduce the dimension of the data. After the optimal weight on each connecting arrow is learned through labeled training data, unlabeled data can be fed through the network to estimate the response variables.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004845.s006" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.s006" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>A Venn diagram of most informative statistics for each output variable (<italic>N</italic><sub>1</sub>, <italic>N</italic><sub>2</sub>, <italic>N</italic><sub>3</sub>, and selection), using the perturbation method.</title>
<p>For each variable, the top 25 statistics were chosen, according to the procedure in <xref ref-type="supplementary-material" rid="pcbi.1004845.s009">S1 Algorithm</xref>. The Venn diagram captures statistics common to each subset of output variables, with notable less informative statistics shown in the lower right. Close, mid, and far represent the genomic region where the statistic was calculated. The numbers after each colon refer to the position of the statistic within its distribution or order. For the SFS statistics, it is number of minor alleles. For each region, there are 50 SFS statistics, 16 BET statistics (distribution between segregating sites), 30 IBS statistics, and 16 LD statistics.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004845.s007" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.s007" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Validation procedure to find the optimal weight decay parameter]Validation procedure for a network with two hidden layers of size 8 and 4.</title>
<p>The x-axis shows increasing values of λ, and the y-axis shows the error on the validation dataset. The curve shows a characteristic shape with low and high λ producing poorer results than an intermediate value. For these hidden layers sizes and this dataset, <inline-formula id="pcbi.1004845.e041"><alternatives><graphic id="pcbi.1004845.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:mrow><mml:mover><mml:mo>λ</mml:mo> <mml:mo>^</mml:mo></mml:mover></mml:mrow> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>0001</mml:mn></mml:math></alternatives></inline-formula> was optimal.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004845.s008" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.s008" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Example of an autoencoder]An example of an autoencoder.</title>
<p>The input data (<bold>x</bold>) is projected into a (usually) lower dimension (<bold>a</bold>), then reconstructed (<inline-formula id="pcbi.1004845.e042"><alternatives><graphic id="pcbi.1004845.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004845.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>). The weights of an autoencoder are optimized such that the difference between the reconstructed data and the original data is minimal.</p>
<p>(TIFF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pcbi.1004845.s009" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004845.s009" xlink:type="simple">
<label>S1 Algorithm</label>
<caption>
<title>Method for finding the most informative statistics for each response variable.</title>
<p>(TIFF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Andrew Kern and Daniel Schrider for helpful discussion on classifying hard and soft sweeps, as well as Yuh Chwen (Grace) Lee for many insightful comments on our <italic>Drosophila melanogaster</italic> selection results. Also, we are very grateful to John Pool and Kevin Thornton for helpful comments on our manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004845.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jones</surname> <given-names>N</given-names></name>. <article-title>Computer science: The learning machines</article-title>. <source>Nature</source>. <year>2014</year>;<volume>505</volume>(<issue>7482</issue>):<fpage>146</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/505146a" xlink:type="simple">10.1038/505146a</ext-link></comment> <object-id pub-id-type="pmid">24402264</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Li</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Jakobsson</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sjödin</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Lascoux</surname> <given-names>M</given-names></name>. <article-title>Joint analysis of demography and selection in population genetics: where do we stand and where could we go?</article-title> <source>Molecular Ecology</source>. <year>2012</year>;<volume>21</volume>(<issue>1</issue>):<fpage>28</fpage>–<lpage>44</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004845.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Duchen</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Živković</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Hutter</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Stephan</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Laurent</surname> <given-names>S</given-names></name>. <article-title>Demographic inference reveals African and European admixture in the North American <italic>Drosophila melanogaster</italic> population</article-title>. <source>Genetics</source>. <year>2013</year>;<volume>193</volume>(<issue>1</issue>):<fpage>291</fpage>–<lpage>301</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1534/genetics.112.145912" xlink:type="simple">10.1534/genetics.112.145912</ext-link></comment> <object-id pub-id-type="pmid">23150605</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>González</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Macpherson</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Messer</surname> <given-names>PW</given-names></name>, <name name-style="western"><surname>Petrov</surname> <given-names>DA</given-names></name>. <article-title>Inferring the Strength of Selection in <italic>Drosophila</italic> under Complex Demographic Models</article-title>. <source>Molecular Biology and Evolution</source>. <year>2009</year>;<volume>26</volume>(<issue>3</issue>):<fpage>513</fpage>–<lpage>526</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/molbev/msn270" xlink:type="simple">10.1093/molbev/msn270</ext-link></comment> <object-id pub-id-type="pmid">19033258</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sella</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Petrov</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Przeworski</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Andolfatto</surname> <given-names>P</given-names></name>. <article-title>Pervasive natural selection in the <italic>Drosophila</italic> genome?</article-title> <source>PLoS Genetics</source>. <year>2009</year>;<volume>5</volume>(<issue>6</issue>):<fpage>e1000495</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pgen.1000495" xlink:type="simple">10.1371/journal.pgen.1000495</ext-link></comment> <object-id pub-id-type="pmid">19503600</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lack</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Cardeno</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Crepeau</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Taylor</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Corbett-Detig</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Stevens</surname> <given-names>KA</given-names></name>, <etal>et al</etal>. <article-title>The Drosophila Genome Nexus: a population genomic resource of 623 <italic>Drosophila melanogaster</italic> genomes, including 197 from a single ancestral range population</article-title>. <source>Genetics</source>. <year>2015</year>;<volume>199</volume>(<issue>4</issue>):<fpage>1229</fpage>–<lpage>1241</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1534/genetics.115.174664" xlink:type="simple">10.1534/genetics.115.174664</ext-link></comment> <object-id pub-id-type="pmid">25631317</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pavlidis</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Jensen</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Stephan</surname> <given-names>W</given-names></name>. <article-title>Searching for footprints of positive selection in whole-genome SNP data from nonequilibrium populations</article-title>. <source>Genetics</source>. <year>2010</year>;<volume>185</volume>:<fpage>907</fpage>–<lpage>922</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1534/genetics.110.116459" xlink:type="simple">10.1534/genetics.110.116459</ext-link></comment> <object-id pub-id-type="pmid">20407129</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pavlidis</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Živković</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Stamatakis</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Alachiotis</surname> <given-names>N</given-names></name>. <article-title>SweeD: likelihood-based detection of selective sweeps in thousands of genomes</article-title>. <source>Molecular Biology and Evolution</source>. <year>2013</year>;<volume>30</volume>(<issue>9</issue>):<fpage>2224</fpage>–<lpage>2224</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/molbev/mst112" xlink:type="simple">10.1093/molbev/mst112</ext-link></comment> <object-id pub-id-type="pmid">23777627</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ronen</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Udpa</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Halperin</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Bafna</surname> <given-names>V</given-names></name>. <article-title>Learning natural selection from the site frequency spectrum</article-title>. <source>Genetics</source>. <year>2013</year>;<volume>195</volume>:<fpage>181</fpage>–<lpage>193</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1534/genetics.113.152587" xlink:type="simple">10.1534/genetics.113.152587</ext-link></comment> <object-id pub-id-type="pmid">23770700</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lin</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Schlötterer</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Futschik</surname> <given-names>A</given-names></name>. <article-title>Distinguishing positive selection from neutral evolution: boosting the performance of summary statistics</article-title>. <source>Genetics</source>. <year>2011</year>;<volume>187</volume>:<fpage>229</fpage>–<lpage>244</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1534/genetics.110.122614" xlink:type="simple">10.1534/genetics.110.122614</ext-link></comment> <object-id pub-id-type="pmid">21041556</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lin</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Futschik</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>H</given-names></name>. <article-title>A fast estimate for the population recombination rate based on regression</article-title>. <source>Genetics</source>. <year>2013</year>;<volume>194</volume>:<fpage>473</fpage>–<lpage>484</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1534/genetics.113.150201" xlink:type="simple">10.1534/genetics.113.150201</ext-link></comment> <object-id pub-id-type="pmid">23589457</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pybus</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Luisi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dall’Olio</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Uzkundun</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Laayouni</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Bertranpetit</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Hierarchical Boosting: a machine-learning framework to detect and classify hard selective sweeps in human populations</article-title>. <source>Bioinformatics</source>. <year>2015</year>;In press. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/btv493" xlink:type="simple">10.1093/bioinformatics/btv493</ext-link></comment> <object-id pub-id-type="pmid">26315912</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Li</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Durbin</surname> <given-names>R</given-names></name>. <article-title>Inference of human population history from individual whole-genome sequences</article-title>. <source>Nature</source>. <year>2011</year>;<volume>475</volume>:<fpage>493</fpage>–<lpage>496</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature10231" xlink:type="simple">10.1038/nature10231</ext-link></comment> <object-id pub-id-type="pmid">21753753</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sheehan</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Harris</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Song</surname> <given-names>YS</given-names></name>. <article-title>Estimating variable effective population sizes from multiple genomes: A sequentially Markov conditional sampling distribution approach</article-title>. <source>Genetics</source>. <year>2013</year>;<volume>194</volume>(<issue>3</issue>):<fpage>647</fpage>–<lpage>662</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1534/genetics.112.149096" xlink:type="simple">10.1534/genetics.112.149096</ext-link></comment> <object-id pub-id-type="pmid">23608192</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref015">
<label>15</label>
<mixed-citation publication-type="other" xlink:type="simple">Steinrücken M, Kamm JA, Song YS. Inference of complex population histories using whole-genome sequences from multiple populations; 2015. BioRxiv preprint, <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1101/026591" xlink:type="simple">http://dx.doi.org/10.1101/026591</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004845.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schiffels</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Durbin</surname> <given-names>R</given-names></name>. <article-title>Inferring human population size and separation history from multiple genome sequences</article-title>. <source>Nature Genetics</source>. <year>2014</year>;<volume>46</volume>:<fpage>919</fpage>–<lpage>925</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/ng.3015" xlink:type="simple">10.1038/ng.3015</ext-link></comment> <object-id pub-id-type="pmid">24952747</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Galtier</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Depaulis</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Barton</surname> <given-names>NH</given-names></name>. <article-title>Detecting bottlenecks and selective sweeps from DNA sequence polymorphism</article-title>. <source>Genetics</source>. <year>2000</year>;<volume>155</volume>:<fpage>981</fpage>–<lpage>987</lpage>. <object-id pub-id-type="pmid">10835415</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gossmann</surname> <given-names>TI</given-names></name>, <name name-style="western"><surname>Woolfit</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Eyre-Walker</surname> <given-names>A</given-names></name>. <article-title>Quantifying the variation in the effective population size within a genome</article-title>. <source>Genetics</source>. <year>2011</year>;<volume>189</volume>:<fpage>1389</fpage>–<lpage>1402</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1534/genetics.111.132654" xlink:type="simple">10.1534/genetics.111.132654</ext-link></comment> <object-id pub-id-type="pmid">21954163</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pritchard</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Seielstad</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Perez-Lezaun</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Feldman</surname> <given-names>MW</given-names></name>. <article-title>Population growth of human Y chromosomes: a study of Y chromosome microsatellites</article-title>. <source>Molecular Biology and Evolution</source>. <year>1999</year>;<volume>16</volume>(<issue>12</issue>):<fpage>1791</fpage>–<lpage>1798</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/oxfordjournals.molbev.a026091" xlink:type="simple">10.1093/oxfordjournals.molbev.a026091</ext-link></comment> <object-id pub-id-type="pmid">10605120</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Beaumont</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Balding</surname> <given-names>DJ</given-names></name>. <article-title>Approximate Bayesian Computation in Population Genetics</article-title>. <source>Genetics</source>. <year>2002</year>;<volume>162</volume>(<issue>4</issue>):<fpage>2025</fpage>–<lpage>2035</lpage>. <object-id pub-id-type="pmid">12524368</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Becquet</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Przeworski</surname> <given-names>M</given-names></name>. <article-title>A new approach to estimate parameters of speciation models with application to apes</article-title>. <source>Genome Res</source>. <year>2007</year>;<volume>17</volume>(<issue>10</issue>):<fpage>1505</fpage>–<lpage>19</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1101/gr.6409707" xlink:type="simple">10.1101/gr.6409707</ext-link></comment> <object-id pub-id-type="pmid">17712021</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jensen</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Thornton</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Andolfatto</surname> <given-names>P</given-names></name>. <article-title>An approximate Bayesian estimator suggests strong, recurrent selective sweeps in <italic>Drosophila</italic></article-title>. <source>PLoS Genetics</source>. <year>2008</year>;<volume>4</volume>(<issue>9</issue>):<fpage>e1000198</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pgen.1000198" xlink:type="simple">10.1371/journal.pgen.1000198</ext-link></comment> <object-id pub-id-type="pmid">18802463</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Peter</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Huerta-Sanchez</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Nielsen</surname> <given-names>R</given-names></name>. <article-title>Distinguishing between selective sweeps from standing variation and from a <italic>de novo</italic> mutation</article-title>. <source>PLoS Genetics</source>. <year>2012</year>;<volume>8</volume>(<issue>10</issue>):<fpage>e1003011</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pgen.1003011" xlink:type="simple">10.1371/journal.pgen.1003011</ext-link></comment> <object-id pub-id-type="pmid">23071458</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Marjoram</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Molitor</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Plagnol</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Tavare</surname> <given-names>S</given-names></name>. <article-title>Markov chain Monte Carlo without likelihoods</article-title>. <source>PNAS</source>. <year>2003</year>;<volume>100</volume>(<issue>26</issue>):<fpage>15324</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0306899100" xlink:type="simple">10.1073/pnas.0306899100</ext-link></comment> <object-id pub-id-type="pmid">14663152</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sisson</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Fan</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Tanaka</surname> <given-names>MM</given-names></name>. <article-title>Sequential Monte carlo without likelihoods</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2007</year>;<volume>104</volume>(<issue>6</issue>):<fpage>1760</fpage>–<lpage>1765</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0607208104" xlink:type="simple">10.1073/pnas.0607208104</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Joyce</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Marjoram</surname> <given-names>P</given-names></name>. <article-title>Approximately sufficient statistics and Bayesian computation</article-title>. <source>Statistical Applications in Genetics and Molecular Biology</source>. <year>2008</year>;<volume>7</volume>(<issue>1</issue>):<fpage>Article26</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2202/1544-6115.1389" xlink:type="simple">10.2202/1544-6115.1389</ext-link></comment> <object-id pub-id-type="pmid">18764775</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Beaumont</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Cornuet</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Marin</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Robert</surname> <given-names>CP</given-names></name>. <article-title>Adaptive approximate Bayesian computation</article-title>. <source>Biometrika</source>. <year>2009</year>;<volume>96</volume>(<issue>4</issue>):<fpage>983</fpage>–<lpage>990</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/biomet/asp052" xlink:type="simple">10.1093/biomet/asp052</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Blum</surname> <given-names>MGB</given-names></name>, <name name-style="western"><surname>François</surname> <given-names>O</given-names></name>. <article-title>Non-linear regression models for Approximate Bayesian Computation</article-title>. <source>Statistics and Computing</source>. <year>2010</year>;<volume>20</volume>(<issue>1</issue>):<fpage>63</fpage>–<lpage>73</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s11222-009-9116-0" xlink:type="simple">10.1007/s11222-009-9116-0</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nunes</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Balding</surname> <given-names>DJ</given-names></name>. <article-title>On optimal selection of summary statistics for approximate Bayesian computation</article-title>. <source>Statistical Applications in Genetics and Molecular Biology</source>. <year>2010</year>;<volume>9</volume>(<issue>1</issue>):<fpage>34</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2202/1544-6115.1576" xlink:type="simple">10.2202/1544-6115.1576</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Didelot</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Everitt</surname> <given-names>RG</given-names></name>, <name name-style="western"><surname>Johansen</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Lawson</surname> <given-names>DJ</given-names></name>, <etal>et al</etal>. <article-title>Likelihood-free estimation of model evidence</article-title>. <source>Bayesian Analysis</source>. <year>2011</year>;<volume>6</volume>(<issue>1</issue>):<fpage>49</fpage>–<lpage>76</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/11-BA602" xlink:type="simple">10.1214/11-BA602</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fearnhead</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Prangle</surname> <given-names>D</given-names></name>. <article-title>Constructing summary statistics for approximate Bayesian computation: semi-automatic approximate Bayesian computation</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>. <year>2012</year>;<volume>74</volume>(<issue>3</issue>):<fpage>419</fpage>–<lpage>474</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1467-9868.2011.01010.x" xlink:type="simple">10.1111/j.1467-9868.2011.01010.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Beaumont</surname> <given-names>MA</given-names></name>. <article-title>Approximate Bayesian computation in evolution and ecology</article-title>. <source>Annual Review of Ecology, Evolution, and Systematics</source>. <year>2010</year>;<volume>41</volume>:<fpage>379</fpage>–<lpage>406</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev-ecolsys-102209-144621" xlink:type="simple">10.1146/annurev-ecolsys-102209-144621</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bazin</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Dawson</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Beaumont</surname> <given-names>MA</given-names></name>. <article-title>Likelihood-Free Inference of Population Structure and Local Adaptation in a Bayesian Hierarchical Model</article-title>. <source>Genetics</source>. <year>2010</year>;<volume>185</volume>(<issue>2</issue>):<fpage>587</fpage>–<lpage>602</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1534/genetics.109.112391" xlink:type="simple">10.1534/genetics.109.112391</ext-link></comment> <object-id pub-id-type="pmid">20382835</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hopfield</surname> <given-names>JJ</given-names></name>. <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>PNAS</source>. <year>1982</year>;<volume>79</volume>(<issue>8</issue>):<fpage>2554</fpage>–<lpage>2558</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.79.8.2554" xlink:type="simple">10.1073/pnas.79.8.2554</ext-link></comment> <object-id pub-id-type="pmid">6953413</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cybenko</surname> <given-names>G</given-names></name>. <article-title>Approximations by superpositions of sigmoidal functions</article-title>. <source>Mathematics of Control, Signals, and Systems</source>. <year>1989</year>;<volume>2</volume>(<issue>4</issue>):<fpage>303</fpage>–<lpage>314</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF02551274" xlink:type="simple">10.1007/BF02551274</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hornik</surname> <given-names>K</given-names></name>. <article-title>Approximation Capabilities of Multilayer Feedforward Networks</article-title>. <source>Neural Networks</source>. <year>1991</year>;<volume>4</volume>(<issue>2</issue>):<fpage>251</fpage>–<lpage>257</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0893-6080(91)90009-T" xlink:type="simple">10.1016/0893-6080(91)90009-T</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Salakhutdinov</surname> <given-names>RR</given-names></name>. <article-title>Reducing the dimensionality of data with neural networks</article-title>. <source>Science</source>. <year>2006</year>;<volume>313</volume>(<issue>5786</issue>):<fpage>504</fpage>–<lpage>507</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1127647" xlink:type="simple">10.1126/science.1127647</ext-link></comment> <object-id pub-id-type="pmid">16873662</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rumelhart</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Williams</surname> <given-names>RJ</given-names></name>. <article-title>Learning representations by back-propagating errors</article-title>. <source>Nature</source>. <year>1986</year>;<volume>323</volume>(<issue>9</issue>):<fpage>533</fpage>–<lpage>536</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/323533a0" xlink:type="simple">10.1038/323533a0</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref039">
<label>39</label>
<mixed-citation publication-type="other" xlink:type="simple">Mohamed Ar, Sainath TN, Dahl G, Ramabhadran B, Hinton GE, Picheny MA. Deep belief networks using discriminative features for phone recognition. In: Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on; 2011. p. 5060–5063.</mixed-citation>
</ref>
<ref id="pcbi.1004845.ref040">
<label>40</label>
<mixed-citation publication-type="other" xlink:type="simple">Graves A, rahman Mohamed A, Hinton G. Speech recognition with deep recurrent neural networks. In: IEEE International Conference on Acoustic Speech and Signal Processing (ICASSP); 2013. p. 6645–6649.</mixed-citation>
</ref>
<ref id="pcbi.1004845.ref041">
<label>41</label>
<mixed-citation publication-type="other" xlink:type="simple">Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems (NIPS); 2012. p. 1097–1105.</mixed-citation>
</ref>
<ref id="pcbi.1004845.ref042">
<label>42</label>
<mixed-citation publication-type="other" xlink:type="simple">Hinton GE, Srivastava N, Krizhevsky A, Sutskever I, Salakhutdinov RR. Improving neural networks by preventing co-adaptation of feature detectors; 2012. ArXiv preprint, <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1207.0580" xlink:type="simple">http://arxiv.org/abs/1207.0580</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004845.ref043">
<label>43</label>
<mixed-citation publication-type="other" xlink:type="simple">Dahl GE, Sainath TN, Hinton GE. Improving deep neural networks for LVCSR using rectified linear units and dropout. In: IEEE International Conference on Acoustic Speech and Signal Processing (ICASSP); 2013. p. 8609–8613.</mixed-citation>
</ref>
<ref id="pcbi.1004845.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kim</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Greene</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Zlateski</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Richardson</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Turaga</surname> <given-names>SC</given-names></name>, <etal>et al</etal>. <article-title>Space-time wiring specificity supports direction selectivity in the retina</article-title>. <source>Nature</source>. <year>2014</year>;<volume>509</volume>(<issue>7500</issue>):<fpage>331</fpage>–<lpage>336</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature13240" xlink:type="simple">10.1038/nature13240</ext-link></comment> <object-id pub-id-type="pmid">24805243</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Qi</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Oja</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Weston</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Noble</surname> <given-names>WS</given-names></name>. <article-title>A unified multitask architecture for predicting local protein properties</article-title>. <source>PLoS One</source>. <year>2012</year>;<volume>7</volume>(<issue>3</issue>):<fpage>e32235</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0032235" xlink:type="simple">10.1371/journal.pone.0032235</ext-link></comment> <object-id pub-id-type="pmid">22461885</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Leung</surname> <given-names>MKK</given-names></name>, <name name-style="western"><surname>Xiong</surname> <given-names>HY</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>LJ</given-names></name>, <name name-style="western"><surname>Frey</surname> <given-names>BJ</given-names></name>. <article-title>Deep learning of the tissue-regulated splicing code</article-title>. <source>Bioinformatics</source>. <year>2014</year>;<volume>30</volume>(<issue>12</issue>):<fpage>i121</fpage>–<lpage>i129</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/btu277" xlink:type="simple">10.1093/bioinformatics/btu277</ext-link></comment> <object-id pub-id-type="pmid">24931975</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Xiong</surname> <given-names>HY</given-names></name>, <name name-style="western"><surname>Alipanahi</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>LJ</given-names></name>, <name name-style="western"><surname>Bretschneider</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Merico</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ryan</surname></name>, <etal>et al</etal>. <article-title>The human splicing code reveals new insights into the genetic determinants of disease</article-title>. <source>Science</source>. <year>2015</year>;<volume>347</volume> (<issue>6218</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1254806" xlink:type="simple">10.1126/science.1254806</ext-link></comment> <object-id pub-id-type="pmid">25525159</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Alipanahi</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Delong</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Weirauch</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Frey</surname> <given-names>BJ</given-names></name>. <article-title>Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning</article-title>. <source>Nature Biotechnology</source>. <year>2015</year>;<volume>33</volume>:<fpage>831</fpage>–<lpage>838</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nbt.3300" xlink:type="simple">10.1038/nbt.3300</ext-link></comment> <object-id pub-id-type="pmid">26213851</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref049">
<label>49</label>
<mixed-citation publication-type="other" xlink:type="simple">Jiang B, Wu T, Zheng C, Wong WH. Learning Summary Statistic for Approximate Bayesian Computation via Deep Neural Network; 2015. ArXiv preprint: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1510.02175" xlink:type="simple">http://arxiv.org/abs/1510.02175</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004845.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tavaré</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Balding</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Griffiths</surname> <given-names>JRC</given-names></name>, <name name-style="western"><surname>Donnelly</surname> <given-names>P</given-names></name>. <article-title>Inferring Coalescence Times From DNA Sequence Data</article-title>. <source>Genetics</source>. <year>1997</year>;<volume>145</volume>:<fpage>505</fpage>–<lpage>518</lpage>. <object-id pub-id-type="pmid">9071603</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Aeschbacher</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Beaumont</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Futschik</surname> <given-names>A</given-names></name>. <article-title>A novel approach for choosing summary statistics in approximate Bayesian computation</article-title>. <source>Genetics</source>. <year>2012</year>;<volume>192</volume>(<issue>3</issue>):<fpage>1027</fpage>–<lpage>1047</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1534/genetics.112.143164" xlink:type="simple">10.1534/genetics.112.143164</ext-link></comment> <object-id pub-id-type="pmid">22960215</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nakagome</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Fukumizu</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Mano</surname> <given-names>S</given-names></name>. <article-title>Kernel approximate Bayesian computation in population genetic inferences</article-title>. <source>Statistical Applications in Genetics and Molecular Biology</source>. <year>2013</year>;<volume>12</volume>(<issue>6</issue>):<fpage>1</fpage>–<lpage>12</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1515/sagmb-2012-0050" xlink:type="simple">10.1515/sagmb-2012-0050</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Blum</surname> <given-names>MGB</given-names></name>, <name name-style="western"><surname>Nunes</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Prangle</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Sisson</surname> <given-names>SA</given-names></name>. <article-title>A comparative review of dimension reduction methods in approximate Bayesian computation</article-title>. <source>Statistical Science</source>. <year>2013</year>;<volume>28</volume>(<issue>2</issue>):<fpage>189</fpage>–<lpage>208</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/12-STS406" xlink:type="simple">10.1214/12-STS406</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wegmann</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Leuenberger</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Neuenschwander</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Excoffier</surname> <given-names>L</given-names></name>. <article-title>ABCtoolbox: a versatile toolkit for approximate Bayesian computations</article-title>. <source>BMC Bioinformatics</source>. <year>2010</year>;<volume>11</volume>:<fpage>116</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/1471-2105-11-116" xlink:type="simple">10.1186/1471-2105-11-116</ext-link></comment> <object-id pub-id-type="pmid">20202215</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref055">
<label>55</label>
<mixed-citation publication-type="other" xlink:type="simple">Elyashiv E, Sattath S, Hu TT, Strustovsky A, McVicker G, Andolfatto P, et al. A genomic map of the effects of linked selection in <italic>Drosophila</italic>; 2014. ArXiv preprint, <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1408.5461" xlink:type="simple">http://arxiv.org/abs/1408.5461</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004845.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ewing</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Hermisson</surname> <given-names>J</given-names></name>. <article-title>MSMS: A Coalescent Simulation Program Including Recombination, Demographic Structure, and Selection at a Single Locus</article-title>. <source>Bioinformatics</source>. <year>2010</year>;<volume>26</volume>:<fpage>2064</fpage>–<lpage>2065</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/btq322" xlink:type="simple">10.1093/bioinformatics/btq322</ext-link></comment> <object-id pub-id-type="pmid">20591904</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Haag-Liautard</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Dorris</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Maside</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Macaskill</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Halligan</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Charlesworth</surname> <given-names>B</given-names></name>, <etal>et al</etal>. <article-title>Direct estimation of per nucleotide and genomic deleterious mutation rates in <italic>Drosophila</italic></article-title>. <source>Nature</source>. <year>2007</year>;<volume>445</volume>:<fpage>82</fpage>–<lpage>85</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature05388" xlink:type="simple">10.1038/nature05388</ext-link></comment> <object-id pub-id-type="pmid">17203060</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Thornton</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Andolfatto</surname> <given-names>P</given-names></name>. <article-title>Approximate Bayesian inference reveals evidence for a recent, severe bottleneck in a Netherlands population of <italic>Drosophila melanogaster</italic></article-title>. <source>Genetics</source>. <year>2006</year>;<volume>172</volume>(<issue>3</issue>):<fpage>1607</fpage>–<lpage>1619</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1534/genetics.105.048223" xlink:type="simple">10.1534/genetics.105.048223</ext-link></comment> <object-id pub-id-type="pmid">16299396</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Haddrill</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Thornton</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Charlesworth</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Andolfatto</surname> <given-names>P</given-names></name>. <article-title>Multilocus patterns of nucleotide variability and the demographic and selection history of <italic>Drosophila melanogaster</italic> populations</article-title>. <source>Genome Research</source>. <year>2005</year>;<volume>15</volume>:<fpage>790</fpage>–<lpage>799</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1101/gr.3541005" xlink:type="simple">10.1101/gr.3541005</ext-link></comment> <object-id pub-id-type="pmid">15930491</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Andolfatto</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Przeworski</surname> <given-names>M</given-names></name>. <article-title>A Genome-Wide Departure From the Standard Neutral Model in Natural Populations of <italic>Drosophila</italic></article-title>. <source>Genetics</source>. <year>2000</year>;<volume>156</volume>:<fpage>257</fpage>–<lpage>268</lpage>. <object-id pub-id-type="pmid">10978290</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Glaser-Schmitt</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Catalán</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Parsch</surname> <given-names>J</given-names></name>. <article-title>Adaptive divergence of a transcriptional enhancer between populations of <italic>Drosophila melanogaster</italic></article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>2013</year>;<volume>368</volume>(<issue>1632</issue>):<fpage>20130024</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rstb.2013.0024" xlink:type="simple">10.1098/rstb.2013.0024</ext-link></comment> <object-id pub-id-type="pmid">24218636</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mueller</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Ram</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>McGraw</surname> <given-names>LA</given-names></name>, <name name-style="western"><surname>Qazi</surname> <given-names>MCB</given-names></name>, <name name-style="western"><surname>Siggia</surname> <given-names>ED</given-names></name>, <name name-style="western"><surname>Clark</surname> <given-names>AG</given-names></name>, <etal>et al</etal>. <article-title>Cross-Species Comparison of <italic>Drosophila</italic> Male Accessory Gland Protein Genes</article-title>. <source>Genetics</source>. <year>2005</year>;<volume>171</volume>(<issue>1</issue>):<fpage>131</fpage>–<lpage>143</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1534/genetics.105.043844" xlink:type="simple">10.1534/genetics.105.043844</ext-link></comment> <object-id pub-id-type="pmid">15944345</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref063">
<label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Liu</surname> <given-names>PZ</given-names></name>, <name name-style="western"><surname>Kaufman</surname> <given-names>TC</given-names></name>. <article-title><italic>Kruppel</italic> is a gap gene in the intermediate germband insect <italic>Oncopeltus fasciatus</italic> and is required for development of both blastoderm and germband-derived segments</article-title>. <source>Development</source>. <year>2004</year>;<volume>131</volume>:<fpage>4567</fpage>–<lpage>4579</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1242/dev.01311" xlink:type="simple">10.1242/dev.01311</ext-link></comment> <object-id pub-id-type="pmid">15342481</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref064">
<label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rubinstein</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Wolfner</surname> <given-names>MF</given-names></name>. <article-title>Reproductive hacking: A male seminal protein acts through intact reproductive pathways in female <italic>Drosophila</italic></article-title>. <source>Fly</source>. <year>2014</year>;<volume>8</volume>(<issue>2</issue>):<fpage>80</fpage>–<lpage>85</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.4161/fly.28396" xlink:type="simple">10.4161/fly.28396</ext-link></comment> <object-id pub-id-type="pmid">25483253</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref065">
<label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mohn</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Sienski</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Handler</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Brennecke</surname> <given-names>J</given-names></name>. <article-title>The Rhino-Deadlock-Cutoff Complex Licenses Noncanonical Transcription of Dual-Strand piRNA Clusters in <italic>Drosophila</italic></article-title>. <source>Cell</source>. <year>2014</year>;<volume>157</volume>(<issue>6</issue>):<fpage>1364</fpage>–<lpage>1379</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cell.2014.04.031" xlink:type="simple">10.1016/j.cell.2014.04.031</ext-link></comment> <object-id pub-id-type="pmid">24906153</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref066">
<label>66</label>
<mixed-citation publication-type="other" xlink:type="simple">Zhou H, Hu S, Matveev R, Yu Q, Li J, Khaitovich P, et al. A Chronological Atlas of Natural Selection in the Human Genome during the Past Half-million Years; 2015. BioRxiv preprint, <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1101/018929" xlink:type="simple">http://dx.doi.org/10.1101/018929</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004845.ref067">
<label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schrider</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Mendes</surname> <given-names>FK</given-names></name>, <name name-style="western"><surname>Hahn</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Kern</surname> <given-names>AD</given-names></name>. <article-title>Soft Shoulders Ahead: Spurious Signatures of Soft and Partial Selective Sweeps Result from Linked Hard Sweeps</article-title>. <source>Genetics</source>. <year>2015</year>;<volume>200</volume>(<issue>1</issue>):<fpage>267</fpage>–<lpage>284</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1534/genetics.115.174912" xlink:type="simple">10.1534/genetics.115.174912</ext-link></comment> <object-id pub-id-type="pmid">25716978</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref068">
<label>68</label>
<mixed-citation publication-type="other" xlink:type="simple">Ng A, Ngiam J, Foo CY, Mai Y, Suen C. Unsupervised Feature Learning and Deep Learning Tutorial; 2013. <ext-link ext-link-type="uri" xlink:href="http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial" xlink:type="simple">http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004845.ref069">
<label>69</label>
<mixed-citation publication-type="other" xlink:type="simple">Dodier R. LBFGS optimization routine, Java translation; 1999. <ext-link ext-link-type="uri" xlink:href="http://riso.sourceforge.net/" xlink:type="simple">http://riso.sourceforge.net/</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1004845.ref070">
<label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Novembre</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Bryc</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kutalik</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Boyko</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Auton</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Genes mirror geography within Europe</article-title>. <source>Nature</source>. <year>2008</year>;<volume>456</volume>:<fpage>98</fpage>–<lpage>101</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature07331" xlink:type="simple">10.1038/nature07331</ext-link></comment> <object-id pub-id-type="pmid">18758442</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref071">
<label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tajima</surname> <given-names>F</given-names></name>. <article-title>Statistical method for testing the neutral mutation hypothesis by DNA polymorphism</article-title>. <source>Genetics</source>. <year>1989</year>;<volume>123</volume>(<issue>3</issue>):<fpage>585</fpage>–<lpage>595</lpage>. <object-id pub-id-type="pmid">2513255</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004845.ref072">
<label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Garud</surname> <given-names>NR</given-names></name>, <name name-style="western"><surname>Messer</surname> <given-names>PW</given-names></name>, <name name-style="western"><surname>Buzbas</surname> <given-names>EO</given-names></name>, <name name-style="western"><surname>Petrov</surname> <given-names>DA</given-names></name>. <article-title>Recent selective sweeps in North American <italic>Drosophila melanogaster</italic> show signatures of soft sweeps</article-title>. <source>PLoS Genetics</source>. <year>2015</year>;<volume>11</volume>(<issue>2</issue>):<fpage>e1005004</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pgen.1005004" xlink:type="simple">10.1371/journal.pgen.1005004</ext-link></comment> <object-id pub-id-type="pmid">25706129</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>