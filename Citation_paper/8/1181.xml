<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group>
<journal-title>PLoS ONE</journal-title></journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-13-48418</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0112980</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology and life sciences</subject><subj-group><subject>Biotechnology</subject><subj-group><subject>Bioengineering</subject><subj-group><subject>Biomedical engineering</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Computer and information sciences</subject><subj-group><subject>Computer applications</subject></subj-group><subj-group><subject>Computing methods</subject><subj-group><subject>Computer graphics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Image processing</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Role of Gist and PHOG Features in Computer-Aided Diagnosis of Tuberculosis without Segmentation</article-title>
<alt-title alt-title-type="running-head">Computer-Aided Diagnosis of Tuberculosis</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Chauhan</surname><given-names>Arun</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Chauhan</surname><given-names>Devesh</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Rout</surname><given-names>Chittaranjan</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Department of Biotechnology and Bioinformatics, Jaypee University of Information Technology, Solan, Himachal Pradesh, India</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Department of Radiology, National Institute of TB and Respiratory Diseases, New Delhi, India</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Kestler</surname><given-names>Hans A.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Ulm, Germany</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">chittaranjan.rout@juit.ac.in</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: AC DC CR. Performed the experiments: AC. Analyzed the data: AC DC CR. Contributed reagents/materials/analysis tools: AC DC CR. Wrote the paper: AC CR. Designed and implemented the Matlab Toolbox: AC CR. Compilation of data sets: AC DC CR.</p></fn>
</author-notes>
<pub-date pub-type="collection"><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>12</day><month>11</month><year>2014</year></pub-date>
<volume>9</volume>
<issue>11</issue>
<elocation-id>e112980</elocation-id>
<history>
<date date-type="received"><day>12</day><month>4</month><year>2014</year></date>
<date date-type="accepted"><day>20</day><month>10</month><year>2014</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Chauhan et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract><sec>
<title>Purpose</title>
<p>Effective diagnosis of tuberculosis (TB) relies on accurate interpretation of radiological patterns found in a chest radiograph (CXR). Lack of skilled radiologists and other resources, especially in developing countries, hinders its efficient diagnosis. Computer-aided diagnosis (CAD) methods provide second opinion to the radiologists for their findings and thereby assist in better diagnosis of cancer and other diseases including TB. However, existing CAD methods for TB are based on the extraction of textural features from manually or semi-automatically segmented CXRs. These methods are prone to errors and cannot be implemented in X-ray machines for automated classification.</p>
</sec><sec>
<title>Methods</title>
<p>Gabor, Gist, histogram of oriented gradients (HOG), and pyramid histogram of oriented gradients (PHOG) features extracted from the whole image can be implemented into existing X-ray machines to discriminate between TB and non-TB CXRs in an automated manner. Localized features were extracted for the above methods using various parameters, such as frequency range, blocks and region of interest. The performance of these features was evaluated against textural features. Two digital CXR image datasets (8-bit DA and 14-bit DB) were used for evaluating the performance of these features.</p>
</sec><sec>
<title>Results</title>
<p>Gist (accuracy 94.2% for DA, 86.0% for DB) and PHOG (accuracy 92.3% for DA, 92.0% for DB) features provided better results for both the datasets. These features were implemented to develop a MATLAB toolbox, TB-Xpredict, which is freely available for academic use at <ext-link ext-link-type="uri" xlink:href="http://sourceforge.net/projects/tbxpredict/" xlink:type="simple">http://sourceforge.net/projects/tbxpredict/</ext-link>. This toolbox provides both automated training and prediction modules and does not require expertise in image processing for operation.</p>
</sec><sec>
<title>Conclusion</title>
<p>Since the features used in TB-Xpredict do not require segmentation, the toolbox can easily be implemented in X-ray machines. This toolbox can effectively be used for the mass screening of TB in high-burden areas with improved efficiency.</p>
</sec></abstract>
<funding-group><funding-statement>The authors have no support or funding to report.</funding-statement></funding-group><counts><page-count count="12"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>X-rays were discovered by Wilhelm Röntgen, a German physicist in 1895 and have revolutionized the field of diagnostics. Chest radiographs (CXRs) are ubiquitous in clinical diagnostics and make up about one-third of all radiological examinations. Chest radiographs are inexpensive and least invasive primary diagnostic tool for tuberculosis (TB). They are used in routine medical checkups and immigrant medical examinations even in well-equipped hospitals where blood and skin tests are available <xref ref-type="bibr" rid="pone.0112980-Tan1">[1]</xref>. The CXRs are also used for mass screening of TB in human immunodeficiency virus (HIV) endemic areas.</p>
<p>Tuberculosis is one of the leading infectious diseases with high mortality rate in developing and under-developed countries. Approximately 1.1 and 0.35 million of deaths were caused worldwide by the disease in non-HIV and HIV people respectively from about 8.8 million incidents reported in 2011 <xref ref-type="bibr" rid="pone.0112980-Global1">[2]</xref>. The disease is diagnosed on the basis of patient's symptoms, CXR and smear microscopy tests. Since the accuracy of smear microscopy test has been less than 50%, diagnosis relies primarily on the interpretation of radiological patterns found in a CXR. Early detection of TB often leads to success of anti-TB therapy, and also helps in gaining control over the transmission of infection as well as development of drug resistant TB. However, lack of resources and skilled radiologists particularly in rural areas of developing countries impedes its effective diagnosis. Therefore, computer-aided diagnosis (CAD) tools assume a lot of significance as they not only reduce diagnostic errors but also increase the efficiency of mass screening in poor-resource settings.</p>
<p>The CAD may be referred to a diagnosis made by a radiologist taking into account the results obtained through computer analysis. Success of the CAD methods in better diagnosis of breast cancer, colon cancer, lungs cancer, prostate cancer, coronary artery disease, congenital heart defects, etc., has suggested that TB may be detected more effectively by incorporating CAD methods <xref ref-type="bibr" rid="pone.0112980-Doi1">[3]</xref>–<xref ref-type="bibr" rid="pone.0112980-Dean1">[6]</xref>. The CAD techniques have been helpful in providing second opinion to the radiologists for their findings as well as assisting them in making their final decision regarding diagnosis with increased accuracy. Interpretation of CXRs is dependent on the expertise and skills of the radiologist, which is subject to human error. Even many district level hospitals in developing countries do not have skilled radiologists and incorrect diagnosis of TB takes place due to erroneous interpretation of CXR patterns. A well-trained CAD method can assume the role of a second CXR reader to some extent and may assist the radiologist in some facets of disease detection and decision making. Although these techniques may by no means be able to achieve the level of cognitive ability and knowledge of a radiologist, nonetheless, a trained classifier can perform the prediction consistently without the intra-observer and inter-observer variability. A recent study has also shown that the performance of CXR readers in diagnosing TB improved with the support of CAD <xref ref-type="bibr" rid="pone.0112980-Ginneken1">[7]</xref>.</p>
<p>Basic methodology employed in most of the earlier reported CAD studies was segmentation and extraction of grey-level co-occurrence matrix (GLCM) textural features <xref ref-type="bibr" rid="pone.0112980-Haralick1">[8]</xref>. Ginneken <italic>et al</italic>. used the GLCM textural features on two datasets for classification of CXRs as TB or non-TB, but the performance of both varied significantly <xref ref-type="bibr" rid="pone.0112980-Ginneken2">[9]</xref>. For one dataset, specificity of 90% was obtained while only 50% was achieved for the other. Detection of clavicle, and abnormalities in texture and shape in the CXRs were combined to develop a TB classification model by Hogeweg <italic>et al</italic> <xref ref-type="bibr" rid="pone.0112980-Hogeweg1">[10]</xref>. The area under the receiver operating characteristic curve (AUC) for the method was 0.86 <xref ref-type="bibr" rid="pone.0112980-Ginneken2">[9]</xref>. Semi-automated segmentation based classification model using textural features provided a prediction accuracy of 92.9% with sensitivity of 0.91 <xref ref-type="bibr" rid="pone.0112980-Tan1">[1]</xref>.</p>
<p>Current CAD methods employed for classification of CXR images are either object or region based <xref ref-type="bibr" rid="pone.0112980-Arzhaeva1">[11]</xref>–<xref ref-type="bibr" rid="pone.0112980-Shen1">[13]</xref>. These methods require segmentation and are prone to magnifying and carrying over low-level errors. The performance of GLCM features was inconsistent across various datasets <xref ref-type="bibr" rid="pone.0112980-Ginneken2">[9]</xref>, and these features lose significance when lungs have different shapes or CXRs have complex appearances due to overlapping of anatomical structures <xref ref-type="bibr" rid="pone.0112980-Tan1">[1]</xref>. Commonly used manual or semi-automatic segmentation methods not only require high expertise but also make the classification model data and machine dependent. Although automated segmentation methods have potential to be used for development of CAD techniques, but studies have shown that automated segmentation of TB CXRs failed in many cases leading to incorrect classification <xref ref-type="bibr" rid="pone.0112980-Tan1">[1]</xref>. Textural features are based on the implicit hypothesis that there exist some specific texture signatures which are dissimilar between non-TB and TB CXRs (<xref ref-type="fig" rid="pone-0112980-g001">Figure 1</xref>). The effectiveness of these textural signatures greatly depends on their ability to correlate with the disease. Since context of the features obtained with respect to the whole image are expected to be more useful than local textural features, therefore, contextual scene based Gist features were used in this study for the prediction of TB from CXR. This method captures biologically plausible features into a signatory low-dimensional vector <xref ref-type="bibr" rid="pone.0112980-Oliva1">[14]</xref>. These features identify salient locations within the image which differ significantly from their neighbours and also accumulate image statistics over the entire scene. Gabor <xref ref-type="bibr" rid="pone.0112980-Manjunath1">[15]</xref>, histogram of oriented gradients (HOG) <xref ref-type="bibr" rid="pone.0112980-Dalal1">[16]</xref>, and pyramid histogram of oriented gradients (PHOG) <xref ref-type="bibr" rid="pone.0112980-Bosch1">[17]</xref> features were also used in this study. PHOG features were computed as they contain local as well as global spatial information. The advantage with these methods is that they can be incorporated to X-ray machines for fully automated detection of TB. Localisation of features was done through optimising and fine-tuning various parameters, like region of interest (ROI), blocks and frequency range.</p>
<fig id="pone-0112980-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0112980.g001</object-id><label>Figure 1</label><caption>
<title>CXRs. Left: TB patient Right: Normal person.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0112980.g001" position="float" xlink:type="simple"/></fig></sec><sec id="s2" sec-type="materials|methods">
<title>Materials and Methods</title>
<p>The methodology is based on pixel-based classification which is illustrated in <xref ref-type="fig" rid="pone-0112980-g002">Figure 2</xref>. The proposed methodology was implemented in MATLAB 2010a.</p>
<fig id="pone-0112980-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0112980.g002</object-id><label>Figure 2</label><caption>
<title>Schematic representation of CAD implementation for TB.</title>
<p>CAD module is applied to the digital CXRs obtained from the computer attached to the the X-ray machine. The CAD module preprocesses (denoises) the image using SURE wavelet denoising, extract features (Gist and PHOG), selects relevant features using chi-square based feature selection method and finally classifies the CXR into TB and non-TB.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0112980.g002" position="float" xlink:type="simple"/></fig><sec id="s2a">
<title>Ethics statement</title>
<p>This study was approved by the institution, National Institute of TB and Respiratory Diseases, and the requirement to obtain informed consent was waived. The data was obtained and analysed anonymously.</p>
</sec><sec id="s2b">
<title>Dataset</title>
<p>Two CXR digital image datasets were obtained from two different X-ray machines available at the National Institute of Tuberculosis and Respiratory Diseases, New Delhi. These CXRs were taken in standard practice conditions followed by clinicians. In developing countries like India, the posterior-anterior (PA) CXRs are mostly taken and lateral CXRs are taken rarely. Therefore, only PA CXRs were used in the creation of the datasets. The CXRs were randomly collected over a period of one year with varied lung presentations. Dataset A (DA) was taken from Diagnox-4050 X-ray machine manufactured by Meditronics with CXRs digitised by AGFA CR35-X. Dataset B (DB) was obtained from PRORAD URS with Canon CXDI detectors. The tube voltage was set to 50 kV and 55 kV for DA and DB respectively. E-7252X and E-7254X X-ray tubes manufactured by Toshiba, Japan were used by Diagnox-4050 and PRORAD URS respectively. Although AGFA CR35-X acquires 12-bit grayscale CXRs, but 8-bit CXRs (DA) were obtained due to imaging system used by the radiologists. 14-bit CXR digital images (DB) were obtained by using PRORAD URS. For both the datasets DA and DB, the acquired images were resized to 1024×1024 resolutions to obtain images with identical dimensions. No change in classification accuracy was noticed with respect to original sized images. CXRs from each dataset were randomly divided into training and test sets. DA comprised of training set (52 non-TB and 52 TB CXRs) and the independent test set (26 non-TB and 26 TB CXRs). Similarly, DB comprised of training set (50 non-TB and 50 TB CXRs) and the independent test set (25 non-TB and 25 TB CXRs). Selection of all non-TB and TB cases was based on the consensus of independent review of each CXR by two highly experienced chest radiologists from National Institute of Tuberculosis and Respiratory Diseases, New Delhi, India and Indira Gandhi Medical College, Shimla, India. The selection of abnormal CXRs, varying from subtle to severe TB findings, was based on radiological findings through the consensus of the radiologists. Datasets are available freely at <ext-link ext-link-type="uri" xlink:href="http://sourceforge.net/projects/tbxpredict/files/data/" xlink:type="simple">http://sourceforge.net/projects/tbxpredict/files/data/</ext-link>.</p>
</sec><sec id="s2c">
<title>Segmentation</title>
<p>Segmentation is the process of dividing an image into a set of distinctive areas or regions that differ significantly qualitatively or quantitatively. It is a critical intermediate step in high level object-recognition tasks. Since existing methods required segmentation of CXRs, manual and automatic segmentation of CXRs for the dataset DA was done merely to evaluate the performance of our method (features extracted from whole image) against features extracted from manual and automatic segmentation. Manual segmentation of CXRs was done with the help of ImageJ software <xref ref-type="bibr" rid="pone.0112980-Abrmoff1">[18]</xref>. To evaluate the effectiveness of classification using features extracted from automated segmentation methods, Chan-Vese method <xref ref-type="bibr" rid="pone.0112980-Chan1">[19]</xref> was used for the segmentation of CXRs from the dataset DA (<xref ref-type="fig" rid="pone-0112980-g003">Figure 3</xref>). The Chan-Vese algorithm is a type of geometric active contour model which begins with a contour in the image plane defining an initial segmentation, and then this contour evolves with respect to evolution equation. The contour is evolved using level set method in such a manner that it terminates on the foreground boundaries. The level set function minimizes the Mumford-Shah functional, which is a type of “fitting energy” functional.</p>
<fig id="pone-0112980-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0112980.g003</object-id><label>Figure 3</label><caption>
<title>Chan-Vese active contour segmentation of a CXR.</title>
<p>a. Original CXR image b. Initial contour c. Iterations for foreground segmentation, and d. Segmented image.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0112980.g003" position="float" xlink:type="simple"/></fig>
<p>Initial contour was developed using Euler-number <xref ref-type="bibr" rid="pone.0112980-Wong1">[20]</xref> based segmentation with an auto-tuned threshold value as shown in <xref ref-type="fig" rid="pone-0112980-g004">Figure 4</xref>. The optimal threshold value for this segmentation is obtained on the basis of minimum and maximum values of the correlated image Euler numbers. Chan-Vese active contours segmentation was then used and the number of iterations for segmentation was fixed to 4000. The terminating criterion for segmentation does not depend on the gradient of the image.</p>
<fig id="pone-0112980-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0112980.g004</object-id><label>Figure 4</label><caption>
<title>X-ray image before and after Euler-based segmentation.</title>
<p>The Euler-based segmented image provides initial contour for Chan-Vese segmentation.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0112980.g004" position="float" xlink:type="simple"/></fig></sec><sec id="s2d">
<title>Preprocessing</title>
<p>Preprocessing involves denoising of the image. Wavelet-based denoising is a well-known method in the field of digital image processing. Mean, Gaussian, anisotropic diffusion, Fourier transformation and the SURE orthonormal wavelet filters <xref ref-type="bibr" rid="pone.0112980-Luisier1">[21]</xref> were evaluated in this study for denoising of the images. Finally, orthonormal wavelet denoising of the image was used for this study as it provided best denoising effect with respect to visual inspection. The denoising process is parameterized as the sum of elementary non-linear processes with unknown weights. The estimate of mean square error (MSE) between denoised and clean image is minimized using Stein's unbiased risk estimate (SURE), a statistically unbiased estimate that depends only on the noisy image. The existence of this as a priori estimate avoids the requirement of statistical model for wavelet coefficients.</p>
</sec><sec id="s2e">
<title>Feature extraction</title>
<p>Textural (GLCM and Gabor) as well as other features, like Gist, HOG and PHOG were extracted from the digital CXRs. Details of the feature extraction methods used in this study are given below:</p>
</sec><sec id="s2f">
<title>GLCM textural features</title>
<p>Textural features are calculated based on the statistical distribution of combinations of pixel intensities at specific positions relative to each other. Based on the number of pixels in each combination, statistics are categorized into first-, second- and higher-order statistics. The GLCM method extracts second-order statistics textural features. GLCM is a matrix with rows and columns equal to the number of grey levels. The matrix element G(i,j | dx,dy) is the relative frequency with which two pixels occur within a given neighbourhood, where i and j are pixel intensities separated by distance (dx,dy). Various textural features were extracted from this matrix (for details refer to <xref ref-type="supplementary-material" rid="pone.0112980.s001">Table S1</xref>). A two pixel offset (2,0 and 0,2) was used in this study.</p>
</sec><sec id="s2g">
<title>Gabor features</title>
<p>Gabor filters have been found to be very effective in texture representation and discrimination <xref ref-type="bibr" rid="pone.0112980-Manjunath1">[15]</xref>, <xref ref-type="bibr" rid="pone.0112980-Grigorescu1">[22]</xref>, <xref ref-type="bibr" rid="pone.0112980-Yang1">[23]</xref>. These filters with different frequencies and orientations are used for extracting textural features from an image. A 2-D Gabor function g(x,y) and its Fourier transform G(u,v) are calculated using <xref ref-type="disp-formula" rid="pone.0112980.e001">Eq. (1)</xref> and <xref ref-type="disp-formula" rid="pone.0112980.e003">Eq. (2)</xref> respectively:</p>
<p><disp-formula id="pone.0112980.e001"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0112980.e001" xlink:type="simple"/><label>(Eq. (1))</label></disp-formula></p>
<p>Where, j  = <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0112980.e002" xlink:type="simple"/></inline-formula>, and W is the frequency of the Gabor function</p>
<p><disp-formula id="pone.0112980.e003"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0112980.e003" xlink:type="simple"/><label>(Eq. (2))</label></disp-formula></p>
<p>Where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0112980.e004" xlink:type="simple"/></inline-formula></p>
<p>Impulse responses of Gabor filters are rotated and scaled versions of the above function. The Gabor filters can be considered as edge detectors with adjustable orientations and scales. A self similar filter dictionary is obtained by the association of rotation parameter θ and scale factor α with the Gabor function g(x,y). Scales and orientations of the Gabor wavelets are represented by M and N respectively.</p>
<p><disp-formula id="pone.0112980.e005"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0112980.e005" xlink:type="simple"/><label>(Eq. (3))</label></disp-formula><disp-formula id="pone.0112980.e006"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0112980.e006" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0112980.e007" xlink:type="simple"/></inline-formula> and K is the total number of orientations.</p>
<p>Gabor wavelet transform for a given image I(x,y) is defined as in <xref ref-type="disp-formula" rid="pone.0112980.e008">Eq. (4)</xref>:</p>
<p><disp-formula id="pone.0112980.e008"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0112980.e008" xlink:type="simple"/><label>(Eq. (4))</label></disp-formula></p>
<p>Where, * represents complex conjugate.</p>
<p>Spatial homogeneity of local textural regions is assumed. µ<sub>mn</sub> and σ<sub>mn</sub> are the mean and standard deviation of the magnitude of Gabor wavelet transform coefficients respectively, and are used to represent regions for classification and retrieval of images.</p>
<p><disp-formula id="pone.0112980.e009"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0112980.e009" xlink:type="simple"/></disp-formula>and</p>
<p><disp-formula id="pone.0112980.e010"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0112980.e010" xlink:type="simple"/><label>(Eq. (5))</label></disp-formula></p>
<p>Gabor features are created using µ<sub>mn</sub> and σ<sub>mn</sub> as feature components. Number of scales and orientations was set to six and ten respectively for creating feature vectors in this study. Frequency range was set between 0.01 and 0.4.</p>
</sec><sec id="s2h">
<title>Gist features</title>
<p>These are global image features and they assist in characterizing various important statistics of a scene. These features are computed by convoluting the filter with an image at different scales and orientations. Thus, high and low frequency repetitive gradient directions of an image can be measured. The scores for filter convolution at each orientation and scale are used as Gist features for an image. These features are currently being used for scene classification <xref ref-type="bibr" rid="pone.0112980-Oliva1">[14]</xref>, <xref ref-type="bibr" rid="pone.0112980-Siagian1">[24]</xref>–<xref ref-type="bibr" rid="pone.0112980-Itti1">[26]</xref>. The first step of Gist feature extraction is filtering of input image into a number of low level visual feature channels, like intensity, colour, orientation, motion and flicker at multiple spatial scales. Center-surround operations within each sub-channel i are performed between filter output maps, O<sub>i</sub>(s), at different scales s. This yields feature maps M<sub>i</sub>(c, s), given a “center” scale c and “surround” scale s. Across scale difference (Θ) between two feature maps is computed by interpolation to the center scale and pointwise absolute difference.</p>
<p>For color and intensity channels, feature maps are computed using<xref ref-type="disp-formula" rid="pone.0112980.e011">Eq. (6)</xref>:</p>
<p><disp-formula id="pone.0112980.e011"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0112980.e011" xlink:type="simple"/><label>(Eq. (6))</label></disp-formula></p>
<p>Feature maps are used to detect conspicuous regions in each channel and are merged linearly to yield a saliency map. Information from the orientation channel is incorporated by employing Gabor filters to the grayscale image (<xref ref-type="disp-formula" rid="pone.0112980.e012">Eq. (7)</xref>) at four spatial center scales (c = 0, 1, 2, 3) and at four different angles (θ<sub>i</sub> = 0<sup>°</sup>, 45<sup>°</sup>, 90<sup>°</sup>, 135<sup>°</sup>).</p>
<p>For orientation channels, feature maps are computed using <xref ref-type="disp-formula" rid="pone.0112980.e012">Eq. (7)</xref>:</p>
<p><disp-formula id="pone.0112980.e012"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0112980.e012" xlink:type="simple"/><label>(Eq. (7))</label></disp-formula></p>
<p>After the computation of low-level center-surround features, each sub-channel extracts a gist vector from its corresponding feature map. Averaging operations are applied in a 4×4 grid of subregions over the map. Sixteen raw gist features (G<sub>i</sub><sup>k,l</sup>(c, s)) are computed per feature map using <xref ref-type="disp-formula" rid="pone.0112980.e013">Eq. (8)</xref>:</p>
<p><disp-formula id="pone.0112980.e013"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0112980.e013" xlink:type="simple"/><label>(Eq. (8))</label></disp-formula>where W and H are width and height of the image respectively.</p>
<p>k and l are the indices in horizontal and vertical directions respectively.</p>
<p>12 orientations per scale and 8 blocks were used for this study.</p>
</sec><sec id="s2i">
<title>HOG features</title>
<p>This method was first introduced by Dalal and Trigg <xref ref-type="bibr" rid="pone.0112980-Dalal1">[16]</xref>. HOG breaks up a CXR image into small cells, computes the HOGs for each cell, normalizes HOGs using block pattern, and provides a descriptor for each cell <xref ref-type="bibr" rid="pone.0112980-Ludwig1">[27]</xref>. These features are generally used for object detection in an image. The basic idea behind HOG features is that shape and appearance of local objects within an image can be described by the intensity gradients distribution. This method involves counting the occurrence of gradient orientation and thereby maintains photometric transformations and geometric invariance. The descriptor generation is comprised of four main steps: gradient computation, orientation binning, descriptor blocks generation, and block normalization. In gradient computation, the filtering of intensity or color data of the image is done with 1-D centred discrete derivative masks (D<sub>x</sub> = [−1,0,1] and D<sub>y</sub> = [−1,0,1]<sup>T</sup>) in horizontal and/or vertical directions. For a given image I, x and y derivatives are obtained using a convolution operation I<sub>x</sub> = I * D<sub>x</sub> and I<sub>y</sub> = I * D<sub>y.</sub> Magnitude and orientation of the gradient is computed by the following equations <xref ref-type="disp-formula" rid="pone.0112980.e014">Eq. (9)</xref> and <xref ref-type="disp-formula" rid="pone.0112980.e015">Eq. (10)</xref> respectively:</p>
<p><disp-formula id="pone.0112980.e014"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0112980.e014" xlink:type="simple"/><label>(Eq. (9))</label></disp-formula></p>
<p><disp-formula id="pone.0112980.e015"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0112980.e015" xlink:type="simple"/><label>(Eq. (10))</label></disp-formula></p>
<p>Orientation binning involves the creation of a cell histogram. Weighted vote is cast by each pixel within a cell based on the gradient computation values. The histogram channels are uniformly spread over 0 to 360 or 0 to 180 degrees depending on signed or unsigned gradient respectively. The cells are then grouped together into spatially connected blocks to account for changes in contrast and illumination. These blocks usually overlap, and two main geometries R-HOG (rectangular) and C-HOG (circular) blocks exist. The blocks are normalised using one of the following normalization factors:</p>
<p>L1-norm: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0112980.e016" xlink:type="simple"/></inline-formula>)</p>
<p>L1-sqrt: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0112980.e017" xlink:type="simple"/></inline-formula></p>
<p>L2-norm: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0112980.e018" xlink:type="simple"/></inline-formula></p>
<p>L2-hys: L2-norm equation with maximum value of ν limited to 0.2 followed by renormalization.</p>
<p>In this study, block size and number of bins were set to 6×6 and 9 respectively. R-HOG was used as it provided better classification accuracy as compared to C-HOG.</p>
</sec><sec id="s2j">
<title>PHOG features</title>
<p>These are spatial shape descriptors which describe an image by its spatial layout and local shape <xref ref-type="bibr" rid="pone.0112980-Bosch1">[17]</xref>. These descriptors are comprised of HOGs over each sub-region of an image at each pyramid resolution level. Local shapes in an image are captured by the distribution over edge orientations within a region while the spatial layout is captured by tiling an image into regions at multiple resolutions.</p>
<p>Initially, the HOG for entire image is computed. The bin size for HOG is fixed to N bins. The CXR is divided into a sequence of increasingly finer spatial grids by repetitively doubling the number of divisions along each axis. The number of points in each grid cell is stored. Since the number of points in a cell at one level is equal to the sum of points contained in the four cells it is divided into at the next level, this is a pyramid representation. HOG is computed for each of these levels. The bin count for the histogram representing a level is the cell count at that level of resolution. This process is repeated until a depth L. During pyramid formation, 2<sup>L</sup> cells are present along each dimension for the grid at level L. PHOG features obtained for an image is a weighted combination of the above HOG features. The summation of calculated PHOG values is then normalized to unity to ascertain that texture rich images are not weighted more strongly than others. In this study, the number of bins on histogram and number of pyramid levels at 360 degrees were set to 8 and 3 respectively.</p>
</sec><sec id="s2k">
<title>Feature selection</title>
<p>Feature selection was performed using chi-square based method. Although several other feature selection methods such as correlation based feature selection, info gain and kernel principal component analysis were evaluated, but the performance of chi-square based feature selection method using WEKA software <xref ref-type="bibr" rid="pone.0112980-Hall1">[28]</xref> was found to be the best with respect to classification results. This method uses chi-square statistics to discretize features repeatedly until some inconsistency is found in the data. Feature ranking was also done by this method and the best features were used for classification of a CXR as TB or non-TB.</p>
</sec><sec id="s2l">
<title>Classification</title>
<p>Support vector machines (SVMs) <xref ref-type="bibr" rid="pone.0112980-Cortes1">[29]</xref>, a machine learning algorithm, is commonly used for classification and regression. The objective of SVM is to find the hyperplane which maximizes distance between data points from two classes. Training data points closest to the hyperplane are called support vectors (<xref ref-type="fig" rid="pone-0112980-g005">Figure 5</xref>). The hyperplane can be defined as:</p>
<fig id="pone-0112980-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0112980.g005</object-id><label>Figure 5</label><caption>
<title>Hyperplane (blue line) representation in SVM.</title>
<p>Red and blue circles represent data points from two different classes. Solid filled circles denote support vectors.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0112980.g005" position="float" xlink:type="simple"/></fig>
<p><disp-formula id="pone.0112980.e019"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0112980.e019" xlink:type="simple"/></disp-formula>where β, β<sub>0</sub> and x are the weight vector, bias and support vectors respectively.</p>
<p>The SVM is a non-probabilistic binary linear classifier, but it can also perform a non-linear classification using the kernel trick by mapping the data points into high-dimensional feature spaces. Some common kernel methods are polynomial homogeneous, polynomial inhomogeneous, Gaussian radial basis function and hyperbolic tangent. Gaussian radial basis function was used in the study. LIBSVM <xref ref-type="bibr" rid="pone.0112980-Chang1">[30]</xref>, a supervised machine learning software, was used for labelling (classification) of a given CXR as TB or non-TB.</p>
</sec></sec><sec id="s3">
<title>Results and Discussion</title>
<p>Current CAD methods use local GLCM textural features extracted from segmented CXRs for the classification of CXRs. Segmentation is an important step for the optimal extraction of features. Implementation of CAD method into X-ray machines for the automatic prediction requires features to be extracted from automatically segmented or whole CXR images. The ground truth that we were striving to reproduce by this CAD method for the two datasets was the judgement or decision of radiologists who read the CXRs.</p>
<sec id="s3a">
<title>Performance of features extracted from automatically segmented CXRs</title>
<p>The performance of the GLCM textural features was quite low for the CXR dataset DA (<xref ref-type="table" rid="pone-0112980-t001">Table 1</xref>). Prediction accuracy of only 65.4% was obtained for GLCM textural features extracted from automatically segmented CXRs, while 80.8% was obtained for manually segmented CXRs from dataset DA. The poor performance of the features extracted from automatic segmented CXRs may be attributed to severe intensity distortions <xref ref-type="bibr" rid="pone.0112980-Tan1">[1]</xref>, variability in lungs shape, and complex appearances of CXRs due to overlapping of anatomical structures.</p>
<table-wrap id="pone-0112980-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0112980.t001</object-id><label>Table 1</label><caption>
<title>Performance of classification of CXRs dataset, DA using various feature extraction methods.</title>
</caption><alternatives><graphic id="pone-0112980-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0112980.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Feature<xref ref-type="table-fn" rid="nt101">*</xref></td>
<td align="left" rowspan="1" colspan="1">Accuracy</td>
<td align="left" rowspan="1" colspan="1">Sensitivity</td>
<td align="left" rowspan="1" colspan="1">Specificity</td>
<td align="left" rowspan="1" colspan="1">Precision</td>
<td align="left" rowspan="1" colspan="1">F-score</td>
<td align="left" rowspan="1" colspan="1">GMean</td>
<td align="left" rowspan="1" colspan="1">AUC</td>
<td align="left" rowspan="1" colspan="1">CI<xref ref-type="table-fn" rid="nt102">#</xref></td>
<td align="left" rowspan="1" colspan="1">SE<xref ref-type="table-fn" rid="nt102">#</xref></td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Gabor<sup>1</sup></td>
<td align="left" rowspan="1" colspan="1">0.865</td>
<td align="left" rowspan="1" colspan="1">0.808</td>
<td align="left" rowspan="1" colspan="1">0.923</td>
<td align="left" rowspan="1" colspan="1">0.913</td>
<td align="left" rowspan="1" colspan="1">0.857</td>
<td align="left" rowspan="1" colspan="1">0.864</td>
<td align="left" rowspan="1" colspan="1">0.888</td>
<td align="left" rowspan="1" colspan="1">0.769–0.958</td>
<td align="left" rowspan="1" colspan="1">0.0482</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Gab<sup>2</sup></td>
<td align="left" rowspan="1" colspan="1">0.865</td>
<td align="left" rowspan="1" colspan="1">0.885</td>
<td align="left" rowspan="1" colspan="1">0.846</td>
<td align="left" rowspan="1" colspan="1">0.852</td>
<td align="left" rowspan="1" colspan="1">0.868</td>
<td align="left" rowspan="1" colspan="1">0.865</td>
<td align="left" rowspan="1" colspan="1">0.919</td>
<td align="left" rowspan="1" colspan="1">0.809–0.976</td>
<td align="left" rowspan="1" colspan="1">0.0392</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Gab<sup>3</sup></td>
<td align="left" rowspan="1" colspan="1">0.712</td>
<td align="left" rowspan="1" colspan="1">0.654</td>
<td align="left" rowspan="1" colspan="1">0.769</td>
<td align="left" rowspan="1" colspan="1">0.739</td>
<td align="left" rowspan="1" colspan="1">0.694</td>
<td align="left" rowspan="1" colspan="1">0.709</td>
<td align="left" rowspan="1" colspan="1">0.771</td>
<td align="left" rowspan="1" colspan="1">0.633–0.876</td>
<td align="left" rowspan="1" colspan="1">0.0668</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Gist<sup>1</sup></td>
<td align="left" rowspan="1" colspan="1">0.942</td>
<td align="left" rowspan="1" colspan="1">0.961</td>
<td align="left" rowspan="1" colspan="1">0.923</td>
<td align="left" rowspan="1" colspan="1">0.925</td>
<td align="left" rowspan="1" colspan="1">0.943</td>
<td align="left" rowspan="1" colspan="1">0.942</td>
<td align="left" rowspan="1" colspan="1">0.957</td>
<td align="left" rowspan="1" colspan="1">0.861–0.994</td>
<td align="left" rowspan="1" colspan="1">0.0301</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Gist<sup>2</sup></td>
<td align="left" rowspan="1" colspan="1">0.885</td>
<td align="left" rowspan="1" colspan="1">0.846</td>
<td align="left" rowspan="1" colspan="1">0.923</td>
<td align="left" rowspan="1" colspan="1">0.917</td>
<td align="left" rowspan="1" colspan="1">0.880</td>
<td align="left" rowspan="1" colspan="1">0.884</td>
<td align="left" rowspan="1" colspan="1">0.905</td>
<td align="left" rowspan="1" colspan="1">0.792–0.969</td>
<td align="left" rowspan="1" colspan="1">0.0444</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Gist<sup>3</sup></td>
<td align="left" rowspan="1" colspan="1">0.827</td>
<td align="left" rowspan="1" colspan="1">0.808</td>
<td align="left" rowspan="1" colspan="1">0.846</td>
<td align="left" rowspan="1" colspan="1">0.840</td>
<td align="left" rowspan="1" colspan="1">0.824</td>
<td align="left" rowspan="1" colspan="1">0.827</td>
<td align="left" rowspan="1" colspan="1">0.868</td>
<td align="left" rowspan="1" colspan="1">0.746–0.946</td>
<td align="left" rowspan="1" colspan="1">0.0497</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">HOG<sup>1</sup></td>
<td align="left" rowspan="1" colspan="1">0.865</td>
<td align="left" rowspan="1" colspan="1">0.846</td>
<td align="left" rowspan="1" colspan="1">0.885</td>
<td align="left" rowspan="1" colspan="1">0.880</td>
<td align="left" rowspan="1" colspan="1">0.863</td>
<td align="left" rowspan="1" colspan="1">0.865</td>
<td align="left" rowspan="1" colspan="1">0.904</td>
<td align="left" rowspan="1" colspan="1">0.790–0.968</td>
<td align="left" rowspan="1" colspan="1">0.0456</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">HOG<sup>2</sup></td>
<td align="left" rowspan="1" colspan="1">0.846</td>
<td align="left" rowspan="1" colspan="1">0.769</td>
<td align="left" rowspan="1" colspan="1">0.923</td>
<td align="left" rowspan="1" colspan="1">0.909</td>
<td align="left" rowspan="1" colspan="1">0.833</td>
<td align="left" rowspan="1" colspan="1">0.843</td>
<td align="left" rowspan="1" colspan="1">0.880</td>
<td align="left" rowspan="1" colspan="1">0.760–0.954</td>
<td align="left" rowspan="1" colspan="1">0.0483</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">HOG<sup>3</sup></td>
<td align="left" rowspan="1" colspan="1">0.865</td>
<td align="left" rowspan="1" colspan="1">0.846</td>
<td align="left" rowspan="1" colspan="1">0.885</td>
<td align="left" rowspan="1" colspan="1">0.880</td>
<td align="left" rowspan="1" colspan="1">0.863</td>
<td align="left" rowspan="1" colspan="1">0.865</td>
<td align="left" rowspan="1" colspan="1">0.907</td>
<td align="left" rowspan="1" colspan="1">0.793–0.970</td>
<td align="left" rowspan="1" colspan="1">0.0440</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">PHOG<sup>1</sup></td>
<td align="left" rowspan="1" colspan="1">0.923</td>
<td align="left" rowspan="1" colspan="1">0.885</td>
<td align="left" rowspan="1" colspan="1">0.962</td>
<td align="left" rowspan="1" colspan="1">0.958</td>
<td align="left" rowspan="1" colspan="1">0.920</td>
<td align="left" rowspan="1" colspan="1">0.922</td>
<td align="left" rowspan="1" colspan="1">0.956</td>
<td align="left" rowspan="1" colspan="1">0.859–0.993</td>
<td align="left" rowspan="1" colspan="1">0.0294</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">PHOG<sup>2</sup></td>
<td align="left" rowspan="1" colspan="1">0.827</td>
<td align="left" rowspan="1" colspan="1">0.769</td>
<td align="left" rowspan="1" colspan="1">0.885</td>
<td align="left" rowspan="1" colspan="1">0.870</td>
<td align="left" rowspan="1" colspan="1">0.816</td>
<td align="left" rowspan="1" colspan="1">0.825</td>
<td align="left" rowspan="1" colspan="1">0.859</td>
<td align="left" rowspan="1" colspan="1">0.735–0.940</td>
<td align="left" rowspan="1" colspan="1">0.0520</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">PHOG<sup>3</sup></td>
<td align="left" rowspan="1" colspan="1">0.808</td>
<td align="left" rowspan="1" colspan="1">0.846</td>
<td align="left" rowspan="1" colspan="1">0.769</td>
<td align="left" rowspan="1" colspan="1">0.786</td>
<td align="left" rowspan="1" colspan="1">0.815</td>
<td align="left" rowspan="1" colspan="1">0.807</td>
<td align="left" rowspan="1" colspan="1">0.828</td>
<td align="left" rowspan="1" colspan="1">0.698–0.919</td>
<td align="left" rowspan="1" colspan="1">0.0577</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">GLCM<sup>2</sup></td>
<td align="left" rowspan="1" colspan="1">0.808</td>
<td align="left" rowspan="1" colspan="1">0.769</td>
<td align="left" rowspan="1" colspan="1">0.846</td>
<td align="left" rowspan="1" colspan="1">0.833</td>
<td align="left" rowspan="1" colspan="1">0.800</td>
<td align="left" rowspan="1" colspan="1">0.807</td>
<td align="left" rowspan="1" colspan="1">0.859</td>
<td align="left" rowspan="1" colspan="1">0.735–0.940</td>
<td align="left" rowspan="1" colspan="1">0.0510</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">GLCM<sup>3</sup></td>
<td align="left" rowspan="1" colspan="1">0.654</td>
<td align="left" rowspan="1" colspan="1">0.769</td>
<td align="left" rowspan="1" colspan="1">0.538</td>
<td align="left" rowspan="1" colspan="1">0.625</td>
<td align="left" rowspan="1" colspan="1">0.690</td>
<td align="left" rowspan="1" colspan="1">0.644</td>
<td align="left" rowspan="1" colspan="1">0.623</td>
<td align="left" rowspan="1" colspan="1">0.478–0.753</td>
<td align="left" rowspan="1" colspan="1">0.0803</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt101"><label/><p>*Features extracted from: <sup>1</sup> indicates whole CXR, <sup>2</sup> indicates manually segmented CXR and <sup>3</sup> indicates automatic segmented CXR.</p></fn><fn id="nt102"><label>#</label><p>CI refers to confidence interval at 95% at P-value &lt;0.0001 whereas SE refers to standard error for AUC.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s3b">
<title>Performance of features extracted from whole-CXR and manually segmented CXRs</title>
<p>Since low prediction accuracy was obtained using GLCM method, therefore, features extracted from the whole image, like Gabor, Gist, HOG, and PHOG were evaluated for their ability to discriminate between the TB and non-TB CXRs. The parameters were fine-tuned, so that the features relevant to TB discrimination were extracted. The Gabor, Gist, HOG, and PHOG features extracted from the whole CXR provided a prediction accuracy of 86.5%, 94.2%, 86.5%, and 92.3% respectively for dataset DA, and 92.0%, 86.0%, 86.0%, and 90% respectively for dataset DB (<xref ref-type="table" rid="pone-0112980-t001">Table 1</xref> and <xref ref-type="table" rid="pone-0112980-t002">2</xref>). The Gist features provided the maximum sensitivity of 0.961 for the dataset DA. This outcome is significant because sensitivity is considered as major criterion when measuring the effectiveness of a classification model for the detection of diseases. Although Gabor features provided comparable prediction accuracy to PHOG for dataset DB, but the latter provided significantly better 5-fold cross-validation accuracy of 86.6% in comparison to the former's 81.5%. Also, Gabor feature extraction is a computationally intensive process than PHOG method. For 5-fold cross-validation, the dataset were divided into five equal parts. One part was used for testing while the other four parts were used for the training of SVM. The procedure was repeated five times. The performance of features extracted from manually segmented images was comparable or lower than to those extracted from whole CXR image (<xref ref-type="table" rid="pone-0112980-t001">Table 1</xref>). This may be attributed to the better spatial and contextual information provided by the whole CXR in comparison to the segmented one. Comparable performance between the whole and manually lung segmented CXRs indicated that these features (Gist, Gabor, HOG, and PHOG) were able to capture discriminative features without the need for segmentation of lungs (<xref ref-type="table" rid="pone-0112980-t001">Table 1</xref>). Comparison of performance for various features is provided in the form bar graph in <xref ref-type="fig" rid="pone-0112980-g006">Figure 6</xref>.</p>
<fig id="pone-0112980-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0112980.g006</object-id><label>Figure 6</label><caption>
<title>Comparison of performance for various features from whole, manually segmented, and automatically segmented CXRs.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0112980.g006" position="float" xlink:type="simple"/></fig><table-wrap id="pone-0112980-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0112980.t002</object-id><label>Table 2</label><caption>
<title>Performance of classification of CXRs dataset, DB using various feature extraction methods.</title>
</caption><alternatives><graphic id="pone-0112980-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0112980.t002" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Feature<xref ref-type="table-fn" rid="nt103">*</xref></td>
<td align="left" rowspan="1" colspan="1">Accuracy</td>
<td align="left" rowspan="1" colspan="1">Sensitivity</td>
<td align="left" rowspan="1" colspan="1">Specificity</td>
<td align="left" rowspan="1" colspan="1">Precision</td>
<td align="left" rowspan="1" colspan="1">F-score</td>
<td align="left" rowspan="1" colspan="1">GMean</td>
<td align="left" rowspan="1" colspan="1">AUC</td>
<td align="left" rowspan="1" colspan="1">CI<xref ref-type="table-fn" rid="nt104">#</xref></td>
<td align="left" rowspan="1" colspan="1">SE<xref ref-type="table-fn" rid="nt104">#</xref></td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Gab<sup>1</sup></td>
<td align="left" rowspan="1" colspan="1">0.920</td>
<td align="left" rowspan="1" colspan="1">0.920</td>
<td align="left" rowspan="1" colspan="1">0.920</td>
<td align="left" rowspan="1" colspan="1">0.920</td>
<td align="left" rowspan="1" colspan="1">0.920</td>
<td align="left" rowspan="1" colspan="1">0.920</td>
<td align="left" rowspan="1" colspan="1">0.936</td>
<td align="left" rowspan="1" colspan="1">0.829–0.986</td>
<td align="left" rowspan="1" colspan="1">0.0377</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Gist<sup>1</sup></td>
<td align="left" rowspan="1" colspan="1">0.860</td>
<td align="left" rowspan="1" colspan="1">0.880</td>
<td align="left" rowspan="1" colspan="1">0.840</td>
<td align="left" rowspan="1" colspan="1">0.846</td>
<td align="left" rowspan="1" colspan="1">0.863</td>
<td align="left" rowspan="1" colspan="1">0.860</td>
<td align="left" rowspan="1" colspan="1">0.893</td>
<td align="left" rowspan="1" colspan="1">0.773–0.962</td>
<td align="left" rowspan="1" colspan="1">0.0520</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">HOG<sup>1</sup></td>
<td align="left" rowspan="1" colspan="1">0.860</td>
<td align="left" rowspan="1" colspan="1">0.800</td>
<td align="left" rowspan="1" colspan="1">0.920</td>
<td align="left" rowspan="1" colspan="1">0.909</td>
<td align="left" rowspan="1" colspan="1">0.851</td>
<td align="left" rowspan="1" colspan="1">0.858</td>
<td align="left" rowspan="1" colspan="1">0.909</td>
<td align="left" rowspan="1" colspan="1">0.793–0.972</td>
<td align="left" rowspan="1" colspan="1">0.0434</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">PHOG<sup>1</sup></td>
<td align="left" rowspan="1" colspan="1">0.900</td>
<td align="left" rowspan="1" colspan="1">0.920</td>
<td align="left" rowspan="1" colspan="1">0.880</td>
<td align="left" rowspan="1" colspan="1">0.885</td>
<td align="left" rowspan="1" colspan="1">0.902</td>
<td align="left" rowspan="1" colspan="1">0.900</td>
<td align="left" rowspan="1" colspan="1">0.918</td>
<td align="left" rowspan="1" colspan="1">0.806–0.977</td>
<td align="left" rowspan="1" colspan="1">0.0404</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt103"><label/><p>*Features extracted from: <sup>1</sup> indicates whole CXR, <sup>2</sup> indicates manually segmented CXR and <sup>3</sup> indicates automatic segmented CXR.</p></fn><fn id="nt104"><label>#</label><p>CI refers to confidence interval at 95% at P-value &lt;0.0001 whereas SE refers to standard error for AUC.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s3c">
<title>Validation</title>
<p>The performance of various feature extraction methods used in this study is provided in <xref ref-type="table" rid="pone-0112980-t001">Table 1</xref> and <xref ref-type="table" rid="pone-0112980-t002">2</xref>. To validate the proposed methodology, five-fold cross validation and ROC curve analysis were done. PHOG features provided five-fold cross-validation accuracies of 83.2% and 86.6% for the datasets DA and DB respectively while 84.4% and 86.6% were obtained for the same datasets using Gist features.</p>
<p>The AUC is a plot between true positive and false positive rates and it determines how well a model can distinguish between the TB and non-TB CXRs <xref ref-type="bibr" rid="pone.0112980-Zweig1">[31]</xref>, <xref ref-type="bibr" rid="pone.0112980-Metz1">[32]</xref>. <xref ref-type="fig" rid="pone-0112980-g007">Figure 7</xref> and <xref ref-type="fig" rid="pone-0112980-g008">8</xref> show high value of AUC for both Gist and PHOG features indicating a good discrimination between the TB and non-TB CXRs <xref ref-type="bibr" rid="pone.0112980-Robin1">[33]</xref>. Every point on the ROC curve represents a specificity and sensitivity pair corresponding to a certain decision threshold. Higher is the overall accuracy of classification; closer is the ROC curve to the upper left corner. When a classifier cannot discriminate between the two classes or groups, the AUC would be equal to 0.5. When there is a perfect discrimination, the AUC would be equal to 1. The 95% confidence interval (CI) is the interval in which the true AUC lies with 95% confidence. P-value is the probability that the observed AUC is found when actually the true AUC is 0.5. If P is smaller than 0.05, it can be concluded that the AUC is significantly different from 0.5 and the classifier has the ability to discriminate between the two classes. High AUC values were obtained (<xref ref-type="table" rid="pone-0112980-t001">Table 1</xref> and <xref ref-type="table" rid="pone-0112980-t002">2</xref>) at 95% confidence interval for our methods at P-level (AUC = 0.5) smaller than 0.0001. Standard error (SE) of AUC was also calculated using the method of Delong <italic>et al</italic>. <xref ref-type="bibr" rid="pone.0112980-DeLong1">[34]</xref> and SE values were found to be quite low (<xref ref-type="table" rid="pone-0112980-t001">Table 1</xref> and <xref ref-type="table" rid="pone-0112980-t002">2</xref>). As inferred from the results of this study, the Gist and PHOG features were quite robust and provided better results without segmentation.</p>
<fig id="pone-0112980-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0112980.g007</object-id><label>Figure 7</label><caption>
<title>ROC plot for classification between TB and non-TB CXRs (DA) using Gist features.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0112980.g007" position="float" xlink:type="simple"/></fig><fig id="pone-0112980-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0112980.g008</object-id><label>Figure 8</label><caption>
<title>ROC plot for classification between TB and non-TB CXRs (DA) using PHOG features.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0112980.g008" position="float" xlink:type="simple"/></fig></sec><sec id="s3d">
<title>TB-Xpredict</title>
<p>Gist and PHOG features were implemented to develop TB-Xpredict, a MATLAB toolbox available at <ext-link ext-link-type="uri" xlink:href="http://sourceforge.net/projects/tbxpredict/" xlink:type="simple">http://sourceforge.net/projects/tbxpredict/</ext-link>. It has a user-friendly graphical user interface (GUI), which provides both training and prediction modules (<xref ref-type="fig" rid="pone-0112980-g009">Figure 9</xref> and <xref ref-type="fig" rid="pone-0112980-g010">10</xref>). The former enables the user to develop a model trained on his/her own CXRs (TB and non-TB CXRs) while the latter can be used for the classification of digital CXR(s). The user can independently upload CXRs into the prediction module to classify them using already trained model provided with the software. Training and prediction modules of the toolbox are available for Gist and PHOG features separately with the use of Gist as default. The toolbox automatically identifies the bit size and file format (dicom or jpeg) of the CXR images.</p>
<fig id="pone-0112980-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0112980.g009</object-id><label>Figure 9</label><caption>
<title>GUI of Prediction and Training Modules in Matlab toolbox, TB-Xpredict.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0112980.g009" position="float" xlink:type="simple"/></fig><fig id="pone-0112980-g010" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0112980.g010</object-id><label>Figure 10</label><caption>
<title>GUI of sample output provided by Matlab toolbox, TB-Xpredict.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0112980.g010" position="float" xlink:type="simple"/></fig>
<p>Since TB-Xpredict does not require prior segmentation of digital CXR images, this toolbox can easily be incorporated into existing X-ray machines for the automated classification of CXR as TB or non-TB. X-ray machines with such modules can effectively be used for the mass screening of TB in high-burden areas. GUI of TB-Xpredict enables user to train and classify CXRs without the need of expertise in image processing. User only needs to upload positive and negative CXRs separately, and the module trains the model in an automatic manner. The newly developed model can be used for the prediction of CXRs using prediction module. Since illumination and other parameters do not differ significantly in test data with respect to training model, automated prediction methods which use CXRs from the same X-ray machine for training as well as classification generally may lead to better prediction accuracy. Furthermore, such model may become more efficient as the number and diversity of training data increases over time.</p>
</sec></sec><sec id="s4">
<title>Conclusions</title>
<p>CAD techniques provide second opinion to the clinicians for their findings, so its implementation is expected to improve their performance in the diagnosis of TB. Features extracted by methods such as Gabor, Gist, HOG, and PHOG enabled SVM to efficiently discriminate between the TB and non-TB whole CXR images. Validations of results using five-fold cross-validation and independent datasets have shown that the performance of these features is significantly higher than that of textural features (<xref ref-type="table" rid="pone-0112980-t001">Table 1</xref> and <xref ref-type="table" rid="pone-0112980-t002">2</xref>). CAD techniques proposed earlier were mainly based on the segmentation of CXRs followed by the extraction of GLCM textural features. The major limitation with these methods is that they cannot be incorporated into existing X-ray machines for automated detection of TB due to their dependency on manual or semi-automated segmentation. Although GLCM textural features extracted from the automatically segmented CXRs can be used for automated detection, however, the performance of this method is exceedingly poor (<xref ref-type="table" rid="pone-0112980-t001">Table 1</xref>). Since the features (Gabor, Gist, HOG, and PHOG) extracted from the whole CXR do not require ROI identification using segmentation, these features can easily be implemented to existing digital X-ray machines for the automated TB detection. As Gist and PHOG features provided the best discrimination between the non-TB and TB for both 8-bit and 14-bit CXRs, these features were used to develop a MATLAB toolbox, TB-Xpredict, which can effortlessly train as well as predict CXRs as TB or non-TB. TB-Xpredict has a very simple GUI for automated training and classification of CXRs, and requires only basic knowledge of computer to operate. It also gives users the option to choose between Gist and PHOG for feature extraction. One of the limitations of the toolbox is that since TB has similar radiological patterns to cancer and some interstitial lung diseases (ILDs), it may wrongly classify CXRs with these diseases as TB CXRs. This toolbox will assist in better diagnosis as well as efficient mass screening of TB in high burden areas.</p>
</sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pone.0112980.s001" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" xlink:href="info:doi/10.1371/journal.pone.0112980.s001" position="float" xlink:type="simple"><label>Table S1</label><caption>
<p><bold>Various grey-level co-occurrence matrix (GLCM) textural features used for the classification.</bold></p>
<p>(DOCX)</p>
</caption></supplementary-material></sec></body>
<back><ref-list>
<title>References</title>
<ref id="pone.0112980-Tan1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tan</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Lim</surname><given-names>CM</given-names></name>, <name name-style="western"><surname>Acharya</surname><given-names>UR</given-names></name>, <name name-style="western"><surname>Tan</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>Abraham</surname><given-names>KT</given-names></name> (<year>2012</year>) <article-title>Computer-Assisted Diagnosis of Tuberculosis: A First Order Statistical Approach to Chest Radiograph</article-title>. <source>J Med Syst</source> <volume>36</volume>: <fpage>2751</fpage>–<lpage>2759</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10916-011-9751-9" xlink:type="simple">10.1007/s10916-011-9751-9</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-Global1"><label>2</label>
<mixed-citation publication-type="other" xlink:type="simple">Global TB Control Report 2012 (2012) World Health Organization, Geneva.</mixed-citation>
</ref>
<ref id="pone.0112980-Doi1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doi</surname><given-names>K</given-names></name> (<year>2007</year>) <article-title>Computer-aided diagnosis in medical imaging: historical review, current status and future potential</article-title>. <source>Comput Med Imaging Graph</source> <volume>31</volume>: <fpage>198</fpage>–<lpage>211</lpage>.</mixed-citation>
</ref>
<ref id="pone.0112980-Noble1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Noble</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Bruening</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Uhl</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Schoelles</surname><given-names>K</given-names></name> (<year>2009</year>) <article-title>Computer-aided detection mammography for breast cancer screening: systematic review and meta-analysis</article-title>. <source>Arch Gynecol Obstet</source> <volume>279</volume>: <fpage>881</fpage>–<lpage>890</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00404-008-0841-y" xlink:type="simple">10.1007/s00404-008-0841-y</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-Gur1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gur</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Sumkin</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>Rockette</surname><given-names>HE</given-names></name>, <name name-style="western"><surname>Ganott</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Hakim</surname><given-names>C</given-names></name>, <etal>et al</etal>. (<year>2004</year>) <article-title>Changes in breast cancer detection and mammography recall rates after the introduction of a computer-aided detection system</article-title>. <source>J Natl Cancer Inst</source> <volume>96</volume>: <fpage>185</fpage>–<lpage>190</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/jnci/djh067" xlink:type="simple">10.1093/jnci/djh067</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-Dean1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dean</surname><given-names>JC</given-names></name>, <name name-style="western"><surname>Ilvento</surname><given-names>CC</given-names></name> (<year>2006</year>) <article-title>Improved cancer detection using computer-aided detection with diagnostic and screening mammography: prospective study of 104 cancers</article-title>. <source>AJR Am J Roentgenol</source> <volume>187</volume>: <fpage>20</fpage>–<lpage>28</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2214/AJR.05.0111" xlink:type="simple">10.2214/AJR.05.0111</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-Ginneken1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ginneken</surname><given-names>BV</given-names></name>, <name name-style="western"><surname>Hogeweg</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Maduskar</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Peters-Bax</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Dawson</surname><given-names>R</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Performance of inexperienced and experienced observers in detection of active tuberculosis on digital chest radiographs with and without the use of computer-aided diagnosis</article-title>. <source>Annual Meeting of the Radiological Society of North America</source></mixed-citation>
</ref>
<ref id="pone.0112980-Haralick1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Haralick</surname><given-names>RM</given-names></name>, <name name-style="western"><surname>Shanmugam</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Dinstein</surname><given-names>I</given-names></name> (<year>1973</year>) <article-title>Textural Features for Image Classification</article-title>. <source>IEEE Trans Syst Man Cybern</source> <volume>3</volume>: <fpage>610</fpage>–<lpage>621</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TSMC.1973.4309314" xlink:type="simple">10.1109/TSMC.1973.4309314</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-Ginneken2"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ginneken</surname><given-names>BV</given-names></name>, <name name-style="western"><surname>Katsuragawa</surname><given-names>S</given-names></name>, <name name-style="western"><surname>ter Haar Romeny</surname><given-names>BM</given-names></name>, <name name-style="western"><surname>Doi</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Viergever</surname><given-names>MA</given-names></name> (<year>2002</year>) <article-title>Automatic detection of abnormalities in chest radiographs using local texture analysis</article-title>. <source>IEEE Trans Med Imaging</source> <volume>21</volume>: <fpage>139</fpage>–<lpage>149</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/42.993132" xlink:type="simple">10.1109/42.993132</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-Hogeweg1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hogeweg</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Mol</surname><given-names>C</given-names></name>, <name name-style="western"><surname>de Jong</surname><given-names>PA</given-names></name>, <name name-style="western"><surname>Dawson</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Ayles</surname><given-names>H</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Fusion of local and global detection systems to detect tuberculosis in chest radiographs</article-title>. <source>Med Image Comput Comput Assist Interv</source> <volume>13</volume>: <fpage>650</fpage>–<lpage>657</lpage> <comment>doi:_<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/978-3-642-15711-081" xlink:type="simple">10.1007/978-3-642-15711-0_81</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-Arzhaeva1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Arzhaeva</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Hogeweg</surname><given-names>L</given-names></name>, <name name-style="western"><surname>de Jong</surname><given-names>PA</given-names></name>, <name name-style="western"><surname>Viergever</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>van Ginneken</surname><given-names>B</given-names></name> (<year>2009</year>) <article-title>Global and local multi-valued dissimilarity-based classification: application to computer-aided detection of tuberculosis</article-title>. <source>Med Image Comput Comput Assist Interv</source> <volume>12</volume>: <fpage>724</fpage>–<lpage>731</lpage> <comment>doi:_<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/978-3-642-04271-388" xlink:type="simple">10.1007/978-3-642-04271-3_88</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-Ginneken3"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ginneken</surname><given-names>BV</given-names></name>, <name name-style="western"><surname>Tax</surname><given-names>DMJ</given-names></name>, <name name-style="western"><surname>Arzhaeva</surname><given-names>Y</given-names></name> (<year>2009</year>) <article-title>Dissimilarity-based classification in the absence of local ground truth: Application to the diagnostic interpretation of chest radiographs</article-title>. <source>Pattern Recognit</source> <volume>42</volume>: <fpage>1768</fpage>–<lpage>1776</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.patcog.2009.01.016" xlink:type="simple">10.1016/j.patcog.2009.01.016</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-Shen1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shen</surname><given-names>RSR</given-names></name>, <name name-style="western"><surname>Cheng</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Basu</surname><given-names>A</given-names></name> (<year>2010</year>) <article-title>A Hybrid Knowledge-Guided Detection Technique for Screening of Infectious Pulmonary Tuberculosis From Chest Radiographs</article-title>. <source>IEEE Trans Biomed Eng 57</source> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TBME.2010.2057509" xlink:type="simple">10.1109/TBME.2010.2057509</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-Oliva1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oliva</surname><given-names>A</given-names></name> (<year>2001</year>) <collab xlink:type="simple">Hospital W, Ave L</collab> (<year>2001</year>) <article-title>Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope</article-title>. <source>Int J Comput Vis</source> <volume>42</volume>: <fpage>145</fpage>–<lpage>175</lpage>.</mixed-citation>
</ref>
<ref id="pone.0112980-Manjunath1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Manjunath</surname><given-names>BS</given-names></name>, <name name-style="western"><surname>Ma</surname><given-names>WY</given-names></name> (<year>1996</year>) <article-title>Texture features for browsing and retrieval of image data</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source> <volume>18</volume>: <fpage>837</fpage>–<lpage>842</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/34.531803" xlink:type="simple">10.1109/34.531803</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-Dalal1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dalal</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Triggs</surname><given-names>B</given-names></name> (<year>2005</year>) <article-title>Histograms of oriented gradients for human detection</article-title>. <source>2005 IEEE Comput Soc Conf Comput Vis Pattern Recognit 1</source> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/CVPR.2005.177" xlink:type="simple">10.1109/CVPR.2005.177</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-Bosch1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bosch</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Zisserman</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Munoz</surname><given-names>X</given-names></name> (<year>2007</year>) <article-title>Representing shape with a spatial pyramid kernel</article-title>. <source>Image Process</source> <volume>5</volume>: <fpage>401</fpage>–<lpage>408</lpage>.</mixed-citation>
</ref>
<ref id="pone.0112980-Abrmoff1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abràmoff</surname><given-names>MD</given-names></name>, <name name-style="western"><surname>Magalhães</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Ram</surname><given-names>SJ</given-names></name> (<year>2004</year>) <article-title>Image processing with ImageJ</article-title>. <source>Biophotonics international</source> <volume>11</volume>: <fpage>36</fpage>–<lpage>43</lpage>.</mixed-citation>
</ref>
<ref id="pone.0112980-Chan1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chan</surname><given-names>TF</given-names></name>, <name name-style="western"><surname>Sandberg</surname><given-names>BY</given-names></name>, <name name-style="western"><surname>Vese</surname><given-names>LA</given-names></name> (<year>2000</year>) <article-title>Active Contours without Edges for Vector-Valued Images</article-title>. <source>J Vis Commun Image Represent</source> <volume>11</volume>: <fpage>130</fpage>–<lpage>141</lpage>.</mixed-citation>
</ref>
<ref id="pone.0112980-Wong1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wong</surname><given-names>LP</given-names></name>, <name name-style="western"><surname>Ewe</surname><given-names>HT</given-names></name> (<year>2005</year>) <article-title>A study of lung cancer detection using chest x-ray images. Proc</article-title>. <source>3<sup>rd</sup> APT Telemedicine Workshop, Kuala Lumpur</source> <volume>3</volume>: <fpage>210</fpage>–<lpage>214</lpage>.</mixed-citation>
</ref>
<ref id="pone.0112980-Luisier1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luisier</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Blu</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Unser</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>A new SURE approach to image denoising: interscale orthonormal wavelet thresholding</article-title>. <source>IEEE Trans Image Process</source> <volume>16</volume>: <fpage>593</fpage>–<lpage>606</lpage>.</mixed-citation>
</ref>
<ref id="pone.0112980-Grigorescu1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grigorescu</surname><given-names>SE</given-names></name>, <name name-style="western"><surname>Petkov</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Kruizinga</surname><given-names>P</given-names></name> (<year>2002</year>) <article-title>Comparison of texture features based on Gabor filters</article-title>. <source>IEEE Trans Image Process</source> <volume>11</volume>: <fpage>1160</fpage>–<lpage>1167</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/ICIAP.1999.797585" xlink:type="simple">10.1109/ICIAP.1999.797585</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-Yang1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yang</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Newsam</surname><given-names>S</given-names></name> (<year>2008</year>) <article-title>Comparing SIFT descriptors and gabor texture features for classification of remote sensed imagery</article-title>. <source>2008 15th IEEE Int Conf Image Process</source> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/ICIP.2008.4712139" xlink:type="simple">10.1109/ICIP.2008.4712139</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-Siagian1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Siagian</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Itti</surname><given-names>L</given-names></name> (<year>2007</year>) <article-title>Rapid biologically-inspired scene classification using features shared with visual attention</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source> <volume>29</volume>: <fpage>300</fpage>–<lpage>312</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TPAMI.2007.40" xlink:type="simple">10.1109/TPAMI.2007.40</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-Oliva2"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oliva</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Torralba</surname><given-names>A</given-names></name> (<year>2006</year>) <article-title>Building the gist of a scene: the role of global image features in recognition</article-title>. <source>Prog Brain Res</source> <volume>155</volume>: <fpage>23</fpage>–<lpage>36</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0079-6123(06)55002-2" xlink:type="simple">10.1016/S0079-6123(06)55002-2</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-Itti1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Itti</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Siagian</surname><given-names>C</given-names></name> (<year>2010</year>) <article-title>Comparison of gist models in rapid scene categorization tasks</article-title>. <source>J Vis</source> <volume>8</volume>: <fpage>734</fpage>–<lpage>734</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1167/8.6.734" xlink:type="simple">10.1167/8.6.734</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-Ludwig1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ludwig</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Delgado</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Goncalves</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Nunes</surname><given-names>U</given-names></name> (<year>2009</year>) <article-title>Trainable classifier-fusion schemes: An application to pedestrian detection</article-title>. <source>2009 12th Int IEEE Conf Intell Transp Syst</source> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/ITSC.2009.5309700" xlink:type="simple">10.1109/ITSC.2009.5309700</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-Hall1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hall</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Frank</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Holmes</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Pfahringer</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Reutemann</surname><given-names>P</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>The WEKA Data Mining Software: An Update</article-title>. <source>ACM SIGKDD Explor Newsl</source> <volume>11</volume>: <fpage>10</fpage>–<lpage>18</lpage> <comment>Available: <ext-link ext-link-type="uri" xlink:href="http://dl.acm.org/citation.cfm?id=1656278" xlink:type="simple">http://dl.acm.org/citation.cfm?id=1656278</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-Cortes1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cortes</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Vapnik</surname><given-names>V</given-names></name> (<year>1995</year>) <article-title>Support-vector networks</article-title>. <source>Machine learning</source> <volume>20</volume>: <fpage>273</fpage>–<lpage>297</lpage>.</mixed-citation>
</ref>
<ref id="pone.0112980-Chang1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chang</surname><given-names>C-C</given-names></name>, <name name-style="western"><surname>Lin</surname><given-names>C-J</given-names></name> (<year>2011</year>) <article-title>LIBSVM: a library for support vector machines</article-title>. <source>ACM Trans Intell Syst Technol</source> <volume>2</volume>: <fpage>1</fpage>–<lpage>39</lpage>.</mixed-citation>
</ref>
<ref id="pone.0112980-Zweig1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zweig</surname><given-names>MH</given-names></name>, <name name-style="western"><surname>Campbell</surname><given-names>G</given-names></name> (<year>1993</year>) <article-title>Receiver-operating characteristic (ROC) plots: a fundamental evaluation tool in clinical medicine</article-title>. <source>Clin Chem</source> <volume>39</volume>: <fpage>561</fpage>–<lpage>577</lpage>.</mixed-citation>
</ref>
<ref id="pone.0112980-Metz1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Metz</surname><given-names>C</given-names></name> (<year>1978</year>) <article-title>Basic principles of ROC analysis</article-title>. <source>Semin Nucl Med</source> <volume>8</volume>: <fpage>283</fpage>–<lpage>298</lpage>. <comment>Available: papers2://publication/uuid/7A60F045-4E92-43C3-9586-6994FE10B463</comment></mixed-citation>
</ref>
<ref id="pone.0112980-Robin1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Robin</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Turck</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Hainard</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Tiberti</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Lisacek</surname><given-names>F</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>pROC: an open-source package for R and S+ to analyze and compare ROC curves</article-title>. <source>BMC Bioinformatics</source> <volume>12</volume>: <fpage>77</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/1471-2105-12-77" xlink:type="simple">10.1186/1471-2105-12-77</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0112980-DeLong1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DeLong</surname><given-names>ER</given-names></name>, <name name-style="western"><surname>DeLong</surname><given-names>DM</given-names></name>, <name name-style="western"><surname>Clarke-Pearson</surname><given-names>DL</given-names></name> (<year>1988</year>) <article-title>Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach</article-title>. <source>Biometrics</source> <volume>44</volume>: <fpage>837</fpage>–<lpage>845</lpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2307/2531595" xlink:type="simple">10.2307/2531595</ext-link></comment></mixed-citation>
</ref>
</ref-list></back>
</article>