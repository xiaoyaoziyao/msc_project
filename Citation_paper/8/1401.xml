<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-18-12680</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0211044</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Random variables</subject><subj-group><subject>Covariance</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Test statistics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Test statistics</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject><subj-group><subject>Normal distribution</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Statistical distributions</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Signal bandwidth</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Earth sciences</subject><subj-group><subject>Seasons</subject><subj-group><subject>Summer</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Linear discriminant analysis</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Linear discriminant analysis</subject></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>A novel scale-space approach for multinormality testing and the <italic>k</italic>-sample problem in the high dimension low sample size scenario</article-title>
<alt-title alt-title-type="running-head">Scale-space approach for multinormality testing and the <italic>k</italic>-sample problem in the HDLSS scenario</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8194-6679</contrib-id>
<name name-style="western">
<surname>Hindberg</surname> <given-names>Kristian</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Hannig</surname> <given-names>Jan</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Godtliebsen</surname> <given-names>Fred</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Department of Mathematics and Statistics, University of Tromsø – The Arctic University of Norway, Tromsø, Norway</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Department of Statistics and Operations Research, University of North Carolina, Chapel Hill, NC, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Fernandez</surname> <given-names>Miguel A.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Universidad de Valladolid, SPAIN</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">kristian.hindberg@uit.no</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>22</day>
<month>1</month>
<year>2019</year>
</pub-date>
<volume>14</volume>
<issue>1</issue>
<elocation-id>e0211044</elocation-id>
<history>
<date date-type="received">
<day>26</day>
<month>4</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>7</day>
<month>1</month>
<year>2019</year>
</date>
</history>
<permissions>
<license xlink:href="https://creativecommons.org/publicdomain/zero/1.0/" xlink:type="simple">
<license-p>This is an open access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/" xlink:type="simple">Creative Commons CC0</ext-link> public domain dedication.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0211044"/>
<abstract>
<p>Two classical multivariate statistical problems, testing of multivariate normality and the <italic>k</italic>-sample problem, are explored by a novel analysis on several resolutions simultaneously. The presented methods do not invert any estimated covariance matrix. Thereby, the methods work in the High Dimension Low Sample Size situation, i.e. when <italic>n</italic> ≤ <italic>p</italic>. The output, a significance map, is produced by doing a one-dimensional test for all possible resolution/position pairs. The significance map shows for which resolution/position pairs the null hypothesis is rejected. For the testing of multinormality, the Anderson-Darling test is utilized to detect potential departures from multinormality at different combinations of resolutions and positions. In the <italic>k</italic>-sample case, it is tested whether <italic>k</italic> data sets can be said to originate from the same unspecified discrete or continuous multivariate distribution. This is done by testing the <italic>k</italic> vectors corresponding to the same resolution/position pair of the <italic>k</italic> different data sets through the <italic>k</italic>-sample Anderson-Darling test. Successful demonstrations of the new methodology on artificial and real data sets are presented, and a feature selection scheme is demonstrated.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>eVita program in the Norwegian Research Council</institution>
</funding-source>
<award-id>176872/V30</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Godtliebsen</surname> <given-names>Fred</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution>National Science Foundation (US)</institution>
</funding-source>
<award-id>1512945 and 1633074</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Hannig</surname> <given-names>Jan</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>J.H. received funding from the National Science Foundation of the United States (<ext-link ext-link-type="uri" xlink:href="https://www.nsf.gov/" xlink:type="simple">https://www.nsf.gov/</ext-link>) under Grant No. 1512945 and 1633074l. F.G. received funding from the eVita program (grant number 176872/V30) of the Norwegian Research Council (<ext-link ext-link-type="uri" xlink:href="https://www.forskningsradet.no" xlink:type="simple">https://www.forskningsradet.no</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="12"/>
<table-count count="2"/>
<page-count count="20"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>In practice, it is frequently assumed that a data set can be described by a multivariate normal distribution. Many common statistical procedures rely on the data being multinormal, something which is often not adequately checked before using the procedures [<xref ref-type="bibr" rid="pone.0211044.ref001">1</xref>–<xref ref-type="bibr" rid="pone.0211044.ref003">3</xref>]. Often, this assumption is false for either the whole data set or parts of it. Another classical problem is the testing of whether <italic>k</italic> multivariate data sets originate from the same distribution. For each of the two problems, a scale-space inspired algorithm that tests all resolutions and positions simultaneously, is presented. See the “<xref ref-type="sec" rid="sec002">Materials and methods</xref>” section for definitions of “resolution” and “position”. The two presented algorithms are very similar apart from the type of one-dimensional tests used. A weighted summation is performed across the dimensions/positions in both algorithms. The notion of resolution is connected to the number of dimensions being summed across, while the different dimensions/positions typically are temporal or spatial samples.</p>
<p>Scale-space theory is a framework for representing signals on multiple scales/resolutions, developed by the computer vision, image processing and signal processing communities. The development of scale-space methodology is typically regarded to start with two papers by Witkin [<xref ref-type="bibr" rid="pone.0211044.ref004">4</xref>, <xref ref-type="bibr" rid="pone.0211044.ref005">5</xref>]. A recent review by Holmström and Pasanen [<xref ref-type="bibr" rid="pone.0211044.ref006">6</xref>] shows how scale-space methodology has been extended to a large number of areas. The goal of statistical scale-space methodology is to extract features from noisy data at several levels of resolution. Typically, the data is an observed time series or a digital image where features at different temporal or spatial scales/resolutions might be of interest. Since the scale-space idea is important in the present paper, we introduce the scale-space idea through the SiZer methodology developed by Chaudhuri and Marron [<xref ref-type="bibr" rid="pone.0211044.ref007">7</xref>]. To this end, we produce the output from SiZer in <xref ref-type="fig" rid="pone.0211044.g001">Fig 1</xref> when applied to an artificial data set. SiZer is based on nonparametric smoothing and the upper panel shows the artificial data points as dots and a large number of curves obtained for different values of the smoothing parameter. In this setting, the scale/resolution corresponds to the value of the applied smoothing parameter. A rough curve in the upper panel corresponds to a small smoothing parameter and hence to a short scale. Long scales correspond to smooth curves obtained by large values of the smoothing parameter. The SiZer map in the lower panel reveals what features the observed data contain at different scales. In this context, a black pixel means that the curve is significantly increasing, a white pixel corresponds to a significantly decreasing feature, and a gray pixel corresponds to a situation where the curve is considered to be flat. From <xref ref-type="fig" rid="pone.0211044.g001">Fig 1</xref>, it can be seen that SiZer flags regions as significantly decreasing and/or increasing for different positions and smoothing parameters.</p>
<fig id="pone.0211044.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211044.g001</object-id>
<label>Fig 1</label>
<caption>
<title>SiZer map of artifical data.</title>
<p>The upper panel shows the artifical values as dots and a set of smoothed curves with different smoothing bandwidths. The solid line, which is very close to the true underlying signal, corresponds to a computer-chosen optimal bandwidth. In the lower panel, the vertical axis corresponds to, from top to bottom, wider to narrower smoothing bandwidths (the horizontal line corresponds to the computer-chosen optimal bandwidth). White and black pixels correspond to significant decrease and increase, respectively. Gray pixels correspond to situations where the background signal can be assumed constant.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211044.g001" xlink:type="simple"/>
</fig>
<p>In the present paper, we will adapt the SiZer methodology to our situation and develop a scale-space methodology that can be useful for the <italic>k</italic>-sample problem and for testing of multinormality.</p>
<p>The presented algorithms have two aspects that make them useful in many situations. As will be shown, the algorithms avoid the need to estimate the covariance matrix, leading to algorithms that can handle the High Dimension Low Sample Size (HDLSS) situation. Furthermore, the algorithms allow an evaluation of the data set for all resolutions and all positions simultaneously. By this approach, it may, for the multinormality testing, be detected if only some parts of the data set originate from a multinormal distribution. For the <italic>k</italic>-sample case, the scale-space approach can detect if one or more of the <italic>k</italic> samples differ on different resolutions and/or positions. By not estimating the covariance matrix, the presented scale-space tests potentially loose some power compared to tests that incorporate the information from the estimated covariance matrix. This loss of power is acceptable on the grounds of being able to handle the HDLSS situation. As a result of the summation, the algorithms will include a large number of one-dimensional tests. Two versions of the Anderson-Darling (AD) test (see the “Anderson-Darling testing” section) are applied as one-dimensional tests for the multinormality testing problem and the <italic>k</italic>-sample problem. The choice of using the AD test is a result of its excellent power against all alternatives and existence of very good approximations for the asymptotic distribution and formulas adjusting for the finite sample sizes [<xref ref-type="bibr" rid="pone.0211044.ref008">8</xref>–<xref ref-type="bibr" rid="pone.0211044.ref010">10</xref>].</p>
<p>For the results presented, the Anderson-Darling (AD) test (see the “Anderson-Darling testing” section) is used for both the multinormality testing and the <italic>k</italic>-sample problem as the one-dimensional test used on the summations. The choice of using the AD test is a result of its excellent power against all alternatives and existence of very good approximations for the asymptotic distribution and formulas adjusting for the finite sample sizes [<xref ref-type="bibr" rid="pone.0211044.ref008">8</xref>–<xref ref-type="bibr" rid="pone.0211044.ref010">10</xref>].</p>
<p>A simple artificial example is presented to illustrate the main ideas of the paper. The data set is generated to have a distribution that is multivariate normal for some of the dimensions and a mixture of two different normal distributions for the rest of the dimensions. In particular, the population is a mixture of two different underlying true signals. In the first population, 20 signals are sampled from a zero mean Gaussian autoregressive process of order 1, more specifically cov(<italic>X</italic><sub><italic>i</italic></sub>, <italic>X</italic><sub><italic>j</italic></sub>) = 0.5<sup>1+|<italic>i</italic>−<italic>j</italic>|</sup>. The remaining 20 signals have the same covariance structure, but a different mean. In particular, the mean of the second population is equal to −2.15 for position 6 to 12 and −3.5 for position 20. For indices 26, …, 40, the expected value increases linearly from 0.1 to 2.5, while the rest of the dimensions have expectation equal to zero. <xref ref-type="fig" rid="pone.0211044.g002">Fig 2</xref> shows all the 40 signals of length 50.</p>
<fig id="pone.0211044.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211044.g002</object-id>
<label>Fig 2</label>
<caption>
<title>All 40 artificial signals of length 50.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211044.g002" xlink:type="simple"/>
</fig>
<p>
<xref ref-type="fig" rid="pone.0211044.g003">Fig 3</xref> shows the resulting significance map from the proposed multinormality test of the data in <xref ref-type="fig" rid="pone.0211044.g002">Fig 2</xref>. The horizontal axis is the same as in <xref ref-type="fig" rid="pone.0211044.g002">Fig 2</xref> and shows the position, while different window widths are given on the vertical axis. These different window widths represent the resolution part of the presented algorithms. Resolution 1 corresponds to testing the marginal distribution of each dimension. Higher resolutions are results of normality tests of local averages at a corresponding position and corresponding window width. For a distribution to be multinormal, all marginals and all local averages must be normally distributed. By going through the test results for all resolution/position pairs, the significance map is produced. Red pixels mark Bonferroni [<xref ref-type="bibr" rid="pone.0211044.ref011">11</xref>] adjusted rejections of the null hypothesis of normality, i.e. indicating that the part of the data matrix that is summed across cannot be considered as a sample from a multinormal distribution. Note that the abrupt deviation from normality at dimension 20 is found on low resolution values, while the more gradual departure from multinormality at dimension 6 to 12 and dimensions 25 to 40 are found on larger resolution values. This example shows that both low and high resolutions may be of importance in the same data set.</p>
<fig id="pone.0211044.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211044.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Significance map of the test for multinormality of an artificial data set.</title>
<p>Red indicates rejection of the null hypothesis (multinormality) for that window width/position. For a given resolution, the horizontal distance between the two gray lines equals the width of the summation window of that resolution.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211044.g003" xlink:type="simple"/>
</fig>
<p>The “Materials and methods” section presents the concept of scale/resolution and space/position as used in this paper, the statistical problems being investigated and the details of the two presented algorithms. Some investigations into the power of the tests are also presented. In the “Results” section, the algorithms are applied to some real data sets, comparisons with other algorithms are done, and a feature selection scheme is presented and tested on real data. Finally, the “Conclusions” section sums up the presented methods.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Materials and methods</title>
<p>Recall that an important motivation for applying a scale-space approach is the fact that different phenomena can be visible/detectable on different resolutions and/or positions of the data set. In classical nonparametric smoothing schemes, some sort of bandwidth parameter has to be chosen [<xref ref-type="bibr" rid="pone.0211044.ref012">12</xref>]. By selecting one bandwidth only, features detectable on other bandwidths will not be found. However, using a scale-space approach, one can look at all bandwidths simultaneously. Scale-space ideas have proven useful in many areas and have been applied to feature detection in curves and images [<xref ref-type="bibr" rid="pone.0211044.ref007">7</xref>, <xref ref-type="bibr" rid="pone.0211044.ref013">13</xref>], density estimation [<xref ref-type="bibr" rid="pone.0211044.ref014">14</xref>], curve fitting [<xref ref-type="bibr" rid="pone.0211044.ref015">15</xref>], Bayesian time series analysis [<xref ref-type="bibr" rid="pone.0211044.ref016">16</xref>] and spectral feature detection [<xref ref-type="bibr" rid="pone.0211044.ref017">17</xref>].</p>
<sec id="sec003">
<title>Assumptions</title>
<p>For the multinormality testing case, let <bold>X</bold><sub>1</sub>, <bold>X</bold><sub>2</sub>, …, <bold>X</bold><sub><italic>n</italic></sub> be a set of <italic>p</italic>-dimensional vectors. The null hypothesis assumes that these vectors originate from a <italic>p</italic>-dimensional multinormal distribution <inline-formula id="pone.0211044.e001"><alternatives><graphic id="pone.0211044.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">μ</mml:mi> <mml:mo>,</mml:mo> <mml:munder><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mo>_</mml:mo></mml:munder> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, i.e.
<disp-formula id="pone.0211044.e002"><alternatives><graphic id="pone.0211044.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mspace width="3.33333pt"/><mml:mo>:</mml:mo> <mml:mspace width="3.33333pt"/><mml:msub><mml:mi mathvariant="bold">X</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold-italic">μ</mml:mi> <mml:mo>,</mml:mo> <mml:munder><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mo>_</mml:mo></mml:munder> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="1.em"/><mml:mo>∀</mml:mo> <mml:mspace width="3.33333pt"/><mml:mi>i</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where the mean vector <italic><bold>μ</bold></italic> and the covariance matrix <inline-formula id="pone.0211044.e003"><alternatives><graphic id="pone.0211044.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:munder><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mo>_</mml:mo></mml:munder></mml:math></alternatives></inline-formula> are unknown. For the presented algorithm, the parameters of this assumed multinormal distribution do not have to be estimated. Note that by avoiding the need for an estimate of the covariance matrix, the algorithm can be applied to data sets with any combination of sample size and sample dimension, as long as the sample size is high enough for the one-dimensional normality test to be applicable.</p>
<p>The algorithm works with any covariance structure and there are no requirements for smoothness of expected values of neighboring dimensions. As will be presented later, the algorithm performs a weighted summation across neighboring dimensions. A motivation behind this summation is that neighboring dimensions frequently have some sort of logical connection to each other, as for example in a time series. When the data set consists of a time series, the different dimensions are equivalent to the different sampling times. If the dimensions are shifted around, the algorithm could produce different results. Therefore, interpretations of the results are easier when the different dimensions have a natural ordering, as for example with spatial or temporal data.</p>
<p>For the <italic>k</italic>-sample case, each of the <italic>k</italic> samples consist of a given number (which can be different for each <italic>k</italic>) of <italic>p</italic>-dimensional vectors with unknown cumulative distribution functions (CDF), given by <italic>F</italic><sub>1</sub>, <italic>F</italic><sub>2</sub>, …, <italic>F</italic><sub><italic>k</italic></sub>, respectively. The null hypothesis is then stated as
<disp-formula id="pone.0211044.e004"><alternatives><graphic id="pone.0211044.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mspace width="3.33333pt"/><mml:mo>:</mml:mo> <mml:mspace width="3.33333pt"/><mml:msub><mml:mi>F</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>F</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>=</mml:mo> <mml:msub><mml:mi>F</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mspace width="1.em"/><mml:mo>∀</mml:mo> <mml:mspace width="3.33333pt"/><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mi>p</mml:mi></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula></p>
<p>Since this methodology only tests whether or not the CDFs all are the same, the CDFs can take any form or belong to any class of distributions. Again, the interpretations of the results are easiest when working with data having a natural ordering.</p>
</sec>
<sec id="sec004">
<title>Concept of resolution and summation across dimensions</title>
<p>One of the main ideas of this manuscript is testing simultaneously for many different resolutions and positions. The resolution value equals the number of different dimensions being summed across. The lowest resolution value of 1 corresponds to a test of the marginal distributions. At resolution 3, the result of the summation for position/dimension <italic>d</italic> is a weighted (see <xref ref-type="disp-formula" rid="pone.0211044.e006">Eq (2)</xref>) summation of the sample values with position index <italic>d</italic> − 1, <italic>d</italic> and <italic>d</italic> + 1. For other resolutions, completely analogous summations are performed. Note that by this summation, small differences within the data can be detected, even though this difference might not be detected for lower resolutions. The set of default resolutions is chosen to be {1 3 5 7 9 11 15 21 29 39 51 65 81 99 … <italic>s</italic><sub>max</sub>}, i.e. for <italic>i</italic> ≥ 5 the resolution values are given as <italic>s</italic><sub><italic>i</italic>+1</sub> = <italic>s</italic><sub><italic>i</italic></sub> + 2 ⋅ (<italic>i</italic> − 4) up to a maximum resolution <italic>s</italic><sub>max</sub> ≤ <italic>p</italic>, where <italic>s</italic><sub>5</sub> = 9. Alternatively, one can choose to only include resolutions up to some upper resolution.</p>
<p>For each of the different resolutions <italic>s</italic>, a weighted summation across different dimensions/positions is performed, producing <bold>L</bold><sub><italic>s</italic>,<italic>d</italic></sub>, where <italic>d</italic> is the position index ranging from 1 to <italic>p</italic> and <bold>L</bold><sub><italic>s</italic>,<italic>d</italic></sub> is a vector of length <italic>n</italic>. The resulting <bold>L</bold><sub><italic>s</italic>,<italic>d</italic></sub>’s form a matrix <inline-formula id="pone.0211044.e005"><alternatives><graphic id="pone.0211044.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:munder><mml:mi mathvariant="bold">L</mml:mi> <mml:mo>_</mml:mo></mml:munder></mml:math></alternatives></inline-formula> with size [<italic>n</italic><sub><italic>s</italic></sub>, <italic>p</italic>, <italic>n</italic>], where <italic>n</italic><sub><italic>s</italic></sub> is the number of resolutions being used. A discrete Epanechnikov [<xref ref-type="bibr" rid="pone.0211044.ref012">12</xref>] window function is used as summation weights. For a given pair of <italic>s</italic> and <italic>d</italic>, the Epanechnikov summation window is a column vector given by
<disp-formula id="pone.0211044.e006"><alternatives><graphic id="pone.0211044.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>d</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≡</mml:mo> <mml:mi>K</mml:mi> <mml:mo>·</mml:mo> <mml:msub><mml:mrow><mml:mo>[</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mi>d</mml:mi></mml:mrow> <mml:mrow><mml:mo>⌈</mml:mo> <mml:mi>s</mml:mi> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn> <mml:mo>⌉</mml:mo></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub> <mml:mo>,</mml:mo> <mml:mspace width="3.33333pt"/><mml:mspace width="3.33333pt"/><mml:mspace width="3.33333pt"/><mml:mspace width="3.33333pt"/><mml:mspace width="3.33333pt"/><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where <italic>K</italic> is some normalizing constant, ⌈⋅⌉ is the ceiling function, and the plus function is defined as [<italic>f</italic>(<italic>x</italic>)]<sub>+</sub> ≡ max[0, <italic>f</italic>(<italic>x</italic>)] for some functional value <italic>f</italic>(<italic>x</italic>). The <bold>L</bold><sub><italic>s</italic>,<italic>d</italic></sub> vector is generated through
<disp-formula id="pone.0211044.e007"><alternatives><graphic id="pone.0211044.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e007" xlink:type="simple"/><mml:math display="block" id="M7"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold">L</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>d</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:munder><mml:mi mathvariant="bold">X</mml:mi> <mml:mo>_</mml:mo></mml:munder> <mml:mo>·</mml:mo> <mml:msub><mml:mi mathvariant="bold">w</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>,</mml:mo> <mml:mi>d</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where the data matrix <inline-formula id="pone.0211044.e008"><alternatives><graphic id="pone.0211044.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:munder><mml:mi mathvariant="bold">X</mml:mi> <mml:mo>_</mml:mo></mml:munder></mml:math></alternatives></inline-formula> has size [<italic>n</italic>, <italic>p</italic>], with the <italic>n</italic> samples of length <italic>p</italic> along each row, and ⋅ indicates normal matrix multiplication. The resulting vector <bold>L</bold><sub><italic>s</italic>,<italic>d</italic></sub> is thereby a weighted summation across the <italic>s</italic> dimensions centered on the <italic>d</italic>-th dimension. <xref ref-type="fig" rid="pone.0211044.g004">Fig 4</xref> shows how the algorithm generates the <inline-formula id="pone.0211044.e009"><alternatives><graphic id="pone.0211044.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:munder><mml:mi mathvariant="bold">L</mml:mi> <mml:mo>_</mml:mo></mml:munder></mml:math></alternatives></inline-formula> matrix and how it is used to generate the output matrix, that is the significance map <inline-formula id="pone.0211044.e010"><alternatives><graphic id="pone.0211044.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:munder><mml:mi mathvariant="bold">R</mml:mi> <mml:mo>_</mml:mo></mml:munder></mml:math></alternatives></inline-formula>, with different resolutions on the vertical axis and position on the horizontal axis.</p>
<fig id="pone.0211044.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211044.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Workflow chart.</title>
<p>The data matrix <inline-formula id="pone.0211044.e011"><alternatives><graphic id="pone.0211044.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:munder><mml:mi mathvariant="bold">X</mml:mi> <mml:mo>_</mml:mo></mml:munder></mml:math></alternatives></inline-formula> has dimensions [<italic>n</italic>, <italic>p</italic>] = [4, 5]. The summation matrix <inline-formula id="pone.0211044.e012"><alternatives><graphic id="pone.0211044.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:munder><mml:mi mathvariant="bold">L</mml:mi> <mml:mo>_</mml:mo></mml:munder></mml:math></alternatives></inline-formula> has dimensions [<italic>n</italic><sub><italic>s</italic></sub>, <italic>p</italic>, <italic>n</italic>] and each <bold>L</bold><sub><italic>s</italic>,<italic>d</italic></sub> is a vector of length <italic>n</italic>. The significance matrix <inline-formula id="pone.0211044.e013"><alternatives><graphic id="pone.0211044.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:munder><mml:mi mathvariant="bold">R</mml:mi> <mml:mo>_</mml:mo></mml:munder></mml:math></alternatives></inline-formula> has dimensions [<italic>n</italic><sub><italic>s</italic></sub>, <italic>p</italic>]. The red box, which only spans one dimension, indicates that for the lowest resolution value, no summation is performed across the dimensions. For the green and blue boxes, summation is performed across dimensions 1–3 and 2–5, respectively. The blue box is adjusted to not extend outside the data matrix. Note that two significance maps are produced, one each for the Bonferroni/FDR approaches, with ones in <inline-formula id="pone.0211044.e014"><alternatives><graphic id="pone.0211044.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:munder><mml:mi mathvariant="bold">R</mml:mi> <mml:mo>_</mml:mo></mml:munder></mml:math></alternatives></inline-formula> marking rejections of the null hypothesis for the corresponding resolutions and positions. When plotting the significance maps, the vertical axis is inverted.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211044.g004" xlink:type="simple"/>
</fig>
<p>As an example one can calculate the vector elements of the <inline-formula id="pone.0211044.e015"><alternatives><graphic id="pone.0211044.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:munder><mml:mi mathvariant="bold">L</mml:mi> <mml:mo>_</mml:mo></mml:munder></mml:math></alternatives></inline-formula> matrix corresponding to the resolution/position pairs (1, 1), (2, 2) and (3, 4) of the data matrix
<disp-formula id="pone.0211044.e016"><alternatives><graphic id="pone.0211044.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd><mml:mrow><mml:munder><mml:mi mathvariant="bold">X</mml:mi> <mml:mo>_</mml:mo></mml:munder> <mml:mo>=</mml:mo> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>3</mml:mn></mml:mtd> <mml:mtd><mml:mn>2</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>2</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>The Epanechnikov weights for the given resolution/position pairs equal <bold>w</bold><sub>1,1</sub> = [1, 0, 0, 0, 0]<sup><italic>T</italic></sup>, <bold>w</bold><sub>2,2</sub> = 1/10 ⋅ [3, 4, 3, 0, 0]<sup><italic>T</italic></sup>, and <bold>w</bold><sub>3,4</sub> = 1/30 ⋅ [0, 5, 8, 9, 8]<sup><italic>T</italic></sup>, where <sup><italic>T</italic></sup> indicates the transpose. The resulting vector elements are
<disp-formula id="pone.0211044.e017"><alternatives><graphic id="pone.0211044.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold">L</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>2</mml:mn> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mspace width="3.33333pt"/><mml:mspace width="3.33333pt"/><mml:mo>,</mml:mo> <mml:mspace width="3.33333pt"/><mml:mspace width="3.33333pt"/><mml:msub><mml:mi mathvariant="bold">L</mml:mi> <mml:mrow><mml:mn>2</mml:mn> <mml:mo>,</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>10</mml:mn></mml:mfrac> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>7</mml:mn> <mml:mo>,</mml:mo> <mml:mn>7</mml:mn> <mml:mo>,</mml:mo> <mml:mn>13</mml:mn> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mspace width="3.33333pt"/><mml:mspace width="3.33333pt"/><mml:mo>,</mml:mo> <mml:mspace width="3.33333pt"/><mml:mspace width="3.33333pt"/><mml:msub><mml:mi mathvariant="bold">L</mml:mi> <mml:mrow><mml:mn>3</mml:mn> <mml:mo>,</mml:mo> <mml:mn>4</mml:mn></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>30</mml:mn></mml:mfrac> <mml:mo>·</mml:mo> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mn>17</mml:mn> <mml:mo>,</mml:mo> <mml:mn>56</mml:mn> <mml:mo>,</mml:mo> <mml:mn>22</mml:mn> <mml:mo>,</mml:mo> <mml:mn>13</mml:mn> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
</sec>
<sec id="sec005">
<title>Normality testing</title>
<p>From the matrix <inline-formula id="pone.0211044.e018"><alternatives><graphic id="pone.0211044.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:munder><mml:mi mathvariant="bold">L</mml:mi> <mml:mo>_</mml:mo></mml:munder></mml:math></alternatives></inline-formula>, the actual one-dimensional normality test statistics are calculated. For each of the (<italic>s</italic>, <italic>d</italic>) pairs, the p-value of the AD test statistic of the vector <bold>L</bold><sub><italic>s</italic>,<italic>d</italic></sub> is stored. To address the problem of multiple testing, the algorithm outputs two significance maps, one based on the Bonferroni approach and one based on False Discovery Rate (FDR) [<xref ref-type="bibr" rid="pone.0211044.ref018">18</xref>]. The <italic>p</italic>-dimensional vector of p-values of each resolution is fed into FDR, generating the FDR-based significance map resolution by resolution. For the Bonferroni approach, the critical value is obtained from the nominal significance level <italic>α</italic> divided by the number of dimensions <italic>p</italic>, producing on average one false alarm every 1/<italic>α</italic> resolution. This follows the usual SiZer recommendation of adjusting the significance for each resolution separately. The alternative, adjusting the output map for all the resolution/position pairs simultaneously, is known from the SiZer literature to be overly conservative [<xref ref-type="bibr" rid="pone.0211044.ref007">7</xref>]. The nominal significance level is by default equal to <italic>α</italic> = 0.05.</p>
</sec>
<sec id="sec006">
<title>The <italic>k</italic>-sample problem</title>
<p>For the <italic>k</italic>-sample problem, the <italic>k</italic> data matrices <inline-formula id="pone.0211044.e019"><alternatives><graphic id="pone.0211044.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:msub><mml:munder><mml:mi mathvariant="bold">X</mml:mi> <mml:mo>_</mml:mo></mml:munder> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mspace width="3.33333pt"/><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> are all put through the summation procedure of <xref ref-type="fig" rid="pone.0211044.g004">Fig 4</xref>, producing <inline-formula id="pone.0211044.e020"><alternatives><graphic id="pone.0211044.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:msub><mml:munder><mml:mi mathvariant="bold">L</mml:mi> <mml:mo>_</mml:mo></mml:munder> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mspace width="3.33333pt"/><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>. For each resolution/position pair (<italic>s</italic>, <italic>d</italic>), the <italic>k</italic> corresponding vectors (of size <italic>n</italic><sub><italic>i</italic></sub>, <italic>i</italic> = 1, …, <italic>k</italic>) from the <inline-formula id="pone.0211044.e021"><alternatives><graphic id="pone.0211044.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:msub><mml:munder><mml:mi mathvariant="bold">L</mml:mi> <mml:mo>_</mml:mo></mml:munder> <mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula> matrices are fed into the <italic>k</italic>-sample AD test [<xref ref-type="bibr" rid="pone.0211044.ref009">9</xref>, <xref ref-type="bibr" rid="pone.0211044.ref019">19</xref>]. The distributions of the sums along the dimensions will in general be different from the marginal distributions. Nevertheless, if the <italic>k</italic> data sets do have the same multivariate distribution, for a given resolution/position pair (<italic>s</italic>, <italic>d</italic>), the distributions of the <italic>k</italic> different summation vectors will be the same. The p-values of the tests are stored and used in the generation of the FDR-based significance map, while the Bonferroni approach finds the critical value as for the multinormality testing. If the null hypothesis is rejected, the (<italic>s</italic>, <italic>d</italic>)-element of the output matrix is marked as a significant element, indicating that at least one of the empirical distributions are significantly different from the others for this resolution/position pair.</p>
</sec>
<sec id="sec007">
<title>Anderson-Darling testing</title>
<p>The two algorithms presented use three different AD tests. The AD goodness-of-fit test is used in the case of checking for multinormality [<xref ref-type="bibr" rid="pone.0211044.ref020">20</xref>–<xref ref-type="bibr" rid="pone.0211044.ref022">22</xref>]. For the two-sample/<italic>k</italic>-sample case, the versions of the AD test suggested by [<xref ref-type="bibr" rid="pone.0211044.ref009">9</xref>] and [<xref ref-type="bibr" rid="pone.0211044.ref019">19</xref>], respectively, are used.</p>
<p>The AD goodness-of-fit test checks the simple null hypothesis that a sample is from a distribution with a known continuous CDF, <italic>F</italic>(<italic>x</italic>). Let <italic>x</italic><sub>1</sub> ≤ <italic>x</italic><sub>2</sub> ≤ ⋯ ≤ <italic>x</italic><sub><italic>n</italic></sub> be the ordered sample of size <italic>n</italic>, and let <italic>u</italic><sub><italic>i</italic></sub> = <italic>F</italic>(<italic>x</italic><sub><italic>i</italic></sub>), <italic>i</italic> = 1, …, <italic>n</italic>. The AD test statistic is defined as
<disp-formula id="pone.0211044.e022"><alternatives><graphic id="pone.0211044.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>A</mml:mi> <mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>≡</mml:mo> <mml:mo>-</mml:mo> <mml:mi>n</mml:mi> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>n</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo form="prefix">ln</mml:mo> <mml:mo>[</mml:mo> <mml:msub><mml:mi>u</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>u</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>-</mml:mo> <mml:mi>i</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula></p>
<p>This clearly shows that the AD test is distribution free, as long as the null distribution is fully known. Approximate expressions for the asymptotic distribution of the AD test are given by [<xref ref-type="bibr" rid="pone.0211044.ref008">8</xref>, <xref ref-type="bibr" rid="pone.0211044.ref010">10</xref>].</p>
<p>When testing for multinormality with unknown distributional parameters, i.e. testing a composite hypothesis, <italic>F</italic>(<italic>x</italic>) is some unknown normal CDF, something which changes the distribution of the AD test statistic. In this case, the sorted data are normalized, producing <italic>z</italic><sub><italic>i</italic></sub>, <italic>i</italic> = 1, …, <italic>n</italic>. Then, <inline-formula id="pone.0211044.e023"><alternatives><graphic id="pone.0211044.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:msubsup><mml:mi>u</mml:mi> <mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>=</mml:mo> <mml:msub><mml:mi>F</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>z</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is produced, where <italic>F</italic><sub>0</sub>(⋅) is the standard normal CDF. These <inline-formula id="pone.0211044.e024"><alternatives><graphic id="pone.0211044.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:msubsup><mml:mi>u</mml:mi> <mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> values are fed into <xref ref-type="disp-formula" rid="pone.0211044.e022">Eq (3)</xref>, and the final test statistic is obtained by applying the correction factor for finite sample sizes given on page 123 of [<xref ref-type="bibr" rid="pone.0211044.ref023">23</xref>]. The p-values and critical values are calculated from the approximations given on page 127 of [<xref ref-type="bibr" rid="pone.0211044.ref023">23</xref>]. Following page 373 of [<xref ref-type="bibr" rid="pone.0211044.ref023">23</xref>], the presented algorithm requires <italic>n</italic> ≥ 8. The presence of ties in the data is a good indicator of non-normality, something which the AD test will reflect too. For instance, if normally distributed data is in some way rounded off, the rejection rate will be higher than the rate expected from the prescribed significance level.</p>
<p>For the <italic>k</italic>-sample case, there is no need to estimate any parameters, and the test statistic reduces to a rank statistic. Hence, under the null hypothesis, the distribution of the test statistic is independent of the distribution of the <italic>k</italic> samples. The two-sample case and the <italic>k</italic>-sample case are treated separately, even though the <italic>k</italic>-sample reduces to the two-sample case in [<xref ref-type="bibr" rid="pone.0211044.ref009">9</xref>] when <italic>k</italic> = 2. The correction factor in [<xref ref-type="bibr" rid="pone.0211044.ref009">9</xref>] is used to produce the final two-sample test statistic. [<xref ref-type="bibr" rid="pone.0211044.ref009">9</xref>] shows that the distribution of the sample-size adjusted two-sample AD test statistic can be approximated well by the asymptotic distribution of the AD goodness-of-fit test for a fully known null distribution. The presented algorithm uses Equation (3.6) in [<xref ref-type="bibr" rid="pone.0211044.ref010">10</xref>] to produce the approximate p-value of the test statistic when <italic>k</italic> = 2.</p>
<p>The general <italic>k</italic>-sample AD test statistic in [<xref ref-type="bibr" rid="pone.0211044.ref019">19</xref>] is given as
<disp-formula id="pone.0211044.e025"><alternatives><graphic id="pone.0211044.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>A</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>N</mml:mi></mml:mrow></mml:msub> <mml:mo>≡</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>N</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>k</mml:mi></mml:munderover> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>n</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mi>N</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:munderover> <mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:msub><mml:mi>M</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:mi>j</mml:mi> <mml:msub><mml:mi>n</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>-</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <italic>N</italic> = <italic>n</italic><sub>1</sub> + <italic>n</italic><sub>2</sub> + ⋯ + <italic>n</italic><sub><italic>k</italic></sub>, and <italic>M</italic><sub><italic>ij</italic></sub> is the number of observations in the <italic>i</italic>-th sample that are not greater than the <italic>j</italic>-th observation of the pooled sample of all <italic>k</italic> samples. Equation (6) in [<xref ref-type="bibr" rid="pone.0211044.ref019">19</xref>] modifies the expression for <italic>A</italic><sub><italic>kN</italic></sub>, to be able to handle ties in the data. The presented algorithm uses the expression adjusted for ties, both for the two-sample and <italic>k</italic>-sample cases. Thereby, <italic>F</italic><sub><italic>i</italic></sub>(<italic>x</italic>) in <xref ref-type="disp-formula" rid="pone.0211044.e004">Eq (1)</xref> can be connected to a continuous or discrete random vector. The interpolation scheme of [<xref ref-type="bibr" rid="pone.0211044.ref019">19</xref>] is used to determine the p-value of <italic>A</italic><sub><italic>kN</italic></sub> when <italic>k</italic> &gt; 2. Inspired by [<xref ref-type="bibr" rid="pone.0211044.ref009">9</xref>], it is required that all <italic>n</italic><sub><italic>i</italic></sub> ≥ 8, <italic>i</italic> = 1, …, <italic>k</italic>.</p>
<p>In theory, any omnibus, univariate test that achieves a specified significance level can be used in the presented framework for testing the results of the weighted summations. Relying on power studies by [<xref ref-type="bibr" rid="pone.0211044.ref024">24</xref>–<xref ref-type="bibr" rid="pone.0211044.ref026">26</xref>], the well-known, univariate Shapiro-Wilk test [<xref ref-type="bibr" rid="pone.0211044.ref024">24</xref>, <xref ref-type="bibr" rid="pone.0211044.ref027">27</xref>, <xref ref-type="bibr" rid="pone.0211044.ref028">28</xref>] is seen as the best alternative to the univariate AD test used for multinormality testing. Other tests that were considered include Watson’s <italic>U</italic><sup>2</sup> test [<xref ref-type="bibr" rid="pone.0211044.ref029">29</xref>], Kuiper’s test [<xref ref-type="bibr" rid="pone.0211044.ref030">30</xref>], Lilliefors’ test [<xref ref-type="bibr" rid="pone.0211044.ref031">31</xref>], the Cramér-von-Mises test [<xref ref-type="bibr" rid="pone.0211044.ref032">32</xref>], the Shapiro-Francia test [<xref ref-type="bibr" rid="pone.0211044.ref033">33</xref>], D’Agostino-Pearson’s <italic>K</italic><sup>2</sup> test [<xref ref-type="bibr" rid="pone.0211044.ref034">34</xref>, <xref ref-type="bibr" rid="pone.0211044.ref035">35</xref>], the Jarque-Bera test [<xref ref-type="bibr" rid="pone.0211044.ref036">36</xref>], and Doornik’s test [<xref ref-type="bibr" rid="pone.0211044.ref037">37</xref>]. Other tests considered for the <italic>k</italic>-sample case include the Kolmogorov-Smirnov test [<xref ref-type="bibr" rid="pone.0211044.ref038">38</xref>], the Cramér-von-Mises test [<xref ref-type="bibr" rid="pone.0211044.ref038">38</xref>], and Watson’s <inline-formula id="pone.0211044.e026"><alternatives><graphic id="pone.0211044.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:msubsup><mml:mi>U</mml:mi> <mml:mi>k</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> test [<xref ref-type="bibr" rid="pone.0211044.ref039">39</xref>].</p>
</sec>
<sec id="sec008">
<title>Cramér-Wold</title>
<p>The Cramér-Wold theorem states that two random column vectors <bold>X</bold> and <bold>Y</bold> have the same distribution if and only if for all row vectors <bold>a</bold>, the random variables <bold>a</bold> ⋅ <bold>X</bold> and <bold>a</bold> ⋅ <bold>Y</bold> have the same distribution [<xref ref-type="bibr" rid="pone.0211044.ref040">40</xref>]. In the presented algorithms, the different summation weights of the Epanechnikov window take the role of <bold>a</bold>. Thereby, when doing the summation and testing for normality/difference between samples for many resolutions, a set of <bold>a</bold> vectors are applied to the single or many data sets. The Cramér-Wold theorem requires that the distribution of <bold>a</bold> ⋅ <bold>X</bold> and <bold>a</bold> ⋅ <bold>Y</bold> are equal for all possible <bold>a</bold> vectors. In the presented setting, only a finite number of vectors are tested. Since the presented algorithms are most suitable for data with some sort of neighboring structure (e.g. time series or spatial data), the important <bold>a</bold> vectors should be those that look at dimensions close to each other to a varying degree. Hence, following the Cramér-Wold theorem, a lack of rejection for (almost) all resolutions/positions should be seen as a good indication of the null hypothesis actually being true for the whole data set.</p>
</sec>
<sec id="sec009">
<title>Significance of rejections</title>
<p>The p-value is available for all the resolution/position pairs. The lower the p-value of a “rejection pair”, the more significant the rejection of the null hypothesis is on that resolution/position. By changing the significance level, one can determine on which resolution/position the null hypothesis is most significantly rejected. In <xref ref-type="fig" rid="pone.0211044.g005">Fig 5</xref> the example of the Introduction is revisited, where significance levels of 0.005 and 0.001 are used, compared to 0.05 in the Introduction. By comparing <xref ref-type="fig" rid="pone.0211044.g005">Fig 5</xref> to <xref ref-type="fig" rid="pone.0211044.g003">Fig 3</xref>, it is clear that for this realization, the most significant region is the single non-normal dimension of position index 20, and the region from index 26 to 40 is the second most non-normal.</p>
<fig id="pone.0211044.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211044.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Significance maps of the scale-space multinormality test for the data of the Introduction.</title>
<p>Left/Right: Significance level of 0.005/0.001.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211044.g005" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec010">
<title>Power of the scale-space tests</title>
<p>There are no clear templates for power studies of the proposed scale-space tests. After the summations are done, the tests use the well-documented AD tests. Thereby, the power of the scale-space tests is connected to the power of the AD tests. Instead, it can be informative to illustrate how the power varies over the different resolution/position pairs of the output matrix for a given example. Assume that the data set has the same structure as in the motivational example of the Introduction. <xref ref-type="fig" rid="pone.0211044.g006">Fig 6</xref> shows the rejection ratio (from 1000 data sets) of the scale-space test for multinormality. As can be seen, one finds the highest powers for the resolution/position pairs that best fit the non-normal dimensions. Similar results would be obtained for the test for comparing <italic>k</italic> data sets.</p>
<fig id="pone.0211044.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211044.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Rejection ratios of all resolution/position pairs for 1 000 replications of the motivational example.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211044.g006" xlink:type="simple"/>
</fig>
<p>To investigate the effect of increased number of dimensions, a number of normally distributed dimensions are added to the right side of the signal of the Introduction. <xref ref-type="table" rid="pone.0211044.t001">Table 1</xref> shows the power of the multinormality test for different number of dimensions and for the FDR/Bonferroni correction. The case of 50 dimensions in total corresponds to the power of the pairs of <xref ref-type="fig" rid="pone.0211044.g006">Fig 6</xref>. From this it is clear that the power decreases as the number of dimensions grows, which is to be expected as adjustments for multiple testing are enforced.</p>
<table-wrap id="pone.0211044.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211044.t001</object-id>
<label>Table 1</label>
<caption>
<title>Power of test for multinormality when the signal of the Introduction is augmented with a number of normally distributed dimensions.</title>
</caption>
<alternatives>
<graphic id="pone.0211044.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211044.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center" rowspan="2" style="border-right:thick"/>
<th align="center" colspan="6">Window width/position pair</th>
</tr>
<tr>
<th align="center" colspan="2">1/20</th>
<th align="center" colspan="2">7/9</th>
<th align="center" colspan="2">9/37</th>
</tr>
<tr>
<th align="center" style="border-bottom:thick;border-right:thick">Dimensions in total</th>
<th align="center" style="border-bottom:thick">FDR</th>
<th align="center" style="border-bottom:thick">Bonf.</th>
<th align="center" style="border-bottom:thick">FDR</th>
<th align="center" style="border-bottom:thick">Bonf.</th>
<th align="center" style="border-bottom:thick">FDR</th>
<th align="center" style="border-bottom:thick">Bonf.</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" style="border-right:thick">50</td>
<td align="char" char=".">0.735</td>
<td align="char" char=".">0.687</td>
<td align="char" char=".">0.863</td>
<td align="char" char=".">0.684</td>
<td align="char" char=".">0.808</td>
<td align="char" char=".">0.563</td>
</tr>
<tr>
<td align="center" style="border-right:thick">100</td>
<td align="char" char=".">0.619</td>
<td align="char" char=".">0.583</td>
<td align="char" char=".">0.773</td>
<td align="char" char=".">0.596</td>
<td align="char" char=".">0.668</td>
<td align="char" char=".">0.440</td>
</tr>
<tr>
<td align="center" style="border-right:thick">250</td>
<td align="char" char=".">0.457</td>
<td align="char" char=".">0.443</td>
<td align="char" char=".">0.578</td>
<td align="char" char=".">0.433</td>
<td align="char" char=".">0.481</td>
<td align="char" char=".">0.272</td>
</tr>
<tr>
<td align="center" style="border-right:thick">500</td>
<td align="char" char=".">0.331</td>
<td align="char" char=".">0.312</td>
<td align="char" char=".">0.411</td>
<td align="char" char=".">0.295</td>
<td align="char" char=".">0.334</td>
<td align="char" char=".">0.197</td>
</tr>
<tr>
<td align="center" style="border-right:thick">1000</td>
<td align="char" char=".">0.264</td>
<td align="char" char=".">0.248</td>
<td align="char" char=".">0.298</td>
<td align="char" char=".">0.219</td>
<td align="char" char=".">0.227</td>
<td align="char" char=".">0.116</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
</sec>
<sec id="sec011" sec-type="results">
<title>Results</title>
<p>The presented algorithms are tested on a number of different data sets. A five percent significance level is used for all the figures, unless otherwise stated. First, the initial example of the Introduction is investigated in more detail.</p>
<sec id="sec012">
<title>Introductory example revisited</title>
<p>For larger resolutions, the scale-space test for multinormality can be shown to increase the mode separation if the distribution has more than one mode. This is demonstrated through some simple examples. Assume that all the dimensions of some data set are unimodal normal with different means and/or variances for different dimensions. The result of the summation will then be some other normal distribution.</p>
<p>A short example of this is given. Assume that the data matrix <inline-formula id="pone.0211044.e027"><alternatives><graphic id="pone.0211044.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:munder><mml:mi mathvariant="bold">X</mml:mi> <mml:mo>_</mml:mo></mml:munder></mml:math></alternatives></inline-formula> has dimensions [10, 3] and that column 1, 2, and 3 contain <inline-formula id="pone.0211044.e028"><alternatives><graphic id="pone.0211044.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0211044.e029"><alternatives><graphic id="pone.0211044.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mn>4</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and <inline-formula id="pone.0211044.e030"><alternatives><graphic id="pone.0211044.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mn>8</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, distributed variables, respectively. The summation (for simplicity, assuming even weights of 1/3) over these three columns would produce a 10-element long vector with distribution <inline-formula id="pone.0211044.e031"><alternatives><graphic id="pone.0211044.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mn>4</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>3</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, which the AD test would detect as normal, i.e. the test would not reject it.</p>
<p>Now assume that the ten samples of a given dimension do not have the same distribution. Assume that the five first samples of the three columns are distributed as <inline-formula id="pone.0211044.e032"><alternatives><graphic id="pone.0211044.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, while the last five are distributed as <inline-formula id="pone.0211044.e033"><alternatives><graphic id="pone.0211044.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. When checking the columns separately, the 10-element vector might not “look” enough different from a unimodal normal distribution to be rejected by the AD test. When summing (again, assuming even weights of 1/3) over the three columns, the distribution of the sum of the first five samples is given by <inline-formula id="pone.0211044.e034"><alternatives><graphic id="pone.0211044.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>3</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, while the last five have a <inline-formula id="pone.0211044.e035"><alternatives><graphic id="pone.0211044.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>3</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> distribution. This shows that the peaks have larger separation (both variances have decreased) as a result of the summation.</p>
</sec>
<sec id="sec013">
<title>Multinormality of temperature data</title>
<p>A data set obtained from the Norwegian Meteorological Institute is analyzed. The data show daily mean temperature for the 92 days of June–August for the period 1937 to 2008 at Blindern, Oslo. This gives a data matrix of dimensions [<italic>n</italic>, <italic>p</italic>] = [72, 92], making algorithms that rely on inversion of the estimated covariance matrix impossible to use. A plot of all the 72 years is given in <xref ref-type="fig" rid="pone.0211044.g007">Fig 7</xref>.</p>
<fig id="pone.0211044.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211044.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Daily mean temperatures at Oslo, Blindern, for the period 1937-2008.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211044.g007" xlink:type="simple"/>
</fig>
<p>
<xref ref-type="fig" rid="pone.0211044.g008">Fig 8</xref> gives the multinormality check results. Note that significant features are found both for the FDR and Bonferroni correction. To see what is going on, the period around time point 75 (i.e. in the middle of August) is shown in <xref ref-type="fig" rid="pone.0211044.g009">Fig 9</xref>. From this figure it seems that the mean temperature is around 15°C, but the temperature distribution around this time is skewed upwards. This means that Oslo at this time of the year experiences larger positive than negative deviations from the mean, which is not a surprising result if you have knowledge about the climate in that area.</p>
<fig id="pone.0211044.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211044.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Significance maps for summer temperatures in Oslo.</title>
<p>See <xref ref-type="fig" rid="pone.0211044.g003">Fig 3</xref> for annotation details.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211044.g008" xlink:type="simple"/>
</fig>
<fig id="pone.0211044.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211044.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Mid-August temperatures in Oslo for the years 1937–2008.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211044.g009" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec014">
<title>Comparison of temperature records</title>
<p>Temperature data sets from two different meteorological stations in the Oslo area are compared. One is located at Ferder lighthouse at the start of the 100 km long Oslo fjord, while the other is located at Fornebu, which is at the very inner part of the Oslo fjord. The two data sets consist of more or less overlapping yearly records, with 64 and 45 complete years, respectively. Years with missing data in the months of interest have been removed. <xref ref-type="fig" rid="pone.0211044.g010">Fig 10</xref> shows the two data sets, and <xref ref-type="fig" rid="pone.0211044.g011">Fig 11</xref> shows the resulting significance maps. It is clear that the temperature distribution at the two stations differ early and late in the summer. From a closer inspection, it is clear that Fornebu is warmer in early summer, while the opposite effect takes place a few months later.</p>
<fig id="pone.0211044.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211044.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Temperature data from Ferder (blue) and Fornebu (red) and mean values marked by thick lines.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211044.g010" xlink:type="simple"/>
</fig>
<fig id="pone.0211044.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211044.g011</object-id>
<label>Fig 11</label>
<caption>
<title>Significance maps from comparing the temperature data of Ferder and Fornebu with the scale-space method.</title>
<p>See <xref ref-type="fig" rid="pone.0211044.g003">Fig 3</xref> for annotation details.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211044.g011" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec015">
<title>Comparison to other methods</title>
<p>Just about all methods for testing for multinormality rely in some way on inverting the estimated covariance matrix. When the number of samples is less or equal to the number of dimensions (HDLSS setting), i.e. when <italic>n</italic> ≤ <italic>p</italic>, the estimated covariance matrix is non-invertible. The projection methods of [<xref ref-type="bibr" rid="pone.0211044.ref041">41</xref>] and the method based on Srivastava’s graphical method in [<xref ref-type="bibr" rid="pone.0211044.ref042">42</xref>] are applicable in this HDLSS setting, but no open implementations of the methods exist for power evaluation. The methods of Liang in [<xref ref-type="bibr" rid="pone.0211044.ref043">43</xref>, <xref ref-type="bibr" rid="pone.0211044.ref044">44</xref>] are also applicable in the HDLSS setting and open implementations exist. The preferred method of Liang [<xref ref-type="bibr" rid="pone.0211044.ref043">43</xref>] first transforms the data matrix, and then projects it onto some lower-dimensional space of dimension <italic>d</italic> ≤ min(<italic>n</italic> − 2, <italic>p</italic>). The transformed data will under the null hypothesis be distributed as a <italic>d</italic>-dimensional standard multinormal vector, something which is checked using the skewness and kurtosis test of [<xref ref-type="bibr" rid="pone.0211044.ref045">45</xref>]. Asymptotic distributions are given, but in the setting of interest (<italic>n</italic> is not large compared to <italic>p</italic>), the use of the Liang test [<xref ref-type="bibr" rid="pone.0211044.ref043">43</xref>] relies on a permutation procedure for generating p-values.</p>
<p>It is not straightforward to compare the presented scale-space method to the Liang procedure since the presented scale-space method does not produce one single answer to the hypothesis testing problem. A simple example is analyzed to illustrate that the presented method outperforms the Liang test in some settings. Assume the same data set structure as in the example of the Introduction, except that the only non-normal part is the mixture of dimensions 6 to 12, the other dimensions are zero mean normally distributed. This setup results in the optimal resolution/position pair being (4, 9), i.e. summing over dimensions 6 to 12. When the non-zero mean value in this area is 2.35, the presented scale-space method has a detection ratio of 0.884/0.918 (Bonferroni/FDR) for the pair (4, 9) (based on 1000 Monte Carlo repetitions). The Liang test has for the same data sets a rejection ratio of 0.659. For the Liang test only the kurtosis test and only the optimal projection dimension (<italic>d</italic> = 1) are used. In a real setting, the optimal projection dimension would not be known and both the skewness and kurtosis test would be used, leading to a significantly lower power when the correction for multiple testing is done. In the same way, when the non-zero mean value is 2.05, the presented scale-space method has a rejection ratio of 0.569/0.628 for the pair (4, 9), while the Liang test has for the same data sets a rejection ratio of 0.480.</p>
<p>For the comparison of two or more data sets, there are several methods that handle the <italic>n</italic> ≤ <italic>p</italic> situation. Many of these methods use some kind of distance measure between the data vectors [<xref ref-type="bibr" rid="pone.0211044.ref046">46</xref>–<xref ref-type="bibr" rid="pone.0211044.ref049">49</xref>]. From these distances, the test statistics are generated, without estimating any covariance matrices. The test by Székely and Rizzo [<xref ref-type="bibr" rid="pone.0211044.ref049">49</xref>] is a <italic>k</italic>-sample extension of the two-sample test suggested by Baringhaus [<xref ref-type="bibr" rid="pone.0211044.ref050">50</xref>]. A similar two-sample test was suggested by Aslan [<xref ref-type="bibr" rid="pone.0211044.ref051">51</xref>]. The Aslan test performed very similar to, but not better than, the Székely-Rizzo/Baringhaus test in the two-sample test case of <xref ref-type="table" rid="pone.0211044.t002">Table 2</xref>. Different projection methods that handle the <italic>n</italic> ≤ <italic>p</italic> situation also exist, e.g. Random Projection (RP) [<xref ref-type="bibr" rid="pone.0211044.ref052">52</xref>] and DiProPerm [<xref ref-type="bibr" rid="pone.0211044.ref053">53</xref>] (see paper for more methods). For the case of interest (<italic>n</italic> ≤ <italic>p</italic>), the tests all rely on permutation procedures to determine the p-value of the test statistic.</p>
<table-wrap id="pone.0211044.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211044.t002</object-id>
<label>Table 2</label>
<caption>
<title>Power of comparing a number of different data sets with a varying number of dimensions (“Dim”) for which there is an expected value difference <italic>δ</italic> in the tested data sets.</title>
<p>For the Hall test, the <italic>T</italic> and <italic>S</italic> tests gave very similar results. Three nearest neighbors were used in the Nearest Neighbor test. The results of the Friedman-Rafsky test are for three trees, which consistently performed better than one and two trees in this setting. The scale-space results are for the Bonferroni/FDR correction, respectively. A 0.10 significance level is used and 2000 Monte Carlo samples are used.</p>
</caption>
<alternatives>
<graphic id="pone.0211044.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211044.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center" style="border-bottom:thick;border-right:thick"/>
<th align="center" style="border-bottom:thick">Dim: 1</th>
<th align="center" style="border-bottom:thick">Dim: 3</th>
<th align="center" style="border-bottom:thick">Dim: 5</th>
<th align="center" style="border-bottom:thick">Dim: 7</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" style="border-bottom:thick;border-right:thick"><bold>Two-sample</bold></td>
<td align="center" style="border-bottom:thick"><bold><italic>δ</italic> = 0.85</bold></td>
<td align="center" style="border-bottom:thick"><bold><italic>δ</italic> = 0.75</bold></td>
<td align="center" style="border-bottom:thick"><bold><italic>δ</italic> = 0.65</bold></td>
<td align="center" style="border-bottom:thick"><bold><italic>δ</italic> = 0.55</bold></td>
</tr>
<tr>
<td align="center" style="border-right:thick">Scale-space</td>
<td align="center">0.579/0.591</td>
<td align="center">0.725/0.746</td>
<td align="center">0.722/0.782</td>
<td align="center">0.612/0.725</td>
</tr>
<tr>
<td align="center" style="border-right:thick">Friedman-Rafsky [<xref ref-type="bibr" rid="pone.0211044.ref046">46</xref>]</td>
<td align="center">0.273</td>
<td align="center">0.513</td>
<td align="center">0.588</td>
<td align="center">0.570</td>
</tr>
<tr>
<td align="center" style="border-right:thick">Hall-Tajvidi [<xref ref-type="bibr" rid="pone.0211044.ref047">47</xref>]</td>
<td align="center">0.166</td>
<td align="center">0.394</td>
<td align="center">0.515</td>
<td align="center">0.513</td>
</tr>
<tr>
<td align="center" style="border-right:thick">Nearest Neighbor [<xref ref-type="bibr" rid="pone.0211044.ref048">48</xref>]</td>
<td align="center">0.256</td>
<td align="center">0.487</td>
<td align="center">0.543</td>
<td align="center">0.531</td>
</tr>
<tr>
<td align="center" style="border-right:thick">Székely-Rizzo [<xref ref-type="bibr" rid="pone.0211044.ref049">49</xref>]</td>
<td align="center">0.400</td>
<td align="center">0.789</td>
<td align="center">0.866</td>
<td align="center">0.843</td>
</tr>
<tr>
<td align="center" style="border-right:thick">RP [<xref ref-type="bibr" rid="pone.0211044.ref052">52</xref>]</td>
<td align="center">0.286</td>
<td align="center">0.410</td>
<td align="center">0.425</td>
<td align="center">0.408</td>
</tr>
<tr>
<td align="center" style="border-right:thick">DiProPerm [<xref ref-type="bibr" rid="pone.0211044.ref053">53</xref>]</td>
<td align="center">0.465</td>
<td align="center">0.551</td>
<td align="center">0.518</td>
<td align="center">0.444</td>
</tr>
<tr>
<td align="center" style="border-bottom:thick;border-right:thick"><bold>Three-sample</bold></td>
<td align="center" style="border-bottom:thick"><bold><italic>δ</italic> = 0.45</bold></td>
<td align="center" style="border-bottom:thick"><bold><italic>δ</italic> = 0.35</bold></td>
<td align="center" style="border-bottom:thick"><bold><italic>δ</italic> = 0.325</bold></td>
<td align="center" style="border-bottom:thick"><bold><italic>δ</italic> = 0.30</bold></td>
</tr>
<tr>
<td align="center" style="border-right:thick">Scale-space</td>
<td align="center">0.608/0.631</td>
<td align="center">0.605/0.633</td>
<td align="center">0.697/0.759</td>
<td align="center">0.719/0.805</td>
</tr>
<tr>
<td align="center" style="border-right:thick">Székely-Rizzo</td>
<td align="center">0.330</td>
<td align="center">0.575</td>
<td align="center">0.740</td>
<td align="center">0.807</td>
</tr>
<tr>
<td align="center" style="border-bottom:thick;border-right:thick"><bold>Seven-sample</bold></td>
<td align="center" style="border-bottom:thick"><bold><italic>δ</italic> = 0.15</bold></td>
<td align="center" style="border-bottom:thick"><bold><italic>δ</italic> = 0.11</bold></td>
<td align="center" style="border-bottom:thick"><bold><italic>δ</italic> = 0.10</bold></td>
<td align="center" style="border-bottom:thick"><bold><italic>δ</italic> = 0.09</bold></td>
</tr>
<tr>
<td align="center" style="border-right:thick">Scale-space</td>
<td align="center">0.731/0.745</td>
<td align="center">0.611/0.633</td>
<td align="center">0.695/0.740</td>
<td align="center">0.672/0.757</td>
</tr>
<tr>
<td align="center" style="border-right:thick">Székely-Rizzo</td>
<td align="center">0.295</td>
<td align="center">0.468</td>
<td align="center">0.622</td>
<td align="center">0.675</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The case of two data sets <italic>X</italic> and <italic>Y</italic> is first investigated. The expected value of <italic>X</italic> is zero for all dimensions, while <italic>Y</italic> has one region of a number of neighboring dimensions with a non-zero expected value. Both <italic>X</italic> and <italic>Y</italic> have the same covariance structure as the example of the Introduction. The number of dimensions of <italic>Y</italic> that have a non-zero mean value is varied, along with this non-zero value. The upper part of <xref ref-type="table" rid="pone.0211044.t002">Table 2</xref> shows the results. The result of the scale-space algorithm refers to the resolution/position pair with the highest rejection ratio.</p>
<p>Of the alternative tests, the method of Székely and Rizzo [<xref ref-type="bibr" rid="pone.0211044.ref049">49</xref>] consistently shows the greatest power in the tested settings. When the difference between <italic>X</italic> and <italic>Y</italic> is across many dimensions, the power of the Székely and Rizzo test is higher than the power of the scale-space approach. If there instead is only one dimension with a different distribution of <italic>X</italic> and <italic>Y</italic>, the power of the scale-space approach is greater than for the Székely test. This means that the Székely is a good alternative approach, but by using the scale-space approach one can determine where in the data set the difference is located.</p>
<p>For the case of <italic>k</italic> = 3, the presented scale-space method is only compared to the method of Székely and Rizzo (the Hall-Tajvidi, RP and DiProPerm tests cannot be extended to <italic>k</italic> &gt; 2). The same covariance structure as for the two-sample case is used for the three data sets <italic>X</italic>, <italic>Y</italic> and <italic>Z</italic>. <italic>X</italic> is zero mean, while <italic>Y</italic> has for some neighboring dimensions a non-zero expected value of <bold><italic>δ</italic></bold>, and <italic>Z</italic> has for the same dimensions a non-zero expected value of −<bold><italic>δ</italic></bold>. See the middle part of <xref ref-type="table" rid="pone.0211044.t002">Table 2</xref> for the results. The case of <italic>k</italic> = 7 is finally investigated in the lower part of <xref ref-type="table" rid="pone.0211044.t002">Table 2</xref>. Here, the different data sets have the same structure as for the case of <italic>k</italic> = 3, but the different data sets <italic>X</italic><sub><italic>i</italic></sub>, <italic>i</italic> = 1, 2, …, 7 have mean values equal to <italic>i</italic> ⋅ <bold><italic>δ</italic></bold> for the non-zero dimensions. From these results, the scale-space method seems to improve compared to the Székely-Rizzo method when the number of data sets increase, and the methods are giving comparable results in the tested settings.</p>
</sec>
<sec id="sec016">
<title>Feature selection</title>
<p>In a classification setting, the p-values of the different resolution/position pairs can be used to find useful scale-space features. The pairs with the smallest p-values should be good candidate features for classification algorithms. The p-values of neighboring pairs will be correlated (for all resolution values larger than 1). An ad hoc strategy to avoid the selection of neighboring pairs is used. That is, say that the most significant pair is at window width 7 (i.e. resolution number 4) and position 5. Then, all pairs for two resolutions down (resolution number 2 and 3) and two resolutions up (resolution number 5 and 6) that sum over the data of position 5, are excluded from being selected as a feature as a result of pair (4, 5) being selected as a feature. The next feature to be selected corresponds to the resolution/position pair, which has not been excluded in the steps before, with the lowest p-value of the pairs not already selected. This is repeated until a wanted number of features are found or there are no good features left to pick from, where a potential feature’s “goodness” is connected to its p-value.</p>
<p>The suggested feature selection algorithm is tested on a setting similar to the example of the Introduction. Here, instead of having one data set with two parts, there are two data sets <italic>X</italic> and <italic>Y</italic>. <italic>X</italic> is distributed as the 20 first samples of the motivational example, while <italic>Y</italic> is distributed as the 20 remaining samples, except that the expected value equals −0.65 for position index 6 to 12 and −1 for position index 20. For indices 26, …, 40, the expected value increases linearly from 0.05 to 0.75.</p>
<p>The suggested feature selection scheme is compared to using all dimensions as inputs to classification algorithms. This is meant as a proof of concept, not a thorough comparison to other methods. The tested sample sizes of both <italic>X</italic> and <italic>Y</italic> were 20, 30 and 60. For the classification, <italic>k</italic> Nearest Neighbor classification (with <italic>k</italic> = 1 and <italic>k</italic> = 3), Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) were used, when applicable [<xref ref-type="bibr" rid="pone.0211044.ref054">54</xref>]. For the scale-space feature selection, the number of features selected ranged from 1 to 15. One pair of <italic>X</italic> and <italic>Y</italic> data sets was used to find the training features. These features were then used to classify 500 <italic>X</italic> and 500 <italic>Y</italic> data sets. This was repeated 100 times, making up in total 100000 tests, and the ratio of correct classification was averaged across these 100000 tests, as shown in <xref ref-type="fig" rid="pone.0211044.g012">Fig 12</xref>. The splitting up was done to average out the fact that different features will be selected depending on the training data set. With three well-selected features, one can capture the main differences in the two data sets, but as the figure shows, one needs on average more than three features to have the maximum ratio of correct classification. The figure shows that using the suggested scale-space features is better than using the raw data in this example.</p>
<fig id="pone.0211044.g012" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211044.g012</object-id>
<label>Fig 12</label>
<caption>
<title>Classifcation results when using scale-space features (solid lines) and all dimensions (dashed lines).</title>
<p>Classification methods are given as 1NN (blue), 3NN (red), LDA (black), QDA (magenta). The vertical axis shows the ratio of correct classifications based on 100000 simulations. Sample sizes from left to right are: 20, 30 and 60.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211044.g012" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec017" sec-type="conclusions">
<title>Conclusions</title>
<p>The scale-space methodology is applied to the testing for multivariate normality and the <italic>k</italic>-sample problem. The summation across dimensions/positions reduces the multivariate problem to a large number of one-dimensional tests. A significance map, showing where and for which resolutions the null hypothesis is rejected, is generated by going through all combinations of the position and resolution parameters. The summation throws away all information of the dependency structure of the data. When there are more multivariate observations than dimensions, i.e. <italic>n</italic> &gt; <italic>p</italic>, the discharging of covariance information will lower the power of the scale-space tests compared to tests that use this information gained through estimation of the covariance matrix. What is gained on the other hand, is the ability to check for multinormality and compare data sets in the High Dimension Low Sample Size setting, something which almost all other methods fail to handle.</p>
<p>The presented algorithms are tested on artificial data and real temperature data sets, showing how both the check for multinormality and how the comparison of data sets can be done through a scale-space approach.</p>
<p>Within the scale-space framework, to the authors’ best knowledge, there is no other algorithm to compare the presented work with, even though a large number of tests for assessing the multinormality of a given data set exist [<xref ref-type="bibr" rid="pone.0211044.ref026">26</xref>, <xref ref-type="bibr" rid="pone.0211044.ref055">55</xref>–<xref ref-type="bibr" rid="pone.0211044.ref057">57</xref>]. To the knowledge of the authors, the only multivariate methods for testing multinormality that handle the case when <italic>n</italic> ≤ <italic>p</italic>, are the methods [<xref ref-type="bibr" rid="pone.0211044.ref041">41</xref>–<xref ref-type="bibr" rid="pone.0211044.ref044">44</xref>]. The preferred Liang method [<xref ref-type="bibr" rid="pone.0211044.ref043">43</xref>] is inferior to the presented method in some relevant aspects and cases.</p>
<p>In the case of comparing <italic>k</italic> data sets, there exist some methods that handle the case where at least one of the sample sizes are less than the number of dimensions. In general, these methods are based on some distance measure between the data vectors, and do not estimate the covariance matrix, or projection onto lower-dimensional spaces. The suggested scale-space method is compared to these methods. In the tested settings, the power of the method of Székely and Hall [<xref ref-type="bibr" rid="pone.0211044.ref049">49</xref>] is comparable to the power of the scale-space approach. The Székely test does not on the other hand provide any info about where the data sets differ, information that is essential for doing feature selection. Selection of relevant features based on the presented scale-space <italic>k</italic>-sample problem algorithm is demonstrated in the “Results” section.</p>
</sec>
<sec id="sec018">
<title>Supporting information</title>
<supplementary-material id="pone.0211044.s001" mimetype="application/zip" position="float" xlink:href="info:doi/10.1371/journal.pone.0211044.s001" xlink:type="simple">
<label>S1 File</label>
<caption>
<title>MATLAB-files for running the presented algorithms.</title>
<p>(ZIP)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0211044.s002" mimetype="application/zip" position="float" xlink:href="info:doi/10.1371/journal.pone.0211044.s002" xlink:type="simple">
<label>S2 File</label>
<caption>
<title>Data sets used in paper.</title>
<p>(ZIP)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The temperature data is freely available through the web portal of The Norwegian Meteorological Institute. Our MATLAB implementation of the <italic>k</italic>-sample AD test is strongly influenced by the AnDarksamtest implementation by A. Trujillo-Ortiz, R. Hernandez-Walls, K. Barba-Rojo, L. Cupul-Magana and R. C. Zavala-Garcia found at <ext-link ext-link-type="uri" xlink:href="http://www.mathworks.com/matlabcentral/fileexchange/17451" xlink:type="simple">http://www.mathworks.com/matlabcentral/fileexchange/17451</ext-link>.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0211044.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cox</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Wermuth</surname> <given-names>N</given-names></name>. <article-title>Tests of Linearity, Multivariate Normality and the Adequacy of Linear Scores</article-title>. <source>Journal of the Royal Statistical Society Series C (Applied Statistics)</source>. <year>1994</year>;<volume>43</volume>(<issue>2</issue>):<fpage>347</fpage>–<lpage>355</lpage>.</mixed-citation>
</ref>
<ref id="pone.0211044.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Farrell</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Salibian-Barrera</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Naczk</surname> <given-names>K</given-names></name>. <article-title>On tests for Multivariate Normality and Associated Simulation Studies</article-title>. <source>Journal of Statistical Computation and Simulation</source>. <year>2007</year>;<volume>77</volume>(<issue>12</issue>):<fpage>1065</fpage>–<lpage>1080</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/10629360600878449" xlink:type="simple">10.1080/10629360600878449</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Looney</surname> <given-names>SW</given-names></name>. <article-title>How to Use Tests for Univariate Normality to Assess Multivariate Normality</article-title>. <source>The American Statistician</source>. <year>1995</year>;<volume>49</volume>(<issue>1</issue>):<fpage>64</fpage>–<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2307/2684816" xlink:type="simple">10.2307/2684816</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref004">
<label>4</label>
<mixed-citation publication-type="other" xlink:type="simple">Witkin AP. Scale-space Filtering In 8th International Joint Conference of Artificial Intelligence, Karlsruhe, West Germany. 1010–1022.</mixed-citation>
</ref>
<ref id="pone.0211044.ref005">
<label>5</label>
<mixed-citation publication-type="other" xlink:type="simple">Witkin AP. Scale-space Filtering: A new Approach to Multi-scale Description. In Acoustics, Speech, and Signal Processing. IEEE International Conference on ICASSP’84, San Diego, California, USA. 9:150–153.</mixed-citation>
</ref>
<ref id="pone.0211044.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Holmström</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Pasanen</surname> <given-names>L</given-names></name>. <article-title>Statistical Scale space Methods</article-title>. <source>International Statistical Review</source>. <year>2017</year>;<volume>81</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>30</lpage>.</mixed-citation>
</ref>
<ref id="pone.0211044.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chaudhuri</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Marron</surname> <given-names>JS</given-names></name>. <article-title>SiZer for Exploration of Structures in Curves</article-title>. <source>Journal of the American Statistical Association</source>. <year>1999</year>;<volume>94</volume>(<issue>447</issue>):<fpage>807</fpage>–<lpage>823</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/01621459.1999.10474186" xlink:type="simple">10.1080/01621459.1999.10474186</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Marsaglia</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Marsaglia</surname> <given-names>J</given-names></name>. <article-title>Evaluating the Anderson-Darling Distribution</article-title>. <source>Journal of Statistical Software</source>. <year>2004</year>;<volume>9</volume>(<issue>2</issue>):<fpage>1</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18637/jss.v009.i02" xlink:type="simple">10.18637/jss.v009.i02</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pettitt</surname> <given-names>AN</given-names></name>. <article-title>A Two-Sample Anderson-Darling Rank Statistic</article-title>. <source>Biometrika</source>. <year>1976</year>;<volume>63</volume>(<issue>1</issue>):<fpage>161</fpage>–<lpage>168</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2307/2335097" xlink:type="simple">10.2307/2335097</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sinclair</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Spurr</surname> <given-names>BD</given-names></name>. <article-title>Approximations to the Distribution Function of the Anderson-Darling Test Statistic</article-title>. <source>Journal of the American Statistical Association</source>. <year>1988</year>;<volume>83</volume>(<issue>404</issue>):<fpage>1190</fpage>–<lpage>1191</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/01621459.1988.10478720" xlink:type="simple">10.1080/01621459.1988.10478720</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref011">
<label>11</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Hochberg</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Tamhane</surname> <given-names>AC</given-names></name>. <chapter-title>Multiple Comparison Procedures</chapter-title>. <source>Wiley series in probability and mathematical statistics. Applied probability and statistics</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>John Wiley &amp; Sons</publisher-name>; <year>1987</year>.</mixed-citation>
</ref>
<ref id="pone.0211044.ref012">
<label>12</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Wand</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>MC</given-names></name>. <chapter-title>Kernel Smoothing</chapter-title>. <volume>vol. 60</volume> of <source>Monographs on Statistics and Applied Probability</source>. <publisher-loc>Boca Raton, FL</publisher-loc>: <publisher-name>Chapman &amp; Hall/CRC</publisher-name>; <year>1995</year>.</mixed-citation>
</ref>
<ref id="pone.0211044.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Godtliebsen</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Marron</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Chaudhuri</surname> <given-names>P</given-names></name>. <article-title>Statistical Significance of Features in Digital Images</article-title>. <source>Image and Vision Computing</source>. <year>2004</year>;<volume>22</volume>(<issue>13</issue>):<fpage>1093</fpage>–<lpage>1104</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.imavis.2004.05.002" xlink:type="simple">10.1016/j.imavis.2004.05.002</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Godtliebsen</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Marron</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Chaudhuri</surname> <given-names>P</given-names></name>. <article-title>Significance in Scale Space for Bivariate Density Estimation</article-title>. <source>Journal of Computational and Graphical Statistics</source>. <year>2002</year>;<volume>11</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1198/106186002317375596" xlink:type="simple">10.1198/106186002317375596</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chaudhuri</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Marron</surname> <given-names>JS</given-names></name>. <article-title>Scale Space View of Curve Estimation</article-title>. <source>The Annals of Statistics</source>. <year>2000</year>;<volume>28</volume>(<issue>2</issue>):<fpage>408</fpage>–<lpage>428</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1214/aos/1016218224" xlink:type="simple">10.1214/aos/1016218224</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Øigård</surname> <given-names>TA</given-names></name>, <name name-style="western"><surname>Rue</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Godtliebsen</surname> <given-names>F</given-names></name>. <article-title>Bayesian Multiscale Analysis for Time Series Data</article-title>. <source>Computational Statistics and Data Analysis</source>. <year>2006</year>;<volume>51</volume>(<issue>3</issue>):<fpage>1719</fpage>–<lpage>1730</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.csda.2006.07.034" xlink:type="simple">10.1016/j.csda.2006.07.034</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sørbye</surname> <given-names>SH</given-names></name>, <name name-style="western"><surname>Hindberg</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Olsen</surname> <given-names>LR</given-names></name>, <name name-style="western"><surname>Rue</surname> <given-names>H</given-names></name>. <article-title>Bayesian Multiscale Feature Detection of log-Spectral Densities</article-title>. <source>Computational Statistics and Data Analysis</source>. <year>2009</year>;<volume>53</volume>(<issue>11</issue>):<fpage>3746</fpage>–<lpage>3754</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.csda.2009.03.020" xlink:type="simple">10.1016/j.csda.2009.03.020</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Benjamini</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hochberg</surname> <given-names>Y</given-names></name>. <article-title>Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing</article-title>. <source>Journal of the Royal Statistical Society Series B (Methodological)</source>. <year>1995</year>;<volume>57</volume>(<issue>1</issue>):<fpage>289</fpage>–<lpage>300</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.2517-6161.1995.tb02031.x" xlink:type="simple">10.1111/j.2517-6161.1995.tb02031.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Scholz</surname> <given-names>FW</given-names></name>, <name name-style="western"><surname>Stephens</surname> <given-names>MA</given-names></name>. <article-title><italic>K</italic>-Sample Anderson-Darling Tests</article-title>. <source>Journal of the American Statistical Association</source>. <year>1987</year>;<volume>82</volume>(<issue>399</issue>):<fpage>918</fpage>–<lpage>924</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/01621459.1987.10478517" xlink:type="simple">10.1080/01621459.1987.10478517</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Anderson</surname> <given-names>TW</given-names></name>, <name name-style="western"><surname>Darling</surname> <given-names>DA</given-names></name>. <article-title>Asymptotic Theory of Certain “Goodness of Fit” Criteria Based on Stochastic Processes</article-title>. <source>Annals of Mathematical Statistics</source>. <year>1952</year>;<volume>23</volume>(<issue>2</issue>):<fpage>193</fpage>–<lpage>212</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1214/aoms/1177729437" xlink:type="simple">10.1214/aoms/1177729437</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Anderson</surname> <given-names>TW</given-names></name>, <name name-style="western"><surname>Darling</surname> <given-names>DA</given-names></name>. <article-title>A Test of Goodness of Fit</article-title>. <source>Journal of the American Statistical Association</source>. <year>1954</year>;<volume>49</volume>(<issue>268</issue>):<fpage>765</fpage>–<lpage>769</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/01621459.1954.10501232" xlink:type="simple">10.1080/01621459.1954.10501232</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lewis</surname> <given-names>PAW</given-names></name>. <article-title>Distribution of the Anderson-Darling Statistic</article-title>. <source>Annals of Mathematical Statistics</source>. <year>1961</year>;<volume>32</volume>(<issue>4</issue>):<fpage>1118</fpage>–<lpage>1124</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1214/aoms/1177704850" xlink:type="simple">10.1214/aoms/1177704850</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref023">
<label>23</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>D’Agostino</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Stephens</surname> <given-names>MA</given-names></name>, editors. <chapter-title>Goodness-of-fit Techniques</chapter-title>. <volume>vol. 68</volume> of <source>Statistics: Textbooks and Monographs</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Marcel Dekker</publisher-name>; <year>1986</year>.</mixed-citation>
</ref>
<ref id="pone.0211044.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shapiro</surname> <given-names>SS</given-names></name>, <name name-style="western"><surname>Wilk</surname> <given-names>MB</given-names></name>. <article-title>An Analysis of Variance Test for Normality (Complete Samples)</article-title>. <source>Biometrika</source>. <year>1965</year>;<volume>52</volume>(<issue>3/4</issue>):<fpage>591</fpage>–<lpage>611</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/biomet/52.3-4.591" xlink:type="simple">10.1093/biomet/52.3-4.591</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Stephens</surname> <given-names>MA</given-names></name>. <article-title>EDF Statistics for Goodness of Fit and Some Comparisons</article-title>. <source>Journal of the American Statistical Association</source>. <year>1974</year>;<volume>69</volume>(<issue>347</issue>):<fpage>730</fpage>–<lpage>737</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/01621459.1974.10480196" xlink:type="simple">10.1080/01621459.1974.10480196</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref026">
<label>26</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Thode</surname> <given-names>HC</given-names> <suffix>Jr</suffix></name>. <source>Testing for Normality</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Marcel Dekker</publisher-name>; <year>2002</year>.</mixed-citation>
</ref>
<ref id="pone.0211044.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rahman</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Govindarajulu</surname> <given-names>Z</given-names></name>. <article-title>A Modification of the test of Shapiro and Wilk for Normality</article-title>. <source>Journal of Applied Statistics</source>. <year>1997</year>;<volume>24</volume>(<issue>2</issue>):<fpage>219</fpage>–<lpage>235</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/02664769723828" xlink:type="simple">10.1080/02664769723828</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Royston</surname> <given-names>P</given-names></name>. <article-title>Approximating the Shapiro-Wilk W-test for Non-normality</article-title>. <source>Statistics and Computing</source>. <year>1992</year>;<volume>2</volume>(<issue>3</issue>):<fpage>117</fpage>–<lpage>119</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/BF01891203" xlink:type="simple">10.1007/BF01891203</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Watson</surname> <given-names>GS</given-names></name>. <article-title>Goodness-of-Fit Tests on a Circle</article-title>. <source>Biometrika</source>. <year>1961</year>;<volume>48</volume>(<issue>1/2</issue>):<fpage>109</fpage>–<lpage>114</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2307/2333135" xlink:type="simple">10.2307/2333135</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Stephens</surname> <given-names>MA</given-names></name>. <article-title>The Goodness-Of-Fit Statistic <italic>V</italic><sub><italic>n</italic></sub>: Distribution and Significance Points</article-title>. <source>Biometrika</source>. <year>1965</year>;<volume>52</volume>(<issue>3/4</issue>):<fpage>309</fpage>–<lpage>321</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2307/2333685" xlink:type="simple">10.2307/2333685</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lilliefors</surname> <given-names>H</given-names></name>. <article-title>On the Kolmogorov-Smirnov Test for Normality with Mean and Variance Unknown</article-title>. <source>Journal of the American Statistical Association</source>. <year>1967</year>;<volume>62</volume>(<issue>318</issue>):<fpage>399</fpage>–<lpage>402</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/01621459.1967.10482916" xlink:type="simple">10.1080/01621459.1967.10482916</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Stephens</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Maag</surname> <given-names>UR</given-names></name>. <article-title>Further Percentage Points for <inline-formula id="pone.0211044.e036"><alternatives><graphic id="pone.0211044.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0211044.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:msubsup><mml:mi>W</mml:mi> <mml:mi>N</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula></article-title>. <source>Biometrika</source>. <year>1968</year>;<volume>55</volume>(<issue>2</issue>):<fpage>428</fpage>–<lpage>430</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/biomet/55.2.428" xlink:type="simple">10.1093/biomet/55.2.428</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shapiro</surname> <given-names>SS</given-names></name>, <name name-style="western"><surname>Francia</surname> <given-names>RS</given-names></name>. <article-title>An Approximate Analysis of Variance Test for Normality</article-title>. <source>Journal of the American Statistical Association</source>. <year>1972</year>;<volume>67</volume>(<issue>337</issue>):<fpage>215</fpage>–<lpage>216</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/01621459.1972.10481232" xlink:type="simple">10.1080/01621459.1972.10481232</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>D’Agostino</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Belanger</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>D’Agostino</surname> <given-names>RB</given-names> <suffix>Jr</suffix></name>. <article-title>A Suggestion for Using Powerful and Informative Tests of Normality</article-title>. <source>The American Statistician</source>. <year>1990</year>;<volume>44</volume>(<issue>4</issue>):<fpage>316</fpage>–<lpage>321</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/00031305.1990.10475751" xlink:type="simple">10.1080/00031305.1990.10475751</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pearson</surname> <given-names>ES</given-names></name>, <name name-style="western"><surname>D’Agostino</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Bowman</surname> <given-names>KO</given-names></name>. <article-title>Tests for Departure from Normality: Comparison of Powers</article-title>. <source>Biometrika</source>. <year>1977</year>;<volume>64</volume>(<issue>2</issue>):<fpage>231</fpage>–<lpage>246</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/biomet/64.2.231" xlink:type="simple">10.1093/biomet/64.2.231</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jarque</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Bera</surname> <given-names>AK</given-names></name>. <article-title>Efficient Tests for Normality, Homoscedasticity and serial Independence of Regression Residuals</article-title>. <source>Economics Letters</source>. <year>1980</year>;<volume>6</volume>(<issue>3</issue>):<fpage>255</fpage>–<lpage>259</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0165-1765(80)90024-5" xlink:type="simple">10.1016/0165-1765(80)90024-5</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Doornik</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Hansen</surname> <given-names>H</given-names></name>. <article-title>An Omnibus Test for Univariate and Multivariate Normality</article-title>. <source>Oxford Bulletin of Economics and Statistics</source>. <year>2008</year>;<volume>70</volume>(<issue>s1</issue>):<fpage>927</fpage>–<lpage>939</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1468-0084.2008.00537.x" xlink:type="simple">10.1111/j.1468-0084.2008.00537.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kiefer</surname> <given-names>J</given-names></name>. <article-title>K-Sample Analogues of the Kolmogorov-Smirnov and Cramér-V. Mises Tests</article-title>. <source>Annals of Mathematical Statistics</source>. <year>1959</year>;<volume>39</volume>(<issue>2</issue>):<fpage>420</fpage>–<lpage>447</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1214/aoms/1177706261" xlink:type="simple">10.1214/aoms/1177706261</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Maag</surname> <given-names>UR</given-names></name>. <article-title>A <italic>k</italic>-Sample Analogue of Watson’s <italic>U</italic><sup>2</sup> Statistic</article-title>. <source>Biometrika</source>. <year>1966</year>;<volume>53</volume>(<issue>3/4</issue>):<fpage>579</fpage>–<lpage>583</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2307/2333662" xlink:type="simple">10.2307/2333662</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref040">
<label>40</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Lehmann</surname> <given-names>EL</given-names></name>. <source>Elements of Large-Sample Theory</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>1998</year>.</mixed-citation>
</ref>
<ref id="pone.0211044.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sürücü</surname> <given-names>Bariş</given-names></name>. <article-title>Goodness-of-Fit Tests for Multivariate Distributions</article-title>. <source>Communications in Statistics—Theory and Methods</source>. <year>2006</year>;<volume>35</volume>:<fpage>1319</fpage>–<lpage>1331</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/03610920600628999" xlink:type="simple">10.1080/03610920600628999</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hanusz</surname> <given-names>Zofia</given-names></name> and <name name-style="western"><surname>Tarasińska</surname> <given-names>Joanna</given-names></name>. <article-title>New Tests for Multivariate Normality Based on Small’s and Srivastava’s Graphical Methods</article-title>. <source>Journal of Statistical Computation and Simulation</source>. <year>2012</year>;<volume>82</volume>(<issue>12</issue>):<fpage>1743</fpage>–<lpage>1752</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/00949655.2011.594051" xlink:type="simple">10.1080/00949655.2011.594051</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Liang</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Fang</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Fang</surname> <given-names>KT</given-names></name>. <article-title>Testing Multinormality Based on Low-dimensional Projection</article-title>. <source>Journal of Statistical Planning and Inference</source>. <year>2000</year>;<volume>86</volume>(<issue>1</issue>):<fpage>129</fpage>–<lpage>141</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0378-3758(99)00168-8" xlink:type="simple">10.1016/S0378-3758(99)00168-8</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Liang</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Tang</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Chan</surname> <given-names>PS</given-names></name>. <article-title>A Generalized Shapiro-Wilk <italic>W</italic> Statistic for Testing High-dimensional Normality</article-title>. <source>Computational Statistics and Data Analysis</source>. <year>2009</year>;<volume>53</volume>(<issue>11</issue>):<fpage>3883</fpage>–<lpage>3891</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.csda.2009.04.016" xlink:type="simple">10.1016/j.csda.2009.04.016</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mardia</surname> <given-names>KV</given-names></name>. <article-title>Measures of Multivariate Skewness and Kurtosis with Applications</article-title>. <source>Biometrika</source>. <year>1970</year>;<volume>57</volume>(<issue>3</issue>):<fpage>519</fpage>–<lpage>530</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/biomet/57.3.519" xlink:type="simple">10.1093/biomet/57.3.519</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Friedman</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Rafsky</surname> <given-names>LC</given-names></name>. <article-title>Multivariate Generalizations of the Wald-Wolfowitz and Smirnov Two-Sample Tests</article-title>. <source>The Annals of Statistics</source>. <year>1979</year>;<volume>7</volume>(<issue>4</issue>):<fpage>697</fpage>–<lpage>717</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1214/aos/1176344722" xlink:type="simple">10.1214/aos/1176344722</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hall</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Tajvidi</surname> <given-names>N</given-names></name>. <article-title>Permutation Tests for Equality of Distributions in High-dimensional Settings</article-title>. <source>Biometrika</source>. <year>2002</year>;<volume>89</volume>(<issue>2</issue>):<fpage>359</fpage>–<lpage>374</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/biomet/89.2.359" xlink:type="simple">10.1093/biomet/89.2.359</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Henze</surname> <given-names>N</given-names></name>. <article-title>A Multivariate Two-Sample Test Based on the Number of Nearest Neighbor Type Coincidences</article-title>. <source>The Annals of Statistics</source>. <year>1988</year>;<volume>16</volume>(<issue>2</issue>):<fpage>772</fpage>–<lpage>783</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1214/aos/1176350835" xlink:type="simple">10.1214/aos/1176350835</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Székely</surname> <given-names>GJ</given-names></name>, <name name-style="western"><surname>Rizzo</surname> <given-names>ML</given-names></name>. <article-title>Testing for Equal Distributions in High Dimensions</article-title>. <source>InterStat</source>. <year>2004</year>;(<issue>5</issue>):<fpage>1</fpage>–<lpage>16</lpage>.</mixed-citation>
</ref>
<ref id="pone.0211044.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Baringhaus</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Franz</surname> <given-names>C</given-names></name>. <article-title>On a new Multivariate Two-sample Test</article-title>. <source>Journal of Multivariate Analysis</source>. <year>2004</year>;<volume>88</volume>(<issue>1</issue>):<fpage>190</fpage>–<lpage>206</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0047-259X(03)00079-4" xlink:type="simple">10.1016/S0047-259X(03)00079-4</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Aslan</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Zech</surname> <given-names>G</given-names></name>. <article-title>Statistical Energy as a Tool for Binning-free, Multivariate Goodness-of-Fit Tests, Two-sample Comparison and Unfolding</article-title>. <source>Nuclear Instruments and Methods in Physics Research A</source>. <year>2005</year>;<volume>537</volume>(<issue>3</issue>):<fpage>626</fpage>–<lpage>636</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.nima.2004.08.071" xlink:type="simple">10.1016/j.nima.2004.08.071</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref052">
<label>52</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Lopes</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Jacob</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Wainwright</surname> <given-names>MJ</given-names></name>. <chapter-title>A More Powerful Two-Sample Test in High Dimensions using Random Projection</chapter-title>. In: <name name-style="western"><surname>Shawe-Taylor</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Zemel</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Bartlett</surname> <given-names>PL</given-names></name>, <name name-style="western"><surname>Pereira</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Weinberger</surname> <given-names>KQ</given-names></name>, editors. <source>Advances in Neural Information Processing Systems</source> <volume>24</volume>. <publisher-name>Curran Associates, Inc.</publisher-name>; <year>2011</year>. p. <fpage>1206</fpage>–<lpage>1214</lpage>.</mixed-citation>
</ref>
<ref id="pone.0211044.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wei</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Wichers</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Marron</surname> <given-names>JS</given-names></name>. <article-title>Direction-Projection-Permutation for High-Dimensional Hypothesis Tests</article-title>. <source>Journal of Computational and Graphical Statistics</source>. <year>2016</year>;<volume>25</volume>(<issue>2</issue>):<fpage>549</fpage>–<lpage>569</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/10618600.2015.1027773" xlink:type="simple">10.1080/10618600.2015.1027773</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref054">
<label>54</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Hastie</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tibshirani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Friedman</surname> <given-names>JH</given-names></name>. <chapter-title>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</chapter-title>. <edition>2nd ed</edition>. <source>Springer Series in Statistics</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2009</year>.</mixed-citation>
</ref>
<ref id="pone.0211044.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Alva</surname> <given-names>JAV</given-names></name>, <name name-style="western"><surname>Estrada</surname> <given-names>EG</given-names></name>. <article-title>A Generalization of Shapiro-Wilk’s Test for Multivariate Normality</article-title>. <source>Communications in Statistics—Theory and Methods</source>. <year>2009</year>;<volume>38</volume>(<issue>11</issue>):<fpage>1870</fpage>–<lpage>1883</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/03610920802474465" xlink:type="simple">10.1080/03610920802474465</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mecklin</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Mundfrom</surname> <given-names>DJ</given-names></name>. <article-title>An Appraisal and Bibliography of Tests for Multivariate Normality</article-title>. <source>International Statistical Review</source>. <year>2004</year>;<volume>72</volume>(<issue>1</issue>):<fpage>123</fpage>–<lpage>138</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1751-5823.2004.tb00228.x" xlink:type="simple">10.1111/j.1751-5823.2004.tb00228.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0211044.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Romeu</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Ozturk</surname> <given-names>A</given-names></name>. <article-title>A Comparative Study of Goodness-of-Fit Tests for Multivariate Normality</article-title>. <source>Journal of Multivariate Analysis</source>. <year>1993</year>;<volume>46</volume>(<issue>2</issue>):<fpage>309</fpage>–<lpage>334</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1006/jmva.1993.1063" xlink:type="simple">10.1006/jmva.1993.1063</ext-link></comment></mixed-citation>
</ref>
</ref-list>
</back>
</article>