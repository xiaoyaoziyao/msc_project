<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN"><front><journal-meta><journal-id journal-id-type="publisher">pcbi</journal-id><journal-id journal-id-type="allenpress-id">plcb</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">07-PLCB-RA-0479R3</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1000007</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Computational Biology/Computational Neuroscience</subject><subject>Neuroscience/Cognitive Neuroscience</subject><subject>Neuroscience/Theoretical Neuroscience</subject></subj-group></article-categories><title-group><article-title>Robustness of Learning That Is Based on Covariance-Driven Synaptic Plasticity</article-title><alt-title alt-title-type="running-head">Robustness of Covariance-Based Plasticity</alt-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Loewenstein</surname><given-names>Yonatan</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib></contrib-group><aff id="aff1">          <addr-line>Departments of Neurobiology and Cognitive Sciences and Interdisciplinary Center for Neural Computation, Hebrew University, Jerusalem, Israel</addr-line>       </aff><contrib-group><contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>Karl J.</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">University College London, United Kingdom</aff><author-notes><corresp id="cor1">* E-mail: <email xlink:type="simple">yonatan@huji.ac.il</email></corresp><fn fn-type="con"><p>Conceived and designed the experiments: YL. Performed the experiments: YL. Analyzed the data: YL. Wrote the paper: YL.</p></fn><fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>3</month><year>2008</year></pub-date><pub-date pub-type="epub"><day>7</day><month>3</month><year>2008</year></pub-date><volume>4</volume><issue>3</issue><elocation-id>e1000007</elocation-id><history><date date-type="received"><day>13</day><month>8</month><year>2007</year></date><date date-type="accepted"><day>21</day><month>1</month><year>2008</year></date></history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2008</copyright-year><copyright-holder>Loewenstein</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract><p>It is widely believed that learning is due, at least in part, to long-lasting modifications of the strengths of synapses in the brain. Theoretical studies have shown that a family of synaptic plasticity rules, in which synaptic changes are driven by covariance, is particularly useful for many forms of learning, including associative memory, gradient estimation, and operant conditioning. Covariance-based plasticity is inherently sensitive. Even a slight mistuning of the parameters of a covariance-based plasticity rule is likely to result in substantial changes in synaptic efficacies. Therefore, the biological relevance of covariance-based plasticity models is questionable. Here, we study the effects of mistuning parameters of the plasticity rule in a decision making model in which synaptic plasticity is driven by the covariance of reward and neural activity. An exact covariance plasticity rule yields Herrnstein's matching law. We show that although the effect of slight mistuning of the plasticity rule on the synaptic efficacies is large, the behavioral effect is small. Thus, matching behavior is robust to mistuning of the parameters of the covariance-based plasticity rule. Furthermore, the mistuned covariance rule results in undermatching, which is consistent with experimentally observed behavior. These results substantiate the hypothesis that approximate covariance-based synaptic plasticity underlies operant conditioning. However, we show that the mistuning of the mean subtraction makes behavior sensitive to the mistuning of the properties of the decision making network. Thus, there is a tradeoff between the robustness of matching behavior to changes in the plasticity rule and its robustness to changes in the properties of the decision making network.</p></abstract><abstract abstract-type="summary"><title>Author Summary</title><p>It is widely believed that learning is due, at least in part, to modifications of synapses in the brain. The ability of a synapse to change its strength is called “synaptic plasticity,” and the rules governing these changes are a subject of intense research. Theoretical studies have shown that a particular family of synaptic plasticity rules, known as covariance rules, could underlie many forms of learning. While it is possible that a biological synapse would be able to approximately implement such abstract rules, it seems unlikely that this implementation would be exact. Covariance rules are inherently sensitive, and even a slight inaccuracy in their implementation is likely to result in substantial changes in synaptic strengths. Thus, the biological relevance of these rules remains questionable. Here we study the consequences of the mistuning of a covariance plasticity rule in the context of operant conditioning. In a previous study, we showed that an approximate phenomenological law of behavior called “the matching law” naturally emerges if synapses change according to the covariance rule. Here we show that although the effect of slight mistuning of the covariance rule on synaptic strengths is substantial, it leads to only small deviations from the matching law. Furthermore, these deviations are observed experimentally. Thus, our results support the hypothesis that covariance synaptic plasticity underlies operant conditioning.</p></abstract><funding-group><funding-statement>This research was supported by a grant from the Ministry of Science, Culture &amp; Sport, Israel, and the Ministry of Research, France. The funding agencies had no role in the design and conduct of the study, in the collection, analysis, and interpretation of the data, in the preparation, review, or approval of the manuscript.</funding-statement></funding-group><counts><page-count count="10"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>Synaptic plasticity that is driven by covariance is the basis of numerous models in computational neuroscience. It is the cornerstone of models of associative memory <xref ref-type="bibr" rid="pcbi.1000007-Hopfield1">[1]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Amit1">[2]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Tsodyks1">[3]</xref>, is used in models of gradient estimation in reinforcement learning <xref ref-type="bibr" rid="pcbi.1000007-Seung1">[4]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Fiete1">[5]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Baras1">[6]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Williams1">[7]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Toyoizumi1">[8]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Sakai1">[9]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Bohte1">[10]</xref> and has been suggested to be the basis of operant conditioning <xref ref-type="bibr" rid="pcbi.1000007-Loewenstein1">[11]</xref>. In statistics, the covariance between two random variables is the mean value of their product, provided that one or both have a zero mean. Accordingly, covariance-based plasticity arises when synaptic changes are driven by the <italic>product</italic> of two stochastic variables, provided that the mean of one or both of these variables is subtracted such that they are measured relative to their mean value.</p><p>In order for a synapse to implement covariance-based plasticity, it must estimate and subtract the mean of a stochastic variable. In many neural systems, signals are subjected to high-pass filtering, in which the mean or “DC component” is attenuated relative to phasic signals <xref ref-type="bibr" rid="pcbi.1000007-Shapley1">[12]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Connors1">[13]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Ahmed1">[14]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Nagel1">[15]</xref>. However, it is rare for the mean to be removed completely <xref ref-type="bibr" rid="pcbi.1000007-Alon1">[16]</xref>. Therefore, while it is plausible that a biological synapse would be able to approximately subtract the mean, it seems unlikely that this mean subtraction will be complete. If mean subtraction is incomplete, the synapse is expected to potentiate constantly. Over time, this potentiation could accumulate and drive the synapse to saturation values that differ considerably from those predicted by the ideal covariance rule (see below). Thus, even if neurobiological systems actually implement approximate covariance-based plasticity, the relevance of the idealized covariance models to the actual behavior is not clear.</p><p>Here, we study the effect of incomplete mean subtraction in a model of operant conditioning, which is based on synaptic plasticity that is driven by the covariance of reward and neural activity. In operant conditioning, the outcome of a behavior changes the likelihood of the behavior to reoccur. The more a behavior is rewarded, the more it is likely to be repeated in the future. A quantitative description of this process of adaptation is obtained in experiments where a subject repeatedly chooses between two alternative options and is rewarded according to his choices. Choice preference is quantified using the ‘fractional choice’ <italic>p<sub>i</sub></italic>, the number of trials in which alternative <italic>i</italic> was chosen divided by the total number of trials. The distribution of rewards delivered to the subject is quantified using the ‘fractional income’ <italic>r<sub>i</sub></italic>, the accumulated rewards harvested from that alternative, divided by the accumulated rewards from all alternatives. In many such experiments, choice behavior can phenomenologically be described by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e001" xlink:type="simple"/><label>(1)</label></disp-formula>where <italic>i</italic> = 1,2 corresponds to the two alternatives, <italic>Dp<sub>i</sub></italic>≡<italic>p<sub>i</sub></italic>−0.5 and <italic>Dr<sub>i</sub></italic>≡<italic>r<sub>i</sub></italic>−0.5. The proportionality constant, <italic>k</italic> corresponds to the susceptibility of choice behavior to the fractional income and its exact value has been a subject of intense debate over the last several decades. According to the ‘matching law’ <italic>k</italic> = 1 and thus <italic>p<sub>i</sub></italic> = <italic>r<sub>i</sub></italic>. In this case it can be shown that choices are allocated such that the average reward per choosing an alternative <italic>i</italic>, is equal for all alternatives <xref ref-type="bibr" rid="pcbi.1000007-Herrnstein1">[17]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Herrnstein2">[18]</xref> (see also <xref ref-type="sec" rid="s4">Materials and Methods</xref>). However, in many experiments the value of <italic>k</italic> is, in fact, slightly smaller than 1, a behavior that is commonly referred to as undermatching <xref ref-type="bibr" rid="pcbi.1000007-Davison1">[19]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Sugrue1">[20]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Lau1">[21]</xref>. An alternative phenomenological description of behavior, known as ‘the generalized matching law’ <xref ref-type="bibr" rid="pcbi.1000007-Davison1">[19]</xref> is <italic>p</italic><sub>1</sub>/<italic>p</italic><sub>2</sub> = (<italic>r</italic><sub>1</sub>/<italic>r</italic><sub>2</sub>)<italic><sup>k</sup></italic>. Expanding the generalized matching law around <italic>r<sub>i</sub></italic> = 0.5 yields Eq. (1) and thus Eq. (1) is an approximation of the generalized matching law. This approximation becomes equality for <italic>k</italic> = 1.</p><p>In a recent study we showed that the matching law is a natural consequence of synaptic plasticity that is driven by the covariance of reward and neural activity <xref ref-type="bibr" rid="pcbi.1000007-Loewenstein1">[11]</xref>. The goal of this paper is to understand the behavioral consequences of deviations from idealized covariance-based plasticity by investigating the behavioral consequences of incomplete subtraction of the mean in the plasticity rule. By studying an analytically solvable neural decision making model, we show that although the effect of small deviations from the idealized covariance-based plasticity on synaptic efficacies is large, the behavioral effect is small. Thus we demonstrate that matching behavior is robust to the mistuning of the parameters of the covariance-based plasticity rule. Furthermore, we show that the mistuning of the mean subtraction leads to undermatching, in line with experimental observations. Our study also reveals that the mistuning of the mean subtraction in the plasticity rule makes matching behavior sensitive to mistuning of the properties of the decision making network. Thus there is a tradeoff between robustness of matching behavior to changes in the plasticity rule and robustness to changes in the properties in the decision making network.</p></sec><sec id="s2"><title>Results</title><sec id="s2a"><title>The Decision-Making Model</title><p>Decision making is commonly studied in experiments in which a subject repeatedly chooses between two alternative actions, each corresponding to a sensory cue. For example, in many primate experiments, the stimuli are two visual targets, and the actions are saccadic eye movements to the targets <xref ref-type="bibr" rid="pcbi.1000007-Sugrue1">[20]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Lau1">[21]</xref>. In our model, the responses to the sensory stimuli are represented by two populations of sensory neurons, whose level of activity is denoted by <italic>N</italic><sub>1</sub> and <italic>N</italic><sub>2</sub> (<xref ref-type="fig" rid="pcbi-1000007-g001">Fig. 1A</xref>). We assume that the two activities <italic>N<sub>i</sub></italic> are independently drawn from the same Gaussian distribution with a positive mean and a coefficient of variation σ (standard deviation divided by the mean). We further assume that the level of variability in the activity of <italic>N<sub>i</sub></italic> is low, σ≪1. This assumption is reasonable if <italic>N<sub>i</sub></italic> corresponds to the average activity of a large population of uncorrelated neurons. Input from these sensory neurons determines the activities of two populations of premotor neurons via <italic>M<sub>i</sub></italic> = <italic>W<sub>i</sub>·N<sub>i</sub></italic> where <italic>W<sub>i</sub></italic> corresponds to the synaptic efficacy of the sensory-to-premotor synapses. Competition between the two premotor populations determines whether the model will choose alternative 1 or 2 in a trial. Unless otherwise noted, alternative 1 is chosen in trials in which <italic>M</italic><sub>1</sub>&gt;<italic>M</italic><sub>2</sub>. Otherwise alternative 2 is chosen. This process of competition between the two premotor populations can be achieved by a winner-take-all network with lateral inhibition <xref ref-type="bibr" rid="pcbi.1000007-Arbib1">[22]</xref>, which is not explicitly modeled here. Thus, the larger the value of a synapse <italic>W<sub>i</sub></italic> is, the more likely it is that alternative <italic>i</italic> will be chosen.</p><fig id="pcbi-1000007-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000007.g001</object-id><label>Figure 1</label><caption><title>The model.</title><p>(A) The decision making network consists of two populations of sensory neurons <italic>N<sub>i</sub></italic>, corresponding to the two targets, and two populations of premotor neurons <italic>M<sub>i</sub></italic>, corresponding to the two actions. Choice is determined by comparing the activities of the two populations of premotor neurons (see text). (B) The effect of the synaptic plasticity rule on synaptic efficacy. The decision making model was simulated in a concurrent VI reward schedule (see <xref ref-type="sec" rid="s4">Materials and Methods</xref>) with equal baiting probabilities, and the efficacy of one of the synapses is plotted as a function of trial number. During the first 300 trials (blue), the synaptic efficacies evolved according to Eq. (2) with α = 0 and β = 1 (and thus γ = 0), resulting in small fluctuations of the efficacy around the initial conditions. A 10% mistuning of the mean subtraction after 300 trials (red arrow) to β = 0.9 (γ = 0.1) resulted in a linear divergence of the efficacy (red line). The addition of a linear decay term to the plasticity rule (Eq. (4) with ρ = 1) after 600 trials (black arrow) resulted in small fluctuations of the efficacy around 0.04 (black line).</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.g001" xlink:type="simple"/></fig></sec><sec id="s2b"><title>Synaptic Plasticity</title><p>Consider the following plasticity rule, in which the change Δ<italic>W<sub>i</sub></italic> in synaptic efficacy <italic>W<sub>i</sub></italic> in a trial is described by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e002" xlink:type="simple"/><label>(2)</label></disp-formula>where η is the plasticity rate, <italic>R</italic> is the reward harvested in the trial, <bold>E</bold>[<italic>R</italic>] is the average of the previously harvested reward, <italic>N<sub>i</sub></italic> is the activity of sensory population <italic>i</italic> in the trial, and <bold>E</bold>[<italic>N</italic>] is the average activity of the sensory population. The index <italic>i</italic> is omitted from the latter average because we assume that the activity of the two populations is drawn from the same distribution; α, β are parameters. This plasticity rule corresponds to reward-modulated presynaptic activity-dependent plasticity <xref ref-type="bibr" rid="pcbi.1000007-Golding1">[23]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Humeau1">[24]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Bailey1">[25]</xref>. If α = 1 and/or β = 1 then Eq. (2) describes a covariance-based synaptic plasticity rule because synaptic changes are driven by the product of two stochastic variables (<italic>N<sub>i</sub></italic> and <italic>R</italic>) where the mean of one or both of these variables is subtracted. In order to gain insights into the behavior of Eq. (2), we consider the <italic>average trajectory approximation</italic>, also known as <italic>mean synaptic dynamics</italic> <xref ref-type="bibr" rid="pcbi.1000007-Kempter1">[26]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Gutig1">[27]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Dayan1">[28]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Heskes1">[29]</xref>, which is the dynamics of the expectation value of the right hand side of Eq. (2). If the plasticity rate η is sufficiently small, the noise accumulated over an appreciable number of trials is small relative to the mean change in the synaptic efficacies, called the synaptic drift <xref ref-type="bibr" rid="pcbi.1000007-Kempter1">[26]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Gutig1">[27]</xref> and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e003" xlink:type="simple"/><label>(3)</label></disp-formula>where we define a mistuning parameter γ = (1−α)·(1−β). γ = 0 corresponds to the idealized covariance rule. Incomplete mean subtraction corresponds to γ&gt;0. Our analysis focuses on choice behavior when mean subtraction is incomplete (γ&gt;0). Similar results are obtained when mean subtraction is overcomplete (γ&lt;0; see <xref ref-type="sec" rid="s4">Materials and Methods</xref>). In principle, even a small mistuning of the mean subtraction may have a substantial effect on choice behavior for the following reason: Consider the dynamics of Eq. (3) for the simple case in which reward <italic>R</italic> and neural activity <italic>N<sub>i</sub></italic> are independent. This corresponds to a case where the neural activity <italic>N<sub>i</sub></italic> does not participate in the decision making process or to the case where reward is independent of choice. In both cases, Cov[<italic>R, N<sub>i</sub></italic>] = 0 and therefore Eq. (3) becomes Δ<italic>W<sub>i</sub></italic>≈η·γ·E[<italic>R</italic>]·E[<italic>N</italic>]. If E[<italic>R</italic>]·E[<italic>N</italic>]&gt;0, the synaptic efficacy <italic>W<sub>i</sub></italic> is expected to grow indefinitely. The divergence of the synaptic efficacies is also expected in the more general case in which the reward and neural activities are not independent. This is illustrated in <xref ref-type="fig" rid="pcbi-1000007-g001">Fig. 1B</xref>, where we simulated the plasticity rule of Eq. (2) in a concurrent variable-interval schedule (VI; see <xref ref-type="sec" rid="s4">Materials and Methods</xref>) and plotted the efficacy of one of the synapses as a function of the trial number. When the covariance rule is finely tuned such that γ = 0 (here we assumed that α = 0, β = 1), the synaptic efficacy, after a transient period (not shown), is approximately constant (blue line). After 300 trials (red, down-facing arrow), the mean subtraction in the plasticity rule was mistuned by 10% such that γ = 0.9 (α = 0, β = 0.9), resulting in the linear divergence of the synaptic efficacy (red line).</p><p>In practice, synaptic efficacies are bounded and such divergence is prevented by synaptic saturation. We model the synaptic saturation by adding a polynomial decay term to the synaptic plasticity rule such that Eq. (2) becomes<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e004" xlink:type="simple"/><label>(4)</label></disp-formula>where ρ&gt;0 is the saturation stiffness parameter. The effect of the decay term on the dynamics of the synaptic efficacy is illustrated in <xref ref-type="fig" rid="pcbi-1000007-g001">Fig. 1B</xref>. After 600 trials (black, left-facing arrow), the plasticity rule of Eq. (2) was replaced with the plasticity rule in Eq. (4) with ρ = 1, resulting in a convergence of the synaptic efficacy to a value that is significantly different from the result of the pure covariance rule (black line).</p><p>The synaptic saturation is modeled here using a saturation stiffness parameter, ρ. When ρ = 1, as in <xref ref-type="fig" rid="pcbi-1000007-g001">Fig. 1B (black line)</xref>, synaptic efficacies decay linearly. The larger the value of ρ, the stiffer the bound. In the limit of ρ→∞, as long as <italic>W<sub>i</sub></italic>&lt;<italic>W<sub>bound</sub></italic> Eq. (4) is equivalent to Eq. (2), but the saturation term prevents <italic>W<sub>i</sub></italic> from exceeding the value <italic>W<sub>bound</sub></italic>.</p></sec><sec id="s2c"><title>Incomplete Mean Subtraction</title><p>The dynamics of Eq. (4) are stochastic and therefore difficult to analyze. If the plasticity rate η is small then many trials with different realizations of choices and rewards are needed in order to make a substantial change in the value of the synaptic efficacies. Therefore intuitively, the stochastic dynamics of Eq. (4) can be viewed as an average deterministic trajectory, with stochastic fluctuations around it, where we expect that this average deterministic dynamics becomes a better approximation to the stochastic dynamics as the plasticity rate η becomes smaller. The conditions under which this intuitive picture is valid are discussed in <xref ref-type="bibr" rid="pcbi.1000007-Heskes1">[29]</xref>. The fixed point of the average trajectory of Eq. (4) is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e005" xlink:type="simple"/><label>(5)</label></disp-formula>and we study choice behavior when synaptic efficacies are given by Eq. (5). Assuming that <italic>p</italic><sub>1</sub>, <italic>p</italic><sub>2</sub>≠0, and γ&gt;0, we show (<xref ref-type="sec" rid="s4">Materials and Methods</xref>) that in the limit of low noise σ≪1, the model undermatches <xref ref-type="bibr" rid="pcbi.1000007-Davison1">[19]</xref>; that is, when <italic>p<sub>i</sub></italic>&lt;0.5 then <italic>p<sub>i</sub></italic>&gt;<italic>r<sub>i</sub></italic> whereas when <italic>p<sub>i</sub></italic>&gt;0.5 then <italic>p<sub>i</sub></italic>&lt;<italic>r<sub>i</sub></italic>. Furthermore, the level of deviation from matching scales with the product of the mistuning and synaptic saturation parameters,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e006" xlink:type="simple"/><label>(6)</label></disp-formula>Finally, expansion of Eq. (6) around <italic>Dp<sub>i</sub></italic> = 0 yields Eq. (1) with<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e007" xlink:type="simple"/><label>(7)</label></disp-formula>Importantly, we show that overcomplete mean subtraction γ&lt;0 also leads to undermatching with the same scaling of the deviations from matching with the mistuning and synaptic saturation parameters (<xref ref-type="sec" rid="s4">Materials and Methods</xref>).</p><p>Consider Eq. (7). When γρ = 0, <italic>k</italic> = 1 and the fractional choice is equal to the fractional income yielding matching behavior. Note that when the mistuning of mean subtraction is small, γ≪1, the deviation of the susceptibility index <italic>k</italic> from 1 is small. This occurs despite the fact that such mistuning has, in general, a substantial effect on the values of the synaptic efficacies (<xref ref-type="fig" rid="pcbi-1000007-g001">Fig. 1B</xref>). Thus, matching behavior is robust to the mistuning of the mean subtraction, even though the synaptic efficacies are not.</p><sec id="s2c1"><title>The role of γ</title><p>For insights into the dependence of the susceptibility on γ, it is useful to consider the differential contributions of the covariance term, and the bias and saturation terms in Eq. (5). The smaller the value of γ, the larger the contribution of the covariance term, making it more similar to the idealized covariance-based plasticity rule that yields <italic>k</italic> = 1 <xref ref-type="bibr" rid="pcbi.1000007-Loewenstein1">[11]</xref>. In contrast, when the value of γ is large, the contribution of the covariance term is small and the efficacies of the two synapses, <italic>W</italic><sub>1</sub> and <italic>W</italic><sub>2</sub> become similar independently of the fractional income. In the limit of γ→∞, the efficacies of the two synapses become equal and the alternatives are chosen with equal probability. Thus, the larger the value of γ in Eq. (7), the smaller the susceptibility of behavior.</p></sec><sec id="s2c2"><title>The role of ρ</title><p>Consider the case of an infinitely hard bound, ρ→∞ in Eq. (4). As long as <italic>W</italic><sup>*</sup>&lt;<italic>W<sub>bound</sub></italic>, (<italic>W</italic><sup>*</sup>/<italic>W<sub>bound</sub></italic>)<sup>ρ</sup> = 0. Because of the incomplete mean subtraction, the two synapses are expected to grow continuously until they reach <italic>W<sub>bound</sub></italic>. For <italic>W</italic><sup>*</sup>&gt;<italic>W<sub>bound</sub></italic>, (<italic>W</italic><sup>*</sup>/<italic>W<sub>bound</sub></italic>)<sup>ρ</sup>→∞. Thus both synaptic efficacies are expected to become equal to the synaptic bound <italic>W<sub>bound</sub></italic>. In this case there is equal probability of choosing either alternative, independently of the fractional income, yielding <italic>k</italic> = 0. In contrast, a soft bound enables the saturation term to balance the bias term without occluding the covariance term. Thus, the smaller the value of ρ, the larger the contribution of the covariance term in the synaptic plasticity rule and the smaller the deviation from matching behavior.</p></sec><sec id="s2c3"><title>The role of σ</title><p>In the limit of low noise in the activity of the sensory neurons σ≪1, choice behavior is independent of the value of σ. For insight into this independence we consider the dual role of trial-to-trial fluctuations in the neural activity of the sensory neurons in our model. Information about past incomes is stored in the synaptic efficacies such that the stronger synapse corresponds to the alternative that yielded a higher income in the past, biasing choice toward that alternative. For this reason we denote the difference in synaptic efficacies as ‘signal’. The trial-to-trial fluctuations in the neural activity of the sensory neurons underlie the stochasticity of choice. In the absence of such fluctuations, the synaptic efficacies determine choice such that the chosen alternative is the one that corresponds to the larger synaptic efficacy. The larger these fluctuations are the more random choice is. We refer to this effect as ‘noise’. However, these fluctuations also play a pivotal role in the learning process. Changes in synaptic efficacy are driven by the covariance of the reward and the neural activity of the sensory neurons. The larger the fluctuations in the activity of these neurons, the larger the covariance and therefore the larger the learning signal, increasing the difference between the synaptic efficacies that correspond to the “rich” and “poor” alternatives. Thus, an increase in the stochasticity in the activities of the sensory neurons increases both the signal and the noise. We show that when σ≪1, the ratio of the signal to noise is independent of σ (<xref ref-type="sec" rid="s4">Materials and Methods</xref>) and therefore the susceptibility of behavior <italic>k</italic> is independent of σ.</p></sec></sec><sec id="s2d"><title>Numerical Simulations</title><p>Eq. (7) is derived assuming that the stochastic dynamics, Eq. (4) has converged to the fixed point of the average trajectory, Eq. (5) and that σ≪1 (<xref ref-type="sec" rid="s4">Materials and Methods</xref>). In order to study the validity of this approximation, we numerically simulated the decision making model with σ = 0.1 and a stochastic synaptic plasticity rule, Eq. (4) in a concurrent VI reward schedule (<xref ref-type="sec" rid="s4">Materials and Methods</xref>). These simulations are presented in <xref ref-type="fig" rid="pcbi-1000007-g002">Fig. 2</xref>. Each symbol in <xref ref-type="fig" rid="pcbi-1000007-g002">Fig. 2A</xref> corresponds to one simulation in which the baiting probabilities of the two targets were kept fixed. The fraction of trials in which action 1 was chosen is plotted against the fractional income earned from action 1. As predicted by Eq. (7), the dependence of the fractional choice on the fractional income is linear, and susceptibility depends on the values of both γ and ρ (red squares, γ = 0.05, ρ = 1; blue diamonds, γ = 0.5, ρ = 1; gray triangles γ = 0.5, ρ = 4; colored lines are the analytical approximation, Eq. (7); the black line is the expected behavior according to the matching law). In order to better quantify the relation between the stochastic dynamics and the analytical approximation, we simulated Eq. (4) for different values of γ and ρ and measured the susceptibility of behavior. The results of these simulations appear in <xref ref-type="fig" rid="pcbi-1000007-g002">Fig. 2B</xref> (blue dots, ρ = 5; red dots, ρ = 1; black dots, ρ = 0.2) and show good fit with the expected behavior from Eq. (7) (lines).</p><fig id="pcbi-1000007-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000007.g002</object-id><label>Figure 2</label><caption><title>Incomplete mean subtraction and deviations from matching behavior.</title><p>(A) The probability of choice as a function of fractional income. Each point corresponds to one simulation of the model, Eq. (4), in a concurrent VI reward schedule with fixed baiting probabilities. The level of deviation from matching behavior (black line) depends on the level of incomplete mean subtraction, γ and synaptic saturation stiffness, ρ. Red squares, γ = 0.05, ρ = 1; blue diamonds, γ = 0.5, ρ = 1; gray triangles γ = 0.5, ρ = 4; colored lines are the analytical approximations, Eq. (7). (B) Susceptibility of behavior as a function of γ. In order to quantify the effect of γ on deviation from matching behavior, we repeated the simulations of A for many values of γ and measured the susceptibility of behavior (the slope of the resultant curve, see text and <xref ref-type="sec" rid="s4">Materials and Methods</xref>). Blue dots, ρ = 5; red dots, ρ = 1; black dots, ρ = 0.2. Lines correspond to the expected slope from the analytical approximation, Eq. (7).</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.g002" xlink:type="simple"/></fig></sec><sec id="s2e"><title>Mistuning of Network Parameters</title><p>In the previous section we analyzed the behavioral consequences of mistuning of the plasticity rule in a particular network model. The question of robustness is equally applicable to the parameters of the decision making network as it is to the parameters of the synaptic plasticity rule. Therefore, in this section we study the robustness of matching behavior to the mistuning of the parameters of the network.</p><p>There are various ways in which the decision making network can be mistuned. We chose to study the effect of a bias in the winner-take-all network, because this is a generic form of error that is likely to significantly affect choice behavior. It is plausible that a winner-take-all network will be able to choose the alternative that corresponds to the larger activity of the two premotor populations in trials in which <italic>M</italic><sub>1</sub> and <italic>M</italic><sub>2</sub> are very different. However, if <italic>M</italic><sub>1</sub> and <italic>M</italic><sub>2</sub> are similar in their level of activity it is likely that a biological implementation of a winner-take-all mechanism, which is not finely tuned, will be biased to favoring one of the alternatives. Formally we assume that alternative 1 is chosen in trials in which (<italic>M</italic><sub>1</sub>−<italic>M</italic><sub>2</sub>)/(<italic>M</italic><sub>1</sub>+<italic>M</italic><sub>2</sub>)&gt;ε where ε is a bias. The unbiased case studied in the previous section corresponds to ε = 0. In contrast, ε&gt;1 or ε&lt;–1 correspond to a strong bias such that choice is independent of the values of <italic>M</italic><sub>1</sub> and <italic>M</italic><sub>2</sub>. With the same assumptions as in the derivation of Eq. (7), <italic>p</italic><sub>1</sub>, <italic>p</italic><sub>2</sub>≠0 and σ≪1, we show (<xref ref-type="sec" rid="s4">Materials and Methods</xref>) that a bias in the winner-take-all mechanism results in a bias in choice that is <italic>O</italic>(ργ·ε/σ). Furthermore, analyzing choice behavior for small value of |<italic>Dp<sub>i</sub></italic>| yields<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e008" xlink:type="simple"/><label>(8)</label></disp-formula>where <italic>k</italic> is given by Eq. (7) and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e009" xlink:type="simple"/><label>(9)</label></disp-formula>is the offset. The offset <italic>b</italic><sub>1</sub> is proportional to the deviation of the susceptibility of behavior from unity, 1−<italic>k</italic>. As discussed in the previous section, this deviation depends on the level of incomplete mean subtraction as well as the synaptic saturation term (Eq. (7). If γ = 0 then <italic>k</italic> = 1 and the offset term vanishes, <italic>b</italic><sub>1</sub> = 0 for any value of bias ε. This robustness of matching behavior to bias in the winner-take-all network is due to the fact that the idealized covariance based plasticity rule can compensate for the bias in the decision making network in almost any neural architecture <xref ref-type="bibr" rid="pcbi.1000007-Loewenstein1">[11]</xref>. In contrast, if γ&gt;0 then the offset <italic>b</italic><sub>1</sub> is proportional to the bias ε. The larger the deviation of the plasticity rule from the idealized covariance rule, the larger the proportionality constant. Thus, there is a tradeoff between the robustness of matching behavior to changes in the plasticity rule and robustness to changes in the parameters of decision making. The larger the mistuning of the plasticity rule, the smaller the robustness of matching behavior to mistuning of the parameters of the decision making network. Importantly, the level of noise in the sensory populations strongly affects the bias in behavior through ε/σ. This contrasts with the independence of the susceptibility parameter <italic>k</italic> of σ. To understand the reason for this result it is useful to note that as discussed in the previous section, the magnitude of trial to trial fluctuations in the activity of the sensory neurons determines the magnitude of the fractional income signal stored in the synaptic efficacies (the difference in the two synaptic efficacies). The smaller the value of σ is, the weaker the fractional income signal and therefore the stronger the relative contribution of the bias in the winner-take-all network to choice. If <italic>N<sub>i</sub></italic> corresponds to the average activity of a large population of uncorrelated neurons, σ is expected to be small and therefore the effect of even small bias in the winner-take-all network on behavior is expected to be large.</p></sec><sec id="s2f"><title>Numerical Simulations</title><p>To study the validity of Eq. (8) numerically, we simulated the synaptic plasticity rule of Eq. (4) in the decision making model of <xref ref-type="fig" rid="pcbi-1000007-g001">Fig. 1A</xref> with a bias ε in the winner-take-all network. Similar to <xref ref-type="fig" rid="pcbi-1000007-g002">Fig. 2A</xref>, <xref ref-type="fig" rid="pcbi-1000007-g003">Fig. 3A</xref> depicts the fraction of trials in which alternative 1 was chosen, which is plotted against the fractional income earned from that alternative. The level of deviation from matching behavior (solid black line) depends on the value of ε (red squares, ε = −3σ; blue diamonds, ε = 0; gray triangle, ε = 3σ; γ = 0.05, ρ = 1). Colored lines are the analytical approximation, Eq. (8). In order to better quantify the relation between the stochastic dynamics and its deterministic approximation, we numerically computed the value of <italic>p</italic><sub>1</sub> that corresponds to δ<italic>r</italic><sub>1</sub> = 0 for different values of ε and γ (<xref ref-type="fig" rid="pcbi-1000007-g003">Fig. 3B</xref>; red, γ = 0.05; blue, γ = 0.5). The results are in line with the expected behavior from Eq. (8) (solid lines).</p><fig id="pcbi-1000007-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1000007.g003</object-id><label>Figure 3</label><caption><title>Bias in the winner-take-all mechanism and deviations from matching behavior.</title><p>(A) The probability of choice as a function of fractional income. Each point corresponds to one simulation of the model (Eq. (4) with ρ = 1) in a concurrent VI reward schedule with fixed baiting probabilities. The level of deviation from matching behavior (black line) depends on the bias in the winner-take-all mechanism. Red squares, ε = −3σ; blue diamonds, ε = 0; gray triangle, ε = 3σ; γ = 0.05; colored lines are the analytical approximation, Eq. (8). (B) Choice bias. The simulation of A was repeated for different values of ε for two values of γ (blue dots, γ = 0.5; red dots, γ = 0.05), and the probability of choosing alternative 1 for a fractional income of <italic>r</italic><sub>1</sub> = 0.5 was measured. Lines correspond to the expected probability of choice from the analytical approximation, Eq. (8).</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.g003" xlink:type="simple"/></fig></sec></sec><sec id="s3"><title>Discussion</title><p>In this study we explored the robustness of matching behavior to inaccurate mean subtraction in a covariance-based plasticity rule. We have shown that (1) although this deviation from the idealized covariance rule has a substantial effect on the synaptic efficacies, its behavioral effect is small. (2) The direction of the behavioral effect of incomplete mean subtraction is towards the experimentally observed undermatching. (3) When the plasticity rule is mistuned, matching behavior becomes sensitive to the properties of the network architecture. Thus, there is a tradeoff between the robustness of matching behavior to changes in the plasticity rule and robustness to changes in the parameters of the decision making network.</p><sec id="s3a"><title>Robustness of Covariance-Based Plasticity</title><p>Covariance-based, Hebbian synaptic plasticity dominates models of associative memory. According to the popular Hopfield model, the change in the synaptic efficacy between pairs of neurons is proportional to the product of their activities in the training session, measured relative to their average activity <xref ref-type="bibr" rid="pcbi.1000007-Hopfield1">[1]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Amit1">[2]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Tsodyks1">[3]</xref>. If the mean subtraction is not finely tuned in this model, the synaptic efficacies diverge with the number of patterns stored. If this divergence is avoided by adding a saturation term to the plasticity rule, the capacity of the network to store a large number of memory patterns is lost <xref ref-type="bibr" rid="pcbi.1000007-Amit1">[2]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Fusi1">[30]</xref>. Thus, fine tuning of the mean subtraction in the plasticity rule is crucial for covariance-based associative memory models. This contrasts with the robustness of matching behavior to the mistuning of the mean subtraction demonstrated here. The difference in robustness stems from the difference in the solution space of the two tasks. Consider a general decision making network model consisting of <italic>n</italic> synapses. If <italic>n</italic>&gt;1 the decision making model is expected to be redundant. There are many possible combinations of synaptic efficacies that yield the same probability of choice and thus are behaviorally indistinguishable. The dimension of the hyperspace of synaptic efficacies that corresponds to a single probability of choice is, in general, <italic>n</italic>−1. Consider now the hyperspace of synaptic efficacies that corresponds to the matching solution <italic>p</italic><sub>1</sub> = <italic>r</italic><sub>1</sub>. Any set of synaptic efficacies that resides within this hyperspace is a fixed point of the family of synaptic plasticity rules that is driven by the covariance of reward and neural activity (in the average trajectory approximation) <xref ref-type="bibr" rid="pcbi.1000007-Loewenstein1">[11]</xref>. In contrast to this manifold of solutions, the approximate covariance plasticity rule with saturation is expected to have a single fixed point. In order for this fixed point to correspond to an approximate matching solution, it should reside near the matching hyperspace. The distance of the fixed point solution from the matching hyperspace depends on the decision making model and the level of mistuning of the covariance plasticity rule. However, because of the high dimensionality of the matching solution, there is a large family of decision making models in which the solution to the approximate covariance plasticity rule resides near the matching hyperspace for that model, for example, the model analyzed here with ε = 0. In contrast, in associative memory models, the volume in the synaptic efficacies hyperspace that can retrieve a large number of particular memories is small <xref ref-type="bibr" rid="pcbi.1000007-Gardner1">[31]</xref> and therefore even small deviations from the covariance plasticity rule will lead to a solution that is far from the memory retrieving hyperspace, resulting in a large reduction in the performance of the network.</p><p>Several studies have reported stochastic gradient learning in a model in which changes in the synaptic efficacy are driven by the product of the reward with a measure of past activity known as the ‘eligibility trace’ <xref ref-type="bibr" rid="pcbi.1000007-Seung1">[4]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Fiete1">[5]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Baras1">[6]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Williams1">[7]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Toyoizumi1">[8]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Sakai1">[9]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Bohte1">[10]</xref>. The mean of the eligibility trace is zero and therefore synaptic plasticity in these models can be said to be driven by the covariance of reward and a measure of past activity. Violation of the zero mean condition is expected to produce a bias in the gradient estimation and could potentially hinder learning. The consequences of mistuning of the mean subtraction in the estimation of the eligibility trace have not been addressed. We predict that the relative volume in the model parameter hyperspace that corresponds to the maximum reward solution will be an important factor in determining whether these gradient learning models are robust or not to the mistuning of the mean subtraction.</p></sec><sec id="s3b"><title>Tradeoff between Sensitivity of Plasticity Rule and Network Architecture</title><p>The level of fine-tuning required for normal brain functioning is unknown and robustness represents a major open issue for many models of brain systems. For example, the fine-tuning of neural parameters involved in the short term memory of analog quantities such as eye position in the oculomotor neural integrator <xref ref-type="bibr" rid="pcbi.1000007-Seung2">[32]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Seung3">[33]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Loewenstein2">[34]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Koulakov1">[35]</xref> or the frequency of a somatosensory stimulation <xref ref-type="bibr" rid="pcbi.1000007-Brody1">[36]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Machens1">[37]</xref> have been studied extensively. It has been suggested that synaptic plasticity keeps the synaptic efficacies finely-tuned <xref ref-type="bibr" rid="pcbi.1000007-Arnold1">[38]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Turaga1">[39]</xref>. However, in those models it is assumed that the parameters of the plasticity rule are finely tuned. In this study we demonstrated a tradeoff between the robustness of behavior to changes in the parameters of the network architecture and the robustness to changes in the parameters of the plasticity rule. This tradeoff is likely to be a property of many models of brain function.</p></sec><sec id="s3c"><title>Deviations from Matching Behavior</title><p>Undermatching in our model is the outcome of inaccurate mean subtraction, whether it is incomplete or overcomplete. This result is expected to hold in other symmetrical decision making models: when the mean subtraction is inaccurate, synaptic efficacies are determined by a combination of a covariance term, and bias and saturation terms. The bias and saturation terms are not influenced by the correlation between the neural activity and the reward. Therefore they drive the synaptic efficacies to values that are independent of the fractional income. If the architecture of the decision making network is symmetrical with respect to the two alternatives (as is the case in our model for ε = 0), they will drive the synaptic efficacies in the direction of a symmetrical solution for which the two alternatives are chosen with equal probability, which corresponds to <italic>k</italic>  = 0. In contrast, the covariance term drives the efficacies to the matching solution, <italic>k</italic> = 1. The combined effect of the covariance term and a small bias and saturation terms is expected to be a behavior for which the susceptibility index <italic>k</italic> is slightly smaller than 1, in line with the experimentally observed slight undermatching. Importantly, the experimentally observed undermatching is consistent with approximate covariance-based synaptic plasticity but does not prove it. Undermatching is also consistent with other models that do not assume this particular synaptic plasticity rule (see below).</p></sec><sec id="s3d"><title>Experimental Predictions</title><p>We hypothesize that the observed matching behavior results from a synaptic plasticity rule that is driven by an approximation to the covariance of reward and neural activity. In this case, behavior adapts because synapses in the brain perform a statistical computation and ‘attempt’ to decorrelate the reward and the fluctuations in neural activity. However, a very different class of matching models has been proposed, in which the brain performs computations that are “financial.” According to these models, subjects keep track of financial quantities such as return or income from each alternative and make choices stochastically according to the difference or ratio of the financial quantities between the two alternatives leading to matching <xref ref-type="bibr" rid="pcbi.1000007-Sugrue1">[20]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Herrnstein3">[40]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Sakai2">[41]</xref>, or undermatching <xref ref-type="bibr" rid="pcbi.1000007-Soltani1">[42]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Corrado1">[43]</xref>. A common feature of these models is the implicit assumption that financial computations and probabilistic choice are implemented in two separate brain modules. One brain module records past reward and choices to calculate quantities such as income and return and the other brain module utilizes these quantities to generate stochastic choice. A covariance-based plasticity rule can be distinguished experimentally from the financial models by making the reward directly contingent on fluctuations in the stochastic neural activity. This could be done by measuring neural activity in a brain area involved in decision making, using microelectrodes or brain imaging, and making reward contingent on these measurements, as well as on actions. This sort of contingency has previously been employed by neurophysiologists, though not in the context of operant matching <xref ref-type="bibr" rid="pcbi.1000007-Fetz1">[44]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Taylor1">[45]</xref>. If, by the construction of the reward schedule, reward directly depends on fluctuations in neural activity, then it would be impossible to decorrelate the reward and the neural activity. According to our covariance hypothesis, the ‘attempt’ of the synaptic plasticity rule to do just this will lead to a change in the dependence of choice on the financial quantities (formally, this will lead to violation of Eq. (21) in <xref ref-type="sec" rid="s4">Materials and Methods</xref>). In contrast, in the financial models, neural fluctuations and learning are mediated through different modules and therefore this contingency will not alter the dependence of choice on financial quantities (see also <xref ref-type="bibr" rid="pcbi.1000007-Loewenstein1">[11]</xref>).</p></sec></sec><sec id="s4"><title>Materials and Methods</title><sec id="s4a"><title>Synaptic Efficacies and Choice Behavior</title><p>As was described above, the identity of choice in the network of <xref ref-type="fig" rid="pcbi-1000007-g001">Fig. 1</xref> is determined by a competition between two premotor neurons <italic>M<sub>i</sub></italic> = <italic>W<sub>i</sub>·N<sub>i</sub></italic>. In the Incomplete mean subtraction section we assume that alternative 1 is chosen in trials in which <italic>M</italic><sub>1</sub>&gt;<italic>M</italic><sub>2</sub>. Otherwise alternative 2 is chosen. Thus, the fraction of trials in which alternative 1 is chosen, or the probability that it is chosen is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e010" xlink:type="simple"/><label>(10)</label></disp-formula>where <italic>A</italic>∈{1,2} denotes the alternative chosen, or<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e011" xlink:type="simple"/><label>(11)</label></disp-formula>where <italic>Z<sub>d</sub></italic>≡(δ<italic>N</italic><sub>1</sub>−δ<italic>N</italic><sub>2</sub>)/(2·E[<italic>N</italic>]), <italic>Z<sub>s</sub></italic>≡(δ<italic>N</italic><sub>1</sub>+δ<italic>N</italic><sub>2</sub>)/(2·E[<italic>N</italic>]), δ<italic>N<sub>i</sub></italic> = <italic>N<sub>i</sub></italic>−E[<italic>N</italic>], <italic>T</italic>≡<italic>W<sub>d</sub></italic>/<italic>W<sub>s</sub></italic>, <italic>W<sub>s</sub></italic>≡(<italic>W</italic><sub>1</sub>+<italic>W</italic><sub>2</sub>)/2, <italic>W<sub>d</sub></italic>≡(<italic>W</italic><sub>1</sub>−<italic>W</italic><sub>2</sub>)/2. Because <italic>N</italic><sub>1</sub> and <italic>N</italic><sub>2</sub> are independent Gaussian variables with a coefficient of variation σ, <italic>Z<sub>d</sub></italic> and <italic>Z<sub>s</sub></italic> are two independent Gaussian variables with zero mean and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e012" xlink:type="simple"/></inline-formula> standard deviation. Therefore, <italic>Z<sub>d</sub></italic>+<italic>T</italic>·<italic>Z<sub>s</sub></italic> is a Gaussian variable with zero mean and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e013" xlink:type="simple"/></inline-formula> standard deviation and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e014" xlink:type="simple"/><label>(12)</label></disp-formula>Note that the assumption that <italic>p</italic><sub>1</sub>,<italic>p</italic><sub>2</sub>≠0 implies that in the limit of σ→0, <italic>T</italic> = <italic>O</italic>(σ).</p><p>Next we use Eq. (11) to compute two quantities that will become useful later:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e015" xlink:type="simple"/><label>(13)</label></disp-formula>and similarly<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e016" xlink:type="simple"/><label>(14)</label></disp-formula>Assuming that <italic>T</italic> = <italic>O</italic>(σ),<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e017" xlink:type="simple"/><label>(15)</label></disp-formula>and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e018" xlink:type="simple"/><label>(16)</label></disp-formula></p></sec><sec id="s4b"><title>Incomplete Mean Subtraction</title><p>In this section we compute the dependence of deviations from matching behavior on γ, assuming that synaptic efficacies are given by the fixed point of the average trajectory, Eq. (5). The precise conditions for the correctness of the approach are discussed in details in <xref ref-type="bibr" rid="pcbi.1000007-Heskes1">[29]</xref>. We further assume that synaptic saturation is linear, ρ = 1. The latter assumption is relaxed in the Incomplete mean subtraction and saturation stiffness section below.</p><p>According to Eq. (11), the probability of choice depends on the ratio of the synaptic efficacies; thus the scaling of the synaptic efficacies by a positive number does not change the probabilities of choice. For clarity we scale the synaptic efficacies of Eq. (5) (assuming ρ = 1) such that,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e019" xlink:type="simple"/><label>(17)</label></disp-formula>Rewriting Eq. (17) in terms of <italic>W<sub>d</sub></italic> and <italic>W<sub>s</sub></italic> yields<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e020" xlink:type="simple"/><label>(18)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e021" xlink:type="simple"/><label>(19)</label></disp-formula>where the asterisk corresponds to the value at the fixed point. Next we separate the covariance terms into trials in which alternative 1 was chosen and trials in which alternative 2 was chosen<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e022" xlink:type="simple"/><label>(20)</label></disp-formula>The reward <italic>R</italic> is a function of the actions <italic>A</italic> and the actions are a function of the neural activities <italic>Z<sub>s</sub></italic> and <italic>Z<sub>d</sub></italic>. Therefore, given the action, the reward and the neural activities are statistically independent and the average of the product of reward and neural activity is equal to the product of the averages, E[<italic>R</italic>/E[<italic>R</italic>]·<italic>Z<sub>x</sub></italic>|<italic>A</italic> = <italic>i</italic>] = E[<italic>R</italic>/E[<italic>R</italic>]|<italic>A</italic> = <italic>i</italic>]·E[<italic>Z<sub>x</sub></italic>|<italic>A</italic> = <italic>i</italic>]. Hence, Eq. (20) becomes<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e023" xlink:type="simple"/><label>(21)</label></disp-formula>Next we separate E[<italic>Z<sub>x</sub></italic>] to trials in which alternative 1 was chosen and trials in which alternative 2 was chosen and use the fact that E[<italic>Z<sub>x</sub></italic>] = 0<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e024" xlink:type="simple"/><label>(22)</label></disp-formula>Substituting Eq. (22) in Eq. (21) yields<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e025" xlink:type="simple"/><label>(23)</label></disp-formula>In order to evaluate the second term in the right hand side of Eq. (23) we note that by definition, <italic>r<sub>i</sub></italic> = <italic>p<sub>i</sub></italic>·E[<italic>R</italic>|<italic>A</italic> = <italic>i</italic>]/E[<italic>R</italic>] and therefore,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e026" xlink:type="simple"/><label>(24)</label></disp-formula>where we assumed that <italic>p</italic><sub>1</sub>,<italic>p</italic><sub>2</sub>≠0 and used the fact that <italic>p</italic><sub>1</sub>+<italic>p</italic><sub>2</sub> = 1 and <italic>r</italic><sub>1</sub>+<italic>r</italic><sub>2</sub> = 1. Substituting Eqs. (13), (14), (23) and (24) in Eqs. (18) and (19) yields<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e027" xlink:type="simple"/><label>(25)</label></disp-formula>and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e028" xlink:type="simple"/><label>(26)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e029" xlink:type="simple"/></inline-formula>. Combining Eqs. (25) and (26),<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e030" xlink:type="simple"/></disp-formula>or<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e031" xlink:type="simple"/><label>(27)</label></disp-formula>Eq. (27) is central to this manuscript. Together with Eq. (12) which relates the probability of choice <italic>p</italic><sub>1</sub> with <italic>T</italic> it determines the level of deviations from matching behavior at the fixed point, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e032" xlink:type="simple"/></inline-formula> (The relation between <italic>r</italic><sub>1</sub> and <italic>p</italic><sub>1</sub> is determined by the reward schedule). Next we use Eq. (27) to show that:</p><list list-type="order"><list-item><p>In the limit of σ→0 the model undermatches.</p></list-item><list-item><p>The level of undermatching is proportional to γ, (Eq. (6)).</p></list-item><list-item><p>Expanding Eq. (27) around <italic>p</italic><sub>1</sub> = 0.5, yields a closed-form solution for <italic>p</italic><sub>1</sub> (Eq. (7)).</p></list-item></list><p>(1) As was discussed above, the assumption that <italic>p</italic><sub>1</sub>,<italic>p</italic><sub>2</sub>≠0 in the limit of σ→0 implies that <italic>T</italic> = <italic>O</italic>(σ)and therefore 1−<italic>T</italic><sup>2</sup>&gt;0. Thus, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e033" xlink:type="simple"/></inline-formula>. Using Eq. (12) and the notations of Eq. (1),<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e034" xlink:type="simple"/><label>(28)</label></disp-formula>(<italic>Dp</italic><sub>1</sub> and <italic>Dr</italic><sub>1</sub> in Eq. (28) are the values at the fixed point and therefore a more accurate notation would have included an asterisk. However, in order to keep notations in the text simple and notations in the <xref ref-type="sec" rid="s4">Materials and Methods</xref> section consistent with the text we omitted the asterisk). When <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e035" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e036" xlink:type="simple"/></inline-formula> whereas when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e037" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e038" xlink:type="simple"/></inline-formula>. Thus we have shown that in the limit of σ→0 the model undermatches.</p><p>(2) Taking the dominant terms in σ in Eq. (27) yields<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e039" xlink:type="simple"/><label>(29)</label></disp-formula><italic>T</italic><sup>*</sup> = <italic>O</italic>(σ) and thus the second term in the right hand side of Eq. (29) is <italic>O</italic>(1); therefore, the level of deviations from matching behavior is <italic>O</italic>(γ), Eq (6).</p><p>(3) In order to obtain a closed form approximation to Eq. (29) we expand Eq. (12) around <italic>Dp</italic><sub>1</sub> = 0 yielding<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e040" xlink:type="simple"/><label>(30)</label></disp-formula>Expanding Eq. (29) around <italic>Dp<sub>i</sub></italic> = 0 and using Eq. (30) yields Eq. (7).</p></sec><sec id="s4c"><title>Bias in Winner-Take-All Mechanism and Choice Behavior</title><p>In order to study the effect of bias in the winner-take-all network on choice behavior, we assume that that alternative 1 is chosen in trials in which (<italic>M</italic><sub>1</sub>−<italic>M</italic><sub>2</sub>)/(<italic>M</italic><sub>1</sub>+<italic>M</italic><sub>2</sub>)&gt;ε where ε is a bias. Formally,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e041" xlink:type="simple"/><label>(31)</label></disp-formula>Rewriting Eq. (31) in terms of <italic>Z<sub>s</sub></italic> and <italic>Z<sub>d</sub></italic> yields<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e042" xlink:type="simple"/><label>(32)</label></disp-formula>where<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e043" xlink:type="simple"/><label>(33)</label></disp-formula>or<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e044" xlink:type="simple"/><label>(34)</label></disp-formula>The assumption that <italic>p</italic><sub>1</sub>,<italic>p</italic><sub>2</sub>≠0 implies in the limit of σ→0 <italic>T</italic>′ = <italic>O</italic>(σ). As in the derivation of Eqs. (13) and (14)<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e045" xlink:type="simple"/><label>(35)</label></disp-formula>and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e046" xlink:type="simple"/><label>(36)</label></disp-formula>Assuming that <italic>T</italic>′ = <italic>O</italic>(σ),<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e047" xlink:type="simple"/><label>(37)</label></disp-formula>and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e048" xlink:type="simple"/><label>(38)</label></disp-formula>From here we follow the same steps as in the derivation of Eq. (27) yielding<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e049" xlink:type="simple"/><label>(39)</label></disp-formula>or<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e050" xlink:type="simple"/><label>(40)</label></disp-formula>Assuming that <italic>T</italic>′<sup>*</sup> = <italic>O</italic>(σ) and taking the limit σ→0 yields<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e051" xlink:type="simple"/><label>(41)</label></disp-formula>Because <italic>r</italic><sub>1</sub>−<italic>p</italic><sub>1</sub> = <italic>O</italic>(1), the assumption that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e052" xlink:type="simple"/></inline-formula> implies that γ·ε/σ = <italic>O</italic>(1). Thus in the limit of σ→0, ε≪1. Taking <italic>O</italic>(ε) terms in Eq. (41) yields<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e053" xlink:type="simple"/><label>(42)</label></disp-formula>The first term in the right hand side of Eq. (42) is equal to the right hand side of Eq. (29) and yields <italic>O</italic>(γ) deviations from matching behavior in the direction of undermatching. The bias in the decision making process, ε affects choice preference through the second term in the right hand side of Eq. (29). For <italic>T</italic>′ = <italic>O</italic>(σ), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e054" xlink:type="simple"/></inline-formula> and the contribution of the bias term ε to deviations from matching is <italic>O</italic>(γ·ε/σ).</p><p>Expanding Eqs. (34) and (42) around <italic>Dp<sub>i</sub></italic> = 0 yields Eq. (8).</p></sec><sec id="s4d"><title>Incomplete Mean Subtraction and Saturation Stiffness</title><p>Rewriting Eq. (5),<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e055" xlink:type="simple"/><label>(43)</label></disp-formula>Next we show that in the limit σ→0 and assuming that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e056" xlink:type="simple"/></inline-formula>, Cov[<italic>R</italic>/E[<italic>R</italic>],<italic>N<sub>i</sub></italic>/E[<italic>N</italic>]]/γ≪1 and therefore the second term in the right hand side of Eq. (43) can be expanded around 1. In order to see this, we follow the same route as in the derivation of Eq. (23) and separate the covariance term into trials in which alternative 1 was chosen and trials in which alternative 2 was chosen<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e057" xlink:type="simple"/><label>(44)</label></disp-formula>As before, the reward <italic>R</italic> is a function to the actions, which in turn, are a function of the neural activity. Therefore, given the action <italic>A</italic>, <italic>R</italic> and δ<italic>N<sub>i</sub></italic> are statistically independent and therefore<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e058" xlink:type="simple"/><label>(45)</label></disp-formula>By construction, E[δ<italic>N<sub>i</sub></italic>/E[<italic>N</italic>]] = 0 and therefore,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e059" xlink:type="simple"/><label>(46)</label></disp-formula>Substituting Eq. (46) in Eq. (45) yields<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e060" xlink:type="simple"/><label>(47)</label></disp-formula>Note that<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e061" xlink:type="simple"/><label>(48)</label></disp-formula>Substituting Eqs. (16) and (15) in Eq. (48) yields,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e062" xlink:type="simple"/><label>(49)</label></disp-formula>Using Eq. (24), the assumption that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e063" xlink:type="simple"/></inline-formula> and taking the limit σ→0, such that σ/γ≪1 yields Cov[<italic>R</italic>/E[<italic>R</italic>],<italic>N<sub>i</sub></italic>/E[<italic>N</italic>]]/γ≪1. In fact, substituting Eq. (6) in Eq. (24), Cov[<italic>R</italic>/E[<italic>R</italic>],<italic>N<sub>i</sub></italic>/E[<italic>N</italic>]]/γ≪1 even when σ/γ↛0 as σ→0. Therefore, using self consistent arguments, the derivation of Eq. (50) is valid even when γ scales like σ. Expanding the second term in the right hand side of Eq. (43) yields,<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e064" xlink:type="simple"/><label>(50)</label></disp-formula>According to Eq. (11), the probability of choice depends only on the ratio <italic>W</italic><sub>1</sub>/<italic>W</italic><sub>2</sub>. Therefore, the first term in the right hand side of Eq. (50) does not affect the probabilities of choice. The saturation stiffness parameter ρ affects the probability of choice through the second term and this effect is equivalent to the scaling of the mistuning parameter γ by ρ. Thus, assuming that synaptic efficacies converge to the fixed point of the average trajectory, Eq. (5), the effect of deviations of the saturation stiffness parameter from unity on choice is equivalent to the scaling of γ by ρ.</p><p>The synaptic saturation term also changes the effective plasticity rate, which will change the conditions of applicability of the average trajectory approximation. This analysis goes beyond the scope of this manuscript and will be discussed elsewhere. In short, changing the value of ρ changes the effective plasticity rate to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e065" xlink:type="simple"/></inline-formula>. Therefore in the simulations in <xref ref-type="fig" rid="pcbi-1000007-g002">Fig. 2</xref> we used<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e066" xlink:type="simple"/><label>(51)</label></disp-formula></p></sec><sec id="s4e"><title>Overcomplete Mean Subtraction and Saturation Stiffness</title><p>According to Eq. (3), when γ&lt;0, <italic>W<sub>i</sub></italic> is expected to depress until it becomes negative. In reality, synaptic efficacies are bounded and synaptic saturation prevents them from changing their sign. We model the synaptic saturation by replacing the synaptic plasticity rule of Eq. (2) by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e067" xlink:type="simple"/><label>(52)</label></disp-formula>where ρ&gt;0 is the saturation stiffness parameter. The larger the value of ρ, the stiffer the bound. In the limit of ρ→∞, as long as <italic>W<sub>i</sub></italic>&gt;<italic>W<sub>low</sub></italic> Eq. (52) is equivalent to Eq. (2), but <italic>W<sub>i</sub></italic> is bounded from going below <italic>W<sub>low</sub></italic>.</p><p>The fixed point of the average trajectory of Eq. (52) is<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e068" xlink:type="simple"/><label>(53)</label></disp-formula>Following the same steps as in the derivation of Eq. (50), the limit σ→0 with the assumption that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e069" xlink:type="simple"/></inline-formula> yields<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e070" xlink:type="simple"/><label>(54)</label></disp-formula>Thus, assuming that synaptic efficacies converge to the fixed point of the average trajectory, Eq. (5), the behavior of a model with <italic>overcomplete</italic> mean subtraction is similar to that of a model with <italic>incomplete</italic> mean subtraction. In both cases the synaptic efficacies are given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e071" xlink:type="simple"/><label>(55)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1000007.e072" xlink:type="simple"/></inline-formula></p></sec><sec id="s4f"><title>Numerical Simulations</title><sec id="s4f1"><title>The reward schedule</title><p>The analytical results presented in this paper hold for a general diminishing-return reward schedule. They are demonstrated in the simulations using a concurrent VI reward schedule <xref ref-type="bibr" rid="pcbi.1000007-Davison1">[19]</xref>,<xref ref-type="bibr" rid="pcbi.1000007-Sugrue1">[20]</xref>. On each trial, the subject chooses between two targets. If the chosen target is baited with reward, the subject receives it, and the target becomes empty. An empty target is rebaited probabilistically, according to the toss of a biased coin. Once baited, a target remains baited until it is chosen. Rewards are binary and no more than a single reward can reside in each target. Therefore, the reward schedule has two parameters: the biases of the two coins used to bait the targets. These biases, or baiting probabilities, control whether a target is “rich” or “poor.” A VI reward schedule has diminishing returns because a target is less likely to be baited if it has been chosen recently, as a consequence of the fact that reward persists at a target once the target is baited.</p></sec><sec id="s4f2"><title>Simulation parameters</title><p>The sum of baiting probabilities in all simulations was kept constant at 0.5; σ = 0.1; E[<italic>N</italic>] = 1; plasticity rate in <xref ref-type="fig" rid="pcbi-1000007-g001">Fig. 1B</xref> is η = 0.05; plasticity rate in <xref ref-type="fig" rid="pcbi-1000007-g002">Figs. 2</xref> and <xref ref-type="fig" rid="pcbi-1000007-g003">3</xref> is scaled according to Eq. (51) with η<sub>0</sub> = 0.001. Each symbol in <xref ref-type="fig" rid="pcbi-1000007-g002">Figs. 2A</xref> and <xref ref-type="fig" rid="pcbi-1000007-g003">3A</xref> corresponds to the average of 10<sup>6</sup> trials of fixed baiting probabilities. Susceptibility was measured by computing the least-square-error linear fit.</p></sec></sec></sec></body><back><ack><p>I am indebted to H. S. Seung for many fruitful discussions and encouragement, and to D. Hansel and M. Shamir for their helpful comments on the manuscript.</p></ack><ref-list><title>References</title><ref id="pcbi.1000007-Hopfield1"><label>1</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group>             <year>1982</year>             <article-title>Neural networks and physical systems with emergent collective computational abilities.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>79</volume>             <fpage>2554</fpage>             <lpage>2558</lpage>          </element-citation></ref><ref id="pcbi.1000007-Amit1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Amit</surname><given-names>DJ</given-names></name><name name-style="western"><surname>Gutfreund</surname><given-names>H</given-names></name><name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group>             <year>1987</year>             <article-title>Information storage in neural networks with low levels of activity.</article-title>             <source>Phys Rev A</source>             <volume>35</volume>             <fpage>2293</fpage>             <lpage>2303</lpage>          </element-citation></ref><ref id="pcbi.1000007-Tsodyks1"><label>3</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tsodyks</surname><given-names>MV</given-names></name><name name-style="western"><surname>Feigelman</surname><given-names>MV</given-names></name></person-group>             <year>1988</year>             <article-title>Enhanced Storage Capacity in Neural Networks with Low Level of Activity.</article-title>             <source>Europhysics Lett</source>             <volume>6</volume>             <fpage>101</fpage>             <lpage>105</lpage>          </element-citation></ref><ref id="pcbi.1000007-Seung1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name></person-group>             <year>2003</year>             <article-title>Learning in spiking neural networks by reinforcement of stochastic synaptic transmission.</article-title>             <source>Neuron</source>             <volume>40</volume>             <fpage>1063</fpage>             <lpage>1073</lpage>          </element-citation></ref><ref id="pcbi.1000007-Fiete1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fiete</surname><given-names>IR</given-names></name><name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name></person-group>             <year>2006</year>             <article-title>Gradient learning in spiking neural networks by dynamic perturbation of conductances.</article-title>             <source>Phys Rev Lett</source>             <volume>97</volume>             <fpage>048104</fpage>          </element-citation></ref><ref id="pcbi.1000007-Baras1"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Baras</surname><given-names>D</given-names></name><name name-style="western"><surname>Meir</surname><given-names>R</given-names></name></person-group>             <year>2007</year>             <article-title>Reinforcement learning, spike time dependent plasticity and the BCM rule.</article-title>             <source>Neural Comput</source>             <volume>19</volume>             <fpage>2245</fpage>             <lpage>2279</lpage>          </element-citation></ref><ref id="pcbi.1000007-Williams1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Williams</surname><given-names>RJ</given-names></name></person-group>             <year>1992</year>             <article-title>Simple statistical gradient-following algorithms for connectionist reinforcement learning.</article-title>             <source>Mach Learn</source>             <volume>8</volume>             <fpage>229</fpage>             <lpage>256</lpage>          </element-citation></ref><ref id="pcbi.1000007-Toyoizumi1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Toyoizumi</surname><given-names>T</given-names></name><name name-style="western"><surname>Pfister</surname><given-names>JP</given-names></name><name name-style="western"><surname>Aihara</surname><given-names>K</given-names></name><name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name></person-group>             <year>2007</year>             <article-title>Optimality model of unsupervised spike-timing-dependent plasticity: synaptic memory and weight distribution.</article-title>             <source>Neural Comput</source>             <volume>19</volume>             <fpage>639</fpage>             <lpage>671</lpage>          </element-citation></ref><ref id="pcbi.1000007-Sakai1"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sakai</surname><given-names>Y</given-names></name><name name-style="western"><surname>Okamoto</surname><given-names>H</given-names></name><name name-style="western"><surname>Fukai</surname><given-names>T</given-names></name></person-group>             <year>2006</year>             <article-title>Computational algorithms and neuronal network models underlying decision processes.</article-title>             <source>Neural Netw</source>             <volume>19</volume>             <fpage>1091</fpage>             <lpage>1105</lpage>          </element-citation></ref><ref id="pcbi.1000007-Bohte1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bohte</surname><given-names>SM</given-names></name><name name-style="western"><surname>Mozer</surname><given-names>MC</given-names></name></person-group>             <year>2007</year>             <article-title>Reducing the variability of neural responses: a computational theory of spike-timing-dependent plasticity.</article-title>             <source>Neural Comput</source>             <volume>19</volume>             <fpage>371</fpage>             <lpage>403</lpage>          </element-citation></ref><ref id="pcbi.1000007-Loewenstein1"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Loewenstein</surname><given-names>Y</given-names></name><name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name></person-group>             <year>2006</year>             <article-title>Operant matching is a generic outcome of synaptic plasticity based on the covariance between reward and neural activity.</article-title>             <source>Proc Natl Acad Sci U S A.</source>             <comment>doi:10.1073/pnas.0505220103</comment>          </element-citation></ref><ref id="pcbi.1000007-Shapley1"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shapley</surname><given-names>R</given-names></name><name name-style="western"><surname>Enroth-Cugell</surname><given-names>C</given-names></name></person-group>             <year>1984</year>             <article-title>Chapter 9 Visual adaptation and retinal gain controls.</article-title>             <source>Progr Retinal Res</source>             <volume>3</volume>             <fpage>263</fpage>             <lpage>346</lpage>          </element-citation></ref><ref id="pcbi.1000007-Connors1"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Connors</surname><given-names>BW</given-names></name><name name-style="western"><surname>Gutnick</surname><given-names>MJ</given-names></name><name name-style="western"><surname>Prince</surname><given-names>DA</given-names></name></person-group>             <year>1982</year>             <article-title>Electrophysiological properties of neocortical neurons in vitro.</article-title>             <source>J Neurophysiol</source>             <volume>48</volume>             <fpage>1302</fpage>             <lpage>1320</lpage>          </element-citation></ref><ref id="pcbi.1000007-Ahmed1"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ahmed</surname><given-names>B</given-names></name><name name-style="western"><surname>Anderson</surname><given-names>JC</given-names></name><name name-style="western"><surname>Douglas</surname><given-names>RJ</given-names></name><name name-style="western"><surname>Martin</surname><given-names>KA</given-names></name><name name-style="western"><surname>Whitteridge</surname><given-names>D</given-names></name></person-group>             <year>1998</year>             <article-title>Estimates of the net excitatory currents evoked by visual stimulation of identified neurons in cat visual cortex.</article-title>             <source>Cereb Cortex</source>             <volume>8</volume>             <fpage>462</fpage>             <lpage>476</lpage>          </element-citation></ref><ref id="pcbi.1000007-Nagel1"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nagel</surname><given-names>KI</given-names></name><name name-style="western"><surname>Doupe</surname><given-names>AJ</given-names></name></person-group>             <year>2006</year>             <article-title>Temporal processing and adaptation in the songbird auditory forebrain.</article-title>             <source>Neuron</source>             <volume>51</volume>             <fpage>845</fpage>             <lpage>859</lpage>          </element-citation></ref><ref id="pcbi.1000007-Alon1"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Alon</surname><given-names>U</given-names></name><name name-style="western"><surname>Surette</surname><given-names>MG</given-names></name><name name-style="western"><surname>Barkai</surname><given-names>N</given-names></name><name name-style="western"><surname>Leibler</surname><given-names>S</given-names></name></person-group>             <year>1999</year>             <article-title>Robustness in bacterial chemotaxis.</article-title>             <source>Nature</source>             <volume>397</volume>             <fpage>168</fpage>             <lpage>171</lpage>          </element-citation></ref><ref id="pcbi.1000007-Herrnstein1"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Herrnstein</surname><given-names>RJ</given-names></name></person-group>             <year>1961</year>             <article-title>Relative and absolute strength of response as a function of frequency of reinforcement.</article-title>             <source>J Exp Anal Behav</source>             <volume>4</volume>             <fpage>267</fpage>             <lpage>272</lpage>          </element-citation></ref><ref id="pcbi.1000007-Herrnstein2"><label>18</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Herrnstein</surname><given-names>RJ</given-names></name></person-group>             <year>1997</year>             <source>The Matching Law: papers in psychology and economics</source>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>Harvard University Press</publisher-name>          </element-citation></ref><ref id="pcbi.1000007-Davison1"><label>19</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Davison</surname><given-names>M</given-names></name><name name-style="western"><surname>McCarthy</surname><given-names>D</given-names></name></person-group>             <year>1988</year>             <source>The Matching Law: A Research Review:</source>             <publisher-name>Lawrence Erlbaum</publisher-name>          </element-citation></ref><ref id="pcbi.1000007-Sugrue1"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sugrue</surname><given-names>LP</given-names></name><name name-style="western"><surname>Corrado</surname><given-names>GS</given-names></name><name name-style="western"><surname>Newsome</surname><given-names>WT</given-names></name></person-group>             <year>2004</year>             <article-title>Matching behavior and the representation of value in the parietal cortex.</article-title>             <source>Science</source>             <volume>304</volume>             <fpage>1782</fpage>             <lpage>1787</lpage>          </element-citation></ref><ref id="pcbi.1000007-Lau1"><label>21</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lau</surname><given-names>B</given-names></name><name name-style="western"><surname>Glimcher</surname><given-names>PW</given-names></name></person-group>             <year>2005</year>             <article-title>Dynamic response-by-response models of matching behavior in rhesus monkeys.</article-title>             <source>J Exp Anal Behav</source>             <volume>84</volume>             <fpage>555</fpage>             <lpage>579</lpage>          </element-citation></ref><ref id="pcbi.1000007-Arbib1"><label>22</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Arbib</surname><given-names>MA</given-names></name><name name-style="western"><surname>Amari</surname><given-names>SI</given-names></name></person-group>             <year>1977</year>             <article-title>Competition and Cooperation in Neural Nets.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Metzler</surname><given-names>J</given-names></name></person-group>             <source>Systems Neuroscience</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Academic Press</publisher-name>             <fpage>119</fpage>             <lpage>165</lpage>          </element-citation></ref><ref id="pcbi.1000007-Golding1"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Golding</surname><given-names>NL</given-names></name><name name-style="western"><surname>Staff</surname><given-names>NP</given-names></name><name name-style="western"><surname>Spruston</surname><given-names>N</given-names></name></person-group>             <year>2002</year>             <article-title>Dendritic spikes as a mechanism for cooperative long-term potentiation.</article-title>             <source>Nature</source>             <volume>418</volume>             <fpage>326</fpage>             <lpage>331</lpage>          </element-citation></ref><ref id="pcbi.1000007-Humeau1"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Humeau</surname><given-names>Y</given-names></name><name name-style="western"><surname>Shaban</surname><given-names>H</given-names></name><name name-style="western"><surname>Bissiere</surname><given-names>S</given-names></name><name name-style="western"><surname>Luthi</surname><given-names>A</given-names></name></person-group>             <year>2003</year>             <article-title>Presynaptic induction of heterosynaptic associative plasticity in the mammalian brain.</article-title>             <source>Nature</source>             <volume>426</volume>             <fpage>841</fpage>             <lpage>845</lpage>          </element-citation></ref><ref id="pcbi.1000007-Bailey1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bailey</surname><given-names>CH</given-names></name><name name-style="western"><surname>Giustetto</surname><given-names>M</given-names></name><name name-style="western"><surname>Huang</surname><given-names>YY</given-names></name><name name-style="western"><surname>Hawkins</surname><given-names>RD</given-names></name><name name-style="western"><surname>Kandel</surname><given-names>ER</given-names></name></person-group>             <year>2000</year>             <article-title>Is heterosynaptic modulation essential for stabilizing Hebbian plasticity and memory?</article-title>             <source>Nat Rev Neurosci</source>             <volume>1</volume>             <fpage>11</fpage>             <lpage>20</lpage>          </element-citation></ref><ref id="pcbi.1000007-Kempter1"><label>26</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kempter</surname><given-names>R</given-names></name><name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name><name name-style="western"><surname>Leo van Hemmen</surname><given-names>J</given-names></name></person-group>             <year>1999</year>             <article-title>Hebbian learning and spiking neurons.</article-title>             <source>Phys Rev E</source>             <volume>59</volume>             <fpage>4498</fpage>             <lpage>4514</lpage>          </element-citation></ref><ref id="pcbi.1000007-Gutig1"><label>27</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gutig</surname><given-names>R</given-names></name><name name-style="western"><surname>Aharonov</surname><given-names>R</given-names></name><name name-style="western"><surname>Rotter</surname><given-names>S</given-names></name><name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group>             <year>2003</year>             <article-title>Learning input correlations through nonlinear temporally asymmetric Hebbian plasticity.</article-title>             <source>J Neurosci</source>             <volume>23</volume>             <fpage>3697</fpage>             <lpage>3714</lpage>          </element-citation></ref><ref id="pcbi.1000007-Dayan1"><label>28</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name></person-group>             <year>2001</year>             <source>Theoretical Neuroscience</source>             <publisher-loc>Cambridge, Massachusetts</publisher-loc>             <publisher-name>MIT</publisher-name>          </element-citation></ref><ref id="pcbi.1000007-Heskes1"><label>29</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Heskes</surname><given-names>TM</given-names></name><name name-style="western"><surname>Kappen</surname><given-names>B</given-names></name></person-group>             <year>1993</year>             <article-title>On-line learning processes in articficial neural networks.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Taylor</surname><given-names>J</given-names></name></person-group>             <source>Mathematical Foundations of Neural Networks</source>             <publisher-loc>Amsterdam</publisher-loc>             <publisher-name>Elsevier</publisher-name>             <fpage>199</fpage>             <lpage>233</lpage>          </element-citation></ref><ref id="pcbi.1000007-Fusi1"><label>30</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name></person-group>             <year>2007</year>             <article-title>Limits on the memory storage capacity of bounded synapses.</article-title>             <source>Nat Neurosci</source>             <volume>10</volume>             <fpage>485</fpage>             <lpage>493</lpage>          </element-citation></ref><ref id="pcbi.1000007-Gardner1"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gardner</surname><given-names>E</given-names></name></person-group>             <year>1988</year>             <article-title>The space of interactions in neural network models.</article-title>             <source>J Phys A: Math Gen</source>             <volume>21</volume>             <fpage>257</fpage>             <lpage>270</lpage>          </element-citation></ref><ref id="pcbi.1000007-Seung2"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name></person-group>             <year>1996</year>             <article-title>How the brain keeps the eyes still.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>93</volume>             <fpage>13339</fpage>             <lpage>13344</lpage>          </element-citation></ref><ref id="pcbi.1000007-Seung3"><label>33</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name><name name-style="western"><surname>Lee</surname><given-names>DD</given-names></name><name name-style="western"><surname>Reis</surname><given-names>BY</given-names></name><name name-style="western"><surname>Tank</surname><given-names>DW</given-names></name></person-group>             <year>2000</year>             <article-title>Stability of the memory of eye position in a recurrent network of conductance-based model neurons.</article-title>             <source>Neuron</source>             <volume>26</volume>             <fpage>259</fpage>             <lpage>271</lpage>          </element-citation></ref><ref id="pcbi.1000007-Loewenstein2"><label>34</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Loewenstein</surname><given-names>Y</given-names></name><name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group>             <year>2003</year>             <article-title>Temporal integration by calcium dynamics in a model neuron.</article-title>             <source>Nat Neurosci</source>             <volume>6</volume>             <fpage>961</fpage>             <lpage>967</lpage>          </element-citation></ref><ref id="pcbi.1000007-Koulakov1"><label>35</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Koulakov</surname><given-names>AA</given-names></name><name name-style="western"><surname>Raghavachari</surname><given-names>S</given-names></name><name name-style="western"><surname>Kepecs</surname><given-names>A</given-names></name><name name-style="western"><surname>Lisman</surname><given-names>JE</given-names></name></person-group>             <year>2002</year>             <article-title>Model for a robust neural integrator.</article-title>             <source>Nat Neurosci</source>             <volume>5</volume>             <fpage>775</fpage>             <lpage>782</lpage>          </element-citation></ref><ref id="pcbi.1000007-Brody1"><label>36</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Brody</surname><given-names>CD</given-names></name><name name-style="western"><surname>Romo</surname><given-names>R</given-names></name><name name-style="western"><surname>Kepecs</surname><given-names>A</given-names></name></person-group>             <year>2003</year>             <article-title>Basic mechanisms for graded persistent activity: discrete attractors, continuous attractors, and dynamic representations.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>13</volume>             <fpage>204</fpage>             <lpage>211</lpage>          </element-citation></ref><ref id="pcbi.1000007-Machens1"><label>37</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Machens</surname><given-names>CK</given-names></name><name name-style="western"><surname>Romo</surname><given-names>R</given-names></name><name name-style="western"><surname>Brody</surname><given-names>CD</given-names></name></person-group>             <year>2005</year>             <article-title>Flexible control of mutual inhibition: a neural model of two-interval discrimination.</article-title>             <source>Science</source>             <volume>307</volume>             <fpage>1121</fpage>             <lpage>1124</lpage>          </element-citation></ref><ref id="pcbi.1000007-Arnold1"><label>38</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Arnold</surname><given-names>DB</given-names></name><name name-style="western"><surname>Robinson</surname><given-names>DA</given-names></name></person-group>             <year>1997</year>             <article-title>The oculomotor integrator: testing of a neural network model.</article-title>             <source>Exp Brain Res</source>             <volume>113</volume>             <fpage>57</fpage>             <lpage>74</lpage>          </element-citation></ref><ref id="pcbi.1000007-Turaga1"><label>39</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Turaga</surname><given-names>SC</given-names></name><name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name><name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name></person-group>             <year>2006</year>             <source>Online learning in a model neural integrator.</source>             <publisher-name>COSYNE</publisher-name>          </element-citation></ref><ref id="pcbi.1000007-Herrnstein3"><label>40</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Herrnstein</surname><given-names>RJ</given-names></name><name name-style="western"><surname>Prelec</surname><given-names>D</given-names></name></person-group>             <year>1991</year>             <article-title>Melioration, a theory of distributed choice.</article-title>             <source>J Econ Perspect</source>             <volume>5</volume>             <fpage>137</fpage>             <lpage>156</lpage>          </element-citation></ref><ref id="pcbi.1000007-Sakai2"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sakai</surname><given-names>Y</given-names></name><name name-style="western"><surname>Fukai</surname><given-names>T</given-names></name></person-group>             <year>2007</year>             <article-title>The actor-critic learning is behind the matching law: Matching vs. optimal behaviors.</article-title>             <source>Neural Computation</source>             <volume>20</volume>             <fpage>227</fpage>             <lpage>251</lpage>          </element-citation></ref><ref id="pcbi.1000007-Soltani1"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Soltani</surname><given-names>A</given-names></name><name name-style="western"><surname>Wang</surname><given-names>XJ</given-names></name></person-group>             <year>2006</year>             <article-title>A biophysically based neural model of matching law behavior: melioration by stochastic synapses.</article-title>             <source>J Neurosci</source>             <volume>26</volume>             <fpage>3731</fpage>             <lpage>3744</lpage>          </element-citation></ref><ref id="pcbi.1000007-Corrado1"><label>43</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Corrado</surname><given-names>GS</given-names></name><name name-style="western"><surname>Sugrue</surname><given-names>LP</given-names></name><name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name><name name-style="western"><surname>Newsome</surname><given-names>WT</given-names></name></person-group>             <year>2005</year>             <article-title>Linear-nonlinear Poisson models of primate choice dynamics.</article-title>             <source>J Exp Anal Behav</source>             <volume>84</volume>             <fpage>581</fpage>             <lpage>617</lpage>          </element-citation></ref><ref id="pcbi.1000007-Fetz1"><label>44</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fetz</surname><given-names>EE</given-names></name></person-group>             <year>1969</year>             <article-title>Operant conditioning of cortical unit activity.</article-title>             <source>Science</source>             <volume>163</volume>             <fpage>955</fpage>             <lpage>958</lpage>          </element-citation></ref><ref id="pcbi.1000007-Taylor1"><label>45</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Taylor</surname><given-names>DM</given-names></name><name name-style="western"><surname>Tillery</surname><given-names>SI</given-names></name><name name-style="western"><surname>Schwartz</surname><given-names>AB</given-names></name></person-group>             <year>2002</year>             <article-title>Direct cortical control of 3D neuroprosthetic devices.</article-title>             <source>Science</source>             <volume>296</volume>             <fpage>1829</fpage>             <lpage>1832</lpage>          </element-citation></ref></ref-list></back></article>