<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN"><front><journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="publisher">pcbi</journal-id><journal-id journal-id-type="flc">plcb</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="doi">10.1371/journal.pcbi.0020165</article-id><article-id pub-id-type="publisher-id">05-PLCB-RA-0344R3</article-id><article-id pub-id-type="sici">plcb-03-01-03</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="System Taxonomy"><subject>None</subject></subj-group></article-categories><title-group><article-title>Computational Aspects of Feedback in Neural Circuits</article-title><alt-title alt-title-type="running-head">Feedback in Neural Circuits</alt-title></title-group><contrib-group><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Maass</surname><given-names>Wolfgang</given-names></name><xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref><xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Joshi</surname><given-names>Prashant</given-names></name><xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Sontag</surname><given-names>Eduardo D</given-names></name><xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref></contrib></contrib-group><aff id="aff1">
				<label>1</label><addr-line>Institute for Theoretical Computer Science, Technische Universitaet Graz, Graz, Austria
			</addr-line></aff><aff id="aff2">
				<label>2</label><addr-line>Department of Mathematics, Rutgers, The State University of New Jersey, Piscataway, New Jersey, United States of America
			</addr-line></aff><contrib-group><contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Kotter</surname><given-names>Rolf</given-names></name><role>Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">Radboud University, The Netherlands</aff><author-notes><fn fn-type="con" id="ack1"><p>WM conceived and designed the experiments. PJ performed the experiments. WM and EDS analyzed the data. WM contributed reagents/materials/analysis tools. WM and EDS wrote the paper.</p></fn><corresp id="cor1">* To whom correspondence should be addressed. E-mail: <email xlink:type="simple">maass@igi.tugraz.at</email></corresp><fn fn-type="conflict" id="ack3"><p> The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="ppub"><month>1</month><year>2007</year></pub-date><pub-date pub-type="epub"><day>19</day><month>1</month><year>2007</year></pub-date><pub-date pub-type="epreprint"><day>24</day><month>10</month><year>2006</year></pub-date><volume>3</volume><issue>1</issue><elocation-id>e165</elocation-id><history><date date-type="received"><day>1</day><month>12</month><year>2005</year></date><date date-type="accepted"><day>24</day><month>10</month><year>2006</year></date></history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2007</copyright-year><copyright-holder>Maass et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract><p>It has previously been shown that generic cortical microcircuit models can perform complex real-time computations on continuous input streams, provided that these computations can be carried out with a rapidly fading memory. We investigate the computational capability of such circuits in the more realistic case where not only readout neurons, but in addition a few neurons <italic>within</italic> the circuit, have been trained for specific tasks. This is essentially equivalent to the case where the output of trained readout neurons is fed back into the circuit. We show that this new model overcomes the limitation of a rapidly fading memory. In fact, we prove that in the idealized case without noise it can carry out any conceivable digital or analog computation on time-varying inputs. But even with noise, the resulting computational model can perform a large class of biologically relevant real-time computations that require a nonfading memory. We demonstrate these computational implications of feedback both theoretically, and through computer simulations of detailed cortical microcircuit models that are subject to noise and have complex inherent dynamics. We show that the application of simple learning procedures (such as linear regression or perceptron learning) to a few neurons enables such circuits to represent time over behaviorally relevant long time spans, to integrate evidence from incoming spike trains over longer periods of time, and to process new information contained in such spike trains in diverse ways according to the current internal state of the circuit. In particular we show that such generic cortical microcircuits with feedback provide a new model for working memory that is consistent with a large set of biological constraints. Although this article examines primarily the computational role of feedback in circuits of neurons, the mathematical principles on which its analysis is based apply to a variety of dynamical systems. Hence they may also throw new light on the computational role of feedback in other complex biological dynamical systems, such as, for example, genetic regulatory networks.</p></abstract><abstract abstract-type="summary"><title>Author Summary</title><p>Circuits of neurons in the brain have an abundance of feedback connections, both on the level of local microcircuits and on the level of synaptic connections between brain areas. But the functional role of these feedback connections is largely unknown. We present a computational theory that characterizes the gain in computational power that feedback can provide in such circuits. It shows that feedback endows standard models for neural circuits with the capability to emulate arbitrary Turing machines. In fact, with suitable feedback they can simulate any dynamical system, in particular any conceivable analog computer. Under realistic noise conditions, the computational power of these circuits is necessarily reduced. But we demonstrate through computer simulations that feedback also provides a significant gain in computational power for quite detailed models of cortical microcircuits with in vivo–like high levels of noise. In particular it enables generic cortical microcircuits to carry out computations that combine information from working memory and persistent internal states in real time with new information from online input streams.</p></abstract><funding-group><funding-statement>This research was partially supported by the Austrian Science Fund FWF grants S9102-N04 and P17229-N04, and PASCAL project IST2002–506778 of the European Union. The work of EDS was partially supported by US National Science Foundation grants DMS-0504557 and DMS-0614371.</funding-statement></funding-group><counts><page-count count="20"/></counts><!--===== Restructure custom-meta-wrap to custom-meta-group =====--><custom-meta-group><custom-meta><meta-name>citation</meta-name><meta-value>Maass W, Joshi P, Sontag ED (2007) Computational aspects of feedback in neural circuits. PLoS Comput Biol 3(1): e165. doi:<ext-link ext-link-type="doi" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.0030008" xlink:type="simple">10.1371/journal.pcbi.0020165</ext-link></meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1"><title>Introduction</title><p>The neocortex performs a large variety of complex computations in real time. It is conjectured that these computations are carried out by a network of cortical microcircuits, where each microcircuit is a rather stereotypical circuit of neurons within a cortical column. A characteristic property of these circuits and networks is an abundance of feedback connections. But the computational function of these feedback connections is largely unknown. Two lines of research have been engaged to solve this problem. In one approach, which one might call the constructive approach, one builds hypothetical circuits of neurons and shows that (under some conditions on the response behavior of its neurons and synapses) such circuits can perform specific computations. In another research strategy, which one might call the analytical approach, one starts with data-based models for actual cortical microcircuits, and analyses which computational operations such “given” circuits can perform under the assumption that a learning process assigns suitable values to some of their parameters (e.g., synaptic efficacies of readout neurons). An underlying assumption of the analytical approach is that complex recurrent circuits, such as cortical microcircuits, cannot be fully understood in terms of the usually considered properties of their components. Rather, system-level approaches that directly address the dynamics of the resulting recurrent neural circuits are needed to complement the bottom-up analysis. This line of research started with the identification and investigation of so-called canonical microcircuits [<xref ref-type="bibr" rid="pcbi-0020165-b001">1</xref>]. Several issues related to cortical microcircuits have also been addressed in the work of Grossberg; see [<xref ref-type="bibr" rid="pcbi-0020165-b002">2</xref>] and the references therein. Subsequently it was shown that quite complex real-time computations on spike trains can be carried out by such “given” models for cortical microcircuits ([<xref ref-type="bibr" rid="pcbi-0020165-b003">3</xref>–<xref ref-type="bibr" rid="pcbi-0020165-b006">6</xref>], see [<xref ref-type="bibr" rid="pcbi-0020165-b007">7</xref>] for a review). A fundamental limitation of this approach was that only those computations could be modeled that can be carried out with a fading memory, more precisely only those computations that require integration of information over a timespan of 200 ms to 300 ms (its maximal length depends on the amount of noise in the circuit and the complexity of the input spike trains [<xref ref-type="bibr" rid="pcbi-0020165-b008">8</xref>]). In particular, computational tasks that require a representation of elapsed time between salient sensory events or motor actions [<xref ref-type="bibr" rid="pcbi-0020165-b009">9</xref>], or an internal representation of expected rewards [<xref ref-type="bibr" rid="pcbi-0020165-b010">10</xref>–<xref ref-type="bibr" rid="pcbi-0020165-b012">12</xref>], working memory [<xref ref-type="bibr" rid="pcbi-0020165-b013">13</xref>], accumulation of sensory evidence for decision making [<xref ref-type="bibr" rid="pcbi-0020165-b014">14</xref>], the updating and holding of analog variables such as for example the desired eye position [<xref ref-type="bibr" rid="pcbi-0020165-b015">15</xref>], and differential processing of sensory input streams according to attentional or other internal states of the neural system [<xref ref-type="bibr" rid="pcbi-0020165-b016">16</xref>] could not be modeled in this way. Previous work on concrete examples of artificial neural networks [<xref ref-type="bibr" rid="pcbi-0020165-b017">17</xref>] and cortical microcircuit models [<xref ref-type="bibr" rid="pcbi-0020165-b018">18</xref>] had already indicated that these shortcomings of the model might arise only if one assumes that learning affects exclusively the synapses of readout neurons that project the results of computations to other circuits or areas, without giving feedback into the circuit from which they extract information. This scenario is in fact rather unrealistic from a biological perspective, since pyramidal neurons in the cortex typically have in addition to their long projecting axon a large number of axon collaterals that provide feedback to the local circuit [<xref ref-type="bibr" rid="pcbi-0020165-b019">19</xref>]. Abundant feedback connections also exist on the network level between different brain areas [<xref ref-type="bibr" rid="pcbi-0020165-b020">20</xref>]. We show in this article that if one takes feedback connections from readout neurons (that are trained for specific tasks) into account, generic cortical microcircuit models can solve all of the previously listed computational tasks. In fact, one can demonstrate this also for circuits whose underlying noise levels and models for neurons and synapses are substantially more realistic than those which had previously been considered in models for working memory and related tasks.</p><p>We show in the Theoretical Analysis section that the significance of feedback for the computational power of neural circuits and other dynamical systems can be explained on the basis of general principles. Theorem 1 implies that a large class of dynamical systems, in particular systems of differential equations that are commonly used to describe the dynamics of firing activity in neural circuits, gain universal computational capabilities for digital and analog computation as soon as one considers them in combination with feedback. A further mathematical result (Theorem 2) implies that the capability to process online input streams in the light of nonfading (or slowly fading) internal states is preserved in the presence of fairly large levels of internal noise. On the basis of this theoretical foundation, one can explain why the computer models of generic cortical microcircuits, which are considered in the section Applications to Generic Cortical Microcircuit Models, are able to solve the previously mentioned benchmark tasks. These results suggest a new computational model for cortical microcircuits, which includes the capability to process online input streams in diverse ways according to different “instructions” that are implemented through high-dimensional attractors of the underlying dynamical system. The high dimensionality of these attractors results from the fact that only a small fraction of synapses need to be modified for their creation. In comparison with the commonly considered low-dimensional attractors, such high-dimensional attractors have additional attractive properties such as compositionality (the intersection of several of them is in general nonempty) and compatibility with real-time computing on online input streams within the same circuit.</p><p>The presentation of theoretical results for abstract circuit models in the Theoretical Analysis section is complemented by mathematical details in the Methods section, under the heading Mathematical Definitions, Details to the Proof of Theorem 1, and Examples, and the heading Mathematical Definitions and Details to the Proof of Theorem 2. Details of the computer simulations of more detailed cortical microcircuit models are discussed in Applications to Generic Cortical Microcircuit Models in the Methods section. A discussion of the results of this paper is given in the Discussion section.</p></sec><sec id="s2"><title>Results</title><p>We consider two types of models for neural circuits.</p><p>The first model type is mean field models, such as those defined by <xref ref-type="disp-formula" rid="pcbi-0020165-e006">Equation 6</xref>, which models the dynamics of firing rates of neurons in neural circuits. These models have the advantage that they are theoretically tractable, but they have the disadvantage that they do not reflect many known details of cortical microcircuits. However we show that the theoretical results that are proven in the section Theoretical Analysis hold for fairly large classes of dynamical systems. Hence, they potentially also hold for some more detailed models of neural circuits.</p><p>The second model type involves quite detailed models of cortical microcircuits consisting of spiking neurons (see the description in Applications to Generic Cortical Microcircuit Models and in Details of the Cortical Microcircuit Models). At present these models cannot be analyzed directly by theoretical methods, hence we can only present statistical data from computer simulations. Our simulation results show that feedback has in these more detailed models a variety of computational consequences that we have derived analytically for the simpler models in Theoretical Analysis. This is not totally surprising insofar as the computations that we consider in the more detailed models can be approximately described in terms of time-varying firing rates for individual neurons.</p><p>In both types of models we focus on computations that transform time-varying input streams into time-varying output streams. The input streams are modeled in Theoretical Analysis by time-varying analog functions <italic>u</italic>(<italic>t</italic>) (that might for example represent time-varying firing rates of neurons that provide afferent inputs) and in Applications to Generic Cortical Microcircuit Models by spike trains generated by Poisson processes with time-varying rates. Output streams are analogously modeled by time-varying firing rates, or directly by spike trains. We believe that such online computations, which transform time-varying inputs into time-varying outputs, provide a better framework for modeling cortical processing of information than computations that transform a static vector of numbers (i.e., a batch input) into a static output. Mappings from time-varying inputs to time-varying outputs are referred to as filters (or operators) in mathematics and engineering. A frequently discussed reference class of linear and nonlinear filters includes those that can be described by Volterra or Wiener series (see, e.g., [<xref ref-type="bibr" rid="pcbi-0020165-b021">21</xref>]). These filters can equivalently be characterized as those filters that are time-invariant (i.e., they are input-driven and have no “internal clock”) and have a fading memory (see [<xref ref-type="bibr" rid="pcbi-0020165-b005">5</xref>]). Fading memory (which is formally defined in Fading-Memory Filters means intuitively that the influence of any specific segment of the input stream on later parts of the output stream becomes negligible when the length of the intervening time interval is sufficiently large. We show in the next two subsections that feedback endows a circuit, which by itself can only carry out computations with fading memory, with flexible ways of combining fading-memory computations on time-varying inputs with computational operations on selected pieces of information in a nonfading memory.</p><sec id="s2a"><title>Theoretical Analysis</title><p>The dynamics of firing rates in recurrent circuits of neurons is commonly modeled by systems of nonlinear differential equations of the form
					<disp-formula id="pcbi-0020165-e001"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e001" xlink:type="simple"/><!-- <mml:math display='block'><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>&prime;</mml:mi></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>&lambda;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>&sigma;</mml:mi><mml:mtext>&thinsp;</mml:mtext><mml:mo stretchy='true'>(</mml:mo><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mi>j</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:munderover></mml:mstyle><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&sdot;</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='true'>)</mml:mo><mml:mo>,</mml:mo><mml:mtext>&ensp;</mml:mtext><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&hellip;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mtext>&thinsp;</mml:mtext><mml:mo>,</mml:mo></mml:math> --></disp-formula>or
					<disp-formula id="pcbi-0020165-e002"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e002" xlink:type="simple"/><!-- <mml:math display='block'><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>&prime;</mml:mi></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>&lambda;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>&sigma;</mml:mi><mml:mtext>&thinsp;</mml:mtext><mml:mo stretchy='true'>(</mml:mo><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mi>j</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:munderover></mml:mstyle><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='true'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&sdot;</mml:mo><mml:mi>&sigma;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='true'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo>,</mml:mo><mml:mtext>&ensp;</mml:mtext><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&hellip;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mtext>&thinsp;</mml:mtext></mml:math> --></disp-formula>[<xref ref-type="bibr" rid="pcbi-0020165-b022">22</xref>–<xref ref-type="bibr" rid="pcbi-0020165-b025">25</xref>]. Here each <italic>x<sub>i</sub></italic>,<italic>i</italic> = 1,…,<italic>n</italic>, is a real-valued variable that represents the current firing rate of the <italic>i<sup>th</sup></italic> neuron or population of neurons in a recurrent neural circuit, and <italic>v</italic>(<italic>t</italic>) is an external input stream. The coefficients <italic>a<sub>ij</sub></italic>,<italic>b<sub>i</sub></italic> denote the strengths of synaptic connections, and <italic>λ<sub>i</sub></italic> &gt; 0 denotes time constants. The function <italic>σ</italic> is some sigmoidal activation function (nondecreasing, with bounded range). In most models of neural circuits, the parameters are chosen so that the resulting dynamical system has a fading memory for preceding inputs. If one makes the synaptic connection strengths <italic>a<sub>ij</sub></italic> in <xref ref-type="disp-formula" rid="pcbi-0020165-e001">Equation 1</xref> or <xref ref-type="disp-formula" rid="pcbi-0020165-e002">Equation 2</xref> so large that recurrent activity does not dissipate, the neural circuit tends to exhibit persistent memory. But it is usually quite difficult to control the content of this persistent memory, since it tends to be swamped with minor details of external inputs (or initial conditions) from the distant past. Hence this chaotic regime of recurrent neural circuits (see <xref ref-type="bibr" rid="pcbi-0020165-b062">[62]</xref> for a review) is apparently also not suitable for biologically realistic online computations that combine new information from the current input with selected (e.g., behaviorally relevant) aspects of external or internal inputs from the past.
				</p><p>Recurrent circuits of neurons (e.g., those described by <xref ref-type="disp-formula" rid="pcbi-0020165-e001">Equations 1</xref> or <xref ref-type="disp-formula" rid="pcbi-0020165-e002">2</xref>) are from a mathematical perspective special cases of dynamical systems. The subsequent mathematical results show that a large variety of dynamical systems, in particular also fading-memory systems of type <xref ref-type="disp-formula" rid="pcbi-0020165-e001">Equation 1</xref> or <xref ref-type="disp-formula" rid="pcbi-0020165-e002">Equation 2</xref>, can overcome in the presence of feedback the computational limitations of a fading memory without necessarily falling into the chaotic regime. In fact, feedback endows them with <italic>universal</italic> capabilities for <italic>analog computing,</italic> in a sense that can be made precise in the following way (see <xref ref-type="fig" rid="pcbi-0020165-g001">Figure 1</xref>A–<xref ref-type="fig" rid="pcbi-0020165-g001">1</xref>C for an illustration):</p><fig id="pcbi-0020165-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020165.g001</object-id><label>Figure 1</label><caption><title>Computational Architectures Considered in Theorems 1 and 2</title><p>(A) A fixed circuit <italic>C</italic> whose dynamics is described by the system (<xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref>).</p><p>(B) An arbitrary given <italic>n<sup>th</sup></italic> order dynamical system (<xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref>) with external input <italic>u</italic>(<italic>t</italic>).</p><p>(C) If the input <italic>v</italic>(<italic>t</italic>) to circuit <italic>C</italic> is replaced by a suitable feedback <italic>K</italic>(<bold>x</bold>(<italic>t</italic>),<italic>u</italic>(<italic>t</italic>)), then this fixed circuit <italic>C</italic> can simulate the dynamic response <italic>z</italic>(<italic>t</italic>) of the arbitrarily given system shown in B, for any input stream <italic>u</italic>(<italic>t</italic>).</p><p>(D) Arbitrary given FSM <italic>A</italic> with l state.</p><p>(E) A noisy fading-memory system with feedback can reliably reproduce the current state <italic>A</italic>(<italic>t</italic>) of the given FSM <italic>A,</italic> except for timepoints <italic>t</italic> shortly after <italic>A</italic> has switched its state.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020165.g001" xlink:type="simple"/></fig><sec id="s2a1"><title>Theorem 1.</title><p><italic>A large class S<sub>n</sub> of systems of differential equations of the form</italic>
						<disp-formula id="pcbi-0020165-e003"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e003" xlink:type="simple"/><!-- <mml:math display='block'><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mi>&prime;</mml:mi></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:mo>&hellip;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:mo>&hellip;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mo>&sdot;</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>&thinsp;</mml:mtext><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&hellip;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mtext>&thinsp;</mml:mtext><mml:mtext>&thinsp;</mml:mtext></mml:math> --></disp-formula><italic>are in the following sense universal for analog computing:</italic>
					</p><p><italic>This system (3) can respond to an external input u</italic>(<italic>t</italic>) <italic>with the dynamics of any n<sup>t</sup> order differential equation of the form</italic>
						<disp-formula id="pcbi-0020165-e004"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e004" xlink:type="simple"/><!-- <mml:math display='block'><mml:msup><mml:mi>z</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>&Prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:mo>&hellip;</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>u</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math> --></disp-formula><italic>(for arbitrary smooth functions G</italic>: ℜ<italic><sup>n</sup></italic> → ℜ) <italic>if the input term v</italic>(<italic>t</italic>) <italic>is replaced in <xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref> by a suitable memoryless feedback function K</italic>(<italic>x</italic><sub>1</sub>(<italic>t</italic>), … ,<italic>x<sub>n</sub></italic>(<italic>t</italic>),<italic>u</italic>(<italic>t</italic>))<italic>, and if a suitable memoryless readout function h</italic>(<bold>x</bold>(<italic>t</italic>)) <italic>is applied to its internal state</italic> <bold>x</bold>(<italic>t</italic>) = 〈<italic>x</italic><sub>1</sub>(<italic>t</italic>),…,<italic>x<sub>n</sub></italic>(<italic>t</italic>)〉<italic>: one can achieve then that h</italic>(<bold>x</bold>(<italic>t</italic>)) = <italic>z</italic>(<italic>t</italic>) <italic>for any solution z</italic>(<italic>t</italic>) <italic>of <xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref>.</italic>
					</p><p>
            <italic>Also the dynamic responses of all systems consisting of several higher order differential equations of the form <xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref> can be simulated by fixed systems of the form <xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref> with a corresponding number of feedbacks.</italic>
          </p><p>This result says more precisely that for any <italic>n<sup>th</sup></italic> order differential equation (<xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref>) there exists a (memory-free) feedback function K: ℜ<italic><sup>n</sup></italic> × ℜ → ℜ and a memory-free readout function <italic>h</italic>: ℜ<italic><sup>n</sup></italic> → ℜ (which can both be chosen to be smooth, in particular continuous) so that, for every external input <italic>u</italic>(<italic>t</italic>),<italic>t</italic> ≥ 0, and each solution <italic>z</italic>(<italic>t</italic>) of the forced system (<xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref>), there is an input <italic>u</italic><sub>0</sub>(<italic>t</italic>) with <italic>u</italic><sub>0</sub>(<italic>t</italic>) ≡ 0 for all <italic>t</italic> ≥ 1, so that the solution <bold>x</bold>(<italic>t</italic>) = 〈<italic>x</italic><sub>1</sub>(<italic>t</italic>),…,<italic>x<sub>n</sub></italic>(<italic>t</italic>)〉 of the fixed system (<xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref>)
						<disp-formula id="pcbi-0020165-e005"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e005" xlink:type="simple"/><!-- <mml:math display='block'><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mi>K</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mo>&emsp;</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mn mathvariant="bold">0</mml:mn></mml:math> --></disp-formula>(for <italic>f</italic>: ℜ<italic><sup>n</sup></italic> → ℜ<italic><sup>n</sup></italic> consisting of 〈<italic>f</italic><sub>1</sub>,…,<italic>f<sub>n</sub></italic>〉 and g: ℜ<italic><sup>n</sup></italic> → ℜ<italic><sup>n</sup></italic> consisting of 〈<italic>g</italic><sub>1</sub>,…<italic>g</italic><sub>n</sub>〉) is such that
						<disp-formula id="pcbi-0020165-e071"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e071" xlink:type="simple"/><!-- <mml:math display='block'><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mi>z</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&emsp;</mml:mo><mml:mtext>for all</mml:mtext><mml:mspace width='3pt'/><mml:mi>t</mml:mi><mml:mo>&ge;</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo></mml:math> --></disp-formula>Note that the function <italic>u</italic><sub>0</sub>(<italic>t</italic>), which is added to the input for <italic>t</italic> &lt; 1 (whereas <italic>u</italic><sub>0</sub>(<italic>t</italic>) = 0 for <italic>t</italic> ≥ 1), allows the system (<xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref>) (and <xref ref-type="disp-formula" rid="pcbi-0020165-e005">Equation 5</xref>) to simulate with a standardized initial condition <bold>x</bold>(0) = <bold>0</bold> for any solution of <xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref> with arbitrary initial conditions.
					</p><p>Theorem 1 implies that even if some fixed dynamical system (<xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref>) from the class <italic>S<sub>n</sub></italic> has fading memory, a suitable feedback <italic>K</italic> and readout function <italic>h</italic> will enable it to carry out specific computations with persistent memory. In fact, it can carry out <italic>any</italic> computation with persistent memory that could possibly be carried out by <italic>any</italic> dynamical system (<xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref>). To get a clear understanding of this universality property, one should note that the feedback function <italic>K</italic> and the readout function <italic>h</italic> depend only on the function <italic>G</italic> that characterizes the simulated system (<xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref>), but not on the external input <italic>u</italic>(<italic>t</italic>) or the particular solution <italic>z</italic>(<italic>t</italic>) of <xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref> that it simulates. Hence, Theorem 1 implies in particular that any system (<xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref>) that belongs to the class <italic>S<sub>n</sub></italic> has in conjunction with several feedbacks the computational power of a universal Turing machine (see [<xref ref-type="bibr" rid="pcbi-0020165-b026">26</xref>] or [<xref ref-type="bibr" rid="pcbi-0020165-b027">27</xref>] for relevant concepts from computation theory). This follows from the fact that every Turing machine (hence any conceivable digital computation, most of which require a persistent memory) can be simulated by systems of equations of the form <xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref> (this was shown in [<xref ref-type="bibr" rid="pcbi-0020165-b028">28</xref>] for the case with continuous time, and in [<xref ref-type="bibr" rid="pcbi-0020165-b029">29</xref>,<xref ref-type="bibr" rid="pcbi-0020165-b030">30</xref>] for recurrent neural networks with discrete time; see [<xref ref-type="bibr" rid="pcbi-0020165-b031">31</xref>] for a review). But possibly more relevant for applications to biological systems is the fact that any fixed system (<xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref>) that belongs to the class <italic>S<sub>n</sub></italic> is able to emulate any conceivable <italic>continuous</italic> dynamic response to an input stream <italic>u</italic>(<italic>t</italic>) if it receives a suitable feedback <italic>K</italic>(<bold>x</bold>(<italic>t</italic>),<italic>u</italic>(<italic>t</italic>)), where <italic>K</italic> can always be chosen to be continuous. Hence one may argue that these systems (<xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref>) are also universal for <italic>analog</italic> computing on time-varying inputs.</p><p>The class <italic>S<sub>n</sub></italic> of dynamical systems become through feedback universal for analog computing subsumes systems of the form
						<disp-formula id="pcbi-0020165-e006"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e006" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:msup><mml:mi>x</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo>&equals;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>&lambda;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>&sigma;</mml:mi><mml:mtext>&thinsp;</mml:mtext><mml:mo stretchy='true'>(</mml:mo><mml:mstyle displaystyle='true'><mml:munderover><mml:mo>&sum;</mml:mo><mml:mi>j</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:munderover></mml:mstyle><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&sdot;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='true'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&sdot;</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo>,</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>&thinsp;</mml:mtext><mml:mtext>&thinsp;</mml:mtext><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&hellip;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mtext>&thinsp;</mml:mtext><mml:mo>;</mml:mo><mml:mtext>&thinsp;</mml:mtext></mml:math> --></disp-formula>for example, if the <italic>λ<sub>i</sub></italic> are pairwise different and <italic>a<sub>ij</sub></italic> = 0 for all <italic>i</italic>,<italic>j,</italic> and all <italic>b<sub>i</sub></italic> are nonzero. Fewer restrictions are needed if more than one feedback to the system (<xref ref-type="disp-formula" rid="pcbi-0020165-e006">Equation 6</xref>) can be used. Systems of the form <xref ref-type="disp-formula" rid="pcbi-0020165-e001">Equation 1</xref> or <xref ref-type="disp-formula" rid="pcbi-0020165-e002">Equation 2</xref> are of a slightly different form, since there the activation function <italic>σ</italic> (that has a bounded range) is applied to the term <italic>v</italic>(<italic>t</italic>). But such systems (<xref ref-type="disp-formula" rid="pcbi-0020165-e001">Equations 1</xref> and <xref ref-type="disp-formula" rid="pcbi-0020165-e002">2</xref>) can still be universal for all <italic>bounded</italic> analog responses of arbitrary dynamical systems (<xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref>), which are arguably the only ones of interest in a biological context. This follows from the fact that if the external input <italic>u</italic>(<italic>t</italic>) of the system (<xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref>), as well as the resulting solution <italic>z</italic>(<italic>t</italic>) and its derivatives <italic>z</italic><sup>(<italic>i</italic>)</sup><italic>t</italic> for <italic>i</italic> ≤ <italic>n</italic> − 1, stay within some bounded range, then the values of the feedback <italic>v</italic>(<italic>t</italic>) that is needed for the simulation of <xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref> by <xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref> will also stay within a bounded range. More precisely, one has that:
					</p><p><italic>For each constant c</italic> &gt; 0 <italic>there is a constant C</italic> &gt; 0 <italic>such that: for every external input u</italic>(<italic>t</italic>),<italic>t ≥</italic> 0, <italic>and each solution z</italic>(<italic>t</italic>) <italic>of the forced system (<xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref>) such that</italic>
						<disp-formula id="pcbi-0020165-e072"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e072" xlink:type="simple"/><!-- <mml:math display='block'><mml:mo>|</mml:mo><mml:mi>u</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:mo>&le;</mml:mo><mml:mi>c</mml:mi><mml:mspace width='3pt'/><mml:mtext>and</mml:mtext><mml:mspace width='3pt'/><mml:mo>|</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:mo>&le;</mml:mo><mml:mi>c</mml:mi><mml:mo>&emsp;</mml:mo><mml:mtext>for all</mml:mtext><mml:mspace width='3pt'/><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext>for all</mml:mtext><mml:mspace width='3pt'/><mml:mi>t</mml:mi><mml:mo>&ge;</mml:mo><mml:mn>0</mml:mn></mml:math> --></disp-formula><italic>the input u</italic><sub>0</sub> <italic>can be picked so that the feedback</italic>
						<disp-formula id="pcbi-0020165-e073"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e073" xlink:type="simple"/><!-- <mml:math display='block'><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mi>K</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>&emsp;</mml:mo><mml:mi>t</mml:mi><mml:mo>&ge;</mml:mo><mml:mn>0</mml:mn></mml:math> --></disp-formula><italic>to <xref ref-type="disp-formula" rid="pcbi-0020165-e001">Equation 1</xref> or 2 satisfies:</italic>
						<disp-formula id="pcbi-0020165-e074"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e074" xlink:type="simple"/><!-- <mml:math display='block'><mml:mo>|</mml:mo><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>|</mml:mo><mml:mo>&le;</mml:mo><mml:mi>C</mml:mi><mml:mo>&emsp;</mml:mo><mml:mtext>for all</mml:mtext><mml:mspace width='3pt'/><mml:mi>t</mml:mi><mml:mo>&ge;</mml:mo><mml:mn>0</mml:mn></mml:math> --></disp-formula>
					</p><p>Thus, if we know a priori that we will only deal with solutions of the differential <xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref> that are bounded by <italic>c</italic>, and inputs are similarly bounded, we could also consider instead of <xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref> a system such as <bold>x′</bold>(<italic>t</italic>) = <italic>f</italic>(<bold>x</bold>(<italic>t</italic>)) + <italic>g</italic>(<bold>x</bold>(<italic>t</italic>))<italic>σ</italic>(<italic>v</italic>(<italic>t</italic>)) with <italic>f</italic>,<italic>g</italic>: ℜ<italic><sup>n</sup></italic> → ℜ<italic><sup>n</sup></italic>, where some bounded activation function <italic>σ</italic>: ℜ → ℜ (e.g., <italic>q</italic> · tanh(<italic>v</italic>), for a suitable constant <italic>q</italic>) is applied to the term <italic>v</italic>(<italic>t</italic>) (as in <xref ref-type="disp-formula" rid="pcbi-0020165-e002">Equation 2</xref>). The resulting feedback term <italic>σ</italic>(<italic>K</italic>(<bold>x</bold>(<italic>t</italic>),<italic>u</italic>(<italic>t</italic>) + <italic>u</italic><sub>0</sub>(<italic>t</italic>))) is then of a mathematical form which is adequate for modeling feedback in neural circuits.</p><p>The <bold>proof</bold> of Theorem 1 builds on results from control theory. One important technique in nonlinear control is <italic>feedback linearization</italic> ([<xref ref-type="bibr" rid="pcbi-0020165-b032">32</xref>,<xref ref-type="bibr" rid="pcbi-0020165-b033">33</xref>]). With this technique, a large class of nonlinear dynamical systems can be transformed through suitable feedback into a linear system (which is then much easier to control). It should be pointed out that this feedback linearization is not a standard linearization method that only yields approximation results, but a method that yields an exact transformation. More generally, one can show in various cases that two dynamical systems, <italic>D</italic><sub>1</sub> and <italic>D</italic><sub>2</sub>, are <italic>feedback equivalent</italic>. The notion of “feedback equivalence” (see Definition of Feedback Equivalence), which is in fact an equivalence relation, expresses that two systems of differential equations can be transformed into each other through application of a suitable feedback and a change of basis in the state space. Such change of basis can be achieved through readout functions <italic>h</italic>(<bold>x</bold>(<italic>t</italic>)) as considered in the claim of Theorem 1. Thus, to show that a fixed system <italic>D</italic><sub>1</sub> has the universality property that is specified in the claim of Theorem 1, it suffices to show that <italic>D</italic><sub>1</sub> is feedback equivalent to all systems of the form <xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref>. Known results about feedback linearization (see [<xref ref-type="bibr" rid="pcbi-0020165-b033">33</xref>], Lemma 5.3.5) imply that the following linear system (<xref ref-type="disp-formula" rid="pcbi-0020165-e007">Equation 7</xref>) is an example of a system <italic>D</italic><sub>1</sub> (consisting of <italic>n</italic> differential equations) which has this universality property:
						<disp-formula id="pcbi-0020165-e007"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e007" xlink:type="simple"/><!-- <mml:math display='block'><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&equals;</mml:mo><mml:msub><mml:mi mathvariant="bold">A</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&plus;</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mi>v</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math> --></disp-formula>with
						<disp-formula id="pcbi-0020165-e075"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e075" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>A</mml:mi></mml:mstyle><mml:mi>n</mml:mi></mml:msub><mml:mtext>&thinsp;</mml:mtext><mml:mo>:</mml:mo><mml:mo>&equals;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo stretchy='true'>(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>&hellip;</mml:mo></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mo>&hellip;</mml:mo></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&vellip;</mml:mo></mml:mtd><mml:mtd><mml:mo>&vellip;</mml:mo></mml:mtd><mml:mtd><mml:mo>&vellip;</mml:mo></mml:mtd><mml:mtd><mml:mo>&dellip;</mml:mo></mml:mtd><mml:mtd><mml:mo>&vellip;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>&hellip;</mml:mo></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>&hellip;</mml:mo></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy='true'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>&thinsp;</mml:mtext><mml:mtext>&thinsp;</mml:mtext><mml:mtext>&thinsp;</mml:mtext><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>b</mml:mi></mml:mstyle><mml:mi>n</mml:mi></mml:msub><mml:mtext>&thinsp;</mml:mtext><mml:mo>:</mml:mo><mml:mo>&equals;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo stretchy='true'>(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&vellip;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mtext>&thinsp;</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy='true'>)</mml:mo><mml:mo>.</mml:mo></mml:math> --></disp-formula>It is in fact very easy to see that any system (<xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref>) can be transformed into the system of <xref ref-type="disp-formula" rid="pcbi-0020165-e007">Equation 7</xref> with the help of feedback: set <italic>x</italic><sub>1</sub>(<italic>t</italic>) = <italic>z</italic>(<italic>t</italic>),<italic>x<sub>i</sub></italic><sub>+1</sub>(<italic>t</italic>) = <italic>z</italic><sup>(<italic>i</italic>)</sup> for <italic>i</italic> = 1,…,<italic>n</italic> − 1, and use the feedback <italic>v</italic>(<italic>t</italic>) = <italic>G</italic>(<bold>x</bold>(<italic>t</italic>)) + <italic>u</italic>(<italic>t</italic>) in <xref ref-type="disp-formula" rid="pcbi-0020165-e007">Equation 7</xref>. To prove that many other dynamical systems have the same universality property as this system (<xref ref-type="disp-formula" rid="pcbi-0020165-e007">Equation 7</xref>), it suffices to observe that feedback equivalence preserves this universality property.
					</p><p>We define the class <italic>S<sub>n</sub></italic> in the claim of Theorem 1 as the class of <italic>feedback linearizable</italic> systems, that is, the class of dynamical systems (<xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref>) that are feedback equivalent to some generic linear system. It can be proved (see Lemma in the section Definition of the Class <italic>S<sub>n</sub></italic>) that every feedback linearizable system (<xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref>) is also feedback equivalent to <xref ref-type="disp-formula" rid="pcbi-0020165-e007">Equation 7</xref>, and hence has the same universality property as <xref ref-type="disp-formula" rid="pcbi-0020165-e007">Equation 7</xref>.</p><p>We give in Definition of Class <italic>S<sub>n</sub></italic> a precise definition of the class <italic>S<sub>n</sub></italic> in terms of feedback equivalence (which is formally defined in Definition of Feedback Equivalence). We present in Details of the Proof of Theorem 1 a formal proof of the simulation result that is claimed in Theorem 1 (taking also initial conditions into account). In addition we formulate in the section A Characterization of <italic>S<sub>n</sub></italic> via Lie Brackets an equivalent criterion for a system (<xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref>) to belong to the class <italic>S<sub>n</sub>,</italic> which can be more easily tested for concrete cases of dynamical systems. This criterion makes use of the Lie bracket formalism that is briefly reviewed in Lie Brackets. Applications of this criterion to neural network equations are discussed in Applications to Neural Network Equations. In particular, we use this criterion to show that some dynamical systems (<xref ref-type="disp-formula" rid="pcbi-0020165-e006">Equation 6</xref>) that are defined by standard equations for recurrent neural circuits belong to the class <italic>S<sub>n</sub></italic>. We also show in Applications to Neural Network Equations that not all systems of the form (<xref ref-type="disp-formula" rid="pcbi-0020165-e006">Equation 6</xref>) belong to the class <italic>S<sub>n</sub>,</italic> rather it depends on the particular choice of parameters <italic>a<sub>ij</sub></italic> and <italic>b<sub>i</sub></italic> in <xref ref-type="disp-formula" rid="pcbi-0020165-e006">Equation 6</xref>.</p><p>Theorem 1 implies that a generic neural circuit may become through feedback a universal computational device, which cannot only simulate any Turing machine, but also any conceivable model for analog computing with bounded dynamic responses. The “program” of such an arbitrary simulated computing machine gets encapsulated in the static functions <italic>K</italic> that characterize the memoryless computational operations that are required from feedback units, and the static readout functions <italic>h</italic>. Since these functions are static, i.e., time-invariant, and continuous, they provide suitable targets for <italic>learning</italic>. More precisely, to train a generic neural circuit to simulate the dynamic response of an arbitrary dynamical system, it suffices to train—apart from readout neurons—a few neurons within the circuit (or within some external loop) to transform the vector <bold>x</bold>(<italic>t</italic>), which represents the current firing activity of its neurons, and the current external input <italic>u</italic>(<italic>t</italic>) into a suitable feedback value <italic>K</italic>(<bold>x</bold>(<italic>t</italic>),<italic>u</italic>(<italic>t</italic>)). This could, for example, be carried out by training a suitable feedforward neural network within the larger circuit, which can approximate any continuous feedback function <italic>K</italic> [<xref ref-type="bibr" rid="pcbi-0020165-b034">34</xref>]. Furthermore, we will show in Applications to Generic Cortical Microcircuit Models that these feedback functions <italic>K</italic> can in many biologically relevant cases be chosen to be linear, so that it would in principle suffice to train a single neuron to compute <italic>K</italic>.</p><p>It is known that the memory capacity of such a circuit is reduced to some finite number of bits if these feedback functions <italic>K</italic> are not learnt perfectly, or if there are other sources of noise in the system. More generally, no analog circuit with noise can simulate arbitrary Turing machines [<xref ref-type="bibr" rid="pcbi-0020165-b035">35</xref>]. But the subsequent Theorem 2 shows that fading-memory systems with noise and imperfect feedback can still achieve the maximal possible computational power within this a priori limitation: they can simulate any given finite state machine (FSM). Note that any Turing machine with tapes of finite length is a special case of a FSM. Furthermore, any existing digital computer is an FSM, hence the computational capability of FSMs is actually quite large.</p><p>To avoid the cumbersome mathematical difficulties that arise when one analyses differential equations with noise, we formulate and prove Theorem 2 on a more abstract level, resorting to the notion of fading-memory filters with noise (see Mathematical Definitions and Details to the Proof of Theorem 2). We assume here that the input–output behavior of those dynamical systems with noise, for which we want to determine the computational impact of (imprecise) state feedback, can be modeled by fading-memory filters with additive noise on their output. The assumption that the amplitude of this noise is bounded is a necessary assumption according to [<xref ref-type="bibr" rid="pcbi-0020165-b036">36</xref>]. We refer to [<xref ref-type="bibr" rid="pcbi-0020165-b004">4</xref>,<xref ref-type="bibr" rid="pcbi-0020165-b005">5</xref>,<xref ref-type="bibr" rid="pcbi-0020165-b037">37</xref>] for further discussions of the relationship between models for neural circuits and fading-memory filters. In particular it was shown in [<xref ref-type="bibr" rid="pcbi-0020165-b037">37</xref>] that every time-invariant fading-memory filter can be approximated by models for neural circuits, provided that these models reflect the empirically found diversity of time constants of neurons and synapses.</p><p><italic>Theorem 2.</italic> <italic>Feedback allows linear and nonlinear fading-memory systems, even in the presence of additive noise with bounded amplitude, to employ for real-time processing of time-varying inputs the computational capability and nonfading states of any given FSM (see <xref ref-type="fig" rid="pcbi-0020165-g001">Figure 1</xref>D–<xref ref-type="fig" rid="pcbi-0020165-g001">1</xref>E).</italic></p><p>A precise formalization of this result is formulated as Theorem 5 in Precise Statement of Theorem 2, and a formal proof of Theorem 5 is given in Proof of the Precise Statement of Theorem 2. The external input <italic>u</italic>(<italic>t</italic>) can in this case be injected directly into the fading-memory system, so that the feedback <italic>K</italic>(<bold>x</bold>(<italic>t</italic>)) depends only on the internal state <bold>x</bold>(<italic>t</italic>) (see <xref ref-type="fig" rid="pcbi-0020165-g001">Figure 1</xref>E). One essential ingredient of the proof is a method for making sure that noise does not get amplified through feedback: the functions <italic>K</italic> that provide feedback values <italic>K</italic>(<bold>x</bold>(<italic>t</italic>)) can be chosen in such a way that they cancel the impact of imprecision in the values <italic>K</italic>(<bold>x</bold>(<italic>s</italic>)) for immediately preceding time steps <italic>s</italic> &lt; <italic>t</italic>.</p></sec></sec><sec id="s2b"><title>Applications to Generic Cortical Microcircuit Models</title><p>We examine in this section computational aspects of feedback in recurrent circuits of spiking neurons that are based on data from cortical microcircuits. The dynamics of these circuits is substantially more complex than the dynamics of circuits described by <xref ref-type="disp-formula" rid="pcbi-0020165-e006">Equation 6</xref>, since it is based on action potentials (spikes) rather than on firing rates. Hence one can expect at best that the temporal dynamics of firing rates in these circuits of spiking neuron is qualitatively similar to that of circuits described by <xref ref-type="disp-formula" rid="pcbi-0020165-e006">Equation 6</xref>.</p><p>The preceding theoretical results imply that it is possible for dynamical systems to carry out computations with persistent memory without acquiring all the computational disadvantages of the chaotic regime, where the memory capacity of the system is dominated by noise. Feedback units can create selective “loopholes” into the fading-memory dynamics of a dissipative system that can only be activated by specific patterns in the input or circuit dynamics. In this way the potential content of persistent memory can be controlled by feedback units that have been trained to recognize such patterns. This feedback may arise from a few neurons within the circuit, or from neurons within a larger feedback loop. The task to approximate a suitable feedback function <italic>K</italic> is less difficult than it may appear on first sight, since it suffices in many cases to approximate a <italic>linear</italic> feedback function. The reason is that sufficiently large generic cortical microcircuit models have an inherent kernel property [<xref ref-type="bibr" rid="pcbi-0020165-b008">8</xref>], in the sense of machine learning [<xref ref-type="bibr" rid="pcbi-0020165-b038">38</xref>]. This means that a large reservoir of diverse nonlinear responses to current and recent input patterns is automatically produced within the recurrent circuit. In particular, nonlinear combinations of variables <italic>a</italic>,<italic>b</italic>,<italic>c</italic>,… (that may result from the circuit input or internal activity) are automatically computed at internal nodes of the circuit. Consequently, numerous low-degree polynomials in these variables <italic>a</italic>,<italic>b</italic>,<italic>c</italic>,… can be approximated by <italic>linear</italic> combinations of outputs of neurons from the recurrent circuit. An example of this effect is demonstrated in <xref ref-type="fig" rid="pcbi-0020165-g002">Figure 2</xref>G, where it is shown that the product of firing rates <italic>r</italic><sub>3</sub>(<italic>t</italic>) and <italic>r</italic><sub>4</sub>(<italic>t</italic>) and of two independently varying afferent spike train inputs can be approximated quite well by a linear readout neuron. The kernel property of biologically realistic cortical microcircuit models is apparently supported by the fact that these circuits have many additional nonlinearities in addition to those that appear in <xref ref-type="disp-formula" rid="pcbi-0020165-e001">Equations 1</xref>, <xref ref-type="disp-formula" rid="pcbi-0020165-e002">2</xref>, and <xref ref-type="disp-formula" rid="pcbi-0020165-e006">6</xref>.</p><fig id="pcbi-0020165-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020165.g002</object-id><label>Figure 2</label><caption><title>State-Dependent Real-Time Processing of Four Independent Input Streams in a Generic Cortical Microcircuit Model</title><p>(A) Four input streams, each consisting of eight spike trains generated by Poisson processes with randomly varying rates <italic>r<sub>i</sub></italic>(<italic>t</italic>),<italic>i</italic> = 1,…4, rates plotted in (B); all rates are given in Hz. The four input streams and the feedback were injected into disjoint sets of neurons in the circuit.</p><p>(C) Resulting firing activity of 100 out of the 600 I&amp;F neurons in the circuit. Spikes from inhibitory neurons marked in red.</p><p>(D) Target activation times of the high-dimensional attractor (blue shading), spike trains of two of the eight I&amp;F neurons that were trained to create the high-dimensional attractor by sending their output spike trains back into the circuit, and average firing rate of all eight neurons (lower trace).</p><p>(E,F) Performance of linear readouts that were trained to switch their real-time computation task depending on the current state of the high-dimensional attractor: output 2 · <italic>r</italic><sub>3</sub>(<italic>t</italic>) instead of <italic>r</italic><sub>3</sub>(<italic>t</italic>) if the high-dimensional attractor is on (E), output <italic>r</italic><sub>3</sub>(<italic>t</italic>) + <italic>r</italic><sub>4</sub>(<italic>t</italic>) instead of | <italic>r</italic><sub>3</sub>(<italic>t</italic>) – <italic>r</italic><sub>4</sub>(<italic>t</italic>) | if the high-dimensional attractor is on (F).</p><p>(G) Performance of linear readout that was trained to output <italic>r</italic><sub>3</sub>(<italic>t</italic>) · <italic>r</italic><sub>4</sub>(<italic>t</italic>), showing that another linear readout from the same circuit can simultaneously carry out nonlinear computations that are invariant to the current state of the high-dimensional attractor.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020165.g002" xlink:type="simple"/></fig><p>One formal difference between neurons in the mean field model (<xref ref-type="disp-formula" rid="pcbi-0020165-e006">Equation 6</xref>) and more realistic models for spiking neurons is that the input to a neuron of the latter type consists of postsynaptic potentials, rather than of firing rates. Hence the time-varying input <bold>x</bold>(<italic>t</italic>) to a readout neuron is in this section not a vector of time-varying firing rates, but a smoothed version of the spike trains of all presynaptic neurons. This smoothing is achieved through application of a linear filter with an exponentially decaying kernel, whose time constant of 30 ms models time constants of receptors and postsynaptic membrane of a readout neuron in a qualitative fashion. Thus, if <bold>w</bold> is a vector of synaptic weights, then <bold>w</bold> · <bold>x</bold>(<italic>t</italic>) models the impact of the firing activity of presynaptic neurons on the membrane potential of a readout neuron.</p><p>We refer in the following to those neurons where the weights of synaptic connections from neurons within the circuit are adapted for a specific computational task (rather than chosen randomly from distributions that are based on biological data, as for all other synapses in the circuit) as <italic>readout neurons</italic>. The output of a readout neuron was modeled in most of our simulations simply by a weighted sum <bold>w</bold> · <bold>x</bold>(<italic>t</italic>) of the previously described vector <bold>x</bold>(<italic>t</italic>). Such output can be interpreted as the time-varying firing rate of a readout neuron. However, we show in <xref ref-type="fig" rid="pcbi-0020165-g002">Figure 2</xref> that these readout neurons can (with a moderate loss in performance) also be modeled by spiking neurons, exactly like the other neurons in the simulated circuit. This demonstrates that not only those circuits that receive feedback from external readout neurons, but also generic recurrent circuits in which a few neurons have been trained for a specific task, acquire computational capabilities for real-time processing that are not restricted to computations with fading memory.</p><p>Theorem 2 suggests that the training of a few of its neurons enables generic neural circuits to employ persistent internal states for state-dependent processing of online input streams. Previous models for nonfading memory in neural circuits [<xref ref-type="bibr" rid="pcbi-0020165-b013">13</xref>,<xref ref-type="bibr" rid="pcbi-0020165-b039">39</xref>–<xref ref-type="bibr" rid="pcbi-0020165-b041">41</xref>] proposed that it is implemented through low-dimensional attractors in the circuit dynamics. These attractors tend to freeze or to entrain the whole state of the circuit, and thereby shut it off from the online input stream (although independent local attractors could emerge in local subcircuits under some conditions [<xref ref-type="bibr" rid="pcbi-0020165-b040">40</xref>]). In contrast, the generation of nonfading memory through a few trained neurons does not entail that the dynamics of the circuit be dominated by their persistent memory states. For example, when a readout neuron gives during some time interval a constant feedback <italic>K</italic>(<bold>x</bold>(<italic>t</italic>)) = <italic>c,</italic> this only constrains the circuit state <bold>x</bold>(<italic>t</italic>) to remain in the sub-manifold {<bold>x:</bold> <italic>K</italic>(<bold>x</bold>) = <italic>c</italic>} of its high-dimensional state space. This sub-manifold is in general high-dimensional. In particular, if <italic>K</italic>(<bold>x</bold>) is a linear function w · x, which often suffices as we will show; the dimensionality of the sub-manifold {<bold>x:</bold> <italic>K</italic>(<bold>x</bold>) = <italic>c</italic>} differs from the dimension of the full state space only by 1. Hence several such sub-manifolds have in general a high-dimensional intersection, and their intersection still leaves sufficiently many degrees of freedom for the circuit state <bold>x</bold>(<italic>t</italic>) to also absorb continuously new information from online input streams. These sub-manifolds are in general not attractors in a strict mathematical sense. Rather, their effective attraction property (or noise-robustness) results from the subsequently described training process (“teacher forcing”). This training process produces weights <bold>w</bold> which have the property that the resulting feedback <inline-formula id="pcbi-0020165-ex001"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex001" xlink:type="simple"/></inline-formula>
					 moves on a trajectory of circuit states that goes through states 
					<bold>x~</bold>(<italic>t</italic>) in the neighborhood of the sub-manifold {<bold>x:</bold> <italic>K</italic>(<bold>x</bold>) = <italic>c</italic>}, closer to this sub-manifold.
				</p><p>We simulated generic cortical microcircuit models consisting of 600 integrate-and-fire (I&amp;F) neurons (for <xref ref-type="fig" rid="pcbi-0020165-g002">Figures 2</xref> and <xref ref-type="fig" rid="pcbi-0020165-g003">3</xref>), and circuits consisting of 600 Hodgkin–Huxley (HH) neurons (for <xref ref-type="fig" rid="pcbi-0020165-g004">Figure 4</xref>), in either case with a rather high level of noise that reflects experimental data on the high conductance state in vivo [<xref ref-type="bibr" rid="pcbi-0020165-b042">42</xref>]. These circuits were not constructed for any particular computational task. In particular, sparse synaptic connectivity between neurons was generated (with a biologically realistic bias towards short connections) by a probabilistic rule. Synaptic parameters were chosen randomly from distributions that depend on the type of pre- and postsynaptic neurons (in accordance with empirical data from [<xref ref-type="bibr" rid="pcbi-0020165-b043">43</xref>,<xref ref-type="bibr" rid="pcbi-0020165-b044">44</xref>]). More precisely, we used biologically realistic models for dynamic synapses whose individual mixture of paired-pulse depression and facilitation (depending on the type of pre- and postsynaptic neuron) was based on these data. It has previously been shown in [<xref ref-type="bibr" rid="pcbi-0020165-b006">6</xref>,<xref ref-type="bibr" rid="pcbi-0020165-b008">8</xref>] that the presence of such dynamic synapses extends the timespan of the inherent fading memory of the circuit. However the computational tasks that are considered in this paper require, apart from a nonfading memory, only a fading memory with a rather short timespan (to make the estimation of the current firing rate of input spike trains feasible). Therefore, the biologically more realistic dynamic synapses could be replaced in this model by simple static synapses, without a change in the performance of the circuit for the subsequently considered tasks. All details of the simulated microcircuit models can be found in Details of the Cortical Microcircuit Models. Details of the subsequently discussed computer experiments are given in the sections Technical Details of <xref ref-type="fig" rid="pcbi-0020165-g005">Figure 5</xref>, Technical Details of <xref ref-type="fig" rid="pcbi-0020165-g002">Figure 2</xref>, and Technical Details of <xref ref-type="fig" rid="pcbi-0020165-g003">Figure 3</xref>.</p><fig id="pcbi-0020165-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020165.g003</object-id><label>Figure 3</label><caption><title>Representation of Time for Behaviorally Relevant Timespans in a Generic Cortical Microcircuit Model</title><p>(A) Afferent circuit input, consisting of a cue in one channel (red) and random spikes (freshly drawn for each trial) in the other channels.</p><p>(B) Response of 100 neurons from the same circuit as in <xref ref-type="fig" rid="pcbi-0020165-g002">Figure 2</xref>, which has here two coexisting high-dimensional attractors. The autonomously generated periodic bursts with a periodic frequency of about 8 Hz are not related to the task, and readouts were trained to become invariant to them.</p><p>(C,D) Feedback from two linear readouts that were simultaneously trained to create and control two high-dimensional attractors. One of them was trained to decay in 400 ms (C), and the other in 600 ms (D) (scale in nA is the average current injected by feedback into a randomly chosen subset of neurons in the circuit).</p><p>(E) Response of the same neurons as in (B), for the same circuit input, but with feedback from a different linear readout that was trained to create a high-dimensional attractor that increases its activity and reaches a plateau of 600 ms after the occurrence of the cue in the input stream.</p><p>(F) Feedback from the linear readout that creates this continuous high-dimensional attractor.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020165.g003" xlink:type="simple"/></fig><fig id="pcbi-0020165-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020165.g004</object-id><label>Figure 4</label><caption><title>A Model for Analog Real-Time Computation on External and Internal Variables in a Generic Cortical Microcircuit (Consisting of 600 Conductance-Based HH Neurons)</title><p>(A,B) Two input streams as in <xref ref-type="fig" rid="pcbi-0020165-g002">Figure 2</xref>.</p><p>(B) Their firing rates <italic>r</italic><sub>1</sub>(<italic>t</italic>),<italic>r</italic><sub>2</sub>(<italic>t</italic>).</p><p>(C) Resulting firing activity of 100 neurons in the circuit.</p><p>(D) Performance of a neural integrator, generated by feedback from a linear readout that was trained to output at any time <italic>t</italic> an approximation <italic>CA</italic>(<italic>t</italic>) of the integral<inline-formula id="pcbi-0020165-ex037"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex037" xlink:type="simple"/></inline-formula>
							 over the difference of both input rates. Feedback values were injected as input currents into a randomly chosen subset of neurons in the circuit. Scale in nA shows average strength of feedback currents, also in (H).
						</p><p>(E) Performance of linear readout that was trained to output 0 as long as <italic>CA</italic>(<italic>t</italic>) stayed below 0.83 nA, and to output <italic>r</italic><sub>2</sub>(<italic>t</italic>) once <italic>CA</italic>(<italic>t</italic>) had crossed this threshold, as long as <italic>CA</italic>(<italic>t</italic>) stayed above 0.66 nA (i.e., in this test run during the shaded time periods).</p><p>(F) Performance of linear readout trained to output <italic>r</italic><sub>1</sub>(<italic>t</italic>) − <italic>CA</italic>(<italic>t</italic>), i.e., a combination of external and internal variables, at any time <italic>t</italic> (both <italic>r</italic><sub>1</sub> and <italic>CA</italic> normalized into the range [0,1]).</p><p>(G) Response of a randomly chosen neuron in the circuit for ten repetitions of the same experiment (with input spike trains generated by Poisson processes with the same time course of firing rates), showing biologically realistic trial-to-trial variability.</p><p>(H) Activity traces of a continuous attractor as in (D), but in eight different trials for eight different fixed values of <italic>r</italic><sub>1</sub> and <italic>r</italic><sub>2</sub> (shown on the right).</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020165.g004" xlink:type="simple"/></fig><fig id="pcbi-0020165-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020165.g005</object-id><label>Figure 5</label><caption><title>Organization of Input and Output Streams for the Three Computational Tasks Considered in the Computer Simulations</title><p>Each input stream consisted of multiple spike trains that provided synaptic inputs to individually chosen subsets of neurons in the recurrent circuit (which is indicated by a gray rectangle).</p><p>(A,C) Input streams consisted of multiple Poisson spike trains with a time-varying firing rate <italic>r</italic><sub>i</sub>(<italic>t</italic>).</p><p>(B) Input consisted of a burst (“cue”) in one spike train (which marks the beginning of a time interval) and independent Poisson spike train (“noise”) in the other input channels.</p><p>(A–C) The actual outputs of the readouts (that were trained individually for each computational task) is shown in <xref ref-type="fig" rid="pcbi-0020165-g002">Figures 2</xref>–<xref ref-type="fig" rid="pcbi-0020165-g004">4</xref>.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020165.g005" xlink:type="simple"/></fig><p>We tested three different types of computational tasks for generic neural circuits with feedback. The same neural circuit can be used for each task, only the organization of input and output streams needs to be chosen individually (see <xref ref-type="fig" rid="pcbi-0020165-g005">Figure 5</xref>). The following procedure was applied to train readout neurons, i.e., to adjust the weights of synaptic connections from neurons in the circuit to readout neurons for specific computational tasks (while leaving all other parameters of the generic microcircuit model unchanged): 1) first those readout neurons were trained that provide feedback, then the other readout neurons; 2) during the training of readout neurons that provide feedback, their actual feedback was replaced by a <italic>noisy</italic> version of their target output (“teacher forcing”); 3) each readout neuron was trained by linear regression to output at any time <italic>t</italic> a particular target value <italic>f</italic>(<italic>t</italic>). Linear regression was applied to a set of datapoints of the form 〈<bold>x</bold>(<italic>t</italic>),<italic>f</italic>(<italic>t</italic>)〉 for many timepoints <italic>t,</italic> where <bold>x</bold>(<italic>t</italic>) is a smoothed version of the spike trains of presynaptic neurons (as defined before).</p><p>Note that teacher forcing, with noisy versions of target feedback values, trains these readouts to correct errors resulting from imprecision in their preceding feedback (rather than amplifying errors). This training procedure is responsible for the robustness of the dynamics of the resulting closed-loop circuits, in particular for the “attractor” properties of the effectively resulting high-dimensional attractors.</p><p>In our first computer experiment, readout neurons were trained to turn a high-dimensional attractor on or off (<xref ref-type="fig" rid="pcbi-0020165-g002">Figure 2</xref>D), in response to bursts in two of the four independent input spike trains. More precisely, eight neurons were trained to represent in their firing activity at any time the information: in which of the input streams, 1 or 2, had a burst most recently occurred? If it had occurred most recently in stream 1, they were trained to fire at 40 Hz, and if a burst had occurred most recently in input stream 2, they were trained not to fire. Hence these neurons were required to represent the nonfading state of a simple FSM, demonstrating in an example the computational capabilities predicted by Theorem 2. <xref ref-type="fig" rid="pcbi-0020165-g002">Figure 2</xref>G demonstrates that the circuit retains its kernel property in spite of the feedback injected into the circuit by these readouts. But beyond the emulation of a simple FSM, the resulting generic cortical microcircuit is able to combine information stored in the current state of the FSM with new information from the online circuit input. For example, <xref ref-type="fig" rid="pcbi-0020165-g002">Figure 2</xref>E shows that other readouts from the same circuit can be trained to amplify their response to specific inputs if the high-dimensional attractor is in the “on” state. Readouts can also be trained to change the function that they compute if the high-dimensional attractor is in the on state (<xref ref-type="fig" rid="pcbi-0020165-g002">Figure 2</xref>F). This provides an example for an online reconfigurable circuit. The readout neurons that provide feedback had been modeled in this computer simulation like the other neurons in the circuit: by I&amp;F neurons with in vivo–like background noise. Hence they can be viewed equivalently as neurons <italic>within</italic> an otherwise generic circuit.</p><p>Another difficult problem in computational neuroscience is to explain how neural circuits can implement a parametric memory, i.e., how they can hold and update an <italic>analog</italic> value that may represent, for example, an intended eye position that a neural integrator computes from a sequence of eye-movement commands [<xref ref-type="bibr" rid="pcbi-0020165-b045">45</xref>], an estimate of elapsed time [<xref ref-type="bibr" rid="pcbi-0020165-b009">9</xref>], or accumulated sensory evidence [<xref ref-type="bibr" rid="pcbi-0020165-b014">14</xref>]. Various designs have been proposed for parametric memory in recurrent circuits, where continuous attractors (also referred to as line attractors) hold and update an analog value. But these approaches are inherently brittle [<xref ref-type="bibr" rid="pcbi-0020165-b041">41</xref>], and have problems in dealing with high noise or online circuit inputs. On the other hand, <xref ref-type="fig" rid="pcbi-0020165-g003">Figure 3</xref> shows that dedicated circuit constructions are not necessary, since feedback from readout neurons in <italic>generic</italic> cortical microcircuits models can also create high-dimensional attractors that hold and update an <italic>analog</italic> value for behaviorally relevant timespans. In fact, due to the high-dimensional character of the resulting high-dimensional attractors, two such analog values can be stored and updated independently (<xref ref-type="fig" rid="pcbi-0020165-g003">Figure 3</xref>C and <xref ref-type="fig" rid="pcbi-0020165-g003">3</xref>D), even within a fairly small circuit. In this example, the readouts that provide feedback were simply trained to increase or reduce their feedback at each timepoint. Note that the resulting circuit activity is qualitatively consistent with recordings from neurons in cortex and striatum during reward expectation [<xref ref-type="bibr" rid="pcbi-0020165-b010">10</xref>–<xref ref-type="bibr" rid="pcbi-0020165-b012">12</xref>]. A similar ramp-like rise and fall of activity as shown in <xref ref-type="fig" rid="pcbi-0020165-g003">Figure 3</xref>C, <xref ref-type="fig" rid="pcbi-0020165-g003">3</xref>D, and 3F has also been recorded in neurons of posterior parietal cortex of the macaque in experiments where the monkey had been trained to classify the duration of elapsed time [<xref ref-type="bibr" rid="pcbi-0020165-b009">9</xref>]. The high dimensionality of the continuous attractors in this model makes it feasible to constrain the circuit state to stay simultaneously in more than one continuous attractor, thereby making it in principle possible to encode complex movement plans that require specific temporal relationships between individual motor commands.</p><p>Our model for parametric memory in cortical circuits is consistent with high noise: <xref ref-type="fig" rid="pcbi-0020165-g004">Figure 4</xref>G shows the typical trial-to-trial variability of a neuron in our simulated circuit of HH neurons with in vivo–like background noise. It qualitatively matches the “wide diversity of neural firing drift patterns in individual fish at all states of tuning” that was observed in the horizontal occulomotor neural integrator in goldfish [<xref ref-type="bibr" rid="pcbi-0020165-b015">15</xref>], and the large trial-to-trial variability of neurons in prefrontal cortex of monkeys reported in [<xref ref-type="bibr" rid="pcbi-0020165-b010">10</xref>]. In addition, this model is consistent with the surprising plasticity that has been observed even in quite specialized neural integrators [<xref ref-type="bibr" rid="pcbi-0020165-b015">15</xref>], since continuous attractors can be created or modified in this model by changing just a few synaptic weights of neurons that are immediately involved. It does not require the presence of long-lasting postsynaptic potentials, NMDA receptors, or other specialized details of biological neurons or synapses, although their inclusion in the model is likely to provide additional temporal stability [<xref ref-type="bibr" rid="pcbi-0020165-b013">13</xref>]. Rather it points to complementary organizational mechanisms on the circuit level, which are likely to enhance the controllability and robustness of continuous attractors in neural circuits. The robustness of this learning-based model can be traced back to the fact that readout neurons can be trained to correct undesired circuit responses resulting from errors in their previous feedback. Furthermore, such error correction is not restricted to linear computational operations, since the previously demonstrated kernel property of these generic circuits allows even linear neurons to implement complex nonlinear control strategies through their feedback. As an example, we demonstrate in <xref ref-type="fig" rid="pcbi-0020165-g004">Figure 4</xref> that even under biologically realistic high-noise conditions a linear readout can be trained to update a continuous attractor (<xref ref-type="fig" rid="pcbi-0020165-g004">Figure 4</xref>D), to filter out input activity during certain time intervals independent of the current state of the continuous attractor (<xref ref-type="fig" rid="pcbi-0020165-g004">Figure 4</xref>E), or to combine the time-varying analog variable encoded by the current state <italic>CA</italic>(<italic>t</italic>) of the continuous attractor with a time-varying variable <italic>r</italic><sub>1</sub>(<italic>t</italic>) that is delivered by an online spike input. Hence, intention-based information processing [<xref ref-type="bibr" rid="pcbi-0020165-b016">16</xref>] and other tasks that involve a merging of external inputs and internal state information can be implemented in this way. <xref ref-type="fig" rid="pcbi-0020165-g004">Figure 4</xref>C shows that a high-dimensional attractor need not entrain the firing activity of neurons in a drastic way, since it just restricts the high-dimensional–circuit dynamics <bold>x</bold>(<italic>t</italic>) to a slightly lower dimensional manifold of circuit states <bold>x</bold>(<italic>t</italic>) that satisfy <bold>w</bold> · <bold>x</bold>(<italic>t</italic>) = <italic>f</italic>(<italic>t</italic>) for the current target output <italic>f</italic>(<italic>t</italic>) of the corresponding linear readout. On the other hand, <xref ref-type="fig" rid="pcbi-0020165-g004">Figure 4</xref>E shows that the activity level <italic>CA</italic>(<italic>t</italic>) of the high-dimensional attractor can nevertheless be detected by other linear readouts, and can simultaneously be combined in a nonlinear manner with a time-varying variable <italic>r</italic><sub>2</sub>(<italic>t</italic>) from one afferent circuit input stream, while remaining invariant to the other afferent input stream.</p><p>Finally, the same generic circuit also provides a model for the integration of evidence for decision making that is compatible with in vivo–like high noise conditions. <xref ref-type="fig" rid="pcbi-0020165-g004">Figure 4</xref>H depicts the timecourse of the same neural integrator as in <xref ref-type="fig" rid="pcbi-0020165-g004">Figure 4</xref>D, but here for the case where the rates <italic>r</italic><sub>1</sub>,<italic>r</italic><sub>2</sub> of the two input streams assume in eight trials eight different constant values after the first 100 ms (while assuming a common value of 65 Hz during the first 100 ms). The resulting timecourse of the continuous attractor is qualitatively similar to the meandering path towards a decision threshold that has been recorded from neurons in area LIP where firing rates represent temporally integrated evidence concerning the dominating direction of random dot movements (see <xref ref-type="fig" rid="pcbi-0020165-g004">Figure 4</xref>A in [<xref ref-type="bibr" rid="pcbi-0020165-b014">14</xref>]).</p></sec></sec><sec id="s3"><title>Discussion</title><p>We have presented a theoretically founded model for real-time computations on complex input streams with persistent internal states in generic cortical microcircuits. This model does not require a handcrafted circuit structure or biologically unrealistic assumptions such as symmetric weight distributions, static synapses that do not exhibit pair-pulsed depression or facilitation, or neuron models with low levels of noise that are not consistent with data on in vivo conditions. Our model only requires the assumption that adaptive procedures (synaptic plasticity) in generic neural circuits can approximate linear regression. Furthermore, in contrast to classical learning paradigms for attractor neural networks, it is here not required that a large fraction of synaptic parameters in the circuit are changed when a new computational task is introduced or a new item is stored in working memory. Rather, it suffices if those neurons that provide the circuit output and a few neurons that provide feedback are subject to synaptic plasticity. Such minimal circuit modifications have the advantage that thereby created attractors of the circuit dynamics are high-dimensional. We have shown that the circuit state can simultaneously be in several of such high-dimensional attractors, and still retain sufficiently many degrees of freedom to absorb and process new information from online input streams. In particular, we have shown in <xref ref-type="fig" rid="pcbi-0020165-g002">Figures 2</xref> and <xref ref-type="fig" rid="pcbi-0020165-g004">4</xref> how bottom-up processing can be reconfigured dependent on discrete internal states (implemented through high-dimensional attractors) by turning certain input channels on or off, and by changing the computational operations that are applied to input variables. Furthermore we have shown in <xref ref-type="fig" rid="pcbi-0020165-g004">Figure 4</xref> that analog variables, which are extracted from an online input stream, can be combined in real-time computations with analog variables that are stored in high-dimensional continuous attractors. This provides in particular a model for the implementation of intention-based information processing [<xref ref-type="bibr" rid="pcbi-0020165-b016">16</xref>] in cortical microcircuits.</p><p>It remains open how learning signals can induce neurons in a biological organism to compute specific linear feedback functions. But at least we have reduced this problem to the feasibility of perceptron-like learning (or more abstractly: to linear regression) for single neurons. Subsequent research will have to determine whether these learning requirements (which can be partially reduced to spike-timing dependent plasticity [<xref ref-type="bibr" rid="pcbi-0020165-b046">46</xref>]) can be justified on the basis of results on unsupervised learning and reinforcement learning [<xref ref-type="bibr" rid="pcbi-0020165-b047">47</xref>] in biological organisms.</p><p>Whereas it was previously already known that one can construct specific circuits that have universal computational capabilities for real-time computing on analog input streams, Theorems 1 and 2 of this article imply that a large variety of dynamical systems (in particular generic cortical microcircuits) can acquire through feedback such universal capabilities for computations that map time-varying inputs to time-varying outputs. It should be noted that these universal computational capabilities differ from the well-known but much weaker universal approximation property of feedforward neural networks (see [<xref ref-type="bibr" rid="pcbi-0020165-b034">34</xref>]), since not only the static output of an arbitrary continuous static function is approximated, but also the dynamic response of arbitrary differential equations of higher-order to time-varying inputs.</p><p>The theoretical results of this article also provide an explanation for the astounding computational capability and flexibility of echo state networks [<xref ref-type="bibr" rid="pcbi-0020165-b017">17</xref>]. In addition they can be used to analyze computational aspects of feedback in other biological dynamical systems besides neural circuits. Several such systems, for example, genetic regulatory networks, are known to implement complex maps from time-varying input streams (e.g., external signals) onto time-varying outputs (e.g., transcription rates). But little is known about the way in which these maps are implemented. Whereas feedback in biological dynamical systems is usually only analyzed and modeled from the perspective of control, we propose that an analysis of its computational aspects is likely to yield a better understanding of the computational capabilities of such systems.</p></sec><sec id="s4"><title>Materials and Methods</title><sec id="s4a"><title>Mathematical definitions, details to the proof of Theorem 1, and examples.</title><p><italic>Definition of feedback equivalence.</italic> We recall that a <italic>smooth</italic> mapping is one for which derivatives of all orders exist (infinite differentiability), and that a <italic>diffeomorphism</italic> T: <italic>T</italic>: ℜ<italic><sup>n</sup></italic> → ℜ<italic><sup>n</sup></italic> is a smooth mapping for which there exists a well-defined smooth inverse <italic>T</italic><sup>−1</sup>: ℜ<italic><sup>n</sup></italic> → ℜ<italic><sup>n</sup></italic>.</p><p><underline>Definition</underline> (see [<xref ref-type="bibr" rid="pcbi-0020165-b033">33</xref>], Definition 5.3.1). Two <italic>n</italic>-dimensional systems <bold>x</bold>′ = <italic>f</italic>(<bold>x</bold>) + <italic>g</italic>(<bold>x</bold>)<italic>v</italic> and <inline-formula id="pcbi-0020165-ex002"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex002" xlink:type="simple"/></inline-formula>
					 (with smooth vector fields <inline-formula id="pcbi-0020165-ex003"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex003" xlink:type="simple"/></inline-formula>
					 <inline-formula id="pcbi-0020165-ex004"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex004" xlink:type="simple"/></inline-formula>
					 <inline-formula id="pcbi-0020165-ex005"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex005" xlink:type="simple"/></inline-formula>
					 <inline-formula id="pcbi-0020165-ex006"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex006" xlink:type="simple"/></inline-formula>
					are called <italic>feedback equivalent</italic> (over the state space ℜ<italic><sup>n</sup></italic>) if there exists 1) a diffeomorphism <italic>T</italic>: ℜ<italic><sup>n</sup></italic> → ℜ<italic><sup>n</sup></italic>, and 2) smooth maps <italic>α</italic>,<italic>β</italic>: ℜ<italic><sup>n</sup></italic> → ℜ with <italic>β</italic>(<bold>x</bold>) ≠ 0 for all <bold>x</bold> ∈ ℜ<italic><sup>n</sup></italic>, such that, for each <bold>x</bold> ∈ ℜ<italic><sup>n</sup></italic>
					<disp-formula id="pcbi-0020165-e076"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e076" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>T</mml:mi><mml:mo>&lowast;</mml:mo></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>)</mml:mo><mml:mi>&alpha;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mover accent='true'><mml:mi>f</mml:mi><mml:mo>&tilde;</mml:mo></mml:mover><mml:mo stretchy='false'>(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>)</mml:mo></mml:math> --></disp-formula>and
					<disp-formula id="pcbi-0020165-e014"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e014" xlink:type="simple"/><!-- <mml:math display='block'><mml:mi>&beta;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>)</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mo>&lowast;</mml:mo></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>)</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mover accent='true'><mml:mi>g</mml:mi><mml:mo>&tilde;</mml:mo></mml:mover><mml:mo stretchy='false'>(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>)</mml:mo></mml:math> --></disp-formula>(where <italic>T</italic><sub>*</sub> denotes the Jacobian of <italic>T</italic>).
				</p><p><italic>Definition of the class S<sub>n</sub>.</italic> Recall that a linear system <bold>x′</bold> = <bold>Ax</bold> + <bold>b</bold><italic>u</italic> is <italic>controllable</italic> if it is possible to drive any state <bold>x</bold><sub>0</sub> to any other state <bold>x</bold><sub>1</sub> using an input (see [<xref ref-type="bibr" rid="pcbi-0020165-b033">33</xref>], Definition 3.1.6). Controllability is a generic property of systems, and amounts to the requirement that the matrix (<bold>b</bold>,<bold>Ab</bold>,…,<bold>A</bold><italic><sup>n</sup></italic><sup>−1</sup><bold>b</bold>) has full rank, where <italic>n</italic> is the dimension of the system (see [<xref ref-type="bibr" rid="pcbi-0020165-b033">33</xref>], Theorem 2). Note that the linear system (<xref ref-type="disp-formula" rid="pcbi-0020165-e007">Equation 7</xref>) satisfies this requirement, and hence is controllable.</p><p>We take <italic>S<sub>n</sub></italic> to be the class of <italic>n</italic>-dimensional systems (<xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref>) that are <italic>(globally) feedback linearizable</italic>, that is to say, the systems (<xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref>) for which there exists some linear controllable system that is feedback equivalent to <xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref> (see [<xref ref-type="bibr" rid="pcbi-0020165-b033">33</xref>], Definition 5.3.2).</p><p>An <italic>n</italic>-dimensional system is feedback linearizable if and only if it is feedback-equivalent to the system (<xref ref-type="disp-formula" rid="pcbi-0020165-e007">Equation 7</xref>) (see [<xref ref-type="bibr" rid="pcbi-0020165-b033">33</xref>], Lemma 5.3.5). Therefore, we have the following:</p><p><underline>Lemma</underline>: <italic>A system (<xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref>), with smooth vector fields f = 〈f</italic><sub>1</sub>,…,<italic>f<sub>n</sub>〉 and g =</italic> 〈<italic>g</italic><sub>1</sub>,…,<italic>g<sub>n</sub></italic>〉 <italic>belongs to S<sub>n</sub> if and only if there exists a diffeomorphism T</italic>: ℜ<italic><sup>n</sup></italic> → ℜ<italic><sup>n</sup> and two smooth maps α</italic>,<italic>β</italic>: ℜ<italic><sup>n</sup></italic> → ℜ<italic>, with β</italic>(<bold>x</bold>) ≠ 0 <italic>for all</italic> <bold>x</bold> ∈ ℜ<italic><sup>n</sup>, such that, for each</italic> <bold>x</bold> ∈ ℜ<italic><sup>n</sup></italic>:
					<disp-formula id="pcbi-0020165-e008"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e008" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>T</mml:mi><mml:mo>&lowast;</mml:mo></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mi>f</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo>&equals;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>A</mml:mi></mml:mstyle><mml:mi>n</mml:mi></mml:msub><mml:mi>T</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>)</mml:mo><mml:mo>&minus;</mml:mo><mml:mfrac><mml:mi>&alpha;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>)</mml:mo><mml:mi>&beta;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>)</mml:mo></mml:mfrac><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>b</mml:mi></mml:mstyle><mml:mi>n</mml:mi></mml:msub></mml:math> --></disp-formula><italic>and</italic>
					<disp-formula id="pcbi-0020165-e009"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e009" xlink:type="simple"/><!-- <mml:math display='block'><mml:mi>&beta;</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mi>T</mml:mi><mml:mo>*</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>&equals;</mml:mo><mml:msub><mml:mi mathvariant="bold">b</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:math> --></disp-formula><italic>where T<sub>*</sub> denotes the Jacobian of T and</italic>
					<disp-formula id="pcbi-0020165-e017"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e017" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>A</mml:mi></mml:mstyle><mml:mi>n</mml:mi></mml:msub><mml:mtext>&thinsp;</mml:mtext><mml:mo>:</mml:mo><mml:mo>&equals;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo stretchy='true'>(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>&hellip;</mml:mo></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mo>&hellip;</mml:mo></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&vellip;</mml:mo></mml:mtd><mml:mtd><mml:mo>&vellip;</mml:mo></mml:mtd><mml:mtd><mml:mo>&vellip;</mml:mo></mml:mtd><mml:mtd><mml:mo>&dellip;</mml:mo></mml:mtd><mml:mtd><mml:mo>&vellip;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>&hellip;</mml:mo></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>&hellip;</mml:mo></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy='true'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mtext>&thinsp;</mml:mtext><mml:mtext>&thinsp;</mml:mtext><mml:mtext>&thinsp;</mml:mtext><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>b</mml:mi></mml:mstyle><mml:mi>n</mml:mi></mml:msub><mml:mtext>&thinsp;</mml:mtext><mml:mo>:</mml:mo><mml:mo>&equals;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo stretchy='true'>(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&vellip;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mtext>&thinsp;</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy='true'>)</mml:mo><mml:mo>.</mml:mo></mml:math> --></disp-formula>
				</p><p>An interpretation of the property given in the above Lemma, that will be used in the proof of Theorem 1 in the section Details to the Proof of Theorem 1, is as follows (see [<xref ref-type="bibr" rid="pcbi-0020165-b033">33</xref>], Chapter 5, for more discussion): For each input <italic>μ</italic>(<italic>t</italic>) and each solution <italic>z</italic>(<italic>t</italic>) of
					<disp-formula id="pcbi-0020165-e018"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e018" xlink:type="simple"/><!-- <mml:math display='block'><mml:msup><mml:mi>z</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:msup><mml:mo>&equals;</mml:mo><mml:mi>&mu;</mml:mi><mml:mo>,</mml:mo></mml:math> --></disp-formula>the vector function <bold>x</bold>(<italic>t</italic>) = <italic>T</italic><sup>−1</sup>(<italic>Z</italic>(<italic>t</italic>)) satisfies <xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref> with the input <italic>v</italic>(<italic>t</italic>) = <italic>α</italic>(<bold>x</bold>(<italic>t</italic>)) + <italic>β</italic>(<bold>x</bold>(<italic>t</italic>))<italic>μ</italic>(<italic>t</italic>), where
					<disp-formula id="pcbi-0020165-e019"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e019" xlink:type="simple"/><!-- <mml:math display='block'><mml:mi>Z</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>&Prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:mo>&hellip;</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>n</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy='false'>)</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>)</mml:mo></mml:math> --></disp-formula>
				</p><p><italic>Details to the proof of Theorem 1.</italic> In this section, we prove the simulation result that is claimed in Theorem 1.</p><p>Take any system (<xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref>) in <italic>S<sub>n</sub></italic> and any system (<xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref>) to be simulated. Using <italic>T</italic>,<italic>α</italic>,<italic>β</italic> as in the Lemma in section Definition of the Class <italic>S<sub>n</sub></italic> that characterizes the class <italic>S<sub>n</sub></italic>, we define:
					<disp-formula id="pcbi-0020165-e020"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e020" xlink:type="simple"/><!-- <mml:math display='block'><mml:mi>K</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mo>)</mml:mo><mml:mo>:</mml:mo><mml:mo>&equals;</mml:mo><mml:mi>a</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>&beta;</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>&lsqb;</mml:mo><mml:mi>G</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>w</mml:mi><mml:mo>&rsqb;</mml:mo></mml:math> --></disp-formula>and we let <italic>h</italic>(<bold>x</bold>) be the first coordinate of <italic>T</italic>(<bold>x</bold>). In the special case where <xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref> describes the dynamics of a circuit according to <xref ref-type="disp-formula" rid="pcbi-0020165-e006">Equation 6</xref>, <italic>α</italic> is a linear function, <italic>β</italic> is a constant, and <italic>T</italic> is an invertible linear map from ℜ<italic><sup>n</sup></italic> to ℜ<italic><sup>n</sup></italic>.
				</p><p>Next, pick an external input <italic>u</italic>(<italic>t</italic>),<italic>t</italic> ≥ 0, and a solution <italic>z</italic>(<italic>t</italic>) of the forced system (<xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref>).</p><p>From the interpretation of feedback linearization given earlier (in the last part of Definition of the Class <italic>S<sub>n</sub></italic>), it follows that for any inputs <italic>u</italic>(<italic>t</italic>) and <italic>u</italic><sub>0</sub>(<italic>t</italic>) (in particular, one could take <italic>u</italic><sub>0</sub> ≡ 0), and each solution <italic>z</italic>(<italic>t</italic>) of
					<disp-formula id="pcbi-0020165-e021"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e021" xlink:type="simple"/><!-- <mml:math display='block'><mml:msup><mml:mi>z</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo>&equals;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mi>G</mml:mi><mml:mo>(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>&Prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:mo>&hellip;</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>n</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy='false'>)</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:math> --></disp-formula>(that is, we use <italic>μ</italic>(<italic>t</italic>) = <italic>G</italic>(<italic>z</italic>(<italic>t</italic>),<italic>z</italic>′(<italic>t</italic>),<italic>z</italic>″(<italic>t</italic>),…,<italic>z</italic>(<italic>n</italic><sup>−1</sup>)(<italic>t</italic>)) + <italic>u</italic>(<italic>t</italic>) + <italic>u</italic><sub>0</sub>(<italic>t</italic>) as the input to <italic>z</italic><sup>(<italic>n</italic>)</sup> = <italic>μ</italic>), the vector function <bold>x</bold>(<italic>t</italic>) = <italic>T</italic><sup>−1</sup>(<italic>Z</italic>(<italic>t</italic>)) satisfies <xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref> with input
					<disp-formula id="pcbi-0020165-e022"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e022" xlink:type="simple"/><!-- <mml:math display='block'><mml:mi>v</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mi>&alpha;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>&beta;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>)</mml:mo><mml:mi>&mu;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mi>K</mml:mi><mml:mo>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo>.</mml:mo></mml:math> --></disp-formula>Furthermore, <italic>Z</italic>(<italic>t</italic>) = <italic>T</italic>(<bold>x</bold>(<italic>t</italic>)) means that <italic>z</italic>(<italic>t</italic>) = <italic>h</italic>(<bold>x</bold>(<italic>t</italic>)), as required for the notion of simulation.
				</p><p>This almost proves the simulation result, except for the fact that there is no reason for the initial value <bold>x</bold>(0) = <italic>T</italic><sup>−1</sup>(<italic>Z</italic>(0)) to be zero, since <italic>z</italic>(<italic>t</italic>) is an arbitrary trajectory. This is where the input <italic>u</italic><sub>0</sub> plays a role. Let <italic>ξ</italic> : = <italic>T</italic>(0). We will show that, given any solution <italic>z</italic>(<italic>t</italic>) and any input <italic>u</italic>(<italic>t</italic>), there is some input <italic>u</italic><sub>0</sub>(<italic>t</italic>), with <italic>u</italic><sub>0</sub>(<italic>t</italic>) ≡ 0 for all <italic>t</italic> ≥ 1, so that the solution of
					<disp-formula id="pcbi-0020165-e010"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e010" xlink:type="simple"/><!-- <mml:math display='block'><mml:msup><mml:mi>y</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo>&equals;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mi>G</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>&Prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:mo>&hellip;</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>n</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy='false'>)</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:math> --></disp-formula>with <italic>y</italic>(0) = <italic>ξ</italic> has the property that <italic>y</italic>(<italic>t</italic>) = <italic>z</italic>(<italic>t</italic>) for all t ≥ 1. (Where <italic>z</italic>(<italic>t</italic>) is the desired trajectory to be simulated, with <italic>u</italic><sub>0</sub> ≡ 0.) Then letting <bold>x</bold>(<italic>t</italic>) = <italic>T</italic><sup>−1</sup>(<italic>Y</italic>(<italic>t</italic>)) instead of <italic>T</italic><sup>−1</sup>(<italic>Z</italic>(<italic>t</italic>)) means that <bold>x</bold>(0) and still <italic>h</italic>(<bold>x</bold>(<italic>t</italic>)) = <italic>y</italic>(<italic>t</italic>) = <italic>z</italic>(<italic>t</italic>) for all t ≥ 1.
				</p><p>Consider now an arbitrary solution <italic>z</italic>(<italic>t</italic>) of <xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref> and let <italic>ζ</italic> be the vector with entries
					<disp-formula id="pcbi-0020165-e024"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e024" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mo>&zeta;</mml:mo><mml:mi>i</mml:mi><mml:mo>&plus;</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mo>:</mml:mo><mml:mo>&equals;</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn></mml:math> --></disp-formula>We next pick a scalar differentiable function <italic>ϕ</italic> such that <italic>ϕ</italic><sup>(<italic>i</italic>)</sup>(0) = <italic>ξ<sub>i</sub></italic><sub>+1</sub> and <italic>ϕ</italic><sup>(<italic>i</italic>)</sup>(1) = <italic>ξ<sub>i</sub></italic><sub>+1</sub> for <italic>i</italic> = 0,…,<italic>n</italic> − 1. (It is easy to see that such functions exist. For example, one may simply consider the linear system <bold>p′</bold> = <bold>A</bold><italic><sub>n</sub></italic><bold>p</bold> + <bold>b</bold><italic><sub>n</sub>q</italic> with states <bold>p</bold> and input q. This is a completely controllable linear system (cf. [<xref ref-type="bibr" rid="pcbi-0020165-b033">33</xref>] Chapter 3), so we just pick an input <italic>q</italic>(<italic>t</italic>) that steers <italic>ξ</italic> into <italic>ζ,</italic> and finally let <italic>ϕ</italic>(<italic>t</italic>) be the first coordinate of <bold>p</bold>(<italic>t</italic>). Now we let
					<disp-formula id="pcbi-0020165-e025"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e025" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>u</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>:</mml:mo><mml:mo>&equals;</mml:mo><mml:msup><mml:mi>&phiv;</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&minus;</mml:mo><mml:mi>G</mml:mi><mml:mo>(</mml:mo><mml:mi>&phiv;</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>&phiv;</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>&minus;</mml:mo><mml:mi>u</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math> --></disp-formula>for <italic>t</italic> &lt; 1, and <italic>u</italic><sub>0</sub>(<italic>t</italic>) ≡ 0 for <italic>t</italic> ≥ 1, and claim that the solution of <xref ref-type="disp-formula" rid="pcbi-0020165-e010">Equation 10</xref> with <italic>y</italic>(0) = <italic>ξ</italic> has the property that <italic>y</italic>(<italic>t</italic>) = <italic>z</italic>(<italic>t</italic>) for all t ≥ 1. Since <italic>u</italic>(<italic>t</italic>) + <italic>u</italic><sub>0</sub>(<italic>t</italic>) = <italic>u</italic>(<italic>t</italic>) for all t ≥ 1, we only need to show that <italic>y</italic><sup>(<italic>i</italic>)</sup>(1) = <italic>z</italic><sup>(<italic>i</italic>)</sup>(1) for every <italic>i</italic> = 0,…,<italic>n</italic> − 1. To see this, in turn, and using uniqueness of solutions of differential equations, it is enough to show that <italic>y</italic>(<italic>t</italic>): = <italic>ϕ</italic>(<italic>t</italic>) satisfies
					<disp-formula id="pcbi-0020165-e026"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e026" xlink:type="simple"/><!-- <mml:math display='block'><mml:msup><mml:mi>&phiv;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo>&equals;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mi>G</mml:mi><mml:mo>(</mml:mo><mml:mi>&phiv;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>&phiv;</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>&phiv;</mml:mi><mml:mo>&Prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:mo>&hellip;</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>&phiv;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>n</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy='false'>)</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:math> --></disp-formula>on the interval [0,1] and has derivatives at <italic>t</italic> = 0 as specified by the vector <italic>ξ</italic>. But this is indeed true by construction.
				</p><p>Finally, we remark that if | <italic>u</italic>(<italic>t</italic>) | ≤ <italic>c</italic> and | <italic>z</italic><sup>(<italic>i</italic>)</sup> | (<italic>t</italic>) ≤ <italic>c</italic> for all <italic>t</italic> ≥ 0, then <bold>x</bold>(<italic>t</italic>) = <italic>T</italic><sup>−1</sup>(<italic>Z</italic>(<italic>t</italic>)) is bounded in norm by a constant that only depends on <italic>c</italic> (since <italic>T</italic><sup>−1</sup> is continuous, by definition of diffeomorphism), and the numbers <italic>b<sub>i</sub></italic>: = <italic>z</italic><sup>(<italic>i</italic>)</sup>(1) are also bounded by a constant that depends only on <italic>c,</italic> so <italic>K</italic>(<bold>x</bold>(<italic>t</italic>),<italic>u</italic>(<italic>t</italic>) + <italic>u</italic><sub>0</sub>(<italic>t</italic>)) also is.</p><p><underline>Corollary 3</underline>. <italic>Analogous results can be shown for the simulation of systems consisting of any number k of higher order differential equations as in <xref ref-type="disp-formula" rid="pcbi-0020165-e004">Equation 4</xref>. In this case fixed systems of first-order differential equations of a form as in <xref ref-type="disp-formula" rid="pcbi-0020165-e003">Equation 3</xref>, but with k memoryless feedback functions K</italic><sub>1</sub><italic>,…K<sub>k</sub> that depend on the simulated higher-order system, can be shown to be able to simulate the dynamic response of arbitrary higher-order systems of differential equations.</italic></p><p><italic>Lie brackets.</italic> The study of controllability and other properties of nonlinear systems is based upon the use of Lie bracket formalism and theory ([<xref ref-type="bibr" rid="pcbi-0020165-b033">33</xref>], Chapter 4). We need this formalism to show in the section Application to Neural Network Equations that the class <italic>S<sub>n</sub></italic> includes some neural networks of the form <xref ref-type="disp-formula" rid="pcbi-0020165-e006">Equation 6</xref>. For any two vector fields <italic>f</italic> and g,
					<disp-formula id="pcbi-0020165-e027"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e027" xlink:type="simple"/><!-- <mml:math display='block'><mml:mo>&lsqb;</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo>&rsqb;</mml:mo><mml:mo>&equals;</mml:mo><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:mi>f</mml:mi><mml:mo>&minus;</mml:mo><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:mi>g</mml:mi></mml:math> --></disp-formula>denotes the <named-content content-type="genus-species" xlink:type="simple">Lie bracket</named-content> of <italic>f</italic> and <italic>g</italic>. Recall that the Lie bracket of two vector fields is a vector field that characterizes the effective direction of movement obtained by performing this “commutator” motion: follow the vector field <italic>f</italic> for <italic>t</italic> time steps, then <italic>g</italic> for <italic>t</italic> time steps, then <italic>f</italic> backward in time for <italic>t</italic> time steps, and finally <italic>g</italic> backward in time for <italic>t</italic> time steps, for small <italic>t</italic> &gt; 0. To be more precise, denote formally by <italic>e<sup>tf</sup></italic> the flow associated to <italic>f</italic>, and similarly for g. Consider the following curve, for any initial state <italic>x</italic><sub>0</sub>:
					<disp-formula id="pcbi-0020165-e028"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e028" xlink:type="simple"/><!-- <mml:math display='block'><mml:mi>&gamma;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo>:</mml:mo><mml:mo>&equals;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:msup><mml:mi>e</mml:mi><mml:mo>&minus;</mml:mo><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt><mml:mi>g</mml:mi></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mo>&minus;</mml:mo><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt><mml:mi>f</mml:mi></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt><mml:mi>g</mml:mi></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt><mml:mi>f</mml:mi></mml:msup><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mtext>&thinsp;</mml:mtext><mml:mo>.</mml:mo></mml:math> --></disp-formula>Applying repeatedly this expansion:
					<disp-formula id="pcbi-0020165-e029"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e029" xlink:type="simple"/><!-- <mml:math display='block'><mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>f</mml:mi></mml:msup><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mtext>&thinsp;</mml:mtext><mml:mo>&equals;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mi>x</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>x</mml:mi><mml:mo>&Prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy='false'>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:mi>t</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mfrac><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mi>f</mml:mi><mml:mo>&lowast;</mml:mo></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy='false'>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math> --></disp-formula>(and similarly for <italic>g</italic>), we obtain that
					<disp-formula id="pcbi-0020165-e030"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e030" xlink:type="simple"/><!-- <mml:math display='block'><mml:msup><mml:mi>e</mml:mi><mml:mo>&minus;</mml:mo><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt><mml:mi>g</mml:mi></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mo>&minus;</mml:mo><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt><mml:mi>f</mml:mi></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt><mml:mi>g</mml:mi></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:msqrt><mml:mi>t</mml:mi></mml:msqrt><mml:mi>f</mml:mi></mml:msup><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mtext>&thinsp;</mml:mtext><mml:mo>&equals;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:msup><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy='false'>&rsqb;</mml:mo></mml:msup><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mtext>&thinsp;</mml:mtext><mml:mo>&plus;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mi>o</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:math> --></disp-formula>as <italic>t</italic> → 0, from which it follows that <italic>γ</italic>′(0) = [<italic>f</italic>,<italic>g</italic>](<italic>x</italic><sub>0</sub>), which means that the direction of [<italic>f</italic>,<italic>g</italic>] is followed when performing the commutator motions. Using the possible noncommutativity of the vector fields, one generates in this manner genuinely new directions of movement in addition to those provided by the linear combinations of <italic>f</italic> and <italic>g</italic>. Well-known examples are provided by the Lie bracket of two rotations around orthogonal axes, which is a rotation around the remaining axis (see for example [<xref ref-type="bibr" rid="pcbi-0020165-b033">33</xref>], page 150), or the motions involved in parking an automobile (see for example [<xref ref-type="bibr" rid="pcbi-0020165-b033">33</xref>], Example 4.3.13).
				</p><p>Iterations of Lie brackets play a key role. Let us introduce, for any given vector field <italic>f,</italic> the operator <italic>ad<sub>f</sub>,</italic> which maps vector fields into vector fields by means of the formula ad<italic><sub>f</sub></italic>(<italic>g</italic>): = [<italic>f</italic>,<italic>g</italic>]. Iterations of the operator ad<italic><sub>f</sub></italic> are defined in the obvious way: <inline-formula id="pcbi-0020165-ex007"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex007" xlink:type="simple"/></inline-formula>
					 and <inline-formula id="pcbi-0020165-ex008"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex008" xlink:type="simple"/></inline-formula>
					.
				</p><p>It is also useful to consider an operator <italic>L<sub>f</sub></italic> that acts on scalar functions. We use the notation <italic>L<sub>f</sub>ϕ,</italic> for any (smooth) vector field <italic>f</italic> and (smooth) function <italic>ϕ,</italic> to denote the Lie derivative of <italic>ϕ</italic> along <italic>f,</italic> that is, ∇<italic>ϕ</italic> · <italic>f</italic>. The function <italic>L<sub>f</sub>ϕ,</italic> which is again smooth, is nothing more than the <italic>directional derivative</italic> of the function <italic>ϕ</italic> in the direction of the vector field <italic>f,</italic> in the sense of elementary calculus. One can also consider iterated applications of the operator <italic>L<sub>f</sub></italic>.</p><p><italic>A characterization of S<sub>n</sub> via lie brackets.</italic> With these notations, we are ready to present a Lie geometric characterization of the class <italic>S<sub>n</sub></italic>. The next theorem follows by combining the proofs of Proposition 5.3.9 and of Theorem 15 in [<xref ref-type="bibr" rid="pcbi-0020165-b033">33</xref>] (with 0 = <italic>X</italic> = ℜ<italic><sup>n</sup></italic> in the notations of that text).</p><p><underline>Theorem 4</underline>. <named-content content-type="genus-species" xlink:type="simple">The system</named-content> <bold>x′</bold> = <italic>f</italic>(<bold>x</bold>) + <italic>g</italic>(<bold>x</bold>)<italic>v is globally feedback linearizable if and only if there exists a smooth function</italic>
					<disp-formula id="pcbi-0020165-e031"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e031" xlink:type="simple"/><!-- <mml:math display='block'><mml:mi>&phiv;</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mtext>&real;</mml:mtext><mml:mi>n</mml:mi></mml:msup><mml:mo>&rarr;</mml:mo><mml:mo>&real;</mml:mo></mml:math> --></disp-formula>having everywhere nonzero gradient and satisfying the following properties: 1) <italic>for each</italic> <bold>x</bold> ∈ ℜ<italic><sup>n</sup>, the vectors</italic> <inline-formula id="pcbi-0020165-ex009"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex009" xlink:type="simple"/></inline-formula>
					 <italic>are linearly independent;</italic> 2) <italic>for each</italic> <bold>x</bold> ∈ ℜ<italic><sup>n</sup> and each j =</italic>0,…,<italic>n −</italic> 2, <inline-formula id="pcbi-0020165-ex010"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex010" xlink:type="simple"/></inline-formula>
					; 3) <italic>the map</italic> <inline-formula id="pcbi-0020165-ex011"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex011" xlink:type="simple"/></inline-formula>
					 <italic>is a bijection</italic> ℜ<italic><sup>n</sup> →</italic> ℜ<italic><sup>n</sup>.</italic>
				</p><p>Observe that the conditions amount to the existence of a well-behaved solution <italic>ϕ</italic> of a set of first-order linear partial differential equations. Existence of a solution of this form is not trivial to verify. To study solvability, in control theory one considers the following conditions:</p><list list-type="simple"><list-item><p><bold>(LI)</bold> The set of vector fields <inline-formula id="pcbi-0020165-ex012"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex012" xlink:type="simple"/></inline-formula>
							 is linearly independent.
						</p></list-item><list-item><p><bold>(INV)</bold> The distribution generated by <inline-formula id="pcbi-0020165-ex013"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex013" xlink:type="simple"/></inline-formula>
							 is involutive.
						</p></list-item></list><p>This last condition means that the Lie bracket of any two of the vector fields <inline-formula id="pcbi-0020165-ex014"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex014" xlink:type="simple"/></inline-formula>
					, for <italic>i</italic> ∈ {0,…,<italic>n</italic> − 2}, should be, for each <bold>x</bold>, a linear combination of these same <italic>n</italic> − 1 vectors.
				</p><p>One then has the following result (see Theorem 15 in [<xref ref-type="bibr" rid="pcbi-0020165-b033">33</xref>]), which is a consequence of Frobenius' Theorem in partial differential equation theory: <italic>A system satisfies both conditions</italic><bold> (LI)</bold> <italic>and</italic> <bold>(INV)</bold> <italic>at a state</italic> <bold>x</bold> <italic>if and only if it is feedback linearizable in some open set containing</italic> <bold>x</bold><italic>.</italic> This provides a useful and complete characterization of local feedback linearizability, and in particular a necessary condition for global feedback linearizability. In examples, often these conditions lead one to a globally defined solution, see, e.g., example 5.3.10 in [<xref ref-type="bibr" rid="pcbi-0020165-b033">33</xref>]).</p><p><italic>Application to neural network equations.</italic> Let us now show with the help of Theorem 4 that the class <italic>Sn</italic> includes some fading-memory systems of the form <xref ref-type="disp-formula" rid="pcbi-0020165-e006">Equation 6</xref>. Indeed, consider any system as follows:
					<disp-formula id="pcbi-0020165-e011"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e011" xlink:type="simple"/><!-- <mml:math display='block'><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mo>&equals;</mml:mo><mml:mo>&minus;</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>&lambda;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>&lambda;</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>&plus;</mml:mo><mml:mi mathvariant="bold">b</mml:mi><mml:mspace width="1pt"/><mml:mi>v</mml:mi></mml:math> --></disp-formula>where the <italic>λ<sub>i</sub></italic> ≠ <italic>λ<sub>j</sub></italic> for each <italic>i</italic> ≠ <italic>j</italic> are all positive, diag(<italic>λ</italic><sub>1</sub>,...,<italic>λ</italic><sub>n</sub>) is the resulting diagonal matrix, and the column vector <bold>b</bold> = col(<italic>b</italic><sub>1</sub>,…,<italic>b</italic><sub>n</sub>) has nonzero entries: <italic>b<sub>i</sub></italic> ≠ 0 for all <italic>i</italic>. (Such a system, which has the form <xref ref-type="disp-formula" rid="pcbi-0020165-e006">Equation 6</xref> with <italic>ϕ</italic>(<italic>A</italic><bold>x</bold>) ≡ 0, consists of <italic>n</italic> first-order linear differential equations in parallel, and is obviously fading-memory.) It is easy to see that, up to signs (−1)<italic><sup>i</sup></italic>, we have
					<disp-formula id="pcbi-0020165-e035"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e035" xlink:type="simple"/><!-- <mml:math display='block'><mml:msubsup><mml:mtext>ad</mml:mtext><mml:mi>f</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mi>g</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mtext>col</mml:mtext><mml:mo stretchy='false'>(</mml:mo><mml:msubsup><mml:mi>&lambda;</mml:mi><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:msubsup><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&hellip;</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&lambda;</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:msub><mml:mi>b</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:math> --></disp-formula>for <italic>i</italic> &gt; 0, and the linear independence of <inline-formula id="pcbi-0020165-ex015"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex015" xlink:type="simple"/></inline-formula>
					 follows from the fact that these constant vectors form a Vandermonde matrix. Then we can pick <italic>ϕ</italic>(<bold>x</bold>) as a linear map <bold>x</bold> → <bold>ax</bold>, where <bold>a</bold> is any vector in ℜ<italic><sup>n</sup></italic> that is orthogonal to all of the vectors
					<disp-formula id="pcbi-0020165-e036"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e036" xlink:type="simple"/><!-- <mml:math display='block'><mml:mtext>col</mml:mtext><mml:mo stretchy='false'>(</mml:mo><mml:msubsup><mml:mi>&lambda;</mml:mi><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:msubsup><mml:msub><mml:mi>b</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&hellip;</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>&lambda;</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:msub><mml:mi>b</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>&equals;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&hellip;</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>2</mml:mn><mml:mtext>&thinsp;</mml:mtext><mml:mo>.</mml:mo></mml:math> --></disp-formula>The map <inline-formula id="pcbi-0020165-ex016"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex016" xlink:type="simple"/></inline-formula>
					 is represented then also by a Vandermonde matrix, so it is a bijection. Hence, conditions 1)−3) of Theorem 4 are satisfied, which implies that the system (<xref ref-type="disp-formula" rid="pcbi-0020165-e011">Equation 11</xref>) belongs to the class <italic>S<sub>n</sub></italic>.
				</p><p>As a further example, we now consider the following system, which also has the general form of the neural network <xref ref-type="disp-formula" rid="pcbi-0020165-e006">Equation 6</xref>:
					<disp-formula id="pcbi-0020165-e037"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e037" xlink:type="simple"/><!-- <mml:math display='block'><mml:mtable><mml:mtr><mml:mtd><mml:maligngroup/><mml:msubsup><mml:mi>x</mml:mi><mml:mn>1</mml:mn><mml:mi>&prime;</mml:mi></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:malignmark/><mml:mo>&equals;</mml:mo><mml:malignmark/><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>&lambda;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>&sigma;</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:msubsup><mml:mi>x</mml:mi><mml:mn>2</mml:mn><mml:mi>&prime;</mml:mi></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:malignmark/><mml:mo>&equals;</mml:mo><mml:malignmark/><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>&lambda;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>&sigma;</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:maligngroup/><mml:msubsup><mml:mi>x</mml:mi><mml:mn>3</mml:mn><mml:mi>&prime;</mml:mi></mml:msubsup><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:malignmark/><mml:mo>&equals;</mml:mo><mml:malignmark/><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>&lambda;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math> --></disp-formula>where <italic>ϕ</italic> is a scalar function, smooth but otherwise arbitrary for now, and <italic>a</italic> as well as the <italic>λ<sub>i</sub></italic> are constants, also arbitrary for now. We will analyze this example using the Lie formalism described in the section Lie Brackets. The system has the form <bold>x′</bold> = <italic>f</italic>(<bold>x</bold>) + <italic>g</italic>(<bold>x</bold>)<italic>v</italic>, with <italic>n</italic> = 3, and <italic>f</italic> and <italic>g</italic> are the following vector fields:
					<disp-formula id="pcbi-0020165-e038"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e038" xlink:type="simple"/><!-- <mml:math display='block'><mml:mi>f</mml:mi><mml:mo>&equals;</mml:mo><mml:mo stretchy='true'>(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>&lambda;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:mi>&sigma;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>&lambda;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:mi>&sigma;</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>&lambda;</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy='true'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo>,</mml:mo><mml:mtext>&emsp;</mml:mtext><mml:mi>g</mml:mi><mml:mo>&equals;</mml:mo><mml:mo stretchy='true'>(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy='true'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo>.</mml:mo></mml:math> --></disp-formula>Note that the Jacobian g<sub>*</sub> of <italic>g</italic> is identically zero, which simplifies the computation of Lie brackets. We calculate ad<italic><sub>f</sub>g</italic>(<bold>x</bold>) = [<italic>f</italic>,<italic>g</italic>](<bold>x</bold>) = −<italic>f</italic><sub>*</sub>(<bold>x</bold>)<italic>g</italic>(<bold>x</bold>) and [<italic>g</italic>,ad<italic><sub>f</sub>g</italic>] = (ad<italic><sub>f</sub>g</italic>)<sub>*</sub>(<bold>x</bold>)g(<bold>x</bold>) as follows:
					<disp-formula id="pcbi-0020165-e039"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e039" xlink:type="simple"/><!-- <mml:math display='block'><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mtext>ad</mml:mtext><mml:mi>f</mml:mi></mml:msub><mml:mi>g</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mo stretchy='true'>(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mo>&minus;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mi>a</mml:mi><mml:mtext>&thinsp;</mml:mtext><mml:msup><mml:mi>&sigma;</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&minus;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:msup><mml:mi>&sigma;</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>&lambda;</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy='true'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mi>g</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mtext>ad</mml:mtext><mml:mi>f</mml:mi></mml:msub><mml:mi>g</mml:mi><mml:mo stretchy='false'>&rsqb;</mml:mo><mml:mo>&equals;</mml:mo><mml:mo stretchy='true'>(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mo>&minus;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:msup><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>&sigma;</mml:mi><mml:mo>&Prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:mi>a</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&minus;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:msup><mml:mi>&sigma;</mml:mi><mml:mo>&Prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy='true'>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math> --></disp-formula>The involutivity condition says that the set of vector fields {<italic>g</italic>,ad<italic><sub>f</sub>g</italic>} should be involutive, which means that [<italic>g</italic>,ad<italic><sub>f</sub>g</italic>](<bold>x</bold>) should be in the span of <italic>g</italic>(<bold>x</bold>) and ad<italic><sub>f</sub>g</italic>(<bold>x</bold>) for all <bold>x</bold>. Let us evaluate <italic>g</italic>,ad<italic><sub>f</sub>g</italic>,[<italic>g</italic>,ad<italic><sub>f</sub>g</italic>] at the particular points for which <italic>x</italic><sub>3</sub> = 0, so that we obtain, respectively, three vectors <italic>v</italic><sub>1</sub>,<italic>v</italic><sub>2</sub>,<italic>v</italic><sub>3</sub> that depend on <italic>x</italic><sub>2</sub> only:
					<disp-formula id="pcbi-0020165-e040"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e040" xlink:type="simple"/><!-- <mml:math display='block'><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mo stretchy='true'>(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy='true'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo>,</mml:mo><mml:mtext>&emsp;</mml:mtext><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mo stretchy='true'>(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mo>&minus;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mi>a</mml:mi><mml:mtext>&thinsp;</mml:mtext><mml:msup><mml:mi>&sigma;</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&minus;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:msup><mml:mi>&sigma;</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>&lambda;</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy='true'>)</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:mo>.</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>v</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mo stretchy='true'>(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mo>&minus;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:msup><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>&sigma;</mml:mi><mml:mo>&Prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&minus;</mml:mo><mml:mtext>&thinsp;</mml:mtext><mml:msup><mml:mi>&sigma;</mml:mi><mml:mo>&Prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy='true'>)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math> --></disp-formula>If [<italic>g</italic>,ad<italic><sub>f</sub>g</italic>](<bold>x</bold>) is in the span of <italic>g</italic>(<bold>x</bold>) and ad<italic><sub>f</sub>g</italic>(<bold>x</bold>) for all vectors <bold>x</bold>, then, in particular, <italic>v</italic><sub>3</sub>(<italic>x</italic><sub>2</sub>) must belong to the span of <italic>v</italic><sub>1</sub>(<italic>x</italic><sub>2</sub>) and <italic>v</italic><sub>2</sub>(<italic>x</italic><sub>2</sub>) for all <italic>x</italic><sub>2</sub>. This means that there is, for each <italic>x</italic><sub>2</sub>, a scalar <italic>r</italic>(<italic>x</italic><sub>2</sub>) such that
					<disp-formula id="pcbi-0020165-e041"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e041" xlink:type="simple"/><!-- <mml:math display='block'><mml:msup><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>&sigma;</mml:mi><mml:mo>&Prime;</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo><mml:msup><mml:mi>&sigma;</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mi>&sigma;</mml:mi><mml:mo>&Prime;</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo><mml:msup><mml:mi>&sigma;</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mo>(</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:math> --></disp-formula>If <italic>a</italic> ≠ 0, it follows that <italic>ϕ</italic>″(<italic>x</italic><sub>2</sub>) = <italic>aϕ</italic>″(<italic>x</italic><sub>2</sub>) for all <italic>x</italic><sub>2</sub>. So, if also <italic>a</italic> ≠ 1, we conclude that <italic>ϕ</italic>″(<italic>x</italic><sub>2</sub>) must vanish for all <italic>x</italic><sub>2</sub>. Thus, the system in our example (assuming <italic>a</italic> ∉ {0,1}) is feedback linearizable only if <italic>ϕ</italic> is a linear function.
				</p><p>On the other hand, consider now the cases <italic>a</italic> = 0 or <italic>a</italic> = 1. Then, the involutivity condition becomes the requirement that there should exist a scalar function <italic>r</italic> such that
					<disp-formula id="pcbi-0020165-e042"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e042" xlink:type="simple"/><!-- <mml:math display='block'><mml:msup><mml:mi>&sigma;</mml:mi><mml:mo>&Prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>)</mml:mo><mml:msup><mml:mi>&sigma;</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msup><mml:mi>&sigma;</mml:mi><mml:mo>&Prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mo>&equals;</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>)</mml:mo><mml:msup><mml:mi>&sigma;</mml:mi><mml:mo>&prime;</mml:mo></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo></mml:math> --></disp-formula>which can be achieved provided only that the function <italic>ϕ</italic>′ is everywhere nonzero (which is true if <italic>ϕ</italic> is, for example, a standard sigmoidal function), simply by taking <italic>r</italic>(<bold>x</bold>) = <italic>ϕ</italic>″(<italic>x</italic><sub>2</sub> + <italic>x</italic><sub>3</sub>) / <italic>ϕ</italic>′(<italic>x</italic><sub>2</sub> + <italic>x</italic><sub>3</sub>). The linear independence condition amounts to showing that the set of vectors {<italic>g</italic>,ad<italic><sub>f</sub>g</italic>,ad<sup>2</sup><italic><sub>f</sub>g</italic>} is linearly independent. Computing the determinant of the matrix that has these vectors as columns, when <italic>a</italic> = 0 we obtain −<italic>ϕ</italic>′(<italic>x</italic><sub>2</sub>)[<italic>ϕ</italic>′(<italic>x</italic><sub>2</sub> + <italic>x</italic><sub>3</sub>)]<sup>2</sup>, which is everywhere nonzero, provided that we again assume that <italic>ϕ</italic> has an everywhere nonzero derivative. Thus, the Lie-theoretic conditions for feedback linearization are satisfied, for any choice of <italic>λ<sub>i</sub></italic>, when <italic>a</italic> = 0. In the case <italic>a</italic> = 1, the same computation gives a determinant of <italic>λ</italic><sub>1</sub> − <italic>λ</italic><sub>2</sub>)[<italic>ϕ</italic>′(<italic>x</italic><sub>2</sub> + <italic>x</italic><sub>3</sub>)]<sup>2</sup>, so the Lie-theoretic conditions for feedback linearization are satisfied, for any choice of <italic>λ<sub>i</sub></italic> such that <italic>λ</italic><sub>1</sub> ≠ <italic>λ</italic><sub>2</sub>.
				</p><sec id="s4a1"><title>Mathematical definitions and details to the proof of Theorem 2.</title><p><italic>Fading-memory filters.</italic> A map (or filter) <italic>F</italic> from input to output streams is defined to have <italic>fading memory</italic> if its current output at time <italic>t</italic> depends (up to some precision <italic>ɛ</italic>) only on values of the input <bold>u</bold> during some finite time interval [<italic>t</italic> – <italic>T</italic>,<italic>t</italic>]. (We use in this section boldface letters to denote input streams, because they typically have a dimension larger than 1.) In formulas: <italic>F</italic> has fading memory if there exists for every <italic>ɛ</italic> &gt; 0 some <italic>δ</italic> &gt; 0 and <italic>T</italic> &gt; 0 so that | (<italic>F</italic><bold>u</bold>)(<italic>t</italic>) − (<italic>F</italic><bold>ũ</bold>)(<italic>t</italic>) | &lt; <italic>ɛ</italic> for any t ∈ ℜ and any input functions <bold>u</bold>,<bold>ũ</bold> with || <bold>u</bold>(<italic>τ</italic>) − <bold>ũ</bold>(<italic>τ</italic>) || &lt; <italic>δ</italic> for all <italic>τ</italic> ∈[<italic>t</italic> − <italic>T</italic>,<italic>t</italic>]. This is a characteristic property of all filters that can be approximated by an integral over the input stream <bold>u</bold>, or more generally by Volterra or Wiener series. Note that nontrivial Turing machines and FSMs <italic>cannot</italic> be approximated by filters with fading memory, since they require a persistent memory.</p><p><italic>Finite state machines.</italic> The deterministic <italic>finite state machine</italic> (FSM), also referred to as deterministic finite automaton, is a standard model for a digital computer, or more generally for any realistic computational device that operates in discrete time with a discrete set of inputs and internal states [<xref ref-type="bibr" rid="pcbi-0020165-b026">26</xref>]. One assumes that an FSM is at any time in one of some finite number <italic>l</italic> of states, and that it receives at any (discrete) time step one input symbol from some alphabet {<italic>s</italic><sub>1</sub>,…,<italic>s<sub>k</sub></italic>} that may consist of any finite number <italic>k</italic> of symbols. Its “program” may consist of any transition function <italic>TR</italic>:{<italic>s</italic><sub>1</sub>,…,<italic>s<sub>k</sub></italic>} × {1,…,<italic>l</italic>} → {1,…,<italic>l</italic>}, where <italic>TR</italic>(<italic>s</italic><sub>i</sub>,<italic>j</italic>′) = <italic>j</italic> denotes the new internal state <italic>j</italic> which the FSM assumes at the next time step after processing input symbol <italic>s<sub>i</sub></italic> in state j′.</p><p><italic>Precise statement of Theorem 2.</italic> We consider here a slight variation of the FSM model, which is more adequate for systems that operate in continuous time and receive analog inputs (for example, trains of spikes in continuous time). We assume that the raw input is some arbitrary <italic>n</italic>-dimensional input stream <bold>u</bold> (i.e., <bold>u</bold>(<italic>t</italic>) ∈ ℜ<italic><sup>n</sup></italic> for every <italic>t</italic> ∈ ℜ<italic><sup>n</sup></italic>). Furthermore we assume that there exist pattern detectors <italic>F</italic><sub>1</sub>,…,<italic>F<sub>k</sub></italic> that report the occurrence of spatio–temporal patterns in the input stream <bold>u</bold> from <italic>k</italic> different classes <italic>C<sub>1</sub></italic>,…,<italic>C</italic><sub>k</sub>. In the case where the input <bold>u</bold> consists of spike trains, these classes could consist, for example, of particular patterns of firing rates, of particular spike patterns, or of particular correlation patterns among some of the input spike trains. It was shown in [<xref ref-type="bibr" rid="pcbi-0020165-b005">5</xref>] that readouts from generic neural microcircuit models can easily be trained to approximate the role of such pattern detectors <italic>F</italic><sub>1</sub>,…,<italic>F<sub>k</sub></italic>. We assume that the detection of a pattern from class <italic>C<sub>i</sub></italic> by pattern detector <italic>F<sub>i</sub></italic> affects the state of the FSM according to its transition function <italic>TR</italic> in a way that corresponds to the presentation of input symbol <italic>s<sub>i</sub></italic> in the discrete-time version: if <italic>j</italic>′ was its preceding state, then it changes now within some finite switching time to state j = <italic>TR</italic>(<italic>s<sub>i</sub></italic>,<italic>j</italic>′).</p><p>To make an implementation of such FSM by a noisy system feasible, we assume that the pattern detectors (<italic>F</italic><sub>1</sub><bold>u</bold>)(<italic>t</italic>),…,(<italic>F<sub>k</sub></italic><bold>u</bold>)(<italic>t</italic>) always assume values ≤0, except during a switching episode. During a switching episode, exactly one of the pattern detectors (<italic>F<sub>i</sub></italic><bold>u</bold>)(<italic>t</italic>) assumes values &gt;0. We assume that this (<italic>F<sub>i</sub></italic><bold>u</bold>(<italic>t</italic>) reaches values ≥1 during this switching episode. We also assume that the length of each switching episode (i.e., the time during which some (<italic>F<sub>i</sub></italic><bold>u</bold>(<italic>t</italic>) assumes values &gt;0) is bounded from above by some constant <italic>δ,</italic> and that the temporal distance between the beginnings of any two different switching episodes is at least Δ + 3δ′ (where Δ is the assumed temporal delay of the feedback in the circuit). To avoid that the subsequent construction is based on unrealistic assumptions, we allow that each pattern detector <italic>F<sub>i</sub></italic> is replaced by some arbitrary filter <inline-formula id="pcbi-0020165-ex017"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex017" xlink:type="simple"/></inline-formula>
						 so that <inline-formula id="pcbi-0020165-ex018"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex018" xlink:type="simple"/></inline-formula>
						 is a continuous function of time (with values in some arbitrary bounded range [−<italic>B</italic>,<italic>B</italic>]) with <inline-formula id="pcbi-0020165-ex019"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex019" xlink:type="simple"/></inline-formula>
						 for any input stream <bold>u</bold> that is considered.
					</p><p>The informal statement of Theorem 2 is made precise by the subsequent Theorem 5 (see <xref ref-type="fig" rid="pcbi-0020165-g006">Figure 6</xref> for an illustration). It exhibits a simple construction method whereby fading-memory filters with additive noise of bounded amplitude can be composed into a closed loop system <italic>C</italic> that emulates an arbitrary given FSM in a noise-robust manner. The resulting system <italic>C</italic> can be embedded into any other fading-memory system, which receives the outputs <italic>CL</italic> – <italic>Ĥ<sub>j</sub></italic>(<italic>t</italic>) of <italic>C</italic> as additional inputs. In this way, any given fading-memory system can integrate the computational capability and nonfading states of the FSM that is emulated by <italic>C</italic> into its own real-time computation on time-varying input streams <bold>u</bold>.</p><fig id="pcbi-0020165-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020165.g006</object-id><label>Figure 6</label><caption><title>Emulation of an FSM by a Noisy Fading-Memory System with Feedback According to Theorem 5</title><p>(A) Underlying open-loop system with noisy pattern detectors <inline-formula id="pcbi-0020165-ex038"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex038" xlink:type="simple"/></inline-formula><sub>1</sub>, …, <inline-formula id="pcbi-0020165-ex039"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex039" xlink:type="simple"/></inline-formula><italic><sub>k</sub></italic> and suitable fading-memory readouts <italic>Ĥ</italic><sub>1</sub>, …, <italic>Ĥ<sub>l</sub></italic> (which may also be subject to noise).</p><p>(B) Resulting noise-robust emulation of an arbitrary given FSM by adding feedback to the system in (A). The same readouts as in (A) (denoted <italic>CL</italic> − <italic>Ĥ<sub>j</sub></italic>(<italic>t</italic>) in the closed loop) now encode the current state of the simulated FSM.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020165.g006" xlink:type="simple"/></fig><p>An essential aspect of the proof of Theorem 5 is that suitable fading-memory filters <italic>H<sub>j</sub></italic> can prevent in the closed loop the accumulation of errors through feedback, even if the ideal fading-memory filters <italic>H<sub>j</sub></italic> are subsequently replaced by imperfect approximations <italic>Ĥ<sub>j</sub></italic>. One just has to construct the ideal fading-memory filters <italic>H<sub>j</sub></italic> in such a way that they take into account that their previous outputs, which have been fed back into the system <italic>C,</italic> may have been corrupted by additive noise. As long as this additive noise of bounded amplitude has not been amplified in the closed loop, the filters <italic>H<sub>j</sub></italic> can still recover which of the finitely many states of the emulated FSM <italic>A</italic> was represented by that noise-corrupted feedback.</p><p>From the perspective of neural circuit models, it is of interest to note that the construction of the system <italic>C</italic> can be replaced by an adaptive procedure, whereby readouts from generic cortical microcircuit models are trained to approximate the target filters <italic>H<sub>j</sub></italic>. General approximation results [<xref ref-type="bibr" rid="pcbi-0020165-b004">4</xref>,<xref ref-type="bibr" rid="pcbi-0020165-b005">5</xref>,<xref ref-type="bibr" rid="pcbi-0020165-b037">37</xref>] imply that if the neural circuit is sufficiently large and contains sufficiently diverse components (for example, dynamic synapses with slightly different parameter values), then the actual outputs <italic>Ĥ<sub>j</sub></italic> of these readouts can approximate the target filters <italic>H<sub>j</sub></italic> uniformly up to any given maximal error <italic>ɛ</italic> &gt; 0. Theorem 5 guarantees that the resulting neural circuit model with these (imperfectly) trained readouts can in the closed loop emulate the given FSM <italic>A</italic> in a reliable manner, provided that the neural circuit model is sufficiently large and diverse so that its readout can achieve an approximation error <italic>ɛ</italic> not larger than 1/4.</p><p><underline>Theorem 5</underline>. <italic>One can construct for any given FSM A, some time-invariant fading-memory filters H</italic><sub>1</sub><italic>,…,H<sub>l</sub> with the property that any approximating filters Ĥ</italic><sub>1</sub>,…,<italic>Ĥ<sub>l</sub> with |H<sub>j</sub> − Ĥ<sub>j</sub></italic> | ≤ 1/4 <italic>provide in the closed loop with delay</italic> Δ <italic>(see <xref ref-type="fig" rid="pcbi-0020165-g006">Figure 6</xref>) outputs CL − Ĥ<sub>1</sub></italic>,…,<italic>CL − Ĥ<sub>l</sub> that simulate the FSM A in the following sense:</italic></p><p><italic>If</italic> [<italic>t</italic><sub>1</sub>,<italic>t</italic><sub>2</sub>] <italic>is some arbitrary time interval between switching episodes of the FSM A with noise-free pattern detectors</italic> (<italic>F</italic><sub>1</sub><bold>u</bold>)(<italic>t</italic>),…,(<italic>F<sub>k</sub></italic><bold>u</bold>)(<italic>t</italic>) <italic>during which A is in state j, then the outputs CL − Ĥ<sub>i</sub>(t) of the approximating filters Ĥ<sub>i</sub> in the closed loop with noisy pattern detectors</italic> <inline-formula id="pcbi-0020165-ex020"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex020" xlink:type="simple"/></inline-formula>
						 <italic>satisfy CL − Ĥ<sub>j</sub></italic>(<italic>t</italic>) <italic>≥ 3/4 and CL − Ĥ<sub>j*</sub></italic>(<italic>t</italic>) <italic>≤ 1/4 for all j<sup>*</sup> ≠ j and all t</italic> ∈ [<italic>t</italic><sub>1</sub>,<italic>t</italic><sub>2</sub>]<italic>.</italic>
					</p><p><italic>Proof of the precise statement of Theorem 2.</italic> We present here a proof of Theorem 5 (see Precise Statement of Theorem 2 section above), which provides a formally precise version of Theorem 2.</p><p>To prove that the given FSM <italic>A</italic> can be implemented in a noise-robust fashion, we construct suitable time-invariant fading-memory filters <italic>H</italic><sub>1</sub>,…,<italic>H<sub>l</sub></italic>. They receive as inputs the time-varying functions <inline-formula id="pcbi-0020165-ex021"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex021" xlink:type="simple"/></inline-formula>
						. In addition, they receive in the open-loop inputs <italic>v</italic><sub>1</sub>(<italic>t</italic>),…,<italic>v<sub>1</sub></italic>(<italic>t</italic>), where each <italic>v<sub>j</sub></italic>(<italic>t</italic>) will be replaced by a delayed version of the output of <italic>H<sub>j</sub></italic> (or <italic>Ĥ<sub>j</sub></italic>) in the closed loop (see <xref ref-type="fig" rid="pcbi-0020165-g006">Figure 6</xref>). The filters <italic>H<sub>j</sub></italic> will be defined in such a way that <italic>H<sub>j</sub></italic>(<italic>t</italic>) ≥ 1 signals in the closed loop that the FSM <italic>A</italic> is at time <italic>t</italic> in state <italic>j</italic>. To make this implementation noise-robust, we make sure that even if one replaces the filters <italic>H<sub>j</sub></italic> by noisy approximations <italic>Ĥ<sub>j</sub></italic>, which satisfy in the open loop | <italic>H<sub>j</sub></italic>(<italic>t</italic>) − <italic>Ĥ<sub>j</sub></italic>(<italic>t</italic>) | ≤ ¼ (for all <italic>t</italic> ∈ ℜ and any time-varying inputs <inline-formula id="pcbi-0020165-ex022"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex022" xlink:type="simple"/></inline-formula>
						 and <italic>v</italic><sub>1</sub>(<italic>t</italic>),…,<italic>v<sub>l</sub></italic>(<italic>t</italic>)), then the closed-loop version of such imperfect approximations <italic>Ĥ<sub>j</sub></italic> simulates the FSM <italic>A</italic> in such a way that <italic>Ĥ<sub>j</sub></italic>(<italic>t</italic>) ≥ ⅓ implies that <italic>A</italic> is in state <italic>j</italic> at time <italic>t</italic>.
					</p><p>Let Δ be the time delay in the feedback for the closed loop. We now define the target outputs <italic>H</italic><sub>1</sub>(<italic>t</italic>),…,<italic>H<sub>l</sub></italic>(<italic>t</italic>) (for the open-loop version, where the <italic>H<sub>j</sub></italic> receive in addition to <inline-formula id="pcbi-0020165-ex023"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex023" xlink:type="simple"/></inline-formula>
						 some arbitrary time-varying variables <italic>v</italic><sub>1</sub>(<italic>t</italic>),…,<italic>v<sub>1</sub></italic>(<italic>t</italic>) with values in [−1,2] as inputs). We define the target outputs of <italic>H</italic><sub>1</sub>,…,<italic>H<sub>l</sub></italic> as a stationary transformation of the time-varying inputs <italic>v<sub>j</sub></italic>(<italic>t</italic>) and of the outputs of the following two other types of time invariant fading-memory filters: (i) <inline-formula id="pcbi-0020165-ex024"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex024" xlink:type="simple"/></inline-formula>
						 for <italic>i</italic> = 1,…,<italic>k;</italic> (ii) <italic>v<sub>j</sub></italic>(<italic>t −</italic> 2<italic>δ) for j =</italic> 1<italic>,…,l</italic>. We will show below in Lemma 6 and Lemma 7 that both of these functions of time can be viewed as outputs of time-invariant fading-memory filters that receive as inputs the time-varying functions <inline-formula id="pcbi-0020165-ex025"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex025" xlink:type="simple"/></inline-formula>
						 (for some arbitrary input stream <italic>u</italic>) and <italic>v<sub>j</sub></italic>(<italic>t</italic>). On the basis of these two Lemmata, it is clear that the <italic>H<sub>j</sub></italic> are time-invariant fading-memory filters if one can define <italic>H</italic><sub>1</sub>(<italic>t</italic>),…,<italic>H<sub>l</sub></italic>(<italic>t</italic>) as (static) continuous functions of the variables <italic>v<sub>j</sub></italic>(<italic>t</italic>) and the outputs of the filters (i) and (ii). In the following we sometimes refer to <italic>H</italic><sub>1</sub>,…,<italic>H<sub>l</sub></italic> as static functions of input vectors (<italic>f</italic><sub>1</sub>(<italic>t</italic>),…,<italic>f<sub>k</sub></italic>(<italic>t</italic>)<italic>v</italic><sub>1</sub>(<italic>t</italic>),…,<italic>v<sub>l</sub></italic>(<italic>t</italic>),<italic>v</italic><sub>1</sub>(<italic>t</italic> − 2<italic>δ</italic>),…,<italic>v<sub>l</sub></italic>(<italic>t</italic> − 2<italic>δ</italic>)) from ℜ<italic><sup>k</sup></italic><sup>+2<italic>l</italic></sup>, and sometimes as filters with time-varying inputs <inline-formula id="pcbi-0020165-ex026"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex026" xlink:type="simple"/></inline-formula>
						 and <italic>v<sub>j</sub></italic> (if we view the filters (i) and (ii) as being part of the computation of <italic>H<sub>j</sub></italic>). To define such functions <italic>H<sub>j</sub></italic>(<italic>t</italic>), we first define for each <italic>j</italic> ∈ {1,…,<italic>l</italic>} two disjoint closed and bounded sets <italic>S<sub>j</sub></italic><sub>,0</sub>,<italic>S<sub>j</sub></italic><sub>,1</sub> ∈ ℜ<italic><sup>k</sup></italic><sup>+2<italic>l</italic></sup>, and we set <italic>H<sub>j</sub></italic>(<bold>x</bold>) = 0 for <bold>x</bold> ∈ <italic>S<sub>j</sub></italic><sub>,0</sub> and <italic>H<sub>j</sub></italic>(<bold>x</bold>) = 1 for <bold>x</bold> ∈ <italic>S<sub>j</sub></italic><sub>,1</sub>. Since the sets <italic>S<sub>j</sub></italic><sub>,0</sub> and <italic>S<sub>j</sub></italic><sub>,1</sub> will have positive distance (i.e., inf{|| <bold>x</bold> − <bold>y</bold> ||<bold>:</bold> <bold>x</bold> ∈<italic>S<sub>j</sub></italic><sub>,0</sub> and <bold>y</bold> ∈ <italic>S<sub>j</sub></italic><sub>,1</sub>} &gt; 0), it follows from standard arguments of analysis that the definition of <italic>H<sub>j</sub></italic> can be continued outside of <italic>S<sub>j</sub></italic><sub>,0</sub>,<italic>S<sub>j</sub></italic><sub>,1</sub> to yield a continuous function from ℜ<italic><sup>k</sup></italic><sup>+2<italic>l</italic></sup> into ℜ.
					</p><p>To define the sets <italic>S<sub>j</sub></italic><sub>,0</sub>,<italic>S<sub>j</sub></italic><sub>,1</sub>, we consider the following two types of conditions:</p><p>(<italic>A<sub>j</sub></italic>) There exist <italic>i</italic> ∈{1,…,<italic>k</italic>} and <italic>j</italic>′ ∈{1,…,<italic>l</italic>} so that <italic>TR</italic>(<italic>i</italic>,<italic>j</italic>′) = <italic>j, f<sub>i</sub></italic>(<italic>t</italic>) ≥ ⅓ and <italic>f<sub>i</sub></italic><sub>′</sub>(<italic>t</italic>) ≤ ¼ for all <italic>i</italic>′ ≠ <italic>i</italic>, <italic>v<sub>j′</sub></italic>(<italic>t</italic> − 2<italic>δ</italic>) ≥ ⅓ and <italic>v<sub>j</sub></italic><sub>*</sub>(<italic>t</italic> − 2<italic>δ</italic>) ≤ ¼ for all <italic>j<sup>*</sup></italic> ≠ <italic>j</italic>′.</p><p>(<italic>B<sub>j</sub></italic>)<italic>f<sub>i</sub></italic>(<italic>t</italic>) ≤ ¼ for <italic>i</italic> = 1,…,<italic>k</italic>, <italic>v<sub>j</sub></italic>(<italic>t</italic>) ≥ ⅓ and <italic>v<sub>j</sub></italic><sub>*</sub>(<italic>t</italic>) ≤ ¼ for all <italic>j<sup>*</sup></italic> ≠ <italic>j</italic>.</p><p>We say that a vector (<italic>f</italic><sub>1</sub>(<italic>t</italic>),…,<italic>f<sub>k</sub></italic>(<italic>t</italic>),<italic>v</italic><sub>1</sub>(<italic>t</italic>),…,<italic>v<sub>l</sub></italic>(<italic>t</italic>),<italic>v</italic><sub>1</sub>(<italic>t</italic> − 2<italic>δ</italic>),…,<italic>v<sub>l</sub></italic>(<italic>t</italic> − 2<italic>δ</italic>)) ∈[−<italic>B</italic>,<italic>B</italic>]<italic><sup>k</sup></italic> × [−1,2]<sup>2<italic>l</italic></sup> belongs to set <italic>S<sub>j</sub></italic><sub>,1</sub> if the conditions <italic>A<sub>j</sub></italic> or <italic>B<sub>j</sub></italic> apply, and to set <italic>S<sub>j</sub></italic><sub>,0</sub> if there exists some <italic>j<sup>*</sup></italic> ≠ <italic>j</italic> so that the conditions <italic>A<sub>j</sub></italic><sub>*</sub> or <italic>B<sub>j</sub></italic><sub>*</sub> apply.</p><p>It follows immediately from the definition of the sets <italic>S<sub>j</sub></italic><sub>,0</sub> and <italic>S<sub>j</sub></italic><sub>,1</sub> that they are closed and bounded. One can also verify immediately that for any <italic>j</italic>,<italic>j</italic>′ ∈ {1,…,<italic>l</italic>} the ∈ conditions <italic>A<sub>j</sub></italic> and <italic>B<sub>j</sub></italic><sub>′</sub> can never be simultaneously satisfied (for any values of the variables <italic>f<sub>i</sub></italic>(<italic>t</italic>),<italic>v<sub>j</sub></italic>(<italic>t</italic>),<italic>v<sub>j</sub></italic>(<italic>t</italic> − 2<italic>δ</italic>)). In addition the conditions <italic>A<sub>j</sub></italic> and <italic>A<sub>j</sub></italic><sub>′</sub> (<italic>B<sub>j</sub></italic> and <italic>B<sub>j</sub></italic><sub>′</sub>) can never be simultaneously satisfied for any <italic>j</italic> ≠ <italic>j</italic>′. This implies that the sets <italic>S<sub>j</sub></italic><sub>,0</sub> and <italic>S<sub>j</sub></italic><sub>,1</sub> are disjoint for each <italic>j</italic> ∈ {1,…,<italic>l</italic>}.</p><p>We define for each <italic>j</italic> ∈ {1,…,<italic>l</italic>} a continuous function <italic>H<sub>j</sub></italic>: ℜ<italic><sup>k</sup></italic><sup>+2<italic>l</italic></sup> → [0,1] by setting
						<disp-formula id="pcbi-0020165-e043"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e043" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>H</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy='false'>)</mml:mo><mml:mo>:</mml:mo><mml:mo>&equals;</mml:mo><mml:mo stretchy='true'>&lcub;</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign='left'><mml:mtext>&emsp;if</mml:mtext><mml:mtext>&thinsp;</mml:mtext><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd></mml:mtd><mml:mtd><mml:mspace width='2em'/><mml:mo>&ge;</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mstyle mathvariant='bold' mathsize='normal'><mml:mi>x</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mo>/</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign='left'><mml:mtext>&emsp;otherwise,</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:math> --></disp-formula>where <italic>dist</italic>(<bold>x</bold>,<italic>S</italic>): = inf{|| <bold>x</bold> − <bold>y</bold> ||: <italic>y</italic> ∈ <italic>S</italic>} for any set <italic>S</italic> ∈ ℜ<italic><sup>k</sup></italic><sup>+2<italic>l</italic></sup>. It is then obvious that <italic>H<sub>j</sub></italic> is a continuous function from ℜ<italic><sup>k</sup></italic><sup>+2<italic>l</italic></sup> into [0,1] with <italic>H<sub>j</sub></italic>(<bold>x</bold>) = 0 for all <bold>x</bold> ∈ <italic>S<sub>j</sub></italic><sub>,0</sub> and <italic>Hj</italic>(<bold>x</bold>) = 1 for all <bold>x</bold> ∈ <italic>S<sub>j</sub></italic><sub>,1</sub>. These functions <italic>H<sub>j</sub></italic> will prevent the amplification of noise in the closed loop, since they assume outputs 1 or 0 in all relevant situations, even if their inputs deviate by up to ¼ from their “ideal” values.
					</p><p>We consider some arbitrary imprecise and/or noisy versions <italic>Ĥj</italic> of these filters <italic>Ĥj</italic> (with inputs <inline-formula id="pcbi-0020165-ex027"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex027" xlink:type="simple"/></inline-formula>
						 <inline-formula id="pcbi-0020165-ex028"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex028" xlink:type="simple"/></inline-formula>
						 and additional inputs <italic>v</italic><sub>1</sub>(<italic>t</italic>),…,<italic>v<sub>l</sub></italic>(<italic>t</italic>) whose output differs at any time <italic>t</italic> by at most ¼ from that of <italic>H<sub>j</sub></italic> (of course in the closed loop these deviations could be accumulated and amplified to values &gt;¼). We want to show that for any such <italic>Ĥ</italic><sub>1</sub>,…,<italic>Ĥ<sub>l</sub></italic> the closed loop version of the circuit implements the given FSM <italic>A</italic>. As initial condition we assume that the given FSM <italic>A</italic> is in state 1 for <italic>t</italic> ≤ 0, and consequently also that <italic>Ĥ</italic><sub>1</sub>(<italic>t</italic>) ≥ ⅓ and <italic>Ĥ<sub>j</sub></italic>(<italic>t)</italic> ≤ ¼ for <italic>j</italic> = 2,…,l, as well as <italic>f<sub>i</sub></italic>(<italic>t</italic>) ≤ ¼ for all <italic>t</italic> ≤ 0 and <italic>i</italic> = 1,…,<italic>k</italic>.
					</p><p>We will now prove the claim of Theorem 5 for arbitrary time intervals [<italic>t</italic><sub>1</sub>,<italic>t</italic><sub>2</sub>] outside of switching episodes. We assume without loss of generality that <italic>t</italic><sub>2</sub> marks the beginning of the next switching episode [<italic>t</italic><sub>2</sub>,<italic>t</italic><sub>3</sub>] for some <italic>t</italic><sub>3</sub> &gt; <italic>t</italic><sub>2</sub> with | <italic>t</italic><sub>3</sub> − <italic>t</italic><sub>2</sub> | ≤ <italic>δ.</italic> Furthermore we assume that either <italic>t</italic><sub>1</sub> = 0 (Case 1), or <italic>t</italic><sub>1</sub> is the endpoint of the preceding switching episode [<italic>t</italic><sub>0</sub>,<italic>t</italic><sub>1</sub>] with | <italic>t</italic><sub>1</sub> − <italic>t</italic><sub>0</sub> | ≤ δ (Case 2). The formal proof is carried out by induction on the number of preceding switching episodes (and Case 2 represents the induction step). In both cases one just needs to analyze the outputs of the previously defined filters <inline-formula id="pcbi-0020165-ex029"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex029" xlink:type="simple"/></inline-formula>
						 in the case where some of their inputs are delayed feedbacks of their previous outputs.
					</p><p><underline>Case 1</underline>: <italic>t</italic><sub>1</sub> = 0. We prove by a nested induction on <italic>m</italic> ∈
						<preformat position="float" preformat-type="Script" xml:space="preserve">N</preformat> that <italic>CL</italic> − <italic>Ĥ</italic><sub>1</sub>(<italic>t</italic>) ≥ ⅓ and CL − <italic>Ĥ<sub>j</sub></italic>(<italic>t</italic>) ≤ ¼ for all <italic>j</italic> &gt; 1 holds for all <italic>t</italic> ∈ [<italic>m</italic> · Δ,(<italic>m</italic> + 1) · Δ) ∩[<italic>t</italic><sub>1</sub>,<italic>t</italic><sub>2</sub>]. Since by assumption no switching episode occurs during [<italic>t</italic><sub>1</sub>,<italic>t</italic><sub>2</sub>], one has <italic>f<sub>i</sub></italic>(<italic>t</italic>) ≤ ¼ for <italic>i</italic> = 1,…,<italic>k</italic> and for all <italic>t</italic> ∈ [<italic>t</italic><sub>1</sub>,<italic>t</italic><sub>2</sub>]. Furthermore, by our assumption on the initial condition of the FSM <italic>A</italic> (for <italic>m</italic> = 0), or by the induction hypothesis of the nested induction (for <italic>m</italic> &gt; 0), we can assume that the variables <italic>v<sub>j</sub></italic>(<italic>t</italic>) of the open loop have now been assigned in the closed loop the values CL – <italic>Ĥ<sub>j</sub></italic>(<italic>t</italic> − Δ); therefore, they are ≥⅓ for <italic>j</italic> = 1 and ≤¼ for all <italic>j</italic> &gt; 1. Hence condition <italic>B</italic><sub>1</sub> in the definition of the sets <italic>S<sub>j</sub></italic>,<sub>0</sub>,<italic>S<sub>j</sub></italic><sub>,1</sub> applies, and the current circuit input is therefore in <italic>S</italic><sub>1,1</sub>. Thus <italic>H</italic><sub>1</sub> = 1 and <italic>H<sub>j</sub></italic> = 0 for <italic>j</italic> &gt; 1, which implies <italic>Ĥ</italic><sub>1</sub> ≥ ⅓ and <italic>Ĥ<sub>j</sub></italic> ≤ ¼ for <italic>j</italic> &gt; 1 in the open loop, hence <italic>CL</italic> − <italic>Ĥ</italic><sub>1</sub>(<italic>t</italic>) ≥ ⅓ and <italic>CL</italic> − <italic>Ĥj</italic>(<italic>t</italic>) ≤ ¼ for <italic>j</italic> &gt; 1 in the closed loop (since <italic>v<sub>j</sub></italic>(<italic>t</italic>) = <italic>CL</italic> − <italic>Ĥ<sub>j</sub></italic>(<italic>t</italic> − Δ) in the closed loop).
					</p><p><underline>Case 2</underline>: <italic>t</italic><sub>1</sub> is the endpoint of a preceding switching episode [<italic>t</italic>0,<italic>t</italic><sub>1</sub>]. Assume that <inline-formula id="pcbi-0020165-ex030"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex030" xlink:type="simple"/></inline-formula>
						 is the (approximating) pattern detector that assumes a value ≥¾ during the preceding switching episode [<italic>t</italic><sub>0</sub>,<italic>t</italic><sub>1</sub>], while <inline-formula id="pcbi-0020165-ex031"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex031" xlink:type="simple"/></inline-formula>
						 for all <italic>i</italic>′ ≠ <italic>i</italic> during [<italic>t</italic><sub>0</sub>,<italic>t</italic><sub>1</sub>]. Let <italic>t</italic>′ ∈[<italic>t</italic><sub>0</sub>,<italic>t</italic><sub>1</sub>] be the first timepoint where <inline-formula id="pcbi-0020165-ex032"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex032" xlink:type="simple"/></inline-formula>
						 reaches a value ≥¾. Then <italic>f<sub>i</sub></italic>(<italic>t</italic>) ≥ ¾ and <italic>f<sub>i*</sub></italic>(<italic>t</italic>) ≤ ¼ for all <italic>i<sup>*</sup></italic> ≠ <italic>i</italic> and for all t ∈ [<italic>t</italic>′,<italic>t</italic>′ + Δ + <italic>δ</italic>] (by the definition of the filters <italic>f<sub>i</sub></italic>(<italic>t</italic>)). Furthermore, one has by the induction hypothesis that for the state <italic>j</italic>′ in which the FSM <italic>A</italic> was before the switching episode [<italic>t</italic><sub>0</sub>,<italic>t</italic><sub>1</sub>] that <italic>CL</italic> − <italic>Ĥ<sub>j</sub></italic><sub>′</sub>(<italic>t</italic> − Δ − 2<italic>δ</italic> ≥ ¾ and <italic>CL</italic> − <italic>Ĥ<sub>j*</sub></italic>(<italic>t</italic> − Δ − 2<italic>δ</italic>) ≤ ¼ for all <italic>j</italic><sup>*</sup> ≠ <italic>j</italic>′ and all t ∈ [<italic>t</italic>′,<italic>t</italic>′ + Δ + 2<italic>δ</italic>]. We exploit here that <italic>t</italic><sub>0</sub> ≤ <italic>t</italic>′ ≤ <italic>t</italic><sub>1</sub> ≤ <italic>t</italic><sub>0</sub> + <italic>δ</italic>, hence <italic>t</italic><sub>0</sub> − Δ − 2<italic>δ</italic> ≤ <italic>t</italic> − Δ − 2<italic>δ</italic> ≤ <italic>t</italic><sub>0</sub> for all t ∈ [<italic>t</italic>′,<italic>t</italic>′ + Δ + <italic>δ</italic>]. Furthermore, we have assumed that the minimal distance between the beginnings of switching episodes is Δ + 3<italic>δ</italic>. Therefore, the considered range [<italic>t</italic><sub>0</sub> − Δ − 2<italic>δ</italic>,<italic>t</italic><sub>0</sub>] for <italic>t</italic> − Δ − 2<italic>δ</italic> is contained in the preceding time interval before the switching episode [<italic>t</italic><sub>0</sub>,<italic>t</italic><sub>1</sub>] to which the induction hypothesis applies.
					</p><p>The previously listed conclusions imply that for <italic>t</italic> ∈ [<italic>t</italic>′,<italic>t</italic>′ + Δ + <italic>δ</italic>] the current input to the open loop lies in the set <italic>S<sub>j</sub></italic><sub>,1</sub> for <italic>j</italic> = <italic>TR</italic>(<italic>i</italic>,<italic>j</italic>′), hence <italic>H<sub>j</sub></italic> = 1 and <italic>Ĥ<sub>j</sub></italic> ≥ ¾, while <italic>H<sub>j*</sub></italic> = 0 and <italic>Ĥ<sub>j*</sub></italic> ≤ ¼ for all other <italic>j<sup>*</sup></italic>. But if one chooses as inputs <italic>v</italic><sub>1</sub>(<italic>t</italic>),…,<italic>v<sub>l</sub></italic>(<italic>t</italic>) to the open loop just those values which the circuit receives in the closed loop, one gets <italic>CL</italic> − <italic>Ĥ<sub>j</sub></italic>(<italic>t</italic>) ≥ ¾ and <italic>CL</italic> − <italic>Ĥ<sub>j*</sub></italic>(<italic>t</italic>) ≤ ¼ for all <italic>j<sup>*</sup></italic> ≠ <italic>j</italic> and all <italic>t</italic> ∈ [<italic>t</italic>′,<italic>t</italic>′ + Δ + <italic>δ</italic>], in particular for all <italic>t</italic> ∈[<italic>t</italic><sub>1</sub>,<italic>t</italic><sub>1</sub> + Δ].</p><p>One can then prove by a nested induction on <italic>m</italic> ∈
						<preformat position="float" preformat-type="Script" xml:space="preserve">N</preformat> like in Case 1 that the outputs <italic>CL</italic> − <italic>Ĥ<sub>j*</sub></italic>(<italic>t</italic>) for <italic>j<sup>*</sup></italic> = 1,…,<italic>l</italic> have the desired values for <italic>t</italic> ∈[<italic>t</italic><sub>1</sub> + <italic>m</italic>Δ,<italic>t</italic><sub>1</sub> + (<italic>m</italic> + 1) · Δ]∩ [<italic>t</italic><sub>1</sub>,<italic>t</italic><sub>2</sub>]. The preceding argument provides the verification of the claim for the initial step <italic>m</italic> = 0 of this nested induction.
					</p><p>To complete the proof of Theorem 5, it only remains to verify the following two simple facts about time-invariant fading-memory filters.</p><p><underline>Lemma 6</underline>. <named-content content-type="genus-species" xlink:type="simple">Assume that</named-content> <inline-formula id="pcbi-0020165-ex033"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex033" xlink:type="simple"/></inline-formula>
						 <italic>is some arbitrary time-invariant fading-memory filter, and Δ</italic>,<italic>δ are arbitrary positive constants. Then the map that assigns to an input stream</italic> <bold>u</bold> <italic>the function</italic> <inline-formula id="pcbi-0020165-ex034"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex034" xlink:type="simple"/></inline-formula>
						 <italic>is also a time-invariant fading-memory filter.</italic>
					</p><p><underline>Proof of Lemma 6</underline>: Assume some <italic>ɛ</italic> &gt; 0 is given. Fix <italic>δ</italic>′ and <italic>T</italic> &gt; 0 so that <inline-formula id="pcbi-0020165-ex035"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex035" xlink:type="simple"/></inline-formula>
						 for all <italic>τ</italic> ∈ [<italic>t</italic> − Δ − <italic>δ</italic>,<italic>t</italic>] and all <bold>u,v</bold> with || <bold>u</bold>(<italic>s</italic>) − <bold>v</bold>(<italic>s</italic>) || &lt; <bold>δ′</bold> for all <italic>s</italic> ∈ [<italic>t</italic> − Δ − <italic>δ</italic> − <italic>T</italic>,<italic>t</italic>].
					</p><p>Then |max{(
						<italic>F̂</italic><italic><sub>i</sub></italic><bold>u</bold>)(τ): <italic>t</italic> − Δ − δ ≤ τ ≤ <italic>t</italic>} −max{(
						<italic>F̂</italic><italic><sub>i</sub></italic><bold>v</bold>)(τ): <italic>t</italic> − Δ − δ ≤ τ ≤ <italic>t</italic>}| &lt; ɛ.
					</p><p><underline>Lemma 7</underline>. <italic>The filter that maps for some arbitrary fixed δ</italic> &gt; 0 <italic>the function u</italic>(<italic>t</italic>) <italic>onto the function u</italic>(<italic>t −</italic> 2<italic>δ</italic>) <italic>is time-invariant and has fading memory.</italic></p><p><underline>Proof of Lemma 7</underline> follows immediately from the definitions (choose T ≥ 2<italic>δ</italic> in the condition for fading memory).</p><p>This completes the proof of Theorem 5, which shows that any given FSM can be reliably implemented by fading-memory filters with feedback even in the presence of noise.</p><p><underline>Remark.</underline> In the application of this theory to cortical microcircuit models, we train readouts from such circuits to <italic>simultaneously</italic> assume the role of the pattern detectors <inline-formula id="pcbi-0020165-ex036"><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.0020165.ex036" xlink:type="simple"/></inline-formula>
						, which become active if some pattern occurs in the input stream that may trigger a state change of the simulated FSM <italic>A</italic>, <italic>and</italic> the role of the fading-memory filters <italic>Ĥ</italic><sub>1</sub>,…,<italic>Ĥ<sub>l</sub></italic>, which create high-dimensional attractors of the circuit dynamics that represent the current state of the FSM <italic>A</italic>.
					</p></sec><sec id="s4a2"><title>Details of the cortical microcircuit models.</title><p>We complement in this section the general description of the simulated cortical microcircuit models from the section Applications to Generic Cortical Microcircuit Models, providing in particular all missing data that are needed to reproduce our simulation results. The original code that was used for these simulations is online available at <ext-link ext-link-type="uri" xlink:href="http://www.lsm.tugraz.at/research/index.html" xlink:type="simple">http://www.lsm.tugraz.at/research/index.html</ext-link>.</p><p>Each circuit consisted of 600 neurons, which were placed on the integer grid points of a 5 × 5 × 24 grid. Twenty percent of these neurons were randomly chosen to be inhibitory. The probability of a synaptic connection from neuron <italic>a</italic> to neuron <italic>b</italic> (as well as that of a synaptic connection from neuron <italic>b</italic> to neuron <italic>a</italic>) was defined as <italic>C</italic> · exp(−<italic>D</italic><sup>2</sup>(<italic>a</italic>,<italic>b</italic>)/<italic>λ</italic><sup>2</sup>), where <italic>D</italic>(<italic>a</italic>,<italic>b</italic>) is the Euclidean distance between neurons <italic>a</italic> and <italic>b</italic>, and <italic>λ</italic> is a parameter that controls both the average number of connections and the average distance between neurons that are synaptically connected (we set <italic>λ</italic> = 3.). Depending on whether the pre- or postsynaptic neuron was excitatory (<italic>E</italic>) or inhibitory <italic>(I),</italic> the value of <italic>C</italic> was set according to [<xref ref-type="bibr" rid="pcbi-0020165-b044">44</xref>] to 0.3 <italic>(EE),</italic> 0.2 <italic>(EI),</italic> 0.4 <italic>(IE),</italic> 0.1 <italic>(II),</italic> yielding an average of 10,900 synapses for the chosen circuit size. External inputs and feedbacks from readouts were connected to populations of neurons in the circuit with randomly chosen connection strengths.</p><p><underline>I&amp;F neurons</underline>. A standard leaky I&amp;F neuron model was used, where the membrane potential <italic>V<sub>m</sub></italic> of a neuron is given by:
						<disp-formula id="pcbi-0020165-e012"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e012" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>&tau;</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mfrac><mml:mi>d</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mfrac><mml:mo>&equals;</mml:mo><mml:mo>&minus;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>&sdot;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>s</mml:mi><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:math> --></disp-formula>where <italic>t<sub>m</sub></italic> is the membrane time constant (30 ms), which subsumes the time constants of synaptic receptors as well as the time constant of the neuron membrane. Other parameters are: absolute refractory period 3 ms (excitatory neurons), 2 ms (inhibitory neurons); threshold 15 mV (for a resting membrane potential V<italic><sub>resting</sub></italic>, assumed to be 0), reset voltage drawn uniformly from the interval [13.8 mV, 14.5 mV] for each neuron; input resistance <italic>R<sub>m</sub></italic>, 1 MΩ, constant nonspecific background current <italic>I<sub>inject</sub></italic> uniformly drawn from the interval [13.8 mV, 14.5 mV] for each neuron; input resistance R<italic><sub>m</sub></italic>, 1 MΩ, constant nonspecific background current I<italic><sub>inject</sub></italic> uniformly drawn from the interval [13.5 nA, 14.5 nA] for each neuron; additional time-varying noise input current <italic>I<sub>noise</sub></italic> drawn every 5 ms from a Gaussian distribution with mean 0; and SD chosen randomly for each neuron from the uniform distribution over the interval [4.0 nA, 5.0 nA]. For each simulation, the initial condition of each I&amp;F neuron, i.e., its membrane voltage at time <italic>t</italic> = 0, was drawn randomly (uniform distribution) from the interval [13.5 mV, 14.9 mV]. Finally, I<italic><sub>syn</sub></italic>(<italic>t</italic>) is the sum of input currents supplied by the explicitly modeled synapses.
					</p><p><underline>HH neurons</underline>: We used single-compartment HH neuron models with passive and active properties modeled according to [<xref ref-type="bibr" rid="pcbi-0020165-b048">48</xref>,<xref ref-type="bibr" rid="pcbi-0020165-b049">49</xref>]. The membrane potential was modeled by
						<disp-formula id="pcbi-0020165-e013"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e013" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>C</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mfrac><mml:mi>d</mml:mi><mml:mi>V</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mfrac><mml:mo>&equals;</mml:mo><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy='false'>)</mml:mo><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>K</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>a</mml:mi></mml:mfrac><mml:msub><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>s</mml:mi><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mtext>&thinsp;</mml:mtext><mml:mo>,</mml:mo></mml:math> --></disp-formula>where <italic>C<sub>m</sub></italic> = 1<italic>μF</italic>/<italic>cm</italic><sup>2</sup> is the specific membrane capacitance, <italic>g<sub>L</sub></italic> = 0.045 <italic>mS</italic>/<italic>cm</italic><sup>2</sup> is the leak conductance density, <italic>E<sub>L</sub></italic> = −80 <italic>mV</italic> is the leak reversal potential, and <italic>I<sub>syn</sub></italic>(<italic>t</italic>) is the input current supplied by explicitly modeled synapses (see the definition below). The membrane area <italic>a</italic> of the neuron was set to be 34,636 <italic>μm</italic><sup>2</sup> as in [<xref ref-type="bibr" rid="pcbi-0020165-b048">48</xref>]. The term <italic>I<sub>noise</sub></italic>(<italic>t</italic>) (see the precise definition below) models smaller background input currents from a large number of more distal neurons, causing a depolarization of the membrane potential and a lower input resistance commonly referred to as “high conductance state” (for a review see [<xref ref-type="bibr" rid="pcbi-0020165-b042">42</xref>]).
					</p><p>In accordance with experimental data on neocortical and hippocampal pyramidal neurons ([<xref ref-type="bibr" rid="pcbi-0020165-b050">50</xref>–<xref ref-type="bibr" rid="pcbi-0020165-b053">53</xref>]) the active currents in the HH neuron model comprise a voltage dependent <italic>Na</italic><sup>+</sup> current <italic>I<sub>Na</sub></italic> ([<xref ref-type="bibr" rid="pcbi-0020165-b054">54</xref>]) and a delayed rectifier <italic>K</italic><sup>+</sup> current <italic>I<sub>Kd</sub></italic> ([<xref ref-type="bibr" rid="pcbi-0020165-b054">54</xref>]). For excitatory neurons, a noninactivating <italic>K</italic><sup>+</sup> current <italic>I<sub>M</sub></italic> ([<xref ref-type="bibr" rid="pcbi-0020165-b055">55</xref>]) responsible for spike frequency adaption was included in the model.</p><p>The voltage-dependent <italic>Na</italic><sup>+</sup> current was modeled by:
						<disp-formula id="pcbi-0020165-e046"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e046" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>I</mml:mi><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:msub><mml:mover accent='true'><mml:mi>g</mml:mi><mml:mo>&macr;</mml:mo></mml:mover><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:msup><mml:mi>m</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mi>h</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0020165-e047"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e047" xlink:type="simple"/><!-- <mml:math display='block'><mml:mfrac><mml:mi>d</mml:mi><mml:mi>m</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mfrac><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>&alpha;</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&minus;</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>&beta;</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mi>m</mml:mi></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0020165-e048"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e048" xlink:type="simple"/><!-- <mml:math display='block'><mml:mfrac><mml:mi>d</mml:mi><mml:mi>h</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mfrac><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>&alpha;</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&minus;</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>&beta;</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mi>h</mml:mi></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0020165-e049"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e049" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>&alpha;</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mfrac><mml:mo>&minus;</mml:mo><mml:mn>0.32</mml:mn><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:mn>13</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mo>&minus;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:mn>13</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mo>/</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy='false'>&rsqb;</mml:mo><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn></mml:mfrac></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0020165-e050"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e050" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>&beta;</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mfrac><mml:mn>0.28</mml:mn><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:mn>40</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:mn>40</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mo>/</mml:mo><mml:mn>5</mml:mn><mml:mo stretchy='false'>&rsqb;</mml:mo><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn></mml:mfrac></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0020165-e051"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e051" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>&alpha;</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mn>0.128</mml:mn><mml:mtext>&thinsp;</mml:mtext><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mo>&minus;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:mn>17</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mo>/</mml:mo><mml:mn>18</mml:mn><mml:mo stretchy='false'>&rsqb;</mml:mo></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0020165-e052"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e052" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>&beta;</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mfrac><mml:mn>4</mml:mn><mml:mn>1</mml:mn><mml:mo>&plus;</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mo>&minus;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:mn>40</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mo>/</mml:mo><mml:mn>5</mml:mn><mml:mo stretchy='false'>&rsqb;</mml:mo></mml:mfrac></mml:math> --></disp-formula>where <italic>V<sub>T</sub></italic> = −63 <italic>mV</italic>, and the inactivation was shifted by 10 <italic>mV</italic> toward hyperpolarized values (<italic>V<sub>S</sub></italic> = 10 <italic>mV</italic>) to reflect the voltage dependence of <italic>Na</italic><sup>+</sup> currents in neocortical pyramidal cells [<xref ref-type="bibr" rid="pcbi-0020165-b056">56</xref>]. The peak conductance densities for the <italic>I<sub>Na</sub></italic> current was chosen to be 500 <italic>pS</italic>/<italic>μm</italic><sup>2</sup>.
					</p><p>The delayed rectifier <italic>K</italic><sup>+</sup> current was modeled by:
						<disp-formula id="pcbi-0020165-e053"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e053" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>I</mml:mi><mml:mi>K</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:msub><mml:mover accent='true'><mml:mi>g</mml:mi><mml:mo>&macr;</mml:mo></mml:mover><mml:mi>K</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:msup><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msup><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0020165-e054"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e054" xlink:type="simple"/><!-- <mml:math display='block'><mml:mfrac><mml:mi>d</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mfrac><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>&alpha;</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&minus;</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>&beta;</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mi>n</mml:mi></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0020165-e055"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e055" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>&alpha;</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mfrac><mml:mo>&minus;</mml:mo><mml:mn>0.032</mml:mn><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:mn>15</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mo>&minus;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:mn>15</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mo>/</mml:mo><mml:mn>5</mml:mn><mml:mo stretchy='false'>&rsqb;</mml:mo><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn></mml:mfrac></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0020165-e056"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e056" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>&beta;</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mn>0.5</mml:mn><mml:mtext>&thinsp;</mml:mtext><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mo>&minus;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>&minus;</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mo>/</mml:mo><mml:mn>40</mml:mn><mml:mo stretchy='false'>&rsqb;</mml:mo></mml:math> --></disp-formula>The peak conductance densities for the <italic>I<sub>Kd</sub></italic> current was chosen to be 100 <italic>pS</italic>/<italic>μm</italic><sup>2</sup>.
					</p><p>The noninactivating <italic>K</italic><sup>+</sup> current was modeled by:
						<disp-formula id="pcbi-0020165-e057"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e057" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:msub><mml:mover accent='true'><mml:mi>g</mml:mi><mml:mo>&macr;</mml:mo></mml:mover><mml:mi>M</mml:mi></mml:msub><mml:mi>n</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo stretchy='false'>)</mml:mo></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0020165-e058"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e058" xlink:type="simple"/><!-- <mml:math display='block'><mml:mfrac><mml:mi>d</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mfrac><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>&alpha;</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&minus;</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>&beta;</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mi>n</mml:mi></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0020165-e059"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e059" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>&alpha;</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mfrac><mml:mn>0.0001</mml:mn><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo>&plus;</mml:mo><mml:mn>30</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mn>1</mml:mn><mml:mo>&minus;</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mo>&minus;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo>&plus;</mml:mo><mml:mn>30</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mo>/</mml:mo><mml:mn>9</mml:mn><mml:mo stretchy='false'>&rsqb;</mml:mo></mml:mfrac></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0020165-e060"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e060" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>&beta;</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mfrac><mml:mo>&minus;</mml:mo><mml:mn>0.0001</mml:mn><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo>&plus;</mml:mo><mml:mn>30</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mn>1</mml:mn><mml:mo>&minus;</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mo stretchy='false'>(</mml:mo><mml:mi>V</mml:mi><mml:mo>&plus;</mml:mo><mml:mn>30</mml:mn><mml:mo stretchy='false'>)</mml:mo><mml:mo>/</mml:mo><mml:mn>9</mml:mn><mml:mo stretchy='false'>&rsqb;</mml:mo></mml:mfrac></mml:math> --></disp-formula>The peak conductance density for the <italic>I<sub>M</sub></italic> current was chosen to be 5 <italic>pS</italic>/<italic>μm</italic><sup>2</sup>.
					</p><p>For each simulation, the initial condition of each neuron, i.e., the membrane voltage at time <italic>t</italic> = 0, was drawn randomly (uniform distribution) from the interval [−70, −60] mV.</p><p>The total <bold>synaptic background current</bold>, <italic>I<sub>noise</sub></italic>(<italic>t</italic>), was a sum of two independent currents:
						<disp-formula id="pcbi-0020165-e061"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e061" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math> --></disp-formula>where <italic>g<sub>e</sub></italic>(<italic>t</italic>) and <italic>g<sub>i</sub></italic>(<italic>t</italic>) are time-dependent excitatory and inhibitory conductances. The values of respective reversal potentials were <italic>E<sub>e</sub></italic> = 0 <italic>mV</italic> and <italic>E<sub>i</sub></italic> = −75 <italic>mV</italic> mV.
					</p><p>The conductances <italic>g<sub>e</sub></italic>(<italic>t</italic>) and <italic>g<sub>i</sub></italic>(<italic>t</italic>) were modeled according to [<xref ref-type="bibr" rid="pcbi-0020165-b048">48</xref>] as a one-variable stochastic process similar to an Ornstein−Uhlenbeck process:
						<disp-formula id="pcbi-0020165-e062"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e062" xlink:type="simple"/><!-- <mml:math display='block'><mml:mfrac><mml:mi>d</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mfrac><mml:mo>&equals;</mml:mo><mml:mo>&minus;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>&tau;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mfrac><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy='false'>&rsqb;</mml:mo><mml:mo>&plus;</mml:mo><mml:msqrt><mml:msub><mml:mi>D</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:msqrt><mml:msub><mml:mi>&chi;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0020165-e063"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e063" xlink:type="simple"/><!-- <mml:math display='block'><mml:mfrac><mml:mi>d</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mfrac><mml:mo>&equals;</mml:mo><mml:mo>&minus;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>&tau;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy='false'>&rsqb;</mml:mo><mml:mo>&plus;</mml:mo><mml:msqrt><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msqrt><mml:msub><mml:mi>&chi;</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy='false'>(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy='false'>)</mml:mo></mml:math> --></disp-formula>where <italic>g<sub>e</sub></italic><sub>0</sub> = 0.012 <italic>μS</italic> and <italic>g<sub>i</sub></italic><sub>0</sub> = 0.057 <italic>μS</italic> are average conductances, <italic>τ<sub>e</sub></italic> = 2.7 ms and <italic>τ<sub>e</sub></italic> = 10.5 ms are time constants, <italic>D<sub>e</sub></italic> = 0.0067 <italic>μS</italic><sup>2</sup>/s and <italic>D<sub>i</sub></italic> = 0.0083 <italic>μS</italic><sup>2</sup>/s are noise-diffusion constants, <italic>χ</italic><sub>1</sub>(<italic>t</italic>) and <italic>χ</italic><sub>2</sub>(<italic>t</italic>) are Gaussian white noise of zero mean and unit standard deviation.
					</p><p>Since these stochastic processes are Gaussian, they can be integrated by an exact update rule:
						<disp-formula id="pcbi-0020165-e064"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e064" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&plus;</mml:mo><mml:mi>&Delta;</mml:mi><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:mo>&lsqb;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&rsqb;</mml:mo><mml:mi>exp</mml:mi><mml:mo></mml:mo><mml:mo>(</mml:mo><mml:mo>&minus;</mml:mo><mml:mi>&Delta;</mml:mi><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>&tau;</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>&plus;</mml:mo><mml:mi>A</mml:mi><mml:mi>e</mml:mi><mml:mspace width="1pt"/><mml:msub><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0020165-e065"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e065" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>&plus;</mml:mo><mml:mi>&Delta;</mml:mi><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&equals;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&plus;</mml:mo><mml:mo>&lsqb;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&rsqb;</mml:mo><mml:mi>exp</mml:mi><mml:mo></mml:mo><mml:mo>(</mml:mo><mml:mo>&minus;</mml:mo><mml:mi>&Delta;</mml:mi><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>&tau;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math> --></disp-formula>where <italic>N</italic><sub>1</sub>(0,1) and <italic>N</italic><sub>2</sub>(0,1) are normal random numbers (zero mean, unit SD) and <italic>A<sub>e</sub></italic> and <italic>A<sub>i</sub></italic> are amplitude coefficients, given by:
						<disp-formula id="pcbi-0020165-e066"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e066" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>A</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:msqrt><mml:mfrac><mml:msub><mml:mi>D</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mi>&tau;</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mn>1</mml:mn><mml:mo>&minus;</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mfrac><mml:mo>&minus;</mml:mo><mml:mn>2</mml:mn><mml:mi>&Delta;</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>&tau;</mml:mi><mml:mi>e</mml:mi></mml:msub></mml:mfrac><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>&rsqb;</mml:mo></mml:msqrt></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0020165-e067"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e067" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:msqrt><mml:mfrac><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mtext>&thinsp;</mml:mtext><mml:msub><mml:mi>&tau;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy='false'>&lsqb;</mml:mo><mml:mn>1</mml:mn><mml:mo>&minus;</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy='false'>(</mml:mo><mml:mfrac><mml:mo>&minus;</mml:mo><mml:mn>2</mml:mn><mml:mi>&Delta;</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mi>&tau;</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac><mml:mo stretchy='false'>)</mml:mo><mml:mo stretchy='false'>&rsqb;</mml:mo></mml:msqrt><mml:mtext>&thinsp;</mml:mtext><mml:mtext>&thinsp;</mml:mtext><mml:mo>.</mml:mo></mml:math> --></disp-formula>
					</p><p>According to [<xref ref-type="bibr" rid="pcbi-0020165-b048">48</xref>], this model captures the spectral and amplitude characteristics of the input conductances of a detailed biophysical model of a neocortical pyramidal cell that was matched to intracellular recordings in cat parietal cortex in vivo. Furthermore, the ratio of the average contributions of excitatory and inhibitory background conductances was chosen to be five in accordance with experimental studies during sensory responses [<xref ref-type="bibr" rid="pcbi-0020165-b057">57</xref>–<xref ref-type="bibr" rid="pcbi-0020165-b059">59</xref>]. The maximum conductances of the synapses were chosen from a Gaussian distribution with a SD of 70% of its mean (with negative values replaced by values chosen from an uniform distribution between 0 and two times the mean).</p><p>We modeled the (short-term) <bold>dynamics of synapses</bold> according to the model proposed in [<xref ref-type="bibr" rid="pcbi-0020165-b043">43</xref>], with the synaptic parameters <italic>U</italic> (use), <italic>D</italic> (time constant for depression), and <italic>F</italic> (time constant for facilitation) randomly chosen from Gaussian distributions that model empirically found data for such connections (see in Methods, Details of the Cortical Microcircuit Models). This model predicts the amplitude <italic>A<sub>k</sub></italic> of the EPSC for the <italic>k<sup>th</sup></italic> spike in a spike train with interspike intervals Δ<sub>1</sub>,Δ<sub>2</sub>,Δ<italic><sub>k</sub></italic><sub>−1</sub> through the equations
						<disp-formula id="pcbi-0020165-e068"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e068" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mi>w</mml:mi><mml:mo>&middot;</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&middot;</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0020165-e069"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e069" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mi>U</mml:mi><mml:mo>&plus;</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&minus;</mml:mo><mml:mi>U</mml:mi><mml:mo>)</mml:mo><mml:mi>exp</mml:mi><mml:mo></mml:mo><mml:mo>(</mml:mo><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>&Delta;</mml:mi><mml:mi>k</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mo>/</mml:mo><mml:mi>F</mml:mi><mml:mo>)</mml:mo></mml:math> --></disp-formula>
						<disp-formula id="pcbi-0020165-e070"><graphic mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.0020165.e070" xlink:type="simple"/><!-- <mml:math display='block'><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&equals;</mml:mo><mml:mn>1</mml:mn><mml:mo>&plus;</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>k</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mi>k</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mi>exp</mml:mi><mml:mo></mml:mo><mml:mo>(</mml:mo><mml:mo>&minus;</mml:mo><mml:msub><mml:mi>&Delta;</mml:mi><mml:mi>k</mml:mi><mml:mo>&minus;</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mo>/</mml:mo><mml:mi>D</mml:mi><mml:mo>)</mml:mo></mml:math> --></disp-formula>with hidden dynamic variables <italic>u</italic> ∈ [0,1] and <italic>R</italic> ∈ [0,1], whose initial values for the first spike are <italic>u</italic><sub>1</sub> = <italic>U</italic> and <italic>R</italic><sub>1</sub> = 1 (see [<xref ref-type="bibr" rid="pcbi-0020165-b060">60</xref>] for a justification of this version of the equations, which corrects a small error in [<xref ref-type="bibr" rid="pcbi-0020165-b043">43</xref>]).
					</p><p>The postsynaptic current for the <italic>k<sup>th</sup></italic> spike in a presynaptic spike train that had been generated at time <italic>t<sub>k</sub></italic>, is modeled for <italic>t</italic> ≥ <italic>t<sub>k</sub></italic> + Δ (where Δ is the transmission delay) by <italic>A<sub>k</sub></italic> exp(−(<italic>t</italic> − <italic>t<sub>k</sub></italic> − Δ)/<italic>τ<sub>s</sub></italic>) with <italic>τ<sub>s</sub></italic> = 3 ms (<italic>τ<sub>s</sub></italic> = 6 ms) for excitatory (inhibitory) synapses. The transmission delays Δ between neurons were chosen uniformly to be 1.5 ms for EE-connections, and 0.8 ms for the other connections. The total <bold>synaptic input current</bold> <italic>i<sub>syn</sub></italic>(<italic>t</italic>) was modeled by the sum of these currents for all synapses onto a neuron.</p><p><underline>Synaptic parameters.</underline> Depending on whether <italic>a</italic> and <italic>b</italic> were excitatory <italic>(E)</italic> or inhibitory <italic>(I),</italic> the mean values of the three parameters <italic>U</italic>,<italic>D</italic>,<italic>F</italic> (with <italic>D</italic>,<italic>F</italic> expressed in seconds, s) were chosen according to [<xref ref-type="bibr" rid="pcbi-0020165-b044">44</xref>] to be .5, 1.1, .05 <italic>(EE),</italic> .05, .125, 1.2 <italic>(EI),</italic> .25, .7, .02 <italic>(IE),</italic> .32, .144, .06 (<italic>II</italic>). The SD of each of these parameters was chosen to be 50% of its mean. The mean of the scaling parameter <italic>w</italic> (in nA) was chosen to be 70 (EE), 150 (EI), −47 (IE), −47 (II). The SD of the parameter <italic>w</italic> was chosen to be 70% of its mean and was drawn from a gamma distribution. In the case of input synapses, the parameter <italic>w</italic> had a value of 70 nA if projecting onto a excitatory neuron and −47 nA if projecting onto an inhibitory neuron.</p><p>The synaptic weights <bold>w</bold> of <bold>readout neurons</bold> were computed by linear regression to minimize the mean squared error (<bold>w</bold> · <bold>x</bold>(<italic>t</italic>) – <italic>f</italic>(<bold>t</bold>))<sup>2</sup> with regard to a specific target output function <italic>f</italic>(<italic>t</italic>) (which is described for each case in the text or figure legends) for a series of randomly generated time-varying circuit input streams <bold>u</bold>(<italic>t</italic>) of length up to 1 s. Up to 200 such time-varying input streams <bold>u</bold>(<italic>t</italic>) were used for training, amounting to at most 200 s of simulated biological time for training the readouts.</p><p>The performance of trained readouts was evaluated by measuring the correlation between <bold>w</bold> · <bold>x</bold>(<italic>t</italic>) and the target function <italic>f</italic>(<italic>t</italic>) during separate testing episodes where the circuit received new input streams <bold>u</bold>(<italic>t</italic>) (that were generated by the same random process as the training inputs).</p><p>All simulations were carried out with the software package CSIM [<xref ref-type="bibr" rid="pcbi-0020165-b061">61</xref>], which is freely available from <ext-link ext-link-type="uri" xlink:href="http://www.lsm.tugraz.at" xlink:type="simple">http://www.lsm.tugraz.at</ext-link>. It uses a C<sup>++</sup>-kernel with Matlab interfaces for input generation and data analysis. As simulation time step, we chose 0.5 ms.</p></sec><sec id="s4d"><title>Technical details of <xref ref-type="fig" rid="pcbi-0020165-g005">Figure 5</xref>.</title><p>Four randomly generated test input streams, each consisting of eight spike trains (see <xref ref-type="fig" rid="pcbi-0020165-g005">Figure 5</xref>A), were injected into four disjoint (but interconnected) subsets of 5 × 5 × 5 = 125 neurons in the circuit consisting of 600 neurons. Feedbacks from readouts were injected into the remaining 100 neurons of the circuit. The set of 100 neurons for which the firing activity is shown in <xref ref-type="fig" rid="pcbi-0020165-g005">Figure 5</xref>C contained 20 neurons from each of the resulting five subsets of the circuit.</p><p><italic>Generation of input streams for training and testing.</italic> The time-varying firing rate <italic>r<sub>i</sub></italic>(<italic>t</italic>) of the eight Poisson spike trains that represented input stream <italic>i</italic> was chosen as follows. The baseline firing rate for streams 1 and 2 (see the lower half of <xref ref-type="fig" rid="pcbi-0020165-g005">Figure 5</xref>A) was chosen to be 5 Hz, with randomly distributed bursts of 120 Hz for 50 ms. The rates for the Poisson processes that generated the spike trains for input streams 3 and 4 were periodically drawn randomly from the two options 30 Hz and 90 Hz. The actual firing rates (i.e., spike counts within a 30-ms window) resulting from this procedure are plotted in <xref ref-type="fig" rid="pcbi-0020165-g005">Figure 5</xref>B.</p><p>To demonstrate that readouts that send feedback into the circuit can just as well represent neurons <italic>within</italic> the circuit, we had chosen the readout neurons that send feedback to be I&amp;F neurons with noise, like the other neurons in the circuit. Each of them received synaptic inputs from a slightly different randomly chosen subset of neurons within the circuit. Furthermore, the signs of weights of these synaptic connections were restricted to be positive (negative) for excitatory (inhibitory) presynaptic neurons.</p><p>The eight readout neurons that provided feedback were trained to represent in their firing activity at any time the information in which of input streams 1 or 2 a burst had most recently occurred. If it occurred most recently in input stream 1, they were trained to fire at 40 Hz, and they were trained not to fire whenever a burst had occurred most recently in input stream 2. The training time was 200 s (of simulated biological time). After training, their output was correct 86% of the time (average over 50 s of input streams, counting the high-dimensional attractor as being in the on state if the average firing rate of the eight readout neurons was above 34 Hz). It was possible to train these readout neurons to acquire such persistent firing behavior, although they only received input from a circuit with fading memory, because they were actually trained to acquire the following behavior: fire whenever the rate in input stream 1 becomes higher than 30 Hz, or if one can detect in the current state <bold>x</bold>(<italic>t</italic>) of the circuit traces of recent high feedback values, provided the rate of input stream 2 stayed below 30 Hz. Obviously this definition of the learning target for readout neurons only requires a fading memory of the circuit.</p><p>The readouts for the other three tasks achieved in 50 tests for new inputs over 1 s (that had been generated by the same distribution as the training inputs, see the preceding description) showed the following average performance: task of panel E: mean correlation: 0.85, task of panel F: mean correlation: 0.63, task of panel G: mean correlation: 0.86.</p></sec><sec id="s4e"><title>Technical details of <xref ref-type="fig" rid="pcbi-0020165-g002">Figure 2</xref>.</title><p>The same circuit as for <xref ref-type="fig" rid="pcbi-0020165-g005">Figure 5</xref> was used. First, two linear readouts with feedback were simultaneously trained to become highly active after the occurrence of the cue in the spike input, and then to linearly reduce their activity, but each within a different timespan (400 ms versus 600 ms). Their feedback into the circuit consisted of two time-varying analog values (representing time-varying firing rates of two populations of neurons), which were both injected (with randomly chosen amplitudes) into the same subset of 350 neurons in the circuit. Their weights <bold>w</bold> were trained by linear regression for a total training time of 120 s (of simulated biological time), consisting of 120 runs of length 1 s with randomly generated input cues (a burst at 200 Hz for 50 ms) and noise inputs (five spike trains at 10 Hz).</p></sec><sec id="s4f"><title>Technical details of <xref ref-type="fig" rid="pcbi-0020165-g003">Figure 3</xref>.</title><p>Time-varying firing rates for the two input streams (each consisting of eight Poisson spike trains) were drawn randomly from values between 10 Hz and 90 Hz. The 16 spike trains from the two input streams, as well as feedback from trained readouts were injected into randomly chosen subsets of neurons. In contrast to the experiment for <xref ref-type="fig" rid="pcbi-0020165-g003">Figure 3</xref>, these circuit inputs were not injected into spatially concentrated clusters of neurons, but to a sparsely distributed subset of neurons scattered throughout the three-dimensional circuit. As a consequence, the firing activity <italic>CA</italic>(<italic>t</italic>) of the high-dimensional attractor (see <xref ref-type="fig" rid="pcbi-0020165-g003">Figure 3</xref>D) cannot be readily detected from the spike raster in <xref ref-type="fig" rid="pcbi-0020165-g003">Figure 3</xref>C. Both the linear readout that sends feedback, and subsequently the other two linear readouts (whose output for a test input to the circuit is shown in <xref ref-type="fig" rid="pcbi-0020165-g003">Figure 3</xref>E and <xref ref-type="fig" rid="pcbi-0020165-g003">3</xref>F), were trained by linear regression during 140 s of simulated biological time.</p><p>Average performance of linear readouts on 100 new test inputs of length 700 ms (that had been generated from the same distribution as the training inputs) was—task of panel D, mean correlation: 0.82; task of panel E, mean correlation: 0.71; task of panel F, mean correlation: 0.79.</p><p>Control experiments (see <xref ref-type="fig" rid="pcbi-0020165-g007">Figure 7</xref>) show that the feedback is essential for the performance of the circuit for these computational tasks.</p><fig id="pcbi-0020165-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.0020165.g007</object-id><label>Figure 7</label><caption><title>Evaluation of the Dependence of the Performance of the Circuit in <xref ref-type="fig" rid="pcbi-0020165-g004">Figure 4</xref> on the Feedback Strength (i.e., on the Mean Amplitude of Current Injection from the Readout Back into Neurons in the Circuit)</title><p>For each feedback strength that was evaluated, the readouts were trained and tested for this feedback strength as for the preceding experiments. Error bars in (B–D) denote standard error. These control experiments show that the feedback is essential for the performance of the circuit.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.0020165.g007" xlink:type="simple"/></fig></sec></sec></sec></body><back><ack><p>Comments from Wulfram Gerstner, Stefan Haeusler, Herbert Jaeger, Konrad Koerding, Henry Markram, Gordon Pipa, Misha Tsodyks, and Tony Zador are gratefully acknowledged. Our computer simulations used software written by Thomas Natschlaeger, Stefan Haeusler, and Michael Pfeiffer.</p></ack><glossary><title>Abbreviations</title><def-list><def-item><term>FSM</term><def><p>finite state machine</p></def></def-item><def-item><term>HH</term><def><p>Hodgkin–Huxley</p></def></def-item><def-item><term>I&amp;F</term><def><p>integrate and fire</p></def></def-item></def-list></glossary><ref-list><title>References</title><ref id="pcbi-0020165-b001"><label>1</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Douglas</surname><given-names>RJ</given-names></name><name name-style="western"><surname>Koch</surname><given-names>C</given-names></name><name name-style="western"><surname>Mahowald</surname><given-names>M</given-names></name><name name-style="western"><surname>Martin</surname><given-names>K</given-names></name><name name-style="western"><surname>Suarez</surname><given-names>H</given-names></name></person-group>
					<year>1995</year>
					<article-title>Recurrent excitation in neocortical circuits.</article-title>
					<source>Science</source>
					<volume>269</volume>
					<fpage>981</fpage>
					<lpage>985</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b002"><label>2</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Grossberg</surname><given-names>S</given-names></name></person-group>
					<year>2003</year>
					<article-title>How does the cerebral cortex work? Development, learning, attention, and 3D vision by laminar circuits of visual cortex.</article-title>
					<source>Behav Cogn Neurosci Rev</source>
					<volume>2</volume>
					<fpage>47</fpage>
					<lpage>76</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b003"><label>3</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Buonomano</surname><given-names>DV</given-names></name><name name-style="western"><surname>Merzenich</surname><given-names>MM</given-names></name></person-group>
					<year>1995</year>
					<article-title>Temporal information transformed into a spatial code by a neural network with realistic properties.</article-title>
					<source>Science</source>
					<volume>267</volume>
					<fpage>1028</fpage>
					<lpage>1030</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b004"><label>4</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name><name name-style="western"><surname>Sontag</surname><given-names>ED</given-names></name></person-group>
					<year>2000</year>
					<article-title>Neural systems as nonlinear filters.</article-title>
					<source>Neural Computation</source>
					<volume>12</volume>
					<fpage>1743</fpage>
					<lpage>1772</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b005"><label>5</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name><name name-style="western"><surname>Natschläger</surname><given-names>T</given-names></name><name name-style="western"><surname>Markram</surname><given-names>H</given-names></name></person-group>
					<year>2002</year>
					<article-title>Real-time computing without stable states: A new framework for neural computation based on perturbations.</article-title>
					<source>Neural Computation</source>
					<volume>14</volume>
					<fpage>2531</fpage>
					<lpage>2560</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b006"><label>6</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Häusler</surname><given-names>S</given-names></name><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name></person-group>
					<year>2007</year>
					<article-title>A statistical analysis of information processing properties of lamina-specific cortical microcircuit models.</article-title>
					<source>Cerebral Cortex</source>
					<comment>epub. Available: <ext-link ext-link-type="uri" xlink:href="http://www.igi.tugraz.at/maass/psfiles/162.pdf" xlink:type="simple">http://www.igi.tugraz.at/maass/psfiles/162.pdf</ext-link>. Accessed 1 December 2006.</comment>
				</element-citation></ref><ref id="pcbi-0020165-b007"><label>7</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Destexhe</surname><given-names>A</given-names></name><name name-style="western"><surname>Marder</surname><given-names>E</given-names></name></person-group>
					<year>2004</year>
					<article-title>Plasticity in single neuron and circuit computations.</article-title>
					<source>Nature</source>
					<volume>431</volume>
					<fpage>789</fpage>
					<lpage>795</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b008"><label>8</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name><name name-style="western"><surname>Natschläger</surname><given-names>T</given-names></name><name name-style="western"><surname>Markram</surname><given-names>H</given-names></name></person-group>
					<year>2004</year>
					<article-title>Fading memory and kernel properties of generic cortical microcircuit models.</article-title>
					<source>J Physiol (Paris)</source>
					<volume>98</volume>
					<fpage>315</fpage>
					<lpage>330</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b009"><label>9</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Leon</surname><given-names>MI</given-names></name><name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name></person-group>
					<year>2003</year>
					<article-title>Representation of time by neurons in the posterior parietal cortex of the macaque.</article-title>
					<source>Neuron</source>
					<volume>38</volume>
					<fpage>317</fpage>
					<lpage>322</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b010"><label>10</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hikosaka</surname><given-names>K</given-names></name><name name-style="western"><surname>Watanabe</surname><given-names>M</given-names></name></person-group>
					<year>2000</year>
					<article-title>Delay activity of orbital and lateral prefrontal neurons of the monkey varying with different rewards.</article-title>
					<source>Cerebral Cortex</source>
					<volume>10</volume>
					<fpage>263</fpage>
					<lpage>267</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b011"><label>11</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Tremblay</surname><given-names>L</given-names></name><name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name></person-group>
					<year>2000</year>
					<article-title>Modifications of reward expectation–related neuronal activity during learning in primate orbitofrontal cortex.</article-title>
					<source>J Neurophysiol</source>
					<volume>83</volume>
					<fpage>1877</fpage>
					<lpage>1885</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b012"><label>12</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name><name name-style="western"><surname>Tremblay</surname><given-names>L</given-names></name><name name-style="western"><surname>Hollerman</surname><given-names>JR</given-names></name></person-group>
					<year>2003</year>
					<article-title>Changes in behavior-related neuronal activity in the striatum during learning.</article-title>
					<source>Trends Neurosci</source>
					<volume>26</volume>
					<fpage>321</fpage>
					<lpage>328</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b013"><label>13</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>XJ</given-names></name></person-group>
					<year>2001</year>
					<article-title>Synaptic reverberation underlying mnemonic persistent activity.</article-title>
					<source>Trends Neurosci</source>
					<volume>24</volume>
					<fpage>455</fpage>
					<lpage>463</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b014"><label>14</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Mazurek</surname><given-names>ME</given-names></name><name name-style="western"><surname>Roitman</surname><given-names>JD</given-names></name><name name-style="western"><surname>Ditterich</surname><given-names>J</given-names></name><name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name></person-group>
					<year>2003</year>
					<article-title>A role for neural integrators in perceptual decision making.</article-title>
					<source>Cerebral Cortex</source>
					<volume>13</volume>
					<fpage>1257</fpage>
					<lpage>1269</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b015"><label>15</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Major</surname><given-names>G</given-names></name><name name-style="western"><surname>Baker</surname><given-names>R</given-names></name><name name-style="western"><surname>Aksay</surname><given-names>E</given-names></name><name name-style="western"><surname>Mensh</surname><given-names>B</given-names></name><name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name><etal/></person-group>
					<year>2004</year>
					<article-title>Plasticity and tuning by visual feedback of the stability of a neural integrator.</article-title>
					<source>Proc Natl Acad Sci U S A</source>
					<volume>101</volume>
					<fpage>7739</fpage>
					<lpage>7744</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b016"><label>16</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name><name name-style="western"><surname>Gold</surname><given-names>JI</given-names></name></person-group>
					<year>2005</year>
					<article-title>The neurophysiology of decision-making as a window on cognition.</article-title>
					<comment>In</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Gazzaniga</surname><given-names>MS</given-names></name></person-group>
					<source>The cognitive neurosciences. 3rd edition</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
					<fpage>1229</fpage>
					<lpage>1241</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b017"><label>17</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Jäger</surname><given-names>H</given-names></name><name name-style="western"><surname>Haas</surname><given-names>H</given-names></name></person-group>
					<year>2004</year>
					<article-title>Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication.</article-title>
					<source>Science</source>
					<volume>304</volume>
					<fpage>78</fpage>
					<lpage>80</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b018"><label>18</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Joshi</surname><given-names>P</given-names></name><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name></person-group>
					<year>2005</year>
					<article-title>Movement generation with circuits of spiking neurons.</article-title>
					<source>Neural Computation</source>
					<volume>17</volume>
					<fpage>1715</fpage>
					<lpage>1738</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b019"><label>19</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>White</surname><given-names>EL</given-names></name></person-group>
					<year>1989</year>
					<source>Cortical circuits</source>
					<publisher-loc>Boston</publisher-loc>
					<publisher-name>Birkhaeuser</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">223</size>
				</element-citation></ref><ref id="pcbi-0020165-b020"><label>20</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name><name name-style="western"><surname>Kötter</surname><given-names>R</given-names></name></person-group>
					<year>2004</year>
					<article-title>Motifs in brain networks.</article-title>
					<source>PLoS Biol</source>
					<volume>2</volume>
					<issue>11</issue>
					<fpage>1910</fpage>
					<lpage>1918</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b021"><label>21</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Rieke</surname><given-names>R</given-names></name><name name-style="western"><surname>Warland</surname><given-names>D</given-names></name><name name-style="western"><surname>van Steveninck</surname><given-names>RRD</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name></person-group>
					<year>1997</year>
					<source>SPIKES: Exploring the neural code</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">416</size>
				</element-citation></ref><ref id="pcbi-0020165-b022"><label>22</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Cowan</surname><given-names>JD</given-names></name></person-group>
					<year>1968</year>
					<article-title>Statistical mechanics of neural nets.</article-title>
					<comment>In</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Caianiello</surname><given-names>ER</given-names></name></person-group>
					<source>Neural networks</source>
					<publisher-loc>Berlin</publisher-loc>
					<publisher-name>Springer</publisher-name>
					<fpage>181</fpage>
					<lpage>188</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b023"><label>23</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Cohen</surname><given-names>MA</given-names></name><name name-style="western"><surname>Grossberg</surname><given-names>S</given-names></name></person-group>
					<year>1983</year>
					<article-title>Absolute stability of global pattern formation and parallel memory storage by competitive neural networks.</article-title>
					<source>IEEE Trans Sys Man Cyber</source>
					<volume>13</volume>
					<fpage>815</fpage>
					<lpage>826</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b024"><label>24</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group>
					<year>1984</year>
					<article-title>Neurons with graded response have collective computational properties like those of two-state neurons.</article-title>
					<source>Proc Natl Acad Sci U S A</source>
					<volume>81</volume>
					<fpage>3088</fpage>
					<lpage>3092</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b025"><label>25</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name></person-group>
					<year>2001</year>
					<source>Theoretical neuroscience: Computational and mathematical modeling of neural systems</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
				</element-citation></ref><ref id="pcbi-0020165-b026"><label>26</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Savage</surname><given-names>JE</given-names></name></person-group>
					<year>1998</year>
					<source>Models of computation: Exploring the power of computing</source>
					<publisher-loc>Reading (Massachusetts)</publisher-loc>
					<publisher-name>Addison-Wesley</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">698</size>
				</element-citation></ref><ref id="pcbi-0020165-b027"><label>27</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name><name name-style="western"><surname>Markram</surname><given-names>H</given-names></name></person-group>
					<year>2006</year>
					<article-title>Theory of the computational function of microcircuit dynamics.</article-title>
					<comment>In</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Grillner</surname><given-names>S</given-names></name><name name-style="western"><surname>Graybiel</surname><given-names>AM</given-names></name></person-group>
					<source>The interface between neurons and global brain function. Dahlem Workshop Report 93</source>
					<publisher-loc>Cambridge (Masschusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
					<fpage>371</fpage>
					<lpage>390</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b028"><label>28</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Branicky</surname><given-names>MS</given-names></name></person-group>
					<year>1995</year>
					<article-title>Universal computation and other capabilities of hybrid and continuous dynamical systems.</article-title>
					<source>Theor Comput Sci</source>
					<volume>138</volume>
					<fpage>67</fpage>
					<lpage>100</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b029"><label>29</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Siegelmann</surname><given-names>H</given-names></name><name name-style="western"><surname>Sontag</surname><given-names>ED</given-names></name></person-group>
					<year>1994</year>
					<article-title>Analog computation via neural networks.</article-title>
					<source>Theor Comput Sci</source>
					<volume>131</volume>
					<fpage>331</fpage>
					<lpage>360</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b030"><label>30</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Siegelmann</surname><given-names>H</given-names></name><name name-style="western"><surname>Sontag</surname><given-names>ED</given-names></name></person-group>
					<year>1995</year>
					<article-title>On the computational power of neural nets.</article-title>
					<source>J Comput Syst Sci</source>
					<volume>50</volume>
					<fpage>132</fpage>
					<lpage>150</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b031"><label>31</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Orponen</surname><given-names>P</given-names></name></person-group>
					<year>1997</year>
					<article-title>A survey of continuous-time computation theory.</article-title>
					<comment>In</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Du</surname><given-names>DZ</given-names></name><name name-style="western"><surname>Ko</surname><given-names>KI</given-names></name></person-group>
					<source>Advances in algorithms, languages, and complexity</source>
					<publisher-loc>Berlin</publisher-loc>
					<publisher-name>Kluwer/Springer</publisher-name>
					<fpage>9</fpage>
					<lpage>224</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b032"><label>32</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Slotine</surname><given-names>JJE</given-names></name><name name-style="western"><surname>Li</surname><given-names>W</given-names></name></person-group>
					<year>1991</year>
					<source>Applied nonlinear control</source>
					<publisher-loc>Upper Saddle River (New Jersey)</publisher-loc>
					<publisher-name>Prentice Hall</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">352</size>
				</element-citation></ref><ref id="pcbi-0020165-b033"><label>33</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Sontag</surname><given-names>ED</given-names></name></person-group>
					<year>1999</year>
					<source>Mathematical control theory</source>
					<publisher-loc>Berlin</publisher-loc>
					<publisher-name>Springer-Verlag</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">531</size>
				</element-citation></ref><ref id="pcbi-0020165-b034"><label>34</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Haykin</surname><given-names>S</given-names></name></person-group>
					<year>1999</year>
					<source>Neural networks: A comprehensive foundation. 2nd edition</source>
					<publisher-loc>Upper Saddle River (New Jersey)</publisher-loc>
					<publisher-name>Prentice Hall</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">842</size>
				</element-citation></ref><ref id="pcbi-0020165-b035"><label>35</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name><name name-style="western"><surname>Orponen</surname><given-names>P</given-names></name></person-group>
					<year>1998</year>
					<article-title>On the effect of analog noise in discrete-time analog computations.</article-title>
					<source>Neural Computation</source>
					<volume>10</volume>
					<fpage>1071</fpage>
					<lpage>1095</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b036"><label>36</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name><name name-style="western"><surname>Sontag</surname><given-names>E</given-names></name></person-group>
					<year>1999</year>
					<article-title>Analog neural nets with Gaussian or other common noise distribution cannot recognize arbitrary regular languages.</article-title>
					<source>Neural Computation</source>
					<volume>11</volume>
					<fpage>771</fpage>
					<lpage>782</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b037"><label>37</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name><name name-style="western"><surname>Markram</surname><given-names>H</given-names></name></person-group>
					<year>2004</year>
					<article-title>On the computational power of recurrent circuits of spiking neurons.</article-title>
					<source>J Comput Syst Sci</source>
					<volume>69</volume>
					<fpage>593</fpage>
					<lpage>616</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b038"><label>38</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Schölkopf</surname><given-names>B</given-names></name><name name-style="western"><surname>Smola</surname><given-names>AJ</given-names></name></person-group>
					<year>2002</year>
					<source>Learning with kernels</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
				</element-citation></ref><ref id="pcbi-0020165-b039"><label>39</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group>
					<year>1982</year>
					<article-title>Neural networks and physical systems with emergent collective computational abilities.</article-title>
					<source>Proc Natl Acad Sci U S A</source>
					<volume>79</volume>
					<fpage>2554</fpage>
					<lpage>2558</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b040"><label>40</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Amit</surname><given-names>DJ</given-names></name><name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name></person-group>
					<year>1997</year>
					<article-title>Model of global spontaneous activity and local structured activity during delay periods in the cerebral cortex.</article-title>
					<source>Cerebral Cortex</source>
					<volume>7</volume>
					<fpage>237</fpage>
					<lpage>252</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b041"><label>41</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Brody</surname><given-names>CD</given-names></name><name name-style="western"><surname>Romo</surname><given-names>R</given-names></name><name name-style="western"><surname>Kepecs</surname><given-names>A</given-names></name></person-group>
					<year>2003</year>
					<article-title>Basic mechanisms for graded persistent activity: Discrete attractors, continuous attractors, and dynamic representations.</article-title>
					<source>Curr Opin Neurobiol</source>
					<volume>13</volume>
					<fpage>204</fpage>
					<lpage>211</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b042"><label>42</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Destexhe</surname><given-names>A</given-names></name><name name-style="western"><surname>Rudolph</surname><given-names>M</given-names></name><name name-style="western"><surname>Pare</surname><given-names>D</given-names></name></person-group>
					<year>2003</year>
					<article-title>The high-conductance state of neocortical neurons in vivo.</article-title>
					<source>Nat Rev Neurosci</source>
					<volume>4</volume>
					<fpage>739</fpage>
					<lpage>751</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b043"><label>43</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Markram</surname><given-names>H</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y</given-names></name><name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name></person-group>
					<year>1998</year>
					<article-title>Differential signaling via the same axon of neocortical pyramidal neurons.</article-title>
					<source>Proc Natl Acad Sci U S A</source>
					<volume>95</volume>
					<fpage>5323</fpage>
					<lpage>5328</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b044"><label>44</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Gupta</surname><given-names>A</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y</given-names></name><name name-style="western"><surname>Markram</surname><given-names>H</given-names></name></person-group>
					<year>2000</year>
					<article-title>Organizing principles for a diversity of GABAergic interneurons and synapses in the neocortex.</article-title>
					<source>Science</source>
					<volume>287</volume>
					<fpage>273</fpage>
					<lpage>278</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b045"><label>45</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Major</surname><given-names>G</given-names></name><name name-style="western"><surname>Baker</surname><given-names>R</given-names></name><name name-style="western"><surname>Aksay</surname><given-names>E</given-names></name><name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name><name name-style="western"><surname>Tank</surname><given-names>DW</given-names></name></person-group>
					<year>2004</year>
					<article-title>Plasticity and tuning of the time course of analog persistent firing in a neural integrator.</article-title>
					<source>Proc Natl Acad Sci U S A</source>
					<volume>101</volume>
					<fpage>7745</fpage>
					<lpage>7750</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b046"><label>46</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Legenstein</surname><given-names>RA</given-names></name><name name-style="western"><surname>Näger</surname><given-names>C</given-names></name><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name></person-group>
					<year>2005</year>
					<article-title>What can a neuron learn with spike-timing–dependent plasticity?</article-title>
					<source>Neural Computation</source>
					<volume>17</volume>
					<fpage>2337</fpage>
					<lpage>2382</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b047"><label>47</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Wickens</surname><given-names>J</given-names></name><name name-style="western"><surname>Kötter</surname><given-names>R</given-names></name></person-group>
					<year>1998</year>
					<article-title>Cellular models of reinforcement.</article-title>
					<comment>In</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Houk</surname><given-names>JC</given-names></name><name name-style="western"><surname>Davis</surname><given-names>JL</given-names></name><name name-style="western"><surname>Beiser</surname><given-names>DG</given-names></name></person-group>
					<source>Models of information processing in the basal ganglia</source>
					<publisher-loc>Cambridge (Massachusetts)</publisher-loc>
					<publisher-name>MIT Press</publisher-name>
				</element-citation></ref><ref id="pcbi-0020165-b048"><label>48</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Destexhe</surname><given-names>A</given-names></name><name name-style="western"><surname>Rudolph</surname><given-names>M</given-names></name><name name-style="western"><surname>Fellous</surname><given-names>JM</given-names></name><name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group>
					<year>2001</year>
					<article-title>Fluctuating synaptic conductances recreate in vivo–like activity in neocortical neurons.</article-title>
					<source>Neuroscience</source>
					<volume>107</volume>
					<fpage>13</fpage>
					<lpage>24</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b049"><label>49</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Destexhe</surname><given-names>A</given-names></name><name name-style="western"><surname>Pare</surname><given-names>D</given-names></name></person-group>
					<year>1999</year>
					<article-title>Impact of network activity on the integrative properties of neocortical pyramidal neurons in vivo. J.</article-title>
					<source>Neurophysiol</source>
					<volume>81</volume>
					<fpage>1531</fpage>
					<lpage>1547</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b050"><label>50</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hoffman</surname><given-names>DA</given-names></name><name name-style="western"><surname>Magee</surname><given-names>JC</given-names></name><name name-style="western"><surname>Colbert</surname><given-names>CM</given-names></name><name name-style="western"><surname>Johnston</surname><given-names>D</given-names></name></person-group>
					<year>1997</year>
					<article-title>K+ channel regulation of signal propagation in dendrites of hippocampal pyramidal neurons.</article-title>
					<source>Nature</source>
					<volume>387</volume>
					<fpage>869</fpage>
					<lpage>875</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b051"><label>51</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Magee</surname><given-names>JC</given-names></name><name name-style="western"><surname>Johnston</surname><given-names>D</given-names></name></person-group>
					<year>1995</year>
					<article-title>Characterization of single voltage–gated Na+ and Ca2+ channels in apical dendrites of rat CA1 pyramidal neurons.</article-title>
					<source>J Physiol</source>
					<volume>487</volume>
					<issue>Part 1</issue>
					<fpage>67</fpage>
					<lpage>90</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b052"><label>52</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Magee</surname><given-names>J</given-names></name><name name-style="western"><surname>Hoffman</surname><given-names>D</given-names></name><name name-style="western"><surname>Colbert</surname><given-names>C</given-names></name><name name-style="western"><surname>Johnston</surname><given-names>D</given-names></name></person-group>
					<year>1998</year>
					<article-title>Electrical and calcium signaling in dendrites of hippocampal pyramidal neurons.</article-title>
					<source>Annu Rev Physiol</source>
					<volume>60</volume>
					<fpage>327</fpage>
					<lpage>346</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b053"><label>53</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Stuart</surname><given-names>GJ</given-names></name><name name-style="western"><surname>Sakmann</surname><given-names>B</given-names></name></person-group>
					<year>1994</year>
					<article-title>Active propagation of somatic action potentials into neocortical pyramidal cell dendrites.</article-title>
					<source>Nature</source>
					<volume>367</volume>
					<fpage>69</fpage>
					<lpage>72</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b054"><label>54</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Traub</surname><given-names>RD</given-names></name><name name-style="western"><surname>Miles</surname><given-names>R</given-names></name></person-group>
					<year>1991</year>
					<source>Neuronal networks of the hippocampus</source>
					<publisher-loc>Cambridge (United Kingdom)</publisher-loc>
					<publisher-name>Cambridge University Press</publisher-name>
					<!--===== Restructure page-count as size[@units="page"] =====--><size units="page">301</size>
				</element-citation></ref><ref id="pcbi-0020165-b055"><label>55</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Mainen</surname><given-names>ZT</given-names></name><name name-style="western"><surname>Joerges</surname><given-names>J</given-names></name><name name-style="western"><surname>Huguenard</surname><given-names>JR</given-names></name><name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group>
					<year>1995</year>
					<article-title>A model of spike initiation in neocortical pyramidal neurons.</article-title>
					<source>Neuron</source>
					<volume>15</volume>
					<fpage>1427</fpage>
					<lpage>1439</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b056"><label>56</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Huguenard</surname><given-names>JR</given-names></name><name name-style="western"><surname>Hamill</surname><given-names>OP</given-names></name><name name-style="western"><surname>Prince</surname><given-names>DA</given-names></name></person-group>
					<year>1988</year>
					<article-title>Developmental changes in <italic>Na</italic><sup>+</sup> conductances in rat neocortical neurons: Appearance of a slowly inactivating component.</article-title>
					<source>J Neurophysiol</source>
					<volume>59</volume>
					<fpage>778</fpage>
					<lpage>795</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b057"><label>57</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Anderson</surname><given-names>J</given-names></name><name name-style="western"><surname>Lampl</surname><given-names>I</given-names></name><name name-style="western"><surname>Reichova</surname><given-names>I</given-names></name><name name-style="western"><surname>Carandini</surname><given-names>M</given-names></name><name name-style="western"><surname>Ferster</surname><given-names>D</given-names></name></person-group>
					<year>2000</year>
					<article-title>Stimulus dependence of two-state fluctuations of membrane potential in cat visual cortex.</article-title>
					<source>Nature Neuroscience</source>
					<volume>3</volume>
					<fpage>617</fpage>
					<lpage>621</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b058"><label>58</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Borg-Graham</surname><given-names>LJ</given-names></name><name name-style="western"><surname>Monier</surname><given-names>C</given-names></name><name name-style="western"><surname>Fregnac</surname><given-names>Y</given-names></name></person-group>
					<year>1998</year>
					<article-title>Visual input evokes transient and strong shunting inhibition in visual cortical neurons.</article-title>
					<source>Nature</source>
					<volume>393</volume>
					<fpage>369</fpage>
					<lpage>373</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b059"><label>59</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Hirsch</surname><given-names>JA</given-names></name><name name-style="western"><surname>Alonso</surname><given-names>JM</given-names></name><name name-style="western"><surname>Reid</surname><given-names>RC</given-names></name><name name-style="western"><surname>Martinez</surname><given-names>LM</given-names></name></person-group>
					<year>1998</year>
					<article-title>Synaptic integration in striate cortical simple cells.</article-title>
					<source>J Neurosci</source>
					<volume>18</volume>
					<fpage>9517</fpage>
					<lpage>9528</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b060"><label>60</label><element-citation publication-type="journal" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name><name name-style="western"><surname>Markram</surname><given-names>H</given-names></name></person-group>
					<year>2002</year>
					<article-title>Synapses as dynamic memory buffers.</article-title>
					<source>Neural Networks</source>
					<volume>15</volume>
					<fpage>155</fpage>
					<lpage>161</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b061"><label>61</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Natschläger</surname><given-names>T</given-names></name><name name-style="western"><surname>Markram</surname><given-names>H</given-names></name><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name></person-group>
					<year>2003</year>
					<article-title>Computer models and analysis tools for neural microcircuits.</article-title>
					<comment>In</comment>
					<person-group person-group-type="editor"><name name-style="western"><surname>Kötter</surname><given-names>R</given-names></name></person-group>
					<source>Neuroscience databases. A practical guide</source>
					<publisher-loc>Boston</publisher-loc>
					<publisher-name>Kluwer</publisher-name>
					<fpage>123</fpage>
					<lpage>138</lpage>
				</element-citation></ref><ref id="pcbi-0020165-b062"><label>62</label><element-citation publication-type="other" xlink:type="simple">
					<person-group person-group-type="author"><name name-style="western"><surname>Legenstein</surname><given-names>RA</given-names></name><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name></person-group>
					<year>2007</year>
					<article-title>Edge of chaos and prediction of computational performance for neural microcircuit models.</article-title>
					<source>Neural Networks. In press</source>
				</element-citation></ref></ref-list></back></article>