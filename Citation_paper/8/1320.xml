<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group>
<journal-title>PLoS ONE</journal-title></journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-13-49333</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0095715</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Computer and information sciences</subject><subj-group><subject>Computer applications</subject></subj-group><subj-group><subject>Computing methods</subject><subj-group><subject>Mathematical computing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>One-Hot Vector Hybrid Associative Classifier for Medical Data Classification</article-title>
<alt-title alt-title-type="running-head">Hybrid Associative Classifier CHAT-OHM</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Uriarte-Arcia</surname><given-names>Abril Valeria</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>López-Yáñez</surname><given-names>Itzamá</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Yáñez-Márquez</surname><given-names>Cornelio</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Neural Networks and Unconventional Computating Lab/Alpha-Beta Group, Centro de Investigación en Computación, Instituto Politécnico Nacional, Ciudad de México, Distrito Federal, México</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Intelligent Computing Lab/Alpha-Beta Group, Centro de Innovación y Desarrollo Tecnológico en Cómputo, Instituto Politécnico Nacional, Ciudad de México, Distrito Federal, México</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Dalby</surname><given-names>Andrew R.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Westminster, United Kingdom</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">auriarteb10@sagitario.cic.ipn.mx</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: AVUA CYM ILY. Performed the experiments: AVUA. Analyzed the data: AVUA CYM. Contributed reagents/materials/analysis tools: AVUA CYM ILY. Wrote the paper: AVUA.</p></fn>
</author-notes>
<pub-date pub-type="collection"><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>21</day><month>4</month><year>2014</year></pub-date>
<volume>9</volume>
<issue>4</issue>
<elocation-id>e95715</elocation-id>
<history>
<date date-type="received"><day>22</day><month>11</month><year>2013</year></date>
<date date-type="accepted"><day>30</day><month>3</month><year>2014</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Uriarte-Arcia et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Pattern recognition and classification are two of the key topics in computer science. In this paper a novel method for the task of pattern classification is presented. The proposed method combines a hybrid associative classifier (Clasificador Híbrido Asociativo con Traslación, <italic>CHAT</italic>, in Spanish), a coding technique for output patterns called one-hot vector and majority voting during the classification step. The method is termed as CHAT One-Hot Majority (CHAT-OHM). The performance of the method is validated by comparing the accuracy of CHAT-OHM with other well-known classification algorithms. During the experimental phase, the classifier was applied to four datasets related to the medical field. The results also show that the proposed method outperforms the original CHAT classification accuracy.</p>
</abstract>
<funding-group><funding-statement>Centro de Investigación en Computación (<ext-link ext-link-type="uri" xlink:href="http://www.cic.ipn.mx/" xlink:type="simple">http://www.cic.ipn.mx/</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="13"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Recognizing objects is an automatic routine task for humans and there is a myriad of problems involving pattern recognition. Simulating the human capacity for objects recognition has been a very important topic for computer sciences. For several decade, various approaches have been developed, which can be implemented on computers, to simulate the human ability to recognize objects. One of such approaches is the associative approach, whose main purpose is to correctly retrieve complete patterns from input patterns.</p>
<p>The first known model of associative memories is the Lernmatrix, developed in 1961 by Karl Steinbuch <xref ref-type="bibr" rid="pone.0095715-Steinbuch1">[1]</xref>. Some years later, an optical device capable of behaving as an associative memory was created by Buneman and Longuet-Higgins. <xref ref-type="bibr" rid="pone.0095715-Willshaw1">[2]</xref>. In 1972, the work of Anderson <xref ref-type="bibr" rid="pone.0095715-Anderson1">[3]</xref>, Kohonen <xref ref-type="bibr" rid="pone.0095715-Kohonen1">[4]</xref>, and to some extent Nakano <xref ref-type="bibr" rid="pone.0095715-Nakano1">[5]</xref>, led to the model that is now known by the generic name of Linear Associator. In this same year Shun-Ichi Amari, published a theoretical work about self-organizing nets of threshold elements <xref ref-type="bibr" rid="pone.0095715-Amari1">[6]</xref>. The work of Amari represents an essential background to one of the most important associative models: the Hopfield memory <xref ref-type="bibr" rid="pone.0095715-Hopfield1">[7]</xref>. In the late 1980’s, Kosko <xref ref-type="bibr" rid="pone.0095715-Kosko1">[8]</xref> developed a bidirectional associative memory from two Hopfield memories. The morphological associative memories were introduced by Ritter <italic>et al</italic>. in 1998 <xref ref-type="bibr" rid="pone.0095715-Ritter1">[9]</xref>, which represented a qualitative leap for associative models. These models incorporated concepts from mathematical morphology, which give them several advantages over the known models. Associative models have been widely and successfully used in different applications such as: pollutant concentration prediction <xref ref-type="bibr" rid="pone.0095715-LpezYaez1">[10]</xref>, pattern classification <xref ref-type="bibr" rid="pone.0095715-Mathai1">[11]</xref>, images processing <xref ref-type="bibr" rid="pone.0095715-Guzmn1">[12]</xref>, <xref ref-type="bibr" rid="pone.0095715-Chartier1">[13]</xref>, among others.</p>
<p>In this paper, a method that combines a hybrid associative classifier, a coding technique for output patterns and majority voting, is presented. The rest of this paper is organized as follows. Section 2 describes all the materials and methods needed to develop our proposal. Section 3 describes how the experimental phase was conducted and discusses the results. Some conclusions are presented in Section 4 and finally the Acknowledge and References are included.</p>
</sec><sec id="s2" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s2a">
<title>Associative Memories</title>
<p>An associative memory <bold>M</bold> is a system that relates input patterns and output patterns as follows <xref ref-type="bibr" rid="pone.0095715-YezMrquez1">[14]</xref>:<disp-formula id="pone.0095715.e001"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095715.e001" xlink:type="simple"/></disp-formula></p>
<p>with x and y being the input and output patterns vectors. Each input vector form an association with its corresponding output vector. An associative memory is represented by a matrix whose <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e002" xlink:type="simple"/></inline-formula>-th component is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e003" xlink:type="simple"/></inline-formula>. For each <italic>k</italic> integer and positive, the corresponding association will be denoted as: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e004" xlink:type="simple"/></inline-formula>. The matrix <bold>M</bold> is generated from a finite set of previously known associations, called the fundamental set. If <italic>μ</italic> is an index, the fundamental set is represented as: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e005" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e006" xlink:type="simple"/></inline-formula> is the cardinality of the fundamental set. The patterns that form the fundamental set are called fundamental patterns. If it holds that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e007" xlink:type="simple"/></inline-formula>, <bold>M</bold> is autoassociative, otherwise it is heteroassociative. If we consider the fundamental set of patterns <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e008" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e009" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e010" xlink:type="simple"/></inline-formula> are the dimensions of the input patterns and output patterns, respectively, it is said that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e011" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e012" xlink:type="simple"/></inline-formula>. Then the <italic>j</italic>-th component of an input pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e013" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e014" xlink:type="simple"/></inline-formula>. Analogously, the <italic>j</italic>-th component of an output pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e015" xlink:type="simple"/></inline-formula> is represented as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e016" xlink:type="simple"/></inline-formula>. Therefore, the fundamental input and output patterns are represented as follows:</p>
<p><disp-formula id="pone.0095715.e017"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095715.e017" xlink:type="simple"/></disp-formula></p>
<p>A distorted version of a pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e018" xlink:type="simple"/></inline-formula> to be recalled will be denoted as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e019" xlink:type="simple"/></inline-formula>. An unknown input pattern to be recalled will be denoted as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e020" xlink:type="simple"/></inline-formula>. If when an unknown input pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e021" xlink:type="simple"/></inline-formula> is fed to an associative memory <bold>M</bold>, and it happens that the output corresponds exactly to the associated pattern, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e022" xlink:type="simple"/></inline-formula> it is said that recalling is correct.</p>
</sec><sec id="s2b">
<title>Lernmatrix</title>
<p>The Lernmatrix is a heteroassociative memory that can function as a binary pattern classifier if the output patterns are properly selected <xref ref-type="bibr" rid="pone.0095715-YezMrquez1">[14]</xref>. It is an input and output system that accepts a binary input pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e023" xlink:type="simple"/></inline-formula> and produce as an output the class <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e024" xlink:type="simple"/></inline-formula>. For a class <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e025" xlink:type="simple"/></inline-formula>, where <italic>m</italic> is the number of classes in the fundamental set, the class is coded according to the following expression: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e026" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e027" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e028" xlink:type="simple"/></inline-formula>. The Lernmatrix is represented by a matrix <bold>M</bold>. At the beginning of the learning phase, each component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e029" xlink:type="simple"/></inline-formula> of <bold>M</bold> is set to zero and then it is updated according to rule <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e030" xlink:type="simple"/></inline-formula>, where:<disp-formula id="pone.0095715.e031"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095715.e031" xlink:type="simple"/></disp-formula>Where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e032" xlink:type="simple"/></inline-formula> is any positive constant that was previously chosen.</p>
<p>The recovery phase consists of finding the class vector for a given vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e033" xlink:type="simple"/></inline-formula>. Finding the class means to obtain the coordinates of the vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e034" xlink:type="simple"/></inline-formula> that corresponds to the pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e035" xlink:type="simple"/></inline-formula>. The <italic>i</italic>-th component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e036" xlink:type="simple"/></inline-formula> of the class vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e037" xlink:type="simple"/></inline-formula> is obtained according to the following expression:<disp-formula id="pone.0095715.e038"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095715.e038" xlink:type="simple"/></disp-formula></p>
</sec><sec id="s2c">
<title>Linear Associator</title>
<p>Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e039" xlink:type="simple"/></inline-formula> be the fundamental set with <xref ref-type="bibr" rid="pone.0095715-YezMrquez2">[15]</xref>:<disp-formula id="pone.0095715.e040"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095715.e040" xlink:type="simple"/></disp-formula></p>
<p>The learning phase consists of two steps:</p>
<list list-type="order"><list-item>
<p>For each of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e041" xlink:type="simple"/></inline-formula> associations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e042" xlink:type="simple"/></inline-formula> find the matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e043" xlink:type="simple"/></inline-formula> of dimensions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e044" xlink:type="simple"/></inline-formula></p>
</list-item></list>
<p><disp-formula id="pone.0095715.e045"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095715.e045" xlink:type="simple"/></disp-formula></p>
<list list-type="order"><list-item>
<p>Sum the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e046" xlink:type="simple"/></inline-formula> matrices to obtain the memory <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e047" xlink:type="simple"/></inline-formula> where the <italic>ij</italic>-th component of <bold>M</bold> can be expressed as follow: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e048" xlink:type="simple"/></inline-formula>.</p>
</list-item></list>
<p>The recovering phase consists of presenting an input pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e049" xlink:type="simple"/></inline-formula> to the memory <bold>M</bold> and performing the following operation:<disp-formula id="pone.0095715.e050"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095715.e050" xlink:type="simple"/></disp-formula></p>
</sec><sec id="s2d">
<title>CHAT</title>
<p>The CHAT is a hybrid associative classifier developed by Santiago-Montero <xref ref-type="bibr" rid="pone.0095715-SantiagoMontero1">[16]</xref>, which is based on two associative memories: the <italic>Lernmatrix</italic> and the <italic>Linear Associator</italic>. This classifier overcomes some limitations that these two memories presented, by ingeniously combining the learning and recovery phases of both models. The first proposed model was called CHA, which combined the learning phase of the Linear Associator and the recovering phase of the Lernmatrix, but sometimes this model fails to perform a correct classification. To overcome this limitation, a new version was proposed, by adding a new step to the model: translation of coordinates axes. This new version was named CHAT. With this axes translation, the new origin is located at the centroid of the input vectors patterns.</p>
<p>Definition 3.1: Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e051" xlink:type="simple"/></inline-formula> be a set of fundamental input patterns, and let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e052" xlink:type="simple"/></inline-formula> be the mean vector of them, where:<disp-formula id="pone.0095715.e053"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095715.e053" xlink:type="simple"/></disp-formula></p>
<p>Definition 3.2: Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e054" xlink:type="simple"/></inline-formula> be a set of fundamental input patterns and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e055" xlink:type="simple"/></inline-formula> a new set of translated patterns generated using the following expression:<disp-formula id="pone.0095715.e056"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095715.e056" xlink:type="simple"/></disp-formula></p>
</sec><sec id="s2e">
<title>CHAT Algorithm</title>
<list list-type="order"><list-item>
<p>Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e057" xlink:type="simple"/></inline-formula> be a set of <italic>n</italic>-dimensional fundamental input patterns with real values in its components, which are grouped into <italic>m</italic> classes.</p>
</list-item><list-item>
<p>To each of the fundamental input patterns belonging to class <italic>k,</italic> an output vector of size <italic>m</italic> is assigned. This vector consists of zeros, except for the <italic>k</italic>-th component, whose value is set to 1.</p>
</list-item><list-item>
<p>Calculate the mean vector of the set of input patterns according to definition 3.1.</p>
</list-item><list-item>
<p>The mean vector is taken as the new origin of the coordinate axes.</p>
</list-item><list-item>
<p>Translate the patterns of the input set according to definition 3.2.</p>
</list-item><list-item>
<p>Apply the learning phase, which is the same as the learning phase of the Linear Associator, to the translated set obtained in the previous step.</p>
</list-item><list-item>
<p>Translate the patterns that have to be classified using the definition 3.2.</p>
</list-item><list-item>
<p>Apply the recovering phase, which is the same as the recovering phase of the Lernmatrix, to the translated set obtained in the previous step.</p>
</list-item></list>
</sec><sec id="s2f">
<title>CHAT-OHM</title>
<p>In this section the description of the proposed method is presented. This proposal is part of the results achieved by several members of the Neural Networks and Unconventional Computing Laboratory of the <italic>Centro de Investigación en Computación, Instituto Politécnico Nacional</italic>, in an attempt to improve the performance of the CHAT model <xref ref-type="bibr" rid="pone.0095715-SantiagoMontero1">[16]</xref>. This joint effort resulted in a number of methods that implemented some variations on the CHAT, being the proposed method one of them.</p>
</sec><sec id="s2g">
<title>One-hot Vectors</title>
<p>One-hot vector is a coding technique for output patterns, which will be used in the proposed method instead of the original coding technique presented in the step 2 of the CHAT algorithm that was described in the previous section.</p>
<p>Definition 3.3: Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e058" xlink:type="simple"/></inline-formula> be a set of translated fundamental output patterns of size <italic>p</italic>. The <italic>i</italic>-th component of each translated fundamental output pattern is coded according to the following expression:<disp-formula id="pone.0095715.e059"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095715.e059" xlink:type="simple"/></disp-formula></p>
</sec><sec id="s2h">
<title>Majority Voting</title>
<p>The classification phase consists of finding the output vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e060" xlink:type="simple"/></inline-formula> to which an unknown input pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e061" xlink:type="simple"/></inline-formula> belongs. Majority voting is a procedure used during the classification phase to perform this task.</p>
<p>Definition 3.4: Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e062" xlink:type="simple"/></inline-formula> be the number of different classes in the fundamental input set. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e063" xlink:type="simple"/></inline-formula> be a set of fundamental input patterns where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e064" xlink:type="simple"/></inline-formula>. For a class <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e065" xlink:type="simple"/></inline-formula>, a masking vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e066" xlink:type="simple"/></inline-formula> of size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e067" xlink:type="simple"/></inline-formula> is coded according to the following expression:<disp-formula id="pone.0095715.e068"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095715.e068" xlink:type="simple"/></disp-formula></p>
<p>Let <bold>M</bold> be a matrix generated during the learning phase of the CHAT-OHM and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e069" xlink:type="simple"/></inline-formula> an unknown <italic>n</italic>-dimensional input pattern to be classified. The recover pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e070" xlink:type="simple"/></inline-formula> is determined as follow:</p>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e071" xlink:type="simple"/></inline-formula></p>
<list list-type="order"><list-item>
<p>For each class <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e072" xlink:type="simple"/></inline-formula>, a counting vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e073" xlink:type="simple"/></inline-formula> is obtained applying an “and” operator between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e074" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e075" xlink:type="simple"/></inline-formula>:</p>
</list-item></list>
<p><disp-formula id="pone.0095715.e076"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095715.e076" xlink:type="simple"/></disp-formula></p>
<list list-type="order"><list-item>
<p>Finally <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e077" xlink:type="simple"/></inline-formula> is obtained using the following expression:</p>
</list-item></list>
<p><disp-formula id="pone.0095715.e078"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095715.e078" xlink:type="simple"/></disp-formula></p>
</sec><sec id="s2i">
<title>CHAT-OHM Algorithm</title>
<list list-type="order"><list-item>
<p>Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e079" xlink:type="simple"/></inline-formula> be a set of <italic>n</italic>-dimensional fundamental input patterns with real values in its components, which are grouped into <italic>m</italic> classes.</p>
</list-item><list-item>
<p>To each of the fundamental input patterns an output vector of size <italic>p</italic> is assigned. This vector is coded according to the definition 3.3.</p>
</list-item><list-item>
<p>Calculate the mean vector of the set of input patterns according to definition 3.1.</p>
</list-item><list-item>
<p>The mean vector is taken as the new origin of the coordinate axes.</p>
</list-item><list-item>
<p>Translate the patterns of the input set according to definition 3.2.</p>
</list-item><list-item>
<p>Apply the learning phase, which is the same that as learning phase of the Linear Associator, to the translated set obtained in the previous step.</p>
</list-item><list-item>
<p>Translate the patterns that have to be classified using the definition 3.2.</p>
</list-item><list-item>
<p>Apply the recovering phase, which is the same as the recovering phase of the Lernmatrix, to the translated set obtained in the previous step. Because of the way in which the classes were coded, we will obtain an output vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e080" xlink:type="simple"/></inline-formula> of size <italic>p</italic> and not the desire output class <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e081" xlink:type="simple"/></inline-formula> of size <italic>m</italic>. Our algorithm performs an extract step.</p>
</list-item><list-item>
<p>Perform the majority voting explained in the previous section.</p>
</list-item></list>
</sec><sec id="s2j">
<title>Data Sets</title>
<p>This section provides a brief description of the dataset used during the experimental phase. All the used datasets were taken from the University of California at Irvine Machine Learning Repository <xref ref-type="bibr" rid="pone.0095715-University1">[17]</xref>. A summary of the main characteristics of the datasets is shown in <xref ref-type="table" rid="pone-0095715-t001">Table 1</xref>.</p>
<table-wrap id="pone-0095715-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095715.t001</object-id><label>Table 1</label><caption>
<title>Characteristics of the datasets used in the experimetal phase.</title>
</caption><alternatives><graphic id="pone-0095715-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095715.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Dataset</td>
<td align="left" rowspan="1" colspan="1">Instances</td>
<td align="left" rowspan="1" colspan="1">Attributes</td>
<td align="left" rowspan="1" colspan="1">Missing Values</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Breast Cancer</bold></td>
<td align="left" rowspan="1" colspan="1">683</td>
<td align="left" rowspan="1" colspan="1">9</td>
<td align="left" rowspan="1" colspan="1">Yes</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Haberman’s Survival</bold></td>
<td align="left" rowspan="1" colspan="1">306</td>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">No</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Liver Disorders</bold></td>
<td align="left" rowspan="1" colspan="1">345</td>
<td align="left" rowspan="1" colspan="1">6</td>
<td align="left" rowspan="1" colspan="1">No</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Hepatitis Disease</bold></td>
<td align="left" rowspan="1" colspan="1">155</td>
<td align="left" rowspan="1" colspan="1">19</td>
<td align="left" rowspan="1" colspan="1">Yes</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap></sec><sec id="s2k">
<title>Haberman’s Survival Dataset</title>
<p>The dataset contains cases from a study conducted at the University of Chicago’s Billings Hospital on the survival of patients who had undergone surgery for breast cancer. The dataset contain 306 instances, which belong to two different classes; 255 instances belong to the first class (patients who survived 5 years or more) and 81 instances belong to the second class (patients who died within 5 years). The dataset has 4 attributes including the class attribute. The purpose of the dataset is to predict the survival status of patients that have undergone breast cancer surgery.</p>
</sec><sec id="s2l">
<title>Wisconsin Breast Cancer Dataset</title>
<p>This dataset was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg. The dataset has information of clinical cases of breast cancer. The dataset contains 699 instances belonging to two classes, 458 instances belong to the first class (benign) and 241 belong to the second class (malignant). Each instance consists of 10 attributes, including the class attribute. The dataset has 16 pattern with one missing values. The instances with missing values were deleted from the original dataset and the resulting data set was used for the experimental phase.</p>
</sec><sec id="s2m">
<title>Liver Disorders Dataset</title>
<p>The Liver Disorders dataset was created by BUPA Medical Research Ltd. This dataset presents the results of a study of liver disorders that might arise from excessive alcohol consumption. It contains 345 instances belonging to two classes, 145 instances belong to the first class and 200 instances belong to the second class. Each instance consists of 7 attributes, including the class attribute.</p>
</sec><sec id="s2n">
<title>Hepatitis Disease Dataset</title>
<p>This dataset contains information of the clinical results of hepatitis patients. It contains 155 instances belonging to two classes, 32 instances belong to the first class (die) and 123 instances belong to the second class (alive). Each instance consists of 20 attributes, including the class attribute. This dataset has multiple missing values. Due to the small size of the dataset and the considerable number of missing values, these cannot be discarded. In this case the missing values were substituted by the class mode for categorical features and by the class mean for continuous values.</p>
</sec><sec id="s2o">
<title>Machine Learning Algorithms</title>
<p>This section provides a short description of the algorithms used during the experimental phase. All of these algorithms are implemented in the WEKA 3: Data Mining Software in Java <xref ref-type="bibr" rid="pone.0095715-WEKA1">[18]</xref>. Further details on the implementation of these algorithms can be found in the following reference <xref ref-type="bibr" rid="pone.0095715-Witten1">[19]</xref>.</p>
</sec><sec id="s2p">
<title>IB1</title>
<p><italic>IB1</italic> is a basic nearest-neighbor instance-based learner that finds the training instance closest in Euclidean distance to the given test instance and predicts the same class as this training instance. If several instances qualify as the closest, the first one found is used <xref ref-type="bibr" rid="pone.0095715-Witten1">[19]</xref>.</p>
</sec><sec id="s2q">
<title>ConjunctiveRule</title>
<p><italic>ConjunctiveRule</italic> learns a simple conjunctive rule learner that predicts either a numeric or a nominal class value. Uncovered test instances are assigned the default class value of the uncovered training instances. The information gain (nominal class) or variance reduction (numeric class) of each antecedent is computed, and rules are pruned using reduced-error pruning <xref ref-type="bibr" rid="pone.0095715-Witten1">[19]</xref>.</p>
</sec><sec id="s2r">
<title>RandomTree</title>
<p>Trees built by RandomTree test a given number of random features at each node, performing no pruning. The tree is constructed considering K randomly chosen attributes at each node. Also has an option to allow estimation of class probabilities based on a hold-out set <xref ref-type="bibr" rid="pone.0095715-Witten1">[19]</xref>.</p>
</sec><sec id="s2s">
<title>RandomForest</title>
<p><italic>RandomForest</italic> constructs random forests by bagging ensembles of random trees. A random forest is a classifier consisting of a collection of tree-structured classifiers and each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest <xref ref-type="bibr" rid="pone.0095715-Breiman1">[20]</xref>.</p>
</sec><sec id="s2t">
<title>BFTree</title>
<p>BFTree constructs a decision tree using a best-first expansion of nodes rather than depth-first expansion used by standard decision tree learners. Pre and post pruning option are available that are based on finding the best number of expansion to use via cross-validation on the training data. While fully grown trees are the same for best-first and depth-first algorithms, the pruning mechanism used by BFTree will yield a different pruned tree structure than that produced by depth-first methods <xref ref-type="bibr" rid="pone.0095715-Witten1">[19]</xref>.</p>
</sec><sec id="s2u">
<title>SMO</title>
<p><italic>SMO</italic> implements John Platt’s sequential minimal optimization algorithm for training a support vector classifier, using kernel functions such as polynomial or Gaussian kernels. Missing values are replaced globally, nominal attributes are transformed into binary ones, and attributes are normalized by default. For further details of the implementation, see <xref ref-type="bibr" rid="pone.0095715-Platt1">[21]</xref>.</p>
</sec><sec id="s2v">
<title>AdaBoostM1</title>
<p><italic>AdaBoostM1</italic> is a variation of boosting, method for combining multiple models seeking models that complement one another. This algorithm is constructed through the combination of various classifiers produced by repeatedly running T rounds a given “weak” learning algorithm on various distributions over the training data. Finally the booster combine the T “weak” hypotheses into a single final hypothesis <xref ref-type="bibr" rid="pone.0095715-Freund1">[22]</xref>.</p>
</sec><sec id="s2w">
<title>MultiBoostAB</title>
<p>MultiBoostAB combines boosting with a variant of wagging to prevent overfitting. Multiboosting is an extension of AdaBoost technique <xref ref-type="bibr" rid="pone.0095715-Webb1">[23]</xref>. Wagging is a technique that allow variance reduction, while AdaBoost perform both variance and bias reduction. MultiBoost is achieved by wagging a set of sub-committees of classifiers, each sub-committee formed by AdaBoost. When forming decision committee using C4.5 as the base learning algorithm, MultiBoost is demonstrated to produce committees with lower error than AdaBoost.</p>
</sec><sec id="s2x">
<title>RBFNetwork</title>
<p><italic>RBFNetwork</italic> implements a normalized Gaussian radial basis function network, deriving the centers and widths of hidden units using k-means and combining the outputs obtained from the hidden layer using logistic regression if the class is nominal and linear regression if it is numeric. The activations of the basis functions are normalized to sum to 1 before they are fed into the linear models <xref ref-type="bibr" rid="pone.0095715-Witten1">[19]</xref>.</p>
</sec><sec id="s2y">
<title>NaiveBayes</title>
<p><italic>NaiveBayes</italic> implements the probabilistic Naïve Bayes classifier. The NaiveBayes algorithm is based on Bayes rule and assumes that the attributes are conditional independent given the class, and it posits that no hidden or latent attributes influence the prediction process <xref ref-type="bibr" rid="pone.0095715-John1">[24]</xref>.</p>
</sec><sec id="s2z">
<title>BayesNet</title>
<p>Bayesian networks are alternative ways of representing a conditional probability distribution by means of directed acyclic graphs (DAGs). In this model, each node represents a random variable and an arrow connecting a parent node with a child node indicates a relationship between them <xref ref-type="bibr" rid="pone.0095715-Christofides1">[25]</xref>. <italic>BayesNet</italic> learns Bayesian nets under two assumptions: nominal attributes (numeric ones are pre-discretized) and no missing values (any such values are replaced globally).</p>
</sec><sec id="s2aa">
<title>NaiveBayesMultinomial</title>
<p>NaiveBayesMultinomial implements the multinomial Bayes’ classifier. A Naïve Bayes classifier is based on Bayes rule but this does not take into account the number of occurrence of an element. The Naïve Bayes Multinomial incorporates frequency to perform classification <xref ref-type="bibr" rid="pone.0095715-Witten1">[19]</xref>.</p>
</sec><sec id="s2ab">
<title>ComplementNaiveBayes</title>
<p>ComplementNaiveBayes builds a Complement Naïve Bayes classifier as described by Rennie <italic>et al</italic> <xref ref-type="bibr" rid="pone.0095715-Rennie1">[26]</xref>. In this work, they proposed heuristic solutions to some problems presented by the Naïve Bayes classifiers. They proposed a solution for skewed data, more training examples for one class than another that causes that the classifier prefer one class over the other.</p>
</sec><sec id="s2ac">
<title>DecisionTable</title>
<p>DecisionTable builds a simple decision table majority classifier, this table has two components: a set of features that are included in the table and a body consisting of labeled instances from the space defined by the features <xref ref-type="bibr" rid="pone.0095715-Kohavi1">[27]</xref>.</p>
</sec><sec id="s2ad">
<title>LWL</title>
<p>LWL is a general algorithm for locally weighted learning. It assigns weights using an instance-based method and builds a classifier from the weighted instances. Different classifiers can be selected, but a good choice is Naïve Bayes for classification problems and linear regression for regression problems. Attribute normalization is turned on by default <xref ref-type="bibr" rid="pone.0095715-Witten1">[19]</xref>.</p>
</sec><sec id="s2ae">
<title>DMNBtext</title>
<p>Another Naïve Bayes scheme for text classification is DMNBtext. This learns a multinomial Naïve Bayes classifier in a combined generative and discriminative fashion. DMNBText injects a discriminative element into parameter learning by considering the current classifier’s prediction for a training instance before updating frequency counts. When processing a given training instance, the counts are incremented by one minus the predicted probability for the instance’s class value. DMNBText allows users to specify how many iterations over the training data the algorithm will make, and whether word frequency information should be ignored, in which case, the method learns a standard Naïve Bayes model rather than a multinomial one <xref ref-type="bibr" rid="pone.0095715-Witten1">[19]</xref>.</p>
</sec><sec id="s2af">
<title>MultiScheme</title>
<p>MultiScheme selects the best classifier from a set of candidates using cross-validation of percentage accuracy or mean-squared error for classification and regression, respectively. The number of folds is a parameter. Performance on training data can be used instead <xref ref-type="bibr" rid="pone.0095715-Witten1">[19]</xref>.</p>
</sec><sec id="s2ag">
<title>Vote</title>
<p>Vote provides a baseline method for combining classifiers. The default scheme is to average their probability estimates or numeric predictions, for classification and regression, respectively. Other combination schemes are available–for example, using majority voting for classification <xref ref-type="bibr" rid="pone.0095715-Witten1">[19]</xref>.</p>
</sec><sec id="s2ah">
<title>VotedPerceptron</title>
<p>VotedPerceptron implements the voted perceptron algorithm. The solution vector found by the perceptron algorithm depends greatly on the order in which the instances are encountered. One way to make the algorithm more stable is to use all the weight vectors encountered during learning, not just the final one, letting them vote on a prediction. Each weight vector contributes a certain number of votes <xref ref-type="bibr" rid="pone.0095715-Witten1">[19]</xref>.</p>
</sec><sec id="s2ai">
<title>Normalization</title>
<p>During the experiments performed over the original data, we observed that some of the datasets present large scale difference between features. To avoid the effect that an overly large variable can have over the classification performance, the datasets were normalized and the experiments were performed with the normalized datasets. Normalization can prevent some features from dominating just because they have large numeric values. Subtracting the mean and dividing by the standard deviation can be an appropriate normalization method for this situation <xref ref-type="bibr" rid="pone.0095715-Duda1">[31]</xref>. The normalization was performed separately on each attribute. Normalization was calculated using the following expression:<disp-formula id="pone.0095715.e082"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095715.e082" xlink:type="simple"/></disp-formula>Where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e083" xlink:type="simple"/></inline-formula> is the normalized value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095715.e084" xlink:type="simple"/></inline-formula>, <italic>μ</italic> is the mean of the population and <italic>σ</italic> is the standard deviation of the population.</p>
</sec><sec id="s2aj">
<title>Wilson’s Edition</title>
<p>One of the most popular filtering algorithms is the Wilson’s Edition <xref ref-type="bibr" rid="pone.0095715-Wilson1">[28]</xref>. The general idea of this method is to identify and remove noisy or atypical patterns, primarily those which exist in the overlap area between two or more classes. The process consists of applying the rule of the <italic>k</italic> nearest neighbor (usually <italic>k</italic> = 3) to estimate the corresponding class of each pattern in the dataset. Those patterns whose class does not correspond to the majority class of the <italic>k</italic>-nearest neighbors will be discarded <xref ref-type="bibr" rid="pone.0095715-Wilson1">[28]</xref>.</p>
</sec><sec id="s2ak">
<title>Algorithm Comparison</title>
<p>One of the objectives of this study is to perform a consistent comparison between the classification performance of our proposal and the classification performance of other well-known pattern classification algorithms. There are two aspects that need to be addressed: select a suitable test set and the method to compare the classification performance of each algorithm. To predict the performance of a classifier, we need to assess the success rate on a dataset that takes no part in the construction (training phase) of the classifier. When the data available is big, there is no problem in the selection of a suitable test set, just use a large training set and a large test set. But the question of predicting performance with limited data is still controversial. There are many techniques, of which cross-validation is the method of choice in most situations. Kohavi <xref ref-type="bibr" rid="pone.0095715-Kohavi2">[29]</xref> compared cross-validation and bootstrap, the results show that bootstrap has low variance, but extremely large bias for some problems; as a consequence stratified 10-fold cross-validation is recommended. To perform the comparison of our proposal with other pattern classification algorithms, we used the 10-fold cross-validation approach.</p>
</sec><sec id="s2al">
<title>Classification Accuracy</title>
<p>For classification problems, the performance of a classifier can be measured in term of the success rate. The classifier predicts the class of each instance in the test set; if the class is correct, it is counted as a success. The success rate is the proportion of success over the whole set of test instances. In this paper, the accuracy of the classifiers is expressed as a percentage, and was computed according to the following expression:<disp-formula id="pone.0095715.e085"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095715.e085" xlink:type="simple"/></disp-formula></p>
</sec><sec id="s2am">
<title>Validation Method</title>
<p>According to <xref ref-type="bibr" rid="pone.0095715-Witten1">[19]</xref> the standard way of predicting the classification accuracy of a learning technique is to use stratified 10-fold cross-validation. This method divides the dataset into 10 parts in which each class is represented in approximately the same proportion as in the full dataset. The classification algorithms will be executed 10 times, in each execution one different part will be used as the test set and the classification algorithm will be trained with the remaining nine parts. The success rate will be calculated for each execution. Finally, the 10 success rates are averaged to yield an overall success rate.</p>
</sec></sec><sec id="s3">
<title>Experiments and Discussion</title>
<p>In this section we present and discuss the results obtained during the experimental phase, throughout which four datasets were used to obtain the classification performance of each of the compared classification algorithms. The datasets used in this section were taken from the UCI Machine Learning Repository <xref ref-type="bibr" rid="pone.0095715-University1">[17]</xref>. A brief summary of the datasets is presented in <xref ref-type="table" rid="pone-0095715-t001">Table 1</xref>.</p>
<p>The performance achieved by the proposed method is compared with the performances of 19 well-known methods taken from the WEKA 3 Data Mining Software <xref ref-type="bibr" rid="pone.0095715-WEKA1">[18]</xref>. Further information about the used algorithms can be found in <xref ref-type="bibr" rid="pone.0095715-Witten1">[19]</xref>. All experiments were conducted using a personal computer with an Intel Core i3-2100 Processor running Ubuntu 13.04 64-bits operating system with 4096 GB of RAM.</p>
<p>To ensure valid comparison of classification performance, the same conditions and validation schemes were applied in each experiment. Classification performance of each of the algorithms was calculated using stratified 10-fold cross-validation, with random re-ordering of the patterns before fold generation. In order to account for the random re-ordering of the patterns, the experiments for each classification algorithm, including our proposal, were executed 10 times using the stratified 10-fold cross-validation approach and the results averaged to obtain a final success rate for each algorithm. These results are used to compare the performance of our proposal and the other classification algorithms.</p>
<sec id="s3a">
<title>Original Datasets</title>
<p>In this subsection we analyze the classification accuracy results of each one of the compared algorithms, when applied to the original four datasets that were selected for this study. <xref ref-type="table" rid="pone-0095715-t002">Table 2</xref> shows the classification accuracy achieved by the original CHAT model and by our proposal in the four datasets. It is worth noting that CHAT-OHM achieved the best classification accuracy for all the datasets. In some cases the improvement in the classification accuracy is quite significant, as in the cases of the Breast Cancer dataset and the Hepatitis Disease dataset, with an improvement of 31.9 percent and 16.77 percent, respectively. The improvement for the Liver Disorders dataset is 5.82 percent, which is still important. The Haberman’s Survival dataset is where we observed the least improvement with only 0.41 percent.</p>
<table-wrap id="pone-0095715-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095715.t002</object-id><label>Table 2</label><caption>
<title>Accuracy comparison with the original method (%) original data.</title>
</caption><alternatives><graphic id="pone-0095715-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095715.t002" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td colspan="4" align="left" rowspan="1">Datasets</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Algorithm</td>
<td align="left" rowspan="1" colspan="1">Breast Cancer</td>
<td align="left" rowspan="1" colspan="1">Haberman’s Survival</td>
<td align="left" rowspan="1" colspan="1">Hepatitis Disease</td>
<td align="left" rowspan="1" colspan="1">Liver Disorders</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>CHAT</bold></td>
<td align="left" rowspan="1" colspan="1">63.10</td>
<td align="left" rowspan="1" colspan="1">65.95</td>
<td align="left" rowspan="1" colspan="1">68.19</td>
<td align="left" rowspan="1" colspan="1">55.63</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>CHAT-OHM</bold></td>
<td align="left" rowspan="1" colspan="1">95.00</td>
<td align="left" rowspan="1" colspan="1">66.36</td>
<td align="left" rowspan="1" colspan="1">84.96</td>
<td align="left" rowspan="1" colspan="1">61.45</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap>
<p><xref ref-type="table" rid="pone-0095715-t003">Table 3</xref> shows the classification accuracy achieved by our proposal and the 19 classification algorithms from WEKA, against which we will compare our method. For each dataset, the highest classification accuracy is emphasized with boldface.</p>
<table-wrap id="pone-0095715-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095715.t003</object-id><label>Table 3</label><caption>
<title>Clasification accuracy comparison (%) original data.</title>
</caption><alternatives><graphic id="pone-0095715-t003-3" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095715.t003" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td colspan="4" align="left" rowspan="1">Dataset</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Algorithm</td>
<td align="left" rowspan="1" colspan="1">Breast Cancer</td>
<td align="left" rowspan="1" colspan="1">Haberman’s Survival</td>
<td align="left" rowspan="1" colspan="1">Hepatitis Disease</td>
<td align="left" rowspan="1" colspan="1">Liver Disorders</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>AdaBoostM1</bold></td>
<td align="left" rowspan="1" colspan="1">95.05</td>
<td align="left" rowspan="1" colspan="1">74.02</td>
<td align="left" rowspan="1" colspan="1">89.8</td>
<td align="left" rowspan="1" colspan="1">65.96</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>BayesNet</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>97.34</bold></td>
<td align="left" rowspan="1" colspan="1">71.73</td>
<td align="left" rowspan="1" colspan="1">87.68</td>
<td align="left" rowspan="1" colspan="1">56.85</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>BFTree</bold></td>
<td align="left" rowspan="1" colspan="1">94.88</td>
<td align="left" rowspan="1" colspan="1">72.33</td>
<td align="left" rowspan="1" colspan="1">88.98</td>
<td align="left" rowspan="1" colspan="1">66.44</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>CHAT-OHM</bold></td>
<td align="left" rowspan="1" colspan="1">95.00</td>
<td align="left" rowspan="1" colspan="1">66.36</td>
<td align="left" rowspan="1" colspan="1">84.96</td>
<td align="left" rowspan="1" colspan="1">61.45</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>ComplementNaiveBayes</bold></td>
<td align="left" rowspan="1" colspan="1">85.43</td>
<td align="left" rowspan="1" colspan="1">73.87</td>
<td align="left" rowspan="1" colspan="1">77.54</td>
<td align="left" rowspan="1" colspan="1">56.57</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>ConjunctiveRule</bold></td>
<td align="left" rowspan="1" colspan="1">91.98</td>
<td align="left" rowspan="1" colspan="1">72.97</td>
<td align="left" rowspan="1" colspan="1">88.99</td>
<td align="left" rowspan="1" colspan="1">56.06</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>DecisionTable</bold></td>
<td align="left" rowspan="1" colspan="1">95.69</td>
<td align="left" rowspan="1" colspan="1">71.90</td>
<td align="left" rowspan="1" colspan="1">88.40</td>
<td align="left" rowspan="1" colspan="1">59.11</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>DMNBtext</bold></td>
<td align="left" rowspan="1" colspan="1">65.01</td>
<td align="left" rowspan="1" colspan="1">73.53</td>
<td align="left" rowspan="1" colspan="1">79.38</td>
<td align="left" rowspan="1" colspan="1">57.98</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>IB1</bold></td>
<td align="left" rowspan="1" colspan="1">95.75</td>
<td align="left" rowspan="1" colspan="1">65.77</td>
<td align="left" rowspan="1" colspan="1">81.03</td>
<td align="left" rowspan="1" colspan="1">62.22</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>LWL</bold></td>
<td align="left" rowspan="1" colspan="1">92.14</td>
<td align="left" rowspan="1" colspan="1">71.90</td>
<td align="left" rowspan="1" colspan="1">89.70</td>
<td align="left" rowspan="1" colspan="1">60.80</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>MultiBoostAB</bold></td>
<td align="left" rowspan="1" colspan="1">94.73</td>
<td align="left" rowspan="1" colspan="1">73.28</td>
<td align="left" rowspan="1" colspan="1">89.49</td>
<td align="left" rowspan="1" colspan="1">65.29</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>MultiScheme</bold></td>
<td align="left" rowspan="1" colspan="1">65.01</td>
<td align="left" rowspan="1" colspan="1">73.53</td>
<td align="left" rowspan="1" colspan="1">79.38</td>
<td align="left" rowspan="1" colspan="1">57.98</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>NaiveBayes</bold></td>
<td align="left" rowspan="1" colspan="1">96.26</td>
<td align="left" rowspan="1" colspan="1"><bold>74.80</bold></td>
<td align="left" rowspan="1" colspan="1">87.50</td>
<td align="left" rowspan="1" colspan="1">54.89</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>NaiveBayesMultinomial</bold></td>
<td align="left" rowspan="1" colspan="1">90.32</td>
<td align="left" rowspan="1" colspan="1">73.74</td>
<td align="left" rowspan="1" colspan="1">78.00</td>
<td align="left" rowspan="1" colspan="1">56.96</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RandomForest</bold></td>
<td align="left" rowspan="1" colspan="1">96.47</td>
<td align="left" rowspan="1" colspan="1">67.94</td>
<td align="left" rowspan="1" colspan="1"><bold>90.61</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>68.44</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RandomTree</bold></td>
<td align="left" rowspan="1" colspan="1">94.74</td>
<td align="left" rowspan="1" colspan="1">64.48</td>
<td align="left" rowspan="1" colspan="1">85.32</td>
<td align="left" rowspan="1" colspan="1">64.10</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RBFNetwork</bold></td>
<td align="left" rowspan="1" colspan="1">96.36</td>
<td align="left" rowspan="1" colspan="1">73.75</td>
<td align="left" rowspan="1" colspan="1">85.78</td>
<td align="left" rowspan="1" colspan="1">65.06</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>SMO</bold></td>
<td align="left" rowspan="1" colspan="1">96.87</td>
<td align="left" rowspan="1" colspan="1">73.33</td>
<td align="left" rowspan="1" colspan="1">88.83</td>
<td align="left" rowspan="1" colspan="1">57.98</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Vote</bold></td>
<td align="left" rowspan="1" colspan="1">65.01</td>
<td align="left" rowspan="1" colspan="1">73.53</td>
<td align="left" rowspan="1" colspan="1">79.38</td>
<td align="left" rowspan="1" colspan="1">57.98</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>VotedPerceptron</bold></td>
<td align="left" rowspan="1" colspan="1">91.08</td>
<td align="left" rowspan="1" colspan="1">73.82</td>
<td align="left" rowspan="1" colspan="1">78.09</td>
<td align="left" rowspan="1" colspan="1">63.53</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap>
<p>As we can observe in <xref ref-type="table" rid="pone-0095715-t003">Table 3</xref>, the CHAT-OHM does not surpass all the other classification algorithms, still it exhibits a competitive classification accuracy. For the Breast Cancer dataset the CHAT-OHM achieved a performance of 95% (9th place), only 2.34% below the best performer, BayesNet. For the Liver Disorders dataset the best classifier was RandomForest, with a 68.44% of classification accuracy, while the CHAT-OHM reached the 9th place with a difference of performance of 6.99%. For the case of Haberman’s Survival dataset CHAT-OHM achieved a classification accuracy of 66.36% which leaves it in 18th place with 8.44% below the best classifier, NaiveBayes. The best performance for Hepatitis Disease dataset was achieved by RandomForest with 90.61% of classification accuracy, while the CHAT-OHM was positioned in the 13th place with a classification accuracy of 84.96%.</p>
<p>Notice, however, that despite not exhibiting the best performance for any given dataset, CHAT-OHM has a consistent behavior: the proposed method reached the 9th place for the Breast Cancer dataset and the Liver Disorders dataset, while being the 13th place for the Hepatitis Disease dataset and the 18th place for the Haberman’s Survival dataset. On the other hand, Bayes Net is the best classifier for the Breast Cancer dataset, while being the 17th place for the Liver Disorders dataset, the 16th place at the Haberman’s Survival dataset, and the 9th place for the Hepatitis Disease dataset. Another example of this inconsistent performance is that of the NaiveBayes algorithm: it is the 5th place for Brest Cancer dataset, the worst method for the Liver Disorders dataset, the best at Haberman’s Survival dataset, and the 10th method for Hepatitis Disease dataset.</p>
</sec><sec id="s3b">
<title>Normalized Datasets</title>
<p>While performing the experiments, we noticed that some attribute values are significantly larger than the values of the rest of the attributes. As recommended by <xref ref-type="bibr" rid="pone.0095715-Duda1">[31]</xref> to avoid the impact of scale change, the dataset can be normalized. The justification usually given for this normalization is that it prevents certain features from dominating merely because they have large numerical values.</p>
<p><xref ref-type="table" rid="pone-0095715-t004">Table 4</xref> shows the classification accuracy achieved by our proposal and the 19 classification algorithms from WEKA, when applied to normalized datasets. For each dataset, the highest classification accuracy is emphasized with boldface. In general, no significant variations were achieved with respect to the results of the datasets without normalization. In most cases the improvement is less than 2 percent, with only two clear exceptions: VotedPerceptron and DMNBtext, which significantly increased their classification accuracy. The former exhibits an improvement of 5.79% for the Breast Cancer dataset and 6.94% for the Hepatitis Disease dataset, while the latter shows an improvement of 24.99% for the Breast Cancer dataset, 6.19% for the Liver Disorders dataset and 9.61% for the Hepatitis Disease dataset.</p>
<table-wrap id="pone-0095715-t004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095715.t004</object-id><label>Table 4</label><caption>
<title>Clasification accuracy comparison (%) normalized data.</title>
</caption><alternatives><graphic id="pone-0095715-t004-4" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095715.t004" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td colspan="4" align="left" rowspan="1">Dataset</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Algorithm</td>
<td align="left" rowspan="1" colspan="1">Breast Cancer</td>
<td align="left" rowspan="1" colspan="1">Haberman’s Survival</td>
<td align="left" rowspan="1" colspan="1">Hepatitis Disease</td>
<td align="left" rowspan="1" colspan="1">Liver Disorders</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>AdaBoostM1</bold></td>
<td align="left" rowspan="1" colspan="1">95.05</td>
<td align="left" rowspan="1" colspan="1">74.02</td>
<td align="left" rowspan="1" colspan="1">89.80</td>
<td align="left" rowspan="1" colspan="1">67.72</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>BayesNet</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>97.34</bold></td>
<td align="left" rowspan="1" colspan="1">71.73</td>
<td align="left" rowspan="1" colspan="1">87.68</td>
<td align="left" rowspan="1" colspan="1">56.62</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>BFTree</bold></td>
<td align="left" rowspan="1" colspan="1">94.80</td>
<td align="left" rowspan="1" colspan="1">72.43</td>
<td align="left" rowspan="1" colspan="1">88.92</td>
<td align="left" rowspan="1" colspan="1">67.15</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>CHAT-OHM</bold></td>
<td align="left" rowspan="1" colspan="1">95.52</td>
<td align="left" rowspan="1" colspan="1">62.45</td>
<td align="left" rowspan="1" colspan="1">89.52</td>
<td align="left" rowspan="1" colspan="1">58.5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>ComplementNaiveBayes</bold></td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>ConjunctiveRule</bold></td>
<td align="left" rowspan="1" colspan="1">91.98</td>
<td align="left" rowspan="1" colspan="1">72.97</td>
<td align="left" rowspan="1" colspan="1">89.05</td>
<td align="left" rowspan="1" colspan="1">56.28</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>DecisionTable</bold></td>
<td align="left" rowspan="1" colspan="1">95.69</td>
<td align="left" rowspan="1" colspan="1">71.90</td>
<td align="left" rowspan="1" colspan="1">88.46</td>
<td align="left" rowspan="1" colspan="1">58.83</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>DMNBtext</bold></td>
<td align="left" rowspan="1" colspan="1">90.00</td>
<td align="left" rowspan="1" colspan="1">73.01</td>
<td align="left" rowspan="1" colspan="1">88.99</td>
<td align="left" rowspan="1" colspan="1">64.17</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>IB1</bold></td>
<td align="left" rowspan="1" colspan="1">95.46</td>
<td align="left" rowspan="1" colspan="1">65.58</td>
<td align="left" rowspan="1" colspan="1">81.03</td>
<td align="left" rowspan="1" colspan="1">63.25</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>LWL</bold></td>
<td align="left" rowspan="1" colspan="1">92.14</td>
<td align="left" rowspan="1" colspan="1">71.90</td>
<td align="left" rowspan="1" colspan="1">89.95</td>
<td align="left" rowspan="1" colspan="1">60.74</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>MultiBoostAB</bold></td>
<td align="left" rowspan="1" colspan="1">94.73</td>
<td align="left" rowspan="1" colspan="1">73.28</td>
<td align="left" rowspan="1" colspan="1">89.49</td>
<td align="left" rowspan="1" colspan="1">64.29</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>MultiScheme</bold></td>
<td align="left" rowspan="1" colspan="1">65.01</td>
<td align="left" rowspan="1" colspan="1">73.53</td>
<td align="left" rowspan="1" colspan="1">79.38</td>
<td align="left" rowspan="1" colspan="1">57.98</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>NaiveBayes</bold></td>
<td align="left" rowspan="1" colspan="1">96.11</td>
<td align="left" rowspan="1" colspan="1">74.66</td>
<td align="left" rowspan="1" colspan="1">87.36</td>
<td align="left" rowspan="1" colspan="1">55.42</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>NaiveBayesMultinomial</bold></td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RandomForest</bold></td>
<td align="left" rowspan="1" colspan="1">96.33</td>
<td align="left" rowspan="1" colspan="1">67.81</td>
<td align="left" rowspan="1" colspan="1"><bold>90.54</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>68.44</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RandomTree</bold></td>
<td align="left" rowspan="1" colspan="1">94.79</td>
<td align="left" rowspan="1" colspan="1">64.96</td>
<td align="left" rowspan="1" colspan="1">83.70</td>
<td align="left" rowspan="1" colspan="1">62.75</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RBFNetwork</bold></td>
<td align="left" rowspan="1" colspan="1">96.36</td>
<td align="left" rowspan="1" colspan="1">73.75</td>
<td align="left" rowspan="1" colspan="1">85.78</td>
<td align="left" rowspan="1" colspan="1">64.81</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>SMO</bold></td>
<td align="left" rowspan="1" colspan="1">96.88</td>
<td align="left" rowspan="1" colspan="1">73.53</td>
<td align="left" rowspan="1" colspan="1">88.70</td>
<td align="left" rowspan="1" colspan="1">57.90</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Vote</bold></td>
<td align="left" rowspan="1" colspan="1">65.01</td>
<td align="left" rowspan="1" colspan="1">73.53</td>
<td align="left" rowspan="1" colspan="1">79.38</td>
<td align="left" rowspan="1" colspan="1">57.98</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>VotedPerceptron</bold></td>
<td align="left" rowspan="1" colspan="1">96.87</td>
<td align="left" rowspan="1" colspan="1"><bold>75.09</bold></td>
<td align="left" rowspan="1" colspan="1">85.03</td>
<td align="left" rowspan="1" colspan="1">65.86</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap>
<p>The performance of CHAT-OHM was not significantly affected by normalization, but for the Hepatitis Disease dataset the improvement of 4.56% changes its rank from the 13th place (<xref ref-type="table" rid="pone-0095715-t003">Table 3</xref>) to the 4th place (<xref ref-type="table" rid="pone-0095715-t004">Table 4</xref>).</p>
<p>The normalization method used in our experiments, produce both positive and negative normalized values. This situation did not allow us to perform the experiment with two classification algorithms from WEKA: ComplementNaiveBayes and NaiveBayesMultinomial, since these algorithms are unable to handle negative values.</p>
</sec><sec id="s3c">
<title>Outliers Treatment</title>
<p>During the testing phase, we also noticed the presence of some atypical pattern in the datasets. To verify the presence of outliers in the datasets, a method for detection and deletion of outliers called Wilson’s Edition was applied to the datasets <xref ref-type="bibr" rid="pone.0095715-Wilson1">[28]</xref>. <xref ref-type="table" rid="pone-0095715-t005">Table 5</xref> shows the amount of outliers found and deleted from the four datasets using this technique. The information presented by this table, shows that the Breast Cancer dataset presents only 3.22% of outliers while Haberman’s Survival dataset presents 36.45%. The fact that most of the classifiers work much better for the Breast Cancer dataset, may be justified by the almost absence of outliers in this dataset. The decision boundary between the classes appears to be better defined for the Breast Cancer dataset, thus the classification algorithms exhibit a higher classification accuracy than the one achieved with the other datasets, where the decision boundaries seem not so well defined.</p>
<table-wrap id="pone-0095715-t005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095715.t005</object-id><label>Table 5</label><caption>
<title>Number of outliers for dataset.</title>
</caption><alternatives><graphic id="pone-0095715-t005-5" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095715.t005" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td colspan="3" align="left" rowspan="1">Original Dataset</td>
<td colspan="4" align="left" rowspan="1">Outliers</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Total Instances</td>
<td align="left" rowspan="1" colspan="1">Class 1 Instances</td>
<td align="left" rowspan="1" colspan="1">Class 2 Instances</td>
<td align="left" rowspan="1" colspan="1">Class 1 Outliers</td>
<td align="left" rowspan="1" colspan="1">Class 2 Outliers</td>
<td align="left" rowspan="1" colspan="1">Total Outliers</td>
<td align="left" rowspan="1" colspan="1">Outliers %</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Breast Cancer</bold></td>
<td align="left" rowspan="1" colspan="1">683</td>
<td align="left" rowspan="1" colspan="1">444</td>
<td align="left" rowspan="1" colspan="1">239</td>
<td align="left" rowspan="1" colspan="1">11</td>
<td align="left" rowspan="1" colspan="1">11</td>
<td align="left" rowspan="1" colspan="1">22</td>
<td align="left" rowspan="1" colspan="1">3.22</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Haberman’s Survival</bold></td>
<td align="left" rowspan="1" colspan="1">345</td>
<td align="left" rowspan="1" colspan="1">145</td>
<td align="left" rowspan="1" colspan="1">200</td>
<td align="left" rowspan="1" colspan="1">71</td>
<td align="left" rowspan="1" colspan="1">55</td>
<td align="left" rowspan="1" colspan="1">126</td>
<td align="left" rowspan="1" colspan="1">36.52</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Hepatitis Disease</bold></td>
<td align="left" rowspan="1" colspan="1">306</td>
<td align="left" rowspan="1" colspan="1">225</td>
<td align="left" rowspan="1" colspan="1">81</td>
<td align="left" rowspan="1" colspan="1">25</td>
<td align="left" rowspan="1" colspan="1">59</td>
<td align="left" rowspan="1" colspan="1">84</td>
<td align="left" rowspan="1" colspan="1">27.45</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Liver Disorders</bold></td>
<td align="left" rowspan="1" colspan="1">155</td>
<td align="left" rowspan="1" colspan="1">32</td>
<td align="left" rowspan="1" colspan="1">123</td>
<td align="left" rowspan="1" colspan="1">14</td>
<td align="left" rowspan="1" colspan="1">13</td>
<td align="left" rowspan="1" colspan="1">27</td>
<td align="left" rowspan="1" colspan="1">17.41</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap>
<p><xref ref-type="table" rid="pone-0095715-t006">Table 6</xref> shows the classification accuracy achieved by our proposal and the 19 classification algorithms from WEKA, when applied to datasets without outliers. For each dataset, the highest classification accuracy is emphasized with boldface. The removal of outliers leads to an improvement for all the classification algorithms presented in this work. For the Breast Cancer dataset 22 outliers were removed, which represent the 3.22% of the original dataset. The improvement in the classification accuracy for this dataset varies from 0.5% to 4.27%. The CHAT-OHM shows an improvement of 3.95% for the Breast Cancer dataset, which changes its position from the 9th place (<xref ref-type="table" rid="pone-0095715-t003">Table 3</xref>) to the 4th place (<xref ref-type="table" rid="pone-0095715-t006">Table 6</xref>), as mentioned before.</p>
<table-wrap id="pone-0095715-t006" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095715.t006</object-id><label>Table 6</label><caption>
<title>Clasification accuracy comparison (%) data without outliers.</title>
</caption><alternatives><graphic id="pone-0095715-t006-6" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095715.t006" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td colspan="4" align="left" rowspan="1">Dataset</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Algorithm</td>
<td align="left" rowspan="1" colspan="1">Breast Cancer</td>
<td align="left" rowspan="1" colspan="1">Haberman’s Survival</td>
<td align="left" rowspan="1" colspan="1">Hepatitis Disease</td>
<td align="left" rowspan="1" colspan="1">Liver Disorders</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>AdaBoostM1</bold></td>
<td align="left" rowspan="1" colspan="1">98.41</td>
<td align="left" rowspan="1" colspan="1">94.84</td>
<td align="left" rowspan="1" colspan="1">96.24</td>
<td align="left" rowspan="1" colspan="1">83.44</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>BayesNet</bold></td>
<td align="left" rowspan="1" colspan="1">99.46</td>
<td align="left" rowspan="1" colspan="1">87.59</td>
<td align="left" rowspan="1" colspan="1">93.72</td>
<td align="left" rowspan="1" colspan="1">76.98</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>BFTree</bold></td>
<td align="left" rowspan="1" colspan="1">97.84</td>
<td align="left" rowspan="1" colspan="1">93.39</td>
<td align="left" rowspan="1" colspan="1">96.40</td>
<td align="left" rowspan="1" colspan="1">84.43</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>CHAT-OHM</bold></td>
<td align="left" rowspan="1" colspan="1">98.95</td>
<td align="left" rowspan="1" colspan="1">90.44</td>
<td align="left" rowspan="1" colspan="1">88.94</td>
<td align="left" rowspan="1" colspan="1">69.23</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>ComplementNaiveBayes</bold></td>
<td align="left" rowspan="1" colspan="1">86.79</td>
<td align="left" rowspan="1" colspan="1">90.60</td>
<td align="left" rowspan="1" colspan="1">86.72</td>
<td align="left" rowspan="1" colspan="1">64.44</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>ConjunctiveRule</bold></td>
<td align="left" rowspan="1" colspan="1">93.78</td>
<td align="left" rowspan="1" colspan="1">89.52</td>
<td align="left" rowspan="1" colspan="1">95.25</td>
<td align="left" rowspan="1" colspan="1">66.53</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>DecisionTable</bold></td>
<td align="left" rowspan="1" colspan="1">97.16</td>
<td align="left" rowspan="1" colspan="1">88.32</td>
<td align="left" rowspan="1" colspan="1">94.31</td>
<td align="left" rowspan="1" colspan="1">77.73</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>DMNBtext</bold></td>
<td align="left" rowspan="1" colspan="1">65.51</td>
<td align="left" rowspan="1" colspan="1">90.12</td>
<td align="left" rowspan="1" colspan="1">86.03</td>
<td align="left" rowspan="1" colspan="1">66.21</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>IB1</bold></td>
<td align="left" rowspan="1" colspan="1">99.35</td>
<td align="left" rowspan="1" colspan="1">93.70</td>
<td align="left" rowspan="1" colspan="1">87.71</td>
<td align="left" rowspan="1" colspan="1">75.13</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>LWL</bold></td>
<td align="left" rowspan="1" colspan="1">96.41</td>
<td align="left" rowspan="1" colspan="1">91.00</td>
<td align="left" rowspan="1" colspan="1">95.71</td>
<td align="left" rowspan="1" colspan="1">66.95</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>MultiBoostAB</bold></td>
<td align="left" rowspan="1" colspan="1">97.25</td>
<td align="left" rowspan="1" colspan="1">92.41</td>
<td align="left" rowspan="1" colspan="1">96.32</td>
<td align="left" rowspan="1" colspan="1">80.11</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>MultiScheme</bold></td>
<td align="left" rowspan="1" colspan="1">65.51</td>
<td align="left" rowspan="1" colspan="1">90.12</td>
<td align="left" rowspan="1" colspan="1">86.03</td>
<td align="left" rowspan="1" colspan="1">66.21</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>NaiveBayes</bold></td>
<td align="left" rowspan="1" colspan="1">97.93</td>
<td align="left" rowspan="1" colspan="1"><bold>95.25</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>97.63</bold></td>
<td align="left" rowspan="1" colspan="1">58.54</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>NaiveBayesMultinomial</bold></td>
<td align="left" rowspan="1" colspan="1">92.24</td>
<td align="left" rowspan="1" colspan="1">90.93</td>
<td align="left" rowspan="1" colspan="1">87.72</td>
<td align="left" rowspan="1" colspan="1">65.44</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RandomForest</bold></td>
<td align="left" rowspan="1" colspan="1">98.55</td>
<td align="left" rowspan="1" colspan="1">93.79</td>
<td align="left" rowspan="1" colspan="1">96.10</td>
<td align="left" rowspan="1" colspan="1"><bold>87.36</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RandomTree</bold></td>
<td align="left" rowspan="1" colspan="1">97.75</td>
<td align="left" rowspan="1" colspan="1">92.85</td>
<td align="left" rowspan="1" colspan="1">93.65</td>
<td align="left" rowspan="1" colspan="1">81.98</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RBFNetwork</bold></td>
<td align="left" rowspan="1" colspan="1">98.46</td>
<td align="left" rowspan="1" colspan="1">93.26</td>
<td align="left" rowspan="1" colspan="1">96.62</td>
<td align="left" rowspan="1" colspan="1">79.03</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>SMO</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>99.50</bold></td>
<td align="left" rowspan="1" colspan="1">91.24</td>
<td align="left" rowspan="1" colspan="1">92.63</td>
<td align="left" rowspan="1" colspan="1">66.21</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Vote</bold></td>
<td align="left" rowspan="1" colspan="1">65.51</td>
<td align="left" rowspan="1" colspan="1">90.12</td>
<td align="left" rowspan="1" colspan="1">86.03</td>
<td align="left" rowspan="1" colspan="1">66.21</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>VotedPerceptron</bold></td>
<td align="left" rowspan="1" colspan="1">92.90</td>
<td align="left" rowspan="1" colspan="1">90.39</td>
<td align="left" rowspan="1" colspan="1">85.40</td>
<td align="left" rowspan="1" colspan="1">73.89</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap>
<p><xref ref-type="table" rid="pone-0095715-t007">Table 7</xref> shows the classification accuracy achieved by our proposal and the 19 classification algorithms from WEKA, when applied to normalized datasets without outliers. For the Liver Disorder dataset, increases in the classification accuracy can be observed when compared with the experiments performed on the original datasets. But if we compare the result of experiments with the datasets without outliers and the ones with the normalized datasets without outliers, the classification accuracy gets worse instead of better. In general, it seems that for this dataset, it is better not to use normalization and instead rely on the removal of outliers. On the other hand CHAT-OHM performed better with the normalized and outliers-free dataset. The original performance was 61.45%, the performance with the outliers-free dataset was 69.23% and the performance with the normalized outliers-free dataset was 74.13%; with this improvement the classifier changes its rank from the 9th place with the original dataset (<xref ref-type="table" rid="pone-0095715-t003">Table 3</xref>) to the 4th place with the normalized outliers-free dataset (<xref ref-type="table" rid="pone-0095715-t007">Table 7</xref>).</p>
<table-wrap id="pone-0095715-t007" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095715.t007</object-id><label>Table 7</label><caption>
<title>Clasification accuracy comparison (%) normalized data without outliers.</title>
</caption><alternatives><graphic id="pone-0095715-t007-7" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095715.t007" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td colspan="4" align="left" rowspan="1">Dataset</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Algorithm</td>
<td align="left" rowspan="1" colspan="1">Breast Cancer</td>
<td align="left" rowspan="1" colspan="1">Haberman’s Survival</td>
<td align="left" rowspan="1" colspan="1">Hepatitis Disease</td>
<td align="left" rowspan="1" colspan="1">Liver Disorders</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>AdaBoostM1</bold></td>
<td align="left" rowspan="1" colspan="1">98.41</td>
<td align="left" rowspan="1" colspan="1">94.84</td>
<td align="left" rowspan="1" colspan="1">96.24</td>
<td align="left" rowspan="1" colspan="1">75.58</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>BayesNet</bold></td>
<td align="left" rowspan="1" colspan="1">99.47</td>
<td align="left" rowspan="1" colspan="1">87.59</td>
<td align="left" rowspan="1" colspan="1">93.72</td>
<td align="left" rowspan="1" colspan="1">66.80</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>BFTree</bold></td>
<td align="left" rowspan="1" colspan="1">97.70</td>
<td align="left" rowspan="1" colspan="1">93.75</td>
<td align="left" rowspan="1" colspan="1">96.40</td>
<td align="left" rowspan="1" colspan="1">75.63</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>CHAT-OHM</bold></td>
<td align="left" rowspan="1" colspan="1">97.69</td>
<td align="left" rowspan="1" colspan="1">90.53</td>
<td align="left" rowspan="1" colspan="1">89.18</td>
<td align="left" rowspan="1" colspan="1">74.13</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>ComplementNaiveBayes</bold></td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>ConjunctiveRule</bold></td>
<td align="left" rowspan="1" colspan="1">93.78</td>
<td align="left" rowspan="1" colspan="1">89.52</td>
<td align="left" rowspan="1" colspan="1">96.33</td>
<td align="left" rowspan="1" colspan="1">61.41</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>DecisionTable</bold></td>
<td align="left" rowspan="1" colspan="1">97.17</td>
<td align="left" rowspan="1" colspan="1">88.32</td>
<td align="left" rowspan="1" colspan="1">94.31</td>
<td align="left" rowspan="1" colspan="1">68.28</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>DMNBtext</bold></td>
<td align="left" rowspan="1" colspan="1">94.49</td>
<td align="left" rowspan="1" colspan="1">90.12</td>
<td align="left" rowspan="1" colspan="1">93.02</td>
<td align="left" rowspan="1" colspan="1">69.28</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>IB1</bold></td>
<td align="left" rowspan="1" colspan="1">99.35</td>
<td align="left" rowspan="1" colspan="1">93.70</td>
<td align="left" rowspan="1" colspan="1">87.71</td>
<td align="left" rowspan="1" colspan="1">69.19</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>LWL</bold></td>
<td align="left" rowspan="1" colspan="1">96.41</td>
<td align="left" rowspan="1" colspan="1">91.00</td>
<td align="left" rowspan="1" colspan="1">95.71</td>
<td align="left" rowspan="1" colspan="1">63.85</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>MultiBoostAB</bold></td>
<td align="left" rowspan="1" colspan="1">97.25</td>
<td align="left" rowspan="1" colspan="1">92.42</td>
<td align="left" rowspan="1" colspan="1">96.32</td>
<td align="left" rowspan="1" colspan="1">72.20</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>MultiScheme</bold></td>
<td align="left" rowspan="1" colspan="1">65.51</td>
<td align="left" rowspan="1" colspan="1">90.12</td>
<td align="left" rowspan="1" colspan="1">86.03</td>
<td align="left" rowspan="1" colspan="1">62.10</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>NaiveBayes</bold></td>
<td align="left" rowspan="1" colspan="1">97.79</td>
<td align="left" rowspan="1" colspan="1"><bold>95.20</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>97.02</bold></td>
<td align="left" rowspan="1" colspan="1">56.85</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>NaiveBayesMultinomial</bold></td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RandomForest</bold></td>
<td align="left" rowspan="1" colspan="1">98.70</td>
<td align="left" rowspan="1" colspan="1">93.75</td>
<td align="left" rowspan="1" colspan="1">96.39</td>
<td align="left" rowspan="1" colspan="1"><bold>77.26</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RandomTree</bold></td>
<td align="left" rowspan="1" colspan="1">97.65</td>
<td align="left" rowspan="1" colspan="1">92.77</td>
<td align="left" rowspan="1" colspan="1">91.67</td>
<td align="left" rowspan="1" colspan="1">71.97</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RBFNetwork</bold></td>
<td align="left" rowspan="1" colspan="1">98.46</td>
<td align="left" rowspan="1" colspan="1">93.26</td>
<td align="left" rowspan="1" colspan="1">96.92</td>
<td align="left" rowspan="1" colspan="1">71.92</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>SMO</bold></td>
<td align="left" rowspan="1" colspan="1">99.50</td>
<td align="left" rowspan="1" colspan="1">91.33</td>
<td align="left" rowspan="1" colspan="1">92.63</td>
<td align="left" rowspan="1" colspan="1">62.05</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Vote</bold></td>
<td align="left" rowspan="1" colspan="1">65.51</td>
<td align="left" rowspan="1" colspan="1">90.12</td>
<td align="left" rowspan="1" colspan="1">86.03</td>
<td align="left" rowspan="1" colspan="1">62.10</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>VotedPerceptron</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>99.88</bold></td>
<td align="left" rowspan="1" colspan="1">93.63</td>
<td align="left" rowspan="1" colspan="1">86.97</td>
<td align="left" rowspan="1" colspan="1">72.10</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap></sec><sec id="s3d">
<title>Improvement Analysis</title>
<p>From the results presented in <xref ref-type="table" rid="pone-0095715-t003">Table 3</xref>, <xref ref-type="table" rid="pone-0095715-t004">4</xref>, <xref ref-type="table" rid="pone-0095715-t006">6</xref>, and <xref ref-type="table" rid="pone-0095715-t007">7</xref>, it is shown that there is no specific classification algorithm that exceed all the other algorithms in all the presented problems. This claim is supported by the No-Free-Lunch Theorems presented by Wolpert and Macready <xref ref-type="bibr" rid="pone.0095715-Wolpert1">[30]</xref>, which establish that for an algorithm, any performance gain in one kind of problem is offset by its performance loss in other kind of problems.</p>
<p><xref ref-type="table" rid="pone-0095715-t008">Table 8</xref>, <xref ref-type="table" rid="pone-0095715-t009">9</xref>, <xref ref-type="table" rid="pone-0095715-t010">10</xref>, and <xref ref-type="table" rid="pone-0095715-t011">11</xref> show the percentage of improvement achieved by our proposal and the 19 classification algorithms from WEKA, when applied to normalized datasets, datasets without outliers, and normalized datasets without outliers, for each of the four datasets used.</p>
<table-wrap id="pone-0095715-t008" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095715.t008</object-id><label>Table 8</label><caption>
<title>Comparison of classification Improvement (%) for Breast Cancer dataset.</title>
</caption><alternatives><graphic id="pone-0095715-t008-8" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095715.t008" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td colspan="3" align="left" rowspan="1">Breast Cancer</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Algorithm</td>
<td align="left" rowspan="1" colspan="1">Normalization</td>
<td align="left" rowspan="1" colspan="1">Without Outliers</td>
<td align="left" rowspan="1" colspan="1">Without Outliers Normalized</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>AdaBoostM1</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">3.36</td>
<td align="left" rowspan="1" colspan="1">3.36</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>BayesNet</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">2.12</td>
<td align="left" rowspan="1" colspan="1">2.13</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>BFTree</bold></td>
<td align="left" rowspan="1" colspan="1">−0.08</td>
<td align="left" rowspan="1" colspan="1">2.96</td>
<td align="left" rowspan="1" colspan="1">2.82</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>CHAT-OHM</bold></td>
<td align="left" rowspan="1" colspan="1">0.52</td>
<td align="left" rowspan="1" colspan="1">3.95</td>
<td align="left" rowspan="1" colspan="1">2.69</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>ComplementNaiveBayes</bold></td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">1.36</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>ConjunctiveRule</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">1.80</td>
<td align="left" rowspan="1" colspan="1">1.80</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>DecisionTable</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">1.47</td>
<td align="left" rowspan="1" colspan="1">1.48</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>DMNBtext</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>24.99</bold></td>
<td align="left" rowspan="1" colspan="1">0.50</td>
<td align="left" rowspan="1" colspan="1"><bold>29.48</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>IB1</bold></td>
<td align="left" rowspan="1" colspan="1">−0.29</td>
<td align="left" rowspan="1" colspan="1">3.60</td>
<td align="left" rowspan="1" colspan="1">3.60</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>LWL</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1"><bold>4.27</bold></td>
<td align="left" rowspan="1" colspan="1">4.27</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>MultiBoostAB</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">2.52</td>
<td align="left" rowspan="1" colspan="1">2.52</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>MultiScheme</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">0.50</td>
<td align="left" rowspan="1" colspan="1">0.50</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>NaiveBayes</bold></td>
<td align="left" rowspan="1" colspan="1">−0.15</td>
<td align="left" rowspan="1" colspan="1">1.67</td>
<td align="left" rowspan="1" colspan="1">1.53</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>NaiveBayesMultinomial</bold></td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">1.92</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RandomForest</bold></td>
<td align="left" rowspan="1" colspan="1">−0.14</td>
<td align="left" rowspan="1" colspan="1">2.08</td>
<td align="left" rowspan="1" colspan="1">2.23</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RandomTree</bold></td>
<td align="left" rowspan="1" colspan="1">0.05</td>
<td align="left" rowspan="1" colspan="1">3.01</td>
<td align="left" rowspan="1" colspan="1">2.91</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RBFNetwork</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">2.10</td>
<td align="left" rowspan="1" colspan="1">2.10</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>SMO</bold></td>
<td align="left" rowspan="1" colspan="1">0.01</td>
<td align="left" rowspan="1" colspan="1">2.63</td>
<td align="left" rowspan="1" colspan="1">2.63</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Vote</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">0.50</td>
<td align="left" rowspan="1" colspan="1">0.50</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>VotedPerceptron</bold></td>
<td align="left" rowspan="1" colspan="1">5.79</td>
<td align="left" rowspan="1" colspan="1">1.82</td>
<td align="left" rowspan="1" colspan="1">8.8</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap><table-wrap id="pone-0095715-t009" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095715.t009</object-id><label>Table 9</label><caption>
<title>Comparison of classification improvement (%) for haberman’s survival dataset.</title>
</caption><alternatives><graphic id="pone-0095715-t009-9" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095715.t009" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td colspan="3" align="left" rowspan="1">Haberman’s Survival</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Algorithm</td>
<td align="left" rowspan="1" colspan="1">Normalization</td>
<td align="left" rowspan="1" colspan="1">Without Outliers</td>
<td align="left" rowspan="1" colspan="1">Without Outliers Normalized</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>AdaBoostM1</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">20.82</td>
<td align="left" rowspan="1" colspan="1">20.82</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>BayesNet</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">15.86</td>
<td align="left" rowspan="1" colspan="1">15.86</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>BFTree</bold></td>
<td align="left" rowspan="1" colspan="1">0.10</td>
<td align="left" rowspan="1" colspan="1">21.06</td>
<td align="left" rowspan="1" colspan="1">21.42</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>ComplementNaiveBayes</bold></td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">16.73</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>ConjunctiveRule</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">16.55</td>
<td align="left" rowspan="1" colspan="1">16.55</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>DecisionTable</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">16.52</td>
<td align="left" rowspan="1" colspan="1">16.42</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>DMNBtext</bold></td>
<td align="left" rowspan="1" colspan="1">−0.52</td>
<td align="left" rowspan="1" colspan="1">16.59</td>
<td align="left" rowspan="1" colspan="1">16.59</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>IB1</bold></td>
<td align="left" rowspan="1" colspan="1">−0.19</td>
<td align="left" rowspan="1" colspan="1">27.93</td>
<td align="left" rowspan="1" colspan="1">27.93</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>LWL</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">19.10</td>
<td align="left" rowspan="1" colspan="1">19.10</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>MultiBoostAB</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">19.13</td>
<td align="left" rowspan="1" colspan="1">19.13</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>MultiScheme</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">16.59</td>
<td align="left" rowspan="1" colspan="1">16.59</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>NaiveBayes</bold></td>
<td align="left" rowspan="1" colspan="1">−0.14</td>
<td align="left" rowspan="1" colspan="1">20.45</td>
<td align="left" rowspan="1" colspan="1">20.40</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>NaiveBayesMultinomial</bold></td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">17.29</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RandomForest</bold></td>
<td align="left" rowspan="1" colspan="1">−0.13</td>
<td align="left" rowspan="1" colspan="1">25.85</td>
<td align="left" rowspan="1" colspan="1">25.81</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RandomTree</bold></td>
<td align="left" rowspan="1" colspan="1">0.48</td>
<td align="left" rowspan="1" colspan="1"><bold>28.37</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>28.29</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RBFNetwork</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">19.51</td>
<td align="left" rowspan="1" colspan="1">19.51</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>SMO</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">17.91</td>
<td align="left" rowspan="1" colspan="1">18.00</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Vote</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">16.59</td>
<td align="left" rowspan="1" colspan="1">16.59</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>VotedPerceptron</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>1.27</bold></td>
<td align="left" rowspan="1" colspan="1">16.57</td>
<td align="left" rowspan="1" colspan="1">19.81</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>CHAT-OHM</bold></td>
<td align="left" rowspan="1" colspan="1">−3.91</td>
<td align="left" rowspan="1" colspan="1">24.08</td>
<td align="left" rowspan="1" colspan="1">24.17</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap><table-wrap id="pone-0095715-t010" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095715.t010</object-id><label>Table 10</label><caption>
<title>Comparison of classification improvement (%) for hepatitis disease dataset.</title>
</caption><alternatives><graphic id="pone-0095715-t010-10" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095715.t010" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td colspan="3" align="left" rowspan="1">Hepatitis Disease</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Algorithm</td>
<td align="left" rowspan="1" colspan="1">Normalization</td>
<td align="left" rowspan="1" colspan="1">Without Outliers</td>
<td align="left" rowspan="1" colspan="1">Without Outliers Normalized</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>AdaBoostM1</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">6.44</td>
<td align="left" rowspan="1" colspan="1">6.44</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>BayesNet</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">6.04</td>
<td align="left" rowspan="1" colspan="1">6.04</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>BFTree</bold></td>
<td align="left" rowspan="1" colspan="1">−0.06</td>
<td align="left" rowspan="1" colspan="1">7.42</td>
<td align="left" rowspan="1" colspan="1">7.42</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>CHAT-OHM</bold></td>
<td align="left" rowspan="1" colspan="1">4.56</td>
<td align="left" rowspan="1" colspan="1">3.98</td>
<td align="left" rowspan="1" colspan="1">4.22</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>ComplementNaiveBayes</bold></td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">9.18</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>ConjunctiveRule</bold></td>
<td align="left" rowspan="1" colspan="1">0.06</td>
<td align="left" rowspan="1" colspan="1">6.26</td>
<td align="left" rowspan="1" colspan="1">7.34</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>DecisionTable</bold></td>
<td align="left" rowspan="1" colspan="1">0.06</td>
<td align="left" rowspan="1" colspan="1">5.91</td>
<td align="left" rowspan="1" colspan="1">5.91</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>DMNBtext</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>9.61</bold></td>
<td align="left" rowspan="1" colspan="1">6.65</td>
<td align="left" rowspan="1" colspan="1"><bold>13.64</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>IB1</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">6.68</td>
<td align="left" rowspan="1" colspan="1">6.68</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>LWL</bold></td>
<td align="left" rowspan="1" colspan="1">0.25</td>
<td align="left" rowspan="1" colspan="1">6.01</td>
<td align="left" rowspan="1" colspan="1">6.01</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>MultiBoostAB</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">6.83</td>
<td align="left" rowspan="1" colspan="1">6.83</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>MultiScheme</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">6.65</td>
<td align="left" rowspan="1" colspan="1">6.65</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>NaiveBayes</bold></td>
<td align="left" rowspan="1" colspan="1">−0.14</td>
<td align="left" rowspan="1" colspan="1">10.13</td>
<td align="left" rowspan="1" colspan="1">9.52</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>NaiveBayesMultinomial</bold></td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">9.72</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RandomForest</bold></td>
<td align="left" rowspan="1" colspan="1">−0.07</td>
<td align="left" rowspan="1" colspan="1">5.49</td>
<td align="left" rowspan="1" colspan="1">5.78</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RandomTree</bold></td>
<td align="left" rowspan="1" colspan="1">−1.62</td>
<td align="left" rowspan="1" colspan="1">8.33</td>
<td align="left" rowspan="1" colspan="1">6.35</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RBFNetwork</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1"><bold>11.14</bold></td>
<td align="left" rowspan="1" colspan="1">11.14</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>SMO</bold></td>
<td align="left" rowspan="1" colspan="1">−0.13</td>
<td align="left" rowspan="1" colspan="1">3.80</td>
<td align="left" rowspan="1" colspan="1">3.80</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Vote</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">6.65</td>
<td align="left" rowspan="1" colspan="1">6.65</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>VotedPerceptron</bold></td>
<td align="left" rowspan="1" colspan="1">6.94</td>
<td align="left" rowspan="1" colspan="1">7.31</td>
<td align="left" rowspan="1" colspan="1">8.88</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap><table-wrap id="pone-0095715-t011" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095715.t011</object-id><label>Table 11</label><caption>
<title>Comparison of classification improvement (%) for liver disorders dataset.</title>
</caption><alternatives><graphic id="pone-0095715-t011-11" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095715.t011" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td colspan="3" align="left" rowspan="1">Liver Disorders</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Algorithm</td>
<td align="left" rowspan="1" colspan="1">Normalization</td>
<td align="left" rowspan="1" colspan="1">Without Outliers</td>
<td align="left" rowspan="1" colspan="1">Without Outliers Normalized</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>AdaBoostM1</bold></td>
<td align="left" rowspan="1" colspan="1">1.76</td>
<td align="left" rowspan="1" colspan="1">17.48</td>
<td align="left" rowspan="1" colspan="1">9.62</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>BayesNet</bold></td>
<td align="left" rowspan="1" colspan="1">−0.23</td>
<td align="left" rowspan="1" colspan="1"><bold>20.13</bold></td>
<td align="left" rowspan="1" colspan="1">9.95</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>BFTree</bold></td>
<td align="left" rowspan="1" colspan="1">0.71</td>
<td align="left" rowspan="1" colspan="1">17.99</td>
<td align="left" rowspan="1" colspan="1">9.19</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>CHAT-OHM</bold></td>
<td align="left" rowspan="1" colspan="1">−2.95</td>
<td align="left" rowspan="1" colspan="1">7.78</td>
<td align="left" rowspan="1" colspan="1"><bold>12.68</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>ComplementNaiveBayes</bold></td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">7.87</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>ConjunctiveRule</bold></td>
<td align="left" rowspan="1" colspan="1">0.22</td>
<td align="left" rowspan="1" colspan="1">10.47</td>
<td align="left" rowspan="1" colspan="1">5.35</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>DecisionTable</bold></td>
<td align="left" rowspan="1" colspan="1">−0.28</td>
<td align="left" rowspan="1" colspan="1">18.62</td>
<td align="left" rowspan="1" colspan="1">9.17</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>DMNBtext</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>6.19</bold></td>
<td align="left" rowspan="1" colspan="1">8.23</td>
<td align="left" rowspan="1" colspan="1">11.30</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>IB1</bold></td>
<td align="left" rowspan="1" colspan="1">1.03</td>
<td align="left" rowspan="1" colspan="1">12.91</td>
<td align="left" rowspan="1" colspan="1">6.97</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>LWL</bold></td>
<td align="left" rowspan="1" colspan="1">−0.06</td>
<td align="left" rowspan="1" colspan="1">6.15</td>
<td align="left" rowspan="1" colspan="1">3.05</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>MultiBoostAB</bold></td>
<td align="left" rowspan="1" colspan="1">−1.00</td>
<td align="left" rowspan="1" colspan="1">14.82</td>
<td align="left" rowspan="1" colspan="1">6.91</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>MultiScheme</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">8.23</td>
<td align="left" rowspan="1" colspan="1">4.12</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>NaiveBayes</bold></td>
<td align="left" rowspan="1" colspan="1">0.53</td>
<td align="left" rowspan="1" colspan="1">3.65</td>
<td align="left" rowspan="1" colspan="1">1.96</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>NaiveBayesMultinomial</bold></td>
<td align="left" rowspan="1" colspan="1">N/A</td>
<td align="left" rowspan="1" colspan="1">8.48</td>
<td align="left" rowspan="1" colspan="1">N/A</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RandomForest</bold></td>
<td align="left" rowspan="1" colspan="1">−0.68</td>
<td align="left" rowspan="1" colspan="1">18.92</td>
<td align="left" rowspan="1" colspan="1">8.82</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RandomTree</bold></td>
<td align="left" rowspan="1" colspan="1">−1.35</td>
<td align="left" rowspan="1" colspan="1">17.88</td>
<td align="left" rowspan="1" colspan="1">7.87</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>RBFNetwork</bold></td>
<td align="left" rowspan="1" colspan="1">−0.25</td>
<td align="left" rowspan="1" colspan="1">13.97</td>
<td align="left" rowspan="1" colspan="1">6.86</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>SMO</bold></td>
<td align="left" rowspan="1" colspan="1">−0.08</td>
<td align="left" rowspan="1" colspan="1">8.23</td>
<td align="left" rowspan="1" colspan="1">4.07</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Vote</bold></td>
<td align="left" rowspan="1" colspan="1">0.00</td>
<td align="left" rowspan="1" colspan="1">8.23</td>
<td align="left" rowspan="1" colspan="1">4.12</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>VotedPerceptron</bold></td>
<td align="left" rowspan="1" colspan="1">2.33</td>
<td align="left" rowspan="1" colspan="1">10.36</td>
<td align="left" rowspan="1" colspan="1">8.57</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap>
<p>For the Brest Cancer dataset, CHAT-OHM exhibits an improvement of 3.95%, being the second algorithm with higher improvement when removing the outliers. The dataset that presented greatest improvements with the removal of outliers was Haberman’s Survival. On average the classification accuracy improved from 71.82% to 91.49%. The improvements for this dataset vary from 15.86% to 28.37%. The CHAT-OHM exhibit an improvement of 24.08% when removing the outliers, positioning itself in the fourth place of the algorithms with higher improvements. For the Liver Disorders dataset the improvements when removing the outliers vary from 3.65% to 20.13%. The CHAT-OHM shows an improvement of 7.78%, which is relatively low when compared with the improvements presented by the rest of the algorithms for this dataset.</p>
<p>With the normalized outliers-free datasets, CHAT-OHM shows an improvement of 12.68% with the Liver Disorder dataset and its rank changes from the 9th place to the 4th place. Also, it was the classifier with the best improvement for this dataset. For the Haberman’s Survival dataset the model exhibit a 24.08% of increase in its performance and it was the fourth best improvement for this dataset.</p>
</sec></sec><sec id="s4">
<title>Conclusions</title>
<p>In this paper, we present a method that combines a Hybrid Associative classifier, a coding technique for output classes and a procedure of majority voting during the classification phase. This method is called CHAT-OHM. During the experimental phase, this method is applied to four different datasets related to the medical field. The performance of the method is compared with 19 machine learning algorithms implemented in WEKA Data Mining Software.</p>
<p>The proposed method uses an associative classifier, the CHAT <xref ref-type="bibr" rid="pone.0095715-SantiagoMontero1">[16]</xref>, combined with a novel coding technique and a voting procedure. The results obtained demonstrate that the proposed method improved the result obtained by the CHAT.</p>
<p>However the experiments show that the CHAT-OHM is sensitive to the presence of outliers. To improve the classification accuracy of this algorithm, it has to be combined with a method of detection and removal of outliers. In the present work we use Wilson’s Edition as such method.</p>
<p>The CHAT-OHM presented fairly good results and a consistent behavior when applied to the four datasets used in this study. The performance of the model was not significant affected by the normalization process. On the other hand it was positive affected by the removal of outliers, displaying remarkable improvement in its performance, such as the ranking improvement for the Breast Cancer (4th place) with a performance increase of 3.95%. Another significant performance enhancement was obtained with the Liver Disorders dataset using normalization and outliers removal, the CHAT-OHM improved its rank to the 4th place with an increase of the classification accuracy of 12.68%.</p>
<p>It should be mentioned that our proposal is part of a family of methods based on the CHAT classifier. The main difference between these methods is the coding technique of each one, such as: Modified Johnson-Möbius binary coding, Gray coding, among others.</p>
</sec></body>
<back>
<ack>
<p>The authors would like to thank the Instituto Politécnico Nacional (Secretaría Académica, COFAA, SIP, CIC, and CIDETEC), the CONACyT, and SNI for their economical support to develop this work.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0095715-Steinbuch1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Steinbuch</surname><given-names>K</given-names></name> (<year>1961</year>) <article-title>Die lernmatrix</article-title>. <source>Kybernetik</source> <volume>1</volume>: <fpage>36</fpage>–<lpage>45</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095715-Willshaw1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Willshaw</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Buneman</surname><given-names>OP</given-names></name>, <name name-style="western"><surname>Longuet-Higgins</surname><given-names>HC</given-names></name> (<year>1969</year>) <article-title>Non-holographic associative memory</article-title>. <source>Nature</source> <volume>222</volume>: <fpage>960</fpage>–<lpage>962</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095715-Anderson1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anderson</surname><given-names>JA</given-names></name> (<year>1972</year>) <article-title>A simple neural network generating an interactive memory</article-title>. <source>Mathematical Biosciences</source> <volume>14</volume>: <fpage>197</fpage>–<lpage>220</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095715-Kohonen1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kohonen</surname><given-names>T</given-names></name> (<year>1972</year>) <article-title>Correlation matrix memories</article-title>. <source>IEEE Transactions on Computers</source> <volume>C-21</volume>: <fpage>353</fpage>–<lpage>359</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095715-Nakano1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nakano</surname><given-names>K</given-names></name> (<year>1972</year>) <article-title>Associatron - a model of associative memory</article-title>. <source>IEEE Transactions on Systems, Man, and Cybernetics</source> <volume>SMC-2</volume>: <fpage>380</fpage>–<lpage>388</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095715-Amari1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amari</surname><given-names>SI</given-names></name> (<year>1972</year>) <article-title>Pattern learning by self-organizing nets of threshold elements</article-title>. <source>System and Computing Controls</source> <volume>3</volume>: <fpage>15</fpage>–<lpage>22</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095715-Hopfield1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name> (<year>1982</year>) <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>79</volume>: <fpage>2554</fpage>–<lpage>2558</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095715-Kosko1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kosko</surname><given-names>B</given-names></name> (<year>1980</year>) <article-title>Bidirectional associative memories</article-title>. <source>IEEE Transactions on Systems, Man, and Cybernetics</source> <volume>18</volume>: <fpage>49</fpage>–<lpage>60</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095715-Ritter1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ritter</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Sussner</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Diaz-de Leon</surname><given-names>J</given-names></name> (<year>1998</year>) <article-title>Morphological associative memories</article-title>. <source>IEEE Transactions on Neural Networks</source> <volume>9</volume>: <fpage>281</fpage>–<lpage>293</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095715-LpezYaez1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>López-Yañez</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Argüelles-Cruz</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Camacho-Nieto</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Yañez-Marquez</surname><given-names>C</given-names></name> (<year>2011</year>) <article-title>Pollutants Time-Series Prediction Using the Gamma Classifier</article-title>. <source>International Journal of Computational Intelligence Systems</source> <volume>4</volume>: <fpage>680</fpage>–<lpage>711</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095715-Mathai1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mathai</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Upadhyaya</surname><given-names>B</given-names></name> (<year>1989</year>) <article-title>Performance analysis and application of the bidirectional associative memory to industrial spectral signatures</article-title>. <source>International Joint Conference on Neural Networks</source> <volume>1</volume>: <fpage>33</fpage>–<lpage>37</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095715-Guzmn1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Guzmán</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Pogrebnyak</surname><given-names>OB</given-names></name>, <name name-style="western"><surname>Yáñez</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Moreno</surname><given-names>JA</given-names></name> (<year>2006</year>) <article-title>Image compression algorithm based on morphological associative memories</article-title>. <source>Progress in Pattern Recognition, Image Analysis and Applications</source> <volume>4225</volume>: <fpage>519</fpage>–<lpage>528</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095715-Chartier1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chartier</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Lepage</surname><given-names>R</given-names></name> (<year>2002</year>) <article-title>Learning and extracting edges from images by a modified Hopfield neural network</article-title>. <source>Proceedings of the 16th International Conference on Pattern Recognition</source> <volume>3</volume>: <fpage>431</fpage>–<lpage>434</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095715-YezMrquez1"><label>14</label>
<mixed-citation publication-type="other" xlink:type="simple">Yáñez-Márquez C, Diaz de León JL (2001). Lernmatrix de Steinbuch, Mexico: IT 48, Serie Verde, CIC-IPN.</mixed-citation>
</ref>
<ref id="pone.0095715-YezMrquez2"><label>15</label>
<mixed-citation publication-type="other" xlink:type="simple">Yáñez-Márquez C, Diaz de León JL (2001). Linear Associator de Anderson-Kohonen, Mexico: IT 50, Serie Verde, CIC-IPN.</mixed-citation>
</ref>
<ref id="pone.0095715-SantiagoMontero1"><label>16</label>
<mixed-citation publication-type="other" xlink:type="simple">Santiago-Montero R (2003). Clasificador Híbrido de Patrones basado en la Lernmatrix de Steinbuch y el Linear Associator de Anderson-Kohonen. MSc Thesis, CIC, IPN, Mexico.</mixed-citation>
</ref>
<ref id="pone.0095715-University1"><label>17</label>
<mixed-citation publication-type="other" xlink:type="simple">University of California, Irvine Machine Learning Repository website. Bache K, Lichman M (2013) UCI Machine Learning Repository. Available: <ext-link ext-link-type="uri" xlink:href="http://archive.ics.uci.edu/ml" xlink:type="simple">http://archive.ics.uci.edu/ml</ext-link> Irvine, CA: University of California, School of Information and Computer Science. Accessed 2014 April 3.</mixed-citation>
</ref>
<ref id="pone.0095715-WEKA1"><label>18</label>
<mixed-citation publication-type="other" xlink:type="simple">WEKA website. Hall M, Frank E, Holmes G, Pfahringer B, Reutemann P, Witten IH (2010) WEKA 3: Data mining software in java. Available: <ext-link ext-link-type="uri" xlink:href="http://www.cs.waikato.ac.nz/ml/weka/Accessed" xlink:type="simple">http://www.cs.waikato.ac.nz/ml/weka/Accessed</ext-link> 2014 April 3.</mixed-citation>
</ref>
<ref id="pone.0095715-Witten1"><label>19</label>
<mixed-citation publication-type="other" xlink:type="simple">Witten IH, Frank E, Hall M A (2011) Data Mining Practical Machine Learning Tools and Techniques. Elsevier.</mixed-citation>
</ref>
<ref id="pone.0095715-Breiman1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Breiman</surname><given-names>L</given-names></name> (<year>2001</year>) <article-title>Random forests</article-title>. <source>Machine Learning</source> <volume>45</volume>: <fpage>5</fpage>–<lpage>32</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095715-Platt1"><label>21</label>
<mixed-citation publication-type="other" xlink:type="simple">Platt JC (1998) Fast training of support vector machines using sequential minimal optimization. In: Schoelkopf B, Burges C, Smola A (Eds.). Advances in Kernel Methods–Support Vector Learning. MIT Press.</mixed-citation>
</ref>
<ref id="pone.0095715-Freund1"><label>22</label>
<mixed-citation publication-type="other" xlink:type="simple">Freund Y, Schapire RE (1996) Experiments with a new boosting algorithm. Thirteenth International Conference on Machine Learning: 148–156.</mixed-citation>
</ref>
<ref id="pone.0095715-Webb1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Webb</surname><given-names>GI</given-names></name> (<year>2000</year>) <article-title>MultiBoosting: A technique for combining boosting and wagging</article-title>. <source>Machine Learning</source> <volume>40</volume>: <fpage>159</fpage>–<lpage>196</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095715-John1"><label>24</label>
<mixed-citation publication-type="other" xlink:type="simple">John GH, Langley P (1995) Estimating continuous distributions in Bayesian classifiers. Eleventh Conference on Uncertainty in Artificial Intelligence: 338–345.</mixed-citation>
</ref>
<ref id="pone.0095715-Christofides1"><label>25</label>
<mixed-citation publication-type="other" xlink:type="simple">Christofides N (1975) Graph Theory: An Algorithmic Approach (Computer Science and Applied Mathematics). Orlando: Academic Press, Inc.</mixed-citation>
</ref>
<ref id="pone.0095715-Rennie1"><label>26</label>
<mixed-citation publication-type="other" xlink:type="simple">Rennie JDM, Shih L, Teevan J, Karger DR (2003) Tackling the poor assumptions of Naïve Bayes text classifiers. Proceeding of the Twentieth International Conference on Machine Learning: 616–623.</mixed-citation>
</ref>
<ref id="pone.0095715-Kohavi1"><label>27</label>
<mixed-citation publication-type="other" xlink:type="simple">Kohavi R (1995) The power of decision tables. 8th European Conference on Machine Learning: 174–189.</mixed-citation>
</ref>
<ref id="pone.0095715-Wilson1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname><given-names>DL</given-names></name> (<year>1972</year>) <article-title>Asymptotic properties of nearest neighbor rule using edited data</article-title>. <source>IEEE Transactions on Systems, Man, and Cybernetics</source> <volume>3</volume>: <fpage>408</fpage>–<lpage>421</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095715-Kohavi2"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kohavi</surname><given-names>R</given-names></name> (<year>1995</year>) <article-title>A study of cross-validation and bootstrap for accuracy estimation and model selection</article-title>. <source>Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence</source> <volume>2</volume>: <fpage>1137</fpage>–<lpage>1145</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095715-Wolpert1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wolpert</surname><given-names>DH</given-names></name>, <name name-style="western"><surname>Macready</surname><given-names>WG</given-names></name> (<year>1997</year>) <article-title>No free lunch theorems for optimization</article-title>. <source>IEEE Transactions on Evolutionary Computation</source> <volume>1</volume>: <fpage>67</fpage>–<lpage>82</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095715-Duda1"><label>31</label>
<mixed-citation publication-type="other" xlink:type="simple">Duda RO, Hart PE, Stork DG (2001) Pattern Classification. United State: Wiley.</mixed-citation>
</ref>
</ref-list></back>
</article>