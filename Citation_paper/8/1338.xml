<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-00190</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003157</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Review</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Biophysics</subject></subj-group><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject></subj-group></subj-group><subj-group><subject>Theoretical biology</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Physics</subject><subj-group><subject>Biophysics</subject></subj-group><subj-group><subject>Interdisciplinary physics</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Social and behavioral sciences</subject><subj-group><subject>Information science</subject><subj-group><subject>Information theory</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Information and Efficiency in the Nervous System—A Synthesis</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Sengupta</surname><given-names>Biswa</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Stemmler</surname><given-names>Martin B.</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>Karl J.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>The Wellcome Trust Centre for Neuroimaging, University College London, London, United Kingdom</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Centre for Neuroscience, Indian Institute of Science, Bangalore, India</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Bernstein Centre Munich, Institute of Neurobiology, Ludwig Maximilians Universität, München, Germany</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Sporns</surname><given-names>Olaf</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Indiana University, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">b.sengupta@ucl.ac.uk</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>7</month><year>2013</year></pub-date>
<pub-date pub-type="epub"><day>25</day><month>7</month><year>2013</year></pub-date>
<volume>9</volume>
<issue>7</issue>
<elocation-id>e1003157</elocation-id>
<history>
<date date-type="received"><day>3</day><month>2</month><year>2013</year></date>
<date date-type="accepted"><day>7</day><month>6</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2013</copyright-year>
<copyright-holder>Sengupta et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>In systems biology, questions concerning the molecular and cellular makeup of an organism are of utmost importance, especially when trying to understand how unreliable components—like genetic circuits, biochemical cascades, and ion channels, among others—enable reliable and adaptive behaviour. The repertoire and speed of biological computations are limited by thermodynamic or metabolic constraints: an example can be found in neurons, where fluctuations in biophysical states limit the information they can encode—with almost 20–60% of the total energy allocated for the brain used for signalling purposes, either via action potentials or by synaptic transmission. Here, we consider the imperatives for neurons to optimise computational and metabolic efficiency, wherein benefits and costs trade-off against each other in the context of self-organised and adaptive behaviour. In particular, we try to link information theoretic (variational) and thermodynamic (Helmholtz) free-energy formulations of neuronal processing and show how they are related in a fundamental way through a complexity minimisation lemma.</p>
</abstract>
<funding-group><funding-statement>This work is supported by a Wellcome Trust/DBT Early Career fellowship to BS. BS is also grateful to financial support obtained from the ESF, Boehringer Ingelheim Fonds, and the EMBO. MBS is supported via funding from the BMBF. KJF is supported by the Wellcome Trust. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="12"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>The design of engineered and biological systems is influenced by a balance between the energetic costs incurred by their operation and the benefits realised by energy expenditure. This balance is set via trade-offs among various factors, many of which act as constraints. In contrast to engineering systems, it has only been possible recently to experimentally manipulate biological systems—at a cellular level —to study the benefits and costs that interact to determine adaptive fitness <xref ref-type="bibr" rid="pcbi.1003157-Sutherland1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Alexander1">[2]</xref>. One such example is the nervous system, where metabolic energy consumption constrains the design of brains <xref ref-type="bibr" rid="pcbi.1003157-Niven1">[3]</xref>. In this review paper, we start by defining computation and information in thermodynamic terms and then look at neuronal computations via the free-energy principle. We then consider the efficiency of information processing in the nervous system and how the complexity of information processing and metabolic energy consumption act as constraints. The final section tries to integrate these perspectives: In brief, we will argue that the principle of maximum efficiency applies to both information processing and thermodynamics; such that—for a given level of accuracy—statistically and metabolically efficient brains will penalise the use of complex representations and associated commodities like energy.</p>
</sec><sec id="s2">
<title>Information Is Physical</title>
<p>A widely used term in neuroscience is “neuronal computation”; but what does computation mean? Simply put, any transformation of information can be regarded as computation, while the transfer of information from a source to a receiver is communication <xref ref-type="bibr" rid="pcbi.1003157-Feynman1">[4]</xref>. To understand the physical basis of computation, let us reconsider Feynman's example of a physical system whose information can be read out. The example is intentionally artificial, to keep the physics simple, but has a direct parallel to neuroscience, as we will show at the end. Consider a box that it is filled with an ideal gas containing <italic>N</italic> atoms. This occupies a volume <italic>V</italic><sub>1</sub>, in which we can ignore forces of attraction or repulsion between the particles. Now suppose that the answer to a question is “yes” if all <italic>N</italic> atoms are on the right-hand side of the box, and “no” if they are on the left. We could use a piston to achieve this. By compressing the gas into a smaller volume <italic>V</italic><sub>2</sub>, a piston performs the work<disp-formula id="pcbi.1003157.e001"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003157.e001" xlink:type="simple"/><label>(1)</label></disp-formula>Classical thermodynamics tells us that the pressure and volume of an ideal gas are linked such that<disp-formula id="pcbi.1003157.e002"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003157.e002" xlink:type="simple"/><label>(2)</label></disp-formula>where <italic>k</italic> is Boltzmann's constant and the temperature <italic>T</italic> is assumed constant. The work done on the gas is then:<disp-formula id="pcbi.1003157.e003"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003157.e003" xlink:type="simple"/><label>(3)</label></disp-formula>As we compress the gas, the atoms speed up and attain kinetic energy, hence heating the box. According to the conservation of energy, the work done on the gas is converted to heat. This heat is dissipated to the external environment to keep the temperature constant. This means that the internal energy <italic>U</italic> of all the particles remains unchanged, such that the work done by the system or change in <italic>Helmholtz free energy A</italic> = <italic>U</italic>–<italic>TS</italic> reduces to the change in thermodynamic entropy <italic>S</italic> = <italic>kH</italic>, where <italic>H</italic> is Shannon entropy:<disp-formula id="pcbi.1003157.e004"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003157.e004" xlink:type="simple"/><label>(4)</label></disp-formula>For a single gas particle, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003157.e005" xlink:type="simple"/></inline-formula> we find that Shannon entropy decreases by ln 2. This means that by compressing the gas there are fewer places that the particles can occupy and we are less uncertain about their whereabouts. In short, we have gained information. What have we learned from this exercise? To obtain information—in other words, to reduce entropy or average uncertainty —one has to perform work. More generally, Landauer's seminal work showed that energy is required when information is erased or deleted via irreversible operations <xref ref-type="bibr" rid="pcbi.1003157-Landauer1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Landauer2">[6]</xref>. In the context of noise or communication, the deletion of incorrect bits therefore requires the dissipation of energy. This dissipation is decreased at lower temperatures because of reduced thermal noise—lower temperatures facilitate a reduction of energy expenditure.</p>
<p>In the brain, volume changes are not the primary mode of conveying information. Instead, the compartments present in the brain, ranging from synaptic clefts to organelles, maintain a relatively constant volume over several seconds at least. What changes on a short time scale are the numbers of molecules, such as transmitters or ions, in these compartments. If we translate volumes to concentrations <italic>c<sub>i</sub></italic> = <italic>N</italic>/<italic>V<sub>i</sub></italic>, the change in entropy due to information transfer becomes<disp-formula id="pcbi.1003157.e006"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003157.e006" xlink:type="simple"/><label>(5)</label></disp-formula>The work is then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003157.e007" xlink:type="simple"/></inline-formula>. If the molecules are charged, the chemical potential sets up an electrical potential (called the Nernst potential), which is the basis for much of the signalling within the brain. For some molecules, such as Na<sup>+</sup> and K<sup>+</sup> ions, the concentration changes during electrical signalling are miniscule relative to the total concentrations of these molecules. By linearising <italic>δW</italic> in the concentration changes, we can easily compute the energetic cost of neuronal signals <xref ref-type="bibr" rid="pcbi.1003157-Stemmler1">[7]</xref>.</p>
<p>In the examples above, the system remains in thermodynamic equilibrium. Recent progress has been made in describing the relationship between Helmholtz free energy and work when the system is driven far from equilibrium—for example, if the gas was compressed quickly. In this more general setting, the Jarzynski equality states <xref ref-type="bibr" rid="pcbi.1003157-Jarzynski1">[8]</xref>:<disp-formula id="pcbi.1003157.e008"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003157.e008" xlink:type="simple"/><label>(6)</label></disp-formula>where the expectation <italic>E</italic>[·]is over an ensemble of paths from the initial to final states. Crucially, the change in Helmholtz free energy (and expected work) does not depend upon the path or the rate at which external parameters (like volume) change. Notice that <xref ref-type="disp-formula" rid="pcbi.1003157.e004">Equation 4</xref> is a special case of <xref ref-type="disp-formula" rid="pcbi.1003157.e008">Equation 6</xref>, when there is only one (infinitely slow) path.</p>
<sec id="s2a">
<title>Summary</title>
<p>In summary, changing the state of a system necessarily entails a change in Helmholtz free energy that is equivalent to the work done on the system. Under isothermal conditions, this changes the thermodynamic entropy, which can be regarded as the average uncertainty or information we have about the (microscopic) state of the system. So is this sufficient to establish the link between thermodynamic free energy and information processing? Not really: because the information here is about the (microscopic) state of the system in question. This does not speak to representational information of the sort associated with biological computations or communication: information of this sort reflects how one system represents another. In the next section, we consider a purely information theoretic perspective on computation that invokes free energy and entropy of a fundamentally different sort.</p>
</sec></sec><sec id="s3">
<title>The Free-Energy Principle</title>
<p><xref ref-type="disp-formula" rid="pcbi.1003157.e004">Equation 4</xref> shows how the basic laws of classical thermodynamics connect the Helmholtz free energy of a system to its entropy, where entropy corresponds to the disorder or average uncertainty about its state. In biological systems, there is a natural tendency to resist disorder—at multiple levels of organisation. The maintenance of sensory and physiological states within characteristic bounds is typical of biological systems and usually relies on some sort of regulatory process, i.e., homeostasis <xref ref-type="bibr" rid="pcbi.1003157-Ashby1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Nicolis1">[10]</xref>. Mathematically, this can be expressed by saying that the (sensory) states of biological systems have characteristically low Shannon entropy, where—under ergodic assumptions—Shannon entropy is (almost surely) the long-term average of self information or surprise (see below). An ergodic system has an invariant phase volume <xref ref-type="bibr" rid="pcbi.1003157-Birkhoff1">[11]</xref>, which is a necessary condition for an organism to exist—in the sense that it would otherwise transgress phase boundaries and cease to exist <xref ref-type="bibr" rid="pcbi.1003157-Friston1">[12]</xref>.</p>
<p>Here, the Shannon entropy plays the same role as thermodynamic entropy but measures the dispersion not over microstates of a thermodynamic (canonical) ensemble, but over some phase functions or macroscopic variables that change with time. These variables can take values that are relatively frequent (low surprise) or infrequent (high surprise). Shannon entropy reflects the average surprise of these variables as they fluctuate over time. By minimising the surprise associated with environmental fluctuations (sensory input), an organism can maintain its physiological states within bounds <xref ref-type="bibr" rid="pcbi.1003157-Friston2">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Friston3">[14]</xref>.</p>
<p>To evaluate surprise, biological systems need to infer the probability of each sensory fluctuation they encounter. In systems like the brain, these inferences need to be made in the blink of an eye. However, calculating the requisite probabilities can be an intricate and lengthy process, making such computations practically intractable. In 1972, the physicist Richard Feynman came up with a clever trick for calculating probabilities (approximately but very efficiently) using <italic>variational free energy</italic> <xref ref-type="bibr" rid="pcbi.1003157-Feynman2">[15]</xref>. The trick is to convert a difficult probability density integration problem into an easy optimisation problem by minimising a free energy bound on the quantity of interest—in our case, the surprise of sensory input. In brief, this entails adjusting probability distributions over the causes of sensory input until they minimise the free energy of sensory input. Notice that we have introduced the notion of causes or hidden states of the world that are responsible for generating sensory samples. Heuristically, this means the system or agent has a model of the world that it uses to evaluate the likelihood or surprise of a sensation. Mathematically, hidden states are fictive variables that are necessary to construct a variational free energy bound on surprise, as we will see next.</p>
<p>Let us assume that self-organising systems like the brain represent their environment probabilistically, in terms of hidden states that cause sensory input. For example, an agent might believe its visual sensations were caused by a bird flying across its field of view. These beliefs can be regarded as real-valued, time-dependent internal or representational states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003157.e009" xlink:type="simple"/></inline-formula>. These internal states encode a conditional probability density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003157.e010" xlink:type="simple"/></inline-formula> over hidden states in the world <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003157.e011" xlink:type="simple"/></inline-formula>—such as the motion, colour, and size of the bird. The objective is to minimise the surprise <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003157.e012" xlink:type="simple"/></inline-formula> of sensations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003157.e013" xlink:type="simple"/></inline-formula>. Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003157.e014" xlink:type="simple"/></inline-formula> denotes a model entailed by a system or an agent, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003157.e015" xlink:type="simple"/></inline-formula> is the probability of observing a particular state under that model. The model is effectively used to generate hypotheses that explain sensory input in terms of hidden states or representations—such as a bird in flight.</p>
<p>As noted above, minimising surprise directly is an intractable problem, so surprise is replaced with its variational free energy bound <xref ref-type="bibr" rid="pcbi.1003157-Feynman2">[15]</xref>. This free energy is a function of sensory and internal states and can now be minimised with respect to the internal states:<disp-formula id="pcbi.1003157.e016"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003157.e016" xlink:type="simple"/><label>(7)</label></disp-formula>Here, <italic>U</italic>(<italic>t</italic>) = −ln <italic>p</italic>(<italic>s</italic>(<italic>t</italic>), <italic>ψ</italic>(<italic>t</italic>)|<italic>m</italic>) corresponds to an internal energy under a generative model of the world, described in terms of the density over sensory and hidden states <italic>p</italic>(<italic>s</italic>,<italic>ψ</italic>|<italic>m</italic>). In <xref ref-type="disp-formula" rid="pcbi.1003157.e016">Equation 7</xref> and throughout <italic>H</italic>[<italic>p</italic>] = <italic>E<sub>p</sub></italic>[−ln <italic>p</italic>] denotes the entropy of a probability distribution. Comparison with <xref ref-type="disp-formula" rid="pcbi.1003157.e004">Equation 4</xref> explains why <italic>F</italic>(<italic>t</italic>) is called free energy—by analogy with its thermodynamic homologue that is defined as internal energy minus entropy. However, it is important to note that variational free energy is not the Helmholtz free energy in <xref ref-type="disp-formula" rid="pcbi.1003157.e004">Equation 4</xref>—it is a functional of a probability distribution over hidden (fictive) states <italic>encoded by</italic> internal states <italic>q</italic>(<italic>ψ</italic>|<italic>μ</italic>), not the probability distribution over the (physical) internal states. This is why variational free energy pertains to information about hidden states that are represented, not the internal states that represent them. In other words, the variational free energy measures the information represented by internal states, not internal states <italic>per se</italic>. Later, we will try to establish the link between variational and Helmholtz free energies. First, we consider the computational implications of minimising variational free energy.</p>
<p>In short, free energy finesses the evaluation of surprise—where an agent can evaluate free energy fairly easily, given the internal energy or a generative model of its environment. The second equality in <xref ref-type="disp-formula" rid="pcbi.1003157.e016">Equation 7</xref> says that free energy is always greater than surprise, because the second term (Kullback-Leibler divergence) is nonnegative. This means that when free energy is minimised with respect to the internal states, free energy approximates surprise and the conditional density approximates the posterior density over hidden states:<disp-formula id="pcbi.1003157.e017"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003157.e017" xlink:type="simple"/><label>(8)</label></disp-formula>This is known as approximate Bayesian inference, which becomes exact when the conditional and posterior densities have the same form <xref ref-type="bibr" rid="pcbi.1003157-Beal1">[16]</xref>. Intuitively, minimising free energy renders the conditional density the true posterior density over hidden states, where both are informed by—or conditioned on—sensory information. In Bayesian parlance, a posterior density describes a belief after sampling some data—in contrast to a prior belief that existed before the data were available. Minimising variational free energy can therefore be regarded using sensory evidence to update prior beliefs to approximate posterior beliefs.</p>
<p>How can we place a concept like variational free energy in the context of neuronal computation? This has a long history—originating in Geoffrey Hinton <xref ref-type="bibr" rid="pcbi.1003157-Hinton1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Hinton2">[18]</xref> and Douglas Hofstadter's <xref ref-type="bibr" rid="pcbi.1003157-Hofstadter1">[19]</xref> work using Ising models for inference in artificial neural networks. Hinton and colleagues realised that variational free energy was mathematically equivalent to the cost function for inference in a neural network, such as a Hopfield model <xref ref-type="bibr" rid="pcbi.1003157-Hopfield1">[20]</xref>—the difference between the prediction made by the neural network and what it actually produced as an output, i.e., the prediction error. These ideas were subsequently absorbed into the free-energy principle <xref ref-type="bibr" rid="pcbi.1003157-Friston4">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Friston5">[22]</xref>, whose key insight was that to reduce the entropy of sensations, the system had to act on the environment. The solution is to assume that both the internal states of the system and its action minimise variational free energy (and implicitly surprise). This dual minimisation maps nicely onto perception and action, where variational free energy can be reduced by optimising internal (representational) states or sensory states through active sensory sampling. This is known as active inference and essentially compels organisms to selectively sample what they expect to sample.</p>
<p>Under certain statistical assumptions, free energy is essentially the difference between the agent's predictions and the actual sensations sampled <xref ref-type="bibr" rid="pcbi.1003157-Friston5">[22]</xref>. Therefore, minimising the free energy is equivalent to reducing prediction error and hence surprise <xref ref-type="bibr" rid="pcbi.1003157-Friston3">[14]</xref>. To minimise free energy or prediction error, the brain can either change its prediction to match sensory input or it can change what it samples to match its predictions <xref ref-type="bibr" rid="pcbi.1003157-Friston4">[21]</xref>. This suggests that the brain is continually making predictions and reevaluating them by comparing inputs with internal predictions to make sense of the world. Is there any empirical evidence that this scheme operates in the nervous system?</p>
<p>Volunteers in a magnetic resonance imaging (MRI) scanner watched two sets of moving dots—one random and the other moving coherently. They showed patterns of distributed brain activation that could only be explained in terms of top-down predictions from deep in the brain to visual centres in the occipital cortex. In other words, top-down predictions from the extrastriate cortex appeared to suppress prediction errors in the striate cortex <xref ref-type="bibr" rid="pcbi.1003157-Harrison1">[23]</xref>. Assuming the visual system is a hierarchy of cortical areas, such predictive coding enables predictions about hidden states of the world—like coherent motion—to influence processing at lower levels <xref ref-type="bibr" rid="pcbi.1003157-Harrison1">[23]</xref>. Similarly, in the auditory cortex, electroencephalographic signals from higher processing centres change brain activity in lower areas <xref ref-type="bibr" rid="pcbi.1003157-Garrido1">[24]</xref>. Using dynamic causal modelling, Garrido <italic>et al.</italic> <xref ref-type="bibr" rid="pcbi.1003157-Garrido1">[24]</xref> found that models with top-down connections explained empirical electrophysiological data far better than the models with only bottom-up connections. Garrido <italic>et al.</italic> <xref ref-type="bibr" rid="pcbi.1003157-Garrido1">[24]</xref> argued that these neuronal responses were consistent with the brain's attempt to conciliate predictions at one level with those in other levels—in other words, to reduce hierarchical prediction error.</p>
<p>What sort of neuronal architectures mediate this prediction error minimisation—or predictive coding? In mammalian brains, cortical areas are organised hierarchically <xref ref-type="bibr" rid="pcbi.1003157-Zeki1">[25]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Felleman1">[26]</xref>, wherein populations of neurons can encode expected states of the world and provide top-down predictions to lower or sensory levels <xref ref-type="bibr" rid="pcbi.1003157-Kawato1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Srinivasan1">[28]</xref>. For example, top-down connections from pyramidal neurons in the deeper layers of the cortex are thought to provide predictions to superficial pyramidal populations of a lower area. This enables forward connections from superficial pyramidal neurons to convey prediction errors, creating recurrent dynamics that suppress prediction errors at each level of the cortical hierarchy <xref ref-type="bibr" rid="pcbi.1003157-Mumford1">[29]</xref>–<xref ref-type="bibr" rid="pcbi.1003157-Rao1">[31]</xref>. The precision of these errors can be modulated by neuromodulation <xref ref-type="bibr" rid="pcbi.1003157-Yu1">[32]</xref>. Such rescaling of prediction errors in proportion to their precision is simply a form of gain control <xref ref-type="bibr" rid="pcbi.1003157-Friston6">[33]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Abbott1">[34]</xref> and may mediate attention. In short, the wetware necessary to minimise free energy appears to be available and is remarkably consistent with its known functional anatomy.</p>
<p>In summary, biological organisms are open self-organising systems that operate far from thermodynamic equilibrium <xref ref-type="bibr" rid="pcbi.1003157-Prigogine1">[35]</xref>. The free-energy principle suggests that organisms avoid phase transitions by minimising (a variational free energy bound on) the Shannon entropy of their sensory states. But how does one reconcile the need of an animal to survive (by avoiding phase transitions) with its innate tendency to forage or explore? This apparent paradox is resolved by noting that active inference is driven by prior beliefs—and these beliefs can entail exploration. In other words, agents expect to explore and would be surprised if they did not. We will return to the central role of priors in the last section.</p>
<sec id="s3a">
<title>Summary</title>
<p>Perception minimises prediction error by optimising synaptic activity (perceptual inference), synaptic efficacy (learning and memory), and synaptic gain (attention and salience) <xref ref-type="bibr" rid="pcbi.1003157-Friston3">[14]</xref>. In doing so, we form an optimal representation of the sensorium. Such strategies of optimisation are mathematically equivalent to predictive coding <xref ref-type="bibr" rid="pcbi.1003157-Elias1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Elias2">[37]</xref> or, as we will see later, maximising the mutual information between sensations and the responses they evoke <xref ref-type="bibr" rid="pcbi.1003157-Stemmler2">[38]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Bell1">[39]</xref>. In the embodied context of action on the environment, free-energy minimisation can also explain active inference in the exteroceptive domain <xref ref-type="bibr" rid="pcbi.1003157-Friston7">[40]</xref> and homoeostasis through minimising interoceptive prediction errors. In short, the idea of free-energy minimisation, stemming from Feynman's beautiful piece of mathematics, allows us to consider perception and action under a general framework—and produce testable hypotheses.</p>
</sec></sec><sec id="s4">
<title>Information Efficiency</title>
<p>In the previous section, we described how variational free energy is intricately linked to surprise—the free-energy principle tells us that an organism should strive to reduce its prediction error thereby reducing free energy. The connection between free energy and information—although obvious—is seldom commented upon (see Table 1 in <xref ref-type="bibr" rid="pcbi.1003157-Peleg1">[41]</xref>). To minimise free energy, the expected prediction error has to be minimised while, at the same time, the entropy of the conditional density is maximised. This is slightly paradoxical because the purpose of free-energy minimisation is to reduce sensory entropy. However, <xref ref-type="disp-formula" rid="pcbi.1003157.e016">Equation 7</xref> shows that if the entropy of sensory states <italic>H</italic>[<italic>p</italic>(<italic>s</italic>|<italic>m</italic>)] is minimised vicariously by minimising free energy over time, then the entropy of the conditional density <italic>H</italic>[<italic>q</italic>(<italic>ψ</italic>|<italic>μ</italic>)]must be maximised at each point in time. This follows from a need to balance accuracy and complexity of the sort seen in Occam's razor. We will return to this in a later section in the context of the principle of maximum entropy <xref ref-type="bibr" rid="pcbi.1003157-Jaynes1">[42]</xref>. In this section, we focus on information theory as a way of describing the quality of representations and the constraints under which these representations are formed.</p>
<p>We know that all animals process and transmit information to survive and reproduce in an uncertain environment. A principled way to understand such signal processing was absent until Claude Shannon's seminal work on information theory <xref ref-type="bibr" rid="pcbi.1003157-Shannon1">[43]</xref>. To understand how messages can be transferred efficiently via telegraphic wires, Shannon derived powerful formalisms that provided fundamental limits on communication <xref ref-type="bibr" rid="pcbi.1003157-Shannon1">[43]</xref>. On one hand, information theory allowed optimisation of complicated devices like satellite communication systems. On the other hand, it fitted comfortably with the bounds established by thermodynamics <xref ref-type="bibr" rid="pcbi.1003157-Jaynes2">[44]</xref>. Some years after its inception, biologists used information theory to study the efficiency of processing in the nervous system. It was realised that efficient representations were permitted by statistical regularities in the sensorium, i.e., hidden states and their sensory consequences that have low entropy (see <xref ref-type="bibr" rid="pcbi.1003157-Atick1">[45]</xref>). However, the influence of random fluctuations and other constraints prohibit a completely efficient encoding of hidden states in the world.</p>
<p>In the nervous system, limited bandwidth and dynamic range create an information bottleneck due to the limited response ranges of the neurons in sensory epithelia <xref ref-type="bibr" rid="pcbi.1003157-Barlow1">[46]</xref>–<xref ref-type="bibr" rid="pcbi.1003157-Barlow3">[48]</xref>. Atick <xref ref-type="bibr" rid="pcbi.1003157-Atick1">[45]</xref> suggests that these bottlenecks can also result from computational limitations at higher levels of sensory processing—citing as an example the “attention bottleneck,” where there is constriction of information processing—in bits per unit time—somewhere between area V4 and the inferotemporal cortex. In brief, sensory receptors are required to compress an enormous range of statistically redundant sensory data into their limited range. One way to achieve this is by compression —imagine an architect's plan of your office. This does not include the dimensions of every brick, just the information necessary to build the office. It has been proposed that sensory systems also apply the principle of compression. They sieve redundant information, such that only information that is necessary to encode hidden states is retained <xref ref-type="bibr" rid="pcbi.1003157-Barlow1">[46]</xref>—in engineering this is called a factorial code. Of course there are many ways to describe such sensory encoding. Others include but are not restricted to feature detection, filtering, etc. Among these, schemes like linear predictive coding and minimum description length formulations have a particularly close and formal relationship with variational formulations.</p>
<p>Sensory receptors (mechanoreceptors, photoreceptors, and the like) are thought to build a factorial representation of the world—such that only independent bits of information are sampled (<xref ref-type="fig" rid="pcbi-1003157-g001">Figure 1</xref>). Interestingly, this has been observed in the large monopolar cells (LMC) in the blowfly compound eye <xref ref-type="bibr" rid="pcbi.1003157-Laughlin1">[49]</xref>. Laughlin <xref ref-type="bibr" rid="pcbi.1003157-Laughlin1">[49]</xref> measured the distribution of the fly's natural environment from horizontal scans of dry woodland and lake-side vegetation and quantified the responses of light-adapted LMCs. Laughlin <xref ref-type="bibr" rid="pcbi.1003157-Laughlin1">[49]</xref> found that the LMC—known to respond to contrast signals—is most sensitive around the most probable input contrast—with sensitivity dropping to zero as the input became more improbable.</p>
<fig id="pcbi-1003157-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003157.g001</object-id><label>Figure 1</label><caption>
<title>Redundancy reduction.</title>
<p>The sensory environment of an animal is highly correlated (redundant). The animal's job is to map such signals as efficiently as possible to its neuronal representations, which are limited by their dynamic range. One way to solve this problem rests on de-correlating the input to provide a minimum entropy description, followed by a gain controller. This form of sensory processing has been observed in the experiments by Laughlin <xref ref-type="bibr" rid="pcbi.1003157-Laughlin1">[49]</xref>, where the circuit maps the de-correlated signal via its cumulative probability distribution to a neuronal response, thereby avoiding saturation. Modified from <xref ref-type="bibr" rid="pcbi.1003157-Atick1">[45]</xref>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003157.g001" position="float" xlink:type="simple"/></fig>
<p>The application of information theory to the nervous system is formally pleasing and has provided some compelling insights. However, it does have limits <xref ref-type="bibr" rid="pcbi.1003157-Johnson1">[50]</xref>: although it allows one to quantify the transmission of information, it has no notion of semantics. It only cares about how much information is present but not about what that information represents. A widely used information theoretic metric in neuroscience is the mutual information, which measures how much a random variable tells us about another random variable <xref ref-type="bibr" rid="pcbi.1003157-Papoulis1">[51]</xref>. If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003157.e018" xlink:type="simple"/></inline-formula> is a stimulus and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003157.e019" xlink:type="simple"/></inline-formula> is the representational response, the mutual information is defined as:<disp-formula id="pcbi.1003157.e020"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003157.e020" xlink:type="simple"/><label>(9)</label></disp-formula>Note that the joint density <italic>p</italic>(<italic>s</italic>,<italic>μ</italic>|<italic>m</italic>) is not the generative model <italic>p</italic>(<italic>s</italic>,<italic>ψ</italic>|<italic>m</italic>) of the previous section—it describes the joint distribution of sensory and internal states, not the joint distribution of sensory and hidden states. <xref ref-type="disp-formula" rid="pcbi.1003157.e020">Equation 9</xref> simply describes the divergence or relative entropy between the joint density and the product of its marginals. The mutual information is zero when the neuronal representation is statistically independent of the stimulus and is equal to the entropy of the stimulus when the representation faithfully encodes the stimulus. Since the mutual information must lie between zero and channel capacity, it is only the channel capacity that limits the information transfer between stimulus and neuronal response.</p>
<p>Estimating channel capacity by maximising empirical estimates of mutual information can be a difficult task, especially when the experimenter has only an informed guess about the stimuli that evoke responses. One way to finesse this problem is to use adaptive sampling of inputs, which hones in on stimuli that are maximally informative about observed responses <xref ref-type="bibr" rid="pcbi.1003157-Benda1">[52]</xref>. Assuming one knows the stimuli to use, the next problem is the curse of dimensionality. In other words, one requires an enormous amount of data to estimate the probability densities required to quantify mutual information. Although, sophisticated machine learning tools try to estimate mutual information from limited data <xref ref-type="bibr" rid="pcbi.1003157-Treves1">[53]</xref>–<xref ref-type="bibr" rid="pcbi.1003157-Paninski2">[55]</xref>, the numerics of mutual information are fraught with difficulties.</p>
<sec id="s4a">
<title>Summary</title>
<p>Irrespective of the thermodynamic or computational imperatives for a biological system, the simple observation that there should be some statistical dependency between sensory samples and the internal states that encode them means that sensory and internal states should have a high mutual information. This leads to the principles of maximum information transfer (a.k.a. infomax) and related principles of minimum redundancy and maximum efficiency <xref ref-type="bibr" rid="pcbi.1003157-Barlow1">[46]</xref>–<xref ref-type="bibr" rid="pcbi.1003157-Barlow3">[48]</xref>. Later, we will see how minimising variational free energy maximises mutual information and what this implies for metabolic costs in terms of Helmholtz free energy. First, we will briefly review the biophysical and metabolic constraints on the information processing that underlies active inference.</p>
</sec></sec><sec id="s5">
<title>Is Inference Costly?</title>
<p>Hitherto, we have considered the strategies that neurons might use for abstracting information from the sensorium. A reliable representation is necessary for an animal to make decisions and act. Such information processing comes at a price, irrespective of whether the animal is at rest or not <xref ref-type="bibr" rid="pcbi.1003157-Ames1">[56]</xref>. Cellular respiration enables an organism to liberate the energy stored in the chemical bonds of glucose (via pyruvate)—the energy in glucose is used to produce ATP. Approximately 90% of mammalian oxygen consumption is mitochondrial, of which approximately 20% is uncoupled by the mitochondrial proton leak and 80% is coupled to ATP synthesis <xref ref-type="bibr" rid="pcbi.1003157-Rolfe1">[57]</xref>. Cells use ATP for cellular maintenance and signalling purposes, via ion channels that use ATP hydrolysis to transport protons against the electromotive force. Given that the biophysical “cash-register” of a cell (the ATPases) can only handle ATP—and not glucose—we will discuss brain metabolism in terms of ATP.</p>
<p>In man, the brain constitutes just 2% of the body mass, while consuming approximately 20% of the body's energy expenditure for housekeeping functions like protein synthesis, maintenance of membrane potentials, etc. <xref ref-type="bibr" rid="pcbi.1003157-Clarke1">[58]</xref>. What consumes such remarkable amounts of energy? Assuming a mean action potential (AP) rate of 4 Hz, a comprehensive breakdown of signalling costs suggests that action potentials use around 47% of the energy consumed—mainly to drive the Na<sup>+</sup>/K<sup>+</sup> pump (<xref ref-type="fig" rid="pcbi-1003157-g002">Figure 2</xref>) <xref ref-type="bibr" rid="pcbi.1003157-Attwell1">[59]</xref>. This pump actively pumps Na<sup>+</sup> ions out of the neuron and K<sup>+</sup> ions inside <xref ref-type="bibr" rid="pcbi.1003157-Skou1">[60]</xref>. In doing so, the pump consumes a single ATP molecule for transporting three Na<sup>+</sup> ions out and two K<sup>+</sup> ions in <xref ref-type="bibr" rid="pcbi.1003157-Skou2">[61]</xref>–<xref ref-type="bibr" rid="pcbi.1003157-Sen1">[63]</xref>. Measurements of ATP consumption from intracellular recordings in fly photoreceptors show similar energy consumption to costs obtained from whole retina oxygen consumption <xref ref-type="bibr" rid="pcbi.1003157-Pangrsic1">[64]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Niven2">[65]</xref>. Indeed, in the absence of signalling, the dominant cost of maintaining the resting potential is attributable to the Na<sup>+</sup>/K<sup>+</sup> pump. Attwell and Laughlin <xref ref-type="bibr" rid="pcbi.1003157-Attwell1">[59]</xref> further estimated that out of 3.29×10<sup>9</sup> ATP/s consumed by a neuron with a mean firing rate of 4 Hz, 47% was distributed for producing APs, while postsynaptic receptors accounted for around 40% of the energy consumption (<xref ref-type="fig" rid="pcbi-1003157-g002">Figure 2</xref>). These figures suggest that action potentials and synapses are the main consumers of energy and that they determine the energy cost in the nervous system.</p>
<fig id="pcbi-1003157-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003157.g002</object-id><label>Figure 2</label><caption>
<title>Attwell and Laughlin's energy budget.</title>
<p>Energy use by various neuronal (cellular) processes that produce, on average, 4 spikes per second. Modified from <xref ref-type="bibr" rid="pcbi.1003157-Attwell1">[59]</xref>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003157.g002" position="float" xlink:type="simple"/></fig>
<p>Experimental studies have shown that neuronal performance is related to energy consumption, both during rest and while signalling <xref ref-type="bibr" rid="pcbi.1003157-Niven2">[65]</xref>. What these studies show is obvious—there is no free lunch. Neurons have to invest metabolic energy to process information. The finite availability of ATP and the heavy demand of neuronal activity suggest neuronal processing has enjoyed great selective pressure. Metabolic energy costs limit not only the possible behavioural repertoire but also the structure and function of many organs, including the brain <xref ref-type="bibr" rid="pcbi.1003157-Niven1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Laughlin2">[66]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Laughlin3">[67]</xref>. The nervous system can use many tricks to promote energy efficiency. Neurons that use sparse (or factorial) codes for communication <xref ref-type="bibr" rid="pcbi.1003157-Barlow3">[48]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Levy1">[68]</xref> save on the number of action potentials required to encode information, or have topographical connectivity schemes to reduce the surface area of axons connecting different brain areas <xref ref-type="bibr" rid="pcbi.1003157-vonderMalsburg1">[69]</xref>–<xref ref-type="bibr" rid="pcbi.1003157-Bullmore2">[71]</xref>. Neurons may also alter their receptor characteristics to match the probability of inputs to form a matched filter <xref ref-type="bibr" rid="pcbi.1003157-Laughlin1">[49]</xref>. Alternatively, specialised signal processing could be employed to convert signals from analogue representation to pulsatile—prohibiting accumulation of noise during information transfer <xref ref-type="bibr" rid="pcbi.1003157-Sarpeshkar1">[72]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Sarpeshkar2">[73]</xref>.</p>
<p>In short, nature can use various means to achieve the objective of energy efficiency—see <xref ref-type="sec" rid="pcbi-1003157-box001">Box 1</xref> for a summary of some strategies. Energy consumption in single neurons depends on the types and the numbers of ion-channels expressed on the lipid bilayer, their kinetics, the cell's size, and the external milieu that changes the equilibrium conditions of the cell. Experimental measures from the blowfly retina show that metabolic efficiency in graded potentials (lacking voltage-gated Na<sup>+</sup> channels) is at least as expensive as in those neurons displaying action potentials—with the former capable of higher transmission rates <xref ref-type="bibr" rid="pcbi.1003157-Laughlin4">[74]</xref>. Similarly, in <italic>Drosophila melanogaster</italic> photoreceptors, absence of Shaker K<sup>+</sup> conductance increases energetic costs by almost two-fold <xref ref-type="bibr" rid="pcbi.1003157-Niven3">[75]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Niven4">[76]</xref>. It has also been suggested that the precise mix of synaptic receptors (AMPA, NMDA, mGlu, Kainate, etc.)—that determine synaptic time constants—influences the energetic cost of the single neuron <xref ref-type="bibr" rid="pcbi.1003157-Attwell2">[77]</xref>. Recent evidence indicates that the biophysical properties generating an action potential can be matched to make them energy efficient <xref ref-type="bibr" rid="pcbi.1003157-Carter1">[78]</xref>–<xref ref-type="bibr" rid="pcbi.1003157-Alle1">[81]</xref>. Fast Na<sup>+</sup> current decay and delayed K<sup>+</sup> current onset during APs in nonmyelinated mossy fibres in the rat hippocampus minimise the overlap between the inward and outward currents, resulting in a reduction of metabolic costs <xref ref-type="bibr" rid="pcbi.1003157-Alle1">[81]</xref>. Similarly, incomplete Na<sup>+</sup> channel inactivation in fast-spiking GABAergic neurons during the falling phase of the AP reduces metabolic efficiency of these neurons <xref ref-type="bibr" rid="pcbi.1003157-Carter1">[78]</xref>. Applying numerical optimisation to published data from a disparate range of APs, Sengupta <italic>et al.</italic> <xref ref-type="bibr" rid="pcbi.1003157-Sengupta1">[80]</xref> showed that there is no direct relationship between size and shape of APs and their energy consumption. This study further established that the temporal profile of the currents underlying APs of some mammalian neurons are nearly perfectly matched to the optimised properties of ionic conductances, so as to minimise the ATP cost. All of these studies show that experimentally measured APs are in fact more efficient than suggested by the previous estimates of Attwell and Laughlin <xref ref-type="bibr" rid="pcbi.1003157-Attwell1">[59]</xref>. This was because until 2001 experimental measurements of membrane currents were scant, impeding the study of the overlap between Na<sup>+</sup> and K<sup>+</sup> currents. The effects of energy-efficient APs on cortical processing were gauged by recalculating Attwell and Laughlin's (2001) estimates by first using the overlap factor of 1.2—found in mouse cortical pyramidal cells—and then assuming the probability that a synaptic bouton releases a vesicle in response to an incoming spike remains at 0.25 <xref ref-type="bibr" rid="pcbi.1003157-Sengupta1">[80]</xref>. Neurons that are 80% efficient have two notable effects (<xref ref-type="fig" rid="pcbi-1003157-g003">Figure 3</xref>). First of all, the specific metabolic rate of the cortical grey matter increases by 60%, and second, the balance of expenditure shifts from action potentials to synapses (<xref ref-type="fig" rid="pcbi-1003157-g003">Figure 3</xref>, <italic>cf.</italic> <xref ref-type="fig" rid="pcbi-1003157-g002">Figure 2</xref>) <xref ref-type="bibr" rid="pcbi.1003157-Sengupta1">[80]</xref>.</p>
<fig id="pcbi-1003157-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003157.g003</object-id><label>Figure 3</label><caption>
<title>A revised energy budget for signalling in the grey matter of the rat brain.</title>
<p>Incorporating the increased efficiency of APs in mammalian neurons into Attwell and Laughlin's (<xref ref-type="fig" rid="pcbi-1003157-g002">Figure 2</xref>) original energy budget—for grey matter in the rat brain—reduces the proportion of the energy budget consumed by APs. Modified from <xref ref-type="bibr" rid="pcbi.1003157-Sengupta1">[80]</xref>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003157.g003" position="float" xlink:type="simple"/></fig><boxed-text id="pcbi-1003157-box001" position="float"><sec id="s5a1">
<title>Box 1. Some principles of computational anatomy</title>
<p><bold>Dimensionality reduction:</bold> Sensory input is high dimensional—a visual scene comprises differences in brightness, colours, numbers of edges, etc. If the retina did not preprocess this visual information, we would have to handle around 36 Gb/s of broadband information, instead of 20 Mb/s of useful data <xref ref-type="bibr" rid="pcbi.1003157-Sarpeshkar2">[73]</xref>. Preprocessing increases the metabolic efficiency of the brain by about 1,500 times. The requisite dimensionality reduction is closely related to minimising complexity—it is self-evident that internal representations or models of the sensorium that use a small number of dimensions or hidden states will have a lower complexity and incur smaller metabolic costs.</p>
<p><bold>Energy-efficient signalling:</bold> Action potentials (APs) are expensive commodities, whether they are used for local computation or long-distance communication <xref ref-type="bibr" rid="pcbi.1003157-Attwell1">[59]</xref>. Energy-efficient APs are characterised by Na<sup>+</sup> channel inactivation, voltage-dependent channel kinetics, and corporative K<sup>+</sup> channels—as described by multiple gating currents, inward-rectifying K<sup>+</sup> channels, and high channel densities <xref ref-type="bibr" rid="pcbi.1003157-Stemmler1">[7]</xref>. These biophysical innovations enable a neuron to produce efficient APs that use the minimal currents necessary to generate a given depolarisation.</p>
<p><bold>Component size and numbers:</bold> Action potentials travel considerable distances along densely packed axons, collaterals, and dendrites. The capacitance that must be charged by APs increases with membrane area <xref ref-type="bibr" rid="pcbi.1003157-Sengupta2">[101]</xref>, constraining the number and length of neuronal processes. It is fairly straightforward to show that—to maintain information transfer—the optimal solution is to decrease the number of components. Assuming all neurons have the same thresholds and energy consumption, the energy-efficient solution is to minimise the number of components, under computational constraints dictated by the ecological niche of the animal <xref ref-type="bibr" rid="pcbi.1003157-Sengupta2">[101]</xref>.</p>
<p><bold>Modular design:</bold> Very-large-scale integration circuits suggest an isometric scaling relation between the number of processing elements and the number of connections (Rent's rule <xref ref-type="bibr" rid="pcbi.1003157-Christie1">[102]</xref>). Neuronal networks have been shown to obey Rent's rule, exhibiting hierarchical modularity that optimises a trade-off between physical cost and topological complexity—wherein these networks are cost-efficiently wired <xref ref-type="bibr" rid="pcbi.1003157-Bassett1">[103]</xref>. A modular design balances the savings in metabolic costs, while preserving computational capacities. Hierarchical modularity also emerges under predictive coding <xref ref-type="bibr" rid="pcbi.1003157-Friston6">[33]</xref>. In this context, the brain becomes a model of its environment, which through the separation of temporal scales necessarily requires a hierarchical connectivity.</p>
<p><bold>Parallel architecture:</bold> The brain processes information in parallel—be it frequency analysis in the inner ear or analysing different attributes of a visual scene using functional segregation. This parallel architecture mirrors those used in modern-day microprocessors. For example, a fast single-core microprocessor may consume 5 Watts and execute a program in 10 seconds. If we bring together two single cores, power will double and execution time will halve, still consuming 50 Joules. Alternatively, a slow double-core microprocessor that expends 2.5 Watts of power to execute the program in 15 seconds could consume only 7.5 Joules. This energy saving works because power is proportional to frequency cubed; therefore, halving the frequency reduces the speed by two but conserves eight times the power, making the microprocessor four times as efficient. In short, if parallel architectures are combined with slow computing speeds, the resulting system is energetically more efficient.</p>
<p><bold>Analogue versus digital:</bold> If analogue computing is so efficient <xref ref-type="bibr" rid="pcbi.1003157-Sengupta3">[104]</xref>, why don't neurons operate on an all analogue basis? The obvious answer is signal processing in the digital (such as AP) domain enables noise suppression. Noise accumulation in analogue systems <xref ref-type="bibr" rid="pcbi.1003157-Sarpeshkar2">[73]</xref> speaks to hybrid processing—the use of analogue preprocessing before optimal digitisation. APs are useful in this context because they have an inbuilt threshold mechanism that attenuates noise. If a presynaptic signal is encoded as an AP and transmitted, there is hardly any conduction loss, thereby enabling a reliable transfer of information.</p>
</sec></boxed-text>
<p>The principle of energy efficiency is not just linked to single neurons. Energy budgets have been calculated for the cortex <xref ref-type="bibr" rid="pcbi.1003157-Lennie1">[82]</xref>, olfactory glomerulus <xref ref-type="bibr" rid="pcbi.1003157-Nawroth1">[83]</xref>, rod photoreceptors <xref ref-type="bibr" rid="pcbi.1003157-Okawa1">[84]</xref>, cerebellum <xref ref-type="bibr" rid="pcbi.1003157-Howarth1">[85]</xref>, and CNS white matter <xref ref-type="bibr" rid="pcbi.1003157-Harris1">[86]</xref>, among others. These studies highlight the fact that the movement of ions across the cell membrane is a dominant cost, defined by the numbers and cellular makeup of the neurons and the proportion of synaptic machinery embedded in the cell membrane (<xref ref-type="fig" rid="pcbi-1003157-g004">Figure 4</xref>). Niven and Laughlin <xref ref-type="bibr" rid="pcbi.1003157-Niven1">[3]</xref> have argued that when signalling costs are high and resting costs are low, representations will be sparse; such that neurons in a population preferentially represent single nonoverlapping events (also see <xref ref-type="bibr" rid="pcbi.1003157-Attneave1">[87]</xref>). Similarly, when resting costs are high and signalling costs are low, the nervous system will favour the formation of denser codes, where greater numbers of neurons within the population are necessary to represent events <xref ref-type="bibr" rid="pcbi.1003157-Niven1">[3]</xref>.</p>
<fig id="pcbi-1003157-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003157.g004</object-id><label>Figure 4</label><caption>
<title>Elements defining metabolic efficiency.</title>
<p>Speed and precision defines the representational capacity of a neuron. Speed or bandwidth is dependent on the membrane time constant and/or the spike rate of the neuron, while precision relies mainly on the types, numbers, and kinetics of synapses and the channels, neuron volume, etc. An efficient brain will maximise speed and precision under energetic constraints.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003157.g004" position="float" xlink:type="simple"/></fig>
<p>Experimental studies of mammalian cortex suggest that the cortex organises itself to minimise total wiring length, while maximising various connectivity metrics <xref ref-type="bibr" rid="pcbi.1003157-Chklovskii1">[88]</xref>. Minimising wiring lengths decreases the surface area of neuronal processes, reducing the energy required for charging the capacitive cell membrane—to sustain and propagate action potentials. In fact, theoretical analyses in pyramidal and Purkinje cells have shown that the dimensions and branching structure of dendritic arbours in these neurons can be explained by minimising the dendritic cost for a potential synaptic connectivity <xref ref-type="bibr" rid="pcbi.1003157-Wen1">[89]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Wen2">[90]</xref>. This can result from increasing the repertoire of possible connectivity patterns among different dendrites, while keeping the metabolic cost low <xref ref-type="bibr" rid="pcbi.1003157-Wen1">[89]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Wen2">[90]</xref>.</p>
<sec id="s5b">
<title>Summary</title>
<p>In summary, we have reviewed several lines of evidence that evolution tries to minimise metabolic costs, where—in the brain—these costs are primarily incurred by the restoration of transmembrane potentials, whose fluctuations encode or represent hidden states of the world. This raises a question: is energy the only constraint in the evolution of animals? Of course not—functional constraints like reliability, speed, precision, etc. <xref ref-type="bibr" rid="pcbi.1003157-Laughlin3">[67]</xref> and structural constraints like optimal wiring <xref ref-type="bibr" rid="pcbi.1003157-Chen1">[91]</xref> are equally important. For example, a single action potential in the squid giant axon consumes orders of magnitude more energy than a hippocampal or a pyramidal neuron, yet evolution has invested that extra Joule to buy speed <xref ref-type="bibr" rid="pcbi.1003157-Sengupta1">[80]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Crotty1">[92]</xref>. In short, structure and function interact to determine the fitness of an animal. Having surveyed the key metabolic constraints under which neuronal processing must proceed, we now try to integrate the information theoretic and metabolic perspectives.</p>
</sec></sec><sec id="s6">
<title>Thermodynamic Efficiency and Free-Energy Minimisation</title>
<p>In this section, we gather together the imperatives for biological self-organisation reviewed above. We hope to show that minimising variational free energy necessarily entails a metabolically efficient encoding that is consistent with the principles of minimum redundancy and maximum information transfer. In brief, we will show that maximising mutual information and minimising metabolic costs are two sides of the same coin: by decomposing variational free energy into accuracy and complexity, one can derive the principle of maximum mutual information as a special case of maximising accuracy, while minimising complexity translates into minimising metabolic costs.</p>
<sec id="s6a">
<title>Metabolic Efficiency and Free Energy</title>
<p>To connect the thermodynamic work or metabolic energy required to represent hidden states to the variational free energy of those representations, we need to consider the relationship between representational internal states and the underlying thermodynamic microstates. Recall that internal states <italic>μ</italic>(<italic>t</italic>) are deterministic quantities that encode a conditional density over hidden states of the world. These macroscopic states can be regarded as <italic>unconstrained internal variables</italic> of a biophysical system; for example, the molar fractions of different molecules in a cellular compartment. The underlying biophysical system can then be associated with a (thermodynamic) canonical ensemble with internal energy:<disp-formula id="pcbi.1003157.e021"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003157.e021" xlink:type="simple"/><label>(10)</label></disp-formula>Here, <italic>p<sub>i</sub></italic> corresponds to the probability of a particular microscopic state and <italic>E<sub>i</sub></italic>(<italic>μ</italic>)to its corresponding energy. Given that the total energy is conserved, this probability is given by the Gibbs measure or Boltzmann distribution:<disp-formula id="pcbi.1003157.e022"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003157.e022" xlink:type="simple"/><label>(11)</label></disp-formula>The partition function <italic>Z</italic>(<italic>T</italic>, <italic>μ</italic>) ensures the probabilities sum to one, while the last equality follows simply from the definition of entropy <italic>H</italic>[<italic>p<sub>i</sub></italic>] = <italic>E<sub>i</sub></italic>[−ln <italic>p<sub>i</sub></italic>]. The Boltzmann distribution describes a system that can exchange energy with a heat bath (or a large number of similar systems) so that its temperature remains constant. The Helmholtz free energy <italic>A</italic>(<italic>T</italic>, <italic>μ</italic>) measures the work obtainable from a closed thermodynamic system at a constant temperature and volume—where a closed system can exchange energy with other systems (but not mass).</p>
<p>The key result we will use from statistical thermodynamics is that the Helmholtz free energy is minimised at equilibrium with respect to any unconstrained internal variables for a closed system at constant temperature <italic>T</italic><sub>0</sub>,<disp-formula id="pcbi.1003157.e023"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003157.e023" xlink:type="simple"/><label>(12)</label></disp-formula>where <italic>A</italic><sub>0</sub>(<italic>T</italic><sub>0</sub>, <italic>μ</italic>) is the free energy of the system at equilibrium or steady state (i.e., constant entropy). This motivates the following Lemma:</p>
<p><bold>Lemma</bold>: <italic>(complexity minimisation) Minimising the complexity of a conditional distribution—whose sufficient statistics are (strictly increasing functions of) some unconstrained internal variables of a thermodynamic system—minimises the Helmholtz free energy of that system.</italic></p>
<p><bold>Proof</bold>: Using standard results from Bayesian statistics <xref ref-type="bibr" rid="pcbi.1003157-Beal1">[16]</xref>, we can express free energy as <italic>complexity</italic> minus <italic>accuracy</italic><disp-formula id="pcbi.1003157.e024"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003157.e024" xlink:type="simple"/><label>(13)</label></disp-formula>The first complexity term is the divergence between the conditional distribution and the prior distribution under the generative model. This effectively counts the degrees of freedom used to encode or predict sensory input. The accuracy is simply the expected log likelihood of the sensory input under the conditional density encoded by internal states. The prior distribution represents beliefs in the absence of sensory input. This corresponds to the distribution encoded by internal states <italic>μ</italic> = <italic>μ</italic><sub>0</sub> when deprived of input for a suitably long time—at which point, we can assume thermodynamic equilibrium, such that Helmholtz free energy is minimised (see <xref ref-type="disp-formula" rid="pcbi.1003157.e023">Equation 12</xref>):<disp-formula id="pcbi.1003157.e025"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003157.e025" xlink:type="simple"/><label>(14)</label></disp-formula>However, in the absence of input, variational free energy reduces to complexity <italic>F</italic><sub>0</sub>(<italic>μ</italic>)≥0, which—by Gibbs inequality—has a minimum of zero. This means that complexity is also minimised.<disp-formula id="pcbi.1003157.e026"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003157.e026" xlink:type="simple"/><label>(15)</label></disp-formula>In sum, the internal states encoding prior beliefs about hidden states of the world are those that minimise Helmholtz free energy and the complexity defined by variational free energy.</p>
<p><bold>Remarks</bold>: All we are saying here is that if a (neuronal) system is deprived of sensory inputs it will obtain thermodynamic equilibrium (or at least a nonequilibrium steady state) and will therefore minimise Helmholtz free energy. This assumes, not implausibly, a constant temperature and volume. Crucially, this is precisely the brain state encoding prior beliefs about sensory input, which means that it is the state of minimum computational complexity. Heuristically, this means that one can associate the complexity cost of variational free energy with metabolic cost—in the sense that they share the same minimum. Crucially, minimising fluctuations in Helmholtz free energy reduces metabolic work by <xref ref-type="disp-formula" rid="pcbi.1003157.e008">Equation 6</xref>. Interestingly, complexity cost also plays a central role in free-energy formulations of optimal control and economic theory <xref ref-type="bibr" rid="pcbi.1003157-Ortega1">[93]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Ortega2">[94]</xref>. Still <italic>et al.</italic> arrive at the same conclusions by treating the thermodynamic system as having an implicit model of its inputs—allowing them to establish the fundamental equivalence between model inefficiency or complexity and thermodynamic inefficiency <xref ref-type="bibr" rid="pcbi.1003157-Still1">[95]</xref>. However, both of these compelling treatments consider homologues of Helmholtz free energy—not variational free energy, which is a functional of a probabilistic model (the conditional distribution).</p>
</sec><sec id="s6b">
<title>Computational Efficiency and Free Energy</title>
<p>The complexity minimisation lemma suggests that commonly occurring representational states—that are <italic>a priori</italic> most probable—are the least costly; for example, resting levels of transmembrane voltage or baseline firing rates. Rare excursions from these states are associated with a high metabolic cost. But how does minimising complexity relate to principles of minimum redundancy? Because representations do not change sensory inputs, they are only required to minimise the free energy of the conditional density. Assuming conditional uncertainty is small, the conditional density can be approximated with a point mass at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003157.e027" xlink:type="simple"/></inline-formula>, such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003157.e028" xlink:type="simple"/></inline-formula> and the free energy becomes (from <xref ref-type="disp-formula" rid="pcbi.1003157.e024">Equation 13</xref>)<disp-formula id="pcbi.1003157.e029"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003157.e029" xlink:type="simple"/><label>(16)</label></disp-formula>The first equality expresses free energy is terms of accuracy and complexity, where the second complexity term just reports the surprise about the conditional representation under prior beliefs. The second equality is the corresponding path integral of free energy (known as free action). Under ergodic assumptions <xref ref-type="bibr" rid="pcbi.1003157-Friston1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Birkhoff2">[96]</xref> this can be expressed as the conditional entropy of sensory input, given the representations and the entropy of the internal states. <xref ref-type="disp-formula" rid="pcbi.1003157.e022">Equation 11</xref> has two important implications. First, it shows that minimising free energy, at each point in time, is equivalent to minimising free action —by the fundamental lemma of variational calculus. In other words, <xref ref-type="disp-formula" rid="pcbi.1003157.e022">Equation 11</xref> is just a restatement of the principle of least action. Second, it shows that minimising free energy maximises the accuracy of representations or minimises their conditional uncertainty (entropy) over time. This is simply a restatement of the principle of minimum redundancy or maximum mutual information <xref ref-type="bibr" rid="pcbi.1003157-Linsker1">[97]</xref>. This follows because minimising uncertainty about sensory inputs, given internal states, implicitly maximises the mutual information between sensory and internal states (for any given sensations):<disp-formula id="pcbi.1003157.e030"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003157.e030" xlink:type="simple"/><label>(17)</label></disp-formula>This suggests that the infomax principle <xref ref-type="bibr" rid="pcbi.1003157-Linsker1">[97]</xref> is a special case of the free-energy principle that is obtained when we discount uncertainty and represent sensory input with point estimates of their causes. In this context, high mutual information is assured by maximising accuracy (e.g., minimising prediction error) and prior beliefs are enforced by minimising complexity. Crucially, minimising complexity minimises metabolic cost.</p>
<p>In short, the infomax principle can be understood in terms of the decomposition of free energy into complexity and accuracy: mutual information or statistical efficiency is optimised when conditional expectations maximise accuracy (or minimise prediction error), while thermodynamic efficiency is assured by minimising complexity. This minimisation ensures that the generative model is not over-parameterized and leads to a parsimonious representation of sensory data that conforms to prior beliefs about their causes. Interestingly, advanced model optimisation techniques use free-energy optimisation to eliminate redundant model parameters <xref ref-type="bibr" rid="pcbi.1003157-Tipping1">[98]</xref>, suggesting that free-energy optimisation might provide a nice explanation for synaptic pruning and homeostasis in the brain during neurodevelopment <xref ref-type="bibr" rid="pcbi.1003157-Paus1">[99]</xref> and sleep <xref ref-type="bibr" rid="pcbi.1003157-Gilestro1">[100]</xref>. In developing the link between metabolic and statistical efficiency, we have assumed that internal neuronal states encode hidden states in terms of their most likely value or expectation. Is there any principled reason to assume this form of neuronal code?</p>
</sec><sec id="s6c">
<title>The Maximum Entropy Principle and the Laplace Assumption</title>
<p>Notice from <xref ref-type="disp-formula" rid="pcbi.1003157.e016">Equation 7</xref> that minimising variational free energy entails maximising the entropy of the conditional density. Intuitively, this is like keeping one's options open when trying to find hypotheses or explanations for sensory input. If we admit an encoding of the conditional density up to second order moments, then the maximum entropy principle <xref ref-type="bibr" rid="pcbi.1003157-Jaynes1">[42]</xref>, implicit in the definition of free energy, requires <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003157.e031" xlink:type="simple"/></inline-formula> to be Gaussian. This is because a Gaussian density has the maximum entropy of all forms that can be specified with two moments. Assuming a Gaussian form is known as the Laplace assumption and enables us to express the entropy of the conditional density in terms of its first moment or expectation. This follows because we can minimise free energy with respect to the conditional covariance as follows:<disp-formula id="pcbi.1003157.e032"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003157.e032" xlink:type="simple"/><label>(18)</label></disp-formula>Here, the conditional precision Π(<italic>μ</italic>) is the inverse of the conditional covariance Σ(<italic>μ</italic>). <xref ref-type="disp-formula" rid="pcbi.1003157.e032">Equation 18</xref> means the free energy becomes a function of conditional expectations and sensory states. This is important because it suggests the brain may represent hidden states of the world in terms of their expected values. This leads to the Laplace code (defined as neuronal encoding under the Laplace assumption), which is arguably the simplest and most flexible of all neuronal codes <xref ref-type="bibr" rid="pcbi.1003157-Friston2">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Friston3">[14]</xref>. Furthermore, under the Laplace code, one can minimise free energy efficiently using predictive coding <xref ref-type="bibr" rid="pcbi.1003157-Mumford1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1003157-Rao1">[31]</xref>. Predictive coding has become one of the most popular ways of understanding message passing in the brain—particularly in the setting of hierarchical perceptual inference. In short, the free-energy principle entails the principle of maximum entropy and leads, in a principled way, to a neuronal encoding of representations in terms of conditional expectations.</p>
<p>The specific nature of the neural code may be exclusive to a species or underlying neural function. Whatever its makeup—expected latency, firing rate, spike timing, phase, etc.—it will exist to harmonize the dialogue between perception and action. In practice, we usually have in mind the instantaneous rate of firing of neuronal populations, which means the internal states encoding posterior beliefs are ensemble averages of ensemble averages—for example, the expectation of (a function of) depolarisation over the neuronal ensemble, where the depolarisation of a single neuron is (a function of) the internal variables of a canonical ensemble.</p>
</sec></sec><sec id="s7">
<title>Conclusion</title>
<p>We have reviewed the thermodynamic and computational (statistical) imperatives for biological self-organisation, with a special focus on neuronal circuits. We have considered the role of classical thermodynamics and the notion of metabolic efficiency—that appears to be an important constraint, under which neurophysiology and neuroanatomy have evolved. From a computational perspective, we have looked at variational free-energy minimisation as the basis for active Bayesian inference and modelling of the environment. The ability to represent and predict hidden environmental states efficiently can be quantified in terms of mutual information. Our synthesis suggests that minimising variational free energy is a sufficient account of the tendency to maximise both metabolic and statistical efficiency. The motivation for minimising variational free energy is to minimise its long-term average to maintain a constant external milieu—as measured by the entropy of an organism's sensory samples over time. By decomposing variational free energy into accuracy and complexity one can understand metabolic efficiency in terms of minimising complexity (which minimises Helmholtz free energy), under the computational constraint that sensory inputs are represented accurately. Conversely, statistical efficiency can be understood in terms of maximising the accuracy (which maximises mutual information), under the constraint that representations have minimal complexity. The link between complexity and metabolic cost rests on the simple observation that, in the absence of sensory input, prior beliefs are encoded by physical variables that minimise Helmholtz free energy.</p>
<p>The nice thing about this formulation is that, under active inference, organisms will selectively sample sensory inputs that conform to their prior beliefs and minimise the complexity of their representations. This means that biological systems will appear to act in a way that minimises fluctuations in Helmholtz free energy—and will aspire to the nonequilibrium steady state that has been assigned to them by evolution.</p>
</sec></body>
<back><ref-list>
<title>References</title>
<ref id="pcbi.1003157-Sutherland1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sutherland</surname><given-names>WJ</given-names></name> (<year>2005</year>) <article-title>The best solution</article-title>. <source>Nature</source> <volume>435</volume>: <fpage>569</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Alexander1"><label>2</label>
<mixed-citation publication-type="other" xlink:type="simple">Alexander RM (1996) Optima for animals. Princeton/Chichester: Princeton University Press.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Niven1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Niven</surname><given-names>JE</given-names></name>, <name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name> (<year>2008</year>) <article-title>Energy limitation as a selective pressure on the evolution of sensory systems</article-title>. <source>J Exp Biol</source> <volume>211</volume>: <fpage>1792</fpage>–<lpage>1804</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Feynman1"><label>4</label>
<mixed-citation publication-type="other" xlink:type="simple">Feynman RP, Hey AJG, Allen RW (1996) Feynman lectures on computation. Reading (Massachusetts): Addison-Wesley.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Landauer1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Landauer</surname><given-names>R</given-names></name> (<year>1961</year>) <article-title>Irreversibility and heat generation in the computing process</article-title>. <source>IBM Journal of Research and Development</source> <volume>5</volume>: <fpage>183</fpage>–<lpage>191</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Landauer2"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Landauer</surname><given-names>R</given-names></name> (<year>1996</year>) <article-title>Minimal energy requirements in communciation</article-title>. <source>Science</source> <volume>272</volume>: <fpage>1914</fpage>–<lpage>1918</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Stemmler1"><label>7</label>
<mixed-citation publication-type="other" xlink:type="simple">Stemmler M, Sengupta B, Laughlin SB, Niven JE (2011) Energetically optimal action potentials. In: Shawe-Taylor J, Zemel RS, Bartlett P, Pereira F, Weinberger KQ, editors. Advances in neural information processing systems 24. pp. 1566–1574.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Jarzynski1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jarzynski</surname><given-names>C</given-names></name> (<year>1997</year>) <article-title>Nonequilibrium equality for free energy differences</article-title>. <source>Phys Rev Lett</source> <volume>78</volume>: <fpage>2690</fpage>–<lpage>2693</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Ashby1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ashby</surname><given-names>WR</given-names></name> (<year>1947</year>) <article-title>Principles of the self-organising dynamic system</article-title>. <source>J Gen Psychol</source> <volume>37</volume>: <fpage>125</fpage>–<lpage>128</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Nicolis1"><label>10</label>
<mixed-citation publication-type="other" xlink:type="simple">Nicolis G, Prigogine I (1977) Self-organisation in non-equilibrium systems. New York: Wiley.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Birkhoff1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Birkhoff</surname><given-names>GD</given-names></name>, <name name-style="western"><surname>Koopman</surname><given-names>BO</given-names></name> (<year>1932</year>) <article-title>Recent contributions to the ergodic theory</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>18</volume>: <fpage>279</fpage>–<lpage>282</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Friston1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Ao</surname><given-names>P</given-names></name> (<year>2012</year>) <article-title>Free energy, value, and attractors</article-title>. <source>Comput Math Methods Med</source> <volume>2012</volume>: <fpage>937860</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Friston2"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name> (<year>2009</year>) <article-title>The free-energy principle: a rough guide to the brain?</article-title> <source>TICS</source> <volume>13</volume>: <fpage>293</fpage>–<lpage>301</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Friston3"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name> (<year>2010</year>) <article-title>The free-energy principle: a unified brain theory?</article-title> <source>Nat Rev Neurosci</source> <volume>11</volume>: <fpage>127</fpage>–<lpage>138</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Feynman2"><label>15</label>
<mixed-citation publication-type="other" xlink:type="simple">Feynman RP (1998) Statistical mechanics: a set of lectures. Boulder (Colorado): Westview Press.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Beal1"><label>16</label>
<mixed-citation publication-type="other" xlink:type="simple">Beal M (2003) Variational algorithms for approximate bayesian inference [PhD thesis]. London: University College London.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Hinton1"><label>17</label>
<mixed-citation publication-type="other" xlink:type="simple">Hinton GE, Sejnowski TJ (1983) Analyzing cooperative computation. In: Proceedings of the 5th Annual Congress of the Cognitive Science Society; 1983; Rochester, New York, United States.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Hinton2"><label>18</label>
<mixed-citation publication-type="other" xlink:type="simple">Hinton GE, Sejnowski TJ (1983) Optimal perceptual inference. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; 1983; Washington, D.C., United States. pp. 448–453.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Hofstadter1"><label>19</label>
<mixed-citation publication-type="other" xlink:type="simple">Hofstadter DR (1984) The copycat project: an experiment in nondeterminism and creative analogies. Boston: MIT Artificial Intelligence Laboratory.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Hopfield1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name> (<year>1982</year>) <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>79</volume>: <fpage>2554</fpage>–<lpage>2558</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Friston4"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Kilner</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Harrison</surname><given-names>L</given-names></name> (<year>2006</year>) <article-title>A free energy principle for the brain</article-title>. <source>J Physiol Paris</source> <volume>100</volume>: <fpage>70</fpage>–<lpage>87</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Friston5"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Mattout</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Trujillo-Barreto</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Ashburner</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Penny</surname><given-names>W</given-names></name> (<year>2007</year>) <article-title>Variational free energy and the Laplace approximation</article-title>. <source>Neuroimage</source> <volume>34</volume>: <fpage>220</fpage>–<lpage>234</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Harrison1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harrison</surname><given-names>LM</given-names></name>, <name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name>, <name name-style="western"><surname>Rees</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name> (<year>2007</year>) <article-title>Extra-classical receptive field effects measured in striate cortex with fMRI</article-title>. <source>Neuroimage</source> <volume>34</volume>: <fpage>1199</fpage>–<lpage>1208</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Garrido1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Garrido</surname><given-names>MI</given-names></name>, <name name-style="western"><surname>Kilner</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name> (<year>2007</year>) <article-title>Evoked brain responses are generated by feedback loops</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>104</volume>: <fpage>20961</fpage>–<lpage>20966</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Zeki1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zeki</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Shipp</surname><given-names>S</given-names></name> (<year>1988</year>) <article-title>The functional logic of cortical connections</article-title>. <source>Nature</source> <volume>335</volume>: <fpage>311</fpage>–<lpage>317</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Felleman1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Felleman</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Essen</surname><given-names>DCV</given-names></name> (<year>1991</year>) <article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title>. <source>Cereb Cortex</source> <volume>1</volume>: <fpage>1</fpage>–<lpage>47</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Kawato1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kawato</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Hayakawa</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Inui</surname><given-names>T</given-names></name> (<year>1993</year>) <article-title>A forward-inverse optics model of reciprocal connections between visual cortical areas</article-title>. <source>Network</source> <volume>4</volume>: <fpage>415</fpage>–<lpage>422</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Srinivasan1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Srinivasan</surname><given-names>MV</given-names></name>, <name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name>, <name name-style="western"><surname>Dubs</surname><given-names>A</given-names></name> (<year>1982</year>) <article-title>Predictive coding: a fresh view of inhibition in the retina</article-title>. <source>Proc R Soc Lond B Biol Sci</source> <volume>216</volume>: <fpage>427</fpage>–<lpage>459</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Mumford1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mumford</surname><given-names>D</given-names></name> (<year>1992</year>) <article-title>On the computational architecture of the neocortex. II. The role of cortico-cortical loops</article-title>. <source>Biol Cybern</source> <volume>66</volume>: <fpage>241</fpage>–<lpage>251</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Crick1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Crick</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Koch</surname><given-names>C</given-names></name> (<year>1998</year>) <article-title>Constraints on cortical and thalamic projections: the no-strong-loops hypothesis</article-title>. <source>Nature</source> <volume>391</volume>: <fpage>245</fpage>–<lpage>250</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Rao1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rao</surname><given-names>RP</given-names></name>, <name name-style="western"><surname>Ballard</surname><given-names>DH</given-names></name> (<year>1999</year>) <article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title>. <source>Nat Neurosci</source> <volume>2</volume>: <fpage>79</fpage>–<lpage>87</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Yu1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yu</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name> (<year>2005</year>) <article-title>Uncertainty, neuromodulation, and attention</article-title>. <source>Neuron</source> <volume>46</volume>: <fpage>681</fpage>–<lpage>692</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Friston6"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name> (<year>2008</year>) <article-title>Hierarchical models in the brain</article-title>. <source>PLoS Comput Biol</source> <volume>4</volume>: <fpage>e1000211</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000211" xlink:type="simple">10.1371/journal.pcbi.1000211</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003157-Abbott1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name>, <name name-style="western"><surname>Varela</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Sen</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Nelson</surname><given-names>SB</given-names></name> (<year>1997</year>) <article-title>Synaptic depression and cortical gain control</article-title>. <source>Science</source> <volume>275</volume>: <fpage>220</fpage>–<lpage>224</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Prigogine1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Prigogine</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Nicolis</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Babloyantz</surname><given-names>A</given-names></name> (<year>1972</year>) <article-title>Thermodynamics of evolution</article-title>. <source>Physics Today</source> <volume>25</volume>: <fpage>38</fpage>–<lpage>44</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Elias1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Elias</surname><given-names>P</given-names></name> (<year>1955</year>) <article-title>Predictive coding-I</article-title>. <source>IRE Transactions on Information Theory</source> <volume>1</volume>: <fpage>16</fpage>–<lpage>24</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Elias2"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Elias</surname><given-names>P</given-names></name> (<year>1955</year>) <article-title>Predictive coding-II</article-title>. <source>IRE Transactions on Information Theory</source> <volume>1</volume>: <fpage>24</fpage>–<lpage>33</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Stemmler2"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stemmler</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Koch</surname><given-names>C</given-names></name> (<year>1999</year>) <article-title>How voltage-dependent conductances can adapt to maximize the information encoded by neuronal firing rate</article-title>. <source>Nat Neurosci</source> <volume>2</volume>: <fpage>521</fpage>–<lpage>527</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Bell1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bell</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name> (<year>1995</year>) <article-title>An information-maximization approach to blind separation and blind deconvolution</article-title>. <source>Neural Comput</source> <volume>7</volume>: <fpage>1129</fpage>–<lpage>1159</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Friston7"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name> (<year>2011</year>) <article-title>What is optimal about motor control?</article-title> <source>Neuron</source> <volume>72</volume>: <fpage>488</fpage>–<lpage>498</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Peleg1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peleg</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Efraim</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Shental</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Kanter</surname><given-names>I</given-names></name> (<year>2010</year>) <article-title>Mutual information via thermodynamics: three different approaches</article-title>. <source>Journal of Statistical Mechanics: Theory and Experiments</source> <volume>Jan</volume>: <fpage>P01014</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Jaynes1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jaynes</surname><given-names>ET</given-names></name> (<year>1957</year>) <article-title>Information theory and statistical mechanics</article-title>. <source>Phys Rev</source> <volume>106</volume>: <fpage>620</fpage>–<lpage>630</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Shannon1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shannon</surname><given-names>C</given-names></name> (<year>1948</year>) <article-title>A mathematical theory of communication</article-title>. <source>Bell System Technical Journal</source> <volume>27</volume>: <fpage>379</fpage>–<lpage>423</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Jaynes2"><label>44</label>
<mixed-citation publication-type="other" xlink:type="simple">Jaynes ET (2003) Probability theory: the logic of science. Cambridge (United Kingdom): Cambridge University Press.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Atick1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Atick</surname><given-names>JJ</given-names></name> (<year>1992</year>) <article-title>Could information theory provide an ecological theory of sensory processing?</article-title> <source>Network</source> <volume>3</volume>: <fpage>213</fpage>–<lpage>251</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Barlow1"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barlow</surname><given-names>HB</given-names></name>, <name name-style="western"><surname>Kaushal</surname><given-names>TP</given-names></name>, <name name-style="western"><surname>Mitchison</surname><given-names>GJ</given-names></name> (<year>1989</year>) <article-title>Finding minimum entropy codes</article-title>. <source>Neural Computation</source> <volume>1</volume>: <fpage>412</fpage>–<lpage>423</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Barlow2"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barlow</surname><given-names>HB</given-names></name>, <name name-style="western"><surname>Kaushal</surname><given-names>TP</given-names></name>, <name name-style="western"><surname>Hawken</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Parker</surname><given-names>AJ</given-names></name> (<year>1987</year>) <article-title>Human contrast discrimination and the threshold of cortical neurons</article-title>. <source>J Opt Soc Am A</source> <volume>4</volume>: <fpage>2366</fpage>–<lpage>2371</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Barlow3"><label>48</label>
<mixed-citation publication-type="other" xlink:type="simple">Barlow HB (1959) Sensory mechanisms, the reduction of redundancy, and intelligence. National Physical Laboratory Symposium. Teddington (United Kingdom): H.M. Stationery Office.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Laughlin1"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laughlin</surname><given-names>S</given-names></name> (<year>1981</year>) <article-title>A simple coding procedure enhances a neuron's information capacity</article-title>. <source>Z Naturforsch C</source> <volume>36</volume>: <fpage>910</fpage>–<lpage>912</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Johnson1"><label>50</label>
<mixed-citation publication-type="other" xlink:type="simple">Johnson D (2002) Four top reasons mutual information does not quantify neural information processing. In: Proceedings of the Annual Computational Neuroscience Meeting; 2002; Chicago, Illinois, United States.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Papoulis1"><label>51</label>
<mixed-citation publication-type="other" xlink:type="simple">Papoulis A (1984) Probability, random variables, and stochastic processes. New York: McGraw-Hill.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Benda1"><label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Benda</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Gollisch</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Machens</surname><given-names>CK</given-names></name>, <name name-style="western"><surname>Herz</surname><given-names>AV</given-names></name> (<year>2007</year>) <article-title>From response to stimulus: adaptive sampling in sensory physiology</article-title>. <source>Curr Opin Neurobiol</source> <volume>17</volume>: <fpage>430</fpage>–<lpage>436</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Treves1"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Treves</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name> (<year>1995</year>) <article-title>The upward bias in measures of information derived from limited data samples</article-title>. <source>Neural Comput</source> <volume>7</volume>: <fpage>399</fpage>–<lpage>407</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Paninski1"><label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name> (<year>2005</year>) <article-title>Asymptotic theory of information-theoretic experimental design</article-title>. <source>Neural Comput</source> <volume>17</volume>: <fpage>1480</fpage>–<lpage>1507</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Paninski2"><label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paninski</surname><given-names>L</given-names></name> (<year>2003</year>) <article-title>Estimation of entropy and mutual information</article-title>. <source>Neural Computation</source> <volume>15</volume>: <fpage>1191</fpage>–<lpage>1253</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Ames1"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ames</surname><given-names>A</given-names></name> (<year>2000</year>) <article-title>CNS energy metabolism as related to function</article-title>. <source>Brain Res Brain Res Rev</source> <volume>34</volume>: <fpage>42</fpage>–<lpage>68</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Rolfe1"><label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rolfe</surname><given-names>DF</given-names></name>, <name name-style="western"><surname>Brown</surname><given-names>GC</given-names></name> (<year>1997</year>) <article-title>Cellular energy utilization and molecular origin of standard metabolic rate in mammals</article-title>. <source>Physiol Rev</source> <volume>77</volume>: <fpage>731</fpage>–<lpage>758</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Clarke1"><label>58</label>
<mixed-citation publication-type="other" xlink:type="simple">Clarke D, Sokoloff L (1999) Circulation and energy metabolism of the brain. In: Siegel GJ, Agranoff BW, Albers RW, Fisher SK, Uhler MD, editors. Basic neurochemistry: molecular, cellular and medical aspects. Philadelphia: Lippincott-Raven. pp. 637–669.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Attwell1"><label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Attwell</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name> (<year>2001</year>) <article-title>An energy budget for signaling in the grey matter of the brain</article-title>. <source>J Cereb Blood Flow Metab</source> <volume>21</volume>: <fpage>1133</fpage>–<lpage>1145</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Skou1"><label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Skou</surname><given-names>JC</given-names></name> (<year>1989</year>) <article-title>The identification of the sodium-pump as the membrane-bound Na<sup>+</sup>/K<sup>+</sup>-ATPase: a commentary on ‘The Influence of Some Cations on an Adenosine Triphosphatase from Peripheral Nerves’</article-title>. <source>Biochim Biophys Acta</source> <volume>1000</volume>: <fpage>435</fpage>–<lpage>438</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Skou2"><label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Skou</surname><given-names>JC</given-names></name> (<year>1957</year>) <article-title>The influence of some cations on an adenosine triphosphatase from peripheral nerves</article-title>. <source>Biochim Biophys Acta</source> <volume>23</volume>: <fpage>394</fpage>–<lpage>401</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Post1"><label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Post</surname><given-names>RL</given-names></name>, <name name-style="western"><surname>Hegyvary</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Kume</surname><given-names>S</given-names></name> (<year>1972</year>) <article-title>Activation by adenosine triphosphate in the phosphorylation kinetics of sodium and potassium ion transport adenosine triphosphatase</article-title>. <source>J Biol Chem</source> <volume>247</volume>: <fpage>6530</fpage>–<lpage>6540</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Sen1"><label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sen</surname><given-names>AK</given-names></name>, <name name-style="western"><surname>Post</surname><given-names>RL</given-names></name> (<year>1964</year>) <article-title>Stoichiometry and localization of adenosine triphosphate-dependent sodium and potassium transport in the erythrocyte</article-title>. <source>J Biol Chem</source> <volume>239</volume>: <fpage>345</fpage>–<lpage>352</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Pangrsic1"><label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pangrsic</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Stusek</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Belusic</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Zupancic</surname><given-names>G</given-names></name> (<year>2005</year>) <article-title>Light dependence of oxygen consumption by blowfly eyes recorded with a magnetic diver balance</article-title>. <source>J Comp Physiol A Neuroethol Sens Neural Behav Physiol</source> <volume>191</volume>: <fpage>75</fpage>–<lpage>84</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Niven2"><label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Niven</surname><given-names>JE</given-names></name>, <name name-style="western"><surname>Anderson</surname><given-names>JC</given-names></name>, <name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name> (<year>2007</year>) <article-title>Fly photoreceptors demonstrate energy-information trade-offs in neural coding</article-title>. <source>PLoS Biol</source> <volume>5</volume>: <fpage>e116</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.0050116" xlink:type="simple">10.1371/journal.pbio.0050116</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003157-Laughlin2"><label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name> (<year>2001</year>) <article-title>Energy as a constraint on the coding and processing of sensory information</article-title>. <source>Curr Opin Neurobiol</source> <volume>11</volume>: <fpage>475</fpage>–<lpage>480</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Laughlin3"><label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name>, <name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name> (<year>2003</year>) <article-title>Communication in neuronal networks</article-title>. <source>Science</source> <volume>301</volume>: <fpage>1870</fpage>–<lpage>1874</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Levy1"><label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Levy</surname><given-names>WB</given-names></name>, <name name-style="western"><surname>Baxter</surname><given-names>RA</given-names></name> (<year>1996</year>) <article-title>Energy efficient neural codes</article-title>. <source>Neural Comput</source> <volume>8</volume>: <fpage>531</fpage>–<lpage>543</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-vonderMalsburg1"><label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>von der Malsburg</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Willshaw</surname><given-names>DJ</given-names></name> (<year>1977</year>) <article-title>How to label nerve cells so that they can interconnect in an ordered fashion</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>74</volume>: <fpage>5176</fpage>–<lpage>5178</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Bullmore1"><label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bullmore</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name> (<year>2012</year>) <article-title>The economy of brain network organization</article-title>. <source>Nat Rev Neurosci</source> <volume>13</volume>: <fpage>336</fpage>–<lpage>349</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Bullmore2"><label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bullmore</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name> (<year>2009</year>) <article-title>Complex brain networks: graph theoretical analysis of structural and functional systems</article-title>. <source>Nat Rev Neurosci</source> <volume>10</volume>: <fpage>186</fpage>–<lpage>198</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Sarpeshkar1"><label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sarpeshkar</surname><given-names>R</given-names></name> (<year>1998</year>) <article-title>Analog versus digital: extrapolating from electronics to neurobiology</article-title>. <source>Neural Comp</source> <volume>10</volume>: <fpage>1601</fpage>–<lpage>1638</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Sarpeshkar2"><label>73</label>
<mixed-citation publication-type="other" xlink:type="simple">Sarpeshkar R (2010) Ultra low power bioelectronics: fundamentals, biomedical applications, and bio-inspired systems. Cambridge (United Kingdom): Cambridge University Press.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Laughlin4"><label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name>, <name name-style="western"><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name>, <name name-style="western"><surname>Anderson</surname><given-names>JC</given-names></name> (<year>1998</year>) <article-title>The metabolic cost of neural information</article-title>. <source>Nat Neurosci</source> <volume>1</volume>: <fpage>36</fpage>–<lpage>41</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Niven3"><label>75</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Niven</surname><given-names>JE</given-names></name>, <name name-style="western"><surname>Vähäsöyrinki</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Kauranen</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Hardie</surname><given-names>RC</given-names></name>, <name name-style="western"><surname>Juusola</surname><given-names>M</given-names></name>, <etal>et al</etal>. (<year>2003</year>) <article-title>The contribution of Shaker K<sup>+</sup> channels to the information capacity of Drosophila photoreceptors</article-title>. <source>Nature</source> <volume>421</volume>: <fpage>630</fpage>–<lpage>634</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Niven4"><label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Niven</surname><given-names>JE</given-names></name>, <name name-style="western"><surname>Vähäsöyrinki</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Juusola</surname><given-names>M</given-names></name> (<year>2003</year>) <article-title>Shaker K<sup>+</sup> channels are predicted to reduce the metabolic cost of neural information in Drosophila photoreceptors</article-title>. <source>Proc Biol Sci</source> <volume>270 Suppl 1</volume>: <fpage>S58</fpage>–<lpage>S61</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Attwell2"><label>77</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Attwell</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Gibb</surname><given-names>A</given-names></name> (<year>2005</year>) <article-title>Neuroenergetics and the kinetic design of excitatory synapses</article-title>. <source>Nat Rev Neurosci</source> <volume>6</volume>: <fpage>841</fpage>–<lpage>849</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Carter1"><label>78</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carter</surname><given-names>BC</given-names></name>, <name name-style="western"><surname>Bean</surname><given-names>BP</given-names></name> (<year>2009</year>) <article-title>Sodium entry during action potentials of mammalian neurons: incomplete inactivation and reduced metabolic efficiency in fast-spiking neurons</article-title>. <source>Neuron</source> <volume>64</volume>: <fpage>898</fpage>–<lpage>909</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Hasenstaub1"><label>79</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hasenstaub</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Otte</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Callaway</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name> (<year>2010</year>) <article-title>Metabolic cost as a unifying principle governing neuronal biophysics</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>107</volume>: <fpage>12329</fpage>–<lpage>12334</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Sengupta1"><label>80</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sengupta</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Stemmler</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name>, <name name-style="western"><surname>Niven</surname><given-names>JE</given-names></name> (<year>2010</year>) <article-title>Action potential energy efficiency varies among neuron types in vertebrates and invertebrates</article-title>. <source>PLoS Comput Biol</source> <volume>6</volume>: <fpage>e1000840</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000840" xlink:type="simple">10.1371/journal.pcbi.1000840</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003157-Alle1"><label>81</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alle</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Roth</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Geiger</surname><given-names>JR</given-names></name> (<year>2009</year>) <article-title>Energy-efficient action potentials in hippocampal mossy fibers</article-title>. <source>Science</source> <volume>325</volume>: <fpage>1405</fpage>–<lpage>1408</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Lennie1"><label>82</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lennie</surname><given-names>P</given-names></name> (<year>2003</year>) <article-title>The cost of cortical computation</article-title>. <source>Curr Biol</source> <volume>13</volume>: <fpage>493</fpage>–<lpage>497</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Nawroth1"><label>83</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nawroth</surname><given-names>JC</given-names></name>, <name name-style="western"><surname>Greer</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>WR</given-names></name>, <name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name>, <name name-style="western"><surname>Shepherd</surname><given-names>GM</given-names></name> (<year>2007</year>) <article-title>An energy budget for the olfactory glomerulus</article-title>. <source>J Neurosci</source> <volume>27</volume>: <fpage>9790</fpage>–<lpage>9800</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Okawa1"><label>84</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Okawa</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Sampath</surname><given-names>AP</given-names></name>, <name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name>, <name name-style="western"><surname>Fain</surname><given-names>GL</given-names></name> (<year>2008</year>) <article-title>ATP consumption by mammalian rod photoreceptors in darkness and in light</article-title>. <source>Curr Biol</source> <volume>18</volume>: <fpage>1917</fpage>–<lpage>1921</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Howarth1"><label>85</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Howarth</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Peppiatt-Wildman</surname><given-names>CM</given-names></name>, <name name-style="western"><surname>Attwell</surname><given-names>D</given-names></name> (<year>2010</year>) <article-title>The energy use associated with neural computation in the cerebellum</article-title>. <source>J Cereb Blood Flow Metab</source> <volume>30</volume>: <fpage>403</fpage>–<lpage>414</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Harris1"><label>86</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harris</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Attwell</surname><given-names>D</given-names></name> (<year>2012</year>) <article-title>The energetics of CNS white matter</article-title>. <source>J Neurosci</source> <volume>32</volume>: <fpage>356</fpage>–<lpage>371</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Attneave1"><label>87</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Attneave</surname><given-names>F</given-names></name> (<year>1954</year>) <article-title>Some informational aspects of visual perception</article-title>. <source>Psychol Rev</source> <volume>61</volume>: <fpage>183</fpage>–<lpage>193</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Chklovskii1"><label>88</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chklovskii</surname><given-names>DB</given-names></name>, <name name-style="western"><surname>Koulakov</surname><given-names>AA</given-names></name> (<year>2004</year>) <article-title>Maps in the brain: what can we learn from them?</article-title> <source>Annu Rev Neurosci</source> <volume>27</volume>: <fpage>369</fpage>–<lpage>392</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Wen1"><label>89</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wen</surname><given-names>Q</given-names></name>, <name name-style="western"><surname>Chklovskii</surname><given-names>DB</given-names></name> (<year>2008</year>) <article-title>A cost-benefit analysis of neuronal morphology</article-title>. <source>J Neurophysiol</source> <volume>99</volume>: <fpage>2320</fpage>–<lpage>2328</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Wen2"><label>90</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wen</surname><given-names>Q</given-names></name>, <name name-style="western"><surname>Stepanyants</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Elston</surname><given-names>GN</given-names></name>, <name name-style="western"><surname>Grosberg</surname><given-names>AY</given-names></name>, <name name-style="western"><surname>Chklovskii</surname><given-names>DB</given-names></name> (<year>2009</year>) <article-title>Maximization of the connectivity repertoire as a statistical principle governing the shapes of dendritic arbors</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>106</volume>: <fpage>12536</fpage>–<lpage>12541</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Chen1"><label>91</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chen</surname><given-names>BL</given-names></name>, <name name-style="western"><surname>Hall</surname><given-names>DH</given-names></name>, <name name-style="western"><surname>Chklovskii</surname><given-names>DB</given-names></name> (<year>2006</year>) <article-title>Wiring optimization can relate neuronal structure and function</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>103</volume>: <fpage>4723</fpage>–<lpage>4728</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Crotty1"><label>92</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Crotty</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Sangrey</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Levy</surname><given-names>WB</given-names></name> (<year>2006</year>) <article-title>Metabolic energy cost of action potential velocity</article-title>. <source>J Neurophysiol</source> <volume>96</volume>: <fpage>1237</fpage>–<lpage>1246</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Ortega1"><label>93</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ortega</surname><given-names>PA</given-names></name>, <name name-style="western"><surname>Brau</surname><given-names>DA</given-names></name> (<year>2012</year>) <article-title>Thermodynamics as a theory of decision-making with information processing costs</article-title>. <source>ArXiv</source> <fpage>1204.6481v1</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Ortega2"><label>94</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ortega</surname><given-names>PA</given-names></name>, <name name-style="western"><surname>Braun</surname><given-names>DA</given-names></name> (<year>2010</year>) <article-title>A minimum relative entropy principle for learning and acting</article-title>. <source>J Artif Int Res</source> <volume>38</volume>: <fpage>475</fpage>–<lpage>511</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Still1"><label>95</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Still</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Sivak</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Bell</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Crooks</surname><given-names>GE</given-names></name> (<year>2012</year>) <article-title>Thermodynamics of prediction</article-title>. <source>Phys Rev Lett</source> <volume>109</volume>: <fpage>120604</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Birkhoff2"><label>96</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Birkhoff</surname><given-names>GD</given-names></name> (<year>1931</year>) <article-title>Proof of the ergodic theorem</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>17</volume>: <fpage>656</fpage>–<lpage>660</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Linsker1"><label>97</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Linsker</surname><given-names>R</given-names></name> (<year>1990</year>) <article-title>Perceptual neural organization: some approaches based on network models and information theory</article-title>. <source>Annu Rev Neurosci</source> <volume>13</volume>: <fpage>257</fpage>–<lpage>281</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Tipping1"><label>98</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tipping</surname><given-names>ME</given-names></name> (<year>2001</year>) <article-title>Sparse bayesian learning and the relevance vector machine</article-title>. <source>J Mach Learn Res</source> <volume>1</volume>: <fpage>211</fpage>–<lpage>244</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Paus1"><label>99</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paus</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Keshavan</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Giedd</surname><given-names>JN</given-names></name> (<year>2008</year>) <article-title>Why do many psychiatric disorders emerge during adolescence?</article-title> <source>Nat Rev Neurosci</source> <volume>9</volume>: <fpage>947</fpage>–<lpage>957</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Gilestro1"><label>100</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gilestro</surname><given-names>GF</given-names></name>, <name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Cirelli</surname><given-names>C</given-names></name> (<year>2009</year>) <article-title>Widespread changes in synaptic markers as a function of sleep and wakefulness in Drosophila</article-title>. <source>Science</source> <volume>324</volume>: <fpage>109</fpage>–<lpage>112</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Sengupta2"><label>101</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sengupta</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Faisal</surname><given-names>AA</given-names></name>, <name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name>, <name name-style="western"><surname>Niven</surname><given-names>JE</given-names></name> (<year>2013</year>) <article-title>The effect of cell size and channel density on neuronal information encoding and energy efficiency</article-title>. <source>J Cereb Blood Flow Metab</source> In press.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Christie1"><label>102</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Christie</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Stroobandt</surname><given-names>D</given-names></name> (<year>2000</year>) <article-title>The interpretation and application of Rent's rule</article-title>. <source>IEEE Journal of VLSI</source> <volume>8</volume>: <fpage>639</fpage>–<lpage>648</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003157-Bassett1"><label>103</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bassett</surname><given-names>DS</given-names></name>, <name name-style="western"><surname>Greenfield</surname><given-names>DL</given-names></name>, <name name-style="western"><surname>Meyer-Lindenberg</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Weinberger</surname><given-names>DR</given-names></name>, <name name-style="western"><surname>Moore</surname><given-names>SW</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Efficient physical embedding of topologically complex information processing networks in brains and computer circuits</article-title>. <source>PLoS Comput Biol</source> <volume>6</volume>: <fpage>e1000748</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000748" xlink:type="simple">10.1371/journal.pcbi.1000748</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003157-Sengupta3"><label>104</label>
<mixed-citation publication-type="other" xlink:type="simple">Sengupta B (2011) Information encoding and energy consumption in single neuron models [PhD thesis]. Cambridge (United Kingdom): University of Cambridge.</mixed-citation>
</ref>
</ref-list></back>
</article>