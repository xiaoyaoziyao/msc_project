<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-00500</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003311</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories>
<title-group>
<article-title>Stochastic Computations in Cortical Microcircuit Models</article-title>
<alt-title alt-title-type="running-head">Stochastic Computations in Cortical Microcircuits</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Habenschuss</surname><given-names>Stefan</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Jonke</surname><given-names>Zeno</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Maass</surname><given-names>Wolfgang</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
</contrib-group>
<aff id="aff1"><addr-line>Graz University of Technology, Institute for Theoretical Computer Science, Graz, Austria</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Sporns</surname><given-names>Olaf</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Indiana University, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">habenschuss@igi.tugraz.at</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: SH ZJ WM. Performed the experiments: ZJ. Wrote the paper: SH ZJ WM. Theoretical analysis: SH.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>11</month><year>2013</year></pub-date>
<pub-date pub-type="epub"><day>14</day><month>11</month><year>2013</year></pub-date>
<volume>9</volume>
<issue>11</issue>
<elocation-id>e1003311</elocation-id>
<history>
<date date-type="received"><day>26</day><month>3</month><year>2013</year></date>
<date date-type="accepted"><day>22</day><month>8</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2013</copyright-year>
<copyright-holder>Habenschuss et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/3.0/" xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/3.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><related-article id="RA1" related-article-type="companion" ext-link-type="uri" vol="" page="e1002211" xlink:type="simple" xlink:href="info:doi/10.1371/journal.pcbi.1002211"> <article-title>Neural Dynamics as Sampling: A Model for Stochastic Computation in Recurrent Networks of Spiking Neurons</article-title></related-article><related-article id="RA2" related-article-type="companion" ext-link-type="uri" vol="" page="e1002294" xlink:type="simple" xlink:href="info:doi/10.1371/journal.pcbi.1002294"> <article-title>Probabilistic Inference in General Graphical Models through Sampling in Stochastic Networks of Spiking Neurons</article-title></related-article>
<abstract>
<p>Experimental data from neuroscience suggest that a substantial amount of knowledge is stored in the brain in the form of probability distributions over network states and trajectories of network states. We provide a theoretical foundation for this hypothesis by showing that even very detailed models for cortical microcircuits, with data-based diverse nonlinear neurons and synapses, have a stationary distribution of network states and trajectories of network states to which they converge exponentially fast from any initial state. We demonstrate that this convergence holds in spite of the non-reversibility of the stochastic dynamics of cortical microcircuits. We further show that, in the presence of background network oscillations, separate stationary distributions emerge for different phases of the oscillation, in accordance with experimentally reported phase-specific codes. We complement these theoretical results by computer simulations that investigate resulting computation times for typical probabilistic inference tasks on these internally stored distributions, such as marginalization or marginal maximum-a-posteriori estimation. Furthermore, we show that the inherent stochastic dynamics of generic cortical microcircuits enables them to quickly generate approximate solutions to difficult constraint satisfaction problems, where stored knowledge and current inputs jointly constrain possible solutions. This provides a powerful new computing paradigm for networks of spiking neurons, that also throws new light on how networks of neurons in the brain could carry out complex computational tasks such as prediction, imagination, memory recall and problem solving.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>The brain has not only the capability to process sensory input, but it can also produce predictions, imaginations, and solve problems that combine learned knowledge with information about a new scenario. But although these more complex information processing capabilities lie at the heart of human intelligence, we still do not know how they are organized and implemented in the brain. Numerous studies in cognitive science and neuroscience conclude that many of these processes involve probabilistic inference. This suggests that neuronal circuits in the brain process information in the form of probability distributions, but we are missing insight into how complex distributions could be represented and stored in large and diverse networks of neurons in the brain. We prove in this article that realistic cortical microcircuit models can store complex probabilistic knowledge by embodying probability distributions in their inherent stochastic dynamics – yielding a knowledge representation in which typical probabilistic inference problems such as marginalization become straightforward readout tasks. We show that in cortical microcircuit models such computations can be performed satisfactorily within a few <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e001" xlink:type="simple"/></inline-formula>. Furthermore, we demonstrate how internally stored distributions can be programmed in a simple manner to endow a neural circuit with powerful problem solving capabilities.</p>
</abstract>
<funding-group><funding-statement>Written under partial support by the European Union project #FP7-248311 (AMARSi) and project #FP7-269921 (BrainScaleS). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="28"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>The question whether brain computations are inherently deterministic or inherently stochastic is obviously of fundamental importance. Numerous experimental data highlight inherently stochastic aspects of neurons, synapses and networks of neurons on virtually all spatial and temporal scales that have been examined <xref ref-type="bibr" rid="pcbi.1003311-Allen1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1003311-Clarke1">[5]</xref>. A clearly visible stochastic feature of brain activity is the trial-to-trial variability of neuronal responses, which also appears on virtually every spatial and temporal scale that has been examined <xref ref-type="bibr" rid="pcbi.1003311-Faisal1">[2]</xref>. This variability has often been interpreted as side-effect of an implementation of inherently deterministic computing paradigms with noisy elements, and it has been attempted to show that the observed noise can be eliminated through spatial or temporal averaging. However, more recent experimental methods, which make it possible to record simultaneously from many neurons (or from many voxels in fMRI), have shown that the underlying probability distributions of network states during spontaneous activity are highly structured and multimodal, with distinct modes that resemble those encountered during active processing. This has been shown through recordings with voltage-sensitive dyes starting with <xref ref-type="bibr" rid="pcbi.1003311-Tsodyks1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Kenet1">[7]</xref>, multi-electrode arrays <xref ref-type="bibr" rid="pcbi.1003311-Luczak1">[8]</xref>, and fMRI <xref ref-type="bibr" rid="pcbi.1003311-Raichle1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Lewis1">[10]</xref>. It was also shown that the intrinsic trial-to-trial variability of brain systems is intimately related to the observed trial-to-trial variability in behavior (see e.g. <xref ref-type="bibr" rid="pcbi.1003311-Fox1">[11]</xref>). Furthermore, in <xref ref-type="bibr" rid="pcbi.1003311-Kelemen1">[12]</xref> it was shown that during navigation in a complex environment where simultaneously two spatial frames of reference were relevant, the firing of neurons in area <italic>CA1</italic> represented both frames in alternation, so that coactive neurons tended to relate to a common frame of reference. In addition it has been shown that in a situation where sensory stimuli are ambiguous, large brain networks switch stochastically between alternative interpretations or percepts, see <xref ref-type="bibr" rid="pcbi.1003311-Leopold1">[13]</xref>–<xref ref-type="bibr" rid="pcbi.1003311-Kim1">[15]</xref>. Furthermore, an increase in the volatility of network states has been shown to accompany episodes of behavioral uncertainty <xref ref-type="bibr" rid="pcbi.1003311-Karlsson1">[16]</xref>. All these experimental data point to inherently stochastic aspects in the organization of brain computations, and more specifically to an important computational role of spontaneously varying network states of smaller and larger networks of neurons in the brain. However, one should realize that the approach to stochastic computation that we examine in this article does not postulate that all brain activity is stochastic or unreliable, since reliable neural responses can be represented by probabilities close to 1.</p>
<p>The goal of this article is to provide a theoretical foundation for understanding stochastic computations in networks of neurons in the brain, in particular also for the generation of structured spontaneous activity. To this end, we prove here that even biologically realistic models <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e002" xlink:type="simple"/></inline-formula> for networks of neurons in the brain have – for a suitable definition of network state – a unique stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e003" xlink:type="simple"/></inline-formula> of network states. Previous work had focused in this context on neuronal models with linear sub-threshold dynamics <xref ref-type="bibr" rid="pcbi.1003311-Brmaud1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Borovkov1">[18]</xref> and constant external input (e.g. constant input firing rates). However, we show here that this holds even for quite realistic models that reflect, for example, data on nonlinear dendritic integration (dendritic spikes), synapses with data-based short term dynamics (i.e., individual mixtures of depression and facilitation), and different types of neurons on specific laminae. We also show that these results are not restricted to the case of constant external input, but rather can be extended to periodically changing input, and to input generated by arbitrary ergodic stochastic processes.</p>
<p>Our theoretical results imply that virtually any data-based model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e004" xlink:type="simple"/></inline-formula>, for networks of neurons featuring realistic neuronal noise sources (e.g. stochastic synaptic vesicle release) implements a Markov process through its stochastic dynamics. This can be interpreted – in spite of its non-reversibility – as a form of sampling from a unique stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e005" xlink:type="simple"/></inline-formula>. One interpretation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e006" xlink:type="simple"/></inline-formula>, which is in principle consistent with our findings, is that it represents the posterior distribution of a Bayesian inference operation <xref ref-type="bibr" rid="pcbi.1003311-Hoyer1">[19]</xref>–<xref ref-type="bibr" rid="pcbi.1003311-Pecevski1">[22]</xref>, in which the current input (evidence) is combined with prior knowledge encoded in network parameters such as synaptic weights or intrinsic excitabilities of neurons (see <xref ref-type="bibr" rid="pcbi.1003311-Friston1">[23]</xref>–<xref ref-type="bibr" rid="pcbi.1003311-Doya1">[26]</xref> for an introduction to the “Bayesian brain”). This interpretation of neural dynamics as sampling from a posterior distribution is intriguing, as it implies that various results of probabilistic inference could then be easily obtained by a simple readout mechanism: For example, posterior marginal probabilities can be estimated (approximately) by observing the number of spikes of specific neurons within some time window (see related data from parietal cortex <xref ref-type="bibr" rid="pcbi.1003311-Huk1">[27]</xref>). Furthermore, an approximate maximal a posteriori (MAP) inference can be carried out by observing which network states occur more often, and/or are more persistent.</p>
<p>A crucial issue which arises is whether reliable readouts from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e007" xlink:type="simple"/></inline-formula> in realistic cortical microcircuit models can be obtained quickly enough to support, e.g., fast decision making in downstream areas. This critically depends on the speed of convergence of the distribution of network states (or distribution of trajectories of network states) from typical initial network states to the stationary distribution. Since the initial network state of a cortical microcircuit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e008" xlink:type="simple"/></inline-formula> depends on past activity, it may often be already quite “close” to the stationary distribution when a new input arrives (since past inputs are likely related to the new input). But it is also reasonable to assume that the initial state of the network is frequently unrelated to the stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e009" xlink:type="simple"/></inline-formula>, for example after drastic input changes. In this case the time required for readouts depends on the expected convergence speed to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e010" xlink:type="simple"/></inline-formula> from – more or less – <italic>arbitrary</italic> initial states. We show that one can prove exponential upper bounds for this convergence speed. But even that does not guarantee fast convergence for a concrete system, because of constant factors in the theoretical upper bound. Therefore we complement this theoretical analysis of the convergence speed by extensive computer simulations for cortical microcircuit models.</p>
<p>The notion of a cortical microcircuit arose from the observation that “it seems likely that there is a basically uniform microcircuit pattern throughout the neocortex upon which certain specializations unique to this or that cortical area are superimposed” <xref ref-type="bibr" rid="pcbi.1003311-Mountcastle1">[28]</xref>. This notion is not precisely defined, but rather a term of convenience: It refers to network models that are sufficiently large to contain examples of the main types of experimentally observed neurons on specific laminae, and the main types of experimentally observed synaptic connections between different types of neurons on different laminae, ideally in statistically representative numbers <xref ref-type="bibr" rid="pcbi.1003311-Douglas1">[29]</xref>. Computer simulations of cortical microcircuit models are practically constrained both by a lack of sufficiently many consistent data from a single preparation and a single cortical area, and by the available computer time. In the computer simulations for this article we have focused on a relatively simple standard model for a cortical microcircuit in the somatosensory cortex <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref> that has already been examined in some variations in previous studies from various perspectives <xref ref-type="bibr" rid="pcbi.1003311-Haeusler2">[31]</xref>–<xref ref-type="bibr" rid="pcbi.1003311-Bastos1">[34]</xref>.</p>
<p>We show that for this standard model of a cortical microcircuit marginal probabilities for single random variables (neurons) can be estimated through sampling even for fairly large instances with 5000 neurons within a few <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e011" xlink:type="simple"/></inline-formula> of simulated biological time, hence well within the range of experimentally observed computation times of biological organisms. The same holds for probabilities of network states for small sub-networks. Furthermore, we show that at least for sizes up to 5000 neurons these “computation times” are virtually independent of the size of the microcircuit model.</p>
<p>We also address the question to which extent our theoretical framework can be applied in the context of periodic input, for example in the presence of background theta oscillations <xref ref-type="bibr" rid="pcbi.1003311-Dragoi1">[35]</xref>. In contrast to the stationary input case, we show that the presence of periodic input leads to the emergence of unique <italic>phase-specific</italic> stationary distributions, i.e., a separate unique stationary distribution for each phase of the periodic input. We discuss basic implications of this result and relate our findings to experimental data on theta-paced path sequences <xref ref-type="bibr" rid="pcbi.1003311-Dragoi1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Gupta1">[36]</xref> and bi-stable activity <xref ref-type="bibr" rid="pcbi.1003311-Jezek1">[37]</xref> in hippocampus.</p>
<p>Finally, our theoretically founded framework for stochastic computations in networks of spiking neurons also throws new light on the question how complex constraint satisfaction problems could be solved by cortical microcircuits <xref ref-type="bibr" rid="pcbi.1003311-Hinton1">[38]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Davenport1">[39]</xref>. We demonstrate this in a toy example for the popular puzzle game Sudoku. We show that the constraints of this problem can be easily encoded by synaptic connections between excitatory and inhibitory neurons in such a way that the stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e012" xlink:type="simple"/></inline-formula> assigns particularly high probability to those network states which encode correct (or good approximate) solutions to the problem. The resulting network dynamics can also be understood as parallel stochastic search with anytime computing properties: Early network states provide very fast heuristic solutions, while later network states are distributed according to the stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e013" xlink:type="simple"/></inline-formula>, therefore visiting with highest probability those solutions which violate only a few or zero constraints.</p>
<p>In order to make the results of this article accessible to non-theoreticians we present in the subsequent <xref ref-type="sec" rid="s2">Results</xref> section our main findings in a less technical formulation that emphasizes relationships to experimental data. Rigorous mathematical definitions and proofs can be found in the <xref ref-type="sec" rid="s4">Methods</xref> section, which has been structured in the same way as the <xref ref-type="sec" rid="s2">Results</xref> section in order to facilitate simultaneous access on different levels of detail.</p>
</sec><sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Network states and distributions of network states</title>
<p>A simple notion of network state at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e014" xlink:type="simple"/></inline-formula> simply indicates which neurons in the network fired within some short time window before <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e015" xlink:type="simple"/></inline-formula>. For example, in <xref ref-type="bibr" rid="pcbi.1003311-Berkes1">[20]</xref> a window size of 2ms was selected. However, the full network state could not be analyzed there experimentally, only its projection onto 16 electrodes in area V1 from which recordings were made. An important methodological innovation of <xref ref-type="bibr" rid="pcbi.1003311-Berkes1">[20]</xref> was to analyze under various conditions the probability distribution of the recorded fragments of network states, i.e., of the resulting bit vectors of length 16 (with a “1” at position <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e016" xlink:type="simple"/></inline-formula> if a spike was recorded during the preceding 2ms at electrode <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e017" xlink:type="simple"/></inline-formula>). In particular, it was shown that during development the distribution over these <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e018" xlink:type="simple"/></inline-formula> network states during spontaneous activity in darkness approximates the distribution recorded during natural vision. Apart from its functional interpretation, this result also raises the even more fundamental question how a network of neurons in the brain can represent and generate a complex distribution of network states. This question is addressed here in the context of data-based models <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e019" xlink:type="simple"/></inline-formula> for cortical microcircuits. We consider notions of network states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e020" xlink:type="simple"/></inline-formula> similar to <xref ref-type="bibr" rid="pcbi.1003311-Berkes1">[20]</xref> (see the simple state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e021" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1C</xref>) and provide a rigorous proof that under some mild assumptions any such model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e022" xlink:type="simple"/></inline-formula> represents and generates for different external inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e023" xlink:type="simple"/></inline-formula> associated different internal distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e024" xlink:type="simple"/></inline-formula> of network states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e025" xlink:type="simple"/></inline-formula>. More precisely, we will show that for any specific input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e026" xlink:type="simple"/></inline-formula> there exists a unique stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e027" xlink:type="simple"/></inline-formula> of network states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e028" xlink:type="simple"/></inline-formula> to which the network converges exponentially fast from any initial state.</p>
<fig id="pcbi-1003311-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003311.g001</object-id><label>Figure 1</label><caption>
<title>Network states and stationary distributions of network states in a cortical microcircuit model.</title>
<p><bold>A</bold>. Data-based cortical microcircuit template from Cereb. Cortex (2007) 17: 149-162 <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e029" xlink:type="simple"/></inline-formula> reprinted by permission of the authors and Oxford University Press. <bold>B</bold>. A small instantiation of this model consisting of 10 network neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e030" xlink:type="simple"/></inline-formula> and 2 additional input neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e031" xlink:type="simple"/></inline-formula>. Neurons are colored by type (blue:input, black:excitatory, red:inhibitory). Line width represents synaptic efficacy. The synapse from neuron 8 to 7 is removed for the simulation described in E. <bold>C</bold>. Notions of network state considered in this article. Markov states are defined by the exact timing of all recent spikes within some time window <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e032" xlink:type="simple"/></inline-formula>, shown here for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e033" xlink:type="simple"/></inline-formula>. Simple states only record which neurons fired recently (0 = no spike, 1 = at least one spike within a short window <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e034" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e035" xlink:type="simple"/></inline-formula> throughout this figure). <bold>D</bold>. Empirically measured stationary distribution of simple network states. Shown is the marginal distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e036" xlink:type="simple"/></inline-formula> for a subset of three neurons 2,7,8 (their spikes are shown in C in black), under two different input conditions (input pattern 1: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e037" xlink:type="simple"/></inline-formula> firing at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e038" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e039" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e040" xlink:type="simple"/></inline-formula>, input pattern 2: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e041" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e042" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e043" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e044" xlink:type="simple"/></inline-formula>). The distribution for each input condition was obtained by measuring the relative time spent in each of the simple states (0,0,0), …, (1,1,1) in a single long trial (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e045" xlink:type="simple"/></inline-formula>). The zero state (0,0,0) is not shown. <bold>E</bold>. Effect of removing one synapse, from neuron 8 to neuron 7, on the stationary distribution of network states (input pattern 1 was presented). <bold>F</bold>. Illustration of trial-to-trial variability in the small cortical microcircuit (input pattern 1). Two trials starting from identical initial network states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e046" xlink:type="simple"/></inline-formula> are shown. Blue bars at the bottom of each trial mark periods where the subnetwork of neurons 2,7,8 was in simple state (1,1,1) at this time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e047" xlink:type="simple"/></inline-formula>. Note that the “blue” initial Markov state is shown only partially: it is actually longer and comprises all neurons in the network (as in panel C, but with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e048" xlink:type="simple"/></inline-formula>). <bold>G</bold>. Two trials starting from a different (“red”) initial network state. Red bars denote periods of state (1,1,1) for “red” trials. <bold>H</bold>. Convergence to the stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e049" xlink:type="simple"/></inline-formula> in this small cortical microcircuit is fast and independent of the initial state: This is illustrated for the relative frequency of simple state (1,1,1) within the first <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e050" xlink:type="simple"/></inline-formula> after input onset. The blue/red line shows the relative frequency of simple state (1,1,1) at each time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e051" xlink:type="simple"/></inline-formula> estimated from many (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e052" xlink:type="simple"/></inline-formula>) “blue”/“red” trials. The relative frequency of simple state (1,1,1) rapidly converges to its stationary value denoted by the symbol <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e053" xlink:type="simple"/></inline-formula> (marked also in panels D and E). The relative frequency converges to the same value regardless of the initial state (blue/red).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003311.g001" position="float" xlink:type="simple"/></fig>
<p>This result can be derived within the theory of Markov processes on general state spaces, an extension of the more familiar theory of Markov chains on finite state spaces to continuous time and infinitely many network states. Another important difference to typical Markov chains (e.g. the dynamics of Gibbs sampling in Boltzmann machines) is that the Markov processes describing the stochastic dynamics of cortical microcircuit models are non-reversible. This is a well-known difference between simple neural network models and networks of spiking neurons in the brain, where a spike of a neuron causes postsynaptic potentials in other neurons - but not vice versa. In addition, experimental results show that brain networks tend to have a non-reversible dynamics also on longer time scales (e.g., stereotypical trajectories of network states <xref ref-type="bibr" rid="pcbi.1003311-Abeles1">[40]</xref>–<xref ref-type="bibr" rid="pcbi.1003311-Luczak3">[43]</xref>).</p>
<p>In order to prove results on the existence of stationary distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e054" xlink:type="simple"/></inline-formula> of network states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e055" xlink:type="simple"/></inline-formula>, one first needs to consider a more complex notion of network state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e056" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e057" xlink:type="simple"/></inline-formula>, which records the history of all spikes in the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e058" xlink:type="simple"/></inline-formula> since time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e059" xlink:type="simple"/></inline-formula> (see <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1C</xref>). The window length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e060" xlink:type="simple"/></inline-formula> has to be chosen sufficiently large so that the influence of spikes before time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e061" xlink:type="simple"/></inline-formula> on the dynamics of the network after time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e062" xlink:type="simple"/></inline-formula> can be neglected. This more complex notion of network state then fulfills the <italic>Markov property</italic>, such that the future network evolution depends on the past only through the current Markov state. The existence of a window length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e063" xlink:type="simple"/></inline-formula> with the Markov property is a basic assumption of the subsequent theoretical results. For standard models of networks of spiking neurons a value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e064" xlink:type="simple"/></inline-formula> around 100ms provides already a good approximation of the Markov property, since this is a typical time during which a post-synaptic potential has a non-negligible effect at the soma of a post-synaptic neuron. For more complex models of networks of spiking neurons a larger value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e065" xlink:type="simple"/></inline-formula> in the range of seconds is more adequate, in order to accommodate for dendritic spikes or the activation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e066" xlink:type="simple"/></inline-formula> receptors that may last 100ms or longer, and the short term dynamics of synapses with time constants of several hundred milliseconds. Fortunately, once the existence of a stationary distribution is proved for such more complex notion of network state, it also holds for any simpler notion of network state (even if these simpler network states do not fulfill the Markov property), that results when one ignores details of the more complex network states. For example, one can ignore all spikes before time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e067" xlink:type="simple"/></inline-formula>, the exact firing times within the window from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e068" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e069" xlink:type="simple"/></inline-formula>, and whether a neuron fired one or several spikes. In this way one arrives back at the simple notion of network state from <xref ref-type="bibr" rid="pcbi.1003311-Berkes1">[20]</xref>.</p>
<sec id="s2a1">
<title>Theorem 1 (Exponentially fast convergence to a stationary distribution)</title>
<p><italic>Let </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e070" xlink:type="simple"/></inline-formula><italic> be an arbitrary model for a network of spiking neurons with stochastic synaptic release or some other mechanism for stochastic firing. </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e071" xlink:type="simple"/></inline-formula><italic> may consist of complex multi-compartment neuron models with nonlinear dendritic integration (including dendritic spikes) and heterogeneous synapses with differential short term dynamics. We assume that this network </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e072" xlink:type="simple"/></inline-formula><italic> receives external inputs from a set of input neurons </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e073" xlink:type="simple"/></inline-formula><italic> which fire according to Poisson processes at different rates </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e074" xlink:type="simple"/></inline-formula><italic>. The vector </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e075" xlink:type="simple"/></inline-formula><italic> of input rates can be either constant over time (</italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e076" xlink:type="simple"/></inline-formula><italic>), or generated by any external Markov process that converges exponentially fast to a stationary distribution.</italic></p>
<p><italic>Then there exists a stationary distribution </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e077" xlink:type="simple"/></inline-formula><italic> of network states </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e078" xlink:type="simple"/></inline-formula><italic>, to which the stochastic dynamics of </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e079" xlink:type="simple"/></inline-formula><italic> converges from any initial state of the network exponentially fast. Accordingly, the distribution of subnetwork states </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e080" xlink:type="simple"/></inline-formula><italic> of any subset of neurons converges exponentially fast to the marginal distribution </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e081" xlink:type="simple"/></inline-formula><italic> of this subnetwork.</italic></p>
<p>Note that Theorem 1 states that the network embodies not only the joint distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e082" xlink:type="simple"/></inline-formula> over all neurons, but simultaneously all marginal distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e083" xlink:type="simple"/></inline-formula> over all possible subsets of neurons. This property follows naturally from the fact that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e084" xlink:type="simple"/></inline-formula> is represented in a sample-based manner <xref ref-type="bibr" rid="pcbi.1003311-Fiser1">[25]</xref>. As a consequence, if one is interested in estimating the marginal distribution of some subset of neurons rather than the full joint distribution, it suffices to observe the activity of the particular subnetwork of interest (while ignoring the remaining network). This is remarkable insofar, as the exact computation of marginal probabilities is in general known to be quite difficult (even NP-complete <xref ref-type="bibr" rid="pcbi.1003311-Koller1">[44]</xref>).</p>
<p>Theorem 1 requires that neurons fire stochastically. More precisely, a basic assumption required for Theorem 1 is that the network behaves sufficiently stochastic at <italic>any point in time</italic>, in the sense that the probability that a neuron fires in an interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e085" xlink:type="simple"/></inline-formula> must be smaller than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e086" xlink:type="simple"/></inline-formula> for any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e087" xlink:type="simple"/></inline-formula>. This is indeed fulfilled by any stochastic neuron model as long as instantaneous firing rates remain bounded. It is also fulfilled by any deterministic neuron model if synaptic transmission is modeled via stochastic vesicle release with bounded release rates. Another assumption is that long-term plasticity and other long-term memory effects have a negligible impact on the network dynamics on shorter timescales which are the focus of this article (milliseconds to a few seconds). Precise mathematical definitions of all assumptions and notions involved in Theorem 1 as well as proofs can be found in <xref ref-type="sec" rid="s4">Methods</xref> (see Lemma 2 and 3).</p>
<p>An illustration for Theorem 1 is given in <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1</xref>. We use as our running example for a cortical microcircuit model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e088" xlink:type="simple"/></inline-formula> the model of <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref> shown in <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1A</xref>, which consists of three populations of excitatory and three populations of inhibitory neurons on specific laminae. Average strength of synaptic connections (measured as mean amplitude of postsynaptic potentials at the soma in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e089" xlink:type="simple"/></inline-formula>, and indicated by the numbers at the arrows in <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1A</xref>) as well as the connection probability (indicated in parentheses at each arrow as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e090" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1A</xref>) are based in this model on intracellular recordings from 998 pairs of identified neurons from the Thomson Lab <xref ref-type="bibr" rid="pcbi.1003311-Thomson1">[45]</xref>. The thickness of arrows in <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1A</xref> reflects the products of those two numbers for each connection. The nonlinear short-term dynamics of each type of synaptic connection was modeled according to data from the Markram Lab <xref ref-type="bibr" rid="pcbi.1003311-Gupta2">[46]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Markram1">[47]</xref>. Neuronal integration and spike generation was modeled by a conductance-based leaky-integrate-and-fire model, with a stochastic spiking mechanism based on <xref ref-type="bibr" rid="pcbi.1003311-Jolivet1">[48]</xref>. See <xref ref-type="sec" rid="s4">Methods</xref> for details.</p>
<p>The external input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e091" xlink:type="simple"/></inline-formula> consists in a cortical microcircuit of inputs from higher cortical areas that primarily target neurons in superficial layers, and bottom-up inputs that arrive primarily in layer 4, but also on other layers (details tend to depend on the cortical area and the species). We model two input streams in a qualitative manner as in <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref>. Also background synaptic input is modeled according to <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref>.</p>
<p><xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1B</xref> shows a small instantiation of this microcircuit template consisting of 10 neurons (we had to manually tune a few connections in this circuit to facilitate visual clarity of subsequent panels). The impact of different external inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e092" xlink:type="simple"/></inline-formula> and of a single synaptic connection from neuron 8 to neuron 7 on the stationary distribution is shown in <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1D and E</xref>, respectively (shown is the marginal distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e093" xlink:type="simple"/></inline-formula> of a subset of three neurons 2,7 and 8). This illustrates that the structure and dynamics of a circuit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e094" xlink:type="simple"/></inline-formula> are intimately linked to properties of its stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e095" xlink:type="simple"/></inline-formula>. In fact, we argue that the stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e096" xlink:type="simple"/></inline-formula> (more precisely: the stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e097" xlink:type="simple"/></inline-formula> for all relevant external inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e098" xlink:type="simple"/></inline-formula>) can be viewed as a mathematical model for the most salient aspects of stochastic computations in a circuit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e099" xlink:type="simple"/></inline-formula>.</p>
<p>The influence of the initial network state on the first <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e100" xlink:type="simple"/></inline-formula> ms of network response is shown in <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1F and G</xref> for representative trials starting from two different initial Markov states (blue/red, two trials shown for each). Variability among trials arises from the inherent stochasticity of neurons and the presence of background synaptic input. <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1H</xref> is a concrete illustration of Theorem 1: it shows that the relative frequency of a specific network state (1,1,1) in a subset of the three neurons 2,7 and 8 converges quickly to its stationary value. Furthermore, it converges to this (same) value regardless of the initial network state (blue/red).</p>
</sec></sec><sec id="s2b">
<title>Stationary distributions of trajectories of network states</title>
<p>Theorem 1 also applies to networks which generate stereotypical trajectories of network activity <xref ref-type="bibr" rid="pcbi.1003311-Luczak2">[41]</xref>. For such networks it may be of interest to consider not only the distribution of network states in a short window (e.g. simple states with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e101" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e102" xlink:type="simple"/></inline-formula>), but also the distribution of longer trajectories produced by the network. Indeed, since Theorem 1 holds for Markov states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e103" xlink:type="simple"/></inline-formula> with any fixed window length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e104" xlink:type="simple"/></inline-formula>, it also holds for values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e105" xlink:type="simple"/></inline-formula> that are in the range of experimentally observed trajectories of network states <xref ref-type="bibr" rid="pcbi.1003311-Luczak2">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Mazor1">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Harvey1">[50]</xref>. Hence, a generic neural circuit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e106" xlink:type="simple"/></inline-formula> automatically has a unique stationary distribution over <italic>trajectories</italic> of (simple) network states for any fixed trajectory length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e107" xlink:type="simple"/></inline-formula>. Note that this implies that a neural circuit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e108" xlink:type="simple"/></inline-formula> has simultaneously stationary distributions of trajectories of (simple) network states of various lengths for arbitrarily large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e109" xlink:type="simple"/></inline-formula>, and a stationary distribution of simple network states. This fact is not surprising if one takes into consideration that if a circuit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e110" xlink:type="simple"/></inline-formula> has a stationary distribution over simple network states this does <italic>not</italic> imply that subsequent simple network states represent independent drawings from this stationary distribution. Hence the circuit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e111" xlink:type="simple"/></inline-formula> may very well produce stereotypical trajectories of simple network states. This feature becomes even more prominent if the underlying dynamics (the Markov process) of the neural circuit is non-reversible on several time scales.</p>
</sec><sec id="s2c">
<title>Extracting knowledge from internally stored distributions of network states</title>
<p>We address two basic types of knowledge extraction from a stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e112" xlink:type="simple"/></inline-formula> of a network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e113" xlink:type="simple"/></inline-formula>: the computation of <italic>marginal probabilities</italic> and <italic>maximal a posteriori (MAP) assignments</italic>. Both computations constitute basic inference problems commonly appearing in real-world applications <xref ref-type="bibr" rid="pcbi.1003311-Wainwright1">[51]</xref>, which are in general difficult to solve as they involve large sums, integrals, or maximization steps over a state space which grows exponentially in the number of random variables. However, already <xref ref-type="bibr" rid="pcbi.1003311-Buesing1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Fiser1">[25]</xref> noted that the <italic>estimation</italic> of marginal probabilities would become straightforward if distributions were represented in the brain in a sample-based manner (such that each network state at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e114" xlink:type="simple"/></inline-formula> represents one sample from the distribution). Theorem 1 provides a theoretical foundation for how such a representation could emerge in realistic data-based microcircuit models on the implementation level: Once the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e115" xlink:type="simple"/></inline-formula> has converged to its stationary distribution, the network state at any time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e116" xlink:type="simple"/></inline-formula> represents a sample from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e117" xlink:type="simple"/></inline-formula> (although subsequent samples are generally not independent). Simultaneously, the subnetwork state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e118" xlink:type="simple"/></inline-formula> of any subset of neurons represents a sample from the marginal distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e119" xlink:type="simple"/></inline-formula>. This is particularly relevant if one interprets <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e120" xlink:type="simple"/></inline-formula> in a given cortical microcircuit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e121" xlink:type="simple"/></inline-formula> as the posterior distribution of an implicit generative model, as suggested for example by <xref ref-type="bibr" rid="pcbi.1003311-Berkes1">[20]</xref> or <xref ref-type="bibr" rid="pcbi.1003311-Buesing1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Pecevski1">[22]</xref>.</p>
<p>In order to place the estimation of marginals into a biologically relevant context, assume that a particular component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e122" xlink:type="simple"/></inline-formula> of the network state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e123" xlink:type="simple"/></inline-formula> has a behavioral relevance. This variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e124" xlink:type="simple"/></inline-formula>, represented by some neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e125" xlink:type="simple"/></inline-formula>, could represent for example the perception of a particular visual object (if neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e126" xlink:type="simple"/></inline-formula> is located in inferior temporal cortex <xref ref-type="bibr" rid="pcbi.1003311-Zhang1">[52]</xref>), or the intention to make a saccade into a specific part of the visual field (if neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e127" xlink:type="simple"/></inline-formula> is located in area LIP <xref ref-type="bibr" rid="pcbi.1003311-Shadlen1">[53]</xref>). Then the computation of the marginal<disp-formula id="pcbi.1003311.e128"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e128" xlink:type="simple"/><label>(1)</label></disp-formula>would be of behavioral significance. Note that this computation integrates information from the internally stored knowledge <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e129" xlink:type="simple"/></inline-formula> with evidence about a current situation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e130" xlink:type="simple"/></inline-formula>. In general this computation is demanding as it involves a sum with exponentially many terms in the network size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e131" xlink:type="simple"/></inline-formula>.</p>
<p>But according to Theorem 1, the correct marginal distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e132" xlink:type="simple"/></inline-formula> is automatically embodied by the activity of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e133" xlink:type="simple"/></inline-formula>. Hence the marginal probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e134" xlink:type="simple"/></inline-formula> can be estimated by simply observing what fraction of time the neuron spends in the state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e135" xlink:type="simple"/></inline-formula>, while ignoring the activity of the remaining network <xref ref-type="bibr" rid="pcbi.1003311-Buesing1">[21]</xref>. In principle, a downstream neuron could gather this information by integrating the spike output of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e136" xlink:type="simple"/></inline-formula> over time.</p>
<p>Marginal probabilities of subpopulations, for example <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e137" xlink:type="simple"/></inline-formula>, can be estimated in a similar manner by keeping track of how much time the subnetwork spends in the state (1,0,1), while ignoring the activity of the remaining neurons. A downstream network could gather this information, for example, by integrating over the output of a readout neuron which is tuned to detect the desired target pattern (1,0,1).</p>
<p>Notably, the estimation of marginals sketched above is guaranteed by ergodic theory to converge to the correct probability as observation time increases (due to Theorem 1 which ensures that the network is an ergodic Markov process, see <xref ref-type="sec" rid="s4">Methods</xref>). In particular, this holds true even for networks with prominent sequential dynamics featuring, for example, stereotypical trajectories. However, note that the observation time required to obtain an accurate estimate may be longer when trajectories are present since subsequent samples gathered from such a network will likely exhibit stronger dependencies than in networks lacking sequential activity patterns. In a practical readout implementation where recent events might be weighed preferentially this could result in more noisy estimates.</p>
<p>Approximate maximal a posteriori (MAP) assignments to small subsets of variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e138" xlink:type="simple"/></inline-formula> can also be obtained in a quite straightforward manner. For given external inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e139" xlink:type="simple"/></inline-formula>, the marginal MAP assignment to the subset of variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e140" xlink:type="simple"/></inline-formula> (with some <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e141" xlink:type="simple"/></inline-formula>) is defined as the set of values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e142" xlink:type="simple"/></inline-formula> that maximize<disp-formula id="pcbi.1003311.e143"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e143" xlink:type="simple"/><label>(2)</label></disp-formula></p>
<p>A sample-based approximation of this operation can be implemented by keeping track of which network states in the subnetwork <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e144" xlink:type="simple"/></inline-formula> occur most often. This could, for example, be realized by a readout network in a two stage process: first the marginal probabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e145" xlink:type="simple"/></inline-formula> of all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e146" xlink:type="simple"/></inline-formula> subnetwork states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e147" xlink:type="simple"/></inline-formula> are estimated (by 8 readout neurons dedicated to that purpose), followed by the selection of the neuron with maximal probability. The selection of the maximum could be achieved in a neural network, for example, through competitive inhibition. Such competitive inhibition would ideally lead to a winner-take-all function such that the neuron with the strongest stimulation (representing the variable assignment with the largest probability) dominates and suppresses all other readout neurons.</p>
</sec><sec id="s2d">
<title>Estimates of the required computation time</title>
<p>Whereas many types of computations (for example probabilistic inference via the junction tree algorithm <xref ref-type="bibr" rid="pcbi.1003311-Wainwright1">[51]</xref>) require a certain computation time, probabilistic inference via sampling from an embodied distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e148" xlink:type="simple"/></inline-formula> belongs to the class of <italic>anytime computing</italic> methods, where rough estimates of the result of a computation become almost immediately available, and are automatically improved when there is more time for a decision. A main component of the convergence time to a reliable result arises from the time which the distribution of network states needs to become independent of its initial state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e149" xlink:type="simple"/></inline-formula>. It is well known that both, network states of neurons in the cortex <xref ref-type="bibr" rid="pcbi.1003311-Arieli1">[54]</xref> and quick decisions of an organism, are influenced for a short time by this initial state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e150" xlink:type="simple"/></inline-formula> (and this temporary dependence on the initial state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e151" xlink:type="simple"/></inline-formula> may in fact have some behavioral advantage, since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e152" xlink:type="simple"/></inline-formula> may contain information about preceding network inputs, expectations, etc.). But it has remained unknown, what range of convergence speeds for inference from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e153" xlink:type="simple"/></inline-formula> is produced by common models for cortical microcircuits <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e154" xlink:type="simple"/></inline-formula>.</p>
<p>We address this question by analyzing the convergence speed of stochastic computations in the cortical microcircuit model of <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref>. A typical network response of an instance of the cortical microcircuit model comprising 560 neurons as in <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref> is shown in <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2A</xref>. We first checked how fast marginal probabilities for single neurons converge to stationary values from different initial network Markov states. We applied the same analysis as in <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1H</xref> to the simple state (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e155" xlink:type="simple"/></inline-formula>) of a single representative neuron from layer 5. <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2B</xref> shows quite fast convergence of the “on”-state probability of the neuron to its stationary value from two different initial states. Note that this straightforward method of checking convergence is rather inefficient, as it requires the repetition of a large number of trials for each initial state. In addition it is not suitable for analyzing convergence to marginals for subpopulations of neurons (see <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2G</xref>).</p>
<fig id="pcbi-1003311-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003311.g002</object-id><label>Figure 2</label><caption>
<title>Fast convergence of marginals of single neurons and more complex quantities in a cortical microcircuit model.</title>
<p><bold>A</bold>. Typical spike response of the microcircuit model based on <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref> comprising 560 stochastic point neurons. Spikes of inhibitory neurons are indicated in red. <bold>B</bold>. Fast convergence of a marginal for a representative layer 5 neuron (frequency of “on”-state, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e156" xlink:type="simple"/></inline-formula>) to its stationary value, shown for two different initial Markov states (blue/red). Statistics were obtained for each initial state from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e157" xlink:type="simple"/></inline-formula> trials. <bold>C</bold>. Gelman-Rubin convergence diagnostic was applied to the marginals of all single neurons (simple states, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e158" xlink:type="simple"/></inline-formula>). In all neurons the Gelman-Rubin value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e159" xlink:type="simple"/></inline-formula> drops to a value close to 1 within a few <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e160" xlink:type="simple"/></inline-formula>, suggesting generally fast convergence of single neuron marginals (shown are 20 randomly chosen neurons; see panel E for a summary of all neurons). The shaded area below 1.1 indicates a range where one commonly assumes that convergence has taken place. <bold>D</bold>. Convergence speed of pairwise spike coincidences (simple states (1,1) of two neurons, 20 randomly chosen pairs of neurons) is comparable to marginal convergence. <bold>E</bold>. Summary of marginal convergence analysis for single neurons in C: Mean (solid) and worst (dashed line) marginal convergence of all 560 neurons. Mean/worst convergence is reached after a few <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e161" xlink:type="simple"/></inline-formula>. <bold>F</bold>. Convergence analysis was applied to networks of different sizes (500–5000 neurons). Mean and worst marginal convergence of single neurons are hardly affected by network size. <bold>G</bold>. Convergence properties of populations of neurons. Dotted: multivariate Gelman-Rubin analysis was applied to a subpopulation of 30 neurons (5 neurons were chosen randomly from each pool). Solid: convergence of a “random readout” neuron which receives spike inputs from 500 randomly chosen neurons in the microcircuit. It turns out that the convergence speed of such a generic readout neuron is even slightly faster than for neurons within the microcircuit (compare with panel E). A remarkable finding is that in all these cases the network size does not affect convergence speed.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003311.g002" position="float" xlink:type="simple"/></fig>
<p>Various more efficient <italic>convergence diagnostics</italic> have been proposed in the context of discrete-time Markov Chain Monte Carlo theory <xref ref-type="bibr" rid="pcbi.1003311-Gelman1">[55]</xref>–<xref ref-type="bibr" rid="pcbi.1003311-Gjoka1">[58]</xref>. In the following, we have adopted the Gelman and Rubin diagnostic, one of the standard methods in applications of MCMC sampling <xref ref-type="bibr" rid="pcbi.1003311-Gelman1">[55]</xref>. The Gelman Rubin convergence diagnostic is based on the comparison of many runs of a Markov chain when started from different randomly drawn initial states. In particular, one compares the typical variance of state distributions during the time interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e162" xlink:type="simple"/></inline-formula> within a single run (within-variance) to the variance during the interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e163" xlink:type="simple"/></inline-formula> between different runs (between-variance). When the ratio <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e164" xlink:type="simple"/></inline-formula> of between- and within-variance approaches 1 this is indicative of convergence. A comparison of panels B and C of <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2</xref> shows that in the case of marginals for single neurons this interpretation fits very well to the empirically observed convergence speed for two different initial conditions. Various values between 1.02 <xref ref-type="bibr" rid="pcbi.1003311-Gjoka1">[58]</xref> and 1.2 <xref ref-type="bibr" rid="pcbi.1003311-Brooks1">[57]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Kass1">[59]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Gelman2">[60]</xref> have been proposed in the literature as thresholds below which the ratio <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e165" xlink:type="simple"/></inline-formula> signals that convergence has taken place. The shaded region in <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2C–G</xref> corresponds to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e166" xlink:type="simple"/></inline-formula> values below a threshold of 1.1. An obvious advantage of the Gelman-Rubin diagnostic, compared with a straightforward empirical evaluation of convergence properties as in <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2B</xref>, is its substantially larger computational efficiency and the larger number of initial states that it takes into account. For the case of multivariate marginals (see <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2G</xref>), a straightforward empirical evaluation of convergence is not even feasible, since relative frequencies of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e167" xlink:type="simple"/></inline-formula> states would have to be analyzed.</p>
<p>Using the Gelman-Rubin diagnostic, we estimated convergence speed for marginals of single neurons (see <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2C</xref>, mean/worst in <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2E</xref>), and for the product of the simple states of two neurons (i.e., pairwise spike coincidences) in <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2D</xref>. We found that in all cases the Gelman-Rubin value drops close to 1 within just a few <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e168" xlink:type="simple"/></inline-formula>. More precisely, for a typical threshold of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e169" xlink:type="simple"/></inline-formula> convergence times are slightly below <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e170" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2C–E</xref>. A very conservative threshold of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e171" xlink:type="simple"/></inline-formula> yields convergence times close to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e172" xlink:type="simple"/></inline-formula>.</p>
<p>The above simulations were performed in a circuit of 560 neurons, but eventually one is interested in the properties of much larger circuits. Hence, a crucial question is how the convergence properties scale with the network size. To this end, we compared convergence in the cortical microcircuit model of <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref> for four different sizes (500, 1000, 2000 and 5000). To ensure that overall activity characteristics are maintained across different sizes, we adopted the approach of <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref> and scaled recurrent postsynaptic potential (PSP) amplitudes inversely proportional to network size. A comparison of mean (solid line) and worst (dashed line) marginal convergence for networks of different sizes is shown in <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2F</xref>. Notably we find that the network size has virtually no effect on convergence speed. This suggests that, at least within the scope of the laminar microcircuit model of <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref>, even very large cortical networks may support fast extraction of knowledge (in particular marginals) from their stationary distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e173" xlink:type="simple"/></inline-formula>.</p>
<p>In order to estimate the required computation time associated with the estimation of marginal probabilities and MAP solutions on small <italic>subpopulations</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e174" xlink:type="simple"/></inline-formula>, one needs to know how fast the marginal probabilities of <italic>vector-valued</italic> states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e175" xlink:type="simple"/></inline-formula> of subnetworks of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e176" xlink:type="simple"/></inline-formula> become independent from the initial state of the network. To estimate convergence speed in small subnetworks, we applied a multivariate version of the Gelman-Rubin method to vector-valued simple states of subnetworks (<xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2G</xref>, dotted lines, evaluated for varying circuit sizes from 500 to 5000 neurons). We find that multivariate convergence of state frequencies for a population of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e177" xlink:type="simple"/></inline-formula> neurons is only slightly slower than for uni-variate marginals. To complement this analysis, we also investigated convergence properties of a “random readout” neuron which integrates inputs from many neurons in a subnetwork. It is interesting to note that the convergence speed of such a readout neuron, which receives randomized connections from a randomly chosen subset of 500 neurons, is comparable to that of single marginals (<xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2F</xref>, solid lines), and in fact slightly faster.</p>
</sec><sec id="s2e">
<title>Impact of different dynamic regimes on the convergence time</title>
<p>An interesting research question is which dynamic or structural properties of a cortical microcircuit model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e178" xlink:type="simple"/></inline-formula> have a strong impact on its convergence speed to the stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e179" xlink:type="simple"/></inline-formula>. Unfortunately, a comprehensive treatment of this question is beyond the scope of this paper, since virtually any aspect of circuit dynamics could be investigated in this context. Even if one focuses on a single aspect, the impact of one circuit feature is likely to depend on the presence of other features (and probably also on the properties of the input). Nonetheless, to lay a foundation for further investigation, first empirical results are given in <xref ref-type="fig" rid="pcbi-1003311-g003">Figure 3</xref>.</p>
<fig id="pcbi-1003311-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003311.g003</object-id><label>Figure 3</label><caption>
<title>Impact of network architecture and network dynamics on convergence speed.</title>
<p>Convergence properties for single neurons (as in <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2C</xref>) in different network architectures were assessed using univariate Gelman-Rubin analysis. Typical network activity is shown on the left, convergence speed on the right (solid: mean marginal, dashed: worst marginal). <bold>A</bold>. Small cortical column model from <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1</xref> (input neurons not shown). <bold>B</bold>. Network with sparse activity (20 neurons). <bold>C</bold>. Network with stereotypical trajectories (50 neurons, inhibitory neurons not shown). Despite strongly irreversible dynamics, convergence is only slightly slower. <bold>D</bold>. Network with bistable dynamics (two competing populations, each comprising 10 neurons). Convergence is slower in this circuit due to low-frequency switching dynamics between two attractors.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003311.g003" position="float" xlink:type="simple"/></fig>
<p>As a reference point, <xref ref-type="fig" rid="pcbi-1003311-g003">Figure 3A</xref> shows a typical activity pattern and convergence speed of single marginals in the small cortical microcircuit model from <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1</xref>. To test whether the overall activity of a network has an obvious impact on convergence speed, we constructed a small network of 20 neurons (10 excitatory, 10 inhibitory) and tuned connection weights to achieve sparse overall activity (<xref ref-type="fig" rid="pcbi-1003311-g003">Figure 3B</xref>). A comparison of panels A and B suggests that overall network activity has no significant impact on convergence speed. To test whether the presence of stereotypical trajectories of network states (similar to <xref ref-type="bibr" rid="pcbi.1003311-Luczak2">[41]</xref>) has a noticeable influence on convergence, we constructed a small network exhibiting strong sequential activity patterns (see <xref ref-type="fig" rid="pcbi-1003311-g003">Figure 3C</xref>). We find that convergence speed is hardly affected, except for the first <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e180" xlink:type="simple"/></inline-formula> (see <xref ref-type="fig" rid="pcbi-1003311-g003">Figure 3C</xref>). Within the scope of this first empirical investigation, we were only able to produce a significant slow-down of the convergence speed by building a network that alternated between two attractors (<xref ref-type="fig" rid="pcbi-1003311-g003">Figure 3D</xref>).</p>
</sec><sec id="s2f">
<title>Distributions of network states in the presence of periodic network input</title>
<p>In Theorem 1 we had already addressed one important case where the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e181" xlink:type="simple"/></inline-formula> receives dynamic external inputs: the case when external input is generated by some Markov process. But many networks of neurons in the brain are also subject to more or less pronounced periodic inputs (“brain rhythms” <xref ref-type="bibr" rid="pcbi.1003311-Engel1">[61]</xref>–<xref ref-type="bibr" rid="pcbi.1003311-Wang1">[63]</xref>), and it is known that these interact with knowledge represented in distributions of network states in specific ways. For instance, it had been shown in <xref ref-type="bibr" rid="pcbi.1003311-Dragoi1">[35]</xref> that the phase of the firing of place cells in the hippocampus of rats relative to an underlying theta-rhythm is related to the expected time when the corresponding location will be reached. Inhibitory neurons in hippocampus have also been reported to fire preferentially at specific phases of the theta cycle (see e.g. Figure S5 in <xref ref-type="bibr" rid="pcbi.1003311-Kelemen1">[12]</xref>). Moreover it was shown that different items that are held in working memory are preferentially encoded by neurons that fire at different phases of an underlying gamma-oscillation in the monkey prefrontal cortex <xref ref-type="bibr" rid="pcbi.1003311-Siegel1">[64]</xref> (see <xref ref-type="bibr" rid="pcbi.1003311-Pipa1">[65]</xref> for further evidence that such oscillations are behaviorally relevant). Phase coding was also reported in superior temporal sulcus during category representation <xref ref-type="bibr" rid="pcbi.1003311-Turesson1">[66]</xref>. The following result provides a theoretical foundation for such phase-specific encoding of knowledge within a framework of stochastic computation in networks of spiking neurons.</p>
<sec id="s2f1">
<title>Theorem 2 (Phase-specific distributions of network states)</title>
<p><italic>Let </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e182" xlink:type="simple"/></inline-formula><italic> be an arbitrary model for a network of stochastic spiking neurons as in Theorem 1. Assume now that the vector of input rates </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e183" xlink:type="simple"/></inline-formula><italic> has in addition to fixed components also some components that are periodic with a period </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e184" xlink:type="simple"/></inline-formula><italic> (such that each input neuron </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e185" xlink:type="simple"/></inline-formula><italic> emits a Poisson spike train with an </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e186" xlink:type="simple"/></inline-formula><italic>-periodically varying firing rate </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e187" xlink:type="simple"/></inline-formula><italic>). Then the distribution of network states </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e188" xlink:type="simple"/></inline-formula><italic> converges for every phase </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e189" xlink:type="simple"/></inline-formula><italic> (</italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e190" xlink:type="simple"/></inline-formula><italic>) exponentially fast to a unique stationary distribution of network states </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e191" xlink:type="simple"/></inline-formula><italic> at this phase </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e192" xlink:type="simple"/></inline-formula><italic> of the periodic network input </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e193" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<p>Hence, a circuit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e194" xlink:type="simple"/></inline-formula> can potentially store in each clearly separable phase <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e195" xlink:type="simple"/></inline-formula> of an (externally) imposed oscillation a different, phase-specific, stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e196" xlink:type="simple"/></inline-formula>. Below we will address basic implications of this result in the context of two experimentally observed phenomena: stereotypical trajectories of network states and bi-stable (or multi-stable) network activity.</p>
<p><xref ref-type="fig" rid="pcbi-1003311-g004">Figure 4A–D</xref> demonstrates the emergence of phase-specific distributions in a small circuit (the same as in <xref ref-type="fig" rid="pcbi-1003311-g003">Figure 3C</xref> but with only one chain) with a built-in stereotypical trajectory similar to a spatial path sequence generated by hippocampal place cell assemblies <xref ref-type="bibr" rid="pcbi.1003311-Dragoi1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Gupta1">[36]</xref>. <xref ref-type="fig" rid="pcbi-1003311-g004">Figure 4A</xref> shows a typical spike pattern in response to rhythmic background stimulation (spikes from inhibitory neurons in red). The background oscillation was implemented here for simplicity via direct rhythmic modulation of the spiking threshold of all neurons. Note that the trajectory becomes particularly often initiated at a specific phase of the rhythm (when neuronal thresholds are lowest), like in experimental data <xref ref-type="bibr" rid="pcbi.1003311-Dragoi1">35</xref>,<xref ref-type="bibr" rid="pcbi.1003311-Gupta1">36</xref>. As a result, different phases within a cycle of the rhythm become automatically associated with distinct segments of the trajectory. One can measure and visualize this effect by comparing the frequency of network states which occur at two different phases, i.e., by comparing the stationary distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e197" xlink:type="simple"/></inline-formula> for these two phases. <xref ref-type="fig" rid="pcbi-1003311-g004">Figure 4B</xref> shows a comparison of phase-specific marginal distributions on a small subnetwork of 3 neurons, demonstrating that phase-specific stationary distributions may indeed vary considerably across different phases. Convergence to the phase-specific stationary distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e198" xlink:type="simple"/></inline-formula> can be understood as the convergence of the probability of any given state to a periodic limit cycle as a function of the phase <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e199" xlink:type="simple"/></inline-formula> (illustrated in <xref ref-type="fig" rid="pcbi-1003311-g004">Figure 4C</xref>). An application of the Gelman-Rubin multivariate diagnostic suggests that this convergence takes places within a few cycles of the theta oscillation (<xref ref-type="fig" rid="pcbi-1003311-g004">Figure 4D</xref>).</p>
<fig id="pcbi-1003311-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003311.g004</object-id><label>Figure 4</label><caption>
<title>Emergence of phase-specific stationary distributions of network states in the presence of periodic network input.</title>
<p><bold>A</bold>. A network with a built-in stereotypical trajectory is stimulated with a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e200" xlink:type="simple"/></inline-formula> background oscillation. The oscillation (top) is imposed on the neuronal thresholds of all neurons. The trajectories produced by the network (bottom) become automatically synchronized to the background rhythm. The yellow shading marks the three neurons for which the analysis in panels B and C was carried out. The two indicated time points (green and purple lines) mark the two phases for which the phase-specific stationary distributions are considered in panels B and D (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e201" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e202" xlink:type="simple"/></inline-formula> into the cycle, with phase-specific distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e203" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e204" xlink:type="simple"/></inline-formula>, respectively). <bold>B</bold>. The empirically measured distributions of network states are observed to differ significantly at two different phases of the oscillation (phases marked in panel A). Shown is for each phase the phase-specific marginal distribution over 3 neurons (4, 5 and 6), using simple states with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e205" xlink:type="simple"/></inline-formula>. The zero state (0,0,0) is not shown. The empirical distribution for each phase <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e206" xlink:type="simple"/></inline-formula> was obtained from a single long run, by taking into account the network states at times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e207" xlink:type="simple"/></inline-formula>, etc., with cycle length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e208" xlink:type="simple"/></inline-formula>. <bold>C</bold>. Illustration of convergence to phase-specific stationary distributions. Shown is the relative frequency of subnetwork state (1,1,0) on the subset of neurons 4,5 and 6 over time, when the network is started from two different initial states (red/blue). In each case, the state frequency quickly approaches a periodic limit cycle. <bold>D</bold>. Convergence to phase-specific stationary distributions takes place within a few cycles of the underlying oscillation. Shown is the multivariate Gelman-Rubin convergence analysis to the phase-specific stationary distribution for two different phases. <bold>E</bold>. Bi-stable network under the influence of a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e209" xlink:type="simple"/></inline-formula> background oscillation. <bold>F</bold>. In response to the periodic stimulation, transitions between the two attractors (modes) become concentrated around a specific phase of the distribution.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003311.g004" position="float" xlink:type="simple"/></fig>
<p>Theta-paced spatial path sequences in hippocampus constitute a particularly well-studied example of phase-specific network activity <xref ref-type="bibr" rid="pcbi.1003311-Dragoi1">[35]</xref>. Our theoretical framework suggests a novel interpretation of these patterns as samples from a Markov chain with a phase-dependent stationary distribution of network states induced by the theta-rhythm. A basic prediction of this interpretation is that two trajectories in successive theta cycles should exhibit significantly stronger similarities than two trajectories from randomly chosen cycles (due to inherent temporal dependencies of the Markov chain). Two trajectories from distant cycles, on the other hand, should relate to each other similarly as randomly chosen pairs of trajectories. Evidence for such an effect has been reported recently by <xref ref-type="bibr" rid="pcbi.1003311-Gupta1">[36]</xref>, where it was found that “sequences separated by 20 cycles approach random chance, whereas sequences separated by only a single theta cycle are more likely to be similar to each other.”</p>
<p>The previously described theoretical framework also provides an interesting new perspective on multi-stability, a wide-spread phenomenon which has been observed in various sensory domains <xref ref-type="bibr" rid="pcbi.1003311-Blake1">[67]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Sterzer1">[68]</xref>. Different authors have noted that multi-stability, both on the neuronal and perceptual level, could be understood as a side effect of sampling from a multi-modal distribution <xref ref-type="bibr" rid="pcbi.1003311-Hoyer1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Buesing1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Gershman1">[69]</xref>. Recent data from hippocampus suggest that oscillations, which had previously received little attention in this context, may play an important role here: <xref ref-type="bibr" rid="pcbi.1003311-Jezek1">[37]</xref> found that switching between different attractors ( = modes of the stationary distribution in our terminology) occurs preferentially at a specific phase during the theta cycle, whereas activity patterns within each cycle preferentially stayed in one attractor. Hence, the precise timing of switching between modes was found to be strongly tied to the theta rhythm. Such chunking of information in separate packages (theta cycles) has been proposed as an important constituent of neural syntax <xref ref-type="bibr" rid="pcbi.1003311-Buzski1">[42]</xref>.</p>
<p>In <xref ref-type="fig" rid="pcbi-1003311-g004">Figure 4E</xref> we reproduce phase-dependent switching in a simple network model of bi-stable dynamics (the same network as in <xref ref-type="fig" rid="pcbi-1003311-g003">Figure 3D</xref>) in the presence of a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e210" xlink:type="simple"/></inline-formula> background oscillation. Indeed, we find that switching occurs preferentially at a specific phase of the oscillation (see <xref ref-type="fig" rid="pcbi-1003311-g004">Figure 4F</xref>) when the total firing rate of the network is lowest. Note that this is consistent with <xref ref-type="bibr" rid="pcbi.1003311-Jezek1">[37]</xref> who found that the separation between representations in different cycles was strongest at the point of the lowest average firing rate in the population (see <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1b</xref> in <xref ref-type="bibr" rid="pcbi.1003311-Jezek1">[37]</xref>). This phenomenon can be explained in our model by noting that the attractors are deeper during periods of high network activity. Conversely, attractors are more shallow when the population firing rate is lower, leading to an increased transition probability between attractors. If one takes a closer look at Proposition 1 and Lemma 1 in <xref ref-type="sec" rid="s4">Methods</xref> one sees that this is also consistent with our theoretical framework: A lower population firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e211" xlink:type="simple"/></inline-formula> translates into a smaller contraction factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e212" xlink:type="simple"/></inline-formula>, implying a tighter bound on the contraction speed of state distributions and thus higher transition probabilities to radically different states from the current (initial) network state.</p>
<p>Altogether, one sees that the presence of background oscillations has relevant functional implications on multi-stability. In particular, the presence of background oscillations in multi-stable networks facilitates both exploitation within a cycle and exploration across cycles: Within a cycle high firing rates force the network into one of the attractors, thereby avoiding interference with other attractors and facilitating the readout of a consistent network state. At the end of a cycle low firing rates allow the network to switch to different attractors, thereby promoting fast convergence to the stationary distribution. The rhythmic deepening and flattening of attractors and the resulting phase-specific attractor dynamics could be particularly useful for the extraction of information from the circuit if downstream networks are phase-locked to the same rhythm, as reported, for example, for the interactions between neurons in hippocampus and prefrontal cortex <xref ref-type="bibr" rid="pcbi.1003311-Siapas1">[70]</xref>.</p>
</sec></sec><sec id="s2g">
<title>Solving constraint satisfaction problems in networks of spiking neurons</title>
<p>Whenever an inhibitory neuron fires, it reduces for a short while the probability of firing for its postsynaptic targets. In fact, new experimental data <xref ref-type="bibr" rid="pcbi.1003311-Haider1">[71]</xref> show that inhibitory neurons impose quite powerful constraints on pyramidal cells. But also how pyramidal cells are embedded into their network environment imposes constraints on local network activity. From this perspective, the resulting firing patterns of a cortical microcircuit can be viewed as stochastically generated solutions of an immensely complex constraint satisfaction problem, that is defined both by external inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e213" xlink:type="simple"/></inline-formula> to the circuit and by the way each excitatory and inhibitory neuron is embedded into its circuit environment. Constraint satisfaction problems are from the computational perspective a particularly interesting class of problems, because many tasks that a brain has to solve, from the generation of a percept from unreliable and ambiguous sources to higher level tasks such as memory recall, prediction, planning, problem solving, and imagination, can be formulated as constraint satisfaction problems <xref ref-type="bibr" rid="pcbi.1003311-Kumar1">[72]</xref>. However, numerous constraint satisfaction problems are known to be NP-hard, thereby limiting the applicability of exact solution strategies. Instead, approximate or heuristic algorithms are commonly used in practice (for example evolutionary algorithms <xref ref-type="bibr" rid="pcbi.1003311-Craenen1">[73]</xref>). Here we propose that networks <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e214" xlink:type="simple"/></inline-formula> of spiking neurons with noise have an inherent capability to solve constraint satisfaction problems in an approximate (heuristic) manner through their stochastic dynamics. The key principle is that those network states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e215" xlink:type="simple"/></inline-formula>, which satisfy the largest number of local constraints, have the highest probability under the distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e216" xlink:type="simple"/></inline-formula>. These constraints are imposed by the way each neuron of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e217" xlink:type="simple"/></inline-formula> is embedded into the circuit, and the current external input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e218" xlink:type="simple"/></inline-formula> which can selectively activate or deactivate specific constraints.</p>
<p>We have selected a specific constraint satisfaction problem for demonstrating the capability of networks of spiking neurons to generate rapidly approximate solutions to constraint satisfaction problems through their inherent stochastic dynamics: solving Sudoku puzzles (see <xref ref-type="fig" rid="pcbi-1003311-g005">Figure 5A</xref>). Sudoku is a well-suited example because it is complex enough to be representative for many problem solving tasks, and lends itself well to visual interpretation and presentation (but note that we do not aim to model here how humans solve Sudoku puzzles). The rules of the Sudoku game can be easily embedded into common models for cortical microcircuits as recurrent networks of Winner-Take-All (WTA) microcircuit motifs <xref ref-type="bibr" rid="pcbi.1003311-Douglas1">[29]</xref>. Each WTA motif is an ensemble of pyramidal cells (on layers 2/3 or 5/6) that are subject to lateral inhibition (see <xref ref-type="fig" rid="pcbi-1003311-g005">Figure 5B</xref>). Each pyramidal cell can in fact be part of several interlocking WTA motifs (<xref ref-type="fig" rid="pcbi-1003311-g005">Figure 5B</xref>, right).</p>
<fig id="pcbi-1003311-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003311.g005</object-id><label>Figure 5</label><caption>
<title>Solving Sudoku, a constraint satisfaction problem, through structured interactions between stochastically firing excitatory and inhibitory neurons.</title>
<p><bold>A</bold>. A “hard” Sudoku puzzle with 26 given numbers (left). The solution (right) is defined uniquely by the set of givens and the additional constraints that each digit must appear only once in each row, column and 3×3 subgrid. <bold>B</bold>. An implementation of the constraints of the Sudoku game in a spiking neural network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e219" xlink:type="simple"/></inline-formula> consists of overlapping WTA circuits. WTA circuits are ubiquitous connection motifs in cortical circuits <xref ref-type="bibr" rid="pcbi.1003311-Douglas1">[29]</xref>. A WTA circuit can be modeled by a set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e220" xlink:type="simple"/></inline-formula> stochastically spiking output neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e221" xlink:type="simple"/></inline-formula> that are subject to lateral inhibition (left). The same pyramidal cell can be part of several such WTA motifs (right). In the Sudoku example, each digit in a Sudoku field is associated with four pyramidal cells which vote for this digit when they emit a spike. Each such pyramidal cell participates in four WTA motifs, corresponding to the constraints that only one digit can be active in each Sudoku field, and that a digit can appear only once in each row, column and 3×3 subgrid. <bold>C</bold>. A typical network run is shown during the last <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e222" xlink:type="simple"/></inline-formula> before the correct solution was found to the Sudoku from panel A (the total solve time was approximately <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e223" xlink:type="simple"/></inline-formula> in this run, see panel D for statistics of solve times). The network performance (fraction of cells with correct values) over time is shown at the top. The spiking activity is shown for 3 (out of the 81) WTA motifs associated with the 3 colored Sudoku fields in A and B. In each of these WTA motifs there are 36 pyramidal cells (9 digits and 4 pyramidal cells for each digit). Spikes are colored green for those neurons which code for the correct digit in each Sudoku field (6, 8 and 4 in the example). <bold>D</bold>. Histogram of solve times (the first time the correct solution was found) for the Sudoku from panel A. Statistics were obtained from 1000 independent runs. The sample mean is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e224" xlink:type="simple"/></inline-formula>. <bold>E</bold>. Average network performance for this Sudoku converges quickly during the first five seconds to a value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e225" xlink:type="simple"/></inline-formula>, corresponding to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e226" xlink:type="simple"/></inline-formula>% correctly found digits (average taken over 1000 runs; shaded area: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e227" xlink:type="simple"/></inline-formula> standard deviations). Thereafter, from all possible <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e228" xlink:type="simple"/></inline-formula> configurations the network spends most time in good approximate solutions. The correct solution occurs particularly often, on average approximately 2% of the time (not shown).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003311.g005" position="float" xlink:type="simple"/></fig>
<p>This architecture makes it easy to impose the interlocking constraints of Sudoku (and of many other constraint satisfaction problems). Each pyramidal cell (or each local group of pyramidal cells) votes for placing a particular digit into an empty field of the grid, that is not dictated by the external input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e229" xlink:type="simple"/></inline-formula>. But this pyramidal cell is subject to the constraints that only one digit can be placed into this field, and that each digit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e230" xlink:type="simple"/></inline-formula> occurs only once in each column, in each row, and in each 3×3 sub-grid. Hence each pyramidal cell is simultaneously part of four inhibitory subnetworks (WTA motifs).</p>
<p>A specific puzzle can be entered by providing strong input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e231" xlink:type="simple"/></inline-formula> to those neurons which represent the given numbers in a Sudoku (<xref ref-type="fig" rid="pcbi-1003311-g005">Figure 5A</xref>, left). This initiates a quite intuitive dynamics: “Clamped” neurons start firing strongly, and as a consequence, neurons which code for conflicting digits in the same Sudoku field, the same row, column or 3×3 sub-grid, become strongly inhibited through di-synaptic inhibition. In many Sudoku fields this will lead to the inhibition of a large number of otherwise freely competing neurons, thereby greatly reducing the space of configurations generated by the network. In some cases, inhibition will immediately quieten all neurons except those associated with a single remaining digit (the only choice consistent with the givens). In the absence of competition, these uninhibited neurons will start firing along with the givens, thereby further constraining neighboring neurons. This form of inhibitory interaction therefore implicitly implements a standard strategy for solving easy Sudokus: checking for fields in which only one possibility remains. In harder Sudokus, however, this simple strategy alone would be typically insufficient, for example when several possibilities remain in all fields. In such cases, where inhibition leaves more than one possible digit open, a tentative digit will be automatically picked randomly by those neurons which happen to fire first among its competitors. This ensures that, instead of getting stuck, the network automatically explores potential configurations in situations where multiple possibilities remain. Altogether, through this combination of constraint enforcement and random exploration, those network states which violate few constraints (good approximate solutions) are visited with much higher probability than states with conflicting configurations. Hence, most time is spent in good approximate solutions. Furthermore, from all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e232" xlink:type="simple"/></inline-formula> Sudoku configurations the solving configuration is visited in this process especially often.</p>
<p><xref ref-type="fig" rid="pcbi-1003311-g005">Figure 5C</xref> shows a typical network run during the last <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e233" xlink:type="simple"/></inline-formula> seconds (out of a total simulation time of approximately <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e234" xlink:type="simple"/></inline-formula>) before the correct solution was found to the Sudoku puzzle from <xref ref-type="fig" rid="pcbi-1003311-g005">Figure 5A</xref>. For this simulation we modeled lateral inhibition in each WTA motif by reciprocally connecting each neuron in the subnetwork to a single inhibitory neuron. For each of the 9 digits in a Sudoku field, we created an associated local group of four pyramidal cells. This can be seen in <xref ref-type="fig" rid="pcbi-1003311-g005">Figure 5C</xref>, where spike responses of pyramidal cells associated with three different Sudoku fields are shown (the three colored fields in <xref ref-type="fig" rid="pcbi-1003311-g005">Figure 5A and B</xref>). Each field has 9 possible digits, and each digit has four associated neurons. Hence, for each of the three Sudoku fields (WTA motifs), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e235" xlink:type="simple"/></inline-formula> neurons are shown. Spikes are colored black for those neurons which code for a wrong digit, and green for the four neurons which code for the correct digit in a Sudoku field (the correct digits in <xref ref-type="fig" rid="pcbi-1003311-g005">Figure 5C</xref> are 6, 8 and 4). The overall performance of the network (fraction of correctly solved fields) during the last 1.5 seconds before the solution is found is shown in <xref ref-type="fig" rid="pcbi-1003311-g005">Figure 5C</xref> above.</p>
<p>In our simulations we found that the solve time (the time until the correct solution is found for the first time) generally depends on the hardness of the Sudoku, in particular on the number of givens. For the “hard” Sudoku with 26 givens from <xref ref-type="fig" rid="pcbi-1003311-g005">Figure 5A</xref>, solve times are approximately exponentially distributed at an average of 29 seconds (<xref ref-type="fig" rid="pcbi-1003311-g005">Figure 5D</xref>). The average performance during the first five seconds of a run (obtained from 1000 independent runs) is shown in <xref ref-type="fig" rid="pcbi-1003311-g005">Figure 5E</xref>. The plot shows quick convergence to a (stationary) average performance of approximately 0.9. This demonstrates that the network spends on average most time in approximate solutions with high performance. Among these high-performance solutions, the correct solution occurs especially often (on average 2% of the time).</p>
</sec></sec><sec id="s3">
<title>Discussion</title>
<sec id="s3a">
<title>A theoretical foundation for memory-based stochastic computation in cortical microcircuits</title>
<p>We have shown that for common noise models in cortical microcircuits, even circuits <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e236" xlink:type="simple"/></inline-formula> with very detailed and diverse non-linear neurons and synapses converge exponentially fast to a stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e237" xlink:type="simple"/></inline-formula> of network states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e238" xlink:type="simple"/></inline-formula>. This holds both for external inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e239" xlink:type="simple"/></inline-formula> that consist of Poisson spike trains of a fixed rate, and for the case where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e240" xlink:type="simple"/></inline-formula> is periodic, or generated by some Markov process with a stationary distribution. The same mathematical framework also guarantees exponentially fast convergence to a stationary distribution of <italic>trajectories</italic> of network states (of any fixed time length), thereby providing a theoretical foundation for understanding stochastic computations with experimentally observed stereotypical trajectories of network states. These results extend and generalize previous work in <xref ref-type="bibr" rid="pcbi.1003311-Brmaud1">[17]</xref> and <xref ref-type="bibr" rid="pcbi.1003311-Borovkov1">[18]</xref> in two ways. First, previous convergence proofs had been given only for networks of simplified neurons in which the (sub-threshold) neuronal integration of pre-synaptic spikes was assumed a linear process, thereby excluding the potential effects of dendritic non-linearities or synaptic short-term dynamics. Second, previous work had focused only on the case where input is provided by neurons with fixed firing rates (a special case of Theorem 1). In addition we show that these convergence proofs can be derived from a fundamental property of stochastic spiking networks, that we have formulated as the Contraction Lemma (Lemma 1 in <xref ref-type="sec" rid="s4">Methods</xref>).</p>
<p>The stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e241" xlink:type="simple"/></inline-formula> provides an attractive target for investigating the stochastic computing capabilities of data-based models <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e242" xlink:type="simple"/></inline-formula> for local circuits or larger networks of neurons in the brain. In contrast to the much simpler case of Boltzmann machines with non-spiking linear neurons and symmetric synaptic connections, it is unlikely that one can attain for cortical microcircuit models <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e243" xlink:type="simple"/></inline-formula> a simple analytical description of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e244" xlink:type="simple"/></inline-formula>. But our computer simulations have shown that this is not necessarily an obstacle for encoding salient constraints for problem solving in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e245" xlink:type="simple"/></inline-formula>, and for merging knowledge that is encoded in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e246" xlink:type="simple"/></inline-formula> with online information from external inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e247" xlink:type="simple"/></inline-formula> in quite fast stochastic computations. In fact, the resulting paradigm for computations in cortical microcircuits supports anytime computing, where one has no fixed computation time. Instead, first estimates of computational results can be produced almost immediately, and can be rapidly communicated to other circuits. In this way, no processor (circuit) has to idle until other processors have completed their subcomputations, thereby avoiding the arguably most critical general bottleneck of massively parallel computing systems. Instead, each microcircuit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e248" xlink:type="simple"/></inline-formula> can contribute continuously to an iterative refinement of a global computation.</p>
</sec><sec id="s3b">
<title>Estimates for the computation time of stochastic computations</title>
<p>Our computer simulations for a standard cortical microcircuit model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e249" xlink:type="simple"/></inline-formula> suggest that convergence to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e250" xlink:type="simple"/></inline-formula> is fast enough to support knowledge extraction from this distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e251" xlink:type="simple"/></inline-formula> within a few <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e252" xlink:type="simple"/></inline-formula>, i.e. within the typical computation time of higher-level brain computations. These first estimates need to be corroborated by further theoretical work and computer simulations. In particular, the relationship between the structure and dynamics of cortical microcircuits and their convergence speed merits further investigation. Furthermore, in the case where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e253" xlink:type="simple"/></inline-formula> is a multi-modal distribution there exists an obvious tradeoff between the convergence speed to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e254" xlink:type="simple"/></inline-formula> and the typical duration of staying in an “attractor” (i.e., a region of the state space which has high probability under <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e255" xlink:type="simple"/></inline-formula>). Staying longer in an attractor obviously facilitates the readout of the result of a computation by downstream networks. A number of experimental data suggest that neuromodulators can move neural circuits (at least in the prefrontal cortex) to different points on this tradeoff curve. For example it is argued in <xref ref-type="bibr" rid="pcbi.1003311-Durstewitz1">[74]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Durstewitz2">[75]</xref> that the activation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e256" xlink:type="simple"/></inline-formula> receptors through dopamine deepens all basins of attraction, making it harder for the network state to leave an attractor. Additional molecular mechanisms that shift the tradeoff between fast sampling (exploration) and the temporal stability of found solutions are reviewed in <xref ref-type="bibr" rid="pcbi.1003311-Arnsten1">[76]</xref>. Another interesting perspective on convergence speed is that slow convergence may be beneficial for certain computations in specific brain areas (especially early sensory areas). Slow convergence enlarges the time span during which the network can integrate information from non-stationary external inputs <xref ref-type="bibr" rid="pcbi.1003311-Maass1">[77]</xref>–<xref ref-type="bibr" rid="pcbi.1003311-Klampfl1">[79]</xref>. In addition the initial state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e257" xlink:type="simple"/></inline-formula> of a network may contain information about preceding events that are computationally useful. Those considerations suggest that there exist systematic differences between the convergence speed to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e258" xlink:type="simple"/></inline-formula> in different neural systems <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e259" xlink:type="simple"/></inline-formula>, and that it can be modulated in at least some systems <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e260" xlink:type="simple"/></inline-formula> dependent on the type of computational task that needs to be solved.</p>
<p>Another important issue is the tradeoff between sampling time and sampling accuracy. In high-level cognitive tasks, for example, it has been argued that “approximate and quick” sample-based decisions are often better than “accurate but slow” decisions <xref ref-type="bibr" rid="pcbi.1003311-Vul1">[80]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Lieder1">[81]</xref>. Of particular interest in this context is the analysis of <xref ref-type="bibr" rid="pcbi.1003311-Lieder1">[81]</xref> who studied the time-accuracy tradeoff during decision making, under the assumption that the mind performs inference akin to MCMC sampling. Due to the nature of MCMC sampling, early samples before convergence (during the burn-in period) are biased towards the initial state of the system. In the absence of time pressure, the optimal strategy is therefore to wait and collect samples for a long period of time (in theory indefinitely). In the presence of even moderate time costs, however, the optimal sampling time can be shown to be finite, a result which can provide a rational explanation of the anchoring effect in cognitive science <xref ref-type="bibr" rid="pcbi.1003311-Lieder1">[81]</xref> (under time pressure people's decisions are influenced by their “initial state”). Notably, the analysis of <xref ref-type="bibr" rid="pcbi.1003311-Lieder1">[81]</xref> was based on the assumption that the MCMC algorithm exhibits geometric convergence, the discrete-time equivalent to the exponential convergence speed proved in this paper for stochastic spiking networks. Applying a similar analysis to study optimal time-accuracy tradeoff points in cortical microcircuits therefore presents a promising avenue for future research.</p>
</sec><sec id="s3c">
<title>Which probability distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e261" xlink:type="simple"/></inline-formula> can be encoded as a stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e262" xlink:type="simple"/></inline-formula> of some neural circuit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e263" xlink:type="simple"/></inline-formula>?</title>
<p>It had been shown in <xref ref-type="bibr" rid="pcbi.1003311-Buesing1">[21]</xref> and <xref ref-type="bibr" rid="pcbi.1003311-Pecevski1">[22]</xref> that, under certain assumptions on the neuron models and circuit structure, in principle every joint distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e264" xlink:type="simple"/></inline-formula> over discrete-valued random variables can be represented as a stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e265" xlink:type="simple"/></inline-formula> of some network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e266" xlink:type="simple"/></inline-formula> of spiking neurons. Forthcoming unpublished results suggest that such internal representations of a given distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e267" xlink:type="simple"/></inline-formula> can even be learned from examples drawn from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e268" xlink:type="simple"/></inline-formula>. This will provide a first step towards understanding how the stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e269" xlink:type="simple"/></inline-formula> of a microcircuit can be adapted through various plasticity processes to encode salient constraints, successful solution strategies (rules), and other types of knowledge. This research direction promises to become especially interesting if one takes into account that knowledge can not only be encoded in the stationary distribution of network states, but also in the simultaneously existing stationary distribution of trajectories of network states.</p>
</sec><sec id="s3d">
<title>Relationship to attractor networks and transients between attractors</title>
<p>Attractor neural networks <xref ref-type="bibr" rid="pcbi.1003311-Hopfield1">[82]</xref> were originally deterministic computational models, where gradient descent leads the network from some given initial state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e270" xlink:type="simple"/></inline-formula> (the input for the computation) to the lowest point of the attractor (the output of the computation) in whose basis of attraction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e271" xlink:type="simple"/></inline-formula> lies. The computational capability of an attractor neural network is substantially larger if its attractor landscape can be reconfigured on the fly by external input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e272" xlink:type="simple"/></inline-formula>, as in <xref ref-type="bibr" rid="pcbi.1003311-Hopfield2">[83]</xref> and in the Sudoku example of this article. This usually requires that the attractors are not programmed directly into the network parameters, but emerge from some more general computational principles (e.g. constraint satisfaction). Attractor neural networks gain additional computational capability if there is some noise in the system <xref ref-type="bibr" rid="pcbi.1003311-Rolls1">[84]</xref>. This enables the network to leave after a while suboptimal solutions <xref ref-type="bibr" rid="pcbi.1003311-Durstewitz3">[85]</xref>. Alternative modeling frameworks for the transient dynamics of neural systems are provided by the liquid computing model <xref ref-type="bibr" rid="pcbi.1003311-Maass1">[77]</xref>, and on a more abstract level by sequences of metastable states in dynamical systems <xref ref-type="bibr" rid="pcbi.1003311-Rabinovich1">[86]</xref>. Here we propose to view both transient and attractor dynamics of complex data-based circuits <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e273" xlink:type="simple"/></inline-formula> from the perspective of probabilistic inference, in particular as neural sampling <xref ref-type="bibr" rid="pcbi.1003311-Buesing1">[21]</xref> (or more abstractly: as MCMC sampling) from their inherent probability distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e274" xlink:type="simple"/></inline-formula> over network states (or trajectories of network states), that serves as the knowledge base of these neural systems.</p>
</sec><sec id="s3e">
<title>A new computational framework for analyzing brain activity</title>
<p>We had focused in our computer simulations on the investigation of the stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e275" xlink:type="simple"/></inline-formula> for models <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e276" xlink:type="simple"/></inline-formula> of cortical microcircuits. But the results of Theorem 1 and Theorem 2 are of course much more general, and in principle apply to models <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e277" xlink:type="simple"/></inline-formula> for networks of neurons in the whole brain <xref ref-type="bibr" rid="pcbi.1003311-Sporns1">[87]</xref>. This perspective suggests understanding spontaneous brain activity (see <xref ref-type="bibr" rid="pcbi.1003311-Raichle1">[9]</xref>) as sampling from this global distribution in the absence of external input, and brain computations with external inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e278" xlink:type="simple"/></inline-formula> as sampling of brain states from conditional distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e279" xlink:type="simple"/></inline-formula>, thereby merging the knowledge base <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e280" xlink:type="simple"/></inline-formula> of the brain with incoming new information <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e281" xlink:type="simple"/></inline-formula>. This computational framework could in principle explain how the brain can merge both types of information in such seemingly effortless manner, a capability that can only partially be reproduced in artificial devices with current technology. Large-scale computer simulations will be needed to test the viability of this hypothesis, in particular the relationship between the known global structure of the brain network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e282" xlink:type="simple"/></inline-formula> and properties of its stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e283" xlink:type="simple"/></inline-formula>, and the convergence speed to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e284" xlink:type="simple"/></inline-formula>. Possibly the brain uses an important trick to speed up convergence during brain-wide sampling, for example by sampling during any concrete brain computation only from a subnetwork <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e285" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e286" xlink:type="simple"/></inline-formula>: those brain areas that control variables that are relevant for this computation. Functional connectivity would be explained from this perspective as opening of communication channels that support sampling from the (marginal) joint distribution of those variables that are stored within the functionally connected brain areas. Structured spontaneous brain activity <xref ref-type="bibr" rid="pcbi.1003311-Raichle1">[9]</xref> would then receive a functional interpretation in terms of updating these marginal joint distributions on the basis of newly acquired knowledge.</p>
</sec><sec id="s3f">
<title>Stochastic solutions of constraint satisfaction problems as a paradigm for higher level brain computation</title>
<p>A surprisingly large number of computational tasks that the brain has to solve, from the formation of a percept from multi-modal ambiguous sensory cues, to prediction, imagination, motor planning, rule learning, problem solving, and memory recall, have the form of constraint satisfaction problems: A global solution is needed that satisfies all or most of a set of soft or hard constraints. However, this characterization per se does not help us to understand how the brain can solve these tasks, because many constraint satisfaction problems are computationally very demanding (in fact, often NP-hard <xref ref-type="bibr" rid="pcbi.1003311-Garey1">[88]</xref>), even for a fast digital computer. In the Sudoku example we have shown that the inherent stochastic dynamics of cortical microcircuits provides a surprisingly simple method for generating <italic>heuristic</italic> solutions to constraint satisfaction problems. This is insofar remarkable, as this computational organization does not require that specific algorithms are programmed into the network for solving specific types of such problems (as it is for example needed for solving Sudoku puzzles according to the ACT-R approach <xref ref-type="bibr" rid="pcbi.1003311-Qin1">[89]</xref>). Rather, it suffices that salient constraints are encoded into the network (e.g. through learning) in such a way that they make certain firing patterns of a subset of neurons more or less likely.</p>
<p>Future work will need to investigate whether and how this approach can be scaled up to larger instances of NP-complete constraint satisfaction problems. For example, it will be interesting to see whether stochastic networks of spiking neurons can also efficiently generate heuristic solutions to energy minimization problems <xref ref-type="bibr" rid="pcbi.1003311-Boykov1">[90]</xref> arising in visual processing.</p>
<p>Furthermore, additional research is needed to address suitable readout mechanisms that stabilize and evaluate promising candidate solutions (see <xref ref-type="bibr" rid="pcbi.1003311-Arnsten1">[76]</xref> for an experimentally supported mechanism that might contribute to this function). This is an important issue since, in its current form, the network will simply continue the stochastic exploration of heuristic solutions even after it has found the optimal solution. Therefore, in the absence of additional mechanisms the network is not able to hold on to (or store) previously found (near-)optimal solutions. To solve this issue one could consider, for example, one or several networks <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e287" xlink:type="simple"/></inline-formula> which generate in parallel heuristic solutions to a given problem. The output of these networks could then be further processed and integrated by a readout network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e288" xlink:type="simple"/></inline-formula> which attempts to extract a MAP solution, for example by adopting a solution from some <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e289" xlink:type="simple"/></inline-formula> only if it has higher value than the currently stored state. Hence, the sampling networks <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e290" xlink:type="simple"/></inline-formula> would have stationary distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e291" xlink:type="simple"/></inline-formula> which encourage exploration and broadly assign probability to many different heuristic solutions, whereas the readout network would ideally exhibit a sharply peaked stationary distribution at the global optimum of the constraint satisfaction problem. Studying the feasibility of this approach requires further research.</p>
</sec><sec id="s3g">
<title>Relationship to models for probabilistic inference in cognitive science</title>
<p>A substantial number of behavioral studies in cognitive science (see e.g. <xref ref-type="bibr" rid="pcbi.1003311-Gershman1">[69]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Griffiths1">[91]</xref>–<xref ref-type="bibr" rid="pcbi.1003311-Tenenbaum1">[94]</xref>) have arrived at the conclusion that several of the previously discussed higher level mental operations are implemented through probabilistic inference. Some of the underlying data also suggest that probabilistic inference is implemented in the brain through some form of sampling (rather than through arithmetical approaches such as belief propagation <xref ref-type="bibr" rid="pcbi.1003311-Koller1">[44]</xref>). But according to <xref ref-type="bibr" rid="pcbi.1003311-Tenenbaum1">[94]</xref>: “The key research questions are as follows: What approximate algorithms does the mind use, how do they relate to engineering approximations in probabilistic AI, and how are they implemented in neural circuits?” This article contributes to these fascinating questions by providing a rigorous theoretical foundation for the hypothesis that neural circuits in the brain represent complex probability distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e292" xlink:type="simple"/></inline-formula> through sampling. In addition, we have provided evidence that this form of sampling in cortical microcircuits may be fast enough to facilitate the approximate estimation of marginals or marginal MAP assignments, which commonly appear in real-world inference tasks, within a few <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e293" xlink:type="simple"/></inline-formula>. A major challenge for future work will be to understand also neuronal plasticity on the implementation level from this perspective. For example, how can prior knowledge be acquired and integrated into the stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e294" xlink:type="simple"/></inline-formula> of a realistic circuit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e295" xlink:type="simple"/></inline-formula> (featuring short-term plasticity, dendritic processing, etc.) in an autonomous fashion, and in a manner consistent with statistically optimal learning <xref ref-type="bibr" rid="pcbi.1003311-Fiser1">[25]</xref>?</p>
</sec><sec id="s3h">
<title>Long-term plasticity and other slower features of network dynamics</title>
<p>In biological networks it is reasonable to assume that the network dynamics unfolds on a continuum of time scales from milliseconds to days. Our goal in this article was to focus on stochastic computations on shorter time scales, between a few milliseconds to seconds. To this end we assumed that there exists a clear separation of time scales between fast and slow dynamical network features, thus allowing us to exclude the effect of slower dynamical processes such as long-term plasticity of synaptic weights during these shorter time scales. In network models and experimental setups where slower processes significantly influence (or interfere with) the dynamics on shorter time scales, it would make sense to extend the concept of a stationary distribution to include, for example, also the synaptic parameters as random variables. A first step in this direction has been made for neurons with linear sub-threshold dynamics and discretized synapses in <xref ref-type="bibr" rid="pcbi.1003311-Borovkov1">[18]</xref>.</p>
</sec><sec id="s3i">
<title>Deterministic network models and chaos</title>
<p>Deterministic network models such as leaky integrate-and-fire neurons without noise (no external background noise, no synaptic vesicle noise and no channel noise) violate the assumptions of Theorem 1 and 2. Furthermore, although realistic neurons are known to possess various noise sources, the theoretical assumptions could in principle still fail if the network is not <italic>sufficiently</italic> stochastic: this would happen, for example, if there exists some strong input (within the limits of typical input activity) which entirely overrules the noise, leading to a firing probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e296" xlink:type="simple"/></inline-formula> in some time interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e297" xlink:type="simple"/></inline-formula> during the network simulation. Such deterministic behavior would correspond to the instantaneous firing rate of a stochastic neuron becoming infinite at some point during that interval (in violation of assumption A2, see <xref ref-type="sec" rid="s4">Methods</xref>: Scope of theoretical results). From an empirical perspective, a simple necessary condition for sufficient stochasticity is the presence of trial-to-trial variability for each single spike produced by a network. Consider, for example, the spike times generated by a specific neuron in a network simulation, in response to some fixed input spike train. If there exists a spike which always occurs at the exact same time during multiple repetitions of this experiment starting from identical initial states, then the assumptions of Theorem 1 and 2 are obviously violated.</p>
<p>For deterministic (or insufficiently stochastic) networks the question arises whether convergence to a unique stationary distribution may still occur under appropriate conditions, perhaps in some modified sense. Notably, it has been recently observed that deterministic networks may indeed lead to apparently stochastic spiking activity <xref ref-type="bibr" rid="pcbi.1003311-Churchland1">[95]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-LitwinKumar1">[96]</xref>. This apparent stochasticity was linked to chaotic spiking dynamics. This suggests that chaos may act as a substitute for “real” noise in deterministic networks (similar to pseudo random-number generators emulating true randomness): Chaotic systems are sensitive to small perturbances in initial conditions, and may thus exponentially amplify otherwise insignificant noise sources such as ubiquitous thermal noise <xref ref-type="bibr" rid="pcbi.1003311-Clarke1">[5]</xref>. Thus, chaos could play an important role in both emulating and amplifying stochasticity on the network level.</p>
<p><xref ref-type="bibr" rid="pcbi.1003311-LitwinKumar1">[96]</xref> focused their analysis of stochasticity on firing rate fluctuations and spiking irregularity, and it remains unclear whether these networks would still appear stochastic if one takes into account full network states (as in this article). The Gelman-Rubin convergence analysis of population activity proposed in this paper could be applied to provide some insight into this question. A more thorough investigation of chaos in the context of our results would also call for a rigorous theoretical analysis of ergodic properties of chaotic spiking networks.</p>
</sec><sec id="s3j">
<title>Further experimentally testable predictions</title>
<p>Our theoretical results demonstrate that every neural system <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e298" xlink:type="simple"/></inline-formula> has a stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e299" xlink:type="simple"/></inline-formula> of network states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e300" xlink:type="simple"/></inline-formula>. This can be tested experimentally, for various behavioral regimes and external inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e301" xlink:type="simple"/></inline-formula>. A first step in this direction has already been carried out in <xref ref-type="bibr" rid="pcbi.1003311-Berkes1">[20]</xref> (see also the discussion in <xref ref-type="bibr" rid="pcbi.1003311-Okun1">[97]</xref>). The hypothesis that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e302" xlink:type="simple"/></inline-formula> serves (for “neutral” external inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e303" xlink:type="simple"/></inline-formula>) as a prior for probabilistic inference through sampling suggests that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e304" xlink:type="simple"/></inline-formula> is constantly modified through prior experience (see <xref ref-type="bibr" rid="pcbi.1003311-Zhang2">[98]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Xu1">[99]</xref> for first results) and learning (see <xref ref-type="bibr" rid="pcbi.1003311-Lewis1">[10]</xref> for fMRI data).</p>
<p>Our Theorem 2 suggests in addition that neural systems <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e305" xlink:type="simple"/></inline-formula> that have a prominent rhythm (such as for example the theta oscillation in the hippocampus) are able to store <italic>several</italic> stationary distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e306" xlink:type="simple"/></inline-formula> of network states, one for each clearly separable phase <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e307" xlink:type="simple"/></inline-formula> of this rhythm. It has already been shown in a qualitative manner that in some behavioral situations certain states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e308" xlink:type="simple"/></inline-formula> appear with substantially high probability at specific phases <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e309" xlink:type="simple"/></inline-formula> of the rhythm (see e.g. <xref ref-type="bibr" rid="pcbi.1003311-Gupta1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Buzsaki1">[62]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Siegel1">[64]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Turesson1">[66]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Harris1">[100]</xref>). But a systematic experimental analysis of phase-dependent distributions of network states in the style of <xref ref-type="bibr" rid="pcbi.1003311-Berkes1">[20]</xref> is missing.</p>
<p>Our Theorem 1 predicts in addition that a generic neural circuit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e310" xlink:type="simple"/></inline-formula> also has a stationary distribution over <italic>trajectories</italic> of network states. The existence of stereotypical trajectories of network states in the awake brain has been frequently reported (see e.g. <xref ref-type="bibr" rid="pcbi.1003311-Abeles1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Luczak2">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Zhang2">[98]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Jones1">[101]</xref>). But a statistical analysis of the distribution of such trajectories, especially also during spontaneous activity, is missing. Of particular interest is the relationship between the distribution of trajectories and the stationary distribution of (simple) network states. Do some network states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e311" xlink:type="simple"/></inline-formula> typically have a high probability because they occur in some high probability trajectory? And how does the distribution of trajectories change during learning?</p>
<p>The model for problem solving that we have presented in <xref ref-type="fig" rid="pcbi-1003311-g005">Figure 5</xref> suggests that external constraints have a significant and characteristic impact on the structure of the stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e312" xlink:type="simple"/></inline-formula>, by reducing the probability of network states which are inconsistent with the current constraints <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e313" xlink:type="simple"/></inline-formula>. In principle, this could be analyzed experimentally. In addition, this model suggests that there may be special mechanisms that prolong the time span during which a neural system <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e314" xlink:type="simple"/></inline-formula> stays in a network state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e315" xlink:type="simple"/></inline-formula> with high probability under <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e316" xlink:type="simple"/></inline-formula>, in order to support a readout of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e317" xlink:type="simple"/></inline-formula> by downstream networks. These mechanisms need to be revealed through experiments.</p>
</sec><sec id="s3k">
<title>New ideas for neuromorphic computation</title>
<p>The Sudoku example has shown that networks of spiking neurons with noise are in principle able to carry out quite complex computations. The constraints of many other demanding constraint satisfaction problems, in fact even of many NP-complete problems, can be encoded quite easily into circuit motifs composed of excitatory and inhibitory spiking neurons, and can be solved through the inherent stochastic dynamics of the network. This provides new computational paradigms and applications for various energy-efficient implementations of networks of spiking neurons in neuromorphic hardware, provided they can be equipped with sufficient amounts of noise. In particular, our results suggest that attractive computational properties of Boltzmann machines can be ported into spike-based hardware. These novel stochastic computing paradigms may also become of interest for other types of innovative computer hardware: Computer technology is approaching during the coming decade the molecular scale, where noise is abundantly available (whether one wants it or not) and it becomes inefficient to push through traditional deterministic computing paradigms.</p>
</sec><sec id="s3l">
<title>Conclusion</title>
<p>The results of this article show that stochastic computation provides an attractive framework for the investigation of computational properties of cortical microcircuits, and of networks of microcircuits that form larger neural systems. In particular it provides a new perspective for relating the structure and dynamics of neural circuits to their computational properties. In addition, it suggests a new way of understanding the organization of brain computations, and how they are modified through learning.</p>
</sec></sec><sec id="s4" sec-type="methods">
<title>Methods</title>
<sec id="s4a">
<title>Network states and distributions of network states</title>
<sec id="s4a1">
<title>Markov states</title>
<p>The Markov state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e318" xlink:type="simple"/></inline-formula> (or more explicitly, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e319" xlink:type="simple"/></inline-formula>) of a network at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e320" xlink:type="simple"/></inline-formula> is defined here as the recent history of spike times of all neurons in the network within the period <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e321" xlink:type="simple"/></inline-formula>. The term “Markov” refers to the fact that, under mild conditions and for a sufficiently long window <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e322" xlink:type="simple"/></inline-formula>, the network dynamics of a neural circuit after time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e323" xlink:type="simple"/></inline-formula> becomes independent of the network activity at times <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e324" xlink:type="simple"/></inline-formula>, given the Markov state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e325" xlink:type="simple"/></inline-formula> and the external input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e326" xlink:type="simple"/></inline-formula>. Hence, the network dynamics has the Markov property with respect to this state definition.</p>
<p>For each neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e327" xlink:type="simple"/></inline-formula> in a neural circuit a spike history of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e328" xlink:type="simple"/></inline-formula> is defined as the list of spike times emitted by neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e329" xlink:type="simple"/></inline-formula> within the window <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e330" xlink:type="simple"/></inline-formula>. Spike times are counted relative to the beginning of the window at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e331" xlink:type="simple"/></inline-formula>. If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e332" xlink:type="simple"/></inline-formula> is the number of spikes within <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e333" xlink:type="simple"/></inline-formula> for neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e334" xlink:type="simple"/></inline-formula>, then the list takes the form,<disp-formula id="pcbi.1003311.e335"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e335" xlink:type="simple"/><label>(3)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e336" xlink:type="simple"/></inline-formula>.</p>
<p>We denote the space of all possible network states of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e337" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e338" xlink:type="simple"/></inline-formula> or, when unambiguous, simply by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e339" xlink:type="simple"/></inline-formula>. Note that this definition is equivalent to the state definition in <xref ref-type="bibr" rid="pcbi.1003311-Borovkov1">[18]</xref>, to which the interested reader is referred for further formal details (e.g. the associated <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e340" xlink:type="simple"/></inline-formula>-algebra <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e341" xlink:type="simple"/></inline-formula> of the state space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e342" xlink:type="simple"/></inline-formula>).</p>
</sec><sec id="s4a2">
<title>Scope of theoretical results: Required properties of the network and neuronal noise models</title>
<p>We study general theoretical properties of stochastic spiking circuit models, driven by some external, possibly vector-valued, input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e343" xlink:type="simple"/></inline-formula>, which could represent for example input rates in a set of input neurons or injected input currents. Formally, the input sequence can assume values from any state space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e344" xlink:type="simple"/></inline-formula>; a concrete example is vector-valued input with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e345" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e346" xlink:type="simple"/></inline-formula> is the number of input dimensions.</p>
<p>We consider in this article two different noise models for a neuron: In noise model I, the spike generation is directly modeled as a stochastic process. All network dynamics, including axonal delays, synaptic transmission, short-term synaptic dynamics, dendritic interactions, integration of input at the soma, etc. can be modeled by a function which maps the Markov state (which includes the recent spike history of the neuron itself) onto an instantaneous spiking probability. This model is highly flexible and may account for various types of neuronal noise. In the more specific noise model II, the firing mechanism of the neuron is assumed to be deterministic, and noise enters its dynamics through stochastic vesicle release at afferent synaptic inputs. Also combinations of noise models I and II in the same neuron and circuit can be assumed for our theoretical results, for example neurons with a generic stochastic spiking mechanism which possess in addition stochastic synapses, or mixtures of neurons from model I and II in the same circuit.</p>
<p>In noise model I, the instantaneous spiking probability of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e347" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e348" xlink:type="simple"/></inline-formula> is given by,<disp-formula id="pcbi.1003311.e349"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e349" xlink:type="simple"/><label>(4)</label></disp-formula>This instantaneous firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e350" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e351" xlink:type="simple"/></inline-formula> is assumed to be bounded and completely determined by the network's current Markov state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e352" xlink:type="simple"/></inline-formula>, for some sufficiently large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e353" xlink:type="simple"/></inline-formula>. More precisely, the following four assumptions are made for noise model I:</p>
<p><italic>A</italic>1 <bold>Spikes are individual events:</bold> We assume that,<disp-formula id="pcbi.1003311.e354"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e354" xlink:type="simple"/><label>(5)</label></disp-formula>which is, for example, fulfilled if each neuron has some independent source of stochasticity.</p>
<p><italic>A</italic>2 <bold>Bounded rates:</bold> The instantaneous firing rates are bounded from above:</p>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e355" xlink:type="simple"/></inline-formula> for some <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e356" xlink:type="simple"/></inline-formula>. The ensuing upper bound on the total network firing rate is denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e357" xlink:type="simple"/></inline-formula>, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e358" xlink:type="simple"/></inline-formula>. It is assumed that instantaneous rates are bounded at any time, and in the presence of any input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e359" xlink:type="simple"/></inline-formula>.</p>
<p><italic>A</italic>3 <bold>Bounded memory:</bold> The firing rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e360" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e361" xlink:type="simple"/></inline-formula> depend on the network's past activity only through the history of recent spikes in a finite window <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e362" xlink:type="simple"/></inline-formula> of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e363" xlink:type="simple"/></inline-formula>. Hence, the <italic>direct</italic> effect of a spike at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e364" xlink:type="simple"/></inline-formula> on future firing rates of all neurons is limited to a bounded “memory period”, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e365" xlink:type="simple"/></inline-formula>. This bounded memory period <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e366" xlink:type="simple"/></inline-formula> can be understood as a lower bound for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e367" xlink:type="simple"/></inline-formula> during the subsequent convergence proofs (since smaller <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e368" xlink:type="simple"/></inline-formula> would violate the Markov property). In addition to this bounded-memory dependence on network spikes, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e369" xlink:type="simple"/></inline-formula> may depend on the current input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e370" xlink:type="simple"/></inline-formula> in any manner consistent with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e371" xlink:type="simple"/></inline-formula>.</p>
<p><italic>A</italic>4 <bold>Time-homogeneity:</bold> The functional mapping from recent spikes and/or input signals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e372" xlink:type="simple"/></inline-formula> to instantaneous firing rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e373" xlink:type="simple"/></inline-formula> does not change over time. In particular, we do not consider long-term plasticity of synaptic weights and/or excitabilities in this work.</p>
<p>Assumptions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e374" xlink:type="simple"/></inline-formula> can be summarized as follows: Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e375" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e376" xlink:type="simple"/></inline-formula> be the trajectories of input and network states as defined above. Then there exists a memory constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e377" xlink:type="simple"/></inline-formula> and rate bounds <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e378" xlink:type="simple"/></inline-formula>, such that for each neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e379" xlink:type="simple"/></inline-formula> there exists a function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e380" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e381" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e382" xlink:type="simple"/></inline-formula>. The function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e383" xlink:type="simple"/></inline-formula> is time-invariant but otherwise unconstrained, and can capture complex dynamical effects such as non-linear dendritic interactions between synaptic inputs or short-term plasticity of synapses.</p>
<p>The input signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e384" xlink:type="simple"/></inline-formula> can formally represent any variable which exerts some arbitrary influence on the instantaneous network dynamics (the neuronal firing functions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e385" xlink:type="simple"/></inline-formula>). In the simplest case, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e386" xlink:type="simple"/></inline-formula> could be a vector of firing rates controlling the spiking behavior of a set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e387" xlink:type="simple"/></inline-formula> input neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e388" xlink:type="simple"/></inline-formula>, such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e389" xlink:type="simple"/></inline-formula> in these neurons. In this case (which we focused on in the main text), input neurons are formally considered part of the circuit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e390" xlink:type="simple"/></inline-formula>. Note that in principle, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e391" xlink:type="simple"/></inline-formula> could also represent the strength of currents which are injected into a subset of neurons in the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e392" xlink:type="simple"/></inline-formula>, or the recent spiking history of a set of external input neurons (“input Markov states”). If the input comprises rates or currents, these can be either fixed (e.g. fixed input firing rates) or dynamically changing (in particular rates which are either subject to stochastic ergodic dynamics, or periodically changing rates). Below convergence proofs will be provided for both fixed and dynamic input conditions. If the input is defined in terms of input Markov states, the dynamic input analysis is applicable under conditions described further below.</p>
<p>In noise model II the basic stochastic event is a synaptic vesicle release (in noise model I it is a spike). Accordingly, the Markov state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e393" xlink:type="simple"/></inline-formula> of a network in noise model II is defined as the list of vesicle release times for each synaptic release site in the network (instead of spike timings for each neuron). We assume here that each synaptic release site releases at a given instance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e394" xlink:type="simple"/></inline-formula> at most one vesicle filled with neurotransmitters. But a synaptic connection between two neurons may consist of multiple synaptic release sites (see <xref ref-type="bibr" rid="pcbi.1003311-Lisman1">[102]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Branco1">[103]</xref> and <xref ref-type="bibr" rid="pcbi.1003311-Borst1">[3]</xref> for reviews). Instead of expressing the network dynamics through an instantaneous firing probability function for each neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e395" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e396" xlink:type="simple"/></inline-formula> (noise model I), for noise model II the network dynamics is expressed in terms of instantaneous release probabilities for each synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e397" xlink:type="simple"/></inline-formula>: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e398" xlink:type="simple"/></inline-formula>. Similar to noise model I, it is assumed that there exists a window length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e399" xlink:type="simple"/></inline-formula>, such that the dynamics of vesicle release at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e400" xlink:type="simple"/></inline-formula> is fully determined by the timing of previous vesicle releases within <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e401" xlink:type="simple"/></inline-formula>, and hence can be expressed in terms of a corresponding variation of the definition of a Markov state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e402" xlink:type="simple"/></inline-formula>. The same framework of assumptions applies as in noise model I: vesicle releases are individual events, and the functions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e403" xlink:type="simple"/></inline-formula> are assumed to be bounded from above by rate constants <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e404" xlink:type="simple"/></inline-formula>.</p>
<p>Combinations of noise model I and II are also possible. In this case, the Markov state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e405" xlink:type="simple"/></inline-formula> may contain both spike times and vesicle release times. The assumptions of noise model I/II described above apply to the corresponding stochastic neurons and vesicle releases, respectively. Altogether, note that all three types of networks (based on model I, II and mixtures of the two) are based on a common framework of definitions and assumptions: in all cases the dynamics is described in terms of stochastic components (neurons, synapses) which generate point events (spikes/vesicle releases) according to instantaneous probabilities which depend on the recent event history of the network.</p>
</sec><sec id="s4a3">
<title>Convergence of state distributions</title>
<p>Below, proofs for the existence and uniqueness of stationary distributions of network states for the considered network models are given. Furthermore, bounds on the convergence speed to this stationary distribution are provided. To obtain a comprehensive picture, convergence is studied under three different input conditions: constant, stochastic, and periodic input. All proofs are described in detail for noise model I. The results transfer in a straightforward manner to noise model II and mixtures of these two models, since the same framework of assumptions applies to all cases.</p>
</sec><sec id="s4a4">
<title>Network dynamics as a Markov process</title>
<p>We view the simulation of a cortical microcircuit model, under a given input condition and starting from a given initial network state, as a random experiment. Formally, we denote the set of all possible outcomes in this random experiment by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e406" xlink:type="simple"/></inline-formula>, the set of all considered <italic>events</italic> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e407" xlink:type="simple"/></inline-formula> (i.e. a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e408" xlink:type="simple"/></inline-formula>-algebra on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e409" xlink:type="simple"/></inline-formula>), and the probability measure which assigns a probability to each event in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e410" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e411" xlink:type="simple"/></inline-formula>. An outcome is the result of a single run of the network. An outcome is associated with an assignment of particular values to all defined random variables. An event is a set of outcomes, for example the set of all outcomes in which neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e412" xlink:type="simple"/></inline-formula> spikes within the first <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e413" xlink:type="simple"/></inline-formula> milliseconds of the experiment. Suppose <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e414" xlink:type="simple"/></inline-formula> is a random variable with some state space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e415" xlink:type="simple"/></inline-formula>, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e416" xlink:type="simple"/></inline-formula> assumes values in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e417" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e418" xlink:type="simple"/></inline-formula> is a set of events on the space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e419" xlink:type="simple"/></inline-formula>. Formally, such a random variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e420" xlink:type="simple"/></inline-formula> is defined as a map <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e421" xlink:type="simple"/></inline-formula>, which assigns a value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e422" xlink:type="simple"/></inline-formula> to every possible outcome <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e423" xlink:type="simple"/></inline-formula>. To denote the probability that the random variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e424" xlink:type="simple"/></inline-formula> assumes some value in the set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e425" xlink:type="simple"/></inline-formula>, we define the short-hand <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e426" xlink:type="simple"/></inline-formula>. Furthermore, if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e427" xlink:type="simple"/></inline-formula> is another random variable we use the notation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e428" xlink:type="simple"/></inline-formula> for conditional probabilities, and write even shorter, when unambiguous, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e429" xlink:type="simple"/></inline-formula>. The base probability space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e430" xlink:type="simple"/></inline-formula> is assumed to be rich enough such that all random variables which are needed in the following exist.</p>
<p>We define the index set of time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e431" xlink:type="simple"/></inline-formula>, and the stochastic process <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e432" xlink:type="simple"/></inline-formula>, as a description of the stochastic evolution of Markov states of a network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e433" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e434" xlink:type="simple"/></inline-formula>. For each time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e435" xlink:type="simple"/></inline-formula> we define a random variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e436" xlink:type="simple"/></inline-formula> (also written <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e437" xlink:type="simple"/></inline-formula>) representing the Markov state of the network at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e438" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e439" xlink:type="simple"/></inline-formula> takes values on the state space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e440" xlink:type="simple"/></inline-formula> of all possible Markov states of some fixed duration <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e441" xlink:type="simple"/></inline-formula>. We denote by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e442" xlink:type="simple"/></inline-formula> the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e443" xlink:type="simple"/></inline-formula>-algebra associated with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e444" xlink:type="simple"/></inline-formula>. The assumptions on the network described in the previous section imply that the process has the Markov property for Markov states of any length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e445" xlink:type="simple"/></inline-formula>, since the future evolution of the process is then entirely independent of the past, given the current Markov state. For the subsequent proofs, we therefore assume some <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e446" xlink:type="simple"/></inline-formula>. We also define a random variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e447" xlink:type="simple"/></inline-formula> of <italic>entire sample paths</italic> on the measurable space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e448" xlink:type="simple"/></inline-formula>, i.e. a map <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e449" xlink:type="simple"/></inline-formula>. Realizations of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e450" xlink:type="simple"/></inline-formula> are sample paths (or trajectories), i.e. functions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e451" xlink:type="simple"/></inline-formula>, taking values in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e452" xlink:type="simple"/></inline-formula>. Since realizations of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e453" xlink:type="simple"/></inline-formula> are functions, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e454" xlink:type="simple"/></inline-formula> can be thought of as a random function.</p>
<p>For subsequent proofs the following definition of a <italic>transition probability kernel</italic> is essential: A transition probability kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e455" xlink:type="simple"/></inline-formula> on a measurable state space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e456" xlink:type="simple"/></inline-formula> is a function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e457" xlink:type="simple"/></inline-formula>, which assigns a probability to the transition from any point <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e458" xlink:type="simple"/></inline-formula> to any set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e459" xlink:type="simple"/></inline-formula>. More precisely, if one fixes a particular “initial state” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e460" xlink:type="simple"/></inline-formula>, then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e461" xlink:type="simple"/></inline-formula> is a probability measure in its target argument <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e462" xlink:type="simple"/></inline-formula>, corresponding to the result of applying the transition kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e463" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e464" xlink:type="simple"/></inline-formula> (in addition, for each event <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e465" xlink:type="simple"/></inline-formula> in the target space, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e466" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e467" xlink:type="simple"/></inline-formula>-measurable in its source argument). Stochastic transition matrices of Markov chains are, e.g., transition probability kernels.</p>
<p>Here we write <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e468" xlink:type="simple"/></inline-formula> for the transition probability kernel corresponding to progression of the state of the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e469" xlink:type="simple"/></inline-formula> from time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e470" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e471" xlink:type="simple"/></inline-formula>, i.e.,<disp-formula id="pcbi.1003311.e472"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e472" xlink:type="simple"/><label>(6)</label></disp-formula>We further define the shorthand <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e473" xlink:type="simple"/></inline-formula> for the progression of duration <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e474" xlink:type="simple"/></inline-formula> starting from initial time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e475" xlink:type="simple"/></inline-formula>. Transition kernels can also be applied to probability measures <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e476" xlink:type="simple"/></inline-formula> of initial states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e477" xlink:type="simple"/></inline-formula> (as opposed to single initial states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e478" xlink:type="simple"/></inline-formula>). We will write <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e479" xlink:type="simple"/></inline-formula> to denote the result of applying the kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e480" xlink:type="simple"/></inline-formula> to an initial probability measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e481" xlink:type="simple"/></inline-formula>. The result <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e482" xlink:type="simple"/></inline-formula> is again a probability measure, assigning a probability to any event <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e483" xlink:type="simple"/></inline-formula> on the state space according to:<disp-formula id="pcbi.1003311.e484"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e484" xlink:type="simple"/><label>(7)</label></disp-formula>Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e485" xlink:type="simple"/></inline-formula> is again a probability measure on the state space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e486" xlink:type="simple"/></inline-formula>, transition kernels can be applied sequentially. Note that due to the Markov property one has, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e487" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e488" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4a5">
<title>Stochastic network dynamics is contracting</title>
<p>Before studying specific input conditions, a few basic key properties of the network dynamics <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e489" xlink:type="simple"/></inline-formula> are developed. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e490" xlink:type="simple"/></inline-formula> be the transition probability kernel corresponding to progression of the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e491" xlink:type="simple"/></inline-formula> from time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e492" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e493" xlink:type="simple"/></inline-formula>. For the proofs below, transitions to the <italic>resting state</italic>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e494" xlink:type="simple"/></inline-formula>, will be of particular importance. The resting state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e495" xlink:type="simple"/></inline-formula> is defined as the “empty” Markov state in which no spikes occurred within the last <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e496" xlink:type="simple"/></inline-formula> time units. The first key observation is the following Proposition:</p>
<p><bold>Proposition 1</bold> <italic>Consider the probability </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e497" xlink:type="simple"/></inline-formula><italic>, that the process </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e498" xlink:type="simple"/></inline-formula><italic> will be in the resting state </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e499" xlink:type="simple"/></inline-formula><italic> at time </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e500" xlink:type="simple"/></inline-formula><italic>, starting from some initial state </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e501" xlink:type="simple"/></inline-formula><italic> at time </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e502" xlink:type="simple"/></inline-formula><italic>. This “return probability” to the resting state is bounded from below by,</italic><disp-formula id="pcbi.1003311.e503"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e503" xlink:type="simple"/><label>(8)</label></disp-formula><italic>where </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e504" xlink:type="simple"/></inline-formula><italic>. This holds regardless of the input trajectory </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e505" xlink:type="simple"/></inline-formula><italic> driving the network.</italic></p>
<p>The proposition follows directly from the fact that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e506" xlink:type="simple"/></inline-formula> bounds the sum of all instantaneous firing rates in the network. Hence with at least probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e507" xlink:type="simple"/></inline-formula> no neuron fires within <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e508" xlink:type="simple"/></inline-formula> time units (cf. <xref ref-type="bibr" rid="pcbi.1003311-Borovkov1">[18]</xref>). In technical terms, this implies that the stochastic kernel corresponding to a duration of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e509" xlink:type="simple"/></inline-formula> fulfills the Doeblin condition <xref ref-type="bibr" rid="pcbi.1003311-Doeblin1">[104]</xref> – a property which is highly useful for proving convergence and ergodicity results.</p>
<p>Proposition 1 entails a central contraction property of stochastic networks of spiking neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e510" xlink:type="simple"/></inline-formula>, which holds in the presence of any input trajectory <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e511" xlink:type="simple"/></inline-formula>, and forms the basis for several subsequent proofs. The following definitions are essential: We will measure below the difference between any two probability distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e512" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e513" xlink:type="simple"/></inline-formula> in terms of the total variation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e514" xlink:type="simple"/></inline-formula> of the signed measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e515" xlink:type="simple"/></inline-formula>. Any such signed measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e516" xlink:type="simple"/></inline-formula> can be expressed in terms of its non-negative and non-positive components, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e517" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e518" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e519" xlink:type="simple"/></inline-formula> are both non-negative measures (but in general no probability measures). The total variation of a signed measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e520" xlink:type="simple"/></inline-formula> on a measurable space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e521" xlink:type="simple"/></inline-formula> is defined as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e522" xlink:type="simple"/></inline-formula>, i.e. the total mass of its positive and negative components. According to this definition, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e523" xlink:type="simple"/></inline-formula>.</p>
<p><bold>Lemma 1 (Contraction Lemma)</bold> <italic>The following strict contraction property holds for the Markov process </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e524" xlink:type="simple"/></inline-formula><italic>, for any </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e525" xlink:type="simple"/></inline-formula><italic>, and for any initial probability measures </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e526" xlink:type="simple"/></inline-formula><italic> and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e527" xlink:type="simple"/></inline-formula><italic> at any time </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e528" xlink:type="simple"/></inline-formula><italic>:</italic><disp-formula id="pcbi.1003311.e529"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e529" xlink:type="simple"/><label>(9)</label></disp-formula><italic>In words: applying the dynamics of the network </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e530" xlink:type="simple"/></inline-formula><italic> for </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e531" xlink:type="simple"/></inline-formula><italic> time units is guaranteed to reduce the distance between any two initial distributions </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e532" xlink:type="simple"/></inline-formula><italic> and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e533" xlink:type="simple"/></inline-formula><italic> of network states by a factor </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e534" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<p><bold>Proof:</bold> Define the auxiliary measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e535" xlink:type="simple"/></inline-formula> as zero everywhere outside <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e536" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e537" xlink:type="simple"/></inline-formula>. Rewrite <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e538" xlink:type="simple"/></inline-formula> in terms of the non-negative measures <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e539" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e540" xlink:type="simple"/></inline-formula>, such that<disp-formula id="pcbi.1003311.e541"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e541" xlink:type="simple"/><label>(10)</label></disp-formula>and note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e542" xlink:type="simple"/></inline-formula> implies that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e543" xlink:type="simple"/></inline-formula>. Then<disp-formula id="pcbi.1003311.e544"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e544" xlink:type="simple"/><label>(11)</label></disp-formula><disp-formula id="pcbi.1003311.e545"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e545" xlink:type="simple"/><label>(12)</label></disp-formula><disp-formula id="pcbi.1003311.e546"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e546" xlink:type="simple"/><label>(13)</label></disp-formula><disp-formula id="pcbi.1003311.e547"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e547" xlink:type="simple"/><label>(14)</label></disp-formula><disp-formula id="pcbi.1003311.e548"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e548" xlink:type="simple"/><label>(15)</label></disp-formula><disp-formula id="pcbi.1003311.e549"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e549" xlink:type="simple"/><label>(16)</label></disp-formula><disp-formula id="pcbi.1003311.e550"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e550" xlink:type="simple"/><label>(17)</label></disp-formula>The equality in (11) follows from linearity of transition probability kernels. The transition to (13) is an application of the triangle inequality. The transition to (14) uses the fact that both <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e551" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e552" xlink:type="simple"/></inline-formula> are non-negative: this follows from Proposition 1, which ensures that the measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e553" xlink:type="simple"/></inline-formula> has at least mass <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e554" xlink:type="simple"/></inline-formula> at the resting state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e555" xlink:type="simple"/></inline-formula> and, hence, for any (non-negative) measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e556" xlink:type="simple"/></inline-formula>,<disp-formula id="pcbi.1003311.e557"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e557" xlink:type="simple"/><label>(18)</label></disp-formula>Finally, note that (15) uses a general property of transition probability kernels <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e558" xlink:type="simple"/></inline-formula>, which ensures that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e559" xlink:type="simple"/></inline-formula>, for any non-negative measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e560" xlink:type="simple"/></inline-formula>.</p>
<p>Note that the above Contraction Lemma which holds for spiking neural networks has some similarities to Lemma 1 in <xref ref-type="bibr" rid="pcbi.1003311-Maass2">[105]</xref> who analyzed artificial analog neural networks in discrete time.</p>
</sec><sec id="s4a6">
<title>Proof of Theorem 1 for fixed input rates</title>
<p>We divided the precise formulation of Theorem 1 into two Lemmata: Lemma 2 is a precise formulation for the case where inputs are fixed (e.g. fixed input rates). Lemma 3 in the next section corresponds to the case where input rates are controlled by a Markov process. The precise assumptions on the network model required for both Lemmata are described above (see “Scope of theoretical results”).</p>
<p>Here we assume that the vector of inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e561" xlink:type="simple"/></inline-formula> provided to the network is kept fixed during a trial. Concretely, this is for example the case if there is a set of input neurons whose rates are fixed. In this case, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e562" xlink:type="simple"/></inline-formula> is a vector of input rates, which remains constant over time. The input neurons are formally considered part of the network in this case. Alternatively, a constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e563" xlink:type="simple"/></inline-formula> could correspond to constant currents which are injected into a subset of neurons.</p>
<p>Under constant input conditions, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e564" xlink:type="simple"/></inline-formula>, the dynamics of the process is time-homogeneous: the transition probability kernels are invariant to time-shifts, i.e.<disp-formula id="pcbi.1003311.e565"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e565" xlink:type="simple"/><label>(19)</label></disp-formula></p>
<p><bold>Lemma 2</bold> <italic>Let </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e566" xlink:type="simple"/></inline-formula><italic>. Then the Markov process </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e567" xlink:type="simple"/></inline-formula><italic> has a unique stationary distribution </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e568" xlink:type="simple"/></inline-formula><italic>, to which it converges exponentially fast,</italic><disp-formula id="pcbi.1003311.e569"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e569" xlink:type="simple"/><label>(20)</label></disp-formula><italic>from any initial Markov state </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e570" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<p><bold>Proof:</bold> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e571" xlink:type="simple"/></inline-formula> is clearly non-explosive, aperiodic and stochastically continuous (cf. <xref ref-type="bibr" rid="pcbi.1003311-Borovkov1">[18]</xref>). To prove exponential ergodicity it thus suffices to show that some skeleton chain is geometrically ergodic (see for example Theorem 18.1 in <xref ref-type="bibr" rid="pcbi.1003311-Borovkov2">[106]</xref>). The skeleton chain <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e572" xlink:type="simple"/></inline-formula> with transition probability kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e573" xlink:type="simple"/></inline-formula> is aperiodic and irreducible and hence has a unique stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e574" xlink:type="simple"/></inline-formula>. Then, through recursive application of Lemma 1 with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e575" xlink:type="simple"/></inline-formula>,<disp-formula id="pcbi.1003311.e576"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e576" xlink:type="simple"/><label>(21)</label></disp-formula><disp-formula id="pcbi.1003311.e577"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e577" xlink:type="simple"/><label>(22)</label></disp-formula>proving geometric ergodicity of the skeleton chain, and thus exponential ergodicity of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e578" xlink:type="simple"/></inline-formula>. The quantitative convergence bound follows from (22) by choosing a singleton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e579" xlink:type="simple"/></inline-formula> as initial distribution, and using the general fact that for any transition probability kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e580" xlink:type="simple"/></inline-formula> and distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e581" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e582" xlink:type="simple"/></inline-formula>,<disp-formula id="pcbi.1003311.e583"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e583" xlink:type="simple"/><label>(23)</label></disp-formula>thus guaranteeing that the total variation distance does not (temporarily) grow between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e584" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e585" xlink:type="simple"/></inline-formula>. </p>
<p>Lemma 2 provides a general ergodicity result for the considered class of stochastic spiking networks in the presence of fixed input rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e586" xlink:type="simple"/></inline-formula>. The proof relies on two key properties of stochastic spiking networks: aperiodicity and irreducibility. These properties can be understood intuitively in the context of <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1H</xref>. If the intrinsic network dynamics was not aperiodic, for example, then one might be able to observe oscillating pattern frequencies over time (as in <xref ref-type="fig" rid="pcbi-1003311-g004">Figure 4C</xref>). Lemma 2 proves that this cannot occur in stochastic spiking networks as long as input rates are fixed. Oscillating pattern frequencies can indeed only emerge when input rates are themselves periodically changing (see Theorem 2 and <xref ref-type="fig" rid="pcbi-1003311-g004">Figure 4</xref>). If the network dynamics was not irreducible on the other hand, i.e. if there were network states which are unreachable from some other network states, then pattern frequencies could potentially be observed to converge to different fixed points for different initial states (e.g. the two lines in <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1H</xref> settling at different values). This cannot occur in stochastic spiking networks due to Proposition 1 which guarantees that the state space is connected through the resting state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e587" xlink:type="simple"/></inline-formula>.</p>
<p>Note that, although aperiodicity and irreducibility are well known necessary and sufficient conditions for ergodicity in discrete time Markov chains on finite state spaces, they are not sufficient for exponential ergodicity in continuous time Markov processes on general state spaces (see <xref ref-type="bibr" rid="pcbi.1003311-Down1">[107]</xref> for precise definitions of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e588" xlink:type="simple"/></inline-formula>-irreducibility and aperiodicity for such processes). Additional conditions in this more complex case which ensure exponential ergodicity, such as nonexplosivity, stochastic continuity and geometric ergodicity of a skeleton chain, have also been taken into account in the proof for Lemma 2 (i.e. stochastic spiking networks also meet these additional criteria).</p>
<p>Lemma 2 constitutes a proof for Theorem 1 for fixed input rates <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e589" xlink:type="simple"/></inline-formula>. In the main text we refer to the stationary distribution of the circuit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e590" xlink:type="simple"/></inline-formula> under fixed input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e591" xlink:type="simple"/></inline-formula> as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e592" xlink:type="simple"/></inline-formula>. The proof above guarantees a stationary distribution for both Markov and simple states. In the main text <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e593" xlink:type="simple"/></inline-formula> refers to the simple network state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e594" xlink:type="simple"/></inline-formula> if not stated otherwise.</p>
</sec><sec id="s4a7">
<title>Proof of Theorem 1 for input rates controlled by a Markov process</title>
<p>Fixed input assumptions may often hold for the external input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e595" xlink:type="simple"/></inline-formula>, driving a stochastic computation in a neural system <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e596" xlink:type="simple"/></inline-formula>, only approximately. Stochastic fluctuations on various spatial and temporal scales may be present in the input. In addition, inputs may have their own short-term stochastic dynamics: Imagine, for example, a visual scene of randomly moving dots. Despite the presence of such short-term dynamical features in the input, in many cases one may still suspect that network state distributions converge. Indeed, below we generalize the convergence results from the constant case to the quite large class of stochastic (and stochastically changing) inputs which are generated by a uniformly ergodic Markov process. Uniform ergodicity is defined as exponential ergodicity (exponentially fast convergence to a unique stationary distribution) with convergence constants which apply uniformly to all initial states <xref ref-type="bibr" rid="pcbi.1003311-Down1">[107]</xref> (this holds for example for the convergence constants in Lemma 2).</p>
<p>Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e597" xlink:type="simple"/></inline-formula> be a time-homogeneous <italic>input Markov process</italic>, in the sense that the input trajectory <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e598" xlink:type="simple"/></inline-formula> provided to the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e599" xlink:type="simple"/></inline-formula> is itself generated randomly from a Markov process <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e600" xlink:type="simple"/></inline-formula>. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e601" xlink:type="simple"/></inline-formula> be the (measurable) state space of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e602" xlink:type="simple"/></inline-formula>. Then define a joint input/network Markov process <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e603" xlink:type="simple"/></inline-formula> on the state space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e604" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e605" xlink:type="simple"/></inline-formula> denotes the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e606" xlink:type="simple"/></inline-formula>-algebra generated by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e607" xlink:type="simple"/></inline-formula>. Further definitions for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e608" xlink:type="simple"/></inline-formula> are analogous to those introduced for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e609" xlink:type="simple"/></inline-formula>.</p>
<p><bold>Lemma 3</bold> <italic>If the input process </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e610" xlink:type="simple"/></inline-formula><italic> is uniformly ergodic, then the joint Markov process </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e611" xlink:type="simple"/></inline-formula><italic> has a unique stationary distribution </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e612" xlink:type="simple"/></inline-formula><italic> on the joint input/network state space, to which convergence occurs exponentially fast, i.e. there exist constants </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e613" xlink:type="simple"/></inline-formula><italic>, such that</italic><disp-formula id="pcbi.1003311.e614"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e614" xlink:type="simple"/><label>(24)</label></disp-formula><italic>for any initial state </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e615" xlink:type="simple"/></inline-formula><italic> of the joint Markov process </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e616" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<p><bold>Proof:</bold> If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e617" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e618" xlink:type="simple"/></inline-formula> were entirely independent processes (if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e619" xlink:type="simple"/></inline-formula> did not influence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e620" xlink:type="simple"/></inline-formula>) then the joint process <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e621" xlink:type="simple"/></inline-formula> would automatically be exponentially ergodic if both <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e622" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e623" xlink:type="simple"/></inline-formula> are. Although in the present case <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e624" xlink:type="simple"/></inline-formula> is not independent of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e625" xlink:type="simple"/></inline-formula>, a weaker version of independence applies: the return probability to the resting state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e626" xlink:type="simple"/></inline-formula> during <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e627" xlink:type="simple"/></inline-formula> is at least <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e628" xlink:type="simple"/></inline-formula> <italic>regardless</italic> of the input trajectory of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e629" xlink:type="simple"/></inline-formula> during that time. This property can be exploited to show that the distribution of hitting times to a joint resting state has an exponential bound. It follows that the joint process is exponentially ergodic. A detailed proof is given in the next section. </p>
<p>The second part of Theorem 1 (exponentially fast convergence for the case of external input generated by an ergodic Markov process) follows from Lemma 3. Note that in the main text we slightly abuse the notation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e630" xlink:type="simple"/></inline-formula> for the dynamic case to indicate the stationary distribution over network states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e631" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e632" xlink:type="simple"/></inline-formula> denotes a specific Markov process controlling the inputs.</p>
</sec><sec id="s4a8">
<title>Detailed proof of Lemma 3</title>
<p>We have split the proof of Lemma 3 into proofs of four auxiliary claims (Propositions 2–5). Consider the following variations of Proposition 1, which hold for the Markov process <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e633" xlink:type="simple"/></inline-formula> describing the joint dynamics of input and network states. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e634" xlink:type="simple"/></inline-formula> denote a particular input sequence defined for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e635" xlink:type="simple"/></inline-formula> (a realization of the input process <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e636" xlink:type="simple"/></inline-formula>) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e637" xlink:type="simple"/></inline-formula> an initial network Markov state (with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e638" xlink:type="simple"/></inline-formula>) at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e639" xlink:type="simple"/></inline-formula>. Then<disp-formula id="pcbi.1003311.e640"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e640" xlink:type="simple"/><label>(25)</label></disp-formula><disp-formula id="pcbi.1003311.e641"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e641" xlink:type="simple"/><label>(26)</label></disp-formula></p>
<p>It is easy to show that these properties, together with the fact that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e642" xlink:type="simple"/></inline-formula> is uniformly ergodic, ensure that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e643" xlink:type="simple"/></inline-formula> is irreducible and aperiodic. Hence, to prove exponential ergodicity of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e644" xlink:type="simple"/></inline-formula> it suffices to show that some skeleton chain is geometrically ergodic <xref ref-type="bibr" rid="pcbi.1003311-Down1">[107]</xref>. To that end, we will consider the skeleton chain <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e645" xlink:type="simple"/></inline-formula> and prove geometric ergodicity by showing that the hitting time distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e646" xlink:type="simple"/></inline-formula> to a <italic>small set</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e647" xlink:type="simple"/></inline-formula> on the joint state space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e648" xlink:type="simple"/></inline-formula> of input and network states admits an exponential bound.</p>
<p>The <italic>hitting time</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e649" xlink:type="simple"/></inline-formula> to some set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e650" xlink:type="simple"/></inline-formula> on the input state space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e651" xlink:type="simple"/></inline-formula> is defined as<disp-formula id="pcbi.1003311.e652"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e652" xlink:type="simple"/><label>(27)</label></disp-formula>For notational convenience we abbreviate in the following <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e653" xlink:type="simple"/></inline-formula>. Due to uniform ergodicity of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e654" xlink:type="simple"/></inline-formula> (which implies Harris recurrence <xref ref-type="bibr" rid="pcbi.1003311-Down1">[107]</xref>), there exists some set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e655" xlink:type="simple"/></inline-formula> to which the hitting time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e656" xlink:type="simple"/></inline-formula> is finite (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e657" xlink:type="simple"/></inline-formula>) from any initial state, with probability one <xref ref-type="bibr" rid="pcbi.1003311-Meyn1">[108]</xref>. Furthermore, there exists according to <xref ref-type="bibr" rid="pcbi.1003311-Down1">[107]</xref> a <italic>small set</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e658" xlink:type="simple"/></inline-formula> and constants <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e659" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e660" xlink:type="simple"/></inline-formula>, such that<disp-formula id="pcbi.1003311.e661"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e661" xlink:type="simple"/><label>(28)</label></disp-formula>This implies that there exists a small set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e662" xlink:type="simple"/></inline-formula> on the input state space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e663" xlink:type="simple"/></inline-formula> which can not only be reached in finite time from any initial input state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e664" xlink:type="simple"/></inline-formula>, but for which the hitting time distribution to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e665" xlink:type="simple"/></inline-formula> has also finite mean and variance (and finite higher-order moments). At least one pair of constants <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e666" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e667" xlink:type="simple"/></inline-formula> which fulfills (28) is guaranteed to exist, but in fact the following Proposition shows that one can specify a particular desired bound on the right-hand side (for reasons which will become clear later), and find a matching <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e668" xlink:type="simple"/></inline-formula> on the left-hand side.</p>
<p><bold>Proposition 2</bold> <italic>There exists a </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e669" xlink:type="simple"/></inline-formula><italic>, such that</italic><disp-formula id="pcbi.1003311.e670"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e670" xlink:type="simple"/><label>(29)</label></disp-formula></p>
<p><bold>Proof:</bold> Define <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e671" xlink:type="simple"/></inline-formula>. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e672" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e673" xlink:type="simple"/></inline-formula> be any valid pair of constants which fulfills (28). The trivial case is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e674" xlink:type="simple"/></inline-formula>. In the remainder of the proof it is assumed that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e675" xlink:type="simple"/></inline-formula> is “too large”, such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e676" xlink:type="simple"/></inline-formula>. By definition of the exponential function, for any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e677" xlink:type="simple"/></inline-formula>,<disp-formula id="pcbi.1003311.e678"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e678" xlink:type="simple"/><label>(30)</label></disp-formula><disp-formula id="pcbi.1003311.e679"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e679" xlink:type="simple"/><label>(31)</label></disp-formula>By Tonelli's theorem, since all summands are non-negative, the order of the double sum can be exchanged:<disp-formula id="pcbi.1003311.e680"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e680" xlink:type="simple"/><label>(32)</label></disp-formula><disp-formula id="pcbi.1003311.e681"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e681" xlink:type="simple"/><label>(33)</label></disp-formula>Note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e682" xlink:type="simple"/></inline-formula> are the moments of the distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e683" xlink:type="simple"/></inline-formula>. By uniform ergodicity of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e684" xlink:type="simple"/></inline-formula>, all moments must exist, and in addition there exists a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e685" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e686" xlink:type="simple"/></inline-formula>. It is straightforward to see that the series then converges for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e687" xlink:type="simple"/></inline-formula>, such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e688" xlink:type="simple"/></inline-formula> is continuous on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e689" xlink:type="simple"/></inline-formula>. Finally, since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e690" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e691" xlink:type="simple"/></inline-formula>, by the intermediate value theorem there exists some <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e692" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e693" xlink:type="simple"/></inline-formula>.</p>
<p>Denote by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e694" xlink:type="simple"/></inline-formula> the time at which the skeleton chain <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e695" xlink:type="simple"/></inline-formula> visits the small set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e696" xlink:type="simple"/></inline-formula> for the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e697" xlink:type="simple"/></inline-formula>-th time:<disp-formula id="pcbi.1003311.e698"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e698" xlink:type="simple"/><label>(34)</label></disp-formula>Furthermore, denote by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e699" xlink:type="simple"/></inline-formula> the time between the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e700" xlink:type="simple"/></inline-formula>-th and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e701" xlink:type="simple"/></inline-formula>-th visit:<disp-formula id="pcbi.1003311.e702"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e702" xlink:type="simple"/><label>(35)</label></disp-formula><disp-formula id="pcbi.1003311.e703"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e703" xlink:type="simple"/><label>(36)</label></disp-formula></p>
<p>According to this definition, one can express the hitting time of degree <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e704" xlink:type="simple"/></inline-formula> as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e705" xlink:type="simple"/></inline-formula>. The following Proposition extends the exponential bound on the first hitting time to hitting times of higher degrees.</p>
<p><bold>Proposition 3</bold> <italic>There exists a </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e706" xlink:type="simple"/></inline-formula><italic>, such that,</italic><disp-formula id="pcbi.1003311.e707"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e707" xlink:type="simple"/><label>(37)</label></disp-formula></p>
<p><bold>Proof:</bold><disp-formula id="pcbi.1003311.e708"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e708" xlink:type="simple"/><label>(38)</label></disp-formula><disp-formula id="pcbi.1003311.e709"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e709" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003311.e710"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e710" xlink:type="simple"/><label>(39)</label></disp-formula><disp-formula id="pcbi.1003311.e711"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e711" xlink:type="simple"/><label>(40)</label></disp-formula></p>
<p>Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e712" xlink:type="simple"/></inline-formula> be the hitting time to the small set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e713" xlink:type="simple"/></inline-formula> on the joint state space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e714" xlink:type="simple"/></inline-formula> of input and network states,<disp-formula id="pcbi.1003311.e715"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e715" xlink:type="simple"/><label>(41)</label></disp-formula></p>
<p>Furthermore, let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e716" xlink:type="simple"/></inline-formula> be the number of visits to the small set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e717" xlink:type="simple"/></inline-formula> prior to and including time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e718" xlink:type="simple"/></inline-formula>,<disp-formula id="pcbi.1003311.e719"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e719" xlink:type="simple"/><label>(42)</label></disp-formula></p>
<p><bold>Proposition 4</bold> <italic>For any input trajectory </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e720" xlink:type="simple"/></inline-formula><italic> and any initial network state </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e721" xlink:type="simple"/></inline-formula><italic>,</italic><disp-formula id="pcbi.1003311.e722"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e722" xlink:type="simple"/><label>(43)</label></disp-formula></p>
<p>This follows from (25) and (26) which ensure that whenever the input process visits the small set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e723" xlink:type="simple"/></inline-formula>, there is also a small probability that the network is in the resting state.</p>
<p><bold>Proposition 5</bold> <italic>There exists a </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e724" xlink:type="simple"/></inline-formula><italic> and a constant </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e725" xlink:type="simple"/></inline-formula><italic> such that,</italic><disp-formula id="pcbi.1003311.e726"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e726" xlink:type="simple"/><label>(44)</label></disp-formula></p>
<p><bold>Proof:</bold> Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e727" xlink:type="simple"/></inline-formula>. Choose some <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e728" xlink:type="simple"/></inline-formula> which fulfills Proposition 3.<disp-formula id="pcbi.1003311.e729"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e729" xlink:type="simple"/><label>(45)</label></disp-formula><disp-formula id="pcbi.1003311.e730"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e730" xlink:type="simple"/><label>(46)</label></disp-formula><disp-formula id="pcbi.1003311.e731"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e731" xlink:type="simple"/><label>(47)</label></disp-formula><disp-formula id="pcbi.1003311.e732"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e732" xlink:type="simple"/><label>(48)</label></disp-formula><disp-formula id="pcbi.1003311.e733"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e733" xlink:type="simple"/><label>(49)</label></disp-formula><disp-formula id="pcbi.1003311.e734"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e734" xlink:type="simple"/><label>(50)</label></disp-formula><disp-formula id="pcbi.1003311.e735"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e735" xlink:type="simple"/><label>(51)</label></disp-formula></p>
<p>By Proposition 5, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e736" xlink:type="simple"/></inline-formula> is exponentially ergodic <xref ref-type="bibr" rid="pcbi.1003311-Down1">[107]</xref>. This completes the proof of Lemma 3.</p>
</sec><sec id="s4a9">
<title>Distribution of trajectories of network states</title>
<p>The Markov states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e737" xlink:type="simple"/></inline-formula> are segments of spiking trajectories of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e738" xlink:type="simple"/></inline-formula>. Hence, all statements developed above apply to convergence of the distribution over these (short) spiking trajectories. If one is interested in the convergence of longer trajectories, the simplest option is to choose a larger <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e739" xlink:type="simple"/></inline-formula>, since any finite <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e740" xlink:type="simple"/></inline-formula> is admissible, and all convergence results readily extend to trajectories of any finite length. A limitation of this approach is that the quantitative convergence statements will suffer from making <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e741" xlink:type="simple"/></inline-formula> too large, since convergence rates scale approximately with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e742" xlink:type="simple"/></inline-formula> (and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e743" xlink:type="simple"/></inline-formula>). Hence, in practice, empirical convergence tests are required to make statements about specific circuits.</p>
</sec></sec><sec id="s4b">
<title>Proof of Theorem 2</title>
<p>If the input sequence is periodic with period <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e744" xlink:type="simple"/></inline-formula>, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e745" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e746" xlink:type="simple"/></inline-formula>, then the Markov process <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e747" xlink:type="simple"/></inline-formula> will be time-periodic, in the sense that transition kernels are invariant to shifts which are multiples of the period <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e748" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003311.e749"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e749" xlink:type="simple"/><label>(52)</label></disp-formula></p>
<p>This implies the following result, which is a more precise version of Theorem 2:</p>
<p><bold>Lemma 4</bold> <italic>Under periodic input, i.e. </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e750" xlink:type="simple"/></inline-formula><italic> for all </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e751" xlink:type="simple"/></inline-formula><italic> with some </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e752" xlink:type="simple"/></inline-formula><italic>, the time-periodic Markov process </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e753" xlink:type="simple"/></inline-formula><italic> with period </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e754" xlink:type="simple"/></inline-formula><italic> has a periodically stationary distribution </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e755" xlink:type="simple"/></inline-formula><italic>, to which convergence occurs exponentially fast from any initial state. In particular, for every </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e756" xlink:type="simple"/></inline-formula><italic> there exists a unique stationary distribution </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e757" xlink:type="simple"/></inline-formula><italic> such that,</italic><disp-formula id="pcbi.1003311.e758"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e758" xlink:type="simple"/><label>(53)</label></disp-formula><italic>from any initial Markov state </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e759" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<p><bold>Proof:</bold> For each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e760" xlink:type="simple"/></inline-formula> there exists a skeleton chain <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e761" xlink:type="simple"/></inline-formula>, with transition probability kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e762" xlink:type="simple"/></inline-formula>, which is time-homogeneous, irreducible, and aperiodic and thus has a unique stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e763" xlink:type="simple"/></inline-formula>. An application of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e764" xlink:type="simple"/></inline-formula>, which corresponds to a full period, decreases the total variation distance to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e765" xlink:type="simple"/></inline-formula> by at least <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e766" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003311.e767"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e767" xlink:type="simple"/><label>(54)</label></disp-formula><disp-formula id="pcbi.1003311.e768"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e768" xlink:type="simple"/><label>(55)</label></disp-formula><disp-formula id="pcbi.1003311.e769"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e769" xlink:type="simple"/><label>(56)</label></disp-formula>The first inequality follows from the fact that applying the remaining <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e770" xlink:type="simple"/></inline-formula> can only further decrease the total variation distance between the two distributions, according to (23). The second inequality is due to Lemma 1.</p>
<p>Lemma 4 then follows from recursive application of (54)–(56) for multiple periods, and choosing a singleton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e771" xlink:type="simple"/></inline-formula> as initial distribution.</p>
<p>In the main text, we use the notation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e772" xlink:type="simple"/></inline-formula> for a phase-specific stationary distribution, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e773" xlink:type="simple"/></inline-formula> denotes a specific periodic input sequence.</p>
</sec><sec id="s4c">
<title>Relation to previous theoretical work</title>
<p>Previous work on the question whether states of spiking neural networks might converge to a unique stationary distribution had focused on the case where neuronal integration of incoming spikes occurs in a linear fashion, i.e., linear subthreshold dynamics followed by a single output non-linearity <xref ref-type="bibr" rid="pcbi.1003311-Brmaud1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Borovkov1">[18]</xref>. In addition these earlier publications did not allow for the experimentally observed short term dynamics of synapses. The earlier publication <xref ref-type="bibr" rid="pcbi.1003311-Brmaud1">[17]</xref> had studied this question as a special case of the mathematical framework of non-linear Hawkes processes, a class of mutually exciting point processes (see also <xref ref-type="bibr" rid="pcbi.1003311-Massouli1">[109]</xref>). The authors had arrived for the more restricted type of neurons which they considered at exponential convergence guarantees under a similar set of assumptions as in this article, in particular bounded memory and bounded instantaneous firing rates (and these results can thus be seen as a special case of Theorem 1, for the case of constant external input). <xref ref-type="bibr" rid="pcbi.1003311-Brmaud1">[17]</xref> also derived convergence results for linearly integrating neurons with unbounded memory dynamics under a different set of assumptions, in particular Lipschitz conditions on the output non-linearity and constraints on the effective connectivity matrix of the network. Whether such alternative set of assumptions can be found also in the context of non-linear integration of incoming spikes (needed e.g. for synaptic short-time dynamics or dendritic non-linearities) remains an open question.</p>
<p>The recent publication <xref ref-type="bibr" rid="pcbi.1003311-Borovkov1">[18]</xref> also focused on neurons with linear sub-threshold dynamics followed by an output non-linearity (termed there non-linear Poisson neurons) with static synapses, and extended the convergence results of <xref ref-type="bibr" rid="pcbi.1003311-Brmaud1">[17]</xref> to networks with Hebbian learning mechanisms. In addition, an important methodological innovation by <xref ref-type="bibr" rid="pcbi.1003311-Borovkov1">[18]</xref> was the introduction of spike history states (which are equivalent to the Markov states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e774" xlink:type="simple"/></inline-formula> in this article) which allowed them to study convergence in the framework of general Markov processes (in contrast to point processes in <xref ref-type="bibr" rid="pcbi.1003311-Brmaud1">[17]</xref>). Theorem 1 in this article contains as a special case the convergence results of <xref ref-type="bibr" rid="pcbi.1003311-Borovkov1">[18]</xref> for their Model I (non-linear Poisson neurons in the absence of Hebbian learning). We note that although <xref ref-type="bibr" rid="pcbi.1003311-Borovkov1">[18]</xref> focused on neurons with linear sub-threshold dynamics (and required that firing rates are strictly greater than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e775" xlink:type="simple"/></inline-formula>), their method of proof for Model I could be readily extended to cover also non-linear sub-threshold dynamics to yield the first part of our Theorem 1 (the case where inputs have constant firing rates).</p>
<p>We are not aware of previous work that studied convergence in spiking networks with dynamic synapses, or in the presence of stochastic or periodic inputs (see the second part of Theorem 1 concerning Markov processes as input, and Theorem 2). We further note that our method of proof builds on a new and rather intuitive intermediate result, Lemma 1 (Contraction Lemma), which may be useful in its own right for two reasons. On the one hand it provides more direct insight into the mechanisms responsible for convergence (the contraction between any two distributions). On the other hand, it holds regardless of the input trajectory <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e776" xlink:type="simple"/></inline-formula>, and hence has in fact an even larger scope of applicability than Theorem 1 and 2. Hence, Lemma 1 could be, for example, applied to study non-stationary evolutions of state distributions in response to arbitrary input trajectories.</p>
</sec><sec id="s4d">
<title>Extracting knowledge from internally stored distributions of network states</title>
<p>A key advantage of sample-based representations of probability distributions is that probabilities and expected values are in principle straightforward to estimate: To estimate the expected value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e777" xlink:type="simple"/></inline-formula> of a function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e778" xlink:type="simple"/></inline-formula> under a distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e779" xlink:type="simple"/></inline-formula> from a number of samples <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e780" xlink:type="simple"/></inline-formula>, simply apply the function to each sample and compute the time average <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e781" xlink:type="simple"/></inline-formula>. As long as the samples <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e782" xlink:type="simple"/></inline-formula> are distributed according to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e783" xlink:type="simple"/></inline-formula>, either independently drawn, or as the result of an ergodic Markov chain/process with stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e784" xlink:type="simple"/></inline-formula>, this estimate is guaranteed to converge to the correct value as one increases the number of samples <xref ref-type="bibr" rid="pcbi.1003311-Gray1">[110]</xref>, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e785" xlink:type="simple"/></inline-formula>. Estimates based on a finite observation window represent an approximation to this exact value.</p>
<p>Under the mild assumptions of Theorem 1 the dynamics of a stochastic spiking network in response to an input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e786" xlink:type="simple"/></inline-formula> are exponentially ergodic and there exists a unique stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e787" xlink:type="simple"/></inline-formula>, according to which network states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e788" xlink:type="simple"/></inline-formula> are distributed. Hence, the expected value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e789" xlink:type="simple"/></inline-formula> of any function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e790" xlink:type="simple"/></inline-formula> under the stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e791" xlink:type="simple"/></inline-formula> can be estimated by computing the sample-based time average<disp-formula id="pcbi.1003311.e792"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e792" xlink:type="simple"/><label>(57)</label></disp-formula></p>
<p>This approach can also be used to estimate marginal probabilities, since probabilities can be expressed as expected values, for example,<disp-formula id="pcbi.1003311.e793"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e793" xlink:type="simple"/><label>(58)</label></disp-formula><disp-formula id="pcbi.1003311.e794"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e794" xlink:type="simple"/><label>(59)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e795" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e796" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e797" xlink:type="simple"/></inline-formula> otherwise. Hence, in order to estimate the probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e798" xlink:type="simple"/></inline-formula> it suffices to measure the relative time the neuron spends in its active state, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e799" xlink:type="simple"/></inline-formula>. Similarly, to estimate the probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e800" xlink:type="simple"/></inline-formula> it is sufficient to keep track of the relative frequency of the pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e801" xlink:type="simple"/></inline-formula>, by computing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e802" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4e">
<title>Simulations of data-based cortical microcircuit models</title>
<p>All simulations of microcircuit models for <xref ref-type="fig" rid="pcbi-1003311-g001">Figures 1</xref>–<xref ref-type="fig" rid="pcbi-1003311-g002"/><xref ref-type="fig" rid="pcbi-1003311-g003"/><xref ref-type="fig" rid="pcbi-1003311-g004">4</xref> were carried out in PCSIM <xref ref-type="bibr" rid="pcbi.1003311-Pecevski2">[111]</xref>. A time step of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e803" xlink:type="simple"/></inline-formula> was chosen throughout. Further analysis of spike trains was performed in Python <xref ref-type="bibr" rid="pcbi.1003311-vanRossum1">[112]</xref>.</p>
<sec id="s4e1">
<title>Stochastic neuron model</title>
<p>A stochastic variation of the leaky integrate-and-fire model with conductance-based integration of synaptic inputs was used, for both excitatory and inhibitory neurons. Sub-threshold dynamics of the membrane potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e804" xlink:type="simple"/></inline-formula> was defined according to a standard leaky integration model with conductance-based synapses <xref ref-type="bibr" rid="pcbi.1003311-Gerstner1">[113]</xref>, using passive membrane parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e805" xlink:type="simple"/></inline-formula> and a resting potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e806" xlink:type="simple"/></inline-formula>. At simulation start, initial potentials were randomly chosen from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e807" xlink:type="simple"/></inline-formula>. Reversal potentials for excitatory synapses and inhibitory synapses were set to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e808" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e809" xlink:type="simple"/></inline-formula>, respectively. Neuronal noise was modeled by a voltage-dependent instantaneous probability of firing (instead of a fixed threshold) <xref ref-type="bibr" rid="pcbi.1003311-Jolivet1">[48]</xref>,<disp-formula id="pcbi.1003311.e810"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e810" xlink:type="simple"/><label>(60)</label></disp-formula>for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e811" xlink:type="simple"/></inline-formula>, with parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e812" xlink:type="simple"/></inline-formula> taken from <xref ref-type="bibr" rid="pcbi.1003311-Jolivet1">[48]</xref>. In contrast to <xref ref-type="bibr" rid="pcbi.1003311-Jolivet1">[48]</xref> we used a non-adaptive threshold, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e813" xlink:type="simple"/></inline-formula>. After a spike, a neuron enters an absolute refractory period of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e814" xlink:type="simple"/></inline-formula>. Thereafter, the membrane is reset to the resting potential and leaky integration is continued. Altogether, the resulting neuronal spiking mechanism is consistent with the theoretical noise model I described in <xref ref-type="disp-formula" rid="pcbi.1003311.e349">equation (4)</xref>.</p>
<p>Note that Theorem 1 also holds for substantially more complex multi-compartment neuron models incorporating, for example, data on signal integration in the dendritic tuft of pyramidal cells <xref ref-type="bibr" rid="pcbi.1003311-Larkum1">[114]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Jiang1">[115]</xref>, and data on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e815" xlink:type="simple"/></inline-formula>-spikes in pyramidal cells on layer 5 <xref ref-type="bibr" rid="pcbi.1003311-Larkum2">[116]</xref>, but we have not yet integrated these into the simulated microcircuit model because of a lack of coherent quantitative data for all the neuron types involved.</p>
</sec><sec id="s4e2">
<title>Synaptic short-term plasticity</title>
<p>The short-term dynamics of synapses in all data-based simulations was modeled according to <xref ref-type="bibr" rid="pcbi.1003311-Markram1">[47]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Maass3">[117]</xref>. The model predicts that at a synapse with “weight” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e816" xlink:type="simple"/></inline-formula> the amplitude <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e817" xlink:type="simple"/></inline-formula> of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e818" xlink:type="simple"/></inline-formula> spike in a spike train with interspike intervals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e819" xlink:type="simple"/></inline-formula> is given by,<disp-formula id="pcbi.1003311.e820"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e820" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003311.e821"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e821" xlink:type="simple"/><label>(61)</label></disp-formula><disp-formula id="pcbi.1003311.e822"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e822" xlink:type="simple"/></disp-formula>where the hidden dynamic variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e823" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e824" xlink:type="simple"/></inline-formula> are initialized for the first spike to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e825" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e826" xlink:type="simple"/></inline-formula>. The parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e827" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e828" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e829" xlink:type="simple"/></inline-formula> represent the utilization of the synaptic efficacy of the first spike after a resting state, the recovery and the facilitation time constants, respectively. These parameters were set based on experimental data on short-term plasticity in dependence of pre- and post-synaptic neuron (excitatory or inhibitory) as in <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref> (see in particular <xref ref-type="table" rid="pcbi-1003311-t001">Table 1</xref> in this reference), by randomly drawing for each neuron values for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e830" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e831" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e832" xlink:type="simple"/></inline-formula> from corresponding data-based Gaussian distributions.</p>
<table-wrap id="pcbi-1003311-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003311.t001</object-id><label>Table 1</label><caption>
<title>Number of randomly chosen neurons per pool for readout neuron in <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2G</xref>.</title>
</caption><alternatives><graphic id="pcbi-1003311-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003311.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">E</td>
<td align="left" rowspan="1" colspan="1">I</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">L2/3</td>
<td align="left" rowspan="1" colspan="1">120</td>
<td align="left" rowspan="1" colspan="1">30</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">L4</td>
<td align="left" rowspan="1" colspan="1">80</td>
<td align="left" rowspan="1" colspan="1">20</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">L5</td>
<td align="left" rowspan="1" colspan="1">200</td>
<td align="left" rowspan="1" colspan="1">50</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap></sec><sec id="s4e3">
<title>Connectivity and synaptic parameters</title>
<p>Synaptic parameters and connectivity rules for the data-based cortical column model were taken from <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref>, see <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1A</xref>. In particular, we adopted from <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref> the connection probabilities and transmission delays for each type of connection (EE, EI, IE, II) and each cortical layer (<xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref>, <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1</xref>), as well as short-term plasticity parameters. Furthermore, synaptic efficacies of individual synapses were drawn from Gamma distributions with data-based means and variances for each type of connection (EE, EI, IE, II) taken from <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref>. Two input streams were connected to the microcircuit, each consisting of 40 input neurons. In contrast to <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref> we used rate-based Poisson input neurons instead of injecting “frozen” spike patterns. Background synaptic inputs were emulated as in <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref> via background input currents to each neuron, with conductances modeled according to <xref ref-type="bibr" rid="pcbi.1003311-Destexhe1">[118]</xref>. To adjust connectivity for cortical microcircuit models of different sizes, we also adopted the method proposed by <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref>, in which recurrent weights are scaled inversely proportional to network size.</p>
<p>We tested the validity of our cortical microcircuit model by comparing the average activity of different layers (see <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2A</xref>) under various conditions against the values reported by <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref>. We confirmed that all layers exhibited very similar average activity to <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref> under all considered conditions.</p>
</sec></sec><sec id="s4f">
<title>Details to small microcircuit model in <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1</xref></title>
<p>The small cortical microcircuit model of <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1B</xref> was constructed based on the cortical column template of <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref>: Synaptic connections between neurons and their weights were chosen to approximately reflect connection probabilities and mean synaptic strengths of the cortical column template <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref>. Due to the very small size of this network, the resulting dynamics was not immediately satisfactory (for example, the influence of inputs on Layer 5 neurons was too weak). To shift the circuit into a more responsive regime, we manually adjusted a few synaptic weights and neuronal excitabilities. In particular, we injected small constant currents into some of the neurons to modulate their intrinsic excitability. Furthermore, to increase activity and correlations between highlighted neurons 2, 7 and 8, we increased synaptic weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e833" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e834" xlink:type="simple"/></inline-formula> by factors 5 and 10, respectively. To set the initial Markov state of the network, preparatory input was shown for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e835" xlink:type="simple"/></inline-formula> before the actual start of the simulation. Two different preparatory inputs were injected to set the two initial states considered in <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1F–H</xref> (first: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e836" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e837" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e838" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e839" xlink:type="simple"/></inline-formula>, second: both <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e840" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e841" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e842" xlink:type="simple"/></inline-formula>). To reproduce the same initial Markov state in multiple trials (for example the two trials shown in <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1F</xref>), the same random seed was used during the preparatory phase for these trials. The random seed was then reinitialized at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e843" xlink:type="simple"/></inline-formula> to different values for each trial.</p>
</sec><sec id="s4g">
<title>Estimates of required computation time</title>
<sec id="s4g1">
<title>Gelman-Rubin univariate and multivariate analysis</title>
<p>Various methods have been developed for measuring convergence speed to a stationary distribution in the context of Markov chain Monte Carlo sampling <xref ref-type="bibr" rid="pcbi.1003311-Cowles1">[56]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Brooks2">[119]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-ElAdlouni1">[120]</xref>. The Gelman Rubin diagnostic, which we adopted in this article, is one of the most widely used methods <xref ref-type="bibr" rid="pcbi.1003311-Gelman1">[55]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Brooks1">[57]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Gjoka1">[58]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Brooks2">[119]</xref>, besides other popular methods such as the diagnostics by Raftery and Lewis <xref ref-type="bibr" rid="pcbi.1003311-Raftery1">[121]</xref> and by Geweke <xref ref-type="bibr" rid="pcbi.1003311-Geweke1">[122]</xref>. We remark that the consensus in the literature is that no single method is perfect in general. Some attractive properties of the Gelman Rubin method are general applicability to any MCMC system (some other methods only work, for example, in the context of Gibbs sampling), ease of use, ease of implementation, computational efficiency, and the fact that results are quantitative (in contrast to graphical diagnostics) <xref ref-type="bibr" rid="pcbi.1003311-Cowles1">[56]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Brooks2">[119]</xref>.</p>
<p>The Gelman-Rubin convergence diagnostic <xref ref-type="bibr" rid="pcbi.1003311-Gelman1">[55]</xref> takes as input samples from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e844" xlink:type="simple"/></inline-formula> different runs (trials/chains/sequences) produced by the same system, started from different initial states. The method was originally developed for discrete-time systems in the context of Markov Chain Monte Carlo sampling. Our simulations use a time step of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e845" xlink:type="simple"/></inline-formula>, so we simply treat each simulation step as one discrete time step in a Markov chain. The Gelman-Rubin method produces as output the potential scale reduction factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e846" xlink:type="simple"/></inline-formula> as a function of time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e847" xlink:type="simple"/></inline-formula>. The scale reduction factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e848" xlink:type="simple"/></inline-formula> is an indicator for whether or not the system has converged at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e849" xlink:type="simple"/></inline-formula>. High values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e850" xlink:type="simple"/></inline-formula> indicate that more time is needed until convergence, while values close to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e851" xlink:type="simple"/></inline-formula> suggest that convergence has (almost) taken place.</p>
<p>For computing the scale reduction factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e852" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e853" xlink:type="simple"/></inline-formula>, samples from the period <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e854" xlink:type="simple"/></inline-formula> from each run of the network are taken into account. In the univariate case one focuses on a particular single variable (such as the marginal simple state of a single neuron, or the simple state of a “random readout” neuron as in the solid lines of <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2G</xref>). Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e855" xlink:type="simple"/></inline-formula> be the number of samples obtained from the period <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e856" xlink:type="simple"/></inline-formula> from each of the simulations. Then one defines<disp-formula id="pcbi.1003311.e857"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e857" xlink:type="simple"/><label>(62)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e858" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e859" xlink:type="simple"/></inline-formula> are between and within-sequence variances, respectively, which can be computed as described in <xref ref-type="bibr" rid="pcbi.1003311-Gelman1">[55]</xref>, based on samples taken from the time period <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e860" xlink:type="simple"/></inline-formula>. In the rare event of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e861" xlink:type="simple"/></inline-formula>, which happens for example if a neuron never fires and hence its state is constant across all runs, we set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e862" xlink:type="simple"/></inline-formula> to 1.</p>
<p>An unfortunate source of confusion is the fact that Gelman and Rubin <xref ref-type="bibr" rid="pcbi.1003311-Gelman1">[55]</xref> originally introduced <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e863" xlink:type="simple"/></inline-formula> in its “variance” form equivalent to <xref ref-type="disp-formula" rid="pcbi.1003311.e857">equation (62)</xref>, but later in <xref ref-type="bibr" rid="pcbi.1003311-Brooks1">[57]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Gelman2">[60]</xref> altered this definition and defined <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e864" xlink:type="simple"/></inline-formula> as the square root of (62). This issue is particularly critical when considering threshold values for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e865" xlink:type="simple"/></inline-formula>: a threshold of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e866" xlink:type="simple"/></inline-formula> was suggested in the context of the original definition <xref ref-type="bibr" rid="pcbi.1003311-Kass1">[59]</xref>. Later, a typical threshold of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e867" xlink:type="simple"/></inline-formula> was suggested, but this lower threshold applied to the modified definition <xref ref-type="bibr" rid="pcbi.1003311-Brooks1">[57]</xref>, <xref ref-type="bibr" rid="pcbi.1003311-Gelman2">[60]</xref>. Squaring this apparently lower threshold yields again a typical threshold of approximately <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e868" xlink:type="simple"/></inline-formula>.</p>
<p>In the multivariate case (e.g. when analyzing convergence of the vector-valued simple state of a small subset of neurons as in the dotted lines of <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2G</xref>) one takes vector-valued (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e869" xlink:type="simple"/></inline-formula>-dimensional) samples, and computes the multivariate potential scale reduction factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e870" xlink:type="simple"/></inline-formula> according to:<disp-formula id="pcbi.1003311.e871"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003311.e871" xlink:type="simple"/><label>(63)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e872" xlink:type="simple"/></inline-formula> is the largest eigenvalue of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e873" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e874" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e875" xlink:type="simple"/></inline-formula> denote within and between sequence covariance matrix estimates (see <xref ref-type="bibr" rid="pcbi.1003311-Brooks3">[123]</xref> for details).</p>
</sec><sec id="s4g2">
<title>Convergence analysis for cortical microcircuit models</title>
<p>Gelman-Rubin values were calculated based on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e876" xlink:type="simple"/></inline-formula> runs, where the duration of each run was <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e877" xlink:type="simple"/></inline-formula> of biological time. We tried also much longer simulations of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e878" xlink:type="simple"/></inline-formula> but did not notice any sign of non-convergent behavior. A random initial state was set in each run by showing random input for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e879" xlink:type="simple"/></inline-formula> before the start of the actual simulation. This initial random input was fed into the network via the two regular input streams (40 neurons each), by assigning to each input neuron a random rate drawn uniformly from a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e880" xlink:type="simple"/></inline-formula> range. Convergence analysis of marginals was performed by applying univariate analysis to single components of the simple state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e881" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e882" xlink:type="simple"/></inline-formula>. From individual marginal convergence values, mean and worst marginal convergence (as in <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2E,F</xref>) were derived by taking at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e883" xlink:type="simple"/></inline-formula> the mean/max over all individual <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e884" xlink:type="simple"/></inline-formula>-values at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e885" xlink:type="simple"/></inline-formula>. For pairwise spike coincidences (see <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2D</xref>), we analyzed samples of the product of simple states of two neurons (the product equals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e886" xlink:type="simple"/></inline-formula> only if both neurons spiked within the last <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e887" xlink:type="simple"/></inline-formula>).</p>
<p>Random readouts for <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2G</xref> were implemented by adding an additional excitatory observer neuron to the network which receives synaptic inputs from a random subset of 500 network neurons (we kept this number 500 fixed across simulations with different network sizes to allow a fair comparison). The number of randomly chosen neurons from each of the pools is given in <xref ref-type="table" rid="pcbi-1003311-t001">Table 1</xref>.</p>
<p>Synapses onto the readout neuron were created in a similar manner as connections within the cortical column model: short-term plasticity parameters were set depending on the type of connection (EE or IE) according to <xref ref-type="bibr" rid="pcbi.1003311-Haeusler1">[30]</xref>. The weights for EE and IE connections were randomly chosen from a Gamma distribution with mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e888" xlink:type="simple"/></inline-formula> and scale parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e889" xlink:type="simple"/></inline-formula>, and mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e890" xlink:type="simple"/></inline-formula> and scale parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e891" xlink:type="simple"/></inline-formula>, respectively. Gelman-Rubin convergence of readouts was then computed as for the marginal case.</p>
<p>Convergence analysis of vector-valued simple states of subsets of neurons (see <xref ref-type="fig" rid="pcbi-1003311-g002">Figure 2G</xref>) was performed by applying multivariate analysis to randomly chosen subnetworks of the cortical column. In particular, we randomly drew 5 neurons from each of the 6 pools, yielding a subnetwork of 30 neurons, and calculated <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e892" xlink:type="simple"/></inline-formula>.</p>
</sec></sec><sec id="s4h">
<title>Impact of different dynamic regimes on the convergence time</title>
<p>In <xref ref-type="fig" rid="pcbi-1003311-g003">Figure 3</xref> we compared convergence times in four different neural circuits. The first circuit was identical to the <italic>small cortical microcircuit</italic> from <xref ref-type="fig" rid="pcbi-1003311-g001">Figure 1</xref>. For the remaining three circuits, the same stochastic point neurons and conductance-based dynamic synapses with delays were used as for the data-based cortical microcircuit model. Dynamic synaptic parameters were set to the corresponding mean values of parameters used in the cortical column model. Synaptic delays of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e893" xlink:type="simple"/></inline-formula> were used for all networks, except for the network with sequential structure (<xref ref-type="fig" rid="pcbi-1003311-g003">Figure 3C</xref>) where delays were <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e894" xlink:type="simple"/></inline-formula>. To modulate the intrinsic excitability of neurons we injected small currents to each neuron. The strengths of injected currents and connections were tuned for each network until the desired network activity was achieved. Synaptic background inputs were injected as in the cortical microcircuit model. To set different initial states (needed for Gelman Rubin analysis), during a preparatory phase of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e895" xlink:type="simple"/></inline-formula> we injected into each neuron a random current chosen from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e896" xlink:type="simple"/></inline-formula>. These small random input currents were strong enough to yield sufficiently diverse initial states. Gelman-Rubin values were then calculated based on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e897" xlink:type="simple"/></inline-formula> runs, where the duration of each run (after the preparatory phase) was <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e898" xlink:type="simple"/></inline-formula> of biological time. Convergence analysis was performed on marginals (individual simple states with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e899" xlink:type="simple"/></inline-formula>). Mean and worst marginals were computed as described in the previous section. </p>
<p>Below are additional details to the circuits used for <xref ref-type="fig" rid="pcbi-1003311-g003">Figure 3B–D</xref>: <italic>The sparsely active network</italic> of <xref ref-type="fig" rid="pcbi-1003311-g003">Figure 3B</xref> comprises one excitatory (E) and one inhibitory (I) population (each 10 neurons). Connections between neurons were drawn randomly according to the following set of connection probabilities: EE = 0.1, EI = 0.1, II = 0.9, IE = 0.9. <italic>The network with sequential structure</italic> of <xref ref-type="fig" rid="pcbi-1003311-g003">Figure 3C</xref> consists of two interconnected subnetworks where each one of them produces a stereotypical trajectory. Each subnetwork consists of a trigger neuron, a subsequent chain of neurons, and a pool of inhibitory neurons. Shown in <xref ref-type="fig" rid="pcbi-1003311-g003">Figure 3C</xref> are only the excitatory chain neurons from each subnetwork (neurons 1–15: first subnetwork; neurons 16–30: second subnetwork). Each excitatory neuron in the chain projects to all other neurons in the same chain with synaptic strengths decreasing with distance according to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e900" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e901" xlink:type="simple"/></inline-formula> applies to the forward direction in the chain and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e902" xlink:type="simple"/></inline-formula> to the backward direction. The trigger neuron projects (forward) to the chain in the same fashion with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e903" xlink:type="simple"/></inline-formula>. All neurons in the chain project to the inhibitory pool, and all neurons in the inhibitory pool project back to the trigger neuron and to the chain. Finally, the two subnetworks are combined such that the inhibitory pool of one subnetwork projects to the trigger neuron and the chain of the other subnetwork, and vice versa. This ensures that only one of the two subnetworks can be active at a time (competition between two trajectories). <italic>The bistable network</italic> of <xref ref-type="fig" rid="pcbi-1003311-g003">Figure 3D</xref> consists of two populations which strongly inhibit each other (each population comprising 10 neurons).</p>
</sec><sec id="s4i">
<title>Distributions of network states in the presence of periodic network input</title>
<p>The theoretical proof for Theorem 2 can be found after the proof of Theorem 1 above. For <xref ref-type="fig" rid="pcbi-1003311-g004">Figure 4F</xref>, a single long simulation (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e904" xlink:type="simple"/></inline-formula>) of the bi-stable network in <xref ref-type="fig" rid="pcbi-1003311-g004">Figure 4E</xref> was carried out. Each of the two pools was defined active at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e905" xlink:type="simple"/></inline-formula> if more than two neurons from the pool had an active simple state at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e906" xlink:type="simple"/></inline-formula> (with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e907" xlink:type="simple"/></inline-formula>). A transition was defined as the succession of a period in which one pool was active and the other pool inactive by a period in which the other became active and the first pool turned inactive. Between those two periods it typically occurs that either both pools are active or both are inactive for some short time. The exact time (and phase within the current cycle) of each transition was defined as the point in the middle of this intermediate period.</p>
</sec><sec id="s4j">
<title>Solving constraint satisfaction problems in networks of spiking neurons</title>
<sec id="s4j1">
<title>Formulation of Sudoku as a constraint satisfaction problem</title>
<p>A constraint satisfaction problem consists of a set of variables defined on some domain and a set of constraints, which limit the space of admissible variable assignments. A solution to a problem consists of an assignment to each variable such that all constraints are met. To formulate Sudoku as a constraint satisfaction problem, we define for each of the 81 fields (from a standard 9×9 grid), which has to be filled with a digit from 1 to 9, a set of 9 binary variables (taking values in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e908" xlink:type="simple"/></inline-formula>) <xref ref-type="bibr" rid="pcbi.1003311-ErcseyRavasz1">[124]</xref>. Each of these binary variables votes for exactly one digit in a field. The rules of the Sudoku game impose constraints on groups of these variables, which can be classified into the following three types.</p>
<p><italic>Given number constraints:</italic> The given numbers of a puzzle are fixed. Hence, the binary variables for the given fields are constrained to fixed values, for example, a given value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e909" xlink:type="simple"/></inline-formula> corresponds to fixed binary values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e910" xlink:type="simple"/></inline-formula>.</p>
<p><italic>Unique field constraints:</italic> In a correct solution, there must be only one digit active in each field. Hence in each field, exactly one of the 9 associated binary variables must be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e911" xlink:type="simple"/></inline-formula>, and all others must be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e912" xlink:type="simple"/></inline-formula> (equivalent to stating that the sum over these binary variables must equal 1).</p>
<p><italic>Unique group constraints:</italic> There are three types of groups: rows, columns and 3×3 subgrids. There are 9 row groups, 9 column groups, and 9 subgrid groups. In any of these groups, each digit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e913" xlink:type="simple"/></inline-formula> must appear only once. Hence, in each group, all binary variables voting for the same digit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e914" xlink:type="simple"/></inline-formula> must sum to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e915" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4j2">
<title>Network architecture for solving Sudoku</title>
<p>Sudoku can be implemented in a spiking neural network by creating for each of the 9 binary variables in each Sudoku field a local group of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e916" xlink:type="simple"/></inline-formula> pyramidal cells. Whenever one of these pyramidal cells fires, the corresponding binary variable is set to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e917" xlink:type="simple"/></inline-formula> for a short period <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e918" xlink:type="simple"/></inline-formula>. The binary variable is defined <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e919" xlink:type="simple"/></inline-formula> only if no neuron in its associated group fired within the last <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e920" xlink:type="simple"/></inline-formula>. This mapping allows one to readout the current (tentative) solution represented by the network at any time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e921" xlink:type="simple"/></inline-formula>. The tentative solution is correct only if all constraints are met. For all simulations we used <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e922" xlink:type="simple"/></inline-formula>, resulting in a total <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e923" xlink:type="simple"/></inline-formula> pyramidal cells. Constraints among Sudoku variables can be implemented via di-synaptic inhibition between the groups of pyramidal cells as detailed below.</p>
<p><italic>Given number constraints</italic> are implemented by providing strong positive input currents selectively to those neurons which code for the given numbers, and negative currents to neurons coding for wrong digits in a given field. <italic>Unique field constraints</italic> are implemented by forming a winner-take-all (WTA) circuit among all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e924" xlink:type="simple"/></inline-formula> neurons associated with the same Sudoku field. A WTA circuit is modeled by a single inhibitory neuron which is reciprocally connected to all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e925" xlink:type="simple"/></inline-formula> pyramidal cells. To reduce the probability that no pyramidal cell fires (which would violate the unique field constraint), thresholds of pyramidal cells are set to low values (see next section for details). <italic>Unique group constraints</italic> are implemented by a WTA circuit in which all neurons in a group which code for the same digit participate. In summary, there are 81 unique field constraints and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e926" xlink:type="simple"/></inline-formula> unique group constraints (in each group there is a constraint for each digit), yielding a total of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e927" xlink:type="simple"/></inline-formula> WTA circuits. These WTA circuits are partially overlapping, in the sense that each pyramidal cell participates in 4 of these WTA circuits (one for the unique value constraint in its field, and three for the unique group constraints in its row/column/subgrid).</p>
<p>Stochastic spike generation in both excitatory and inhibitory neurons is implemented consistent with the theoretical noise model I (see next section for details). The network thus fulfills all theoretical conditions for Theorem 1, and is guaranteed to have a unique stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e928" xlink:type="simple"/></inline-formula> of network states, to which it converges exponentially fast. This landscape will have automatically peaks at those states of the network which fulfill most of the game constraints, since each of the WTA circuits ensures that invalid configurations with respect to that constraint are unlikely to occur. Any specific Sudoku problem can be set by providing input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e929" xlink:type="simple"/></inline-formula> to the network in the form of strong currents to those neurons which correspond to the given values. This automatically modifies the landscape of the stationary distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e930" xlink:type="simple"/></inline-formula> such that only (or predominantly) solutions consistent with the givens are generated. Finally, due to neuronal noise the network can quickly probe different peaks in the landscape (different promising solution candidates) and escape them equally fast. Importantly, this process may occur at different places in the Sudoku puzzle simultaneously. Hence, one can interpret the network dynamics also as a highly parallel stochastic search algorithm.</p>
</sec><sec id="s4j3">
<title>Details to implementation and simulations for <xref ref-type="fig" rid="pcbi-1003311-g005">Figure 5</xref></title>
<p>Simulations for <xref ref-type="fig" rid="pcbi-1003311-g005">Figure 5</xref> were performed in NEVESIM, an event-based simulator for networks of spiking neurons developed in C++ with a Python Interface <xref ref-type="bibr" rid="pcbi.1003311-Pecevski3">[125]</xref>. The puzzle in <xref ref-type="fig" rid="pcbi-1003311-g005">Figure 5A</xref> was generated and rated “hard” by “Sudoku Solutions” <xref ref-type="bibr" rid="pcbi.1003311-Aire1">[126]</xref>. Spike generation is modeled according to <xref ref-type="disp-formula" rid="pcbi.1003311.e810">equation (60)</xref>, with parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e931" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e932" xlink:type="simple"/></inline-formula>. The stochastic threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e933" xlink:type="simple"/></inline-formula> was set to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e934" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e935" xlink:type="simple"/></inline-formula> for excitatory and inhibitory neurons, respectively. An absolute refractory period of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e936" xlink:type="simple"/></inline-formula> was chosen for pyramidal cells. To maximize the speed up of event-based simulations, PSPs were modeled in a simplified manner as current-based rectangular pulses of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e937" xlink:type="simple"/></inline-formula> (in contrast to the more complex conductance based integration of synaptic inputs used for cortical microcircuit models).</p>
<p>WTA circuits were formed by reciprocally connecting a single inhibitory neuron to all participating pyramidal cells. The single inhibitory neuron was modeled to mimic the response of a population of inhibitory neurons (i.e. strong inhibition for a prolonged amount of time), using an absolute refractory period of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e938" xlink:type="simple"/></inline-formula>, and strong bidirectional connections from and to excitatory neurons (synaptic weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e939" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e940" xlink:type="simple"/></inline-formula>, respectively).</p>
<p>To set a particular puzzle, given numbers were fixed by providing strong input currents to the corresponding pyramidal cells. In particular, neurons coding for the given numbers in a Sudoku field received a constant positive input current (a constant input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e941" xlink:type="simple"/></inline-formula> on the membrane potential). Neurons coding for conflicting digits in given Sudoku fields received a constant negative input current of strength <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e942" xlink:type="simple"/></inline-formula>.</p>
<p>A final practical remark concerns the number of neurons coding for each binary variable, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e943" xlink:type="simple"/></inline-formula>. We found that networks with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e944" xlink:type="simple"/></inline-formula> have a number of attractive properties compared to networks with single neuron coding. In particular firing rates of individual neurons can be lower (for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003311.e945" xlink:type="simple"/></inline-formula> a pyramidal cell would need to constantly burst to indicate a steady active state). Also, synaptic efficacies among neurons can be made weaker, and overall spike response patterns appear more biologically plausible. In view of a potential implementation in analog neuromorphic hardware, population coded variable assignments are also less prone to single unit failures or device mismatch.</p>
</sec></sec></sec></body>
<back>
<ack>
<p>We would like to thank Stefan Häusler, Robert Legenstein, Dejan Pecevski, Johannes Bill and Kenneth Harris for helpful discussions. We are grateful to Dejan Pecevski for developing the NEVESIM simulator, an event based simulator for networks of spiking neurons written in C++ with a Python Interface (<ext-link ext-link-type="uri" xlink:href="http://sim.igi.tugraz.at/" xlink:type="simple">http://sim.igi.tugraz.at/</ext-link>).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1003311-Allen1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Allen</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Stevens</surname><given-names>CF</given-names></name> (<year>1994</year>) <article-title>An evaluation of causes for unreliability of synaptic transmission</article-title>. <source>PNAS</source> <volume>91</volume>: <fpage>10380</fpage>–<lpage>10383</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Faisal1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Faisal</surname><given-names>AA</given-names></name>, <name name-style="western"><surname>Selen</surname><given-names>LPJ</given-names></name>, <name name-style="western"><surname>Wolpert</surname><given-names>DM</given-names></name> (<year>2008</year>) <article-title>Noise in the nervous system</article-title>. <source>Nature Reviews Neuroscience</source> <volume>9</volume>: <fpage>292</fpage>–<lpage>303</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Borst1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Borst</surname><given-names>JG</given-names></name> (<year>2010</year>) <article-title>The low synaptic release probability in vivo</article-title>. <source>Trends in Neurosciences</source> <volume>33</volume>: <fpage>259</fpage>–<lpage>266</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Yarom1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yarom</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Hounsgaard</surname><given-names>J</given-names></name> (<year>2011</year>) <article-title>Voltage fluctuations in neurons: signal or noise?</article-title> <source>Physiol Rev</source> <volume>91</volume>: <fpage>917</fpage>–<lpage>929</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Clarke1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clarke</surname><given-names>PG</given-names></name> (<year>2012</year>) <article-title>The limits of brain determinacy</article-title>. <source>Proc Biol Sci</source> <volume>279</volume>: <fpage>1665</fpage>–<lpage>1674</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Tsodyks1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Kenet</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Grinvald</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Arieli</surname><given-names>A</given-names></name> (<year>1999</year>) <article-title>Linking spontaneous activity of single cortical neurons and the underlying functional architecture</article-title>. <source>Science</source> <volume>286</volume>: <fpage>1943</fpage>–<lpage>1946</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Kenet1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kenet</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Bibitchkov</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Grinvald</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Arieli</surname><given-names>A</given-names></name> (<year>2003</year>) <article-title>Spontaneously emerging cortical representations of visual attributes</article-title>. <source>Nature</source> <volume>425</volume>: <fpage>954</fpage>–<lpage>956</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Luczak1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luczak</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Barth</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>KD</given-names></name> (<year>2009</year>) <article-title>Spontaneous events outline the realm of possible sensory responses in neocortical populations</article-title>. <source>Neuron</source> <volume>62</volume>: <fpage>413</fpage>–<lpage>425</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Raichle1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Raichle</surname><given-names>ME</given-names></name> (<year>2010</year>) <article-title>Two views of brain function</article-title>. <source>Trends in Cognitive Sciences</source> <volume>14</volume>: <fpage>180</fpage>–<lpage>190</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Lewis1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lewis</surname><given-names>CM</given-names></name>, <name name-style="western"><surname>Baldassarre</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Committeri</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Romani</surname><given-names>GL</given-names></name>, <name name-style="western"><surname>Corbetta</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>Learning sculpts the spontaneous activity of the resting human brain</article-title>. <source>PNAS</source> <volume>106</volume>: <fpage>17558</fpage>–<lpage>17563</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Fox1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fox</surname><given-names>MD</given-names></name>, <name name-style="western"><surname>Snyder</surname><given-names>AZ</given-names></name>, <name name-style="western"><surname>Vincent</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Raichle</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>Intrinsic fluctuations within cortical systems account for intertrial variability in human behavior</article-title>. <source>Neuron</source> <volume>56</volume>: <fpage>171</fpage>–<lpage>184</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Kelemen1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kelemen</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Fenton</surname><given-names>AA</given-names></name> (<year>2010</year>) <article-title>Dynamic grouping of hippocampal neural activity during cognitive control of two spatial frames</article-title>. <source>BLoS Biology</source> <volume>8</volume>: <fpage>e1000403</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Leopold1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leopold</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name> (<year>1996</year>) <article-title>Activity changes in early visual cortex reflect monkeys' percepts during binocular rivalry</article-title>. <source>Nature</source> <volume>379</volume>: <fpage>549</fpage>–<lpage>553</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Leopold2"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leopold</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name> (<year>1999</year>) <article-title>Multistable phenomena: changing views in perception</article-title>. <source>Trends in Cognitive Sciences</source> <volume>3</volume>: <fpage>254</fpage>–<lpage>264</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Kim1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kim</surname><given-names>CY</given-names></name>, <name name-style="western"><surname>Blake</surname><given-names>R</given-names></name> (<year>2005</year>) <article-title>Psychophysical magic: rendering the visible invisible</article-title>. <source>Trends in Cognitive Sciences</source> <volume>9</volume>: <fpage>381</fpage>–<lpage>388</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Karlsson1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Karlsson</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Tervo</surname><given-names>DG</given-names></name>, <name name-style="western"><surname>Karpova</surname><given-names>AY</given-names></name> (<year>2012</year>) <article-title>Network resets in medial prefrontal cortex mark the onset of behavioral uncertainty</article-title>. <source>Science</source> <volume>338</volume>: <fpage>135</fpage>–<lpage>139</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Brmaud1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brémaud</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Massoulié</surname><given-names>L</given-names></name> (<year>1996</year>) <article-title>Stability of nonlinear Hawkes processes</article-title>. <source>The Annals of Probability</source> <volume>24</volume>: <fpage>1563</fpage>–<lpage>1588</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Borovkov1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Borovkov</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Decrouez</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Gilson</surname><given-names>M</given-names></name> (<year>2012</year>) <article-title>On stationary distributions of stochastic neural networks</article-title>. <source>arXiv</source> <fpage>1206.4489</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Hoyer1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hoyer</surname><given-names>PO</given-names></name>, <name name-style="western"><surname>Hyvärinen</surname><given-names>A</given-names></name> (<year>2003</year>) <article-title>Interpreting neural response variability as Monte Carlo sampling of the posterior</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>15</volume>: <fpage>293</fpage>–<lpage>300</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Berkes1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berkes</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Orban</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Lengyel</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Fiser</surname><given-names>J</given-names></name> (<year>2011</year>) <article-title>Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment</article-title>. <source>Science</source> <volume>331</volume>: <fpage>83</fpage>–<lpage>87</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Buesing1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buesing</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Bill</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Nessler</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2011</year>) <article-title>Neural dynamics as sampling: a model for stochastic computation in recurrent networks of spiking neurons</article-title>. <source>PLoS Computational Biology</source> <volume>7</volume>: <fpage>e1002211</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Pecevski1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pecevski</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Buesing</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2011</year>) <article-title>Probabilistic inference in general graphical models through sampling in stochastic networks of spiking neurons</article-title>. <source>PLoS Computational Biology</source> <volume>7</volume>: <fpage>e1002294</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Friston1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name> (<year>2010</year>) <article-title>The free-energy principle: a unified brain theory?</article-title> <source>Nature Reviews Neuroscience</source> <volume>11</volume>: <fpage>127</fpage>–<lpage>138</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Vilares1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vilares</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Kording</surname><given-names>K</given-names></name> (<year>2011</year>) <article-title>Bayesian models: the structure of the world, uncertainty, behavior, and the brain</article-title>. <source>Annals of the New York Academy of Sciences</source> <volume>1224</volume>: <fpage>22</fpage>–<lpage>39</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Fiser1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fiser</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Berkes</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Orban</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Lengyel</surname><given-names>M</given-names></name> (<year>2010</year>) <article-title>Statistically optimal perception and learning: from behavior to neural representation</article-title>. <source>Trends in Cognitive Sciences</source> <volume>14</volume>: <fpage>119</fpage>–<lpage>130</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Doya1"><label>26</label>
<mixed-citation publication-type="book" xlink:type="simple">Doya K, Ishii S, Pouget A, Rao RPN (2007) Bayesian brain: Probabilistic approaches to neural coding. Cambridge, MA: MIT Press.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Huk1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huk</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Shadlen</surname><given-names>M</given-names></name> (<year>2005</year>) <article-title>Neural activity in macaque parietal cortex reflects temporal integration of visual motion signals during perceptual decision making</article-title>. <source>The Journal of Neuroscience</source> <volume>25</volume>: <fpage>10420</fpage>–<lpage>10436</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Mountcastle1"><label>28</label>
<mixed-citation publication-type="book" xlink:type="simple">Mountcastle VB (1998) Perceptual neuroscience: The cerebral cortex. Cambridge, MA: Harvard University Press, 362–381 pp.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Douglas1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Douglas</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Martin</surname><given-names>K</given-names></name> (<year>2004</year>) <article-title>Neuronal circuits of the neocortex</article-title>. <source>Annual Reviews of Neuroscience</source> <volume>27</volume>: <fpage>419</fpage>–<lpage>451</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Haeusler1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Haeusler</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2007</year>) <article-title>A statistical analysis of information-processing properties of lamina-specific cortical microcircuit models</article-title>. <source>Cerebral Cortex</source> <volume>17</volume>: <fpage>149</fpage>–<lpage>162</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Haeusler2"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Haeusler</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Schuch</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2009</year>) <article-title>Motif distribution, dynamical properties, and computational performance of two data-based cortical microcircuit templates</article-title>. <source>Journal of Physiology, Paris</source> <volume>103</volume>: <fpage>73</fpage>–<lpage>87</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Rasch1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rasch</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Schuch</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2011</year>) <article-title>Statistical comparision of spike responses to natural stimuli in monkey area V1 with simulated responses of a detailed laminar network model for a patch of V1</article-title>. <source>J Neurophysiol</source> <volume>105</volume>: <fpage>757</fpage>–<lpage>778</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Potjans1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Potjans</surname><given-names>TC</given-names></name>, <name name-style="western"><surname>Diesmann</surname><given-names>M</given-names></name> (<year>2012</year>) <article-title>The cell-type specific cortical microcircuit: relating structure and activity in a full-scale spiking network model</article-title>. <source>Cerebral Cortex</source> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cerror/bhs358" xlink:type="simple">10.1093/cerror/bhs358</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003311-Bastos1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bastos</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Usrey</surname><given-names>WM</given-names></name>, <name name-style="western"><surname>Adams</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Mangun</surname><given-names>GR</given-names></name>, <name name-style="western"><surname>Fries</surname><given-names>P</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Canonical microcircuits for predictive coding</article-title>. <source>Neuron</source> <volume>76</volume>: <fpage>695</fpage>–<lpage>711</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Dragoi1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dragoi</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Buzsaki</surname><given-names>G</given-names></name> (<year>2006</year>) <article-title>Temporal encoding of place sequences by hippocampal cell assemblies</article-title>. <source>Neuron</source> <volume>50</volume>: <fpage>145</fpage>–<lpage>157</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Gupta1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gupta</surname><given-names>A</given-names></name>, <name name-style="western"><surname>der Meer</surname><given-names>MAA</given-names></name>, <name name-style="western"><surname>Touretzky</surname><given-names>DS</given-names></name>, <name name-style="western"><surname>Redish</surname><given-names>AD</given-names></name> (<year>2012</year>) <article-title>Segmentation of spatial experience by hippocampal theta sequences</article-title>. <source>Nature Neuroscience</source> <volume>15</volume>: <fpage>1032</fpage>–<lpage>1039</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Jezek1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jezek</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Henriksen</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Treves</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Moser</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Moser</surname><given-names>M</given-names></name> (<year>2011</year>) <article-title>Theta-paced flickering between place-cell maps in the hippocampus</article-title>. <source>Nature</source> <volume>478</volume>: <fpage>246</fpage>–<lpage>249</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Hinton1"><label>38</label>
<mixed-citation publication-type="other" xlink:type="simple">Hinton GE, Sejnowski TJ, Ackley DH (1984) Boltzmann machines: constraint satisfaction networks that learn. Technical Report CMS-CS-84-119, CMU Computer Science Department.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Davenport1"><label>39</label>
<mixed-citation publication-type="book" xlink:type="simple">Davenport A, Tsang E, Wang CJ, Zhu K (1994) GENET: a connectionist architecture for solving constraint satisfaction problems by iterative improvement. In: Proceedings of the National Conference on Artificial Intelligence. John Wiley &amp; Sons Ltd, pp. 325–325.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Abeles1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abeles</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Bergman</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Gat</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Meilijson</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Seidemann</surname><given-names>E</given-names></name>, <etal>et al</etal>. (<year>1995</year>) <article-title>Cortical activity flips among quasi-stationary states</article-title>. <source>PNAS</source> <volume>92</volume>: <fpage>8616</fpage>–<lpage>8620</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Luczak2"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luczak</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Barth</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Marguet</surname><given-names>SL</given-names></name>, <name name-style="western"><surname>Buzski</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>KD</given-names></name> (<year>2007</year>) <article-title>Sequential structure of neocortical spontaneous activity in vivo</article-title>. <source>PNAS</source> <volume>104</volume>: <fpage>347</fpage>–<lpage>352</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Buzski1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buzsáki</surname><given-names>G</given-names></name> (<year>2010</year>) <article-title>Neural syntax: cell assemblies, synapsembles, and readers</article-title>. <source>Neuron</source> <volume>68</volume>: <fpage>362</fpage>–<lpage>385</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Luczak3"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luczak</surname><given-names>A</given-names></name>, <name name-style="western"><surname>MacLean</surname><given-names>JN</given-names></name> (<year>2012</year>) <article-title>Default activity patterns at the neocortical microcircuit level</article-title>. <source>Frontiers in Integrative Neuroscience</source> <volume>6</volume> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnint.2012.00030" xlink:type="simple">10.3389/fnint.2012.00030</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003311-Koller1"><label>44</label>
<mixed-citation publication-type="book" xlink:type="simple">Koller D, Friedman N (2009) Probabilistic graphical models: Principles and techniques. Cambridge, MA: MIT Press.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Thomson1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thomson</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>West</surname><given-names>DC</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Bannister</surname><given-names>AP</given-names></name> (<year>2002</year>) <article-title>Synaptic connections and small circuits involving excitatory and inhibitory neurons in layers 2–5 of adult rat and cat neocortex: triple intracellular recordings and biocytin labelling in vitro</article-title>. <source>Cerebral Cortex</source> <volume>12</volume>: <fpage>936</fpage>–<lpage>953</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Gupta2"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gupta</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Markram</surname><given-names>H</given-names></name> (<year>2000</year>) <article-title>Organizing principles for a diversity of gabaergic interneurons and synapses in the neocortex</article-title>. <source>Science</source> <volume>287</volume>: <fpage>273</fpage>–<lpage>278</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Markram1"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Markram</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name> (<year>1998</year>) <article-title>Differential signaling via the same axon of neocortical pyramidal neurons</article-title>. <source>PNAS</source> <volume>95</volume>: <fpage>5323</fpage>–<lpage>5328</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Jolivet1"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jolivet</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Rauch</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Lüscher</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2006</year>) <article-title>Predicting spike timing of neocortical pyramidal neurons by simple threshold models</article-title>. <source>Journal of Computational Neuroscience</source> <volume>21</volume>: <fpage>35</fpage>–<lpage>49</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Mazor1"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mazor</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Laurent</surname><given-names>G</given-names></name> (<year>2005</year>) <article-title>Transient dynamics versus fixed points in odor representations by locust antennal lobe projection neurons</article-title>. <source>Neuron</source> <volume>48</volume>: <fpage>661</fpage>–<lpage>673</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Harvey1"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harvey</surname><given-names>CD</given-names></name>, <name name-style="western"><surname>Coen</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Tank</surname><given-names>DW</given-names></name> (<year>2012</year>) <article-title>Choice-specific sequencis in parietal cortex during a virtual-navigation decision task</article-title>. <source>Nature</source> <volume>484</volume>: <fpage>62</fpage>–<lpage>68</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Wainwright1"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wainwright</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Jordan</surname><given-names>MI</given-names></name> (<year>2008</year>) <article-title>Graphical models, exponential families, and variational inference</article-title>. <source>Foundations and Trends in Machine Learning</source> <volume>1</volume>: <fpage>1</fpage>–<lpage>305</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Zhang1"><label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Meyers</surname><given-names>EM</given-names></name>, <name name-style="western"><surname>Bichot</surname><given-names>NP</given-names></name>, <name name-style="western"><surname>Serre</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>TA</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Object decoding with attention in inferior temporal cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>108</volume>: <fpage>8850</fpage>–<lpage>8855</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Shadlen1"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name>, <name name-style="western"><surname>Newsome</surname><given-names>WT</given-names></name> (<year>2001</year>) <article-title>Neural basis of a perceptual decision in the parietal cortex (area lip) of the rhesus monkey</article-title>. <source>Journal of Neurophysiology</source> <volume>86</volume>: <fpage>1916</fpage>–<lpage>1936</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Arieli1"><label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Arieli</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Sterkin</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Grinvald</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Aertsen</surname><given-names>A</given-names></name> (<year>1996</year>) <article-title>Dynamics of ongoing activity: explanation of the large variability in evoked cortical responses</article-title>. <source>Science</source> <volume>273</volume>: <fpage>1868</fpage>–<lpage>1871</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Gelman1"><label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gelman</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Rubin</surname><given-names>DB</given-names></name> (<year>1992</year>) <article-title>Inference from iterative simulation using multiple sequences</article-title>. <source>Statistical Science</source> <volume>7</volume>: <fpage>457</fpage>–<lpage>472</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Cowles1"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cowles</surname><given-names>MK</given-names></name>, <name name-style="western"><surname>Carlin</surname><given-names>BP</given-names></name> (<year>1996</year>) <article-title>Markov chain Monte Carlo convergence diagnostics: A comparative review</article-title>. <source>Journal of the American Statistical Association</source> <volume>91</volume>: <fpage>883</fpage>–<lpage>904</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Brooks1"><label>57</label>
<mixed-citation publication-type="book" xlink:type="simple">Brooks S, Gelman A, Jones G, Meng XL (2010) Handbook of Markov Chain Monte Carlo: Methods and Applications. Chapman &amp; Hall, 163–174 pp.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Gjoka1"><label>58</label>
<mixed-citation publication-type="book" xlink:type="simple">Gjoka M, Kurant M, Butts CT, Markopoulou A (2010) Walking in facebook: A case study of unbiased sampling of osns. In: INFOCOM, 2010 Proceedings IEEE. IEEE, pp. 1–9.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Kass1"><label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kass</surname><given-names>RE</given-names></name>, <name name-style="western"><surname>Carlin</surname><given-names>BP</given-names></name>, <name name-style="western"><surname>Gelman</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Neal</surname><given-names>RM</given-names></name> (<year>1998</year>) <article-title>Markov Chain Conte Carlo in practice: A roundtable discussion</article-title>. <source>The American Statistician</source> <volume>52</volume>: <fpage>93</fpage>–<lpage>100</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Gelman2"><label>60</label>
<mixed-citation publication-type="book" xlink:type="simple">Gelman A, Carlin JB, Stern HS, Rubin DB (2004) Bayesian Data Analysis, Second Edition (Chapman &amp; Hall/CRC Texts in Statistical Science). Chapman and Hall/CRC, 2 edition, 294–297 pp.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Engel1"><label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Engel</surname><given-names>AK</given-names></name>, <name name-style="western"><surname>Fries</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Singer</surname><given-names>W</given-names></name> (<year>2001</year>) <article-title>Dynamic predictions: oscillations and synchrony in top-down processing</article-title>. <source>Nat Rev Neurosci</source> <volume>2</volume>: <fpage>704</fpage>–<lpage>16</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Buzsaki1"><label>62</label>
<mixed-citation publication-type="book" xlink:type="simple">Buzsaki G (2009) Rhythms of the brain. Oxford: Oxford University Press.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Wang1"><label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname><given-names>XJ</given-names></name> (<year>2010</year>) <article-title>Neurophysiological and computational principles of cortical rhythms in cognition</article-title>. <source>Physiological Reviews</source> <volume>90</volume>: <fpage>1195</fpage>–<lpage>1268</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Siegel1"><label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Siegel</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Warden</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Miller</surname><given-names>EK</given-names></name> (<year>2009</year>) <article-title>Phase-dependent neuronal coding of objects in short-term memory</article-title>. <source>PNAS</source> <volume>106</volume>: <fpage>21341</fpage>–<lpage>21346</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Pipa1"><label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pipa</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Staedtler</surname><given-names>ES</given-names></name>, <name name-style="western"><surname>Rodriguez</surname><given-names>EF</given-names></name>, <name name-style="western"><surname>Waltz</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Muckli</surname><given-names>L</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Performance- and stimulus-dependent oscillations in monkey prefrontal cortex during short-term memory</article-title>. <source>Frontiers in Integrative Neuroscience</source> <volume>3</volume>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Turesson1"><label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Turesson</surname><given-names>HK</given-names></name>, <name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name>, <name name-style="western"><surname>Hoffman</surname><given-names>KL</given-names></name> (<year>2012</year>) <article-title>Category-selective phase coding in the superior temporal sulcus</article-title>. <source>PNAS</source> <volume>109</volume>: <fpage>19438</fpage>–<lpage>19443</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Blake1"><label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Blake</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name> (<year>2002</year>) <article-title>Visual competition</article-title>. <source>Nature Reviews Neuroscience</source> <volume>3</volume>: <fpage>13</fpage>–<lpage>21</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Sterzer1"><label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sterzer</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Kleinschmidt</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Rees</surname><given-names>G</given-names></name> (<year>2009</year>) <article-title>The neural bases of multistable perception</article-title>. <source>Trends in cognitive sciences</source> <volume>13</volume>: <fpage>310</fpage>–<lpage>318</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Gershman1"><label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gershman</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Vul</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname><given-names>J</given-names></name> (<year>2012</year>) <article-title>Multistability and perceptual inference</article-title>. <source>Neural Computation</source> <volume>24</volume>: <fpage>1</fpage>–<lpage>24</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Siapas1"><label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Siapas</surname><given-names>AG</given-names></name>, <name name-style="western"><surname>Lubenov</surname><given-names>EV</given-names></name>, <name name-style="western"><surname>Wilson</surname><given-names>MA</given-names></name> (<year>2005</year>) <article-title>Prefrontal phase locking to hippocampal theta oscillations</article-title>. <source>Neuron</source> <volume>46</volume>: <fpage>141</fpage>–<lpage>151</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Haider1"><label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Haider</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Husser</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Carandini</surname><given-names>M</given-names></name> (<year>2013</year>) <article-title>Inhibition dominates sensory responses in the awake cortex</article-title>. <source>Nature</source> <volume>493</volume>: <fpage>97</fpage>–<lpage>100</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Kumar1"><label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kumar</surname><given-names>V</given-names></name> (<year>1992</year>) <article-title>Algorithms for constraint-satisfaction problems: A survey</article-title>. <source>AI magazine</source> <volume>13</volume>: <fpage>32</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Craenen1"><label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Craenen</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Eiben</surname><given-names>A</given-names></name>, <name name-style="western"><surname>van Hemert</surname><given-names>JI</given-names></name> (<year>2003</year>) <article-title>Comparing evolutionary algorithms on binary constraint satisfaction problems</article-title>. <source>Evolutionary Computation, IEEE Transactions on</source> <volume>7</volume>: <fpage>424</fpage>–<lpage>444</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Durstewitz1"><label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname><given-names>D</given-names></name> (<year>2006</year>) <article-title>A few important points about dopamine's role in neural network dynamics</article-title>. <source>Pharmacopsychiatry</source> <volume>39</volume>: <fpage>572</fpage>–<lpage>575</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Durstewitz2"><label>75</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname><given-names>D</given-names></name> (<year>2009</year>) <article-title>Implications of synaptic biophysics for recurrent network dynamics and active memory</article-title>. <source>Neural Networks</source> <volume>22</volume>: <fpage>1189</fpage>–<lpage>1200</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Arnsten1"><label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Arnsten</surname><given-names>AFT</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Paspalas</surname><given-names>CD</given-names></name> (<year>2012</year>) <article-title>Neuromodulation of thought: flexibilities and vulnerabilities in prefrontal cortical network synapses</article-title>. <source>Neuron</source> <volume>76</volume>: <fpage>223</fpage>–<lpage>239</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Maass1"><label>77</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Natschlaeger</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Markram</surname><given-names>H</given-names></name> (<year>2002</year>) <article-title>Real-time computing without stable states: a new framework for neural computation based on perturbations</article-title>. <source>Neural Computation</source> <volume>14</volume>: <fpage>2531</fpage>–<lpage>2560</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Nikolic1"><label>78</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nikolic</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Haeusler</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Singer</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2009</year>) <article-title>Distributed fading memory for stimulus properties in the primary visual cortex</article-title>. <source>PLoS Biology</source> <volume>7</volume>: <fpage>1</fpage>–<lpage>19</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Klampfl1"><label>79</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Klampfl</surname><given-names>S</given-names></name>, <name name-style="western"><surname>David</surname><given-names>SV</given-names></name>, <name name-style="western"><surname>Yin</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2012</year>) <article-title>A quantitative analysis of information about past and present stimuli encoded by spikes of A1 neurons</article-title>. <source>Journal of Neurophysiology</source> <volume>108</volume>: <fpage>1366</fpage>–<lpage>1380</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Vul1"><label>80</label>
<mixed-citation publication-type="other" xlink:type="simple">Vul E, Goodman ND, Griffiths TL, Tenenbaum JB (2009) One and done? optimal decisions from very few samples. In: Proceedings of the 31st Annual Conference of the Cognitive Science Society. volume 1, pp. 66–72.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Lieder1"><label>81</label>
<mixed-citation publication-type="book" xlink:type="simple">Lieder F, Griffiths T, Goodman N (2013) Burn-in, bias, and the rationality of anchoring. In: Proc. of NIPS 2012. MIT Press, volume 25, pp. 2690–2698.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Hopfield1"><label>82</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name> (<year>1982</year>) <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>PNAS</source> <volume>79</volume>: <fpage>2554</fpage>–<lpage>2558</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Hopfield2"><label>83</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Tank</surname><given-names>D</given-names></name> (<year>1986</year>) <article-title>Computing with neural circuits: a model</article-title>. <source>Science</source> <volume>233</volume>: <fpage>625</fpage>–<lpage>633</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Rolls1"><label>84</label>
<mixed-citation publication-type="book" xlink:type="simple">Rolls ET, Deco G (2010) The noisy brain: Stochastic dynamics as a principle of brain function. Oxford: Oxford University Press, 73–77 pp.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Durstewitz3"><label>85</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Deco</surname><given-names>G</given-names></name> (<year>2008</year>) <article-title>Computational significance of transient dynamics in cortical networks</article-title>. <source>European Journal of Neuroscience</source> <volume>27</volume>: <fpage>217</fpage>–<lpage>227</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Rabinovich1"><label>86</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rabinovich</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Huerta</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Laurent</surname><given-names>G</given-names></name> (<year>2008</year>) <article-title>Transient dynamics for neural processing</article-title>. <source>Science</source> <volume>321</volume>: <fpage>48</fpage>–<lpage>50</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Sporns1"><label>87</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name> (<year>2011</year>) <article-title>The human connectome: a complex network</article-title>. <source>Annals of the New York Academy of Sciences</source> <volume>1224</volume>: <fpage>109</fpage>–<lpage>125</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Garey1"><label>88</label>
<mixed-citation publication-type="book" xlink:type="simple">Garey M, Johnson D (1979) Computers and Intractability: A Guide to the Theory of NPCompleteness. Mathematical Sciences. New York, NY: Freeman.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Qin1"><label>89</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Qin</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Xiang</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Zhou</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>K</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Neural bases for basic processes in heuristic problem solving: take solving sudoku puzzles as an example</article-title>. <source>PsyCh Journal</source> <volume>1</volume>: <fpage>101</fpage>–<lpage>117</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Boykov1"><label>90</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boykov</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Veksler</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Zabih</surname><given-names>R</given-names></name> (<year>2001</year>) <article-title>Fast approximate energy minimization via graph cuts</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source> <volume>23</volume>: <fpage>1222</fpage>–<lpage>1239</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Griffiths1"><label>91</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Griffiths</surname><given-names>TL</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name> (<year>2006</year>) <article-title>Optimal predictions in everyday cognition</article-title>. <source>Psychological Science</source> <volume>17</volume>: <fpage>767</fpage>–<lpage>773</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Vul2"><label>92</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vul</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Pashler</surname><given-names>H</given-names></name> (<year>2008</year>) <article-title>Measuring the crowd within: probabilistic representations within individuals</article-title>. <source>Psychological Science</source> <volume>19</volume>: <fpage>645</fpage>–<lpage>647</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Denison1"><label>93</label>
<mixed-citation publication-type="other" xlink:type="simple">Denison S, Bonawitz E, Gopnik A, Griffiths TL (2009) Preschoolers sample from probability distributions. In: Proceedings of the 32nd Annual Conference of the Cognitive Science Society. volume 29, pp. 1–10.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Tenenbaum1"><label>94</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tenenbaum</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Kemp</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Griffiths</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Goodman</surname><given-names>N</given-names></name> (<year>2011</year>) <article-title>How to grow a mind: statistics, structure, and abstraction</article-title>. <source>Science</source> <volume>331</volume>: <fpage>1279</fpage>–<lpage>1285</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Churchland1"><label>95</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Churchland</surname><given-names>MM</given-names></name>, <name name-style="western"><surname>Abbott</surname><given-names>L</given-names></name> (<year>2012</year>) <article-title>Two layers of neural variability</article-title>. <source>Nature neuroscience</source> <volume>15</volume>: <fpage>1472</fpage>–<lpage>1474</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-LitwinKumar1"><label>96</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Litwin-Kumar</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Doiron</surname><given-names>B</given-names></name> (<year>2012</year>) <article-title>Slow dynamics and high variability in balanced cortical networks with clustered connections</article-title>. <source>Nature neuroscience</source> <volume>15</volume>: <fpage>1498</fpage>–<lpage>1505</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Okun1"><label>97</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Okun</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Yger</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Marguet</surname><given-names>SL</given-names></name>, <name name-style="western"><surname>Gerard-Mercier</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Benucci</surname><given-names>A</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Population rate dynamics and multineuron firing patterns in sensory cortex</article-title>. <source>Journal of Neuroscience</source> <volume>32</volume>: <fpage>17108</fpage>–<lpage>17119</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Zhang2"><label>98</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname><given-names>QF</given-names></name>, <name name-style="western"><surname>Wen</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>D</given-names></name>, <name name-style="western"><surname>She</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>JY</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Priming with real motion biases visual cortical response to bistable apparent motion</article-title>. <source>PNAS</source> <volume>109</volume>: <fpage>20691</fpage>–<lpage>20696</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Xu1"><label>99</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Xu</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Jiang</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Poo</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name> (<year>2012</year>) <article-title>Activity recall in a visual cortical ensemble</article-title>. <source>Nature Neuroscience</source> <volume>15</volume>: <fpage>449</fpage>–<lpage>456</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Harris1"><label>100</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harris</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Csicsvari</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Hirase</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Dragoi</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Buzsáki</surname><given-names>G</given-names></name> (<year>2003</year>) <article-title>Organization of cell assemblies in the hippocampus</article-title>. <source>Nature</source> <volume>424</volume>: <fpage>552</fpage>–<lpage>556</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Jones1"><label>101</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jones</surname><given-names>LM</given-names></name>, <name name-style="western"><surname>Fontanini</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Sadacca</surname><given-names>BF</given-names></name>, <name name-style="western"><surname>Miller</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Katz</surname><given-names>DB</given-names></name> (<year>2007</year>) <article-title>Natural stimuli evoke dynamic sequences of states in sensory cortical ensembles</article-title>. <source>PNAS</source> <volume>104</volume>: <fpage>18772</fpage>–<lpage>18777</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Lisman1"><label>102</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lisman</surname><given-names>JE</given-names></name>, <name name-style="western"><surname>Raghavachari</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Tsien</surname><given-names>RW</given-names></name> (<year>2007</year>) <article-title>The sequence of events that underlie quantal transmission at central glutamatergic synapses</article-title>. <source>Nature Reviews Neuroscience</source> <volume>8</volume>: <fpage>597</fpage>–<lpage>609</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Branco1"><label>103</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Branco</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Staras</surname><given-names>K</given-names></name> (<year>2009</year>) <article-title>The probability of neurotransmitter release: variability and feedback control at single synapes</article-title>. <source>Nature Reviews Neuroscience</source> <volume>10</volume>: <fpage>373</fpage>–<lpage>383</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Doeblin1"><label>104</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doeblin</surname><given-names>W</given-names></name> (<year>1937</year>) <article-title>Sur le propriétés asymtotiques de mouvement régis par certain types de chaînes simples</article-title>. <source>Bull Math Soc Roumaine Sci</source> <volume>39</volume> (<issue>1</issue>) <fpage>115</fpage>–<lpage>61</lpage> (<issue>2</issue>) –</mixed-citation>
</ref>
<ref id="pcbi.1003311-Maass2"><label>105</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Sontag</surname><given-names>E</given-names></name> (<year>1999</year>) <article-title>Analog neural nets with Gaussian or other common noise distributions cannot recognize arbitrary regular languages</article-title>. <source>Neural Computation</source> <volume>11</volume>: <fpage>771</fpage>–<lpage>782</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Borovkov2"><label>106</label>
<mixed-citation publication-type="book" xlink:type="simple">Borovkov AA (1998) Ergodicity and stability of stochastic processes. Hoboken, NJ:Wiley, 225–230 pp.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Down1"><label>107</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Down</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Meyn</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Tweedie</surname><given-names>R</given-names></name> (<year>1995</year>) <article-title>Exponential and uniform ergodicity of Markov processes</article-title>. <source>The Annals of Probability</source> <volume>23</volume>: <fpage>1671</fpage>–<lpage>1691</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Meyn1"><label>108</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meyn</surname><given-names>SP</given-names></name>, <name name-style="western"><surname>Tweedie</surname><given-names>RL</given-names></name> (<year>1993</year>) <article-title>Stability of markovian processes ii: Continuous-time processes and sampled chains</article-title>. <source>Advances in Applied Probability</source> <fpage>487</fpage>–<lpage>517</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Massouli1"><label>109</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Massoulié</surname><given-names>L</given-names></name> (<year>1998</year>) <article-title>Stability results for a general class of interacting point processes dynamics, and applications</article-title>. <source>Stochastic processes and their applications</source> <volume>75</volume>: <fpage>1</fpage>–<lpage>30</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Gray1"><label>110</label>
<mixed-citation publication-type="book" xlink:type="simple">Gray RM (2009) Probability, random processes, and ergodic properties. New York: Springer. 42</mixed-citation>
</ref>
<ref id="pcbi.1003311-Pecevski2"><label>111</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pecevski</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Natschlaeger</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Schuch</surname><given-names>K</given-names></name> (<year>2009</year>) <article-title>PCSIM: a parallel simulation environment for neural circuits fully integrated with python</article-title>. <source>Frontiers in Neuroinformatics</source> <volume>3</volume>: <fpage>11</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-vanRossum1"><label>112</label>
<mixed-citation publication-type="other" xlink:type="simple">van Rossum G, Drake FL (2001). Python reference manual. Pythonlabs, Virginia, USA, 2001. Available at <ext-link ext-link-type="uri" xlink:href="http://www.python.org" xlink:type="simple">http://www.python.org</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Gerstner1"><label>113</label>
<mixed-citation publication-type="book" xlink:type="simple">Gerstner W, Kistler WM (2002) Spiking Neuron Models. Cambridge: Cambridge University Press.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Larkum1"><label>114</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Larkum</surname><given-names>ME</given-names></name> (<year>2013</year>) <article-title>The yin and yang of cortical layer 1</article-title>. <source>Nature Neuroscience</source> <volume>16</volume>: <fpage>114</fpage>–<lpage>115</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Jiang1"><label>115</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jiang</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Stornetta</surname><given-names>RL</given-names></name>, <name name-style="western"><surname>Zhu</surname><given-names>JJ</given-names></name> (<year>2013</year>) <article-title>The organization of two new cortical interneuronal circuits</article-title>. <source>Nature Neuroscience</source> <volume>16</volume>: <fpage>210</fpage>–<lpage>218</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Larkum2"><label>116</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Larkum</surname><given-names>M</given-names></name> (<year>2012</year>) <article-title>A cellular mechanism for cortical associations: an organizing principle for the cerebral cortex</article-title>. <source>Trends in Neurosciences</source> <volume>951</volume>: <fpage>1</fpage>–<lpage>11</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Maass3"><label>117</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Markram</surname><given-names>H</given-names></name> (<year>2002</year>) <article-title>Synapses as dynamic memory buffers</article-title>. <source>Neural Networks</source> <volume>15</volume>: <fpage>155</fpage>–<lpage>161</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Destexhe1"><label>118</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Destexhe</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Rudolph</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Fellous</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name> (<year>2001</year>) <article-title>Fluctuating synaptic conductances recreate <italic>in vivo</italic>-like activity in neocortical neurons</article-title>. <source>Neuroscience</source> <volume>107</volume>: <fpage>13</fpage>–<lpage>24</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Brooks2"><label>119</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brooks</surname><given-names>SP</given-names></name>, <name name-style="western"><surname>Roberts</surname><given-names>GO</given-names></name> (<year>1998</year>) <article-title>Assessing convergence of Markov chain Monte Carlo algorithms</article-title>. <source>Statistics and Computing</source> <volume>8</volume>: <fpage>319</fpage>–<lpage>335</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-ElAdlouni1"><label>120</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>El Adlouni</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Favre</surname><given-names>AC</given-names></name>, <name name-style="western"><surname>Bobée</surname><given-names>B</given-names></name> (<year>2006</year>) <article-title>Comparison of methodologies to assess the convergence of Markov chain Monte Carlo methods</article-title>. <source>Computational Statistics &amp; Data Analysis</source> <volume>50</volume>: <fpage>2685</fpage>–<lpage>2701</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Raftery1"><label>121</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Raftery</surname><given-names>AE</given-names></name>, <name name-style="western"><surname>Lewis</surname><given-names>S</given-names></name>, <etal>et al</etal>. (<year>1992</year>) <article-title>How many iterations in the Gibbs sampler</article-title>. <source>Bayesian Statistics</source> <volume>4</volume>: <fpage>763</fpage>–<lpage>773</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Geweke1"><label>122</label>
<mixed-citation publication-type="other" xlink:type="simple">Geweke J (1991) Evaluating the accuracy of sampling-based approaches to the calculation of posterior moments. Staff Report 148, Federal Reserve Bank of Minneapolis.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Brooks3"><label>123</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brooks</surname><given-names>SP</given-names></name>, <name name-style="western"><surname>Gelman</surname><given-names>A</given-names></name> (<year>1998</year>) <article-title>General methods for monitoring convergence of iterative simulations</article-title>. <source>Journal of Computational and Graphical Statistics</source> <volume>7</volume>: <fpage>434</fpage>–<lpage>455</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-ErcseyRavasz1"><label>124</label>
<mixed-citation publication-type="other" xlink:type="simple">Ercsey-Ravasz M, Toroczkai Z (2012) The chaos within Sudoku. Scientific Reports 2.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Pecevski3"><label>125</label>
<mixed-citation publication-type="other" xlink:type="simple">Pecevski D. NEVESIM – an event based simulator for networks of spiking neurons. <ext-link ext-link-type="uri" xlink:href="http://sim.igi.tugraz.at/" xlink:type="simple">http://sim.igi.tugraz.at/</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003311-Aire1"><label>126</label>
<mixed-citation publication-type="other" xlink:type="simple">Aire Technologies. Sudoku solutions. <ext-link ext-link-type="uri" xlink:href="http://www.sudoku-solutions.com" xlink:type="simple">http://www.sudoku-solutions.com</ext-link>. Accessed February 27, 2013.</mixed-citation>
</ref>
</ref-list></back>
</article>