<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pone.0204897</article-id>
<article-id pub-id-type="publisher-id">PONE-D-17-41884</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Oncology</subject><subj-group><subject>Cancers and neoplasms</subject><subj-group><subject>Breast tumors</subject><subj-group><subject>Breast cancer</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Forecasting</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Forecasting</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Oncology</subject><subj-group><subject>Cancer treatment</subject><subj-group><subject>Cancer chemotherapy</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Pharmaceutics</subject><subj-group><subject>Drug therapy</subject><subj-group><subject>Chemotherapy</subject><subj-group><subject>Cancer chemotherapy</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Clinical medicine</subject><subj-group><subject>Clinical oncology</subject><subj-group><subject>Cancer chemotherapy</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Oncology</subject><subj-group><subject>Clinical oncology</subject><subj-group><subject>Cancer chemotherapy</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Pharmaceutics</subject><subj-group><subject>Drug therapy</subject><subj-group><subject>Chemotherapy</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Gene expression</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Oncology</subject><subj-group><subject>Cancer treatment</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Genome analysis</subject><subj-group><subject>Gene prediction</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Genomics</subject><subj-group><subject>Genome analysis</subject><subj-group><subject>Gene prediction</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Pathology and laboratory medicine</subject><subj-group><subject>Clinical pathology</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Comparison of the modified unbounded penalty and the LASSO to select predictive genes of response to chemotherapy in breast cancer</article-title>
<alt-title alt-title-type="running-head">Modified unbounded penalty and the LASSO to select predictive genes in breast cancer</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-0521-7236</contrib-id>
<name name-style="western">
<surname>Collignon</surname>
<given-names>Olivier</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Han</surname>
<given-names>Jeongseop</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>An</surname>
<given-names>Hyungmi</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Oh</surname>
<given-names>Seungyoung</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Lee</surname>
<given-names>Youngjo</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Luxembourg Institute of Health, Competence Center for Methodology and Statistics, Luxembourg</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Department of Mathematics, Korea Military Academy, Seoul, South Korea</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Department of Statistics, College of Natural Sciences, Seoul National University, Seoul, South Korea</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Roviello</surname>
<given-names>Giandomenico</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Istituto di Ricovero e Cura a Carattere Scientifico Centro di Riferimento Oncologico della Basilicata, ITALY</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">youngjo@snu.ac.kr</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>1</day>
<month>10</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<year>2018</year>
</pub-date>
<volume>13</volume>
<issue>10</issue>
<elocation-id>e0204897</elocation-id>
<history>
<date date-type="received">
<day>28</day>
<month>11</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>17</day>
<month>9</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Collignon et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0204897"/>
<abstract>
<p>Covariate selection is a fundamental step when building sparse prediction models in order to avoid overfitting and to gain a better interpretation of the classifier without losing its predictive accuracy. In practice the LASSO regression of Tibshirani, which penalizes the likelihood of the model by the L1 norm of the regression coefficients, has become the gold-standard to reach these objectives. Recently Lee and Oh developed a novel random-effect covariate selection method called the modified unbounded penalty (MUB) regression, whose penalization function can equal minus infinity at 0 in order to produce very sparse models. We sought to compare the predictive accuracy and the number of covariates selected by these two methods in several high-dimensional datasets, consisting in genes expressions measured to predict response to chemotherapy in breast cancer patients. These comparisons were performed by building the Receiver Operating Characteristics (ROC) curves of the classifiers obtained with the selected genes and by comparing their area under the ROC curve (AUC) corrected for optimism using several variants of bootstrap internal validation and cross-validation. We found consistently in all datasets that the MUB penalization selected a remarkably smaller number of covariates than the LASSO while offering a similar—and encouraging—predictive accuracy. The models selected by the MUB were actually nested in the ones obtained with the LASSO. Similar findings were observed when comparing these results to those obtained in their first publication by other authors or when using the area under the Precision-Recall curve (AUCPR) as another measure of predictive performance. In conclusion, the MUB penalization seems therefore to be one of the best options when sparsity is required in high-dimension. Further investigation in other datasets is however required to validate these findings.</p>
</abstract>
<funding-group>
<funding-statement>The authors received no specific funding for this work.</funding-statement>
</funding-group>
<counts>
<fig-count count="8"/>
<table-count count="3"/>
<page-count count="15"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>We have uploaded our code and our datasets as Supporting Information. As regards the datasets, we re-used the data from the following PLOS ONE article: de Ronde JJ, Bonder MJ, Lips EH, Rodenhuis S, Wessels LF. Breast cancer subtype specific classifiers of response to neoadjuvant chemotherapy do not outperform classifiers trained on all subtypes. PLOS ONE. 2014;9(2):e88551: <ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosone/article?id=10.1371/journal..0088551#s6" xlink:type="simple">http://journals.plos.org/plosone/article?id=10.1371/journal..0088551#s6</ext-link>, where it is mentioned that the data are public: "These datasets are publicly available from the Gene Expression Omnibus website (MAQC2 GEO ID is GSE16716, BESOU GEO ID is GSE32646, and MDACC GEO ID is GSE20271)."</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>1. Introduction</title>
<p>When building prediction models, covariate selection is a fundamental step in order to maximize the interpretability of the classifier and to avoid overfitting [<xref ref-type="bibr" rid="pone.0204897.ref001">1</xref>–<xref ref-type="bibr" rid="pone.0204897.ref005">5</xref>] while maintaining the predictive accuracy. In particular, sparse models, i.e. including a very limited number of covariates, are very attractive because their fitting depends only on the estimation of a few parameters, offering an easier interpretation of the model. In practice, the financial cost of these models is also potentially lower since only a few covariates are to be measured to accurately classify a new individual. Indeed, in medicine and biology for example, predictive biomarkers can be very costly to measure and therefore the larger the number of covariates needed in a predictive model the higher its effective cost. In this respect, the LASSO regression has become the gold standard for covariate selection [<xref ref-type="bibr" rid="pone.0204897.ref005">5</xref>, <xref ref-type="bibr" rid="pone.0204897.ref006">6</xref>]. This method is based on a penalization of the likelihood of the model by the L1 norm of the vector of the regression coefficients of the covariates. Indeed, non-differentiability of the penalization function at 0 enables to produce sparse selection. Variable selection methods based on likelihood penalization also encompass the Elastic Net penalty [<xref ref-type="bibr" rid="pone.0204897.ref007">7</xref>] and the Smoothly Clipped Absolute Deviation (SCAD) penalty [<xref ref-type="bibr" rid="pone.0204897.ref008">8</xref>]. Bayesian alternatives such as spike and slab, and the Bayesian LASSO are also available [<xref ref-type="bibr" rid="pone.0204897.ref009">9</xref>–<xref ref-type="bibr" rid="pone.0204897.ref011">11</xref>]. More recently Lee and Oh [<xref ref-type="bibr" rid="pone.0204897.ref012">12</xref>] developed a novel random-effect covariate selection method called the MUB regression, whose penalization function can equal minus infinity at 0. This method offered promising results in simulations and in toy datasets [<xref ref-type="bibr" rid="pone.0204897.ref013">13</xref>, <xref ref-type="bibr" rid="pone.0204897.ref014">14</xref>] and therefore deserves further practical investigation.</p>
<p>The objective of the present work is to compare the performances of the LASSO and the MUB in practice when applied to high-dimensional data. To this end, the datasets previously described and analysed by de Ronde et al [<xref ref-type="bibr" rid="pone.0204897.ref015">15</xref>] are an interesting case study because of the very large number of covariates measured and their conclusive sample size (i.e. sufficiently large to make prediction feasible). More precisely, this database is a collection of four different datasets, which consist in gene expression profiles measured to predict response to chemotherapy in breast cancer patients. Using the data shared by the authors, our goal was to build the best predictor of response to chemotherapy using the shortest list of discriminant genes in each of the four datasets as well as in the pooled database. We especially sought to compare both the number of covariates selected using the LASSO and the MUB as well as the predictive accuracy of the corresponding models, as measured by the AUC of the related classifiers. In order to compare both variable selection methods using another performance criterion as a sensitivity analysis, their respective AUCPR were also computed [<xref ref-type="bibr" rid="pone.0204897.ref016">16</xref>]. Finally we also compared our results to those published previously by de Ronde et al [<xref ref-type="bibr" rid="pone.0204897.ref015">15</xref>].</p>
<p>The paper is organized as follow. In the first section, we remind the reader about how random-effect covariate selection can be used to derive penalized regressions such as the LASSO and the MUB and we describe how these methods relate to other variable selection methods available in the literature. This is then followed by a description of the datasets being considered. The statistical analysis planned to compare both penalization methods is then described in details. We also explain how our findings were compared to the results of de Ronde et al [<xref ref-type="bibr" rid="pone.0204897.ref015">15</xref>]. In the following sections the results of the different experiments are reported and discussed before we finish with concluding remarks.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>2. Material and methods</title>
<sec id="sec003">
<title>2.1 Random-effect covariate selection and penalized regressions</title>
<p>Lee and Oh [<xref ref-type="bibr" rid="pone.0204897.ref012">12</xref>] introduced a random-effect model that leads to a family of penalized likelihood estimators, including the ridge, LASSO and MUB estimators.</p>
<p>For a set of observations indexed by 1 ≤ <italic>i</italic> ≤ <italic>n</italic>, consider the following random-effect model:
<disp-formula id="pone.0204897.e001">
<alternatives>
<graphic id="pone.0204897.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0204897.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="0.25em"/><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
where for 1 ≤ <italic>i</italic> ≤ <italic>n</italic>, <italic>Y</italic><sub><italic>i</italic></sub> is the response variable, <italic>x</italic><sub><italic>i</italic></sub> = (<italic>x</italic><sub><italic>i</italic>,1</sub>,…,<italic>x</italic><sub><italic>i</italic>,<italic>p</italic></sub>) is the vector of the covariates, <italic>β</italic><sub>1</sub>,…,<italic>β</italic><sub><italic>p</italic></sub> are the regression coefficients and <italic>ϕ</italic> &gt; 0 the variance of the error term <italic>e</italic><sub><italic>i</italic></sub>. Suppose that conditionally on <italic>u</italic><sub><italic>j</italic></sub>, we have for 1 ≤ <italic>j</italic> ≤ <italic>p</italic>,
<disp-formula id="pone.0204897.e002">
<alternatives>
<graphic id="pone.0204897.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0204897.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">σ</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
where σ &gt; 0 is a fixed dispersion parameter, and <italic>u</italic><sub><italic>j</italic></sub>’s are an iid sample of a gamma distribution with rate and shape parameters both equal to 1/<italic>τ</italic> &gt; 0, such that the density function <italic>f</italic><sub><italic>τ</italic></sub> can be written
<disp-formula id="pone.0204897.e003">
<alternatives>
<graphic id="pone.0204897.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0204897.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
with E(<italic>u</italic><sub><italic>j</italic></sub>) = 1 and Var(<italic>u</italic><sub><italic>j</italic></sub>) = <italic>τ</italic>.</p>
<p>In this random effect model, sparsity of the selection can be achieved in an apparent way, since if the random effect estimate <italic>u</italic><sub><italic>i</italic></sub> ≈ 0 then <italic>β</italic><sub><italic>j</italic></sub> ≈ 0. Note that σ<italic>u</italic><sub><italic>j</italic></sub> = (<italic>a</italic>σ)(<italic>u</italic><sub><italic>j</italic></sub>/<italic>a</italic>) for any <italic>a</italic> &gt; 0, which means <italic>σ</italic> and <italic>u</italic><sub><italic>j</italic></sub> are not separately identifiable and justifies the constrain <italic>E</italic>(<italic>u</italic><sub><italic>j</italic></sub>) = 1 for all τ. By considering the distribution of the random effects as a negative penalty, the h-likelihood estimator [<xref ref-type="bibr" rid="pone.0204897.ref017">17</xref>, <xref ref-type="bibr" rid="pone.0204897.ref018">18</xref>] can be viewed as a penalized-likelihood estimator.</p>
<p>Lee and Oh [<xref ref-type="bibr" rid="pone.0204897.ref012">12</xref>, <xref ref-type="bibr" rid="pone.0204897.ref019">19</xref>] proposed an algorithm to obtain an estimator of the regression coefficients of the model using the h-likelihood [<xref ref-type="bibr" rid="pone.0204897.ref017">17</xref>, <xref ref-type="bibr" rid="pone.0204897.ref018">18</xref>]. They showed that for fixed τ, the <italic>j-</italic>th term of the penalty function <italic>p</italic><sub><italic>λ</italic></sub>(<italic>β</italic>) can be written
<disp-formula id="pone.0204897.e004">
<alternatives>
<graphic id="pone.0204897.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0204897.e004" xlink:type="simple"/>
<mml:math display="block" id="M4">
<mml:mfrac><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>τ</mml:mi></mml:mrow></mml:mfrac><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mfrac><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
where <inline-formula id="pone.0204897.e005"><alternatives><graphic id="pone.0204897.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0204897.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>≡</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mi>β</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>8</mml:mn><mml:mi mathvariant="normal">τ</mml:mi><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">τ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. Then, <italic>β</italic> can be estimated by controlling τ and <italic>λ</italic> = <italic>ϕ</italic>/σ. The reparametrization <italic>λ</italic> = <italic>ϕ</italic>/<italic>σ</italic> is valid because the estimation of <italic>β</italic> only depends on the ratio of both parameters <italic>ϕ</italic> and σ. The random effect <italic>u</italic><sub><italic>j</italic></sub> for the <italic>j-</italic>th parameter <italic>β</italic><sub><italic>j</italic></sub> imposes a penalty for <italic>β</italic><sub><italic>j</italic></sub> and is substituted by <inline-formula id="pone.0204897.e006"><alternatives><graphic id="pone.0204897.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0204897.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. This emphasizes the fact that <italic>β</italic> is an unknown fixed parameter and is constrained by the random effect <italic>u</italic>.</p>
<p>This random effect model leads to a family of various penalty functions <italic>p</italic><sub><italic>λ</italic></sub>(<italic>β</italic>) indexed by τ and <italic>λ</italic>. For example, when τ = 0, the penalty function <italic>p</italic><sub><italic>λ</italic></sub>(<italic>β</italic>) becomes <inline-formula id="pone.0204897.e007"><alternatives><graphic id="pone.0204897.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0204897.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mfrac><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> which is the one of the ridge regression. Similarly, when τ = 2, the penalty function <inline-formula id="pone.0204897.e008"><alternatives><graphic id="pone.0204897.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0204897.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mi>σ</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the same as the one of the LASSO regression. By definition the modified unbounded penalty (MUB) corresponds to the case where τ &gt; 2. A graphical representation of the penalty function <italic>p</italic><sub>λ</sub>(∙) is given on <xref ref-type="fig" rid="pone.0204897.g001">Fig 1</xref> (extracted from [<xref ref-type="bibr" rid="pone.0204897.ref018">18</xref>]) for τ = 0,2,10 and 30 with <italic>λ</italic> = 0.5 (dotted line), <italic>λ</italic> = 1 (solid line), and 1.5 (dashed line). It appears that as the curve becomes more concave near the origin, the sparsity of local solutions increases, and as the slope becomes flat, the amount of shrinkage lessens. The resulting penalty allows thus to control the amount of sparsity and shrinkage by choosing the values of τ and λ. The ridge and LASSO regressions include a bounded penalty and produce shrinkage estimation. Moreover the LASSO has a discontinuity of its derivative at zero which leads to a sparse selection, while the ridge penalty is differentiable at zero and therefore does not offer the possibility to select covariates. The Elastic Net is based on both the ridge and the LASSO penalties. All these methods, including Elastic Net, can however suffer from a limited sparsity in the sense that they can still select too many covariates. With the MUB, the penalty function is not differentiable at zero when τ ≥ 2 and becomes unbounded when τ &gt; 2. The MUB therefore controls both the amount of sparsity and shrinkage and offers a sparse estimation of the regression parameters without losing prediction accuracy [<xref ref-type="bibr" rid="pone.0204897.ref012">12</xref>, <xref ref-type="bibr" rid="pone.0204897.ref013">13</xref>].</p>
<fig id="pone.0204897.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0204897.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Penalty function <italic>p</italic><sub><italic>λ</italic></sub>(<italic>β</italic>) at different values of w, for λ = 0.5 (dotted line), λ = 1 (solid line) and λ = 1.5 (dashed line).</title>
<p>In general, larger values of <bold>λ</bold> are associated with larger penalties, hence more shrinkage and sparsity [<xref ref-type="bibr" rid="pone.0204897.ref018">18</xref>].</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0204897.g001" xlink:type="simple"/>
</fig>
<p>On <xref ref-type="fig" rid="pone.0204897.g002">Fig 2</xref> is plotted the relationship between the ordinary least squares estimator (x-axis) and corresponding penalized likelihood estimator (y-axis) obtained respectively with the LASSO, the MUB and the SCAD penalties (solid line indicates λ = 1, dashed line λ = 2 and dotted line λ = 0.5; for more details, see Ch.11 in [<xref ref-type="bibr" rid="pone.0204897.ref018">18</xref>]). It can be observed that (1): all three methods produce shrinkage estimators compared to ordinary least squares estimator; (2): the higher the value of the estimators of the MUB and the SCAD, the closer they get to the ordinary least squares estimator. This oracle property cannot be satisfied for the LASSO; (3): the SCAD becomes the ordinary least square estimator for higher value, whereas the MUB shrinks the coefficients.</p>
<fig id="pone.0204897.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0204897.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Relationship between the coefficients from the true model (x-axis) and those of a sparse regression model (y-axis) obtained respectively with the LASSO, the MUB and the SCAD penalties for λ = 0.5 (dotted line), λ = 1 (solid line) and λ = 2 (dashed line) [<xref ref-type="bibr" rid="pone.0204897.ref018">18</xref>].</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0204897.g002" xlink:type="simple"/>
</fig>
<p>Sparse covariate selection is beneficial for the interpretation of the model, while shrinkage estimation is beneficial for prediction accuracy. We therefore chose to apply both the LASSO and the MUB penalizations to our datasets in order to select predictive genes of response to chemotherapy in breast cancer and to compare their performances.</p>
<p>Note that Bayesian methods like spike and slab, and the Bayesian LASSO have also been developed for variable selection. They achieve the sparsity through the use of the specific prior distribution of <italic>β</italic>, whereas the MUB controls the sparsity via the use of specific distribution of <italic>u</italic> [<xref ref-type="bibr" rid="pone.0204897.ref009">9</xref>–<xref ref-type="bibr" rid="pone.0204897.ref011">11</xref>].</p>
</sec>
<sec id="sec004">
<title>2.2 Breast cancer chemotherapy datasets</title>
<p>Our analysis was based on four curated gene expression datasets which have been described previously in details by de Ronde et al [<xref ref-type="bibr" rid="pone.0204897.ref015">15</xref>]. The names of the patients and of the genes had been anonymised and the patients had signed informed consent. In each dataset the expression of 12010 genes was measured by microarray in female patients with breast cancer treated by chemotherapy. Moreover, the pathological complete response (pCR) of these patients to treatment was evaluated by biopsy. Each of the datasets corresponds to a specific clinical subtype of breast cancer, as explained below.</p>
<p>Breast cancer is indeed a very heterogeneous pathology and response to treatment depends on the clinical subtype of the disease. As explained in the original study of de Ronde et al [<xref ref-type="bibr" rid="pone.0204897.ref015">15</xref>], the binary hormonal receptor status of each patient (oestrogen receptor (ER+ or ER-), progesterone receptor (PR+ or PR-) and human epidermal growth factor receptor 2 (HER2+ or HER2-)) allows defining five different breast cancer subtypes: the HER2+ subtype, the luminal subtype (HER2-, ER+), the triple negative subtype (HER2-, ER-, PR-), the (HER2+, ER-) subtype and the (HER2+, ER+) subtype.</p>
<p>In our pooled database, 394 patients were included in the study among which 87 (22%) were classified as having no residual cancer after treatment whereas 307 (78%) were classified as not responding to the treatment. The number of patients of each clinical subtype is summarized in <xref ref-type="table" rid="pone.0204897.t001">Table 1</xref> and categorized by response to chemotherapy (adapted from de Ronde et al [<xref ref-type="bibr" rid="pone.0204897.ref015">15</xref>]). Note that the sample size of the (HER2+, ER+) subtype was not sufficiently large to be clinically conclusive and was therefore not investigated neither in our study nor in the seminal one [<xref ref-type="bibr" rid="pone.0204897.ref015">15</xref>].</p>
<table-wrap id="pone.0204897.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0204897.t001</object-id>
<label>Table 1</label> <caption><title>Patient disposition by breast cancer clinical subtype and response to chemotherapy.</title> <p>pCR: pathological complete response.</p></caption>
<alternatives>
<graphic id="pone.0204897.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0204897.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Dataset</th>
<th align="center">pCR—n(%)</th>
<th align="center">No pCR—n(%)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">HER2+</td>
<td align="center">31 (38)</td>
<td align="center">51 (62)</td>
</tr>
<tr>
<td align="left">Luminal</td>
<td align="center">14 (7)</td>
<td align="center">185 (93)</td>
</tr>
<tr>
<td align="left">Triple Negative</td>
<td align="center">40 (38)</td>
<td align="center">64 (62)</td>
</tr>
<tr>
<td align="left">HER2 +, ER -</td>
<td align="center">25 (56)</td>
<td align="center">20 (44)</td>
</tr>
<tr>
<td align="left">pooled database</td>
<td align="center">87 (22)</td>
<td align="center">307 (78)</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec005">
<title>2.3 Statistical analysis</title>
<p>Overall, our database consisted in 12010 continuous covariates measured in 394 individuals in order to predict their response to chemotherapy, which is a binary response variable. For a binary response variable, the following logit model is considered:
<disp-formula id="pone.0204897.e009">
<alternatives>
<graphic id="pone.0204897.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0204897.e009" xlink:type="simple"/>
<mml:math display="block" id="M9">
<mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.12em"/><mml:msub><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mo>(</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
where for 1 ≤ <italic>i</italic> ≤ <italic>n</italic>, <italic>Y</italic><sub><italic>i</italic></sub> is the response variable, <italic>x</italic><sub><italic>i</italic></sub> = (<italic>x</italic><sub><italic>i</italic>,1</sub>,…,<italic>x</italic><sub><italic>i</italic>,<italic>p</italic></sub>) is the vector of the covariates, <italic>β</italic><sub>1</sub>,…,<italic>β</italic><sub><italic>p</italic></sub> are the regression parameters. The penalized likelihood function, namely the h-likelihood, is given by
<disp-formula id="pone.0204897.e010">
<alternatives>
<graphic id="pone.0204897.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0204897.e010" xlink:type="simple"/>
<mml:math display="block" id="M10">
<mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mspace width="0.25em"/><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula></p>
<p>And the beta coefficients are then obtained by maximizing (2).</p>
<p>In total five analyses were performed: one for each of the four clinical subtypes and one for the pooled database made of the four pooled subtypes.</p>
<p>Our objective was to compare the number of genes selected and the predictive accuracy of LASSO and MUB regressions in predicting response to chemotherapy in each of the four subtypes and in the pooled database. To do so, the same analysis was applied to each dataset:</p>
<p>First, optimal values of the tuning parameters of each method were selected among a pre-set grid of candidate values by 3-fold cross-validation.</p>
<p>In detail, we searched 15 grid points equally spaced between 2 and 30 for <italic>τ</italic>, and 30 grid points between 0 and 1 for <italic>σ</italic>, respectively [<xref ref-type="bibr" rid="pone.0204897.ref013">13</xref>] and found (<italic>τ</italic>,<italic>σ</italic>) which maximize the penalized likelihood function (2).</p>
<p>More precisely each dataset was randomly divided in 3 subsets of the same size. Two out of the 3 subsets were selected and pooled in order to estimate all the regression coefficients of the LASSO and MUB. A classifier of response to chemotherapy was then built up with the linear predictor including the covariates having received an estimated nonzero coefficient. The third subset was used as a test sample on which the likelihood value of the classifier was evaluated. Each of the three subsets was sequentially used as a test sample whereas the remaining two other subsets were pooled together as training sample. The values of the tuning parameters that led to the selection of the set of covariates maximizing the likelihood value of the model were finally retained as the optimal ones.</p>
<p>Second, once the optimal values were selected, these were attributed by default to the tuning parameters of LASSO and MUB regressions which were then re-fitted using the whole dataset. For each method, the linear predictor was built including the covariates having received a nonzero estimated regression coefficient and its predictive accuracy was evaluated by plotting its ROC curve and computing the corresponding AUC by reclassification (i.e. using the same original dataset as both training and test sample). The difference between the AUC obtained with LASSO and MUB was tested for nullity using DeLong’s paired test [<xref ref-type="bibr" rid="pone.0204897.ref020">20</xref>]. Finally, in order to correct for the optimism caused by the use of the same dataset for both the estimation of the model parameters and the evaluation of the predictive accuracy of the classifiers by the AUC, different internal validation techniques were applied and compared. First, the same double-loop cross validation procedure as the one described by de Ronde et al [<xref ref-type="bibr" rid="pone.0204897.ref015">15</xref>], where the data were originally analysed, was implemented. Second, three variants of bootstrap internal validation techniques were used: the regular bootstrap, the bootstrap .632 and the bootstrap .632+. These methods are described in details in Steyerberg et al [<xref ref-type="bibr" rid="pone.0204897.ref021">21</xref>] and Collignon [<xref ref-type="bibr" rid="pone.0204897.ref022">22</xref>]. All three methods use bootstrap samples of the original dataset (i.e. random samples with replacement of the actual dataset with the same size) as training samples to fit the LASSO and MUB regressions and select the most discriminant covariates. The original dataset is then used as a test sample. For bootstrap .632 and .632+ variants the individuals randomly selected in the current bootstrap training sample are first removed from the test sample before computing the AUC. For the regular bootstrap, the difference between the AUC obtained by reclassification of the bootstrap training sample and the AUC obtained by classifying the test sample was averaged over 100 bootstrap replications in order to estimate the optimism of the method. The corrected AUC was then obtained by subtracting the average optimism from the AUC originally computed by reclassification. For the .632 and .632+ bootstrap variants, the corrected AUC was a weighted average of the AUC obtained in reclassification and of the average of the 100 different AUC obtained on the test sample (the weighting of the .632+ variant favours the performance obtained on the test sample in case of overfitting). In order to evaluate the robustness of the covariate selection, the mean and standard deviation of the number of covariates selected was computed over the 100 bootstrap replications of the LASSO and MUB. Moreover, the percentage of selection of each covariate over the 100 bootstrap replications was also computed.</p>
<p>Although the AUC is the most widely accepted measure of models predictive performances [<xref ref-type="bibr" rid="pone.0204897.ref001">1</xref>, <xref ref-type="bibr" rid="pone.0204897.ref003">3</xref>], other criteria can be of interest. For example when the sample sizes of the two groups to discriminate are markedly unbalanced, the AUCPR can also be informative [<xref ref-type="bibr" rid="pone.0204897.ref016">16</xref>]. In our case the imbalance between the “pCR” and “no pCR” groups was not extreme but we also computed as a sensitivity analysis the AUCPR of the LASSO and the MUB. Note that the variables used in these classifiers are the same as for the ROC curves since the beta coefficients are estimated by maximizing the the h-likelihood.</p>
</sec>
<sec id="sec006">
<title>2.4 Comparison with the original study</title>
<p>In their original study, de Ronde et al [<xref ref-type="bibr" rid="pone.0204897.ref015">15</xref>] combined different covariate selection techniques to several classifiers in order to build the best predictive model of response to chemotherapy. Performances were corrected for optimism by double-loop cross validation. Among these methods, the model maximizing the corrected AUC was retained as the best one. In order to compare its performances to ours, we retrieved the mean number of genes selected during double-loop cross validation as well as the corresponding corrected AUC for the best model developed by de Ronde et al [<xref ref-type="bibr" rid="pone.0204897.ref015">15</xref>] (see also Supplementary Information of their paper (<ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0088551#s6" xlink:type="simple">http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0088551#s6</ext-link>).) Note no results about bootstrap internal validation was available in the original study.</p>
</sec>
</sec>
<sec id="sec007" sec-type="results">
<title>3. Results</title>
<p>In order to illustrate our analysis with the largest sample size possible, the results obtained with the pooled database are described in details before summarizing those achieved in each of the four clinical subtypes.</p>
<sec id="sec008">
<title>3.1 Pooled database</title>
<p>In the pooled database, 24 covariates were selected using the MUB penalization whereas 70 were selected with the LASSO. In <xref ref-type="fig" rid="pone.0204897.g003">Fig 3</xref> are depicted on the y-axis the regression coefficients of the covariates selected by at least one of the methods. It appears that the covariates selected by the MUB are actually a subset of those obtained by the LASSO. The corresponding models are therefore nested. For the covariates retained by both methods, the estimated MUB regression coefficients were greater in absolute value than the LASSO ones. This finding confirms that the MUB controls the amount of shrinkage, as discussed in Section 2.1. It is interesting to emphasize here the inadequacy of the likelihood ratio test (LRT) to compare the model fits. Indeed, based on the idea that adding supplementary variables to a model should increase its fit, the LRT is a tool to assess if the corresponding increase in likelihood is significant, by adjusting for the number of covariates used in both models, which are nested. However, the MUB model is based on a smaller number of parameters and yet gives a larger likelihood than the LASSO. It is because the LASSO could shrink the coefficients excessively. Comparing the model fits with the LRT would lead to a non-significant p-value whereas the MUB clearly outperformed the LASSO while requiring less parameters.</p>
<fig id="pone.0204897.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0204897.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Pooled database—Bargraph representing on the y-axis the regression coefficients of the covariates selected by at least one of the covariate selection method in the pooled database.</title>
<p>The genes on the x-axis are sorted by decreasing order of their LASSO regression coefficients and are labelled as anonymised in the database.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0204897.g003" xlink:type="simple"/>
</fig>
<p>On <xref ref-type="fig" rid="pone.0204897.g004">Fig 4</xref> are depicted the two ROC curves of the linear predictors built with the selected covariates of each method. It appears that both curves are almost superimposed, leading to a similar predictive performance with a rounded AUC of 94% (p = 0.94, De Long test). The MUB reached thus the same performance as the LASSO with about one third of the number of covariates selected.</p>
<fig id="pone.0204897.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0204897.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Pooled database—ROC curves of the linear predictors built with the selected covariates of each method.</title>
<p>MUB: solid line; LASSO dotted line.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0204897.g004" xlink:type="simple"/>
</fig>
<p>Since the training and test samples were the same, the two AUC are overoptimistic and had thus to be corrected for overfitting. Using the regular bootstrap internal validation technique, the results obtained with both methods remained very comparable since the corrected AUC was 83% for the MUB and 84% for the LASSO. The boxplot of the AUC corrected for the optimism calculated with each bootstrap training sample is displayed on <xref ref-type="fig" rid="pone.0204897.g005">Fig 5</xref> and shows that over the 100 replications the results remained very similar, although the LASSO appeared however to be slightly superior to the MUB in terms of predictive accuracy. The same observations were made when using different validation techniques: 80% vs 82% with the bootstrap .632, 78% vs 80% with the bootstrap .632+ and 75% vs 76% by double-loop cross-validation for the MUB and the LASSO respectively (corresponding boxplots are shown in Figures A, B and C in <xref ref-type="supplementary-material" rid="pone.0204897.s001">S1 File</xref>). However, it has to be emphasized that over the 100 bootstrap replications, the mean (standard deviation) number of covariates selected with the MUB was remarkably smaller than the LASSO’s: 32.87 (2.48) vs 51.27 (3.48). The boxplot of the difference between the number of covariates selected by the LASSO and the MUB over the 100 bootstrap training samples is depicted on <xref ref-type="fig" rid="pone.0204897.g006">Fig 6</xref>, and it appears clearly that the MUB penalization always selected less covariates than the LASSO. On <xref ref-type="fig" rid="pone.0204897.g007">Fig 7</xref> is represented the percentage of time each gene was selected during the bootstrap replications by each covariate selection method. Only the 15 most selected genes are represented. The slopes of the curves appeared to be rather similar, although the ranking of the genes (by decreasing percentage of selection) was markedly different.</p>
<fig id="pone.0204897.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0204897.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Pooled database—boxplot of the AUC corrected by the regular bootstrap internal validation technique using the optimism calculated within each bootstrap training sample.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0204897.g005" xlink:type="simple"/>
</fig>
<fig id="pone.0204897.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0204897.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Pooled database—boxplot of the difference between the number of covariates selected by the LASSO and the MUB over the 100 bootstrap training samples.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0204897.g006" xlink:type="simple"/>
</fig>
<fig id="pone.0204897.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0204897.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Pooled database—percentage of time each gene was selected during the bootstrap replications by each covariate selection method.</title>
<p>Only the 15 most selected genes are represented.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0204897.g007" xlink:type="simple"/>
</fig>
<p>On <xref ref-type="fig" rid="pone.0204897.g008">Fig 8</xref> are plotted the respective PR curves of the LASSO and the MUB applied to the pooled dataset. It can be seen that the curves are very close to each other and again the respective AUCPR show no marked difference (LASSO: 95%, MUB: 96% in reclassification, LASSO: 43%, MUB: 41% by double-loop cross-validation). The boxplots of the AUCPR corrected by the double loop CV procedure are shown in Figure D in <xref ref-type="supplementary-material" rid="pone.0204897.s001">S1 File</xref>. It is moreover important to emphasize that the variables used to build these classifiers are the same as for the ROC curves. Indeed these variables are selected by maximisation of the h-likelihood, which is a criterion independent of the measure of predictive performance chosen.</p>
<fig id="pone.0204897.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0204897.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Pooled database–PR curves of the linear predictors built with the selected covariates of each method.</title>
<p>MUB: solid line; LASSO dotted line.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0204897.g008" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec009">
<title>3.2 Results by clinical subtypes</title>
<p>An overview of the results obtained with both covariate selection methods in each dataset is summarized in <xref ref-type="table" rid="pone.0204897.t002">Table 2</xref> (See Figures E to AJ in <xref ref-type="supplementary-material" rid="pone.0204897.s001">S1 File</xref>).</p>
<table-wrap id="pone.0204897.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0204897.t002</object-id>
<label>Table 2</label> <caption><title>Overview of the results obtained with both covariate selection methods in each dataset.</title> <p>n: sample size; sd: standard deviation; AUC: Area Under the Curve; CV: cross-validation.</p></caption>
<alternatives>
<graphic id="pone.0204897.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0204897.t002" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center" colspan="8">LASSO</th>
<th align="center" colspan="7">MUB</th>
<th align="center"/>
</tr>
<tr>
<th align="center">dataset</th>
<th align="center">n</th>
<th align="center">number of selected<break/>covariates</th>
<th align="center">mean number of covariates selected during validation (sd)</th>
<th align="center">AUC</th>
<th align="center">AUC regular bootstrap</th>
<th align="center">AUC .632</th>
<th align="center">AUC .632+</th>
<th align="center">AUC double-loop CV</th>
<th align="center">number of covariates selected</th>
<th align="center">mean number of covariates selected during validation<break/>(sd)</th>
<th align="center">AUC</th>
<th align="center">AUC regular bootstrap</th>
<th align="center">AUC .632</th>
<th align="center">AUC .632+</th>
<th align="center">AUC double-loop CV</th>
<th align="center">De Longs' test p-value</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">All</td>
<td align="center">394</td>
<td align="center">70</td>
<td align="center">51.27 (3.48)</td>
<td align="center">94%</td>
<td align="center">84%</td>
<td align="center">82%</td>
<td align="center">80%</td>
<td align="center">76%</td>
<td align="center">24</td>
<td align="center">32.87 (2.48)</td>
<td align="center">94%</td>
<td align="center">83%</td>
<td align="center">80%</td>
<td align="center">78%</td>
<td align="center">75%</td>
<td align="center">0.9411</td>
</tr>
<tr>
<td align="center">Her2 +</td>
<td align="center">82</td>
<td align="center">14</td>
<td align="center">37.54 (2.62)</td>
<td align="center">88%</td>
<td align="center">90%</td>
<td align="center">78%</td>
<td align="center">71%</td>
<td align="center">66%</td>
<td align="center">2</td>
<td align="center">15.02 (1.49)</td>
<td align="center">83%</td>
<td align="center">88%</td>
<td align="center">78%</td>
<td align="center">71%</td>
<td align="center">63%</td>
<td align="center">0.0977</td>
</tr>
<tr>
<td align="center">Luminal</td>
<td align="center">199</td>
<td align="center">30</td>
<td align="center">43.91 (7.82)</td>
<td align="center">100%</td>
<td align="center">87%</td>
<td align="center">77%</td>
<td align="center">69%</td>
<td align="center">62%</td>
<td align="center">25</td>
<td align="center">3.22 (1.07)</td>
<td align="center">100%</td>
<td align="center">85%</td>
<td align="center">77%</td>
<td align="center">69%</td>
<td align="center">61%</td>
<td align="center">0.1864</td>
</tr>
<tr>
<td align="center">Triple -</td>
<td align="center">104</td>
<td align="center">7</td>
<td align="center">47.76 (3.03)</td>
<td align="center">80%</td>
<td align="center">89%</td>
<td align="center">75%</td>
<td align="center">65%</td>
<td align="center">58%</td>
<td align="center">4</td>
<td align="center">12.99 (1.29)</td>
<td align="center">83%</td>
<td align="center">86%</td>
<td align="center">75%</td>
<td align="center">65%</td>
<td align="center">57%</td>
<td align="center">0.325</td>
</tr>
<tr>
<td align="center">Her2+ ER-</td>
<td align="center">45</td>
<td align="center">28</td>
<td align="center">23.97 (2.02)</td>
<td align="center">100%</td>
<td align="center">89%</td>
<td align="center">75%</td>
<td align="center">65%</td>
<td align="center">59%</td>
<td align="center">3</td>
<td align="center">10.55 (1.21)</td>
<td align="center">92%</td>
<td align="center">85%</td>
<td align="center">76%</td>
<td align="center">66%</td>
<td align="center">54%</td>
<td align="center">0.0917</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Overall the results elicited on the pooled database were also observed in each of the clinical subtypes. Indeed in each of them, the MUB always selected less covariates than the LASSO, as for example 2 vs 14 for the Her2+ subtype. The covariates selected by the MUB were always a subset of those selected by the LASSO and the models were therefore nested. De Long’s test to compare the AUC of the classifiers in reclassification was never significant. Predictive performances tended to vary marginally with the validation method. They were in general slightly superior for the LASSO when correcting the AUC by the regular bootstrap internal validation technique or the double-loop cross-validation, whereas they were almost identical and even sometimes slightly superior for the MUB when correcting it with the bootstrap .632 or 632+ variants. The initial amount of overfitting was large in the luminal and Her2+ ER- subtypes where almost perfect AUC (i.e. equalling to 100%) were observed before correction, which could be attributed to the small group sample size of these subtypes. Interestingly the same pattern was always observed in all clinical subtypes for both covariate selection methods, in which the AUC were systematically ranked in the following decreasing order: regular bootstrap internal validation, bootstrap 632, bootstrap 632+, double-loop cross-validation.</p>
<p>The regression coefficients were also less shrunken with the LASSO than the MUB in the Her2+ and triple negative subtypes while the converse tended to be observed in the luminal and Her2+ ER- subtypes (See Figures E, M, U and AC in <xref ref-type="supplementary-material" rid="pone.0204897.s001">S1 File</xref>). It has however to be pointed out that the luminal subtype contained a very small proportion of patients responding to chemotherapy (7%) while the Her2+ ER- subtype had a very low sample size. These results have therefore to be treated with caution.</p>
<p>Surprisingly, using the MUB penalization the number of covariates selected in the actual luminal subtype was much larger than the mean number of covariates selected during bootstrap internal validation (25 vs 3.22). The converse was observed with the LASSO in the triple negative subtype, were a very small number of covariates was selected as compared to the mean number of covariates selected during bootstrap internal validation (7 vs 47.76).</p>
</sec>
<sec id="sec010">
<title>3.3 Comparison with the original study</title>
<p>The mean number of covariates selected during double loop CV by the best model of de Ronde et al [<xref ref-type="bibr" rid="pone.0204897.ref015">15</xref>] and the corresponding corrected AUC are reported in <xref ref-type="table" rid="pone.0204897.t003">Table 3</xref> and compared to our own results. Despite the fact that no results on bootstrap internal validation was available for the study of de Ronde et al, it is hoped that this would give an order of magnitude and would help inform the comparison.</p>
<table-wrap id="pone.0204897.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0204897.t003</object-id>
<label>Table 3</label> <caption><title>Mean number of covariates selected and AUC corrected by double loop cross-validation for the LASSO, the MUB and the best model of de Ronde et al [<xref ref-type="bibr" rid="pone.0204897.ref015">15</xref>].</title></caption>
<alternatives>
<graphic id="pone.0204897.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0204897.t003" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center" colspan="2">LASSO</th>
<th align="center" colspan="2">MUB</th>
<th align="center" colspan="2">Best model from de Ronde et al [<xref ref-type="bibr" rid="pone.0204897.ref015">15</xref>]</th>
</tr>
<tr>
<th align="center">dataset</th>
<th align="center">mean number of covariates selected during bootstrap internal validation</th>
<th align="center">AUC double-loop CV</th>
<th align="center">mean number of covariates selected during bootstrap internal validation</th>
<th align="center">AUC double-loop CV</th>
<th align="center">mean number of covariates selected during double loop CV</th>
<th align="center">AUC double-loop CV</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">All</td>
<td align="center">51</td>
<td align="center">76%</td>
<td align="center">33</td>
<td align="center">75%</td>
<td align="center">31</td>
<td align="center">77%</td>
</tr>
<tr>
<td align="center">Her2 +</td>
<td align="center">38</td>
<td align="center">66%</td>
<td align="center">15</td>
<td align="center">63%</td>
<td align="center">31</td>
<td align="center">69%</td>
</tr>
<tr>
<td align="center">Luminal</td>
<td align="center">44</td>
<td align="center">62%</td>
<td align="center">3</td>
<td align="center">61%</td>
<td align="center">41</td>
<td align="center">61%</td>
</tr>
<tr>
<td align="center">Triple Negative</td>
<td align="center">48</td>
<td align="center">58%</td>
<td align="center">13</td>
<td align="center">57%</td>
<td align="center">53</td>
<td align="center">68%</td>
</tr>
<tr>
<td align="center">Her2+ ER-</td>
<td align="center">24</td>
<td align="center">59%</td>
<td align="center">11</td>
<td align="center">54%</td>
<td align="center">58</td>
<td align="center">51%</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Using double loop CV, the performances published by de Ronde et al [<xref ref-type="bibr" rid="pone.0204897.ref015">15</xref>] were slightly superior for the Her2+ subtype and the pooled database, and were markedly better for the triple negative subtype. However they were outperformed by the LASSO and MUB in luminal and Her2+ER- subtypes. Interestingly, in each clinical subtype the average number of selected covariates was always smaller with the MUB than with the best model of de Ronde et al [<xref ref-type="bibr" rid="pone.0204897.ref015">15</xref>], whereas the LASSO only selected fewer genes in the triple negative and Her2+ER- subtypes. In the pooled database, on average the MUB only selected two more variables than the method of de Ronde et al, while the LASSO selected twenty more.</p>
</sec>
</sec>
<sec id="sec011" sec-type="conclusions">
<title>4. Conclusion</title>
<p>In this study, we found consistently in several different datasets that the MUB penalization tended to select a remarkably smaller number of covariates than the LASSO’s while offering similar predictive accuracy (the models obtained were actually nested). Indeed the difference between the performances of the classifiers built with the covariates selected with each covariate selection method was relatively slight and varied moderately by datasets and validation technique. When comparing the results obtained to those published in the study where the data were originally described and analysed, the predictive accuracy obtained with the LASSO and the MUB varied only moderately from the performance of de Ronde et al [<xref ref-type="bibr" rid="pone.0204897.ref015">15</xref>]. However, we found again that the number of covariates selected appeared to be much smaller. In these high-dimensional datasets, the MUB appeared therefore to be an efficient method to select a small number of important predictive genes of resistance to chemotherapy while offering encouraging predictive performances. Although Bayesian alternatives could be considered, we think that in a high-dimensional setting, the h-likelihood based method is easier to estimate. Thus, when sparsity is required, MUB seemed therefore to be the best option in high-dimension. Investigation in other datasets is to be planned in order to confirm these findings in other high-dimensional datasets.</p>
</sec>
<sec id="sec012">
<title>Supporting information</title>
<supplementary-material id="pone.0204897.s001" mimetype="application/vnd.openxmlformats-officedocument.presentationml.presentation" position="float" xlink:href="info:doi/10.1371/journal.pone.0204897.s001" xlink:type="simple">
<label>S1 File</label>
<caption>
<title>Boxplots of the AUC corrected by the bootstrap .632 and .632+ variants and by double-loop cross-validation on the pooled database as well as all Figures illustrating the results obtained with the specific clinical subtype subtypes.</title>
<p>The boxplots of the AUCPR obtained by double-loop cross-validation on the pooled database are also given.</p>
<p>(PPTX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0204897.s002" position="float" xlink:href="info:doi/10.1371/journal.pone.0204897.s002" xlink:type="simple">
<label>S1 Code</label>
<caption>
<title>Main R code to run the analysis.</title>
<p>(R)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0204897.s003" position="float" xlink:href="info:doi/10.1371/journal.pone.0204897.s003" xlink:type="simple">
<label>S1 Functions</label>
<caption>
<title>Library of R functions to run the main code.</title>
<p>(R)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The Authors are extremely grateful to Lodewyk F.A. Wessels from the Netherlands Cancer Institute and Delft University of Technology for sharing the data used in this research paper as well as for its fruitful advice.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0204897.ref001"><label>1</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Harrell</surname> <given-names>F</given-names></name>. <source>Regression modeling strategies: with applications to linear models, logistic and ordinal regression, and survival analysis</source>: <publisher-name>Springer</publisher-name>; <year>2015</year>.</mixed-citation></ref>
<ref id="pone.0204897.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sauerbrei</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Royston</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Binder</surname> <given-names>H</given-names></name>. <article-title>Selection of important variables and determination of functional form for continuous predictors in multivariable model building</article-title>. <source>Statistics in medicine</source>. <year>2007</year>;<volume>26</volume>(<issue>30</issue>):<fpage>5512</fpage>–<lpage>28</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/sim.3148" xlink:type="simple">10.1002/sim.3148</ext-link></comment> <object-id pub-id-type="pmid">18058845</object-id></mixed-citation></ref>
<ref id="pone.0204897.ref003"><label>3</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Steyerberg</surname> <given-names>E</given-names></name>. <source>Clinical prediction models: a practical approach to development, validation, and updating</source>: <publisher-name>Springer Science &amp; Business Media</publisher-name>; <year>2008</year>.</mixed-citation></ref>
<ref id="pone.0204897.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Collignon</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Monnez</surname> <given-names>J-M</given-names></name>. <article-title>Clustering of the Values of a Response Variable and Simultaneous Covariate Selection Using a Stepwise Algorithm</article-title>. <source>Applied Mathematics</source>. <year>2016</year>;<volume>7</volume>:<fpage>1639</fpage>–<lpage>48</lpage>.</mixed-citation></ref>
<ref id="pone.0204897.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tibshirani</surname> <given-names>R</given-names></name>. <article-title>Regression shrinkage and selection via the lasso</article-title>. <source>Journal of the Royal Statistical Society Series B (Methodological)</source>. <year>1996</year>:<fpage>267</fpage>–<lpage>88</lpage>.</mixed-citation></ref>
<ref id="pone.0204897.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fan</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lv</surname> <given-names>J</given-names></name>. <article-title>A selective overview of variable selection in high dimensional feature space</article-title>. <source>Statistica Sinica</source>. <year>2010</year>;<volume>20</volume>(<issue>1</issue>):<fpage>101</fpage>. <object-id pub-id-type="pmid">21572976</object-id></mixed-citation></ref>
<ref id="pone.0204897.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zou</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Hastie</surname> <given-names>T</given-names></name>. <article-title>Regularization and variable selection via the elastic net</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>. <year>2005</year>;<volume>67</volume>(<issue>2</issue>):<fpage>301</fpage>–<lpage>20</lpage>.</mixed-citation></ref>
<ref id="pone.0204897.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fan</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>R</given-names></name>. <article-title>Variable selection via nonconcave penalized likelihood and its oracle properties</article-title>. <source>Journal of the American statistical Association</source>. <year>2001</year>;<volume>96</volume>(<issue>456</issue>):<fpage>1348</fpage>–<lpage>60</lpage>.</mixed-citation></ref>
<ref id="pone.0204897.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Park</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Casella</surname> <given-names>G</given-names></name>. <article-title>The bayesian lasso</article-title>. <source>Journal of the American Statistical Association</source>. <year>2008</year>;<volume>103</volume>(<issue>482</issue>):<fpage>681</fpage>–<lpage>6</lpage>.</mixed-citation></ref>
<ref id="pone.0204897.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ishwaran</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Rao</surname> <given-names>JS</given-names></name>. <article-title>Spike and slab variable selection: frequentist and Bayesian strategies</article-title>. <source>The Annals of Statistics</source>. <year>2005</year>;<volume>33</volume>(<issue>2</issue>):<fpage>730</fpage>–<lpage>73</lpage>.</mixed-citation></ref>
<ref id="pone.0204897.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O'Hara</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Sillanpää</surname> <given-names>MJ</given-names></name>. <article-title>A review of Bayesian variable selection methods: what, how and which</article-title>. <source>Bayesian analysis</source>. <year>2009</year>;<volume>4</volume>(<issue>1</issue>):<fpage>85</fpage>–<lpage>117</lpage>.</mixed-citation></ref>
<ref id="pone.0204897.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Oh</surname> <given-names>H-S</given-names></name>. <article-title>A new sparse variable selection via random-effect model</article-title>. <source>Journal of Multivariate Analysis</source>. <year>2014</year>;<volume>125</volume>:<fpage>89</fpage>–<lpage>99</lpage>.</mixed-citation></ref>
<ref id="pone.0204897.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kwon</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Oh</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>Y</given-names></name>. <article-title>The use of random-effect models for high-dimensional variable selection problems</article-title>. <source>Computational Statistics &amp; Data Analysis</source>. <year>2016</year>;<volume>103</volume>:<fpage>401</fpage>–<lpage>12</lpage>.</mixed-citation></ref>
<ref id="pone.0204897.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ng</surname> <given-names>CT</given-names></name>, <name name-style="western"><surname>Oh</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>Y</given-names></name>. <article-title>Going beyond oracle property: Selection consistency and uniqueness of local solution of the generalized linear model</article-title>. <source>Statistical Methodology</source>. <year>2016</year>;<volume>32</volume>:<fpage>147</fpage>–<lpage>60</lpage>.</mixed-citation></ref>
<ref id="pone.0204897.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>de Ronde</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Bonder</surname> <given-names>MJ</given-names></name>, <name name-style="western"><surname>Lips</surname> <given-names>EH</given-names></name>, <name name-style="western"><surname>Rodenhuis</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Wessels</surname> <given-names>LF</given-names></name>. <article-title>Breast cancer subtype specific classifiers of response to neoadjuvant chemotherapy do not outperform classifiers trained on all subtypes</article-title>. <source>PloS one</source>. <year>2014</year>;<volume>9</volume>(<issue>2</issue>):<fpage>e88551</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0088551" xlink:type="simple">10.1371/journal.pone.0088551</ext-link></comment> <object-id pub-id-type="pmid">24558399</object-id></mixed-citation></ref>
<ref id="pone.0204897.ref016"><label>16</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Boyd</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Eng</surname> <given-names>KH</given-names></name>, <name name-style="western"><surname>Page</surname> <given-names>CD</given-names></name>, editors. <chapter-title>Area under the precision-recall curve: Point estimates and confidence intervals</chapter-title>. <source>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</source>; <year>2013</year>: <publisher-name>Springer</publisher-name>.</mixed-citation></ref>
<ref id="pone.0204897.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Nelder</surname> <given-names>JA</given-names></name>. <article-title>Hierarchical generalized linear models</article-title>. <source>Journal of the Royal Statistical Society Series B (Methodological)</source>. <year>1996</year>:<fpage>619</fpage>–<lpage>78</lpage>.</mixed-citation></ref>
<ref id="pone.0204897.ref018"><label>18</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Lee</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Nelder</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Pawitan</surname> <given-names>Y</given-names></name>. <source>Generalized linear models with random effects: unified analysis via H-likelihood</source>, <edition>2nd edition</edition>: <publisher-name>CRC Press</publisher-name>; <year>2017</year>.</mixed-citation></ref>
<ref id="pone.0204897.ref019"><label>19</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Lee</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Oh</surname> <given-names>H-S</given-names></name>. <source>Random-effect models for variable selection</source>: <publisher-name>Department of Statistics, Stanford University</publisher-name>; <year>2009</year>.</mixed-citation></ref>
<ref id="pone.0204897.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DeLong</surname> <given-names>ER</given-names></name>, <name name-style="western"><surname>DeLong</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Clarke-Pearson</surname> <given-names>DL</given-names></name>. <article-title>Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach</article-title>. <source>Biometrics</source>. <year>1988</year>:<fpage>837</fpage>–<lpage>45</lpage>. <object-id pub-id-type="pmid">3203132</object-id></mixed-citation></ref>
<ref id="pone.0204897.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Steyerberg</surname> <given-names>EW</given-names></name>, <name name-style="western"><surname>Harrell</surname> <given-names>FE</given-names></name>, <name name-style="western"><surname>Borsboom</surname> <given-names>GJ</given-names></name>, <name name-style="western"><surname>Eijkemans</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Vergouwe</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Habbema</surname> <given-names>JDF</given-names></name>. <article-title>Internal validation of predictive models: efficiency of some procedures for logistic regression analysis</article-title>. <source>Journal of clinical epidemiology</source>. <year>2001</year>;<volume>54</volume>(<issue>8</issue>):<fpage>774</fpage>–<lpage>81</lpage>. <object-id pub-id-type="pmid">11470385</object-id></mixed-citation></ref>
<ref id="pone.0204897.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Collignon</surname> <given-names>O</given-names></name>. <article-title>Methodological issues in the design of a rheumatoid arthritis activity score and its cut-offs</article-title>. <source>Clinical epidemiology</source>. <year>2014</year>;<volume>6</volume>:<fpage>221</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2147/CLEP.S64811" xlink:type="simple">10.2147/CLEP.S64811</ext-link></comment> <object-id pub-id-type="pmid">25031546</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>