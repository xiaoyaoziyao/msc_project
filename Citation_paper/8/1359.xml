<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-11-01005</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002393</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
            </subj-group>
            <subj-group>
              <subject>Developmental neuroscience</subject>
            </subj-group>
            <subj-group>
              <subject>Neurophysiology</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Computer science</subject>
          <subj-group>
            <subject>Computer modeling</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Neuroscience</subject>
          <subject>Computer Science</subject>
        </subj-group>
      </article-categories><title-group><article-title>Depression-Biased Reverse Plasticity Rule Is Required for Stable Learning at Top-Down Connections</article-title><alt-title alt-title-type="running-head">Feedback Synapses Use Reverse Plasticity Rule</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Burbank</surname>
            <given-names>Kendra S.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Kreiman</surname>
            <given-names>Gabriel</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="aff" rid="aff3">
            <sup>3</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1"><label>1</label><addr-line>Department of Neurology and Ophthalmology, Children's Hospital Boston, Harvard Medical School, Boston, Massachusetts, United States of America</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Center for Brain Science, Harvard University, Cambridge, Massachusetts, United States of America</addr-line>       </aff><aff id="aff3"><label>3</label><addr-line>Swartz Center for Theoretical Neuroscience, Harvard University, Cambridge, Massachusetts, United States of America</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Behrens</surname>
            <given-names>Tim</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">University of Oxford, United Kingdom</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">Gabriel.kreiman@tch.harvard.edu</email></corresp>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: KSB GK. Performed the experiments: KSB. Analyzed the data: KSB GK. Wrote the paper: KSB GK.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>3</month>
        <year>2012</year>
      </pub-date><pub-date pub-type="epub">
        <day>1</day>
        <month>3</month>
        <year>2012</year>
      </pub-date><volume>8</volume><issue>3</issue><elocation-id>e1002393</elocation-id><history>
        <date date-type="received">
          <day>12</day>
          <month>7</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>1</day>
          <month>1</month>
          <year>2012</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2012</copyright-year><copyright-holder>Burbank, Kreiman</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>Top-down synapses are ubiquitous throughout neocortex and play a central role in cognition, yet little is known about their development and specificity. During sensory experience, lower neocortical areas are activated before higher ones, causing top-down synapses to experience a preponderance of post-synaptic activity preceding pre-synaptic activity. This timing pattern is the opposite of that experienced by bottom-up synapses, which suggests that different versions of spike-timing dependent synaptic plasticity (STDP) rules may be required at top-down synapses. We consider a two-layer neural network model and investigate which STDP rules can lead to a distribution of top-down synaptic weights that is stable, diverse and avoids strong loops. We introduce a temporally reversed rule (rSTDP) where top-down synapses are potentiated if post-synaptic activity precedes pre-synaptic activity. Combining analytical work and integrate-and-fire simulations, we show that only depression-biased rSTDP (and not classical STDP) produces stable and diverse top-down weights. The conclusions did not change upon addition of homeostatic mechanisms, multiplicative STDP rules or weak external input to the top neurons. Our prediction for rSTDP at top-down synapses, which are distally located, is supported by recent neurophysiological evidence showing the existence of temporally reversed STDP in synapses that are distal to the post-synaptic cell body.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>The complex circuitry in the cerebral cortex is characterized by bottom-up connections, which carry feedforward information from the sensory periphery to higher areas, and top-down connections, where the information flow is reversed. Changes over time in the strength of synaptic connections between neurons underlie development, learning and memory. A fundamental mechanism to change synaptic strength is spike timing dependent plasticity, whereby synapses are strengthened whenever pre-synaptic spikes shortly precede post-synaptic spikes and are weakened otherwise; the relative timing of spikes therefore dictates the direction of plasticity. Spike timing dependent plasticity has been observed in multiple species and different brain areas. Here, we argue that top-down connections obey a learning rule with a reversed temporal dependence, which we call reverse spike timing dependent plasticity. We use mathematical analysis and computational simulations to show that this reverse time learning rule, and not previous learning rules, leads to a biologically plausible connectivity pattern with stable synaptic strengths. This reverse time learning rule is supported by recent neuroanatomical and neurophysiological experiments and can explain empirical observations about the development and function of top-down synapses in the brain.</p>
      </abstract><funding-group><funding-statement>This work was supported by the NIH New Innovator Award (DP2 OD005393), NIH (R21 EY019710), NSF (0954570 and 1010109) and the Whitehall Foundation. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="16"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Connectivity patterns between different areas in neocortex are often discussed in terms of bottom-up and top-down connections <xref ref-type="bibr" rid="pcbi.1002393-Felleman1">[1]</xref>, . With few exceptions, communication between any two connected neocortical areas occurs in both directions <xref ref-type="bibr" rid="pcbi.1002393-Felleman1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Salin1">[4]</xref>. Feedforward or “bottom-up” connections are those which run from lower neocortical areas (such as visual area V1) to higher areas (such as V2); they typically but not always originate in layers 2/3 and synapse onto neurons in layer 4 <xref ref-type="bibr" rid="pcbi.1002393-Felleman1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Salin1">[4]</xref>, . By contrast, feedback or “top-down” connections, which run from higher neocortical areas to lower ones, typically originate in layer 6 and frequently synapse onto distal dendrites in layer 1. While bottom-up synapses have been widely studied and modeled, the development, functions and properties of the more-abundant top-down connections are less well understood <xref ref-type="bibr" rid="pcbi.1002393-Bullier1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Douglas1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Callaway1">[7]</xref>.</p>
      <p>Here we investigate the learning rules that govern the development of top-down connections in neocortex. We study variations on a classical paradigm describing changes in synaptic strength between two neurons: spike-timing dependent plasticity (STDP) <xref ref-type="bibr" rid="pcbi.1002393-Levy1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Markram1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Bi1">[10]</xref>. According to STDP, when a pre-synaptic spike occurs within tens of milliseconds before a post-synaptic spike, the synaptic strength is enhanced. Conversely, when a pre-synaptic spike occurs shortly after a post-synaptic spike, the synaptic strength is decreased. STDP has been observed in a wide variety of systems and conditions <xref ref-type="bibr" rid="pcbi.1002393-Dan1">[11]</xref> and has been examined in many computational studies as well (e.g. <xref ref-type="bibr" rid="pcbi.1002393-Rao1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Song1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Babadi1">[14]</xref>; for reviews, see <xref ref-type="bibr" rid="pcbi.1002393-Abbott1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Kepecs1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Worgotter1">[17]</xref>).</p>
      <p>In order to calculate the effects of different types of learning rules in neocortical circuits, the relative timing of firing events during signal propagation needs to be taken into account. During activity evoked by transient stimuli, neurons in a lower area such as V1 will generally be activated before neurons in a higher area (such as V4) <xref ref-type="bibr" rid="pcbi.1002393-Maunsell1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Schmolesky1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Marsalek1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Maunsell2">[21]</xref>. Under this scenario, bottom-up synapses will experience a predominance of pre-synaptic spikes followed by postsynaptic ones (“pre-post” spike pairs). For top-down synapses, on the other hand, the identities of the pre- and post-synaptic neurons are reversed, meaning that stimulus-evoked activity will be experienced as a predominance of “post-pre” spike pairs. Here, motivated by this identity reversal, we hypothesize that the learning rule at top-down synapses might exhibit unusual temporal dependences. Specifically, we propose that learning at top-down synapses follows a temporally reversed version of spike-time-dependent plasticity, which we call rSTDP (<xref ref-type="fig" rid="pcbi-1002393-g001"><bold>Figure 1</bold></xref>).</p>
      <fig id="pcbi-1002393-g001" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002393.g001</object-id>
        <label>Figure 1</label>
        <caption>
          <title>Schematic description of the model and learning rules.</title>
          <p><bold>a</bold>. Schematic description of the model used in the analytical and computational work. The model consists of two layers: a “lower” cortical area (units with activity <italic>L<sub>i</sub></italic>(t)) and a “higher” cortical area (units with activity <italic>H<sub>j</sub>(t)</italic>). <bold>b</bold>. The strength of the all-to-all bottom-up connections from the lower area to the higher area is represented by the matrix <bold>Q</bold> (gray arrows). These synapses occur in proximal dendrites and their weights are fixed unless otherwise noted. The strength of the all-to-all top-down connections from the higher area to the lower area is represented by the matrix <bold>W</bold> (black arrows). These synapses occur in distal dendrites. The <bold>W</bold> weights evolve according to the plasticity rules described in <bold>c–d</bold>. There are no connections within each layer. <bold>c</bold>. Schematic description of “classical” spike-time dependent plasticity (cSTDP). For a given synapse, the y-axis indicates the change in the weight (<italic>Δw</italic>) and the x-axis represents the temporal difference between the post-synaptic action potential and the pre-synaptic action potential (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e001" xlink:type="simple"/></inline-formula>). The green curve shows the learning rule used in the analytical section while the blue curve shows the learning rule used in the integrate-and-fire simulations. In cSTDP, a pre-synaptic action potential followed by a post-synaptic action potential (<italic>Δt&gt;0</italic>) leads to potentiation (<italic>Δw&gt;0</italic>). The learning rate at each synapse is controlled by the parameter <italic>μ</italic> and the ratio of depression to potentiation is controlled by <italic>α</italic>. In the computational simulations, the parameter <italic>τ<sub>STDP</sub></italic> controls the rate of weight change with <italic>Δt</italic>. <bold>d</bold>. Schematic description of “reverse” STDP (rSTDP).</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.g001" xlink:type="simple"/>
      </fig>
      <p>We compare the long-term effects of training a population of top-down synapses using either classical STDP (cSTDP) or rSTDP. We argue that the plasticity rules must lead to a distribution of top-down synaptic weights that fulfills the following three key properties. (1) <italic>Top-down weights should be stable</italic>. When the statistics describing the environment are stationary, the top-down connections should settle into an unchanging pattern, allowing the information carried through top-down connections to be consistently interpreted. (2) <italic>Top-down weights should be diverse</italic>. We expect to observe a continuous distribution of strengths in top-down connections with a significant spread (as opposed to binary weights or all weights taking the same value) <xref ref-type="bibr" rid="pcbi.1002393-Babadi1">[14]</xref>. Functionally, a diverse set of top-down connections can perform a richer set of computations. (For discussions of the computational properties of synapses with graded strengths, see <xref ref-type="bibr" rid="pcbi.1002393-Fusi1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Satel1">[23]</xref>.) (3) <italic>Top-down weights should be weak</italic>. Specifically, top-down connections should not create any strong loops <xref ref-type="bibr" rid="pcbi.1002393-Abbott1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Crick1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Douglas2">[25]</xref>, as these can amplify neuronal activity to pathological levels. We emphasize that this condition does not preclude the existence of strong <italic>individual</italic> top-down connections; these are permitted so long as the combined effect of all bottom-up and top-down connections does not lead to runaway excitation.</p>
      <p>Using analytical methods and numerical simulations, we compare networks whose top-down connections exhibit plasticity via rSTDP with those whose top-down connections exhibit classical STDP. We further examine the effects of biasing learning towards depression or towards potentiation. We argue that depression-biased rSTDP, but not cSTDP, can lead to a stable, diverse and weak distribution of top-down weights. Finally, we show that the model's predictions are consistent with recent experimental findings about the relationship between plasticity and neuroanatomy.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <p>We study the characteristics of synaptic plasticity learning rules at top-down synapses and evaluate whether the resulting distribution of synaptic strengths fulfill the three properties outlined above: stability, diversity and weakness. We start by considering a simple model that we can solve analytically, and then evaluate the results with integrate-and-fire simulations. The network models described in the analytical and integrate-and-fire sections share the same basic structure (<xref ref-type="fig" rid="pcbi-1002393-g001"><bold>Figure 1a–b</bold></xref>). The model consists of two levels of neurons, with every neuron in the lower level connected reciprocally to every neuron in the higher level. A number of simplifications should be noted: (i) there are no lateral connections within a level; (ii) there is no separation of excitatory and inhibitory neurons although weights can take positive or negative values; (iii) external inputs arrive only at the lower level (except in section “<bold>Top-down modulatory signals”</bold>).</p>
      <p>The steps in each computational experiment were to a) generate a network with initial bottom-up and top-down synaptic strengths; b) specify an external stimulus to initiate activity in lower-level neurons; c) calculate the resultant neuronal activity over time in the network; d) change synaptic weights according to this activity and to our specified learning rule; and e) repeat steps b–d until we can determine the characteristics of the final weight distribution. In most cases, we modify only the top-down weights in step (d), keeping the bottom-up weights constant (however, we explore concurrent modification of bottom-up weights and top-down weights in the section “<bold>Additional stability mechanisms</bold>”.) The outcomes of this basic paradigm are calculated analytically in the first section and determined through simulations in the integrate-and-fire section.</p>
      <sec id="s2a">
        <title>Analytical model of plasticity at top-down synapses</title>
        <p>In this section, we consider model neurons whose activities at each time-point are linear sums of the synaptic inputs at the previous time-point. During a stimulus presentation, the activity in lower-level neurons at the first time-point <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e002" xlink:type="simple"/></inline-formula> is given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e003" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e004" xlink:type="simple"/></inline-formula> is a vector describing the external inputs to the lower-level neurons. Activity in higher-level neurons in the next time-point is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e005" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e006" xlink:type="simple"/></inline-formula> is a matrix describing bottom-up synaptic weights. Activity then propagates back to the lower-level neurons, with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e007" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e008" xlink:type="simple"/></inline-formula> is the matrix of top-down weights. We assume that plasticity is slow, so that we can approximate top-down weights as unchanging during a single stimulus presentation (see <bold><xref ref-type="supplementary-material" rid="pcbi.1002393.s001">Text S1</xref></bold>). Activity continues to move up and down through the network during the stimulus presentation.</p>
        <p>At the end of each stimulus presentation, we determine the change in synaptic strength for each pair of neurons by considering the joint activities of those two neurons, as calculated in every pair of adjacent time-points during a stimulus presentation. Because we focus on the top-down synapses, the higher-level units are pre-synaptic and the lower-level units are post-synaptic. The learning rule is a simplified version of spike-timing dependent plasticity (STDP) (<xref ref-type="fig" rid="pcbi-1002393-g001"><bold>Figure 1c–d</bold></xref> and <bold><xref ref-type="supplementary-material" rid="pcbi.1002393.s001">Text S1</xref></bold>). The learning rule is written here for clarity with two terms: the first term represents joint activity from events when the post-synaptic lower-level units are activated <italic>before</italic> the pre-synaptic higher-level units (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e009" xlink:type="simple"/></inline-formula>), while the second term describes joint activity from events when the post-synaptic lower-level units are activated <italic>after</italic> the pre-synaptic higher-level units (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e010" xlink:type="simple"/></inline-formula>). We write two equations, one describing cSTDP (<xref ref-type="fig" rid="pcbi-1002393-g001"><bold>Figure 1c</bold></xref><bold>, </bold><bold>Eq 1</bold>) and the other one describing rSTDP (<xref ref-type="fig" rid="pcbi-1002393-g001"><bold>Figure 1d</bold></xref><bold>,</bold> <bold>Eq 1′</bold>):<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e011" xlink:type="simple"/><label>(1)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e012" xlink:type="simple"/><label>(1′)</label></disp-formula></p>
        <p>The learning rate of synaptic plasticity is set by a parameter<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e013" xlink:type="simple"/></inline-formula>. The parameter describing the balance between depression and potentiation is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e014" xlink:type="simple"/></inline-formula>; when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e015" xlink:type="simple"/></inline-formula>, depression dominates over potentiation. <bold>Equation 1</bold> reflects cSTDP, in which the weights increase from joint activity where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e016" xlink:type="simple"/></inline-formula> and decrease when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e017" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1002393-g001"><bold>Figure 1c</bold></xref>). The alternative learning rule considered here (rSTDP) is given by <bold>Equation 1′</bold>; in this case the weights decrease from joint activity when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e018" xlink:type="simple"/></inline-formula> and increase when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e019" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1002393-g001"><bold>Figure 1d</bold></xref>). As discussed below, this sign reversal between cSTDP and rSTDP is at the heart of the discussion about the stability of the learning rule for top-down synapses.</p>
        <p>Using the expressions for neuronal activity and synaptic plasticity, we can determine and characterize <italic>fixed points</italic> of the system. These are sets of top-down weights which produce activity that, on average, leads to no further change in the weights. Fixed points represent potential places where the weights might settle after multiple stimulus presentations. To find an expression for fixed points, we plug the expressions for neuronal activity into <bold>Equation 1</bold> or <bold>1′</bold> and look for points where Δ<bold>W</bold> becomes zero (<bold><xref ref-type="supplementary-material" rid="pcbi.1002393.s001">Text S1</xref></bold>).</p>
        <p>We show that any fixed point <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e020" xlink:type="simple"/></inline-formula> must obey a simple relation: for cSTDP, the relation is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e021" xlink:type="simple"/></inline-formula>; for rSTDP, it is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e022" xlink:type="simple"/></inline-formula>. Here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e023" xlink:type="simple"/></inline-formula> is the cross-correlation matrix formed by averaging the joint initial activity of pairs of lower-level neurons across many external stimuli. These equations imply that for cSTDP <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e024" xlink:type="simple"/></inline-formula> is an eigenvalue of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e025" xlink:type="simple"/></inline-formula>, and that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e026" xlink:type="simple"/></inline-formula> is an eigenvalue of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e027" xlink:type="simple"/></inline-formula> for rSTDP.</p>
        <p>From these relations, we see that for both cSTDP and rSTDP, top-down weights at a fixed point will typically be <italic>diverse</italic>: they will make up a continuous distribution, and will not be binary or single-valued. Counter-examples exist only for very particular choices of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e028" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e029" xlink:type="simple"/></inline-formula>, such as when the distribution of bottom-up weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e030" xlink:type="simple"/></inline-formula> is itself single-valued. We conclude that fixed points in this model will generally meet the criterion of diversity, regardless of the parameters of the learning rule.</p>
        <p>We also note that potential fixed points depend both on the bottom-up weights (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e031" xlink:type="simple"/></inline-formula>) and on the statistical structure of the external inputs (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e032" xlink:type="simple"/></inline-formula>). The presence of the correlation term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e033" xlink:type="simple"/></inline-formula>, specifically, means that we can describe the learning rule as <italic>correlative</italic>. In the special case where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e034" xlink:type="simple"/></inline-formula> is invertible, the relations simplify to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e035" xlink:type="simple"/></inline-formula>for cSTDP and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e036" xlink:type="simple"/></inline-formula>for rSTDP, meaning that top-down connections simply reproduce a scaled version of earlier lower-level activity (see further discussion below.)</p>
      </sec>
      <sec id="s2b">
        <title>Requirements for prevention of strong loops</title>
        <p>We ask whether top-down weights at fixed points meet the criterion of weakness (defined as the absence of strong excitatory loops.) A strong loop exists whenever there are patterns of neuronal activity which are amplified as they pass up and down through the network. Because the network is linear, activity at any time-point can be calculated by multiplying the previous activity by the matrix <bold>W<sub>0</sub>Q</bold> (for example <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e037" xlink:type="simple"/></inline-formula>.) The activity will increase, implying the existence of strong loops, whenever the matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e038" xlink:type="simple"/></inline-formula> has eigenvalues greater than one. As discussed above, for fixed points, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e039" xlink:type="simple"/></inline-formula> has eigenvalues of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e040" xlink:type="simple"/></inline-formula> (for cSTDP) and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e041" xlink:type="simple"/></inline-formula>(for rSTDP) (see <bold><xref ref-type="supplementary-material" rid="pcbi.1002393.s001">Text S1</xref></bold> for further details). This means that strong loops must exist at every fixed point for depression-biased cSTDP and for potentiation-biased rSTDP. Thus, the only plasticity rules which can produce weak and potentially stable top-down weights are potentiation-biased cSTDP and depression-biased rSTDP.</p>
      </sec>
      <sec id="s2c">
        <title>Depression-biased reverse STDP is required for development of unchanging top-down weights</title>
        <p>Finally, we consider the requirement for stability. We evaluate whether fixed points are stable or not by performing a linear stability analysis, which examines the effect of plasticity when weights are close to but not equal to a fixed point. If the fixed point is stable, plasticity must draw the weights ever closer; if it is unstable, plasticity will push weights away from the fixed point.</p>
        <p>To perform the stability analysis, we calculate how the difference between the current top-down weights and the fixed point changes over time <xref ref-type="bibr" rid="pcbi.1002393-Bacciotti1">[26]</xref> (<bold><xref ref-type="supplementary-material" rid="pcbi.1002393.s001">Text S1</xref></bold>). We show that under cSTDP, at least one component of the difference between the current top-down weights and the fixed point will actually <italic>grow</italic> over time as a result of plasticity, and hence the fixed point must be <italic>unstable</italic>. Therefore, in the model architecture presented here, networks where top-down connections are trained with cSTDP cannot have any stable fixed points. By contrast, networks in which top-down connections are learned with rSTDP may have stable fixed points. We conclude that fixed points in this model can meet the criterion of stability only for rSTDP. Putting these results together, we see that only for depression-biased rSTDP can plasticity lead to sets of top-down weights that simultaneously meet the criteria of stability, diversity and weakness.</p>
        <p>An intuitive understanding of the requirement for rSTDP can be gained by considering only the first three time-points in a stimulus presentation. The top-down weights only affect activity starting at time <italic>t = 2</italic>. For cSTDP, the pre-post synaptic joint activity from times 1 and 2 leads to potentiation and increased activity at <italic>t = 2</italic>, which in turns causes further potentiation. In this positive feedback loop the weights can increase indefinitely. By contrast, with rSTDP, joint activity from times 1 and 2 leads to depression. Any increase in the strength of the top-down weights will cause more activity at <italic>t = 2</italic> and thus lead to additional depression, bringing the weights back into balance. Therefore these circuits will tend to self-stabilize. The analytical work discussed above and in the <bold><xref ref-type="supplementary-material" rid="pcbi.1002393.s001">Text S1</xref></bold> together with the simulations in the next section formalize and extend this argument beyond the initial time points.</p>
        <p>We emphasize that the requirement for rSTDP only applies to learning at top-down synapses. A similar analysis can be performed for bottom-up synapses by holding <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e042" xlink:type="simple"/></inline-formula> constant while modifying <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e043" xlink:type="simple"/></inline-formula>. In the <bold><xref ref-type="supplementary-material" rid="pcbi.1002393.s001">Text S1</xref></bold>, we show for simple cases that stable training of bottom-up synapses requires cSTDP. Therefore, the results presented here are consistent with the existence of a conventional plasticity rule (cSTDP) at bottom-up connections while implying the necessity of a temporally reversed plasticity rule (rSTDP) for top-down connections. Concurrent changes in <bold>W</bold> and <bold>Q</bold> are considered in section <bold>“Additional stability mechanisms”</bold>.</p>
        <p>We also considered the case where the lower and upper cortical areas were not reciprocally connected. In the <bold><xref ref-type="supplementary-material" rid="pcbi.1002393.s001">Text S1</xref></bold>, we show that rSTDP is still required at top-down connections in this case. Because the mathematics in this case are somewhat simpler, we were able to move beyond linear neurons and show that the requirement for rSTDP holds when the neurons have an arbitrary non-linear but monotonic activation function. We also show that in this case the bias towards depression is not necessary, since strong excitatory loops cannot develop in the absence of reciprocal connections.</p>
      </sec>
      <sec id="s2d">
        <title>Example of development of top-down weights</title>
        <p>As a sanity check and to illustrate the dynamical changes in the weights as a consequence of the learning rule, we created a numerical implementation of our analytical network by using <bold>Equation A22</bold> (see <bold><xref ref-type="supplementary-material" rid="pcbi.1002393.s001">Text S1</xref></bold>). <xref ref-type="fig" rid="pcbi-1002393-g002"><bold>Figure 2</bold></xref> shows the results of a simulation with rSTDP and depression dominating (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e044" xlink:type="simple"/></inline-formula>). The evolution of the top-down weight matrix <bold>W</bold> over multiple stimulus presentations is shown in <xref ref-type="fig" rid="pcbi-1002393-g002"><bold>Figure 2a</bold></xref>. The weights change rapidly at the beginning and converge to a stable solution. The magnitude of the changes in <bold>W</bold> approaches zero as the algorithm converges (<xref ref-type="fig" rid="pcbi-1002393-g002"><bold>Figure 2b</bold></xref>) and the standard deviation of the weights approaches a constant value (<xref ref-type="fig" rid="pcbi-1002393-g002"><bold>Figure 2c</bold></xref>). We predicted that the top-down weight matrix <bold>W</bold> would approach the inverse of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e045" xlink:type="simple"/></inline-formula> when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e046" xlink:type="simple"/></inline-formula> is invertible, as it is in <xref ref-type="fig" rid="pcbi-1002393-g002"><bold>Figure 2</bold></xref>. <xref ref-type="fig" rid="pcbi-1002393-g002"><bold>Figure 2d</bold></xref> shows that the correlation coefficient between <bold>W</bold> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e047" xlink:type="simple"/></inline-formula> indeed approaches 1 over time. The final distribution of weights is continuous and diverse (as opposed to being binary or single-valued) (<xref ref-type="fig" rid="pcbi-1002393-g002"><bold>Figure 2e</bold></xref>). Finally, all the eigenvalues of <bold>W</bold> are below 1 (<xref ref-type="fig" rid="pcbi-1002393-g002"><bold>Figure 2f</bold></xref>) as required to avoid runaway excitation. In sum, we have illustrated that the circuit simulated in <xref ref-type="fig" rid="pcbi-1002393-g002"><bold>Figure 2</bold></xref> fulfills the three requisite criteria: the final distribution of weights is stable, diverse and does not lead to runaway excitation.</p>
        <fig id="pcbi-1002393-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002393.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Example numerical implementation of the analytical results for depression-biased rSTDP learning.</title>
            <p><bold>a.</bold> Development of top-down synaptic weights (<bold>W</bold>) over multiple stimulus presentations. <italic>N</italic> indicates the stimulus presentation number and here we show 4 snapshots of <bold>W</bold><italic>(N)</italic>. This model had 20 lower units and 20 higher units. The strength of each synaptic weight is represented by the color in the <bold>W</bold> matrix (see scale on the right). The algorithm converged after 2005 iterations and the final <bold>W</bold> is shown on the right (see <xref ref-type="sec" rid="s4">Methods</xref> for convergence criteria). <bold>b–d.</bold> Measures of weight stability and diversity. <bold>b.</bold> Norm of the change in the top-down weight matrix (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e048" xlink:type="simple"/></inline-formula>) as a function of stimulus presentation number <italic>N</italic> (see text). As the algorithm converges, the change in the weights becomes smaller. The dotted lines mark the iterations corresponding to the snapshots shown in part <bold>a</bold>. <bold>c.</bold> Standard deviation of the distribution of top-down weights as a function of iteration presentation number (loosely represented in the y-axis as std(<bold>W</bold>)). The final value in this plot (<italic>N</italic> = 2005) corresponds to the standard deviation of the distribution shown in part <bold>e</bold>. <bold>d.</bold> Pearson correlation coefficient between the vectorized <bold>W</bold><italic>(N)</italic> and <bold>W</bold>(<italic>N-100</italic>) (blue line, calculated only for <italic>N&gt; = 100</italic>) and between <bold>W</bold>(<italic>N</italic>) and the predicted value of <bold>W</bold> at the fixed point (<bold>W*</bold> = <bold>Q<sup>−1</sup></bold>; green line, see text for details). As the algorithm converges, <bold>W</bold><italic>(N)</italic>→<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e049" xlink:type="simple"/></inline-formula>. <bold>e.</bold> Measure of weight diversity: Distribution of the final synaptic weights after the algorithm converged. Bin size = 4. <bold>f</bold>. Measure of absence of strong loops: Mean (blue) and maximum (green) eigenvalue of the matrix <bold>WQ</bold>, as a function of stimulus presentation number. This matrix describes the activity changes produced in a full up-down loop through the network. Eigenvalues greater than one would correspond to the existence of strong loops. The maximum eigenvalue never surpasses 0.33, which is equal to 1/<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e050" xlink:type="simple"/></inline-formula>. The mean eigenvalue also eventually stabilizes at this value.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.g002" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s2e">
        <title>In an integrate-and-fire simulation, depression-biased rSTDP leads to stable, diverse and weak top-down weights</title>
        <p>We supplement the analytical results above by relaxing many of the assumptions and simulating a network under biologically more realistic conditions. We performed numerical simulations of a network of noisy and leaky integrate-and-fire neurons (<bold>Eqs. 2</bold><bold>–</bold><bold>5</bold>, <xref ref-type="sec" rid="s4"><bold>Methods</bold></xref>). The simulations differed in four key ways from the analytical work above. First and most importantly, the integrate-and-fire model neurons enabled us to better simulate the non-linear responses that neurons typically display in response to their inputs as well as explicitly include spikes and plasticity rules based on spike timing (<bold>Eqs. 2</bold><bold>–</bold><bold>4</bold>). Second, the external input to lower-level neurons occurred over an extended period of time. Third, instead of considering only adjacent pairs of spikes we used a plasticity rule which varied smoothly in strength depending on the time difference between pre- and post-synaptic spikes (<bold>Eq. 5</bold>). Fourth, we introduced noise into our simulations in the form of noisy synaptic inputs. In principle, these differences could lead to qualitatively different effects on the requirement for depression-biased rSTDP.</p>
        <p>Apart from these differences, the model used in the simulations was similar to the one used in the analytical formulation (<xref ref-type="fig" rid="pcbi-1002393-g001"><bold>Figure 1</bold></xref>). Bottom-up connections were fixed (and generated as in <xref ref-type="fig" rid="pcbi-1002393-g002"><bold>Figure 2a</bold></xref>). The external inputs to each lower-level neuron were drawn from Gaussian distributions as described in <xref ref-type="sec" rid="s4"><bold>Methods</bold></xref>. Top-down weights were initially set to zero. In <xref ref-type="fig" rid="pcbi-1002393-g003"><bold>Figure 3</bold></xref>, we show the evolution of top-down weights in one example simulation in which we used depression-biased rSTDP. In <xref ref-type="fig" rid="pcbi-1002393-g004"><bold>Figure 4</bold></xref>, we show typical results of these simulations for each of the four main possible learning rules.</p>
        <fig id="pcbi-1002393-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002393.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Example of the dynamics and evolution of top-down weights in the integrate-and-fire model.</title>
            <p><bold>a.</bold> Snapshots showing the evolution of <bold>W</bold><italic>(N)</italic> in the integrate-and-fire network simulations over time defined by the number of stimulus presentations (<italic>N</italic>). The format is the same as in <xref ref-type="fig" rid="pcbi-1002393-g002"><bold>Figure 2a</bold></xref>. This model had 100 lower units and 100 higher units. The parameters used in this simulation are shown in the last column of <xref ref-type="table" rid="pcbi-1002393-t001"><bold>Table 1</bold></xref>, with rSTDP and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e051" xlink:type="simple"/></inline-formula> = 1.2. <bold>b–c.</bold> Measures of weight stability. <bold>b</bold>. Standard deviation of the distribution of top-down weights as a function of the stimulus presentation number. The convergence criterion for the standard deviation was that the slope of this plot (calculated as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e052" xlink:type="simple"/></inline-formula> with <italic>ΔN</italic> = 6000) be less than 10<sup>−5</sup>. The convergence criterion was met at the point indicated by the red asterisk. The dotted vertical lines correspond to the times of the five snapshots shown in part <bold>a</bold>. <bold>c.</bold> Blue line: Pearson correlation coefficient between the vectorized <bold><italic>W</italic></bold><italic>(N)</italic> and <bold><italic>W</italic></bold><italic>(N-ΔN)</italic>, for Δ<italic>N</italic> = 3000 iterations. For comparison with <xref ref-type="fig" rid="pcbi-1002393-g002"><bold>Figure 2</bold></xref>, we also show the correlation coefficient between <bold>W</bold>(<italic>N</italic>) and the inverse of <bold>Q</bold> (green line). We note that in the integrate and fire simulations we do not expect <bold>W</bold>(<italic>N</italic>) to converge to the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e053" xlink:type="simple"/></inline-formula>described in the text and <xref ref-type="fig" rid="pcbi-1002393-g002"><bold>Figure 2</bold></xref>. A simulation run was classified as ‘convergent’ when the correlation coefficient was greater than 0.99 and when the std criterion in part <bold>b</bold> was met. In this example, the simulation achieved the correlation criterion at <italic>T</italic> = 75000 (red asterisk). <bold>d</bold>. Measure of weight diversity: Distribution of the synaptic weights for the final snapshot. Bin size = 0.1. <bold>e</bold>. Measure of absence of strong loops: Average firing rate for lower-level neurons as a function of stimulus presentation number. The average firing rate almost immediately stabilizes to a constant value, and does not increase to pathological levels as occurs in the presence of strong excitatory loops.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.g003" xlink:type="simple"/>
        </fig>
        <fig id="pcbi-1002393-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002393.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Representative results of integrate-and-fire simulations for different learning rules.</title>
            <p>We consider here four possible learning rules: classical STDP (cSTDP, <bold>c</bold>,<bold>d</bold>), reverse STDP (rSTDP, <bold>a</bold>,<bold>b</bold>), depression-biased (<bold>a</bold>,<bold>c</bold>) and potentiation-based learning (<bold>b</bold>,<bold>d</bold>). For each learning rule, we show the results for a representative simulation (see summary results in <xref ref-type="fig" rid="pcbi-1002393-g005"><bold>Figure 5</bold></xref><bold>.</bold>) The format and conventions for the subplots are the same as in <xref ref-type="fig" rid="pcbi-1002393-g003"><bold>Figure 3</bold></xref>. The subplots show the Pearson correlation coefficient between the vector containing all the entries of <bold>W</bold><italic>(N)</italic> and that for <bold>W</bold><italic>(N-ΔN)</italic>, for ΔN = 3,000 iterations (first subplot), the standard deviation of the distribution of weights (second subplot), the distribution of weights (third subplot), the average firing rate of the lower level units (fourth subplot) and the final <bold>W</bold>. The simulation in part <bold>a</bold> converged; the convergence criteria were met at the value of <italic>N</italic> indicated by an asterisk. The simulations in <bold>b–d</bold> were classified as having “extreme weights” meaning that &gt;50% of the weights were either at 0 or at the weight boundaries (±50). The arrows in the second subplot in <bold>b–d</bold> denote inflection points where the weights reached the boundaries and the standard deviation started to decrease. The parameters for each of these simulations are listed in the last column of <xref ref-type="table" rid="pcbi-1002393-t001"><bold>Table 1</bold></xref>, with specifics as follows. <bold>a</bold> rSTDP, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e054" xlink:type="simple"/></inline-formula> = 1.2; <bold>b</bold>: rSTDP, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e055" xlink:type="simple"/></inline-formula> = 0.9; <bold>c</bold>: cSTDP, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e056" xlink:type="simple"/></inline-formula> = 1.2; <bold>d</bold>: cSTDP, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e057" xlink:type="simple"/></inline-formula> = 0.9. For the simulations in <bold>b–d</bold>, the weights varied most strongly across lower-level neurons, leading to the appearance of vertical bands in the final subplots (note the differences in the color scale and standard deviation values in <bold>4b–d</bold> compared to <bold>4a</bold>). Some lower-level neurons experienced greater joint activity than others due to the choice of <bold>Q</bold> (and hence greater plasticity); the instability of learning in these simulations then magnified these initial imbalances.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.g004" xlink:type="simple"/>
        </fig>
        <p>Each simulation was classified with one of four possible outcomes (see <xref ref-type="sec" rid="s4"><bold>Methods</bold></xref> for details). The first of these outcomes was “converged”; in order to qualify, a simulation's final top-down weights needed to satisfy our three key criteria of stability, diversity, and weakness. We assessed stability by calculating the cross-correlation of the current weights with those from previous time-points (<xref ref-type="fig" rid="pcbi-1002393-g003"><bold>Figure 3c</bold></xref><bold>, </bold><xref ref-type="fig" rid="pcbi-1002393-g004"><bold>4a</bold></xref> first subplot) as well as comparing the standard deviation of current and past weight distributions (<xref ref-type="fig" rid="pcbi-1002393-g003"><bold>Figure 3b</bold></xref><bold>, </bold><xref ref-type="fig" rid="pcbi-1002393-g004"><bold>4a</bold></xref> second subplot). We assessed diversity by asking whether the standard deviation of the top-down weights, when the simulation was stopped, surpassed a threshold value of 0.3 (<xref ref-type="fig" rid="pcbi-1002393-g003"><bold>Figure 3d</bold></xref>, <xref ref-type="fig" rid="pcbi-1002393-g004"><bold>4a</bold></xref> third subplot). We ensured that weights had not become too strong, assessing the absence of strong loops, by requiring that a convergent simulation have less than 50% of its weights at the maximum or minimum allowed weight. Simulations not labeled as “Converged” were categorized as “Weights too similar”, “Extreme weights”, or – in the rare cases when weights had not stabilized after 625,000 stimulus presentations – “Did not converge” (<xref ref-type="fig" rid="pcbi-1002393-g004"><bold>Figure 4b–d</bold></xref>),</p>
        <p>We next asked how the results of the simulations illustrated in <xref ref-type="fig" rid="pcbi-1002393-g003"><bold>Figure 3</bold></xref> and <xref ref-type="fig" rid="pcbi-1002393-g004"><bold>4a</bold></xref> depended on the parameters used in the simulations. In particular, we asked whether convergence required depression-biased rSTDP as it did for the linear network. We ran 6912 simulations, spanning a wide range of different sets of parameters as outlined in <xref ref-type="table" rid="pcbi-1002393-t001"><bold>Table 1</bold></xref>, including two bottom-up weight matrices <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e058" xlink:type="simple"/></inline-formula> and two external stimulus correlation matrices <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e059" xlink:type="simple"/></inline-formula>. We ran each simulation three times with different initial conditions. We summarize the results of this parameter landscape characterization in <xref ref-type="fig" rid="pcbi-1002393-g005"><bold>Figure 5</bold></xref>. Among the simulations with depression-biased rSTDP, convergence did not require fine-tuning of parameters – more than 90% of the simulations were categorized as convergent. Critically, none of the simulations with any of the other learning rules (potentiation biased rSTDP, potentiation or depression biased cSTDP) led to convergent simulations. Thus, in spite of the differences from the analytic work, the integrate-and-fire network simulations also lead us to a requirement for depression-biased rSTDP to achieve a stable, diverse and weak distribution of top-down weights.</p>
        <fig id="pcbi-1002393-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002393.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Summary of the results of the integrate-and-fire network simulations.</title>
            <p>We consider the four possible learning rules illustrated in <xref ref-type="fig" rid="pcbi-1002393-g004"><bold>Figure 4</bold></xref>. Here we show the proportion of all the computational simulations in a parameter search (<xref ref-type="sec" rid="s4"><bold>Methods</bold></xref>, <xref ref-type="table" rid="pcbi-1002393-t001"><bold>Table 1</bold></xref>) using integrate-and-fire units that converged (green), that reached extreme weights (red) or that did not converge (light blue). For comparison with <xref ref-type="fig" rid="pcbi-1002393-g006"><bold>Figure 6</bold></xref>, we included a category for simulations in which weights failed to achieve sufficient diversity (dark blue), although none of the current simulations fell into that category. The quantitative criteria for classifying the stimulations into these four categories as well as the network and parameters spanned are described in the text. The total number of simulations for each learning rule were 2298, 2304, 1148, and 1152. The only convergent simulations were seen for depression-biased rSTDP.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.g005" xlink:type="simple"/>
        </fig>
        <table-wrap id="pcbi-1002393-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002393.t001</object-id><label>Table 1</label><caption>
            <title>Parameters used in the integrate-and-fire simulations.</title>
          </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002393-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.t001" xlink:type="simple"/><table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <td align="left" colspan="1" rowspan="1">Parameter</td>
                <td align="left" colspan="1" rowspan="1">Description</td>
                <td align="left" colspan="1" rowspan="1">Values explored for <xref ref-type="fig" rid="pcbi-1002393-g005">Figure 5</xref> and <xref ref-type="fig" rid="pcbi-1002393-g007">7</xref></td>
                <td align="left" colspan="1" rowspan="1">Values explored for <xref ref-type="fig" rid="pcbi-1002393-g003">Figures 3</xref>, <xref ref-type="fig" rid="pcbi-1002393-g004">4</xref>, <xref ref-type="fig" rid="pcbi-1002393-g006">6</xref>, <xref ref-type="fig" rid="pcbi-1002393-g008">8</xref></td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">Learning type</td>
                <td align="left" colspan="1" rowspan="1">rSTDP, cSTDP</td>
                <td align="left" colspan="1" rowspan="1">rSTDP, cSTDP for <xref ref-type="fig" rid="pcbi-1002393-g003">Figs. 3</xref>, <xref ref-type="fig" rid="pcbi-1002393-g004">4</xref>, <xref ref-type="fig" rid="pcbi-1002393-g006">6</xref>. rSTDP for <xref ref-type="fig" rid="pcbi-1002393-g008">Fig. 8</xref>.</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <italic>A</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">Depression/Potentiation balance</td>
                <td align="left" colspan="1" rowspan="1">0.9, 1.2, 3</td>
                <td align="left" colspan="1" rowspan="1">0.9,1.2 for <xref ref-type="fig" rid="pcbi-1002393-g003">Figs. 3</xref>, <xref ref-type="fig" rid="pcbi-1002393-g004">4</xref>, <xref ref-type="fig" rid="pcbi-1002393-g006">6</xref>. 0.9 for <xref ref-type="fig" rid="pcbi-1002393-g008">Fig. 8</xref>.</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <italic>D</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">Synaptic transmission delay</td>
                <td align="left" colspan="1" rowspan="1">1, 15 ms</td>
                <td align="left" colspan="1" rowspan="1">15 ms</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <italic>τ<sub>STDP</sub></italic>
                </td>
                <td align="left" colspan="1" rowspan="1">STDP time constant</td>
                <td align="left" colspan="1" rowspan="1">5, 10, 20 ms</td>
                <td align="left" colspan="1" rowspan="1">20 ms</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <italic>S</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">Noise level</td>
                <td align="left" colspan="1" rowspan="1">1000, 2000 spikes/sec</td>
                <td align="left" colspan="1" rowspan="1">2000 spikes/sec</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <italic>τ<sub>syn</sub></italic>
                </td>
                <td align="left" colspan="1" rowspan="1">Synaptic time constant</td>
                <td align="left" colspan="1" rowspan="1">5, 15 ms</td>
                <td align="left" colspan="1" rowspan="1">15 ms</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <italic>σ<sub>input</sub></italic>
                </td>
                <td align="left" colspan="1" rowspan="1">Input variance</td>
                <td align="left" colspan="1" rowspan="1">100%,200%</td>
                <td align="left" colspan="1" rowspan="1">100%</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <italic>σ<sub>noise</sub></italic>
                </td>
                <td align="left" colspan="1" rowspan="1">Noise variance</td>
                <td align="left" colspan="1" rowspan="1">100%, 200%</td>
                <td align="left" colspan="1" rowspan="1">50%</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <italic>W <sub>min</sub>, W <sub>max</sub></italic>
                </td>
                <td align="left" colspan="1" rowspan="1">Minimum/maximum weight</td>
                <td align="left" colspan="1" rowspan="1">50 for <xref ref-type="fig" rid="pcbi-1002393-g003">Figs. 3</xref>,<xref ref-type="fig" rid="pcbi-1002393-g004">4</xref>,<xref ref-type="fig" rid="pcbi-1002393-g006">6</xref>; 20 in <xref ref-type="fig" rid="pcbi-1002393-g005">Fig. 5</xref></td>
                <td align="left" colspan="1" rowspan="1">50</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <italic>F</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">Target firing rate, for homeostatic scaling</td>
                <td align="left" colspan="1" rowspan="1">n/a</td>
                <td align="left" colspan="1" rowspan="1">n/a for <xref ref-type="fig" rid="pcbi-1002393-g003">Figs. 3</xref>–<xref ref-type="fig" rid="pcbi-1002393-g004">4</xref>, <xref ref-type="fig" rid="pcbi-1002393-g008">8</xref>; 20,80,120 spikes/sec for <xref ref-type="fig" rid="pcbi-1002393-g006">Fig. 6</xref></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e060" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">Relative strength of learning, for homeostatic scaling</td>
                <td align="left" colspan="1" rowspan="1">n/a</td>
                <td align="left" colspan="1" rowspan="1">n/a for <xref ref-type="fig" rid="pcbi-1002393-g003">Figs. 3</xref>–<xref ref-type="fig" rid="pcbi-1002393-g004">4</xref>, <xref ref-type="fig" rid="pcbi-1002393-g008">8</xref>; 0.1,1,10,100 for <xref ref-type="fig" rid="pcbi-1002393-g006">Fig. 6</xref></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <italic>ζ</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">Relative strength of learning, for concurrent plasticity</td>
                <td align="left" colspan="1" rowspan="1">n/a</td>
                <td align="left" colspan="1" rowspan="1">n/a for <xref ref-type="fig" rid="pcbi-1002393-g003">Figs. 3</xref>–<xref ref-type="fig" rid="pcbi-1002393-g004">4</xref>, <xref ref-type="fig" rid="pcbi-1002393-g008">8</xref>; −10,−1,−0.1, 0.1,1,10 for <xref ref-type="fig" rid="pcbi-1002393-g006">Fig. 6</xref></td>
              </tr>
            </tbody>
          </table></alternatives><table-wrap-foot>
            <fn id="nt101">
              <label/>
              <p>Only parameters that were varied are shown in here; for other parameters that were fixed across simulations, see <xref ref-type="sec" rid="s4">Methods</xref>.</p>
            </fn>
          </table-wrap-foot></table-wrap>
      </sec>
      <sec id="s2f">
        <title>Additional stability mechanisms</title>
        <p>We have to this point considered only networks with pure STDP-type plasticity at top-down connections, and we have shown that cSTDP is unstable in these networks. We now modify the basic plasticity rule from <bold>Equation 1</bold> in one of several ways – by considering concurrent changes in bottom-up weights, by adding homeostatic synaptic scaling <xref ref-type="bibr" rid="pcbi.1002393-Turrigiano1">[27]</xref> or by using a multiplicative STDP rule <xref ref-type="bibr" rid="pcbi.1002393-vanRossum1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Rubin1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Gutig1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Burkitt1">[31]</xref>. These last two mechanisms have been shown to stabilize inherently unstable Hebbian learning in feedforward networks <xref ref-type="bibr" rid="pcbi.1002393-Burkitt1">[31]</xref> and recurrent networks <xref ref-type="bibr" rid="pcbi.1002393-vanRossum1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Gilson1">[32]</xref>. However, this stabilization can cause a loss of synaptic competition <xref ref-type="bibr" rid="pcbi.1002393-Babadi1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-vanRossum1">[28]</xref>. We asked how our conclusions would be affected by adding these mechanisms. For each of these mechanisms, we modified the linear firing-rate model (<xref ref-type="sec" rid="s4"><bold>Methods</bold></xref>) and evaluated the systems numerically and using our integrate-and-fire model (<xref ref-type="sec" rid="s4"><bold>Methods</bold></xref>).</p>
        <p>In homeostatic synaptic scaling, all incoming synapses to a given neuron are modified simultaneously so as to help a neuron maintain a target firing rate. To model this homeostatic mechanism, we first applied the weight changes predicted by STDP, then multiplied all top-down connection weights and external inputs to a given neuron by a factor that depended on the difference between the current firing rate and the target firing rate (<xref ref-type="sec" rid="s4"><bold>Methods</bold></xref>). First, we tested homeostatic scaling with the depression-biased rSTDP learning rule in our linear model. We confirmed that, as in the case without synaptic scaling, these networks generally converged to stable, diverse and weak distributions of weights. Next, we considered <italic>potentiation</italic>-biased rSTDP learning rules, which were unstable in the non-scaling case. We found that with homeostatic synaptic scaling, although learning did sometimes acquire stable and weak connection weights, the distributions were never diverse: the standard deviations of the weights was always at least <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e061" xlink:type="simple"/></inline-formula> times smaller than those in the depression-biased cases. Finally, we looked at cSTDP learning rules, with either potentiation or depression biases. We found no combinations of parameters in which homeostatic synaptic scaling with cSTDP led to stable and diverse top-down weights. We confirmed each of these results using integrate-and-fire simulations: homeostatic synaptic scaling only allowed for convergent behavior with depression-biased STDP, and led to extreme weights or loss of diversity in every other case (<xref ref-type="table" rid="pcbi-1002393-t002"><bold>Table 2</bold></xref>, <xref ref-type="fig" rid="pcbi-1002393-g006"><bold>Figure 6</bold></xref>). In several depression-biased rSTDP simulations, the pull towards homeostasis was enough to shift the steady-state weight values high enough that a fraction of the feedback weights moved into the “extreme” range, causing more simulations to be labeled as “extreme weights” than in the case without homeostasis (<xref ref-type="fig" rid="pcbi-1002393-g005"><bold>Figure 5</bold></xref>); however, learning was not truly unstable in these cases.</p>
        <fig id="pcbi-1002393-g006" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002393.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>Summary of the results of the integrate-and-fire network simulations with additional stability mechanisms.</title>
            <p>We show the results of simulations with homeostatic scaling, multiplicative plasticity, or concurrent bottom-up and top-down plasticity (<xref ref-type="sec" rid="s4"><bold>Methods</bold></xref>, <xref ref-type="table" rid="pcbi-1002393-t001"><bold>Table 1</bold></xref>). The format is the same as in <xref ref-type="fig" rid="pcbi-1002393-g005"><bold>Figure 5</bold></xref>. The only convergent simulations were seen for depression-biased rSTDP, in the homeostatic scaling and concurrent plasticity cases. For all other learning conditions, homeostatic scaling simulations and concurrent plasticity reached extreme weights. Multiplicative plasticity always led to a lack of diversity.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.g006" xlink:type="simple"/>
        </fig>
        <table-wrap id="pcbi-1002393-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002393.t002</object-id><label>Table 2</label><caption>
            <title>Summary of results for modifications to the plasticity rules described in <bold>Equation 1</bold>.</title>
          </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002393-t002-2" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.t002" xlink:type="simple"/><table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <tbody>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <bold>Homeostatic Scaling</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <italic>Potentiation bias</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <italic>Depression bias</italic>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <italic>rSTDP</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">Stable but not diverse/EW</td>
                <td align="left" colspan="1" rowspan="1"><bold>Stable and diverse</bold>/<bold>C</bold>, WTS, EW</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <italic>cSTDP</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">Unstable/EW</td>
                <td align="left" colspan="1" rowspan="1">Unstable/EW</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <bold>Multiplicative Scaling</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <italic>Potentiation bias</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <italic>Depression bias</italic>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <italic>rSTDP</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">Unstable/WTS,</td>
                <td align="left" colspan="1" rowspan="1">Stable but not diverse/WTS</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <italic>cSTDP</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">Unstable/WTS</td>
                <td align="left" colspan="1" rowspan="1">Unstable/WTS</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <bold>Concurrent changes in Q</bold>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <italic>Potentiation bias</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <italic>Depression bias</italic>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <italic>rSTDP</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">Unstable/EW</td>
                <td align="left" colspan="1" rowspan="1"><bold>Stable</bold>/<bold>C</bold></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <italic>cSTDP</italic>
                </td>
                <td align="left" colspan="1" rowspan="1">Unstable/EW</td>
                <td align="left" colspan="1" rowspan="1">Unstable/EW</td>
              </tr>
            </tbody>
          </table></alternatives><table-wrap-foot>
            <fn id="nt102">
              <label/>
              <p>We considered three modifications: Homeostatic scaling, Multiplicative scaling and concurrent changes in the bottom-up and top-down weights (see text for a description of these modified rules). The first line in each entry of the table describes the results of the numeric implementation of the analytical work (based on <bold>Equation A22</bold>). The second line of each entry describes the results of the Integrate and Fire simulations, by listing all the different outcomes seen for a particular modification. <bold>C</bold> = “Converged”, WTS = “Weights too similar”, EW = “Extreme Weights” (see main text and <xref ref-type="sec" rid="s4">Methods</xref> for a quantitative definition of each of these categories).</p>
            </fn>
          </table-wrap-foot></table-wrap>
        <p>Another modification of STDP used in several studies is a multiplicative learning rule <xref ref-type="bibr" rid="pcbi.1002393-vanRossum1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Rubin1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Gutig1">[30]</xref> in which the change in a synaptic weight depends both on the current value of that weight and on amounts of pre- and post-synaptic activity. Here, we consider the particular implementation used in <xref ref-type="bibr" rid="pcbi.1002393-Gutig1">[30]</xref>, in which the strength of potentiation is linearly proportional to the distance between the current weight and a maximum weight, while the strength of depression is proportional to the distance of the current weight from zero (<bold>Eq. 6</bold><bold>–</bold><bold>7</bold><bold>, </bold><xref ref-type="sec" rid="s4"><bold>Methods</bold></xref>). To test the effects of multiplicative scaling in our linear model, we modified <bold>Equation A22</bold>, for several values of the maximum weight (<xref ref-type="sec" rid="s4"><bold>Methods</bold></xref>), and we tested learning rules with rSTDP or cSTDP and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e062" xlink:type="simple"/></inline-formula> greater than or less than 1. In every case, all of the synaptic weights eventually clustered tightly at single values either close to zero or close to the maximum weight: we lost all diversity in the synaptic weights, analogous to a loss of synaptic competition. We therefore conclude that multiplicative learning is insufficient to allow for the development of stable and diverse synaptic weights under either cSTDP or potentiation-biased learning. The results were similar in the integrate-and-fire simulations: every simulation was classified as “weights too similar” (<xref ref-type="table" rid="pcbi-1002393-t002"><bold>Table 2</bold></xref><bold>, </bold><xref ref-type="fig" rid="pcbi-1002393-g006"><bold>Figure 6</bold></xref>).</p>
        <p>We argue that the loss of diversity under multiplicative scaling is due to the quadratic nature of the multiplicative learning rule (<bold>Eq 6</bold><bold>–</bold><bold>7</bold><bold>, </bold><xref ref-type="sec" rid="s4"><bold>Methods</bold></xref>), in which <bold>W</bold> appears explicitly and multiplies <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e063" xlink:type="simple"/></inline-formula>, which implicitly depends on <bold>W</bold>. Quadratic learning rules will tend to be bi-stable, with fixed-point weights either very strong (near the maximum allowed value) or very weak (near zero). This binary weight pattern has indeed been observed in fully recurrent networks trained with a multiplicative cSTDP rule <xref ref-type="bibr" rid="pcbi.1002393-Gilson1">[32]</xref>; we interpret the results of our simulations as feedback weights clustering at the stronger of the two potential fixed points. Binary learning of this sort can create a new functional connectivity within a network; for instance, it can lead to the reduction of loops <xref ref-type="bibr" rid="pcbi.1002393-Kozloski1">[33]</xref>. However, it is not a satisfactory solution here because we require diversity in the top-down weights.</p>
        <p>The results presented thus far have assumed that the bottom-up weights remain unchanged and that there is plasticity only in the top-down weights. We evaluated whether the results would change when bottom-up connections were allowed to change concurrently with the top-down connections. We started with a set of randomly determined set of bottom-up weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e064" xlink:type="simple"/></inline-formula> (<xref ref-type="sec" rid="s4"><bold>Methods</bold></xref>), but we now allowed <bold>Q</bold> to change over time with a learning rule analogous to that in <bold>Equation 1</bold> (<bold>Eq. 8</bold>). We considered all combinations of cSTDP and rSTDP as well as depression versus potentiation bias for plasticity (16 possible combinations). For both the numerical implementation of the linear work and for the integrate-and-fire simulations, we found convergent learning only when top-down connections were trained with depression-biased rSTDP (<xref ref-type="table" rid="pcbi-1002393-t002"><bold>Table 2</bold></xref><bold>, </bold><xref ref-type="fig" rid="pcbi-1002393-g006"><bold>Figure 6</bold></xref>). Stability did not depend critically on the parameters of bottom-up learning; we found stable examples for bottom-up plasticity both with cSTDP and with rSTDP and with both depression and potentiation biases. We observed that the fraction of convergent simulations was increased relative to the case with no bottom-up plasticity (<xref ref-type="fig" rid="pcbi-1002393-g005"><bold>Figure 5</bold></xref>). This improvement and further variations on simultaneous bottom-up and top-down learning deserve further study in future work.</p>
      </sec>
      <sec id="s2g">
        <title>Top-down modulatory signals</title>
        <p>Until this point, we have assumed that external input to the system arrives in the form of initial activity in the lower layer. This is a good way of modeling the bottom-up flow of information that might be expected to dominate during sensory-driven activity (e.g. flashes of visual stimuli). However, it is clear that top-down signals modulate and transform inputs as they arrive (e.g. <xref ref-type="bibr" rid="pcbi.1002393-Bullier1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Engel1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Rao2">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Lee1">[36]</xref>). We asked whether and how such additional external input to the top layer impacts the stabilizing effects of rSTDP.</p>
        <p>We ran integrate-and-fire and numerical simulations using the same parameters from <xref ref-type="fig" rid="pcbi-1002393-g005"><bold>Figure 5</bold></xref>, with the addition of simultaneous external input to the top layer (<xref ref-type="sec" rid="s4"><bold>Methods</bold></xref><bold>, </bold><xref ref-type="fig" rid="pcbi-1002393-g007"><bold>Figure 7</bold></xref>). We considered different possible scenarios where the external input to the top units could be stronger (10 times), equal or weaker (1/10) than the external input to the bottom units. We found that depression-biased rSTDP was still able to generate sets of top-down weights which were stable, diverse, and weak (<xref ref-type="fig" rid="pcbi-1002393-g007"><bold>Figure 7a–c</bold></xref>). When the external input to the top neurons was very strong (arguably a biologically less realistic condition <xref ref-type="bibr" rid="pcbi.1002393-Bullier1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Callaway1">[7]</xref>), there were fewer simulations that converged, corresponding to a more restricted set of parameter values (<xref ref-type="fig" rid="pcbi-1002393-g007"><bold>Figure 7c</bold></xref>). We also observed several simulations which met our convergence criteria even for cSTDP or potentiation bias. However, neurons in these simulations exhibited significantly less activity than in the depression-biased rSTDP case (<xref ref-type="fig" rid="pcbi-1002393-g007"><bold>Figure 7d</bold></xref>). These cases constitute examples of a trivial fixed-point with low-activity levels where small amounts of potentiation and depression from higher and lower-layer external inputs cancel each other out.</p>
        <fig id="pcbi-1002393-g007" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002393.g007</object-id>
          <label>Figure 7</label>
          <caption>
            <title>Summary of the results of the integrate-and-fire simulations with external input to bottom-layer and higher-layer neurons.</title>
            <p><bold>a–c.</bold> In the simulations described here, external input was conveyed both to lower-level neurons and to higher-level neurons. The ratio of the external input strength to higher-level neurons to lower-level neurons was 0.1 in <bold>a</bold>, 1 in <bold>b</bold> and 10 in part <bold>c</bold>. The format and other parameters are the same as in <xref ref-type="fig" rid="pcbi-1002393-g005"><bold>Figure 5</bold></xref>. <bold>d.</bold> For those simulations that converged (green in parts <bold>a–c</bold>), the histogram shows the distribution of average activity levels. The gray bars denote simulations using rSTDP and the dark bars denote simulations using cSTDP. Results from all three strength ratios (<bold>a–c</bold>) are combined in this plot. Those few simulations which are convergent under cSTDP have very low average firing rates.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.g007" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s2h">
        <title>Computational significance of rSTDP learning</title>
        <p>We have focused thus far on the requirements to make learning at top-down connections stable, diverse, and weak. These properties are necessary regardless of the computational role of top-down connections in any particular brain area. We now take initial steps towards considering the computations performed by the top-down connections after training in the particular architecture studied here. For linear neurons, we look at the fixed points of the training algorithm. As shown above, when the feedforward weight matrix <bold>Q</bold> is invertible, the fixed point <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e065" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e066" xlink:type="simple"/></inline-formula>. This means that after training, top-down connections create a scaled reconstruction of the initial lower-level neuronal activity. We show in the <xref ref-type="supplementary-material" rid="pcbi.1002393.s001">Text S1</xref> that this principle applies even when <bold>Q</bold> is not invertible (so that perfect reconstruction is not always possible); in this case, the rSTDP learning rule minimizes the reconstruction error defined as the square of the difference between the input and its reconstruction.</p>
        <p>For networks of integrate-and-fire neurons, the picture is slightly more complicated. Frequently, the final <bold>W</bold> is well correlated with <bold>Q</bold><sup>−1</sup> (e.g. <xref ref-type="fig" rid="pcbi-1002393-g003"><bold>Figure 3c</bold></xref>). However, because of the non-linear nature of these neurons, the input strength is not always simply related to the amount of subsequent firing; in certain parameter regions, the input strength has more effect on the timing of neuronal firing than on the overall rate. We therefore focus on a regime where overall input is weak, so that only neurons with stronger inputs were able to fire. We did this by subtracting a constant value from the feedforward weight matrix <bold>Q</bold> used in previous sections (<xref ref-type="sec" rid="s4"><bold>Methods</bold></xref>). Under these conditions, we observed that after training with depression-biased rSTDP, the effect of the resulting top-down connections is to recreate an approximation to a scaled version of the original input (<xref ref-type="fig" rid="pcbi-1002393-g008"><bold>Figure 8</bold></xref>). In <xref ref-type="fig" rid="pcbi-1002393-g008"><bold>Figure 8a</bold></xref>, we show an example of how the network, after training, is capable of reconstructing a given activity pattern. The input to each lower-layer neuron (blue line) causes an early bout of activity in the lower-layer neurons (green line). Later in the stimulus presentation, the lower-level activity is due to feedback via the top-down connections. When the top-down weights have not yet been trained, this activity bears little resemblance to the initial activity (cyan line). However, after training is completed, the activity pattern constitutes a good reconstruction of the original input (red line). This effect is quantified in <xref ref-type="fig" rid="pcbi-1002393-g008"><bold>Figure 8b</bold></xref>, which shows an increase in the correlation between early time and late-time neuronal activity as a function of the number of training iterations.</p>
        <fig id="pcbi-1002393-g008" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002393.g008</object-id>
          <label>Figure 8</label>
          <caption>
            <title>Integrate-and-fire network trained with rSTDP learns to reconstruct its input.</title>
            <p><bold>a.</bold> Example of the network's ability to reconstruct its inputs after training using depression-biased rSTDP. By construction, the strength of external input during a single stimulus presentation to each neuron in the lower layer (input strength, blue line) is similar to the average spike rate of each lower-level neuron during the initial period from 0–50 ms (initial activity, green line). The cyan and red lines show the average spike rate of each lower-level neuron during the later period (late activity, 80–160 ms), when activity is due to top-down stimulation, using the top-down weights given early in training (after 10 iterations, cyan line) or after 51,000 iterations (red line). <bold>b.</bold> Average correlation coefficient between early time and late-time neuronal activity rates as a function of the number of training iterations. The average is computed over n = 100 distinct external input stimuli, and the error bars represent the standard deviation of the correlation coefficients for the 100 stimuli. The arrows indicate the iteration numbers illustrated in part <bold>a</bold>.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.g008" xlink:type="simple"/>
        </fig>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>We studied plasticity at top-down synapses in a model of two reciprocally connected neocortical areas, such as visual areas V1 and V2. The strength of top-down synapses evolved according to an activity-dependent STDP-type learning rule. We asked which plasticity rules lead to a distribution of top-down weights which met three criteria: stability, diversity, and weakness (lack of strong loops). We studied this biological model analytically and using computer simulations, and we concluded that top-down synapses could achieve these three criteria only when their strength was governed by a depression-biased temporally-reversed STDP rule, rSTDP. By contrast, both classical STDP (cSTDP) and potentiation-biased rSTDP led to pathological outcomes such as the uncontrolled growth of synaptic weights or run-away neuronal excitation.</p>
      <p>Under a temporally reversed STDP learning rule, post-synaptic spikes shortly followed by pre-synaptic spikes lead to potentiation and pre-synaptic spikes shortly followed by post-synaptic spikes lead to depression (<xref ref-type="fig" rid="pcbi-1002393-g001"><bold>Figure 1d</bold></xref>). Our theoretical prediction for this type of temporal dependency is consistent with recent empirical evidence documented in several experimental systems (for a review, see <xref ref-type="bibr" rid="pcbi.1002393-Caporale1">[37]</xref>). In slices of rat visual cortex, pre-synaptic activity followed by post-synaptic activity caused synaptic depression while post-synaptic activity followed by pre-synaptic activity induced potentiation in distal L2/3 to L5 and L5 to L5 synapses <xref ref-type="bibr" rid="pcbi.1002393-Sjostrom1">[38]</xref>. A similar effect was observed in rat barrel cortex, where pairing single EPSPs with subsequent postsynaptic bursts induced depression at L2/3 to L5 distal synapses, while potentiation was induced when the timing was reversed <xref ref-type="bibr" rid="pcbi.1002393-Letzkus1">[39]</xref>. Importantly, rSTDP has been observed only in <italic>distal synapses</italic> whereas cSTDP has been observed in synapses near the soma.</p>
      <p>The neuroanatomical location of top-down synapses suggests that they are ideal candidates to display this temporally reversed form of synaptic plasticity: anatomical work shows that top-down connections occur predominantly at distal synapses <xref ref-type="bibr" rid="pcbi.1002393-Felleman1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Salin1">[4]</xref>. For example, tracing studies show that the synapses from visual area V2 to visual area V1 end up forming synaptic connections in the distal dendrites of layer 1 in V1 <xref ref-type="bibr" rid="pcbi.1002393-Virga1">[6]</xref>.</p>
      <p>We considered depression and potentiation biased versions of STDP through the parameter <italic>α</italic> (<xref ref-type="fig" rid="pcbi-1002393-g001"><bold>Figure 1c–d</bold></xref>). We found that a potentiation bias can lead to runaway excitation. Several experiments in different systems have found biases towards depression <xref ref-type="bibr" rid="pcbi.1002393-Sjostrom1">[38]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Letzkus1">[39]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Debanne1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Feldman1">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Froemke1">[42]</xref> (see however <xref ref-type="bibr" rid="pcbi.1002393-Markram1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Bi1">[10]</xref>). A depression bias was also discussed and implemented in computational studies (e.g. <xref ref-type="bibr" rid="pcbi.1002393-Song1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Babadi1">[14]</xref>).</p>
      <p>Throughout most of our study, bottom-up connections were fixed to focus on the development of top-down connections because experimental studies suggest that bottom-up synapses may mature earlier than their top-down counterparts <xref ref-type="bibr" rid="pcbi.1002393-Coogan1">[43]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Burkhalter1">[44]</xref>. However, in <xref ref-type="fig" rid="pcbi-1002393-g006"><bold>Figure 6</bold></xref> we consider concurrent plasticity at bottom-up and top-down connections and show that this does not change our requirements for rSTDP at top-down synapses. We emphasize that we do <italic>not</italic> expect plasticity at bottom-up synapses to require rSTDP. Indeed, in the <bold><xref ref-type="supplementary-material" rid="pcbi.1002393.s001">Text S1</xref></bold> we show a case in which bottom-up synapses were only stable when trained with cSTDP, which is consistent with experimental evidence showing cSTDP at these synapses.</p>
      <p>Critical to the analysis presented here was our choice of three criteria for successful plasticity: weights need to be stable, diverse and weak. What support can be found experimentally for the idea that top-down weights in biological neural networks exhibit these three properties? With regards to stability, there is evidence that many dendritic and axonal structures in adult cortex are stable over long periods of time yet change dramatically upon large changes to the sensory environment (for a review, see <xref ref-type="bibr" rid="pcbi.1002393-Holtmaat1">[45]</xref>).</p>
      <p>The degree of diversity in top-down connections remains poorly understood at the experimental level. Some evidence implies that top-down synapses can connect neurons with different tuning preferences <xref ref-type="bibr" rid="pcbi.1002393-Rockland1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Stettler1">[46]</xref> (but see also <xref ref-type="bibr" rid="pcbi.1002393-Shmuel1">[47]</xref>), which might seem to be consistent with a generic, modulatory role for top-down signals, not requiring any particular diversity of synaptic weights. However, variations in synaptic weights occurring within the context of a broad non-selective connectivity pattern <xref ref-type="bibr" rid="pcbi.1002393-Lund1">[48]</xref> could provide a mechanism for specificity of these signals. Several computational models that aim to describe the functions of top-down connections implicitly or explicitly assume a high degree of specificity (see e.g. <xref ref-type="bibr" rid="pcbi.1002393-Rao2">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Lee1">[36]</xref>).</p>
      <p>We define weak distributions of top-down connections as those which keep the network from exhibiting any strong loops. It has long been recognized that strong loops must be avoided in cortical circuits <xref ref-type="bibr" rid="pcbi.1002393-Abbott1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Crick1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Douglas2">[25]</xref>, as these can amplify neuronal activity to pathological levels.</p>
      <p>Our results depended crucially on several features in our biological model. First among these was our focus on <italic>top-down</italic> synapses (in contrast to bottom-up synapses which may require cSTDP). The second important feature was the timing of neuronal activity. We modeled each stimulus presentation as a flow of activity affecting first lower area and then higher area neurons; this initial bottom-up direction of flow was crucial for determining the effects of our timing-based learning rules. Different timing patterns could affect our results, an effect which we briefly explored in <xref ref-type="fig" rid="pcbi-1002393-g007"><bold>Figure 7</bold></xref>. Third, the requirement for a depression bias in the learning rule arose because the cortical areas in our model were reciprocally connected, allowing for neuronal activity to reverberate up and down through the network. It is only in this context that activity can build up to pathological levels when strong loops exist.</p>
      <p>Several other features in our model did not prove to be crucial to our results. For instance, reciprocal connectivity between the two cortical areas was <italic>not</italic> necessary in order for top-down connections to require rSTDP. In the <bold><xref ref-type="supplementary-material" rid="pcbi.1002393.s001">Text S1</xref></bold>, we showed that rSTDP is still required in a case where higher-level neurons are activated independently of lower level neurons, even for neurons with non-linear activation functions. (The external higher-level input in this case could be the result of a separate path, as in thalamic input feeding into both V1 and V2, or it could be a simplified description of a complicated multi-synaptic feedforward path between the two areas.) Similarly, none of our results depend on reciprocal connections between any two individual neurons.</p>
      <p>Furthermore, rSTDP still led to adequate solutions in cases where modulatory external input to the top layer was added (<xref ref-type="fig" rid="pcbi-1002393-g007"><bold>Figure 7a–b</bold></xref>). When the external input to the top layer was 10 times stronger than the external input to the bottom layer a smaller fraction of tested parameter values led to adequate solutions (<xref ref-type="fig" rid="pcbi-1002393-g007"><bold>Figure 7c</bold></xref>). Biological data seems to suggest that external input to the top units would have a modulatory role consistent with the values in <bold>7a</bold> or even <bold>7b</bold> rather than <bold>7c</bold> <xref ref-type="bibr" rid="pcbi.1002393-Bullier1">[2]</xref>–<xref ref-type="bibr" rid="pcbi.1002393-Salin1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Callaway1">[7]</xref>. Yet, the results in 7c suggest that the stability of depression-biased rSTDP may show a stronger dependence on the particular parameter values when strong external input to the top layer is present compared to the situation when weaker external input to the top layer is present. We expect the effects of external stimuli to the top layer and bottom layer to differ given the asymmetry in our model imposed by changing <bold>W</bold> while maintaining <bold>Q</bold> fixed in <bold>Equation 1</bold>.</p>
      <p>Our results also did not appear to depend on the exact form of the STDP learning rule. We used two different forms in our analytical and integrate-and-fire work (see <xref ref-type="fig" rid="pcbi-1002393-g001"><bold>Figure 1</bold></xref>), including a variety of parameters in the integrate-and-fire case (<xref ref-type="table" rid="pcbi-1002393-t001"><bold>Table 1</bold></xref>), and additionally examined modifications including homeostatic scaling and multiplicative plasticity (<xref ref-type="fig" rid="pcbi-1002393-g006"><bold>Figure 6</bold></xref>). In every case, the requirement for rSTDP was unchanged. Yet, while we have considered several possible modifications, we cannot rule out the existence of additional biological mechanisms that could help stabilize the network. For example, recent elegant work has shown that temporal shifts in the STDP rule also lead to stable and diverse solutions <xref ref-type="bibr" rid="pcbi.1002393-Babadi1">[14]</xref>. It is interesting to point out that in the vicinity of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e067" xlink:type="simple"/></inline-formula> and on one side of the STDP learning rule, the net effect of the modifications introduced in <xref ref-type="bibr" rid="pcbi.1002393-Babadi1">[14]</xref> are similar to the ones we propose here.</p>
      <p>Our integrate-and-fire simulations allowed us to relax many of the biologically unrealistic simplifications made in our analytical work. The simulations allowed us to make a better approximation of the complex nonlinear firing dynamics of real biological neurons, including synaptic transmission delays and noise. The results of these simulations are concordant with the analytical predictions and were robust to changes in many of the parameters in the simulations (<xref ref-type="table" rid="pcbi-1002393-t001"><bold>Table 1</bold></xref>, <xref ref-type="fig" rid="pcbi-1002393-g005"><bold>Figure 5</bold></xref>) as well as different choices for the fixed bottom-up connection weights. Although simulations cannot exhaustively sample the entire parameter space, the parameter landscape described here in combination with the analytical work suggest the generality of the conclusions. Thus, we argue that our results may be relevant in biological circuits.</p>
      <p>Using analytical work and integrate-and-fire simulations, we explored the computational significance of the rSTDP learning rule by showing that the network could learn to reconstruct its inputs (<xref ref-type="fig" rid="pcbi-1002393-g008"><bold>Figure 8</bold></xref>). When the bottom-up weight matrix is orthogonal, the learning rule used here can lead to symmetric bottom-up and top-down weight matrices, which are known to show interesting computational properties (e.g. <xref ref-type="bibr" rid="pcbi.1002393-Rao2">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Hopfield1">[49]</xref>). A symmetric matrix also implies specificity in top-down modulatory signals as assumed in several computational models <xref ref-type="bibr" rid="pcbi.1002393-Callaway1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Engel1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Rao2">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1002393-Lee1">[36]</xref>. Input reconstruction is closely related to “predictive coding” models <xref ref-type="bibr" rid="pcbi.1002393-Rao2">[35]</xref>, in which top-down information flow carries a prediction about subsequent lower-level activity. Predictive coding models also include the calculation of an error signal, which is the <italic>difference</italic> between the predicted and the actual activity; implementation of this error signal would presumably require the inclusion of populations of inhibitory neurons. It is intriguing to note that our rSTDP model does calculate exactly the required top-down signal for predictive coding. Another possible function for reconstructive signals is in the area of error correction. Suppose that the feedforward connections <bold>Q</bold> have been selected (or trained) with a method such as Principal Component Analysis (PCA) or Independent Component Analysis (ICA), so that the activity of the higher-layer neurons is a projection of lower-level activity which retains functionally important information while discarding irrelevant or noisy components. Then the reconstruction, given by the feedback connections, may be a de-noised version of the original input (e.g. <xref ref-type="bibr" rid="pcbi.1002393-Hyvrinen1">[50]</xref>). This is also the principle used in de-noising autoencoders <xref ref-type="bibr" rid="pcbi.1002393-Vincent1">[51]</xref>.</p>
      <p>Ultimately, we hope that the hypothesis of reversed temporal dependence for plasticity at top-down synapses will be evaluated at the experimental level. The recent neurophysiological findings of temporal variations in STDP give experimental support for the existence of rSTDP at synapses which have distal dendritic locations, as top-down synapses do. Combining these findings with our computational results, we predict that a learning rule similar to rSTDP will be found to govern plasticity in top-down synapses in neocortex.</p>
    </sec>
    <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <sec id="s4a">
        <title>Numerical simulations of the analytical work</title>
        <p>We considered a two-layer linear model that we can study analytically (<bold><xref ref-type="supplementary-material" rid="pcbi.1002393.s001">Text S1</xref></bold>). We illustrated the dynamical weight changes in this linear model by numerically simulating a network with 20 lower-area neurons and 20 higher-area neurons (<xref ref-type="fig" rid="pcbi-1002393-g001"><bold>Figure 1a–b</bold></xref>). Each lower-area neuron was connected reciprocally to every higher-area neuron (but some of the weights could be zero). Although weights could be positive or negative, in the interest of simplicity and to reduce free parameters, we did not separate neurons into excitatory and inhibitory ones. The bottom-up weight matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e068" xlink:type="simple"/></inline-formula> was chosen manually at the onset and was fixed (i.e. <bold>Q</bold> did not evolve according to plasticity rules) unless noted otherwise (<xref ref-type="fig" rid="pcbi-1002393-g006"><bold>Figure 6</bold></xref>). Because we expected our final top-down weights <bold>W</bold> to be dependent on the inverse of <bold>Q</bold>, we wanted <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e069" xlink:type="simple"/></inline-formula> to be a well-conditioned random matrix. We generated it using the following algorithm: (i). Generate a uniformly distributed random matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e070" xlink:type="simple"/></inline-formula>, the same size as the desired <bold>Q.</bold> In some cases, for visualization purposes, smooth <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e071" xlink:type="simple"/></inline-formula> using a circular Gaussian filter of width 3 pixels. (ii). Calculate the polar decomposition of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e072" xlink:type="simple"/></inline-formula> by finding unitary matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e073" xlink:type="simple"/></inline-formula>and positive semi-definite matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e074" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e075" xlink:type="simple"/></inline-formula>. (iii). Calculate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e076" xlink:type="simple"/></inline-formula>. (iv). Normalize <bold>Q</bold> by dividing each column by its mean, then dividing the matrix by its maximum value and multiplying by 5. For the simulation in <xref ref-type="fig" rid="pcbi-1002393-g002"><bold>Figure 2</bold></xref>, we did include the smoothing step and we set <italic>ε</italic> = 0.1. The top-down weights <bold>W</bold> were initialized to random, normally distributed values. <bold>W</bold> evolved according to the plasticity rule in <bold>Equation A22</bold>.</p>
        <p>We stopped the simulations when either one of three conditions was reached: 1. If the matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e077" xlink:type="simple"/></inline-formula> had any eigenvalues greater than one, we stopped the simulation and classified the outcome as ‘Extreme Weights’. 2. If the standard deviation of the weights was less than 10% of the initial value, we stopped the simulation and classified the outcome as ‘Too similar’. 3. When the standard deviation of the weights had stopped changing and the average weight changes became small and constant in magnitude, we classified the simulation as ‘converged’. At each time point, we considered the previous 50 stimulus presentations, and computed the average values and slopes for the standard deviation of the top-down weights and the changes in weights. We then required that (i) the slopes for the standard deviation and the weight changes be less than 0.1% of their respective average values, and (ii) either the average change in weights was less than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e078" xlink:type="simple"/></inline-formula> or the slope of the change and weights was smaller than the initial change in weights.</p>
      </sec>
      <sec id="s4b">
        <title>Integrate-and-fire simulations</title>
        <p>The architecture was the same as that for the numerical simulations of the analytical work, described above, except that each layer of the network contained 100 neurons. The nature of the numerical simulations created some additional differences to the analytical work. Our use of fixed time-steps (1 ms) ensured that there was a maximum firing rate that neurons could ever attain; we also imposed upper and lower limits on the values that top-down weights could attain (<xref ref-type="table" rid="pcbi-1002393-t001"><bold>Table 1</bold></xref>). Pathological scenarios which would cause activity or weights to become infinite in the analytical model would, in simulations, cause the firing rates or synaptic weights to reach their maximum allowed values. These constraints were not expected to affect network behavior in the cases where weights achieve an unchanging and diverse distribution, which were those that concerned us here.</p>
        <p>The bottom-up weights <bold>Q</bold> were chosen as for the numerical simulation of the analytical work and were fixed unless otherwise noted. For the simulations in <xref ref-type="fig" rid="pcbi-1002393-g003"><bold>Figures 3</bold></xref> and <xref ref-type="fig" rid="pcbi-1002393-g004"><bold>4</bold></xref>, we included the smoothing step and set <italic>ε</italic> = 1. For the simulations in <xref ref-type="fig" rid="pcbi-1002393-g005"><bold>Figures 5</bold></xref> and <xref ref-type="fig" rid="pcbi-1002393-g006"><bold>6</bold></xref>, we did not include the smoothing step and we set <italic>ε</italic> = 0.1. We generated the initial top-down weights <bold>W</bold> as a uniformly distributed random matrix whose values ranged from −0.05 to 0.05.</p>
        <p>Each lower level neuron's membrane potential <italic>V<sub>i</sub></italic> evolved according to <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e079" xlink:type="simple"/><label>(2)</label></disp-formula>with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e080" xlink:type="simple"/></inline-formula> a membrane time constant of 10 ms, <italic>V<sub>rest</sub></italic> = −74 mV, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e081" xlink:type="simple"/></inline-formula> = 0 mV. Parameters whose values are not specified here were varied during the course of experiments; see <xref ref-type="table" rid="pcbi-1002393-t001"><bold>Table 1</bold></xref>. The neuron fired an action potential when its membrane potential reached <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e082" xlink:type="simple"/></inline-formula>mV; when this occurred, the membrane potential was reset to −60 mV. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e083" xlink:type="simple"/></inline-formula> was a conductance determined by the incoming spikes that have occurred since neuron <italic>i</italic> fired its last action potential according to:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e084" xlink:type="simple"/><label>(3)</label></disp-formula>where, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e085" xlink:type="simple"/></inline-formula> = 0.04. <italic>J<sub>i</sub>(t)</italic> is the rate of incoming external spikes due to the stimulus, and <italic>d</italic> is a synaptic transmission delay. <italic>S<sub>i</sub></italic> is the number of spikes corresponding to excitatory noise, and its value is chosen randomly at each time-point from a Gaussian distribution of mean <italic>S</italic> spikes/sec and standard deviation equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e086" xlink:type="simple"/></inline-formula> times the mean. <italic>w<sub>ji</sub></italic> is the synaptic top-down weight connecting neuron <italic>j</italic> to neuron <italic>i</italic>, and <italic>H<sub>j</sub>(t)</italic> is 1 if higher-level neuron <italic>j</italic> fired an action potential at time <italic>t</italic> and 0 otherwise. Finally, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e087" xlink:type="simple"/></inline-formula> is a synaptic time constant. Higher-level neurons evolve according to a similar rule, except that they do not receive external stimulus input, so we have<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e088" xlink:type="simple"/><label>(4)</label></disp-formula>where <italic>L<sub>i</sub>(t)</italic> and <italic>q<sub>ij</sub></italic> represent the lower-level action potentials and bottom-up weights, respectively. We simulated the above dynamics using time-steps of 1 ms.</p>
        <p>At the beginning of our simulations, we created a random cross-correlation matrix <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e089" xlink:type="simple"/></inline-formula>. Then, for each stimulus presentation, we randomly generated a vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e090" xlink:type="simple"/></inline-formula> describing the strength of external input to each lower-level neuron, chosen such that their average cross-correlation when calculated across many stimulus presentations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e091" xlink:type="simple"/></inline-formula> was equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e092" xlink:type="simple"/></inline-formula>. Within every stimulus presentation, the input strength <italic>J<sub>i</sub>(t)</italic> was chosen at each time-point from a Gaussian distribution with mean <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e093" xlink:type="simple"/></inline-formula> and standard deviation equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e094" xlink:type="simple"/></inline-formula> times the mean. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e095" xlink:type="simple"/></inline-formula> was 20,000 spikes/sec. <italic>J<sub>0</sub>(t)</italic> describes the time evolution of the input. It was the combination of an initial transient in the form of a Gaussian of height 1 centered at 30 ms with a width of 20 ms, followed by a sustained tonic input at 1/5 the maximum height that lasted for an additional 80 ms.</p>
        <p>The synaptic strengths were modified by every pair of spikes which occurred during a stimulus presentation, according to the rules for rSTDP and cSTDP. For cSTDP, the rule was<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e096" xlink:type="simple"/><label>(5)</label></disp-formula></p>
        <p>For rSTDP, the rule was<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e097" xlink:type="simple"/><label>(5′)</label></disp-formula></p>
        <p>We set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e098" xlink:type="simple"/></inline-formula> to 0.01, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e099" xlink:type="simple"/></inline-formula> to 160 ms, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e100" xlink:type="simple"/></inline-formula> to 80 ms.</p>
        <p>We stopped the simulations when either one of two conditions was reached:</p>
        <list list-type="order">
          <list-item>
            <p>If more than 50% of the top-down weights were within a distance of 0.1 of the maximum or minimum weights, we stopped the simulation and classified the outcome as ‘Extreme Weights’ (red bars in <xref ref-type="fig" rid="pcbi-1002393-g005"><bold>Figures 5</bold></xref> and <xref ref-type="fig" rid="pcbi-1002393-g006"><bold>6</bold></xref>).</p>
          </list-item>
          <list-item>
            <p>If the cross-correlation between the current top-down weights and the weights of 3,000 stimulus presentations prior was greater than 0.99 <italic>and</italic> if the change in standard deviation of the distribution of top-down weights over the previous 6,000 presentations was less than 0.1% of the current value, we declared that the weights had stabilized, and stopped the simulation. If at this point the standard deviation of the weights was less than 0.3, we classified the outcome as ‘Weights too similar’ (blue bars in <xref ref-type="fig" rid="pcbi-1002393-g005"><bold>Figures 5</bold></xref> and <xref ref-type="fig" rid="pcbi-1002393-g006"><bold>6</bold></xref>). If, on the other hand, the standard deviation of the weights was greater than 0.3, we classified the outcome as ‘Convergent’ (green bars in <xref ref-type="fig" rid="pcbi-1002393-g005"><bold>Figures 5</bold></xref> and <xref ref-type="fig" rid="pcbi-1002393-g006"><bold>6</bold></xref>).</p>
          </list-item>
        </list>
        <p>If neither stopping condition was reached after 625,000 stimulus presentations, we classified the simulation as ‘Did not converge’ (light blue bars in <xref ref-type="fig" rid="pcbi-1002393-g005"><bold>Figures 5</bold></xref> and <xref ref-type="fig" rid="pcbi-1002393-g006"><bold>6</bold></xref>). This last situation occurred in only a small fraction of the simulations.</p>
        <p>We considered the parameters described in <xref ref-type="table" rid="pcbi-1002393-t001"><bold>Table 1</bold></xref> and ran a set of 6,912 simulations, to describe the conditions and sets of parameters for which learning would or would not converge. We varied 8 parameters, with 2–3 possible values for each parameter, and we considered all possible combinations. The results are summarized in <xref ref-type="fig" rid="pcbi-1002393-g005"><bold>Figure 5</bold></xref>.</p>
      </sec>
      <sec id="s4c">
        <title>Homeostatic synaptic scaling</title>
        <p>For both the numerical implementation of the analytical work and the integrate-and-fire simulations, we made the same modifications to plasticity: after every 30 stimulus presentations, we calculated <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e101" xlink:type="simple"/></inline-formula>, the vector of firing rates for every lower-level neuron averaged during those 30 presentations. We then applied the change <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e102" xlink:type="simple"/></inline-formula> for a target firing rate <italic>F</italic> (the same for all neurons), where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e103" xlink:type="simple"/></inline-formula> denotes a relative learning rate. This multiplied all the top-down inputs to a lower-level neuron by a constant value that is close to 1 when the neuron's firing is close to the target rate, or far from 1 otherwise. We also multiplied the strength of all future external inputs by an amount <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e104" xlink:type="simple"/></inline-formula>. Taken together, these two changes were equivalent to changing the strength of all synaptic inputs to a lower-level neuron, both the bottom-up synapses carrying the external input and the top-down synapses carrying the feedback signal. We verified that this moved the firing rate towards the target value.</p>
      </sec>
      <sec id="s4d">
        <title>Multiplicative scaling</title>
        <p>We modified our learning rule to be dependent on the current weights, as follows. For the numerical implementation of the analytical model, we used<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e105" xlink:type="simple"/><label>(6)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e106" xlink:type="simple"/><label>(6′)</label></disp-formula>Here, the bullet represents entry-wise multiplication and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e107" xlink:type="simple"/></inline-formula> was varied among 3, 27, 30, and 60. For the integrate-and-fire simulations, we used<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e108" xlink:type="simple"/><label>(7)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e109" xlink:type="simple"/><label>(7′)</label></disp-formula>Here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e110" xlink:type="simple"/></inline-formula> was the standard value used in the integrate-and-fire simulations, 50.</p>
      </sec>
      <sec id="s4e">
        <title>Concurrent changes in Q</title>
        <p>After each stimulus presentation, we applied the changes to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e111" xlink:type="simple"/></inline-formula> as usual by performing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e112" xlink:type="simple"/></inline-formula>. Additionally, we changed <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e113" xlink:type="simple"/></inline-formula>. For the linear model, we used<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e114" xlink:type="simple"/><label>(8)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e115" xlink:type="simple"/><label>(8′)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e116" xlink:type="simple"/></inline-formula> represented the relative strength of learning for top-down connections and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e117" xlink:type="simple"/></inline-formula> was the potentiation/depression bias for bottom-up connections, which could be different from that for top-down connections. We modified the learning rule for our integrate-and-fire simulations in the analogous way.</p>
      </sec>
      <sec id="s4f">
        <title>Higher-layer external input</title>
        <p>For the numerical simulations in <xref ref-type="fig" rid="pcbi-1002393-g007"><bold>Figure 7</bold></xref>, we generated a cross-correlation matrix for higher-layer external inputs which was different from that for lower-layer external inputs. We modified our algorithm to include additional activity from the higher-layer inputs when calculating joint activity levels for learning by replacing <bold>Equation 4</bold> with the following equation, similar to <bold>Equation 3</bold>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e118" xlink:type="simple"/><label>(9)</label></disp-formula></p>
        <p>The external inputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e119" xlink:type="simple"/></inline-formula> were generated in the same manner as the lower-layer inputs given in in <bold>Equation 3</bold>. In different simulations, we varied the strength of external higher-layer input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002393.e120" xlink:type="simple"/></inline-formula> to be 0.1, 1, and 10 times the strength of the lower-layer input. We ran the simulations over the same 1,728 parameters used previously (but did not additionally run over the four combinations of different bottom-up and input cross-correlation matrixes).</p>
      </sec>
      <sec id="s4g">
        <title>Measurement of reconstruction error</title>
        <p>We modified the feedforward weight matrix <bold>Q</bold> by multiplying it by 2, subtracting the mean, and adding 0.5. Using this matrix, we trained the top-down weights as described previously. We then evaluated the ability of the top-down signals to provide a reconstruction of the original input at different time points after stimulus presentation and at different stages of training. We presented an early burst of external input to the network using a modified time-course that was zero after 50 ms. We measured the total number of spikes for each lower-level neuron during the first 50 ms, and separately during the time from 80–160 ms. We subtracted the later-time activity from that calculated in a network where the top-down weights were zero. Typically, in the absence of top-down weights, there was no later-time activity. We calculated the Pearson correlation coefficient between the vector of later-time mean activity levels with the vector of early-time activity level. We then repeated this procedure for <italic>n</italic> = 100 distinct external stimulus inputs, and averaged the correlation values. The results are shown in <xref ref-type="fig" rid="pcbi-1002393-g008"><bold>Figure 8</bold></xref>. The correlation coefficients reached their maximum value and stabilized after stimulus presentations, so we used the weights at this time to generate the final activity (red line) in <xref ref-type="fig" rid="pcbi-1002393-g008"><bold>Figure 8a</bold></xref>.</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pcbi.1002393.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002393.s001" xlink:type="simple">
        <label>Text S1</label>
        <caption>
          <p>Analytical formulation and analytical solutions.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>We thank Yoram Burak, Eran Mukamel, Joel Leibo, Jed Singer and Ulf Knoblich for comments on the manuscript.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002393-Felleman1">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Felleman</surname><given-names>DJ</given-names></name><name name-style="western"><surname>Van Essen</surname><given-names>DC</given-names></name></person-group>             <year>1991</year>             <article-title>Distributed hierarchical processing in the primate cerebral cortex.</article-title>             <source>Cereb Cortex</source>             <volume>1</volume>             <fpage>1</fpage>             <lpage>47</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Bullier1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bullier</surname><given-names>J</given-names></name></person-group>             <year>2001</year>             <article-title>Integrated model of visual processing.</article-title>             <source>Brain Res Brain Res Rev</source>             <volume>36</volume>             <fpage>96</fpage>             <lpage>107</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Douglas1">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Douglas</surname><given-names>RJ</given-names></name><name name-style="western"><surname>Martin</surname><given-names>KA</given-names></name></person-group>             <year>2004</year>             <article-title>Neuronal circuits of the neocortex.</article-title>             <source>Annu Rev Neurosci</source>             <volume>27</volume>             <fpage>419</fpage>             <lpage>451</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Salin1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Salin</surname><given-names>PA</given-names></name><name name-style="western"><surname>Bullier</surname><given-names>J</given-names></name></person-group>             <year>1995</year>             <article-title>Corticocortical connections in the visual system: structure and function.</article-title>             <source>Physiol Rev</source>             <volume>75</volume>             <fpage>107</fpage>             <lpage>154</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Rockland1">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rockland</surname><given-names>KS</given-names></name><name name-style="western"><surname>Pandya</surname><given-names>DN</given-names></name></person-group>             <year>1979</year>             <article-title>Laminar origins and terminations of cortical connections of the occipital lobe in the rhesus monkey.</article-title>             <source>Brain Res</source>             <volume>179</volume>             <fpage>3</fpage>             <lpage>20</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Virga1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Virga</surname><given-names>A</given-names></name><name name-style="western"><surname>Rockland</surname><given-names>KS</given-names></name></person-group>             <year>1989</year>             <article-title>Terminal Arbors of Individual “Feedback” Axons Projecting from Area V2 to V1 in the Macaque Monkey: A Study Using Immunohistochemistry of Anterogradely Transported Phaseolus vulgaris-leucoagglutinin.</article-title>             <source>J Comp Neurol</source>             <volume>285</volume>             <fpage>54</fpage>             <lpage>72</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Callaway1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Callaway</surname><given-names>EM</given-names></name></person-group>             <year>2004</year>             <article-title>Feedforward, feedback and inhibitory connections in primate visual cortex.</article-title>             <source>Neural Netw</source>             <volume>17</volume>             <fpage>625</fpage>             <lpage>632</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Levy1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Levy</surname><given-names>WB</given-names></name><name name-style="western"><surname>Steward</surname><given-names>O</given-names></name></person-group>             <year>1983</year>             <article-title>Temporal contiguity requirements for long-term associative potentiation/depression in the hippocampus.</article-title>             <source>Neuroscience</source>             <volume>8</volume>             <fpage>791</fpage>             <lpage>797</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Markram1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Markram</surname><given-names>H</given-names></name><name name-style="western"><surname>Lubke</surname><given-names>J</given-names></name><name name-style="western"><surname>Frotscher</surname><given-names>M</given-names></name><name name-style="western"><surname>Sakmann</surname><given-names>B</given-names></name></person-group>             <year>1997</year>             <article-title>Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs.</article-title>             <source>Science</source>             <volume>275</volume>             <fpage>213</fpage>             <lpage>215</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Bi1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bi</surname><given-names>GQ</given-names></name><name name-style="western"><surname>Poo</surname><given-names>MM</given-names></name></person-group>             <year>1998</year>             <article-title>Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type.</article-title>             <source>J Neurosci</source>             <volume>18</volume>             <fpage>10464</fpage>             <lpage>10472</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Dan1">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name><name name-style="western"><surname>Poo</surname><given-names>MM</given-names></name></person-group>             <year>2006</year>             <article-title>Spike timing-dependent plasticity: from synapse to perception.</article-title>             <source>Physiol Rev</source>             <volume>86</volume>             <fpage>1033</fpage>             <lpage>1048</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Rao1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rao</surname><given-names>RP</given-names></name><name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group>             <year>2001</year>             <article-title>Spike-timing-dependent Hebbian plasticity as temporal difference learning.</article-title>             <source>Neural Comput</source>             <volume>13</volume>             <fpage>2221</fpage>             <lpage>2237</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Song1">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>S</given-names></name><name name-style="western"><surname>Miller</surname><given-names>KD</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name></person-group>             <year>2000</year>             <article-title>Competitive Hebbian learning through spike-timing-dependent synaptic plasticity.</article-title>             <source>Nature</source>             <volume>3</volume>             <fpage>919</fpage>             <lpage>926</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Babadi1">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Babadi</surname><given-names>B</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name></person-group>             <year>2010</year>             <article-title>Intrinsic stability of temporally shifted spike-timing dependent plasticity.</article-title>             <source>PLoS Comput Biol</source>             <volume>6</volume>             <fpage>e1000961</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Abbott1">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Abbott</surname><given-names>L</given-names></name><name name-style="western"><surname>Nelson</surname><given-names>S</given-names></name></person-group>             <year>2000</year>             <article-title>Synaptic plasticity: taming the beast.</article-title>             <source>Nat Neurosci</source>             <volume>3</volume>             <fpage>1178</fpage>             <lpage>1183</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Kepecs1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kepecs</surname><given-names>A</given-names></name><name name-style="western"><surname>van Rossum</surname><given-names>MCW</given-names></name><name name-style="western"><surname>Song</surname><given-names>S</given-names></name><name name-style="western"><surname>Tegner</surname><given-names>J</given-names></name></person-group>             <year>2002</year>             <article-title>Spike-timing-dependent plasticity: common themes and divergent vistas.</article-title>             <source>Biol Cybern</source>             <volume>87</volume>             <fpage>446</fpage>             <lpage>458</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Worgotter1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Worgotter</surname><given-names>F</given-names></name><name name-style="western"><surname>Porr</surname><given-names>B</given-names></name></person-group>             <year>2005</year>             <article-title>Temporal sequence learning, prediction, and control: a review of different models and their relation to biological mechanisms.</article-title>             <source>Neural Comput</source>             <volume>17</volume>             <fpage>245</fpage>             <lpage>319</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Maunsell1">
        <label>18</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Maunsell</surname><given-names>J</given-names></name></person-group>             <year>1987</year>             <article-title>Physiological Evidence for Two Visual Subsystems.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Vaina</surname><given-names>L</given-names></name></person-group>             <source>Matters of Intelligence</source>             <publisher-loc>Dordrecht, Holland</publisher-loc>             <publisher-name>Reidel Press</publisher-name>             <fpage>59</fpage>             <lpage>87</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Schmolesky1">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Schmolesky</surname><given-names>M</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y</given-names></name><name name-style="western"><surname>Hanes</surname><given-names>D</given-names></name><name name-style="western"><surname>Thompson</surname><given-names>K</given-names></name><name name-style="western"><surname>Leutgeb</surname><given-names>S</given-names></name><etal/></person-group>             <year>1998</year>             <article-title>Signal timing across the macaque visual system.</article-title>             <source>J Neurophysiol</source>             <volume>79</volume>             <fpage>3272</fpage>             <lpage>3278</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Marsalek1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Marsalek</surname><given-names>P</given-names></name><name name-style="western"><surname>Koch</surname><given-names>C</given-names></name><name name-style="western"><surname>Maunsell</surname><given-names>J</given-names></name></person-group>             <year>1997</year>             <article-title>On the relationship between synaptic input and spike output jiter in individual neurons.</article-title>             <source>Proc Natl Acad Sci USA</source>             <volume>94</volume>             <fpage>735</fpage>             <lpage>740</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Maunsell2">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Maunsell</surname><given-names>JHR</given-names></name><name name-style="western"><surname>Gibson</surname><given-names>JR</given-names></name></person-group>             <year>1992</year>             <article-title>Visual Response Latencies in Striate Cortex of the Macaque Monkey.</article-title>             <source>J Neurophysiol</source>             <volume>68</volume>             <fpage>13</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Fusi1">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name><name name-style="western"><surname>Drew</surname><given-names>PJ</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name></person-group>             <year>2005</year>             <article-title>Cascade models of synaptically stored memories.</article-title>             <source>Neuron</source>             <volume>45</volume>             <fpage>599</fpage>             <lpage>611</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Satel1">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Satel</surname><given-names>J</given-names></name><name name-style="western"><surname>Trappenberg</surname><given-names>T</given-names></name><name name-style="western"><surname>Fine</surname><given-names>A</given-names></name></person-group>             <year>2009</year>             <article-title>Are binary synapses superior to graded weight representations in stochastic attractor networks?</article-title>             <source>Cogn Neurodyn</source>             <volume>3</volume>             <fpage>243</fpage>             <lpage>250</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Crick1">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Crick</surname><given-names>F</given-names></name><name name-style="western"><surname>Koch</surname><given-names>C</given-names></name></person-group>             <year>1998</year>             <article-title>Constraints on cortical thalamic projections: the no-strong-loops hypothesis.</article-title>             <source>Nature</source>             <volume>391</volume>             <fpage>245</fpage>             <lpage>250</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Douglas2">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Douglas</surname><given-names>RJ</given-names></name><name name-style="western"><surname>Koch</surname><given-names>C</given-names></name><name name-style="western"><surname>Mahowald</surname><given-names>M</given-names></name><name name-style="western"><surname>Martin</surname><given-names>KA</given-names></name><name name-style="western"><surname>Suarez</surname><given-names>HH</given-names></name></person-group>             <year>1995</year>             <article-title>Recurrent excitation in neocortical circuits.</article-title>             <source>Science</source>             <volume>269</volume>             <fpage>981</fpage>             <lpage>985</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Bacciotti1">
        <label>26</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bacciotti</surname><given-names>A</given-names></name><name name-style="western"><surname>Rosier</surname><given-names>L</given-names></name></person-group>             <year>2005</year>             <source>Liapunov Functions and Stability in Control Theory</source>             <publisher-loc>London</publisher-loc>             <publisher-name>Springer</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Turrigiano1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Turrigiano</surname><given-names>G</given-names></name><name name-style="western"><surname>Leslie</surname><given-names>K</given-names></name><name name-style="western"><surname>Desai</surname><given-names>N</given-names></name><name name-style="western"><surname>Rutherford</surname><given-names>L</given-names></name><name name-style="western"><surname>Nelson</surname><given-names>S</given-names></name></person-group>             <year>1998</year>             <article-title>Activity-dependent scaling of quantal amplitude in neocortical neurons.</article-title>             <source>Nature</source>             <volume>391</volume>             <fpage>892</fpage>             <lpage>896</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-vanRossum1">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>van Rossum</surname><given-names>M</given-names></name><name name-style="western"><surname>Bi</surname><given-names>CQ</given-names></name><name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name></person-group>             <year>1999</year>             <article-title>Stable Hebbian learning from spike timing-dependent plasticity.</article-title>             <source>J Neurosci</source>             <volume>20</volume>             <fpage>8812</fpage>             <lpage>8821</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Rubin1">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rubin</surname><given-names>J</given-names></name><name name-style="western"><surname>Lee</surname><given-names>DD</given-names></name><name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group>             <year>2001</year>             <article-title>Equilibrium properties of temporally asymmetric Hebbian plasticity.</article-title>             <source>Phys Rev Lett</source>             <volume>86</volume>             <fpage>364</fpage>             <lpage>367</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Gutig1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gutig</surname><given-names>R</given-names></name><name name-style="western"><surname>Aharonov</surname><given-names>R</given-names></name><name name-style="western"><surname>Rotter</surname><given-names>S</given-names></name><name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group>             <year>2003</year>             <article-title>Learning input correlations through nonlinear temporally asymmetric Hebbian plasticity.</article-title>             <source>J Neurosci</source>             <volume>23</volume>             <fpage>3697</fpage>             <lpage>3714</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Burkitt1">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Burkitt</surname><given-names>A</given-names></name><name name-style="western"><surname>Meffin</surname><given-names>H</given-names></name><name name-style="western"><surname>Grayden</surname><given-names>D</given-names></name></person-group>             <year>2004</year>             <article-title>Spike-timing-dependent plasticity: the relationship to rate-based learning for models with weight dynamics determined by a stable fixed point.</article-title>             <source>Neural Comput</source>             <volume>16</volume>             <fpage>885</fpage>             <lpage>894</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Gilson1">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gilson</surname><given-names>M</given-names></name><name name-style="western"><surname>Burkitt</surname><given-names>AN</given-names></name><name name-style="western"><surname>van Hemmen</surname><given-names>L</given-names></name></person-group>             <year>2010</year>             <article-title>STDP in recurrent neuronal networks.</article-title>             <source>Front Comput Neurosci</source>             <volume>4</volume>             <fpage>1</fpage>             <lpage>15</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Kozloski1">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kozloski</surname><given-names>J</given-names></name><name name-style="western"><surname>Cecchi</surname><given-names>GA</given-names></name></person-group>             <year>2010</year>             <article-title>A theory of loop formation and elimination by spike timing-dependent plasticity.</article-title>             <source>Front Neural Circuits</source>             <volume>4</volume>             <fpage>1</fpage>             <lpage>11</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Engel1">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Engel</surname><given-names>AK</given-names></name><name name-style="western"><surname>Fries</surname><given-names>P</given-names></name><name name-style="western"><surname>Singer</surname><given-names>W</given-names></name></person-group>             <year>2001</year>             <article-title>Dynamic predictions: oscillations and synchrony in top-down processing.</article-title>             <source>Nat Rev Neurosci</source>             <volume>2</volume>             <fpage>704</fpage>             <lpage>716</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Rao2">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rao</surname><given-names>RP</given-names></name><name name-style="western"><surname>Ballard</surname><given-names>DH</given-names></name></person-group>             <year>1999</year>             <article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects.</article-title>             <source>Nat Neurosci</source>             <volume>2</volume>             <fpage>79</fpage>             <lpage>87</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Lee1">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>TS</given-names></name><name name-style="western"><surname>Mumford</surname><given-names>D</given-names></name></person-group>             <year>2003</year>             <article-title>Hierarchical Bayesian inference in the visual cortex.</article-title>             <source>J Opt Soc Am A Opt Image Sci Vis</source>             <volume>20</volume>             <fpage>1434</fpage>             <lpage>1448</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Caporale1">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Caporale</surname><given-names>N</given-names></name><name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name></person-group>             <year>2008</year>             <article-title>Spike timing-dependent plasticity: a Hebbian learning rule.</article-title>             <source>Annu Rev Neurosci</source>             <volume>31</volume>             <fpage>25</fpage>             <lpage>46</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Sjostrom1">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sjostrom</surname><given-names>PJ</given-names></name><name name-style="western"><surname>Hausser</surname><given-names>M</given-names></name></person-group>             <year>2006</year>             <article-title>A cooperative switch determines the sign of synaptic plasticity in distal dendrites of neocortical pyramidal neurons.</article-title>             <source>Neuron</source>             <volume>51</volume>             <fpage>227</fpage>             <lpage>238</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Letzkus1">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Letzkus</surname><given-names>JJ</given-names></name><name name-style="western"><surname>Kampa</surname><given-names>BM</given-names></name><name name-style="western"><surname>Stuart</surname><given-names>GJ</given-names></name></person-group>             <year>2006</year>             <article-title>Learning rules for spike timing-dependent plasticity depend on dendritic synapse location.</article-title>             <source>J Neurosci</source>             <volume>26</volume>             <fpage>10420</fpage>             <lpage>10429</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Debanne1">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Debanne</surname><given-names>D</given-names></name><name name-style="western"><surname>Gahwiler</surname><given-names>BH</given-names></name><name name-style="western"><surname>Thompson</surname><given-names>SM</given-names></name></person-group>             <year>1998</year>             <article-title>Long-term synaptic plasticity between pairs of individual CA3 pyramidal cells in rat hippocampal slice cultures.</article-title>             <source>J Physiol</source>             <volume>507</volume>             <fpage>237</fpage>             <lpage>247</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Feldman1">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Feldman</surname><given-names>DE</given-names></name></person-group>             <year>2000</year>             <article-title>Timing-based LTP and LTD at vertical inputs to layer II/III pyramidal cells in rat barrel cortex.</article-title>             <source>Neuron</source>             <volume>27</volume>             <fpage>45</fpage>             <lpage>56</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Froemke1">
        <label>42</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Froemke</surname><given-names>RC</given-names></name><name name-style="western"><surname>Poo</surname><given-names>MM</given-names></name><name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name></person-group>             <year>2005</year>             <article-title>Spike-timing-dependent synaptic plasticity depends on dendritic location.</article-title>             <source>Nature</source>             <volume>434</volume>             <fpage>221</fpage>             <lpage>225</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Coogan1">
        <label>43</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Coogan</surname><given-names>T</given-names></name><name name-style="western"><surname>Burkhalter</surname><given-names>A</given-names></name></person-group>             <year>1993</year>             <article-title>Hierarchical Organization of Areas in Rat Visual Cortex.</article-title>             <source>J Neurosci</source>             <volume>13</volume>             <fpage>3749</fpage>             <lpage>3772</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Burkhalter1">
        <label>44</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Burkhalter</surname><given-names>A</given-names></name></person-group>             <year>1993</year>             <article-title>Development of forward and feedback connections between areas V1 and V2 of human visual cortex.</article-title>             <source>Cereb Cortex</source>             <volume>3</volume>             <fpage>476</fpage>             <lpage>487</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Holtmaat1">
        <label>45</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Holtmaat</surname><given-names>A</given-names></name><name name-style="western"><surname>Svoboda</surname><given-names>K</given-names></name></person-group>             <year>2009</year>             <article-title>Experience-dependent structural synaptic plasticity in the mammalian brain.</article-title>             <source>Nat Rev Neurosci</source>             <volume>10</volume>             <fpage>647</fpage>             <lpage>658</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Stettler1">
        <label>46</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Stettler</surname><given-names>DD</given-names></name><name name-style="western"><surname>Das</surname><given-names>A</given-names></name><name name-style="western"><surname>Bennett</surname><given-names>J</given-names></name><name name-style="western"><surname>Gilbert</surname><given-names>CD</given-names></name></person-group>             <year>2002</year>             <article-title>Lateral connectivity and contextual interactions in macaque primary visual cortex.</article-title>             <source>Neuron</source>             <volume>36</volume>             <fpage>739</fpage>             <lpage>750</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Shmuel1">
        <label>47</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shmuel</surname><given-names>A</given-names></name><name name-style="western"><surname>Korman</surname><given-names>M</given-names></name><name name-style="western"><surname>Sterkin</surname><given-names>A</given-names></name><name name-style="western"><surname>Harel</surname><given-names>M</given-names></name><name name-style="western"><surname>Ullman</surname><given-names>S</given-names></name><etal/></person-group>             <year>2005</year>             <article-title>Retinotopic axis specificity and selective clustering of feedback projections from V2 to V1 in the owl monkey.</article-title>             <source>J Neurosci</source>             <volume>25</volume>             <fpage>2117</fpage>             <lpage>31</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Lund1">
        <label>48</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Lund</surname><given-names>J</given-names></name></person-group>             <year>2002</year>             <article-title>Specificity and non-specificity of synaptic connections within mammalian visual cortex.</article-title>             <source>J Neurocytol</source>             <volume>31</volume>             <fpage>203</fpage>             <lpage>209</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Hopfield1">
        <label>49</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group>             <year>1982</year>             <article-title>Neural networks and physical systems with emergent collective computational abilities.</article-title>             <source>Proc Natl Acad Sci USA</source>             <volume>79</volume>             <fpage>2554</fpage>             <lpage>2558</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Hyvrinen1">
        <label>50</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hyvärinen</surname><given-names>A</given-names></name><name name-style="western"><surname>Oja</surname><given-names>E</given-names></name><name name-style="western"><surname>Hoyer</surname><given-names>P</given-names></name><name name-style="western"><surname>Hurri</surname><given-names>J</given-names></name></person-group>             <year>1998</year>             <article-title>Image Feature Extraction by Sparse Coding and Independent Component Analysis.</article-title>             <fpage>1268</fpage>             <lpage>1273</lpage>             <comment>In: Proceedings. Fourteenth International Conference on Pattern Recognition; 16–20 Aug 1998; Brisbane, Queensland, Australia</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002393-Vincent1">
        <label>51</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Vincent</surname><given-names>P</given-names></name><name name-style="western"><surname>Larochelle</surname><given-names>H</given-names></name><name name-style="western"><surname>Bengio</surname><given-names>Y</given-names></name><name name-style="western"><surname>Manzagol</surname><given-names>P</given-names></name></person-group>             <year>2008</year>             <article-title>Extracting and composing robust features with denoising autoencoders.</article-title>             <comment>In: Proceedings of the 25th international conference on Machine learning; New York, New York, United States</comment>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>