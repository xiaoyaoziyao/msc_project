<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pone.0211735</article-id>
<article-id pub-id-type="publisher-id">PONE-D-18-27983</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Face</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Face</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject><subj-group><subject>Happiness</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject><subj-group><subject>Happiness</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Bioassays and physiological analysis</subject><subj-group><subject>Electrophysiological techniques</subject><subj-group><subject>Muscle electrophysiology</subject><subj-group><subject>Electromyography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Management engineering</subject><subj-group><subject>Decision analysis</subject><subj-group><subject>Decision trees</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Decision analysis</subject><subj-group><subject>Decision trees</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Video recording</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Using computer-vision and machine learning to automate facial coding of positive and negative affect intensity</article-title>
<alt-title alt-title-type="running-head">Decoding facial expressions of emotion</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6654-9010</contrib-id>
<name name-style="western">
<surname>Haines</surname>
<given-names>Nathaniel</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5888-2769</contrib-id>
<name name-style="western">
<surname>Southward</surname>
<given-names>Matthew W.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-3778-9346</contrib-id>
<name name-style="western">
<surname>Cheavens</surname>
<given-names>Jennifer S.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Beauchaine</surname>
<given-names>Theodore</given-names>
</name>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Ahn</surname>
<given-names>Woo-Young</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Psychology, The Ohio State University, Columbus, Ohio, United States of America</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Department of Psychology, Seoul National University, Seoul, Korea</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Hinojosa</surname>
<given-names>José A.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Universidad Complutense Madrid, SPAIN</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">wahn55@snu.ac.kr</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>5</day>
<month>2</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="collection">
<year>2019</year>
</pub-date>
<volume>14</volume>
<issue>2</issue>
<elocation-id>e0211735</elocation-id>
<history>
<date date-type="received">
<day>25</day>
<month>9</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>18</day>
<month>1</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Haines et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0211735"/>
<abstract>
<p>Facial expressions are fundamental to interpersonal communication, including social interaction, and allow people of different ages, cultures, and languages to quickly and reliably convey emotional information. Historically, facial expression research has followed from discrete emotion theories, which posit a limited number of distinct affective states that are represented with specific patterns of facial action. Much less work has focused on dimensional features of emotion, particularly positive and negative affect intensity. This is likely, in part, because achieving inter-rater reliability for facial action and affect intensity ratings is painstaking and labor-intensive. We use computer-vision and machine learning (CVML) to identify patterns of facial actions in 4,648 video recordings of 125 human participants, which show strong correspondences to positive and negative affect intensity ratings obtained from highly trained coders. Our results show that CVML can both (1) determine the importance of different facial actions that human coders use to derive positive and negative affective ratings when combined with interpretable machine learning methods, and (2) efficiently automate positive and negative affect intensity coding on large facial expression databases. Further, we show that CVML can be applied to individual human judges to infer which facial actions they use to generate perceptual emotion ratings from facial expressions.</p>
</abstract>
<funding-group>
<funding-statement>The authors received no specific funding for this work.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="2"/>
<page-count count="23"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Raw video data cannot be shared publicly due to our participants' informed consent process. However, all post-processed Action Unit data, human coder ratings, and R scripts are uploaded to the GitHub repository, which allow full replication of all results and figures (<ext-link ext-link-type="uri" xlink:href="https://github.com/CCS-Lab/Haines_CVML_2018" xlink:type="simple">https://github.com/CCS-Lab/Haines_CVML_2018</ext-link>). Importantly, the uploaded data and R scripts are sufficient to replicate all analyses and figures in the manuscript, therefore the raw data are not necessary for replication. The Ohio State IRB can be contacted on issues regarding raw video data by contacting Michael Donovan, Buck-IRB Liaison, at 614-292-6950 or <email xlink:type="simple">donovan.6@osu.edu</email> and referencing protocol #2011B0071.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The ability to effectively communicate emotion is essential for adaptive human function. Of all the ways that we communicate emotion, facial expressions are among the most flexible—their universality allows us to rapidly convey information to people of different ages, cultures, and languages. Further, facial expressions signal complex action tendencies including threat and cooperative intent [<xref ref-type="bibr" rid="pone.0211735.ref001">1</xref>–<xref ref-type="bibr" rid="pone.0211735.ref003">3</xref>]. Unsurprisingly, the ability to produce and recognize facial expressions of emotion is of interest to researchers throughout the social and behavioral sciences.</p>
<p>Facial expressions can be interpreted using either message- or sign-based approaches [<xref ref-type="bibr" rid="pone.0211735.ref004">4</xref>]. Message-based approaches describe the meaning conveyed by a facial expression (e.g., happiness), whereas sign-based approaches describe observable facial actions that embody/comprise messages (e.g., cheek raising may indicate happiness). Although message-based approaches are used effectively by psychologists to measure facial expression messages (e.g., happiness), they do not describe facial behavior comprehensively. Instead, they rely on expert judgments of holistic facial expressions—provided by highly trained coders—rather than on facial movements themselves. This renders message-based approaches susceptible to sources of individual differences (unreliability) among human coders that are not inherent to sign-based approaches (e.g., emotional inference on movements after detecting them), which can impede valid comparisons of results across studies and research sites—even when the same construct is measured.</p>
<p>In comparison, multiple comprehensive, standardized sign-based protocols have been developed and used to answer a variety of research questions [<xref ref-type="bibr" rid="pone.0211735.ref004">4</xref>]. Among these protocols, the Facial Action Coding System (FACS; [<xref ref-type="bibr" rid="pone.0211735.ref005">5</xref>]) may be the most widely used. FACS comprises approximately 33 anatomically-based facial actions (termed action units [AUs]), which interact to generate different facial expressions.</p>
<p>Originally developed from a basic emotion theory perspective, the relation between FACS-based AUs and discrete emotions is an active research topic [<xref ref-type="bibr" rid="pone.0211735.ref006">6</xref>]. Distinct patterns of AUs reliably map onto each basic emotion category (happiness, sadness, anger, fear, surprise, and disgust), and the existence of distinct patterns of AUs that people use to label different emotional expressions is often used as evidence to support discrete theories of emotion (see [<xref ref-type="bibr" rid="pone.0211735.ref007">7</xref>]). For example, oblique lip-corner contraction (AU12), together with cheek raising (AU6) reliably signals enjoyment [<xref ref-type="bibr" rid="pone.0211735.ref008">8</xref>], while brow furrowing (AU4) tends to signal negative emotions like anger and sadness (e.g., [<xref ref-type="bibr" rid="pone.0211735.ref009">9</xref>]). Recently, research on how people perceive discrete emotions from AUs has revealed up to 21 discrete categories composed of compound basic emotions (e.g., happily-surprised; [<xref ref-type="bibr" rid="pone.0211735.ref010">10</xref>]). Together, these studies suggest that people use the presence of distinct AUs to evaluate emotional content from facial expressions [<xref ref-type="bibr" rid="pone.0211735.ref011">11</xref>], a hypothesis supported by neuroimaging studies showing that differential patterns of BOLD responding in the posterior superior temporal sulcus discriminate between AUs [<xref ref-type="bibr" rid="pone.0211735.ref012">12</xref>].</p>
<p>Despite the clear links between AUs and discrete emotion perception, little is known about how AUs map onto dimensional features of emotion [<xref ref-type="bibr" rid="pone.0211735.ref007">7</xref>], especially positive and negative affect (i.e., valence). This is a potentially important oversight given the centrality of valance to dimensional theories of emotion (e.g., [<xref ref-type="bibr" rid="pone.0211735.ref013">13</xref>–<xref ref-type="bibr" rid="pone.0211735.ref015">15</xref>]), of which valence is the most consistently replicated dimension [<xref ref-type="bibr" rid="pone.0211735.ref016">16</xref>]. Early work using facial electromyography (EMG) showed that zygomatic (AU12) and corrugator (AU4) activity may indicate more positive and more negative subjective intensity, respectively (e.g., [<xref ref-type="bibr" rid="pone.0211735.ref009">9</xref>]). However, later studies found that interactions between multiple AUs better describe valence intensity (e.g., [<xref ref-type="bibr" rid="pone.0211735.ref017">17</xref>]), and in follow-up work, researchers have proposed that the face may represent positive and negative affect simultaneously with independent sets of AUs (e.g., [<xref ref-type="bibr" rid="pone.0211735.ref018">18</xref>]). Of course, the number of AUs that can be simultaneously measured using facial EMG is inherently limited by the number of electrodes that can be used without obstructing the face. Subsequently, facial EMG can only be used to identify a small set of AUs that may be linked to perceived valence intensity. In one of the few studies directly linking AUs to perceived valence intensity, Messinger et al. [<xref ref-type="bibr" rid="pone.0211735.ref019">19</xref>] found that cheek raising (AU6) was common to perceptual judgments of both intense positive and negative affect, which challenges the idea that people may use a single AU to make inference on the entire range of valence intensity. Altogether, current evidence suggests that zygomatic (AU12) and corrugator (AU4) activity indicate perceived positive and negative affect, but the extent to which these and other discrete facial actions map onto the entire range of perceived positive or negative affect intensity is unclear. Note that contemporary theories of emotion propose valence as a core affective state that arises in varying intensity <italic>before</italic> emotional experiences are labelled as happy, sad, etc. [<xref ref-type="bibr" rid="pone.0211735.ref020">20</xref>], suggesting that AUs linked to positive and negative affect are fundamental to the recognition of all other perceived emotions. Therefore, determining the extent to which specific patterns of AUs map to positive and negative affect is important for building on and testing contemporary models of emotion production and recognition.</p>
<p>Comprehensive follow-up investigations have been difficult to pursue, in part, because facial EMG can only detect a very limited number of AUs simultaneously, and manual alternatives are both labor- and time-intensive and require highly skilled annotators. Indeed, FACS training requires an average of 50–100 hours, and minutes of video can take expert coders multiple hours to rate reliably [<xref ref-type="bibr" rid="pone.0211735.ref021">21</xref>]. These characteristics limit sample sizes, reduce feasibility of replication efforts, and discourage researchers from coding facial expressions. Instead, researchers tend to rely on measures of emotional responding that are not observable in social interactions (e.g., heart rate variability). Recently, automated computer-vision and machine learning (CVML) based approaches have emerged that make it possible to scale AU annotation to larger numbers of participants (e.g., [<xref ref-type="bibr" rid="pone.0211735.ref022">22</xref>–<xref ref-type="bibr" rid="pone.0211735.ref024">24</xref>]) thus making follow-up studies more feasible. In fact, inter-disciplinary applications of CVML have allowed researchers to automatically identify pain severity (e.g., [<xref ref-type="bibr" rid="pone.0211735.ref025">25</xref>]), depressive states (e.g., [<xref ref-type="bibr" rid="pone.0211735.ref026">26</xref>]), and discrete emotions from facial expressions (e.g., [<xref ref-type="bibr" rid="pone.0211735.ref027">27</xref>]).</p>
<p>Work using CVML to detect valence intensity from facial expressions is ongoing (see [<xref ref-type="bibr" rid="pone.0211735.ref028">28</xref>]). In fact, there are annual competitions held to develop CVML models that best characterize dimensional features of emotions such as valence and arousal (e.g., [<xref ref-type="bibr" rid="pone.0211735.ref029">29</xref>]). Currently, basic emotions can be coded automatically with accuracy comparable to human coders, but valence intensity models show lower concurrent validity. For example, state-of-the-art CVML models show correlations between human- and computer-coded valence ranging from <italic>r</italic> = .60-.71 [<xref ref-type="bibr" rid="pone.0211735.ref030">30</xref>,<xref ref-type="bibr" rid="pone.0211735.ref031">31</xref>]. While impressive, there are two limitations that have impeded the use of CVML to make inferences on positive and negative affect intensity. Below, we outline each of these limitations and offer our solutions.</p>
<p>First, CVML models are often constructed using difficult to interpret machine learning models that detect valence directly from frame-by-frame video input without intermediately capturing AUs. Therefore, it is both unclear if: (1) successful valence detection depends on prior detection of specific AUs, and (2) machine learning can provide useful insights into how people interpret specific facial actions. In the current study, we show that CVML can be used to both identify well known relationships between AUs and perceived positive and negative affect intensity in addition to revealing novel relationships.</p>
<p>Second, how valence intensity is represented—and therefore measured—varies substantially across studies. For example, some previous CVML models of valence intensity have been developed from relatively small samples or on continuously collected valence ratings (human ratings collected in real-time using dials or joysticks), while others are developed based on static images. It is unclear if such models generalize to other research settings where participants’ emotional expressions to evocative stimuli are coded within discrete, trial-by-trial time intervals (e.g., [<xref ref-type="bibr" rid="pone.0211735.ref032">32</xref>]). Indeed, contemporary work using CVML has shifted from evaluating facial expressions in controlled laboratory settings toward accurately capturing continuous facial expressions of emotion “in the wild”, which is a much more difficult task (e.g., [<xref ref-type="bibr" rid="pone.0211735.ref030">30</xref>,<xref ref-type="bibr" rid="pone.0211735.ref033">33</xref>]). However, given the highly contextual nature of facial expression recognition [<xref ref-type="bibr" rid="pone.0211735.ref020">20</xref>], controlled laboratory settings are ideal for identifying AUs that are specific to perceived core affective processes such as positive and negative affect. Further, most valence-detecting CVML models assume a unidimensional valence continuum as opposed to separable continua for positive and negative affect—to our knowledge, there are few opensource datasets used in CVML research that characterize valence as multi-dimensional (see [<xref ref-type="bibr" rid="pone.0211735.ref034">34</xref>]), and very little work has been done with CVML to separate positive and negative affect (cf. [<xref ref-type="bibr" rid="pone.0211735.ref035">35</xref>]). Notably, positive and negative affect can vary independently and have different predictive values [<xref ref-type="bibr" rid="pone.0211735.ref010">10</xref>,<xref ref-type="bibr" rid="pone.0211735.ref015">15</xref>,<xref ref-type="bibr" rid="pone.0211735.ref036">36</xref>], suggesting that CVML models designed to account for each dimension separately may be most beneficial for behavioral science applications.</p>
<p>Using a well-validated method of emotion induction and both computer-vision measurement of discrete facial actions and continuous measures of positive and negative affect intensity, we (1) identified specific correspondences between perceived emotion intensity and discrete facial AUs, and (2) developed a reliable, valid, and efficient method of automatically measuring the separable dimensions of positive and negative affect intensity. Based on previous work on subjective valence intensity using facial EMG, we hypothesized that CVML would identify AUs 12 and 4 as of the most important AUs for positive and negative affect intensity, respectively. Additionally, we hypothesized that the effects of AUs 12 and 4 on positive and negative affect intensity would depend on the activation of other AUs, and that these interactions could be probed with interpretable machine learning methods. Importantly, data used to train and validate our CVML models were collected from a commonly-used psychological task and contained 4,648 video-recorded, evoked facial expressions from 125 human subjects across multiple task instructions. Our findings shed light on the mechanisms of valence recognition from facial expressions and point the way to novel research applications of large-scale emotional facial expression coding.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Method</title>
<sec id="sec003">
<title>Participants</title>
<p>Video recordings and human coder data were collected as part of a larger study [<xref ref-type="bibr" rid="pone.0211735.ref032">32</xref>]. The current study included 125 participants (84 females), ages 18–35 years. All participants gave informed consent prior to the study, and the study protocol (#2011B0071) was approved by The Ohio State Behavioral and Social Sciences Institutional Review Board. Self-reported ethnicities of participants were as follows: Caucasian (<italic>n</italic> = 96), East Asian (<italic>n</italic> = 14), African-American (<italic>n</italic> = 5), Latino (<italic>n</italic> = 3), South Asian (<italic>n</italic> = 3), and unspecified (<italic>n</italic> = 4). Note that we tested for racial/ethnic differences in valence coding accuracy, and using Bayesian comparisons we found evidence favoring no differences in accuracy between groups (see <xref ref-type="sec" rid="sec018">Supporting Information</xref>).</p>
</sec>
<sec id="sec004">
<title>Measures</title>
<sec id="sec005">
<title>Emotion-evoking task</title>
<p>We used an emotion-evoking task, depicted in <xref ref-type="fig" rid="pone.0211735.g001">Fig 1</xref>, that has been used in several previous studies to elicit facial expressions of emotion across multiple task instructions [<xref ref-type="bibr" rid="pone.0211735.ref032">32</xref>,<xref ref-type="bibr" rid="pone.0211735.ref037">37</xref>]. Participants viewed 42 positive and negative images selected from the International Affective Picture System (IAPS) to balance valence and arousal. Selections were based on previously reported college-student norms [<xref ref-type="bibr" rid="pone.0211735.ref038">38</xref>]. Images were presented in 6 blocks of 7 trials each, whereby each block consisted of all positive or all negative images. For each block, participants were asked to either <italic>enhance</italic>, <italic>react normally</italic>, or <italic>suppress</italic> their naturally evoked emotional expressions to the images. These instructions effectively increased variability in facial expressions within participants. Further, effortful enhancement and suppression of facial expressions is common across many real-world social situations where specific emotional expressions are expected to reach desired outcomes. Given known individual differences in suppression and enhancement of facial expressions [<xref ref-type="bibr" rid="pone.0211735.ref032">32</xref>,<xref ref-type="bibr" rid="pone.0211735.ref037">37</xref>], we expected that these task instructions would allow us to create a more generalizable CVML model than with no instructions at all. Block order was randomized across participants. Instructions were given so that each valence was paired once with each condition. All images were presented for 10 s, with 4 s between each image presentation. Participants’ reactions to each image were video-recorded with a 1080p computer webcam (Logitech HD C270). Due to experimenter error, 1 participant’s videos were not recorded correctly, and 7 participants were shown only 41 recordings, resulting in 6,293 usable recordings. Among these, 3 were corrupted and could not be viewed. Thus, 6,290 10-s recordings were potentially available.</p>
<fig id="pone.0211735.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211735.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Emotion-evoking task.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211735.g001" xlink:type="simple"/>
</fig>
<p>In each of the 3 blocks containing positive and negative image content, participants were asked to either <italic>enhance</italic>, <italic>react normally</italic>, or <italic>suppress</italic> their emotional expressions, so that each valence type (i.e., positive or negative) was paired once with each task instruction (enhance, react normally, suppress). All images were selected from the International Affective Picture System [<xref ref-type="bibr" rid="pone.0211735.ref038">38</xref>]. Participants’ reactions to the images were video recorded and their facial expressions were subsequently rated for positive and negative emotion intensity by a team of trained coders. The same recordings were then analyzed by FACET, a computer vision tool which automatically identifies facial Action Units (AUs). Note that the individual in this figure is of the first author. The individual in this manuscript has given written informed consent (as outlined in PLOS consent form) to publish these case details.</p>
</sec>
<sec id="sec006">
<title>Manual coding procedure</title>
<p>A team of three trained human coders, unaware of participants’ task instructions, independently viewed and rated each 10-s recording for both positive and negative emotion intensity. Presentation of recordings was randomized for each coder. Ratings were collected on a 7-point Likert scale ranging from 1 (<italic>no emotion</italic>) to 7 (<italic>extreme emotion</italic>), where positive and negative affect were coded independently following each presentation. Coders completed an initial training phase during which they rated recordings of pre-selected non-study cases and discussed specific facial features that influenced their decisions (see the <xref ref-type="sec" rid="sec018">Supporting Information</xref> for the coding guide). The goal of this training was to ensure that all coders could reliably agree on emotion intensity ratings. In addition, coders participated in once-monthly meetings throughout the coding process to ensure reliability and reduce drift. Agreement between coders across all usable recordings (6,290 recordings) was high, with intraclass correlation coefficients (ICCs(3); [<xref ref-type="bibr" rid="pone.0211735.ref039">39</xref>]) of .88 and .94 for positive and negative ratings, respectively. The ICC(3) measure reported above indicates absolute agreement of the average human-coder rating within each condition (<italic>enhance</italic>, <italic>react normally</italic>, <italic>suppress</italic>) for each of the 150 participants in the original study [<xref ref-type="bibr" rid="pone.0211735.ref032">32</xref>]. To prepare data for CVML analysis, we performed an additional quality check to screen out videos in which participants’ faces were off-camera or covered. Any recording in which a participant’s face was covered, obscured, or off-camera for 1 s or more was removed from analysis. If 50% or more of a participant’s recordings were excluded, we excluded all of his/her recordings to ensure that we had enough within-subject data to use for within-subject model performance analyses. This resulted in a total of 4,648 usable recordings across 125 participants. With over 4,000 individually-coded recordings, our sample size is in the typical range for machine learning applications [<xref ref-type="bibr" rid="pone.0211735.ref040">40</xref>].</p>
</sec>
<sec id="sec007">
<title>Automated coding procedure</title>
<p>We then analyzed each of the 4,648 recordings with FACET [<xref ref-type="bibr" rid="pone.0211735.ref024">24</xref>]. FACET is a computer-vision tool that automatically detects 20 FACS-based AUs (see <xref ref-type="supplementary-material" rid="pone.0211735.s007">S1 Table</xref> for descriptions and depictions of FACET-detected AUs). While there are no published validation studies of FACET’s AU detection accuracy to our knowledge, there are many studies validating the Computer Expression Recognition Toolbox (CERT), which is FACET’s opensource predecessor [<xref ref-type="bibr" rid="pone.0211735.ref041">41</xref>]. Validation studies of CERT show that it can discriminate between 18 different AUs with high accuracy rates (e.g., average 2AFC = 80–90%, [<xref ref-type="bibr" rid="pone.0211735.ref041">41</xref>]). Further, FACET has shown better than human accuracy in detecting basic emotions across multiple datasets (e.g., &gt; 95%, [<xref ref-type="bibr" rid="pone.0211735.ref024">24</xref>]), which strongly relies on accurately capturing the AUs that describe each basic emotion category. Note that FACET was recently purchased by Apple Inc. and is no longer available to the public. However, there are other commercial software options available for automated AU detection including Noldus’s FaceReader, Affectiva’s AFFDEX, and the opensource OpenFace package, each of which have been validated in previous studies [<xref ref-type="bibr" rid="pone.0211735.ref022">22</xref>–<xref ref-type="bibr" rid="pone.0211735.ref024">24</xref>]. Importantly, the methodology we use in the current study is not specific to FACET and any of the above software tools could be utilized to replicate our analyses. FACET outputs values for each AU indicating the algorithm’s confidence in the AU being present. Confidence values are output at a rate of 30 Hz, resulting in a time-series of confidence values for each AU being present with each frame of a video-recording. Each point in the time-series is a continuous number ranging from about -16 to 16, whereby more positive and more negative numbers indicate increased and decreased probability of the presence of a given AU, respectively. We refer to this sequence of numbers as an AU evidence time-series.</p>
<p>Each AU evidence time-series was converted to a point estimate by taking the area under the curve (AUC) of the given time-series and dividing the AUC by the total length of time that a face was detected throughout the clip. This creates a normalized measure that does not render biased weights to clips of varying quality (e.g., clips in which participants’ faces are occasionally not detected). Point-estimates computed this way represent the expected probability that a participant expressed a given AU across time. We used the AU evidence time-series point estimates as predictor (independent) variables to train a machine learning model to predict human valence intensity ratings. It took FACET less than 3 days to extract AU evidence time-series data from all recordings (running on a standard 8-core desktop computer). Note that we did not use a baseline correction for each subject, which would require human annotation of a neutral facial expression segment for each participant. Therefore, the models reported here may be applied to novel facial recordings with no human judgment.</p>
<p>In addition to raw AU scores, FACET computes scores for positive and negative affect which reflect the probability that a facial expression is of either positive or negative affect. Although these scores reflect presence of positive or negative affect rather than intensity, we report them alongside our results to emphasize the added predictive validity achieved by our method. We used the same preprocessing steps for FACET’s positive and negative affect scores as for the AUs (i.e. we computed the normalized AUC values for each recording).</p>
</sec>
</sec>
<sec id="sec008">
<title>Machine learning procedure</title>
<p><xref ref-type="fig" rid="pone.0211735.g002">Fig 2</xref> depicts the machine learning procedure. We trained a random forest (RF) model to predict human-coded valence ratings from the AU evidence time-series point estimates described above (see <xref ref-type="sec" rid="sec018">Supporting Information</xref> for details on training). RFs are constructed by generating multiple decision trees and averaging predictions of all trees together. We chose the RF model because (1) it can automatically capture interactions between independent variables, and we know that humans use multiple AUs simultaneously when evaluating facial expressions; (2) the importance of each independent variable can be easily extracted from the RF to make inferences regarding which AUs human coders attended to while rating valence intensity (analogous to interpreting <italic>beta</italic> weights from a multiple regression; [<xref ref-type="bibr" rid="pone.0211735.ref040">40</xref>]); and (3) RFs have previously shown robust representations of the mapping from facial features (e.g., AUs) to discrete emotions and valence intensity [<xref ref-type="bibr" rid="pone.0211735.ref042">42</xref>,<xref ref-type="bibr" rid="pone.0211735.ref043">43</xref>]. We additionally tested regularized regression models including the least absolute shrinkage and selection operator (LASSO), ridge regression, and elastic-net, but these linear models did not adequately capture the human ratings. Further, we tested a Deep Neural Network model that performed similarly to the reported RF results (see <xref ref-type="sec" rid="sec018">Supporting Information</xref> for model comparison), and due to its ease of use and interpretation we decided to only report the RF model results in the main text .Given high agreement among coders and a large literature showing that aggregating continuous ratings from multiple, independent coders leads to reliable estimates despite item-level noise (i.e., ratings for each recording; see [<xref ref-type="bibr" rid="pone.0211735.ref044">44</xref>]), we used the average of all coders’ ratings for each recording as the outcome (dependent) variable to train the RF.</p>
<fig id="pone.0211735.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211735.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Machine learning procedure.</title>
<p>The goal of our first analysis was to determine whether or not CVML could perform similarly to humans in rating facial expressions of emotion. For each AU evidence time-series, we computed the normalized (i.e., divided by the total time that FACET detected a face) Area Under the Curve (AUC), which captures the probability that a given AU is present over time. All AUC values (20 total) were entered as predictors into the random forest (RF) model to predict the average coder rating for each recording. To test how similar the model ratings were to human ratings, we separated the data into training (3,060 recordings) and test (1,588 recordings) sets. We fit the RF to the training set and made predictions on the unseen test set. Model performance was assessed by comparing the Pearson and intraclass correlations between computer- and human-generated ratings in the test sets.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211735.g002" xlink:type="simple"/>
</fig>
<p>The RF model contains 2 tuning parameters, namely: (1) <italic>ntrees</italic>–the number of decision trees used in the forest, and (2) <italic>mtry</italic>–the number of predictors to sample from at each decision node (i.e., “split”) in a tree. A grid search over <italic>ntrees</italic> ∈{100, 200, 300,…,1000} showed that out-of-bag prediction accuracy converged by 500 trees for both positive and negative datasets (not reported). A grid search over <italic>mtry</italic> ∈{1, 2, 3,…,20} revealed negligible differences in out-of-bag prediction accuracy for values ranging from 5 to 20. Because RFs do not over-fit the data with an increasing number of trees [<xref ref-type="bibr" rid="pone.0211735.ref040">40</xref>], we set <italic>ntrees</italic> = 500 for models presented in all reported analyses to ensure convergence. Because initial grid searches over <italic>mtry</italic> failed to improve the model, we set <italic>mtry</italic> heuristically [<xref ref-type="bibr" rid="pone.0211735.ref040">40</xref>] as <italic>mtry</italic> = <italic>p</italic>/3, where <italic>p</italic> represents the number of predictors (i.e., 1 for each AU) in an <italic>n</italic> × <italic>p</italic> matrix (<italic>n</italic> = number of cases) used to train the model. We fit the RF model using the <italic>easyml</italic> R package [<xref ref-type="bibr" rid="pone.0211735.ref045">45</xref>], which provides a wrapper function for the <italic>randomForest</italic> R package [<xref ref-type="bibr" rid="pone.0211735.ref046">46</xref>]. All R codes and de-identified data (i.e. FACET output and human coder ratings) used for model fitting along with the trained RF models are available on our lab GitHub, which allow for replication of all analyses and figures (<ext-link ext-link-type="uri" xlink:href="https://github.com/CCS-Lab/Haines_CVML_2018" xlink:type="simple">https://github.com/CCS-Lab/Haines_CVML_2018</ext-link>).</p>
<sec id="sec009">
<title>Correspondence between human coders and model predictions</title>
<p>Model performance refers to how similar the model- and human-generated valence intensity rating are. To assess model performance, we split the 4,648 recordings into training (<italic>n</italic> = 3,060; 65.8%) and test (<italic>n</italic> = 1,588; 34.2%) sets, trained the model on the training set (see the <xref ref-type="sec" rid="sec018">Supporting Information</xref> for details), and then made predictions on the unseen test set to assess how well the RF predicted valence intensity ratings on new data. The data were split randomly with respect to participants so that the training and test data contained 66% and 34% of each participant’s recordings, respectively. This separation ensured that training was conducted with all participants, thus creating a more generalizable final model. We fit a separate RF model to positive and negative human ratings. To see if the way we split the training and test data influenced our results, we made 1,000 different training/test-set splits and assessed model performance across all splits [<xref ref-type="bibr" rid="pone.0211735.ref047">47</xref>,<xref ref-type="bibr" rid="pone.0211735.ref048">48</xref>]. We used Pearson correlations and ICC coefficients to check model performance on training- and test-sets. Pearson correlations measure the amount of variance in human ratings captured by the model, whereas ICCs measure absolute agreement between human- and model-predicted ratings at the item level (i.e., per recording). Therefore, high correlations and ICCs indicate the model is capturing a large amount of variance in human coder ratings and generating ratings using a similar scale as human coders, respectively. We used McGraw and Wong’s ICC(1), as opposed to other ICC methods [<xref ref-type="bibr" rid="pone.0211735.ref039">39</xref>], because we were interested in absolute agreement across all clips, regardless of condition/participant. One-way models were used to compute ICCs in all cases. In general, ICCs between .81 and 1.00 are considered “almost perfect” (i.e., excellent) and ICCs between .61 and .80 are considered “substantial” (i.e., good; [<xref ref-type="bibr" rid="pone.0211735.ref049">49</xref>]). We used regression-based approaches and performance measures as opposed to classification-based alternatives (e.g., F1 scores on models trained to classify intensity ratings) because the averaged coder ratings across recordings resembled continuous, real numbers more so than ordinal, categorical intensity scores. Additionally, regression-based models are commonly used in developing models that predict valence and/or arousal intensity. We also checked model performance using a different folding scheme for separating training and test sets which ensured that participants’ recordings were not shared across splits. This analysis revealed negligible differences in prediction accuracy for positive ratings and a decrease in accuracy for negative ratings, which suggests that more training data may be necessary to capture negative as opposed to positive affect intensity (see <xref ref-type="sec" rid="sec018">Supporting Information</xref>).</p>
</sec>
<sec id="sec010">
<title>Importance of AUs for positive and negative affect</title>
<p>To identify the specific AUs that human coders were influenced most by when making affective ratings, we fit the RF model to the entire dataset (all 4,648 recordings) without splitting into training and test sets. We used this method to identify independent variables that were robust across all samples [<xref ref-type="bibr" rid="pone.0211735.ref047">47</xref>,<xref ref-type="bibr" rid="pone.0211735.ref048">48</xref>]. After fitting the RF models, the importance of each independent variable was estimated using <italic>partial dependence</italic> [<xref ref-type="bibr" rid="pone.0211735.ref050">50</xref>], a measure of the expected standard deviation in the outcome variable (e.g., positive or negative affect intensity) as a function of a given predictor variable (e.g., AU12) averaged across all other predictor variables (e.g., all AUs except AU12). In fact, in special cases, the absolute values of the multiple regression beta weights are equivalent to the corresponding partial dependence metric [<xref ref-type="bibr" rid="pone.0211735.ref050">50</xref>], which makes partial dependence a useful metric for assessing the importance of predictors when using “black-box” methods such as RFs. Crucially, and unlike other methods of measuring variable importance, partial dependence can also be used to probe both directionality and interaction effects when plotted as a function of the model predictors [<xref ref-type="bibr" rid="pone.0211735.ref050">50</xref>].</p>
<p>To determine if CVML could adequately capture the relative importance of AUs for each individual coder, we also fit the RF to each coder’s ratings independently. We used randomization tests to determine the minimum number of ratings necessary to accurately infer which AUs the coders attended to while generating emotion ratings. For each of the 3 coders, we performed the following steps: (1) randomly sample <italic>n</italic> recordings rated by coder <italic>i</italic>, (2) fit the RF model to the subset of <italic>n</italic> recordings/ratings according to the model fitting procedures outlined above, (3) compute the ICC(2) of the extracted RF feature importances (i.e., <italic>partial dependence</italic>) between the subsampled model and the model fit to all recordings/ratings from coder <italic>i</italic>, and (4) iterate steps 1–3 thirty times for each value of <italic>n</italic> (note that different subsets of <italic>n</italic> recordings/ratings were selected for each of these thirty iterations). We varied <italic>n</italic> ∈ {10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 115, 125, 135, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000, 1200, 1400, 1600, 1800, 2000, 2500, 3000}.</p>
</sec>
</sec>
</sec>
<sec id="sec011" sec-type="results">
<title>Results</title>
<sec id="sec012">
<title>Model performance across participants</title>
<p><xref ref-type="table" rid="pone.0211735.t001">Table 1</xref> shows correlations between the model-predicted and the average of the human coders’ ratings per recording across both training and test sets. Overall, the RF showed good to excellent performance across both training and test sets for positive and negative ratings. Notably, these results were supported by both the Pearson correlations and the ICCs, suggesting that the RF produced ratings that not only captured variance in, but also showed high agreement with, human ratings. Sensitivity analyses (see <xref ref-type="fig" rid="pone.0211735.g003">Fig 3</xref>) indicated that model performance was robust across different training and test splits of the data. These results suggest that variance in human-coded valence intensity can be captured by the presence of discrete AUs.</p>
<fig id="pone.0211735.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211735.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Sensitivity of model performance to different training/test splits.</title>
<p>Results of sensitivity analyses across different splits of the training and test sets. We created 1,000 different splits of the training and test sets, fit the RF to each training set, and then made predictions on each respective test set. We stored the Pearson correlations between human- and model-generated ratings for each iteration. Distributions therefore represent uncertainty in prediction accuracy. Means of the distributions (superimposed on respective graphs) are represented by dashed red lines.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211735.g003" xlink:type="simple"/>
</fig>
<table-wrap id="pone.0211735.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211735.t001</object-id>
<label>Table 1</label> <caption><title>Correlations between human- and computer-generated valence ratings.</title></caption>
<alternatives>
<graphic id="pone.0211735.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211735.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center" rowspan="3">Model:<break/>Data Set</th>
<th align="center" colspan="4">Correlation [95% CI]</th>
</tr>
<tr>
<th align="center" colspan="2"><italic>r</italic></th>
<th align="center" colspan="2">ICC(1)</th>
</tr>
<tr>
<th align="center">(+)</th>
<th align="center">(–)</th>
<th align="center">(+)</th>
<th align="center">(–)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">RF Ratings:<break/>Training</td>
<td align="center">.89 [.88, .90]</td>
<td align="center">.77 [.75, .78]</td>
<td align="center">.88 [.87, .89]</td>
<td align="center">.71 [.69, .72]</td>
</tr>
<tr>
<td align="center">RF Ratings:<break/>Test</td>
<td align="center">.88 [.87, .89]</td>
<td align="center">.74 [.72, .77]</td>
<td align="center">.87 [.86, .88]</td>
<td align="center">.68 [.65, .71]</td>
</tr>
<tr>
<td align="center">FACET Ratings: Training + Test</td>
<td align="center">.71 [.70, .73]</td>
<td align="center">.40 [.38, .43]</td>
<td align="center">-.43 [-.46, -.41]</td>
<td align="center">-.22 [-.25, -.20]</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t001fn001"><p><italic>Notes</italic>. (+) = positive valence ratings; (–) = negative valence ratings; <italic>r</italic> = Pearson’s correlation; ICC = Intraclass correlation coefficient. Training and test sets contained 3,060 and 1,588 recordings, respectively. Note that because FACET’s default positive and negative valence scores were not informed by our dataset, we present the correlations of FACET scores across the entire dataset as opposed to separately for training and test sets. ICC(1) scores are not necessarily interpretable for FACET’s positive and negative affect scores because FACET’s scale of measurement is arbitrary (i.e. ranging from about -16 to +16), whereas the human coders made judgements on a meaningful 1–7 scale. Nevertheless, we report them for completeness.</p></fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="sec013">
<title>Model performance within participants</title>
<p>We also checked model performance for each of the 125 participants by computing correlations between human- and model-generated ratings for each participant separately (<xref ref-type="fig" rid="pone.0211735.g004">Fig 4</xref>). Although the RF model performed well for many participants in the positive (median <italic>r</italic> = .91, ICC(1) = .80) and negative (median <italic>r</italic> = .73, ICC(1) = .51) affect test sets, 5 participants within the positive and 7 participants within the negative affect test-set yielded negative correlations between human- and computer-generated emotion ratings (<xref ref-type="fig" rid="pone.0211735.g004">Fig 4</xref>). Further analyses of within-participant model performance revealed significant positive associations between within-subject variance in model-predicted ratings and within-participant prediction accuracy (all <italic>r</italic>s ≥ .54, <italic>p</italic>s &lt; .001; see <xref ref-type="supplementary-material" rid="pone.0211735.s002">S2A Fig</xref>). We found the same relation between human-assigned ratings and within-participant variance (see <xref ref-type="supplementary-material" rid="pone.0211735.s002">S2B Fig</xref>). This suggests that the RF model was more accurate in predicting human-rated emotion if participants expressed a wider range of emotional intensity.</p>
<fig id="pone.0211735.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211735.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Model performance within participants.</title>
<p>Distributions of within-participant Pearson correlations for positive and negative ratings in the training (all 125 participants) and test (122 participants; correlations could not be computed for 3 participants who had 0 variance in human ratings) sets. Red dashed lines represent median within-participant Pearson correlations for each distribution. Intraclass correlations for corresponding figures are reported in text.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211735.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec014">
<title>Importance of AUs across task instructions</title>
<p>To identify which facial expressions human coders may have used to generate positive and negative emotion ratings, we examined the importance of all AUs in predicting human emotion ratings (<xref ref-type="fig" rid="pone.0211735.g005">Fig 5</xref>). Note that importance values for the RF do not indicate directional effects, but instead reflect relative importance of a given AU in predicting human-coded positive/negative affect intensity. The RF identified AUs 12 (<italic>lip corner pull</italic>), 6 (<italic>cheek raiser</italic>), and 25 (<italic>lips part</italic>) as three of the five most important AUs for predicting positive emotion. In contrast to positive ratings, relative importance values for AUs of negative ratings were distributed more evenly across AUs, a trend which was also found when the RF was fit individually to each coder (see <italic>Coder-specific AU importance measures</italic> below). Notably, the importance of AUs for positive and negative emotion ratings were largely independent. In fact, when the ICC(3) is computed by treating positive and negative importance weights for each AU as averaged ratings from two “coders”, the ICC(3) is negative and non-significant (ICC(3) = –.48, <italic>p</italic> = .80), which would only be expected if different facial expressions were important for the coders to rate positive versus negative valence. Lastly, the RF identified stronger interactive effects between AUs for positive relative to negative affect intensity (<xref ref-type="fig" rid="pone.0211735.g005">Fig 5</xref>). Specifically, interactions between AUs 12*18 and 2*12 together accounted for ~25% of the interactive effects for positive affect, which is exceedingly high given the 190 possible 2-way interactions. Conversely, interactions between AUs for negative affect intensity were more uniformly important, apart from the interaction between AUs 4*5. These differences in interactions between positive and negative affect may be partially attributable to the larger number of possible AU combinations that can indicate negative rather than positive affect.</p>
<fig id="pone.0211735.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211735.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Main and interactive effects among and between AUs for positive and negative ratings.</title>
<p>Relative importance of the main effects and interactions among all AUs for positive and negative human-coder ratings. Relative importance (normalized <italic>partial dependence</italic> from the RF model) is a measure the SD in the outcome variable (i.e. positive or negative affect intensity) attributable to each AU while integrating over all other AUs, and it can be interpreted as how important a given AU is with respect to all other AUs. Note that partial dependence is not directional (see <xref ref-type="fig" rid="pone.0211735.g006">Fig 6</xref> for directional effects). Visual depictions of the 5 most important AUs for predicting positive and negative ratings are shown on the graphs. Because there are 190 possible combinations of AUs for displaying interactive effects, we only show the top 20 here for brevity.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211735.g005" xlink:type="simple"/>
</fig>
<p>The partial dependence analysis measures revealed that the main effects of the 5 most important AUs were in the expected directions for both positive and negative affect intensity ratings (<xref ref-type="fig" rid="pone.0211735.g006">Fig 6</xref>). Specifically, AUs 12, 6, and 25 were positively related to increased positive affect intensity, while AUs 4, 5, 9, and 10 were positively related to increased negative affect intensity. Intriguingly, we found that AU18 was negatively related to increased positive affect intensity, which may be attributed to either its masking effects on AU12 or its relation anger. Indeed, the largest interaction for positive affect was between AUs 12 and 18, where high presence scores for AU12 in combination with low presence scores for AU18 predicted high positive affect intensity. For negative affect intensity, we found an interaction between AUs 1 and 5 such that negative affect was most intense when AU5 had high presence scores while AU1 had low presence scores, despite both AUs showing independent, positive relationships with increased negative affect. We found a similar relationship between AUs 5 and 9, which revealed that negative affect was strongest when AUs 5 and 9 had high and low presence scores, respectively. These finding may be attributable to AUs 5 relationship<bold><italic>s</italic></bold> to fear, surprise, and arousal, of which arousal is often used as an indicator of more intense emotion by human judges (e.g, [<xref ref-type="bibr" rid="pone.0211735.ref051">51</xref>]).</p>
<fig id="pone.0211735.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211735.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Directionality of main and interactive effects.</title>
<p>Partial dependence plots of the 5 most important main and interactive effects for both positive and negative affect intensity ratings. Partial dependence indicates the predicted affect intensity while integrating over all other AUs. Panel (A) shows the directionality of main effects, where increasing (decreasing) values indicate positive (negative) effects as AU presence increases. Panel (B) shows directionality of interactive effects, where warmer (cooler) colors indicate higher (lower) affect intensity ratings given specific combinations of AU presence scores on the x- and y-axes.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211735.g006" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec015">
<title>Sensitivity of AUs to task instructions</title>
<p>To determine if task instructions (<italic>enhance</italic>, <italic>react normally</italic>, <italic>suppress</italic>) affected model performance or our interpretation of which AUs map onto positive and negative affect, we fit the RF model to all recordings from each condition separately and then compared model performance and AU importance scores across conditions. <xref ref-type="table" rid="pone.0211735.t002">Table 2</xref> shows correlations between human- and computer-generated valence ratings within the different conditions, and summary statistics for AU evidence scores within each condition are provided in <xref ref-type="supplementary-material" rid="pone.0211735.s008">S2 Table</xref>. For positive ratings, correlations were consistently high (<italic>r</italic>s &gt; .80) across all conditions. In contrast, for negative ratings, correlations were highest in the enhance condition, followed by the react normally and suppress conditions. Of note, all correlations between human- and computer-generated ratings were lower when data were separated by condition compared to when condition was ignored (cf., <xref ref-type="table" rid="pone.0211735.t002">Table 2</xref> to <xref ref-type="table" rid="pone.0211735.t001">Table 1</xref>). This suggests the lower number of recordings included in the training samples may be partially responsible for lower model performance, but also that CVML performs best when trained on a wider range of emotional intensity. Indeed, our supplementary analyses showed that when participants had lower variance in affect intensity (determined by either human or model ratings), the correspondence between human and model ratings tended to be lower as well (see <xref ref-type="supplementary-material" rid="pone.0211735.s002">S2 Fig</xref>). This finding suggests that lower model performance in the Suppression condition may be due to limited variation in human ratings for the model to predict.</p>
<table-wrap id="pone.0211735.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211735.t002</object-id>
<label>Table 2</label> <caption><title>Correlations between human- and computer-generated ratings within conditions.</title></caption>
<alternatives>
<graphic id="pone.0211735.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211735.t002" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Condition</th>
<th align="center" colspan="4">Correlation [95% CI]</th>
<th align="center" colspan="2">Number of recordings</th>
</tr>
<tr>
<th align="left"/>
<th align="center" colspan="2"><italic>r</italic></th>
<th align="center" colspan="2">ICC(1)</th>
<th align="center" rowspan="2">Training</th>
<th align="center" rowspan="2">Test</th>
</tr>
<tr>
<th align="left"/>
<th align="center">(+)</th>
<th align="center">(–)</th>
<th align="center">(+)</th>
<th align="center">(–)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Enhance</td>
<td align="center">.81 [.78, .84]</td>
<td align="left">.64 [.59, .68]</td>
<td align="center">.79 [.76, .82]</td>
<td align="center">.61 [.55, .66]</td>
<td align="center">1,047</td>
<td align="center">569</td>
</tr>
<tr>
<td align="center">Normal</td>
<td align="center">.81 [.78, .84]</td>
<td align="center">.55 [.49, .61]</td>
<td align="center">.79 [.76, .82]</td>
<td align="center">.49 [.42, .55]</td>
<td align="center">880</td>
<td align="center">516</td>
</tr>
<tr>
<td align="center">Suppress</td>
<td align="center">.85 [.83, .87]</td>
<td align="center">.44 [.38, .51]</td>
<td align="center">.83 [.80, .85]</td>
<td align="center">.35 [.28, .42]</td>
<td align="center">1,040</td>
<td align="center">596</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t002fn001"><p><italic>Notes</italic>. (+) = positive valence ratings; (–) = negative valence ratings; <italic>r</italic> = Pearson’s correlation; ICC = Intraclass correlation coefficient. All results reported are on test sets.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>Despite only moderate correlations for negative ratings in these conditions, relative importance values for AUs across conditions showed minimal differences (<xref ref-type="fig" rid="pone.0211735.g007">Fig 7</xref>). In fact, ICCs between AU importance values across conditions were excellent for both positive and negative ratings (<xref ref-type="fig" rid="pone.0211735.g007">Fig 7</xref>). Taken with our supplementary analysis of variation in human ratings and model performance, these results suggest that the task instructions did not strongly influence the interpretation of important AUs for detecting positive and negative affect intensity across coders.</p>
<fig id="pone.0211735.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0211735.g007</object-id>
<label>Fig 7</label>
<caption>
<title>AU relative importance values across task instructions.</title>
<p>Relative importance of each AU for positive valence and negative valence human-coder ratings within each of the three task instructions (<italic>enhance</italic>, <italic>react normally</italic>, <italic>suppress</italic>). Intraclass correlation coefficients–both treating importance values as average [ICC(3)] and single [ICC(1)] units–are superimposed. We show ICC(3) here because the AU importance scores could be interpreted as “averages” across all recordings.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0211735.g007" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec016">
<title>Coder-specific AU importance measures</title>
<p>All three coders showed similarly-ordered importance profiles, indicating that they attended to similar AUs while generating emotion ratings (<xref ref-type="supplementary-material" rid="pone.0211735.s003">S3 Fig</xref>). Agreement between all three individual coders’ importance profiles supported this claim—non-normalized ICC(3)s were high for both positive (ICC(3) = 0.93) and negative (ICC(3) = 0.90) importance profiles. The randomization test revealed how many recordings were necessary to adequately estimate the relative importance of AUs for each individual coder. For positive ratings, ICC(2)s for all 3 coders reached 0.75 (regarded as “excellent” agreement; see 39) after approximately 60 recordings/ratings. For negative ratings, ICC(2)s for all 3 coders reached 0.75 after approximately 150 recordings/ratings (see <xref ref-type="supplementary-material" rid="pone.0211735.s004">S4 Fig</xref>). Because the recordings in our task were 10 s long and coders rated positive/negative emotion intensity after each recording, the task used in the current study could be condensed to about 150 recordings (&lt;30 minutes) and still reveal coder-specific AU importance measures with good accuracy. Future studies may be able to shorten the task even further by testing shorter video recordings (i.e., less than 10 s per recording).</p>
</sec>
</sec>
<sec id="sec017" sec-type="conclusions">
<title>Discussion</title>
<p>Our study offers strong evidence that people use discrete AUs to make wholistic judgments regarding positive and negative affect intensity from facial expressions, indicating that patterns of discrete AUs reliably represent dimensions of facial expressions of emotion (analogous to how specific patterns of AUs map to the basic emotions). Our CVML analysis identified AU12, AU6, and AU25 as especially important features for positive affect intensity ratings. Together, these AUs represent the core components of a genuine smile [<xref ref-type="bibr" rid="pone.0211735.ref052">52</xref>]. Note that AU12 and AU6 interact to signify a <italic>Duchenne smile</italic>, which can indicate genuine happiness [<xref ref-type="bibr" rid="pone.0211735.ref008">8</xref>], and previous research demonstrates that accurate observer-coded enjoyment ratings rely on AU6 [<xref ref-type="bibr" rid="pone.0211735.ref053">53</xref>]. Additionally, the five most important AUs we identified for negative affect intensity map on to those found in negative, discrete emotions such as fear and anger (AUs 4 and 5), disgust (AU9), and sadness (AU4). While AU12 and AU4 have been implicated in positive and negative affect for some time (e.g., [<xref ref-type="bibr" rid="pone.0211735.ref009">9</xref>]), this is the first study of its kind to determine the relative importance of these and other AUs in determining positive and negative affect intensity. Importantly, the strong correspondence that we found between specific sets of AUs and positive and negative valence intensity suggests that contemporary models of constructed emotion may be further tested against basic emotion theories in experimental settings. For example, future studies may investigate the time course of facial expression detection, where basic versus constructed emotion theories make differential predictions on whether basic emotional categories versus emotional dimensions are recognized more accurately and/or rapidly.</p>
<p>Together, the AUs that we identified for positive and negative affect are consistent with prior studies suggesting that positive and negative facial expressions occupy separate dimensions [<xref ref-type="bibr" rid="pone.0211735.ref015">15</xref>,<xref ref-type="bibr" rid="pone.0211735.ref054">54</xref>]. Notably, the AUs accounting for the majority of the variance in positive affect had no overlap with those for negative affect, evidenced by near-zero ICCs, indicating that our human coders used distinct patterns of facial expressions to evaluate positive versus negative intensity ratings. The existence of distinct patterns of AUs which represent positive and negative affect intensity explains paradoxical findings that facial expressions can be simultaneously evaluated as both positive and negative (e.g., happily-disgusted; [<xref ref-type="bibr" rid="pone.0211735.ref010">10</xref>]). Importantly, prior studies have shown that automated facial expression recognition tools such as FACET sometimes fail to recognize blended expressions as accurately as human observers do, which is in part human observers rely strongly on affective valence whereas tools such as FACET rely on morphological features when making classifying expressions (e.g., AUs; [<xref ref-type="bibr" rid="pone.0211735.ref055">55</xref>]). Our results suggest that this inherent limitation of automated tools can potentially be overcome if morphological features are used to train models to predict valence intensity, which may then allow CVML to make better distinctions between prototypical and blended facial expressions. Further, our supplementary results suggest that the use of CVML to determine the relative importance of AUs for positive and negative affect recognition within individual coders is a potentially important avenue for future research. While the current study only determined relative AU importance for three trained coders (see <xref ref-type="supplementary-material" rid="pone.0211735.s003">S3</xref> and <xref ref-type="supplementary-material" rid="pone.0211735.s004">S4</xref> Figs), future studies may collect emotion ratings from larger, naïve groups of participants and perform similar analyses to assess for potential individual differences.</p>
<p>Our results also provide support for the use of CVML as a valid, efficient alternative to human coders, and with further validation we expect CVML to expand the possibilities of future facial expression research in the social and behavioral sciences. For example, adoption of automatic facial coding tools will allow researchers to more easily incorporate facial expressions into models of human decision making. Decades of research show clear links between facial expressions of emotion and cognitive processes in aggregate (see [<xref ref-type="bibr" rid="pone.0211735.ref056">56</xref>,<xref ref-type="bibr" rid="pone.0211735.ref057">57</xref>]), yet the dynamics between cognitive mechanisms and facial expressions are poorly understood in part due to difficulties accompanying manual coding. In fact, we are currently using computational modeling to explore cognition-expression relationships with the aid of CVML [<xref ref-type="bibr" rid="pone.0211735.ref058">58</xref>], which would be infeasible with manual coding of facial expressions. For example, in the current study it took less than three days to automatically extract AUs from 4,648 video recordings and train ML models to generate valence intensity ratings (using a standard desktop computer). In stark contrast, it took six months for three undergraduate human coders to be recruited, trained, and then code <italic>affect intensity</italic> across our 125 subjects—FACS coding would have taken much longer, rendering the scale of this project infeasible.</p>
<p>Models used in this study predicted positive emotion intensity with greater accuracy than negative emotion intensity, which may be due to the number of discrete facial actions associated with negative compared to positive emotional expressions. To support this claim, we found that importance scores for negative, but not positive, emotion ratings were spread across many different AUs and showed more variation across task instructions (Figs <xref ref-type="fig" rid="pone.0211735.g005">5</xref> and <xref ref-type="fig" rid="pone.0211735.g007">7</xref>). This suggests that a wider range of facial expressions were used by coders when generating negative rather than positive emotion ratings. Future studies might address this with CVML models that can detect more than the 20 AUs used here. Additionally, our results suggest that negative affect intensity requires more training data for CVML than positive affect, as evidenced by large discrepancies in model performance between our CVML model that ignored the task instructions compared to those that we fit to data from each task instruction separately. Future studies might address this by devoting more time to collecting and coding negative, rather than positive, affective facial expressions.</p>
<p>Our interpretation of the computer-vision coded AUs in this study is potentially limited because we did not compare reliability of AU detection between FACET and human FACS experts. Additionally, FACET only detects 20 of the approximately 33 AUs described by FACS, so it is possible that there were other important AUs to which the human coders attended when generating valence ratings that we were unable to capture. However, our models showed excellent prediction accuracy on new data (i.e., capturing ~80% of the variance in human ratings of positive affect intensity), and we identified theoretically meaningful patterns of AUs for positive and negative emotion intensity that are consistent with prior studies (e.g., components of the <italic>Duchenne smile</italic>). Crucially, of the AUs that were identified as important for positive and negative affect intensity, our interpretable machine learning analyses revealed that each AU had main and interactive effects that were in the theoretically predicted directions (e.g., AU12 and AU4 predicting increased positive and negative affect intensity, respectively). It is unlikely that we would achieve these results if FACET did not reliably detect similar, important AUs which represented the intensity of positive and negative facial expressions produced by our 125 participants. Further, because FACET is intended for commercial use, it has been trained on a large number of participants across a variety of different genders, ages, and ethnicities, which is likely why our model generalized well across ethnicities despite our predominantly Caucasian sample (see <xref ref-type="sec" rid="sec018">Supporting Information</xref>). Finally, as computer vision advances, we expect that more AUs will be easier to detect. CVML provides a scalable method that can be re-applied to previously collected facial expression recordings as technology progresses. Our interpretation of the relative importance of AUs for perceptual ratings of positive and negative affect intensity is clearly limited by our relatively low number of coders. However, the strong correspondence we found between human- and model-predicted affect intensity is made stronger by the number of subjects and recordings per subject used to train our models, and our supplementary analyses showed that our design may be expanded to larger numbers of “coders” (i.e. participants) with a substantially reduced number of recordings to empirically probe coder-specific AU importance measures for positive and negative affect intensity recognition (see <xref ref-type="supplementary-material" rid="pone.0211735.s004">S4 Fig</xref>).</p>
<p>Although this study investigated positive and negative affect, our method could easily be extended to identify facial actions that are associated with other emotional constructs (e.g., arousal). The ability to identify specific AUs responsible for facial expression recognition has implications for various areas within the social and behavioral sciences. Opportunities may be particularly pronounced for psychopathology research, where deficits and/or biases in recognizing facial expressions of emotion are associated with a number of psychiatric disorders, including autism, alcoholism, and depression [<xref ref-type="bibr" rid="pone.0211735.ref059">59</xref>–<xref ref-type="bibr" rid="pone.0211735.ref061">61</xref>]. CVML provides a framework through which both normal and abnormal emotion recognition can be studied efficiently and mechanistically, which could lead to rapid and cost-efficient markers of emotion recognition in psychopathology [<xref ref-type="bibr" rid="pone.0211735.ref062">62</xref>].</p>
</sec>
<sec id="sec018">
<title>Supporting information</title>
<supplementary-material id="pone.0211735.s001" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pone.0211735.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Sensitivity of model performance to different training scheme.</title>
<p>Test set performance for the RF model fit using 1,000 training/test splits where separate participants were used to train and test the model. Note that performance for positive affect intensity—but not negative affect intensity—is indistinguishable from results reported in the main text (c.f. <xref ref-type="fig" rid="pone.0211735.g003">Fig 3</xref>), suggesting that models of negative affect intensity may require a more diverse set of training data (i.e. more participants) compared to positive affect intensity.</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0211735.s002" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pone.0211735.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Probing within-participant model performance.</title>
<p>(A) Pearson’s correlations between within-participant model performance (Pearson’s <italic>r</italic>; see <xref ref-type="fig" rid="pone.0211735.g004">Fig 4</xref>) and the logarithm of within-participant human rating standard deviation (<italic>SD</italic>). Human-rated <italic>SD</italic>s were computed as the logarithm of the <italic>SD</italic> of human coders’ ratings across a given participants’ recordings. Cases with zero variance in human ratings (i.e., all ratings were “1”) are excluded from this analysis. Correlations and the number of participants included in each comparison are superimposed on their respective graphs. All correlations are significant (<italic>p</italic>s &lt; 0.001). (B) Pearson’s Correlations between within-participant model performance (see <xref ref-type="fig" rid="pone.0211735.g004">Fig 4</xref>) and the logarithm of within-participant computer rating standard deviation. Computer-rated <italic>SD</italic>s were computed in the same way as human-rated <italic>SD</italic>s, but the model estimates were used in place of the true human ratings. All correlations are significant (<italic>p</italic>s &lt; 0.001).</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0211735.s003" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pone.0211735.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Coder-specific AU importance measures.</title>
<p>Partial dependence scores (not normalized to show relative differences) extracted from the RF model fit separately to each coder. Coders all show similarly ordered importance profiles, suggesting that they attended to similar facial expressions while generating emotion ratings. Note that positive importance estimates are distributed across fewer predictors (i.e., AUs 6, 12, and 18), whereas negative importance estimates are more spread out throughout all predictors. Agreement between all three individual coders’ importance profiles was high, with ICC(3)s of .93 and .90 for positive and negative ratings, respectively.</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0211735.s004" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pone.0211735.s004" xlink:type="simple">
<label>S4 Fig</label>
<caption>
<title>Number of recordings necessary to accurately estimate AU importance.</title>
<p>Grid searches over the number of recordings/ratings necessary to achieve reliable estimates of AU importances for each valence-coder pair (coders appear in the same order as in <xref ref-type="supplementary-material" rid="pone.0211735.s003">S3 Fig</xref>). Reliability is indexed by the ICC(2) between AU importance profiles (i.e. <italic>partial dependence</italic>) extracted from the model fit to all the recordings that coders rated versus the model fit to subsets of recordings that they rated. Note that the ICC(2) assumes that importance estimates are “average” units (similar to ICC(3)s in <xref ref-type="fig" rid="pone.0211735.g006">Fig 6</xref>). The RF model was fit to each sample of size <italic>n</italic> along the <italic>x</italic>-axis, AU importance profiles were extracted from the model, and ICC(2)s were then calculated between the given sample and full-data AU importance profile scores. We iterated this procedure 20 times within each different sample size to estimate the variation in estimates across recordings. Shading reflects the 2 standard errors from the mean ICC within each sample across all 30 iterations. The red-dashed line indicates an ICC(2) of .75, which is considered “excellent”. For positive ratings, the ICC(2) reached .75 after ~60 recordings/ratings for each coder. For negative ratings, all coders reached an ICC(2) of .75 by ~150 recordings/ratings.</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0211735.s005" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pone.0211735.s005" xlink:type="simple">
<label>S5 Fig</label>
<caption>
<title>Regularized regression model performance.</title>
<p>Results of the Elastic Net with various settings for α (including the LASSO at α = 1 and Ridge Regression at α = 0). Distributions shown are generated in the same way as those in <xref ref-type="fig" rid="pone.0211735.g003">Fig 3</xref>. Model performance was not affected by changes in α, thus, the LASSO model was selected and compared against the RF model.</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0211735.s006" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pone.0211735.s006" xlink:type="simple">
<label>S6 Fig</label>
<caption>
<title>Deep neural network model performance.</title>
<p>Performance of the DNN in both training and test sets across a grid of different numbers of hidden layers and nodes per hidden layer. Note that the RF model performed similarly to the DNN across all the values within the grid.</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0211735.s007" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pone.0211735.s007" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Facial action units detected by FACET.</title>
<p><italic>Note</italic>. Pictures and descriptions of all Action Units used in the current study. Images were adapted from <ext-link ext-link-type="uri" xlink:href="https://www.cs.cmu.edu/~face/facs.htm" xlink:type="simple">https://www.cs.cmu.edu/~face/facs.htm</ext-link>.</p>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0211735.s008" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pone.0211735.s008" xlink:type="simple">
<label>S2 Table</label>
<caption>
<title>Average evidence scores for action units within conditions.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0211735.s009" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pone.0211735.s009" xlink:type="simple">
<label>S1 Supporting Information</label>
<caption>
<title/>
<p>(DOCX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank S. Bowman-Gibson for aiding in the manual quality check for all recordings, and J. Haaser, J. Borden, S. Choudhury, S. Okey, T. St. John, M. Stone, and S. Tolliver for manually coding videos. We also thank J. Cohn, J. Myung, A. Rogers, and H. Hahn for their comments and suggestions on previous drafts of the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0211735.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Krumhuber</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Manstead</surname> <given-names>ASR</given-names></name>, <name name-style="western"><surname>Cosker</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Marshall</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Rosin</surname> <given-names>PL</given-names></name>, <name name-style="western"><surname>Kappas</surname> <given-names>A</given-names></name>. <article-title>Facial dynamics as indicators of trustworthiness and cooperative behavior</article-title>. <source>Emotion</source>. <year>2007</year>;<volume>7</volume>(<issue>4</issue>):<fpage>730</fpage>–<lpage>735</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/1528-3542.7.4.730" xlink:type="simple">10.1037/1528-3542.7.4.730</ext-link></comment> <object-id pub-id-type="pmid">18039040</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reed</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>DeScioli</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pinker</surname> <given-names>S</given-names></name>. <article-title>The Commitment Function of Angry Facial Expressions</article-title>. <source>Psychological Science</source>. <year>2014</year>;<volume>25</volume>(<issue>8</issue>):<fpage>1511</fpage>–<lpage>1517</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0956797614531027" xlink:type="simple">10.1177/0956797614531027</ext-link></comment> <object-id pub-id-type="pmid">24898726</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reed</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Zeglen</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Schmidt</surname> <given-names>K</given-names></name>. <article-title>Facial expressions as honest signals of cooperative intent in a one-shot anonymous Prisoner’s Dilemma game</article-title>. <source>Evol Hum Behav</source>. <year>2012</year>;<volume>33</volume>(<issue>3</issue>):<fpage>200</fpage>–<lpage>209</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref004"><label>4</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Cohn</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Ekman</surname> <given-names>P</given-names></name>. <chapter-title>Measuring facial action</chapter-title>. In: <name name-style="western"><surname>Harrigan</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Rosenthal</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>, editors. <source>The new handbook of nonverbal behavior for research methods in the affective sciences</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>2005</year>. p. <fpage>9</fpage>–<lpage>64</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref005"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Ekman P, Friesen W, Hager JC. Facial action coding system: The manual on CD ROM. [CD-ROM]. Salt Lake City; 2002.</mixed-citation></ref>
<ref id="pone.0211735.ref006"><label>6</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Ekman</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Rosenberg</surname> <given-names>EL</given-names></name>. <source>What the face reveals: basic and applied studies of spontaneous expression using the facial action coding system (FACS)</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>2005</year>.</mixed-citation></ref>
<ref id="pone.0211735.ref007"><label>7</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Keltner</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ekman</surname> <given-names>P</given-names></name>. <chapter-title>Facial expression of emotion</chapter-title>. In: <name name-style="western"><surname>Lewis</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Haviland-Jones</surname> <given-names>JM</given-names></name>, editors. <source>Handbook of emotions</source>. <edition>2nd ed</edition>. <publisher-loc>New York</publisher-loc>: <publisher-name>Guilford Press</publisher-name>; <year>2000</year>. P. <fpage>236</fpage>–<lpage>249</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ekman</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Davidson</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Friesen</surname> <given-names>W</given-names></name>. <article-title>The Duchenne smile: Emotional expression and brain physiology: II</article-title>. <source>Journal of Personality and Social Psychology</source>. <year>1990</year>;<volume>58</volume>(<issue>2</issue>):<fpage>342</fpage>–<lpage>353</lpage>. <object-id pub-id-type="pmid">2319446</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brown</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>GE</given-names></name>. <article-title>Relationships between facial electromyography and subjective experience during affective imagery</article-title>. <source>Biol Psychol</source>. <year>1980</year>;<volume>11</volume>(<issue>1</issue>):<fpage>49</fpage>–<lpage>62</lpage>. <object-id pub-id-type="pmid">7248403</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Du</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tao</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Martinez</surname> <given-names>AM</given-names></name>. <article-title>Compound facial expressions of emotion</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2014</year>;<volume>111</volume>(<issue>15</issue>):<fpage>E1454</fpage>–<lpage>E1462</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1322355111" xlink:type="simple">10.1073/pnas.1322355111</ext-link></comment> <object-id pub-id-type="pmid">24706770</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Martinez</surname> <given-names>AM</given-names></name>. <article-title>Visual Perception of facial expressions of emotion</article-title>. <source>Curr Opin Psychol</source>. <year>2017</year>;<volume>17</volume>:<fpage>27</fpage>–<lpage>33</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.copsyc.2017.06.009" xlink:type="simple">10.1016/j.copsyc.2017.06.009</ext-link></comment> <object-id pub-id-type="pmid">28950969</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Srinivasan</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Golomb</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Martinez</surname> <given-names>AM</given-names></name>. <article-title>A neural basis of facial action recognition in humans</article-title>. <source>J Neurosci</source>. <year>2017</year>;<volume>36</volume>(<issue>16</issue>): <fpage>4434</fpage>–<lpage>4442</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Russell</surname> <given-names>JA</given-names></name>. <article-title>A circumplex model of affect</article-title>. <source>J Pers Soc Psychol</source>. <year>1980</year>;<volume>39</volume>(<issue>6</issue>):<fpage>1161</fpage>–<lpage>1178</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schlosberg</surname> <given-names>H</given-names></name>. <article-title>Three dimensions of emotion</article-title>. <source>Psychol Rev</source>. <year>1954</year>;<volume>61</volume>(<issue>2</issue>):<fpage>81</fpage>–<lpage>88</lpage>. <object-id pub-id-type="pmid">13155714</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Watson</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Tellegen</surname> <given-names>A</given-names></name>. <article-title>Toward a consensual structure of mood</article-title>. <source>Psychol Bull</source>. <year>1985</year>; <volume>98</volume>(<issue>2</issue>):<fpage>2918</fpage>–<lpage>235</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Ellsworth</surname> <given-names>PC</given-names></name>. <article-title>Patterns of cognitive appraisal in emotion</article-title>. <source>J Pers Soc Psychol</source>. <year>1985</year>;<volume>48</volume>(<issue>4</issue>):<fpage>813</fpage>–<lpage>838</lpage>. <object-id pub-id-type="pmid">3886875</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cacioppo</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Petty</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Losch</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>HS</given-names></name>. <article-title>Electromyographic activity over facial muscle regions can differentiate the valence and intensity of affective reactions</article-title>. <source>J Pers Soc Psychol</source>. <year>1986</year>;<volume>50</volume>(<issue>2</issue>):<fpage>260</fpage>–<lpage>268</lpage>. <object-id pub-id-type="pmid">3701577</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Larsen</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Norris</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Cacioppo</surname> <given-names>JT</given-names></name>. <article-title>Effects of positive and negative affect on electromyographic activity over zygomaticus major and corrugator supercilii</article-title>. <source>Psychophysiology</source>. <year>2003</year>;<volume>40</volume>:<fpage>776</fpage>–<lpage>785</lpage>. <object-id pub-id-type="pmid">14696731</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Messinger</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Mattson</surname> <given-names>WI</given-names></name>, <name name-style="western"><surname>Mahoor</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Colm</surname> <given-names>JF</given-names></name>. <article-title>The eyes have it: making positive expressions more positive and negative expressions more negative</article-title>. <source>Emotion</source>. <year>2012</year>;<volume>12</volume>(<issue>3</issue>):<fpage>430</fpage>–<lpage>436</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0026498" xlink:type="simple">10.1037/a0026498</ext-link></comment> <object-id pub-id-type="pmid">22148997</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barrett</surname> <given-names>LF</given-names></name>. <article-title>Valence is a basic building block of emotional life</article-title>. <source>Journal of Research in Personality</source>. <year>2006</year>;<volume>40</volume>(<issue>1</issue>):<fpage>35</fpage>–<lpage>55</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bartlett</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Hager</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Ekman</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Sejnowski</surname> <given-names>TJ</given-names></name>. <article-title>Measuring facial expressions by computer image analysis</article-title>. <source>Psychophysiology</source>. <year>2003</year>;<volume>36</volume>(<issue>2</issue>):<fpage>253</fpage>–<lpage>263</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref022"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">Baltrusaitis T, Robinson P, Morency LP. Openface: an open source facial behavior analysis toolkit. In: 2016 IEEE Winter conference on Applications of Computer Vision; 2016 March 7–9; Lake Placid, NY.</mixed-citation></ref>
<ref id="pone.0211735.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lewinski</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>den Uyl</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Butler</surname> <given-names>C</given-names></name>. <article-title>Automated facial coding: Validation of basic emotions and FACS AUs in FaceReader</article-title>. <source>J Neurosci Psychol Econ</source>. <year>2014</year>; <volume>7</volume>(<issue>4</issue>):<fpage>227</fpage>–<lpage>236</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stöckli</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Schulte-Mecklenbeck</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Borer</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Samson</surname> <given-names>AC</given-names></name>. <article-title>Facial expression analysis with AFFDEX and FACET: a validation study</article-title>. <source>Behav Res Methods</source>. <year>2017</year>;<volume>26</volume>(<issue>5</issue>):<fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sikka</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Ahmed</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Diaz</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Goodwin</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Craig</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Bartlett</surname> <given-names>MS</given-names></name>, <etal>et al</etal>. <article-title>Automated assessment of children’s postoperative pain using computer vision</article-title>. <source>Pediatrics</source>. <year>2015</year>;<volume>136</volume>(<issue>1</issue>):<fpage>e124</fpage>–<lpage>e131</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1542/peds.2015-0029" xlink:type="simple">10.1542/peds.2015-0029</ext-link></comment> <object-id pub-id-type="pmid">26034245</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref026"><label>26</label><mixed-citation publication-type="other" xlink:type="simple">Dibeklioğlu, Hammal Z, Yang Y, Cohn JF. Multimodal detection of depression in clinical interviews. In: 2015 ACM on International Conference on Multimodal Interaction—ICMI ‘15; 2015 Nov 9–13; New York, New York, USA. ACM Press; 2015.</mixed-citation></ref>
<ref id="pone.0211735.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kotsia</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Pitas</surname> <given-names>I</given-names></name>. <article-title>Facial expression recognition in image sequences using geometric deformation features and support vector machines</article-title>. <source>IEEE Trans Image Process</source>. <year>2007</year>;<volume>16</volume>(<issue>1</issue>):<fpage>172</fpage>–<lpage>187</lpage>. <object-id pub-id-type="pmid">17283776</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gunes</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Pantic</surname> <given-names>M</given-names></name>. <article-title>Automatic, dimensional, and continuous emotion recognition</article-title>. <source>International Journal of Synthetic Emotions</source>. <year>2010</year>; <volume>1</volume>(<issue>1</issue>):<fpage>68</fpage>–<lpage>99</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref029"><label>29</label><mixed-citation publication-type="other" xlink:type="simple">Ringeval F, Schuller B, Valstar M, Jaiswal S, Marchi E, Lalanne D, et al. AV+EC 2015: The First Affect Recognition Challenge Bridging Across Audio, Video, and Physiological Data. In: 5th International Workshop on Audio/Visual Emotion Challenge; 2015 Oct 26–30; Brisbane, Australia.</mixed-citation></ref>
<ref id="pone.0211735.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mollahosseini</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hasani</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Mahoor</surname> <given-names>MH</given-names></name>. <article-title>AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild</article-title>. <source>IEEE Trans Affect Comput</source>. <year>2017</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TAFFC.2016.2582490" xlink:type="simple">10.1109/TAFFC.2016.2582490</ext-link></comment></mixed-citation></ref>
<ref id="pone.0211735.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nicolaou</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Gunes</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Pantic</surname> <given-names>M</given-names></name>. (2011). <article-title>Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space</article-title>. <source>IEEE Trans Affect Comput</source>. <year>2011</year>;<volume>2</volume>:<fpage>92</fpage>–<lpage>105</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Southward</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Cheavens</surname> <given-names>JS</given-names></name>. (2017). <article-title>Assessing the relation between flexibility in emotional expression and symptoms of anxiety and depression: The roles of context sensitivity and feedback sensitivity</article-title>. <source>J Soc Clin Psychol</source>. <month>Feb</month> <year>2017</year>;<volume>36</volume>(<issue>2</issue>):<fpage>142</fpage>–<lpage>157</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1521/jscp.2017.36.2.142" xlink:type="simple">10.1521/jscp.2017.36.2.142</ext-link></comment> <object-id pub-id-type="pmid">28490833</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kossaifi</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Tzimiropoulos</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Todorovic</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pantic</surname> <given-names>M</given-names></name>. (2017). <article-title>AFEW-VA database for valence and arousal estimation in-the-wild</article-title>. <source>Image Vis Comput</source>. <month>Sept</month> <year>2017</year>;<volume>65</volume>:<fpage>23</fpage>–<lpage>26</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Haamer</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Rusadze</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Lüsi</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Ahmed</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Sergio</surname> <given-names>Escalera</given-names></name>, <etal>et al</etal>. <article-title>Review on Emotion Recognition Databases | IntechOpen</article-title> [Internet]. <source>Intech open</source>. IntechOpen; <year>2018</year> [cited 2018Sep23]. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.intechopen.com/books/human-robot-interaction-theory-and-application/review-on-emotion-recognition-databases" xlink:type="simple">https://www.intechopen.com/books/human-robot-interaction-theory-and-application/review-on-emotion-recognition-databases</ext-link></mixed-citation></ref>
<ref id="pone.0211735.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bailenson</surname> <given-names>JN</given-names></name>, <name name-style="western"><surname>Pontikakis</surname> <given-names>ED</given-names></name>, <name name-style="western"><surname>Mauss</surname> <given-names>IB</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Jabon</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Hutcherson</surname> <given-names>CAC</given-names></name>, <etal>et al</etal>. <article-title>Real-time classification of evoked emotions using facial feature tracking and physiological responses</article-title>. <source>Int J Hum Comput Stud</source>. <year>2008</year>;<volume>66</volume>(<issue>5</issue>):<fpage>303</fpage>–<lpage>317</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Watson</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Clark</surname> <given-names>LA</given-names></name>, <name name-style="western"><surname>Tellegen</surname> <given-names>A</given-names></name>. <article-title>Development and validation of brief measures of positive and negative affect: The PANAS scales</article-title>. <source>J Pers Soc Psychol</source>. <year>1998</year>;<volume>54</volume>(<issue>6</issue>):<fpage>1063</fpage>–<lpage>1070</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bonanno</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Papa</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lalande</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Westphal</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Coifman</surname> <given-names>K</given-names></name>. <article-title>The importance of being flexible: The ability to both enhance and suppress emotional expression predicts long-term adjustment</article-title>. <source>Psychol Sci</source>. <year>2004</year>;<volume>15</volume>(<issue>7</issue>):<fpage>482</fpage>–<lpage>487</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.0956-7976.2004.00705.x" xlink:type="simple">10.1111/j.0956-7976.2004.00705.x</ext-link></comment> <object-id pub-id-type="pmid">15200633</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref038"><label>38</label><mixed-citation publication-type="other" xlink:type="simple">Lang PJ, Bradley MM, Cuthbert BN. International Affective Picture System (IAPS): Technical manual and affective ratings (Technical Report A-4). Gainesville, FL; 1995.</mixed-citation></ref>
<ref id="pone.0211735.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McGraw</surname> <given-names>KO</given-names></name>, <name name-style="western"><surname>Wong</surname> <given-names>SP</given-names></name>. (1996). <article-title>Forming inferences about some intraclass correlation coefficients</article-title>. <source>Psychol Methods</source>. <year>1996</year>;<volume>1</volume>(<issue>4</issue>):<fpage>30</fpage>–<lpage>46</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref040"><label>40</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Hastie</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tibshirani</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Friedman</surname> <given-names>J</given-names></name>. <source>The elements of statistical learning</source>. <publisher-loc>New York(NY)</publisher-loc>: <publisher-name>Springer New York</publisher-name>; <year>2009</year>.</mixed-citation></ref>
<ref id="pone.0211735.ref041"><label>41</label><mixed-citation publication-type="other" xlink:type="simple">Littlewort G, Whitehill J, Wu T, Fasel I, Frank M, Movellan J, Bartlett M. The computer expression recognition toolbox (CERT). In Automatic Face &amp; Gesture Recognition and Workshops (FG 2011), 2011 IEEE International Conference on 2011 Mar 21 (pp. 298–305). IEEE.</mixed-citation></ref>
<ref id="pone.0211735.ref042"><label>42</label><mixed-citation publication-type="other" xlink:type="simple">Amirian M, Kächele M, Thiam P, Kessler V, Schwenker F. Continuous Multimodal Human Affect Estimation using Echo State Networks: Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge; 2016 Oct 16; Amsterdam, Netherlands. Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge; 2016. p. 67–74.</mixed-citation></ref>
<ref id="pone.0211735.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pu</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Fan</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Ji</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>Z</given-names></name>. <article-title>Facial expression recognition from image sequences using twofold random forest classifier</article-title>. <source>Neurocomputing</source>. <year>2015</year>;<volume>168</volume>:<fpage>1173</fpage>–<lpage>1180</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref044"><label>44</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Rosenthal</surname> <given-names>R</given-names></name>. <chapter-title>Conducting judgment studies: some methodological issues</chapter-title>. In: <name name-style="western"><surname>Harrigan</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Rosenthal</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>, editors. <source>Series in Affective Science. The new handbook of methods in nonverbal behavior research</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>2005</year>. p. <fpage>199</fpage>–<lpage>234</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ahn</surname> <given-names>WY</given-names></name>, <name name-style="western"><surname>Hendricks</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Haines</surname> <given-names>N</given-names></name>. <source>Easyml: Easily Build and Evaluate Machine Learning Models</source>. bioRxiv. <year>2017</year>.</mixed-citation></ref>
<ref id="pone.0211735.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liaw</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wiener</surname> <given-names>M</given-names></name>. <source>R News</source>. <year>2002</year><month>Dec</month>;</mixed-citation></ref>
<ref id="pone.0211735.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ahn</surname> <given-names>WY</given-names></name>, <name name-style="western"><surname>Ramesh</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Moeller</surname> <given-names>FG</given-names></name>, <name name-style="western"><surname>Vassileva</surname> <given-names>J</given-names></name>. <article-title>Utility of machine-learning approaches to identify behavioral markers for substance use disorders: impulsivity dimensions as predictors of current cocaine dependence</article-title>. <source>Front in Psychiatry</source>. <year>2016</year>; <volume>7</volume>:<fpage>290</fpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ahn</surname> <given-names>WY</given-names></name>, <name name-style="western"><surname>Vassileva</surname> <given-names>J</given-names></name>. <article-title>Machine-learning identifies substance-specific behavioral markers for opiate and stimulant dependence</article-title>. <source>Drug Alcohol Depend</source>. <year>2016</year>;<volume>161</volume>:<fpage>247</fpage>–<lpage>257</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.drugalcdep.2016.02.008" xlink:type="simple">10.1016/j.drugalcdep.2016.02.008</ext-link></comment> <object-id pub-id-type="pmid">26905209</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Landis</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>GG</given-names></name>. <article-title>The measurement of observer agreement for categorical data</article-title>. <source>Biometrics</source>. <year>1997</year>;<volume>33</volume>(<issue>1</issue>):<fpage>159</fpage>–<lpage>174</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Greenwell</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Boehmke</surname> <given-names>BC</given-names></name>, <name name-style="western"><surname>McCarthy</surname> <given-names>AJ</given-names></name>. <source>A Simple and Effective Model-Based Variable Importance Measure</source>. arXiv preprint arXiv:1805.04755. <year>2018</year> <month>May</month> <day>12</day>.</mixed-citation></ref>
<ref id="pone.0211735.ref051"><label>51</label><mixed-citation publication-type="other" xlink:type="simple">Boukricha H, Wachsmuth I, Hofstätter A, Grammer K. Pleasure-arousal-dominance driven facial expression simulation. In Affective Computing and Intelligent Interaction and Workshops, 2009. ACII 2009. 3rd International Conference on 2009 Sep 10 (pp. 1–7). IEEE.</mixed-citation></ref>
<ref id="pone.0211735.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Korb</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>With</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Niedenthal</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Kaiser</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Grandjean</surname> <given-names>D</given-names></name>. <article-title>The perception and mimicry of facial movements predict judgments of smile authenticity</article-title>. <source>PLoS ONE</source>. <year>2014</year>; <volume>9</volume>(<issue>6</issue>):<fpage>e99194</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0099194" xlink:type="simple">10.1371/journal.pone.0099194</ext-link></comment> <object-id pub-id-type="pmid">24918939</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frank</surname> <given-names>MG</given-names></name>, <name name-style="western"><surname>Ekman</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Friesen</surname> <given-names>WV</given-names></name>. <article-title>Behavioral markers and recognizability of the smile of enjoyment</article-title>. <source>J Pers Soc Psychol</source>. <year>1993</year>; <volume>64</volume>(<issue>1</issue>):<fpage>83</fpage>–<lpage>93</lpage>. <object-id pub-id-type="pmid">8421253</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Belsky</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hsieh</surname> <given-names>KH</given-names></name>, <name name-style="western"><surname>Crnic</surname> <given-names>K</given-names></name>. <article-title>Infant positive and negative emotionality: One dimension or two?</article-title> <source>Dev Psychol</source>. <year>1996</year>;<volume>32</volume>(<issue>2</issue>):<fpage>289</fpage>–<lpage>298</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Del Líbano</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Calvo</surname> <given-names>MG</given-names></name>, <name name-style="western"><surname>Fernández-Martín</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Recio</surname> <given-names>G</given-names></name>. <article-title>Discrimination between smiling faces: Human observers vs. automated face analysis</article-title>. <source>Acta Psychologica</source>. <year>2018</year> <month>June</month> <day>30</day>;<volume>187</volume>:<fpage>19</fpage>–<lpage>29</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.actpsy.2018.04.019" xlink:type="simple">10.1016/j.actpsy.2018.04.019</ext-link></comment> <object-id pub-id-type="pmid">29758397</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Erickson</surname> <given-names>K</given-names></name>,<name name-style="western"><surname>Schulkin</surname> <given-names>J</given-names></name>. <article-title>Facial expressions of emotion: A cognitive neuroscience perspective</article-title>. <source>Brain Cog</source>. <year>2003</year>;<volume>52</volume>(<issue>1</issue>):<fpage>52</fpage>–<lpage>60</lpage>.</mixed-citation></ref>
<ref id="pone.0211735.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Izard</surname> <given-names>CE</given-names></name>. <article-title>Basic emotions, relations among emotions, and emotion-cognition relations</article-title>. <source>Psychol Rev</source>. <year>1992</year>; <volume>99</volume>(<issue>3</issue>):<fpage>561</fpage>–<lpage>565</lpage>. <object-id pub-id-type="pmid">1502277</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Haines</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Rass</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Shin</surname> <given-names>YW</given-names></name>, <name name-style="western"><surname>Busemeyer</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>O’Donnell</surname> <given-names>B</given-names></name>, <etal>et al</etal>. (in preparation). <source>Regret induces rapid learning from experience-based decisions: A model-based facial expression analysis approach</source>.</mixed-citation></ref>
<ref id="pone.0211735.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Celani</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Battacchi</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Arcidiacono</surname> <given-names>L</given-names></name>. <article-title>The understanding of the emotional meaning of facial expressions in people with autism</article-title>.<source>J Autism Dev Disord</source>. <year>1999</year>;<volume>29</volume>(<issue>1</issue>):<fpage>57</fpage>–<lpage>66</lpage>. <object-id pub-id-type="pmid">10097995</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Philippot</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Kornreich</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Blairy</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Baert</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Dulk</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Bon</surname> <given-names>OL</given-names></name>, <etal>et al</etal>. (1999). <article-title>Alcoholics’ deficits in the decoding of emotional facial expression</article-title>. <source>Alcohol Clin Exp Res</source>. <year>1999</year>;<volume>23</volume>(<issue>6</issue>):<fpage>1031</fpage>–<lpage>1038</lpage>. <object-id pub-id-type="pmid">10397287</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rubinow D</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Post</surname> <given-names>RM</given-names></name>. <article-title>Impaired recognition of affect in facial expression in depressed patients</article-title>. <source>Biol Psychiatry</source>. <year>1992</year>;<volume>31</volume>(<issue>9</issue>): <fpage>947</fpage>–<lpage>953</lpage>. <object-id pub-id-type="pmid">1637932</object-id></mixed-citation></ref>
<ref id="pone.0211735.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ahn</surname> <given-names>W-Y</given-names></name>, <name name-style="western"><surname>Busemeyer</surname> <given-names>JR</given-names></name>. <article-title>Challenges and promises for translating computational tools into clinical practice</article-title>. <source>Curr Opin Behav Sci</source>. <year>2016</year>; <volume>1</volume>:<fpage>1</fpage>–<lpage>7</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>