<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group>
<journal-title>PLoS ONE</journal-title></journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-13-19073</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0085175</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Circuit models</subject><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Cognition</subject><subject>Decision making</subject><subject>Motor reactions</subject></subj-group></subj-group><subj-group><subject>Sensory systems</subject><subj-group><subject>Visual system</subject></subj-group></subj-group><subj-group><subject>Learning and memory</subject><subject>Motor systems</subject><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Computer science</subject><subj-group><subject>Algorithms</subject></subj-group><subj-group><subject>Computer architecture</subject><subj-group><subject>Computer hardware</subject></subj-group></subj-group><subj-group><subject>Computing systems</subject><subj-group><subject>Hybrid computing</subject></subj-group></subj-group><subj-group><subject>Text mining</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Engineering</subject><subj-group><subject>Electrical engineering</subject></subj-group><subj-group><subject>Electronics engineering</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Physics</subject><subj-group><subject>Solid state physics</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>AHaH Computing–From Metastable Switches to Attractors to Machine Learning</article-title>
<alt-title alt-title-type="running-head">AHaH Computing</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Nugent</surname><given-names>Michael Alexander</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Molter</surname><given-names>Timothy Wesley</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>M. Alexander Nugent Consulting, Santa Fe, New Mexico, United States of America</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>KnowmTech LLC, Albuquerque, New Mexico, United States of America</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Xeiam LLC, Santa Fe, New Mexico, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Abbott</surname><given-names>Derek</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Adelaide, Australia</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">i@alexnugent.name</email></corresp>
<fn fn-type="conflict"><p>The authors of this paper have a financial interest in the technology derived from the work presented in this paper. Patents include the following: US6889216, Physical neural network design incorporating nanotechnology; US6995649, Variable resistor apparatus formed utilizing nanotechnology; US7028017, Temporal summation device utilizing nanotechnology; US7107252, Pattern recognition utilizing a nanotechnology-based neural network; US7398259, Training of a physical neural network; US7392230, Physical neural network liquid state machine utilizing nanotechnology; US7409375, Plasticity-induced self organizing nanotechnology for the extraction of independent components from a data stream; US7412428, Application of hebbian and anti-hebbian learning to nanotechnology-based physical neural networks; US7420396, Universal logic gate utilizing nanotechnology; US7426501, Nanotechnology neural network methods and systems; US7502769, Fractal memory and computational methods and systems based on nanotechnology; US7599895, Methodology for the configuration and repair of unreliable switching elements; US7752151, Multilayer training in a physical neural network formed utilizing nanotechnology; US7827131, High density synapse chip using nanoparticles; US7930257, Hierarchical temporal memory utilizing nanotechnology; US8041653, Method and system for a hierarchical temporal memory utilizing a router hierarchy and hebbian and anti-hebbian learning; US8156057, Adaptive neural network utilizing nanotechnology-based components. Additional patents are pending. Authors of the paper are owners of the commercial companies performing this work. Companies include the following: Cover Letter; KnowmTech LLC, Intellectual Property Holding Company: Author Alex Nugent is a Co-owner; M. Alexander Nugent Consulting, Research and Development: Author Alex Nugent is owner and Tim Molter employee; Xeiam LLC, Technical Architecture: Authors Tim Molter and Alex Nugent are co-owners. Products resulting from the technology described in this paper are currently being developed. This does not alter the authors’ adherence to all the PLOS ONE policies on sharing data and materials. The authors agree to make freely available any materials and data described in this publication that may be reasonably requested for the purpose of academic, non-commercial research. As part of this, the authors have open-sourced all code and data used to generated the results of this paper under a “M. Alexander Nugent Consulting Research License”.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: MAN TWM. Performed the experiments: MAN TWM. Analyzed the data: MAN TWM. Contributed reagents/materials/analysis tools: MAN TWM. Wrote the paper: MAN TWM.</p></fn>
</author-notes>
<pub-date pub-type="collection"><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>10</day><month>2</month><year>2014</year></pub-date>
<volume>9</volume>
<issue>2</issue>
<elocation-id>e85175</elocation-id>
<history>
<date date-type="received"><day>7</day><month>5</month><year>2013</year></date>
<date date-type="accepted"><day>23</day><month>11</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Nugent, Molter</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Modern computing architecture based on the separation of memory and processing leads to a well known problem called the von Neumann bottleneck, a restrictive limit on the data bandwidth between CPU and RAM. This paper introduces a new approach to computing we call AHaH computing where memory and processing are combined. The idea is based on the attractor dynamics of volatile dissipative electronics inspired by biological systems, presenting an attractive alternative architecture that is able to adapt, self-repair, and learn from interactions with the environment. We envision that both von Neumann and AHaH computing architectures will operate together on the same machine, but that the AHaH computing processor may reduce the power consumption and processing time for certain adaptive learning tasks by orders of magnitude. The paper begins by drawing a connection between the properties of volatility, thermodynamics, and Anti-Hebbian and Hebbian (AHaH) plasticity. We show how AHaH synaptic plasticity leads to attractor states that extract the independent components of applied data streams and how they form a computationally complete set of logic functions. After introducing a general memristive device model based on collections of metastable switches, we show how adaptive synaptic weights can be formed from differential pairs of incremental memristors. We also disclose how arrays of synaptic weights can be used to build a neural node circuit operating AHaH plasticity. By configuring the attractor states of the AHaH node in different ways, high level machine learning functions are demonstrated. This includes unsupervised clustering, supervised and unsupervised classification, complex signal prediction, unsupervised robotic actuation and combinatorial optimization of procedures–all key capabilities of biological nervous systems and modern machine learning algorithms with real world application.</p>
</abstract>
<funding-group><funding-statement>This work has been supported in part by the Air Force Research Labs (AFRL) and Navy Research Labs (NRL) under the SBIR/STTR programs AF10-BT31, AF121-049 and N12A-T013 (<ext-link ext-link-type="uri" xlink:href="http://www.sbir.gov/about/about-sttr" xlink:type="simple">http://www.sbir.gov/about/about-sttr</ext-link>; <ext-link ext-link-type="uri" xlink:href="http://www.sbir.gov/#" xlink:type="simple">http://www.sbir.gov/#</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="29"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>How does nature compute? Attempting to answer this question naturally leads one to consider biological nervous systems, although examples of computation abound in other manifestations of life. Some examples include plants <xref ref-type="bibr" rid="pone.0085175-Grime1">[1]</xref>–<xref ref-type="bibr" rid="pone.0085175-Scialdone1">[5]</xref>, bacteria <xref ref-type="bibr" rid="pone.0085175-vonBodman1">[6]</xref>, protozoan <xref ref-type="bibr" rid="pone.0085175-Nakagaki1">[7]</xref>, and swarms <xref ref-type="bibr" rid="pone.0085175-Bonabeau1">[8]</xref>, to name a few. Most attempts to understand biological nervous systems fall along a spectrum. One end of the spectrum attempts to mimic the observed physical properties of nervous systems. These models necessarily contain parameters that must be tuned to match the biophysical and architectural properties of the natural model. Examples of this approach include Boahen’s neuromorphic circuit at Stanford University and their Neurogrid processor <xref ref-type="bibr" rid="pone.0085175-Choudhary1">[9]</xref>, the mathematical spiking neuron model of Izhikevich <xref ref-type="bibr" rid="pone.0085175-Izhikevich1">[10]</xref> as well as the large scale modeling of Eliasmith <xref ref-type="bibr" rid="pone.0085175-Eliasmith1">[11]</xref>. The other end of the spectrum abandons biological mimicry in an attempt to algorithmically solve the problems associated with brains such as perception, planning and control. This is generally referred to as machine learning. Algorithmic examples include support vector maximization <xref ref-type="bibr" rid="pone.0085175-Boser1">[12]</xref>, <italic>k</italic>-means clustering <xref ref-type="bibr" rid="pone.0085175-MacQueen1">[13]</xref> and random forests <xref ref-type="bibr" rid="pone.0085175-Breiman1">[14]</xref>. Many approaches fall somewhere along the spectrum between mimicry and machine learning, such as the CAVIAR <xref ref-type="bibr" rid="pone.0085175-SerranoGotarredona1">[15]</xref> and Cognimem <xref ref-type="bibr" rid="pone.0085175-Sardar1">[16]</xref> neuromorphic processors as well as IBM’s <italic>neurosynaptic core</italic> <xref ref-type="bibr" rid="pone.0085175-Arthur1">[17]</xref>. In this paper we consider an alternative approach outside of the typical spectrum by asking ourselves a simple but important question: How can a brain compute given that it is built of volatile components?</p>
<p>A brain, like all living systems, is a far-from-equilibrium energy dissipating structure that constantly builds and repairs itself. We can shift the standard question from “how do brains compute?” or “what is the algorithm of the brain?” to a more fundamental question of “how do brains build and repair themselves as dissipative attractor-based structures?” Just as a ball will roll into a depression, an attractor-based system will fall into its attractor states. Perturbations (damage) will be fixed as the system reconverges to its attractor state. As an example, if we cut ourselves <italic>we heal</italic>. To bestow this property on our computing technology we must find a way to represent our computing structures as attractors. In this paper we detail how the attractor points of a plasticity rule we call Anti-Hebbian and Hebbian (AHaH) plasticity are computationally complete logic functions as well as building blocks for machine learning functions. We further show that AHaH plasticity can be attained from simple memristive circuitry attempting to maximize circuit power dissipation in accordance with ideas in nonequilibrium thermodynamics.</p>
<p>Our goal is to lay a foundation for a new type of practical computing based on the configuration and repair of volatile switching elements. We traverse the large gap from volatile memristive devices to demonstrations of computational universality and machine learning. The reader should keep in mind that the subject matter in this paper is necessarily diverse, but is essentially an elaboration of these three points:</p>
<list list-type="order"><list-item>
<p>AHaH plasticity emerges from the interaction of volatile competing energy dissipating pathways.</p>
</list-item><list-item>
<p>AHaH plasticity leads to attractor states that can be used for universal computation and advanced machine learning</p>
</list-item><list-item>
<p>Neural nodes operating AHaH plasticity can be constructed from simple memristive circuits.</p>
</list-item></list>
<sec id="s1a">
<title>The Adaptive Power Problem</title>
<p>Through constant dissipation of free energy, living systems continuously repair their seemingly fragile state. A byproduct of this condition is that living systems are intrinsically adaptive at all scales, from cells to ecosystems. This presents a difficult challenge when we attempt to simulate such large scale adaptive networks with modern von Neumann computing architectures. Each adaptation event must necessarily reduce to memory–processor communication as the state variables are modified. The energy consumed in shuttling information back and forth grows in line with the number of state variables that must be continuously modified. For large scale adaptive systems like the brain, the inefficiencies become so large as to make simulations impractical.</p>
<p>As an example, consider that IBM’s recent cat-scale cortical simulation of 1 billion neurons and 10 trillion synapses <xref ref-type="bibr" rid="pone.0085175-Ananthanarayanan1">[18]</xref> required 147,456 CPUs, 144 TB of memory, running at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e010" xlink:type="simple"/></inline-formula> real-time. At a power consumption of 20 W per CPU, this is 2.9 MW. Under perfect scaling, a real-time simulation of a human-scale cortex would dissipate over 7 GW of power. The number of adaptive variables under constant modification in the IBM simulation is orders of magnitude less than the biological counterpart and yet its power dissipation is orders of magnitude larger. Another example from Google to train neural networks on YouTube data roughly doubled the accuracy from previous attempts <xref ref-type="bibr" rid="pone.0085175-Le1">[19]</xref>. The effort took an array of 16,000 CPU cores working at full capacity for 3 days. The model contained 1 billion connections, which although impressive pales in comparison to biology. The average human neocortex contains 150,000 billion connections <xref ref-type="bibr" rid="pone.0085175-Pakkenberg1">[20]</xref> and the number of synapses in the neocortex is a fraction of the total number of connections in the brain. At 20 W per core, Google’s simulation consumed about 320 kW. Under perfect scaling, a human-scale simulation would dissipate 48 GW of power.</p>
<p>At the core of the adaptive power problem is the energy wasted during memory–processor communication. The ultimate solution to the problem entails finding ways to let memory configure itself, and AHaH computing is one such method.</p>
</sec><sec id="s1b">
<title>The Adaptive Power Solution</title>
<p>Consider two switches, one non-volatile and the other volatile. Furthermore, consider what it takes to change the state of each of these switches, which is the most fundamental act of adaptation or reconfiguration. Abstractly, a switch can be represented as a potential energy well with two or more minima.</p>
<p>In the non-volatile case, sufficient energy must be applied to overcome the barrier potential. Energy must be dissipated in proportion to the barrier height once a switching event takes place. Rather than just the switch, it is also the electrode leading to the switch that must be raised to the switch barrier energy. As the number of adaptive variables increases, the power required to sustain the switching events scales as the total distance needed to communicate the switching events and the square of the voltage.</p>
<p>A volatile switch on the other hand cannot be read without damaging its state. Each read operation lowers the switch barriers and increases the probability of random state transitions. Accumulated damage to the state must be actively repaired. In the absence of repair, the act of reading the state is alone sufficient to induce state transitions. The distance that must be traversed between memory and processing of an adaptation event goes to zero as the system becomes intrinsically adaptive. The act of accessing the memory <italic>becomes</italic> the act of configuring the memory.</p>
<p>In the non-volatile case some process external to the switch (i.e. an algorithm on a CPU) must provide the energy needed to effect the state transition. In the volatile case an external process must <italic>stop</italic> providing the energy needed for state repair. These two antisymmetric conditions can be summarized as: “Stability for free, adaptation for a price” and “adaptation for free, stability for a price”, respectively.</p>
<p>Not only does it make physical sense to build large scale adaptive systems from volatile components but furthermore there is no supporting evidence to suggest it is possible to do the contrary. A brain is a volatile dissipative out-of-equilibrium structure. It is therefore reasonable that a volatile solution to machine learning at low power and high densities exists. The goal of AHaH computing is to find and exploit this solution.</p>
</sec><sec id="s1c">
<title>Historical Background</title>
<p>In 1936, Turing, best known for his pioneering work in computation and his seminal paper ‘On computable numbers’ <xref ref-type="bibr" rid="pone.0085175-Turing1">[21]</xref>, provided a formal proof that a machine could be constructed to be capable of performing any conceivable mathematical computation if it were representable as an algorithm. This work rapidly evolved to become the computing industry of today. Few people are aware that, in addition to the work leading to the digital computer, Turing anticipated connectionism and neuron-like computing. In his paper ‘Intelligent machinery’ <xref ref-type="bibr" rid="pone.0085175-Turing2">[22]</xref>, which he wrote in 1948 but was not published until well after his death in 1968, Turing described a machine that consists of artificial neurons connected in any pattern with modifier devices. Modifier devices could be configured to pass or destroy a signal, and the neurons were composed of NAND gates that Turing chose because any other logic function can be created from them.</p>
<p>In 1944, physicist Schrödinger published the book <italic>What is Life?</italic> based on a series of public lectures delivered at Trinity College in Dublin. Schrödinger asked the question: “How can the events in space and time which take place within the spatial boundary of a living organism be accounted for by physics and chemistry?” He described an aperiodic crystal that predicted the nature of DNA, yet to be discovered, as well as the concept of <italic>negentropy</italic> being the entropy of a living system that it exports to keep its own entropy low <xref ref-type="bibr" rid="pone.0085175-Schrdinger1">[23]</xref>.</p>
<p>In 1949, only one year after Turing wrote ‘Intelligent machinery’, synaptic plasticity was proposed as a mechanism for learning and memory by Hebb <xref ref-type="bibr" rid="pone.0085175-Hebb1">[24]</xref>. Ten years later in 1958 Rosenblatt defined the theoretical basis of connectionism and simulated the <italic>perceptron</italic>, leading to some initial excitement in the field <xref ref-type="bibr" rid="pone.0085175-Rosenblatt1">[25]</xref>.</p>
<p>In 1953, Barlow discovered neurons in the frog brain fired in response to specific visual stimuli <xref ref-type="bibr" rid="pone.0085175-Barlow1">[26]</xref>. This was a precursor to the experiments of Hubel and Wiesel who showed in 1959 the existence of neurons in the primary visual cortex of the cat that selectively responds to edges at specific orientations <xref ref-type="bibr" rid="pone.0085175-Hubel1">[27]</xref>. This led to the theory of receptive fields where cells at one level of organization are formed from inputs from cells in a lower level of organization.</p>
<p>In 1960, Widrow and Hoff developed ADALINE, a physical device that used electrochemical plating of carbon rods to emulate the synaptic elements that they called <italic>memistors</italic> <xref ref-type="bibr" rid="pone.0085175-Widrow1">[28]</xref>. Unlike memristors, memistors are three terminal devices, and their conductance between two of the terminals is controlled by the time integral of the current in the third. This work represents the first integration of memristive-like elements with electronic feedback to emulate a learning system.</p>
<p>In 1969, the initial excitement with perceptrons was tampered by the work of Minsky and Papert, who analyzed some of the properties of perceptrons and illustrated how they could not compute the XOR function using only local neurons <xref ref-type="bibr" rid="pone.0085175-Minsky1">[29]</xref>. The reaction to Minsky and Papert diverted attention away from connection networks until the emergence of a number of new ideas, including Hopfield networks (1982) <xref ref-type="bibr" rid="pone.0085175-Hopfield1">[30]</xref>, back propagation of error (1986) <xref ref-type="bibr" rid="pone.0085175-Rumelhart1">[31]</xref>, adaptive resonance theory (1987) <xref ref-type="bibr" rid="pone.0085175-Grossberg1">[32]</xref>, and many other permutations. The wave of excitement in neural networks began to fade as the key problem of generalization versus memorization became better appreciated and the computing revolution took off.</p>
<p>In 1971, Chua postulated on the basis of symmetry arguments the existence of a missing fourth two terminal circuit element called a memristor (<italic>memory resistor</italic>), where the resistance of the memristor depends on the integral of the input applied to the terminals <xref ref-type="bibr" rid="pone.0085175-Chua1">[33]</xref>, <xref ref-type="bibr" rid="pone.0085175-Chua2">[34]</xref>.</p>
<p>VLSI pioneer Mead published with Conway the landmark text <italic>Introduction to VLSI Systems</italic> in 1980 <xref ref-type="bibr" rid="pone.0085175-Mead1">[35]</xref>. Mead teamed with John Hopfield and Feynman to study how animal brains compute. This work helped to catalyze the fields of Neural Networks (Hopfield), Neuromorphic Engineering (Mead) and Physics of Computation (Feynman). Mead created the world’s first neural-inspired chips including an artificial retina and cochlea, which was documented in his book <italic>Analog VLSI Implementation of Neural Systems</italic> published in 1989 <xref ref-type="bibr" rid="pone.0085175-Mead2">[36]</xref>.</p>
<p>Beinenstock, Cooper and Munro published a theory of synaptic modification in 1982 <xref ref-type="bibr" rid="pone.0085175-Bienenstock1">[37]</xref>. Now known as the BCM plasticity rule, this theory attempts to account for experiments measuring the selectivity of neurons in primary sensory cortex and its dependency on neuronal input. When presented with data from natural images, the BCM rule converges to selective oriented receptive fields. This provides compelling evidence that the same mechanisms are at work in cortex, as validated by the experiments of Hubel and Wiesel. In 1989 Barlow reasoned that such selective response should emerge from an unsupervised learning algorithm that attempts to find a factorial code of independent features <xref ref-type="bibr" rid="pone.0085175-Barlow2">[38]</xref>. Bell and Sejnowski extended this work in 1997 to show that the independent components of natural scenes are edge filters <xref ref-type="bibr" rid="pone.0085175-Bell1">[39]</xref>. This provided a concrete mathematical statement on neural plasticity: Neurons modify their synaptic weight to extract independent components. Building a mathematical foundation of neural plasticity, Oja and collaborators derived a number of plasticity rules by specifying statistical properties of the neuron’s output distribution as objective functions. This lead to the principle of <italic>independent component analysis</italic> (ICA) <xref ref-type="bibr" rid="pone.0085175-Hyvrinen1">[40]</xref>, <xref ref-type="bibr" rid="pone.0085175-Comon1">[41]</xref>.</p>
<p>At roughly the same time, the theory of support vector maximization emerged from earlier work on statistical learning theory from Vapnik and Chervonenkis and has become a generally accepted solution to the generalization versus memorization problem in classifiers <xref ref-type="bibr" rid="pone.0085175-Boser1">[12]</xref>, <xref ref-type="bibr" rid="pone.0085175-SchlkopfSimard1">[42]</xref>.</p>
<p>In 2004, Nugent et al. showed how the AHAH plasticity rule is derived via the minimization of a kurtosis objective function and used as the basis of self-organized fault tolerance in support vector machine network classifiers. Thus, the connection that margin maximization coincides with independent component analysis and neural plasticity was demonstrated <xref ref-type="bibr" rid="pone.0085175-Nugent1">[43]</xref>, <xref ref-type="bibr" rid="pone.0085175-Nugent2">[44]</xref>. In 2006, Nugent first detailed how to implement the AHaH plasticity rule in memristive circuitry and demonstrated that the AHaH attractor states can be used to configure a universal reconfigurable logic gate <xref ref-type="bibr" rid="pone.0085175-Nugent3">[45]</xref>–<xref ref-type="bibr" rid="pone.0085175-Nugent5">[47]</xref>.</p>
<p>In 2008, HP Laboratories announced the production of Chua’s postulated electronic device, the memristor <xref ref-type="bibr" rid="pone.0085175-Yang1">[48]</xref> and explored their use as synapses in neuromorphic circuits <xref ref-type="bibr" rid="pone.0085175-Snider1">[49]</xref>. Several memristive devices were previously reported by this time, predating HP Laboratories <xref ref-type="bibr" rid="pone.0085175-Stewart1">[50]</xref>–<xref ref-type="bibr" rid="pone.0085175-Tsubouchi1">[54]</xref>, but they were not described as memristors. In the same year, Hylton and Nugent launched the Systems of Neuromorphic Adaptive Plastic Scalable Electronics (SyNAPSE) program with the goal of demonstrating large scale adaptive learning in integrated memristive electronics at biological scale and power. Since 2008 there has been an explosion of worldwide interest in memristive devices <xref ref-type="bibr" rid="pone.0085175-Oblea1">[55]</xref>–<xref ref-type="bibr" rid="pone.0085175-Jackson1">[59]</xref> device models <xref ref-type="bibr" rid="pone.0085175-Choi1">[60]</xref>–<xref ref-type="bibr" rid="pone.0085175-Biolek1">[65]</xref>, their connection to biological synapses <xref ref-type="bibr" rid="pone.0085175-Chang2">[66]</xref>–<xref ref-type="bibr" rid="pone.0085175-MerrikhBayat3">[72]</xref>, and use in alternative computing architectures <xref ref-type="bibr" rid="pone.0085175-Morabito1">[73]</xref>–<xref ref-type="bibr" rid="pone.0085175-Indiveri1">[84]</xref>.</p>
</sec></sec><sec id="s2">
<title>Theory</title>
<sec id="s2a">
<title>On the Origins of Algorithms and the 4th Law of Thermodynamics</title>
<p>Turing spent the last two years of his life working on mathematical biology and published a paper titled ‘The chemical basis of morphogenesis’ in 1952 <xref ref-type="bibr" rid="pone.0085175-Turing3">[85]</xref>. Turing was likely struggling with the concept that algorithms represent structure, brains and life in general are clearly capable of creating such structure, and brains are ultimately a biological chemical process that emerge from chemical homogeneity. How does complex spatial-temporal structure such as an algorithm emerge from the interaction of a homogeneous collection of units?</p>
<p>Answering this question in a physical sense leads one straight into the controversial 4th law of thermodynamics. The 4th law is is attempting to answer a simple question with profound consequences if a solution is found: If the 2nd law says everything tends towards disorder, why does essentially everything we see in the Universe contradict this? At almost every scale of the Universe we see self-organized structures, from black holes to stars, planets and suns to our own earth, the life that abounds on it and in particular the brain. Non-biological systems such as Benard convection cells <xref ref-type="bibr" rid="pone.0085175-Getling1">[86]</xref>, tornadoes, lightning and rivers, to name just a few, show us that matter does not tend toward disorder in practice but rather does quite the opposite. In another example, metallic spheres in a non-conducting liquid medium exposed to an electric field will self-organize into fractal dendritic trees <xref ref-type="bibr" rid="pone.0085175-Athelogou1">[87]</xref>.</p>
<p>One line of argument is that ordered structures create entropy faster than disordered structures do and self-organizing dissipative systems are the result of <italic>out of equilibrium thermodynamics</italic>. In other words, there may not actually be a distinct 4th law, and all observed order may actually result from dynamics yet to be unraveled mathematically from the 2nd law. Unfortunately this argument does not leave us with an understanding sufficient to allow us to exploit the phenomena in our technology. In this light, our work with AHaH attractor states may provide a clue as to the nature of the 4th law in so much as it lets us construct useful self-organizing and adaptive computing systems.</p>
<p>One particularly clear and falsifiable formulation of the 4th law comes from Swenson in 1989:</p>
<p>“A system will select the path or assembly of paths out of available paths that minimizes the potential or maximizes the entropy at the fastest rate given the constraints <xref ref-type="bibr" rid="pone.0085175-Swenson1">[88]</xref>.”</p>
<p>Others have converged on similar thoughts. For example, Bejan postulated in 1996 that:</p>
<p>“For a finite-size system to persist in time (to live), it must evolve in such a way that it provides easier access to the imposed currents that flow through it <xref ref-type="bibr" rid="pone.0085175-Bejan1">[89]</xref>.”</p>
<p>Bejan’s formulation seems intuitively correct when one looks at nature, although it has faced criticism that it is too vague since it does not say what particle is flowing. We observe that in many cases the particle is either directly a carrier of free energy dissipation or else it gates access, like a key to a lock, to free energy dissipation of the units in the collective. These particles are not hard to spot. Examples include water in plants, ATP in cells, blood in bodies, neurotrophins in brains, and money in economies.</p>
<p>More recently, Jorgensen and Svirezhev have put forward the <italic>maximum power principle</italic> <xref ref-type="bibr" rid="pone.0085175-Jorgensen1">[90]</xref> and Schneider and Sagan have elaborated on the simple idea that “nature abhors a gradient” <xref ref-type="bibr" rid="pone.0085175-Schneider1">[91]</xref>. Others have put forward similar notions much earlier. Morowitz claimed in 1968 that the flow of energy from a source to a sink will cause at least one cycle in the system <xref ref-type="bibr" rid="pone.0085175-Schneider1">[91]</xref> and Lotka postulated the <italic>principle of maximum energy flux</italic> in 1922 <xref ref-type="bibr" rid="pone.0085175-Lotka1">[92]</xref>.</p>
</sec><sec id="s2b">
<title>The Container Adapts</title>
<p>Hatsopoulos and Keenan’s <italic>law of stable equilibrium</italic> <xref ref-type="bibr" rid="pone.0085175-Hatsopoulos1">[93]</xref> states that:</p>
<p>“When an isolated system performs a process, after the removal of a series of internal constraints, it will always reach a unique state of equilibrium; this state of equilibrium is independent of the order in which the constraints are removed.”</p>
<p>The idea is that a system erases any knowledge about how it arrived in equilibrium. Schneider and Sagan state this observation in their book <italic>Into the Cool: Energy Flow, Thermodynamics, and Life</italic> <xref ref-type="bibr" rid="pone.0085175-Schneider1">[91]</xref> by claiming: “These principles of erasure of the path, or past, as work is produced on the way to equilibrium hold for a broad class of thermodynamic systems.” This principle has been illustrated by connected rooms, where doors between the rooms are opened according to a particular sequence, and only one room is pressurized at the start. The end state is the same regardless of the path taken to get there. The problem with this analysis is that it relies on an external agent: the door opener.</p>
<p>We may reformulate this idea in the light of an adaptive container, as shown in <xref ref-type="fig" rid="pone-0085175-g001">Figure 1</xref>. A first <italic>replenished</italic> pressurized container <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e055" xlink:type="simple"/></inline-formula> is allowed to diffuse into two non-pressurized empty containers <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e056" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e057" xlink:type="simple"/></inline-formula> though a region of matter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e058" xlink:type="simple"/></inline-formula>. Let us presume that the initial fluid conductance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e059" xlink:type="simple"/></inline-formula> between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e060" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e061" xlink:type="simple"/></inline-formula> is less than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e062" xlink:type="simple"/></inline-formula>. Competition for limited resources within the matter (conservation of matter) enforces the condition that the sum of conductances is constant:<disp-formula id="pone.0085175.e063"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e063" xlink:type="simple"/><label>(1)</label></disp-formula></p>
<fig id="pone-0085175-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g001</object-id><label>Figure 1</label><caption>
<title>AHaH process.</title>
<p>A) A first replenished pressurized container <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e001" xlink:type="simple"/></inline-formula> is allowed to diffuse into two non-pressurized empty containers <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e002" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e003" xlink:type="simple"/></inline-formula> though a region of matter M. B) The gradient <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e004" xlink:type="simple"/></inline-formula> reduces faster than the gradient <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e005" xlink:type="simple"/></inline-formula> due to the conductance differential. C) This causes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e006" xlink:type="simple"/></inline-formula> to grow more than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e007" xlink:type="simple"/></inline-formula>, reducing the conductance differential and leading to anti-Hebbian learning. D) The first detectable signal (work) is available at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e008" xlink:type="simple"/></inline-formula> owing to the differential that favors it. As a response to this signal, events may transpire in the environment that open up new pathways to particle dissipation. The initial conductance differential is reinforced leading to Hebbian learning.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g001" position="float" xlink:type="simple"/></fig>
<p>Now we ask how the container adapts as the system attempts to come to equilibrium. If it is the <italic>gradient</italic> that is driving the change in the conductance, then it becomes immediately clear that the container will adapt in such a way as to erase any initial differential conductance:<disp-formula id="pone.0085175.e064"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e064" xlink:type="simple"/><label>(2)</label></disp-formula></p>
<p>The gradient <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e065" xlink:type="simple"/></inline-formula> will reduce faster than the gradient <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e066" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e067" xlink:type="simple"/></inline-formula> will grow more than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e068" xlink:type="simple"/></inline-formula>. When the system comes to equilibrium we will find that the conductance differential, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e069" xlink:type="simple"/></inline-formula> has been reduced.</p>
<p>The sudden pressurization of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e070" xlink:type="simple"/></inline-formula> may have an effect on the environment. In the moments right after the flow sets up, the first detectable signal (work) will be available at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e071" xlink:type="simple"/></inline-formula> owing to the differential that favors it. As a response to this signal, any number of events could transpire in the environment that open up new pathways to particle dissipation. The initial conductance differential will be reinforced as the system rushes to equalize the gradient in this newly discovered space. Due to conservation of adaptive resources (<xref ref-type="disp-formula" rid="pone.0085175.e063">Equation 1</xref>), an increase in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e072" xlink:type="simple"/></inline-formula> will require a drop in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e073" xlink:type="simple"/></inline-formula>, and vice versa. The result is that as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e074" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e075" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e076" xlink:type="simple"/></inline-formula> and the system selects one pathway over another. The process illustrated in <xref ref-type="fig" rid="pone-0085175-g001">Figure 1</xref> creates structure so long as new sinks are constantly found and a constant particle source is available.</p>
<p>We now map this thermodynamic process to anti-Hebbian and Hebbian (AHaH) plasticity and show that the resulting attractor states support universal algorithms and broad machine learning functions. We furthermore show how AHaH plasticity can be implemented via physically adaptive memristive circuitry.</p>
</sec><sec id="s2c">
<title>Anti-Hebbian and Hebbian (AHaH) Plasticity</title>
<p>The thermodynamic process outlined above can be understood more broadly as: (1) particles spread out along all available pathways through the environment and in doing so erode any differentials that favor one branch over the other, and (2) pathways that lead to dissipation (the flow of the particles) are stabilized. Let us first identify a synaptic weight, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e077" xlink:type="simple"/></inline-formula>, as the differential conductance formed from two energy dissipating pathways:<disp-formula id="pone.0085175.e078"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e078" xlink:type="simple"/><label>(3)</label></disp-formula></p>
<p>We can now see that the synaptic weight possess state information. If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e079" xlink:type="simple"/></inline-formula> the synapse is positive and if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e080" xlink:type="simple"/></inline-formula> then it is negative. With this in mind we can explicitly define AHaH learning:</p>
<list list-type="bullet"><list-item>
<p>Anti-Hebbian (erase the path): Any modification to the synaptic weight that reduces the probability that the synaptic state will remain the same upon subsequent measurement.</p>
</list-item><list-item>
<p>Hebbian (select the path): Any modification to the synaptic weight that increases the probability that the synaptic state will remain the same upon subsequent measurement.</p>
</list-item></list>
<p>Our use of Hebbian learning follows a standard mathematical generalization of Hebb’s famous postulate:</p>
<p>“When an axon of cell A is near enough to excite B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased <xref ref-type="bibr" rid="pone.0085175-Hebb1">[24]</xref>.”</p>
<p>Hebbian learning can be represented mathematically as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e081" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e082" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e083" xlink:type="simple"/></inline-formula> are the activities of the pre- and post-synaptic neurons and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e084" xlink:type="simple"/></inline-formula> is the change to the synaptic weight between them. Anti-Hebbian learning is the negative of Hebbian: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e085" xlink:type="simple"/></inline-formula>. Notice that intrinsic to this mathematical definition is the notion of state. The pre- and post-synaptic activities as well as the weight may be positive or negative. We achieve the notion of state in our physical circuits via differential conductances (<xref ref-type="disp-formula" rid="pone.0085175.e078">Equation 3</xref>).</p>
</sec><sec id="s2d">
<title>Linear Neuron Model</title>
<p>To begin our mapping of AHaH plasticity to computing and machine learning systems we use a standard linear neuron model. The choice of a linear neuron is motivated by the fact that they are ubiquitous in machine learning and also because it is easy to achieve the linear sum function in a physical circuit, since currents naturally sum.</p>
<p>The inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e086" xlink:type="simple"/></inline-formula> in a linear model are the outputs from other neurons or spike encoders (to be discussed). The weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e087" xlink:type="simple"/></inline-formula> are the strength of the inputs. The larger <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e088" xlink:type="simple"/></inline-formula>, the more <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e089" xlink:type="simple"/></inline-formula> affects the neuron’s output. Each input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e090" xlink:type="simple"/></inline-formula> is multiplied by a corresponding weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e091" xlink:type="simple"/></inline-formula> and these values, combined with the bias <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e092" xlink:type="simple"/></inline-formula>, are summed together to form the output <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e093" xlink:type="simple"/></inline-formula>:<disp-formula id="pone.0085175.e094"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e094" xlink:type="simple"/><label>(4)</label></disp-formula></p>
<p>The weights and bias change according to AHaH plasticity, which we further detail in the sections that follow. The AHaH rule acts to <italic>maximize the margin</italic> between positive and negative classes. In what follows, <italic>AHaH nodes</italic> refer to linear neurons implementing the AHaH plasticity rule.</p>
</sec><sec id="s2e">
<title>AHaH Attractors Extract Independent Components</title>
<p>What we desire is a mechanism to extract the underlying building blocks or <italic>independent components</italic> of a data stream, irrespective of the number of discrete channels those components are communicated over. One method to accomplish this task is independent component analysis. The two broadest mathematical definitions of independence as used in ICA are (1) minimization of mutual information between competing nodes and (2) maximization of non-Gaussianity of the output of a single node. The non-Gaussian family of ICA algorithms uses negentropy and kurtosis as mathematical objective functions from which to derive a plasticity rule. To find a plasticity rule capable of ICA we can minimize a kurtosis objective function over the node output activation. The result is ideally the opposite of a peak: a bimodal distribution. That is, we seek a hyperplane that separates the input data into two classes resulting in two distinct <italic>positive</italic> and <italic>negative</italic> distributions. Using a kurtosis objective function, it can be shown that a plasticity rule of the following form emerges <xref ref-type="bibr" rid="pone.0085175-Nugent1">[43]</xref>:<disp-formula id="pone.0085175.e106"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e106" xlink:type="simple"/><label>(5)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e107" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e108" xlink:type="simple"/></inline-formula> are constants that control the relative contribution of Hebbian and anti-Hebbian plasticity, respectively. <xref ref-type="disp-formula" rid="pone.0085175.e106">Equation 5</xref> is one form of many that we call the <italic>AHaH rule</italic>. The important functional characteristics that <xref ref-type="disp-formula" rid="pone.0085175.e106">Equation 5</xref> shares with all the other forms is that as the magnitude of the post-synaptic activation grows, the weight update transitions from Hebbian to anti-Hebbian learning.</p>
</sec><sec id="s2f">
<title>AHaH Attractors Make Optimal Decisions</title>
<p>An AHaH node is a hyperplane attempting to bisect its input space so as to make a binary decision. There are many hyperplanes to choose from and the question naturally arises as to which one is best. The generally agreed answer to this question is “the one that maximizes the separation (margin) of the two classes.” The idea of <italic>maximizing the margin</italic> is central to support vector machines, arguably one of the more successful machine learning algorithms. As demonstrated in <xref ref-type="bibr" rid="pone.0085175-Nugent1">[43]</xref>, <xref ref-type="bibr" rid="pone.0085175-Nugent2">[44]</xref>, as well as the results of this paper, the attractor states of the AHaH rule coincide with the maximum-margin solution.</p>
</sec><sec id="s2g">
<title>AHaH Attractors Support Universal Algorithms</title>
<p>Given a discrete set of inputs and a discrete set of outputs it is possible to account for all possible transfer functions via a logic function. Logic is usually taught as small two-input gates such as NAND and OR. However, when one looks at a more complicated algorithm such as a machine learning classifier, it is not so clear that it is performing a logic function. As demonstrated in following sections, AHaH attractor states are computationally complete logic functions. For example, when robotic arm actuation or prediction is demonstrated, self-configuring logic functions is also being demonstrated.</p>
<p>In what follows we will be adopting a <italic>spike encoding</italic>. A spike encoding consists of either a spike (1) or no spike (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e109" xlink:type="simple"/></inline-formula>). In digital logic, the state ‘0’ is opposite or complimentary to the state ‘1’ and it can be communicated. One cannot communicate a pulse of <italic>nothing</italic> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e110" xlink:type="simple"/></inline-formula>). For this reason, we refer to a spike as ‘1’ and no spike as a ‘<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e111" xlink:type="simple"/></inline-formula>’ or <italic>floating</italic> to avoid this confusion. Furthermore, the output of an AHaH node can be positive or negative and hence possess a <italic>state</italic>. We can identify these positive and negative output states as logical outputs, for example the standard logical ‘1’ is <italic>positive</italic> and ‘0’ is <italic>negative</italic>.</p>
<p>Let us analyze the simplest possible AHaH node; one with only two inputs. The three possible input patterns are:<disp-formula id="pone.0085175.e112"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e112" xlink:type="simple"/><label>(6)</label></disp-formula></p>
<p>Stable synaptic states will occur when the sum over all weight updates is zero. We can plot the AHaH node’s stable decision boundary on the same plot with the data that produced it. This can be seen in <xref ref-type="fig" rid="pone-0085175-g002">Figure 2</xref>, where decision boundaries A, B and C are labeled. Although the D state is theoretically achievable, it has been difficult to achieve in circuit simulations, and for this reason we exclude it as an available state. Note that every state has a corresponding anti-state. The AHaH plasticity is a local update rule that is attempting to maximize the margin between opposing positive and negative data distributions. As the positive distribution pushes the decision boundary away (making the weights more positive), the magnitude of the positive updates decreases while the magnitude of the opposing negative updates increases. The net result is that strong attractor states exist when the decision boundary can cleanly separate a data distribution.</p>
<fig id="pone-0085175-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g002</object-id><label>Figure 2</label><caption>
<title>Attractor states of a two-input AHaH node.</title>
<p>The AHaH rule naturally forms decision boundaries that maximize the margin between data distributions (black blobs). This is easily visualized in two dimensions, but it is equally valid for any number of inputs. Attractor states are represented by decision boundaries A, B, C (green dotted lines) and D (red dashed line). Each state has a corresponding anti-state: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e009" xlink:type="simple"/></inline-formula>. State A is the null state and its occupation is inhibited by the bias. State D has not yet been reliably achieved in circuit simulations.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g002" position="float" xlink:type="simple"/></fig>
<p>We refer to the A state as the null state. The null state occurs when an AHaH node assigns the same weight value to each synapse and outputs the same state for every pattern. The null state is mostly useless computationally, and its occupation is inhibited by bias weights. Through strong anti-Hebbian learning, the bias weights force each neuron to split the output space equally. As the neuron <italic>locks on</italic> to a stable bifurcation, the effect of the bias weights is minimized and the decision margin is maximized via AHaH learning on the input weights.</p>
<p>Recall Turing’s idea of a network of NAND gates connected by <italic>modifier devices</italic> as mentioned in the Historical Background section. The AHaH nodes extract independent component states, the <italic>alphabet</italic> of the data stream. As illustrated in <xref ref-type="fig" rid="pone-0085175-g003">Figure 3</xref>, by providing the sign of the output of AHaH nodes to static NAND gates, a universal reconfigurable logic gate is possible. Configuring the AHaH attractor states, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e113" xlink:type="simple"/></inline-formula>, configures the logic function. We can do even better than this however.</p>
<fig id="pone-0085175-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g003</object-id><label>Figure 3</label><caption>
<title>Universal reconfigurable logic.</title>
<p>By connecting the output of AHaH nodes (circles) to the input of static NAND gates, one may create a universal reconfigurable logic gate by configuring the AHaH node attractor states (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e011" xlink:type="simple"/></inline-formula>). The structure of the data stream on binary encoded channels <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e012" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e013" xlink:type="simple"/></inline-formula> support AHaH attractor states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e014" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pone-0085175-g002">Figure 2</xref>). Through configuration of node attractor states the logic function of the circuit can be configured and all logic functions are possible. If inputs are represented as a spike encoding over four channels then AHaH node attractor states can attain all logic functions without the use of NAND gates.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g003" position="float" xlink:type="simple"/></fig>
<p>We can achieve all logic functions directly (without NAND gates) if we define a <italic>spike logic</italic> code, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e114" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e115" xlink:type="simple"/></inline-formula>, as shown in <xref ref-type="table" rid="pone-0085175-t001">Table 1</xref>. As any algorithm or procedure can be attained from combinations of logic functions, AHaH nodes are building blocks from which any algorithm can be built. This analysis of logic is necessary to prove that AHaH attractor states can support any algorithm, not that AHaH computing is intended to replace modern methods of high speed digital logic.</p>
<table-wrap id="pone-0085175-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.t001</object-id><label>Table 1</label><caption>
<title>Spike logic patterns.</title>
</caption><alternatives><graphic id="pone-0085175-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Logic Pattern</td>
<td align="left" rowspan="1" colspan="1">Spike Logic Pattern</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">(0, 0)</td>
<td align="left" rowspan="1" colspan="1">(1, <italic>z</italic>, 1, <italic>z</italic>)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">(0, 1)</td>
<td align="left" rowspan="1" colspan="1">(1, <italic>z</italic>, <italic>z</italic>, 1)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">(1, 0)</td>
<td align="left" rowspan="1" colspan="1">(<italic>z</italic>, 1, 1, <italic>z</italic>)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">(1, 1)</td>
<td align="left" rowspan="1" colspan="1">(<italic>z</italic>, 1, <italic>z</italic>, 1)</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt101"><label/><p>Digital logic states ‘0’ and ‘1’ across two input lines are converted to a spike encoding across four input lines. A spike encoding consists of either spikes (1) or no spikes (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e015" xlink:type="simple"/></inline-formula>). This encoding insures that the number of spikes at any given time is constant.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s2h">
<title>AHaH Attractors are Bits</title>
<p>Every AHaH attractor consists of a state/anti-state pair that can be configured and therefore appears to represent a <italic>bit</italic>. In the limit of only one synapse and one input line activation, the state of the AHaH node is the state of the synapse just like a typical bit. As the number of simultaneous inputs grows past one, the <italic>AHaH bit</italic> becomes a collective over all interacting synapses. For every AHaH attractor state that outputs a ‘1’, for example, there exists an equal and opposite AHaH attractor state that will output a ‘−1’. The state/anti-state property of the AHaH attractors follows mathematically from ICA, since ICA is in general not able to uniquely determine the sign of the source signals. The AHaH bits open up the possibility of configuring populations to achieve computational objectives. We take advantage of AHaH bits in the AHaH clustering and AHaH motor controller examples presented later in this paper. It is important to understand that AHaH attractor states are a reflection of the underlying statistics of the data stream and cannot be fully understood as just the collection of synapses that compose it. Rather, it is both the collection of synapses and also the structure of the information that is being processed that result in an AHaH attractor state. If we equate the data being processed as a sequence of measurements of the AHaH bit’s state, we arrive at an interesting observation: the act of measurement not only effects the state of the AHaH bit, it actually <italic>defines</italic> it. Without the data structure imposed by the sequence of measurements, the state would simply not exist. This bears some similarity to ideas that emerge from quantum mechanics.</p>
</sec><sec id="s2i">
<title>AHaH Memristor Circuit</title>
<p>Although we discuss a <italic>functional</italic> or <italic>mathematical</italic> representation of the AHaH node, AHaH computing necessarily has its foundation in a physical embodiment or circuit. The AHaH rule is achievable if one provides for competing adaptive dissipating pathways. The modern memristor provides us with just such an adaptive pathway. Two memristors provide us with two competing pathways. While some neuromorphic computing research has focused on exploiting the synapse-like behavior of a single memristor <xref ref-type="bibr" rid="pone.0085175-Jo1">[68]</xref>, <xref ref-type="bibr" rid="pone.0085175-Thomas1">[83]</xref> or using two serially connected memristive devices with different polarities <xref ref-type="bibr" rid="pone.0085175-MerrikhBayat1">[67]</xref>, we implement synaptic weights via a differential pair of memristors with the same polarities (<xref ref-type="fig" rid="pone-0085175-g004">Figure 4</xref>) <xref ref-type="bibr" rid="pone.0085175-Nugent3">[45]</xref>–<xref ref-type="bibr" rid="pone.0085175-Nugent5">[47]</xref> acting as competing dissipation pathways.</p>
<fig id="pone-0085175-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g004</object-id><label>Figure 4</label><caption>
<title>A differential pair of memristors forms a synapse.</title>
<p>A differential pair of memristors is used to form a synaptic weight, allowing for both a sign and magnitude. The bar on the memristor is used to indicate polarity and corresponds to the lower potential end when driving the memristor into a higher conductance state. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e016" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e017" xlink:type="simple"/></inline-formula> form a voltage divider causing the voltage at node y to be some value between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e018" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e019" xlink:type="simple"/></inline-formula>. When driven correctly in the absence of Hebbian feedback a synapse will evolve to a symmetric state where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e020" xlink:type="simple"/></inline-formula> V, alleviating issues arising from device inhomogeneities.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g004" position="float" xlink:type="simple"/></fig>
<p>The circuits capable of achieving AHaH plasticity can be broadly categorized by the electrode configuration that forms the differential synapses as well as how the input activation (current) is converted to a feedback voltage that drives unsupervised anti-Hebbian learning <xref ref-type="bibr" rid="pone.0085175-Nugent4">[46]</xref>, <xref ref-type="bibr" rid="pone.0085175-Nugent5">[47]</xref>. Synaptic currents can be converted to a feedback voltage statically (resistors or memristors), dynamically (capacitors), or actively (operational amplifiers). Each configuration requires unique circuitry to drive the electrodes so as to achieve AHaH plasticity, and multiple driving methods exist. The result is that a very large number of AHaH circuits exist, and it is well beyond the scope of this paper to discuss all configurations. Herein, a ‘2-1’ two-phase circuit configuration is introduced because of its compactness and because it is amenable to mathematical analysis.</p>
<p>The functional objective of the AHaH circuit shown in <xref ref-type="fig" rid="pone-0085175-g005">Figure 5</xref> is to produce an analog output on electrode y, given an arbitrary spike input of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e116" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e117" xlink:type="simple"/></inline-formula> active inputs and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e118" xlink:type="simple"/></inline-formula> inactive (floating) inputs. The circuit consists of one or more memristor pairs (synapses) sharing a common electrode labeled y. Driving voltage sources are indicated with circles and labeled with an S, B or F, referring to spike, bias, or feedback respectively. The individual driving voltage sources for spike inputs of the AHaH circuit are labeled <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e119" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e120" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e121" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e122" xlink:type="simple"/></inline-formula>. The driving voltage sources for bias inputs are labeled <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e123" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e124" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e125" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e126" xlink:type="simple"/></inline-formula>. The driving voltage source for supervised and unsupervised learning is labeled F. The subscript values a and b indicate the positive and negative dissipative pathways, respectively.</p>
<fig id="pone-0085175-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g005</object-id><label>Figure 5</label><caption>
<title>AHaH 2-1 two-phase circuit diagram.</title>
<p>The circuit produces an analog voltage signal on the output at node y given a spike pattern on its inputs labeled <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e021" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e022" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e023" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e024" xlink:type="simple"/></inline-formula>. The bias inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e025" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e026" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e027" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e028" xlink:type="simple"/></inline-formula> are equivalent to the spike pattern inputs except that they are always active when the spike pattern inputs are active. F is a voltage source used to implement supervised and unsupervised learning via the AHaH rule. The polarity of the memristors for the bias synapse(s) is inverted relative to the input memristors. The output voltage, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e029" xlink:type="simple"/></inline-formula>, contains both state (positive/negative) and confidence (magnitude) information.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g005" position="float" xlink:type="simple"/></fig>
<p>During the read phase, driving voltage sources <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e127" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e128" xlink:type="simple"/></inline-formula> are set to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e129" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e130" xlink:type="simple"/></inline-formula> respectively for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e131" xlink:type="simple"/></inline-formula> active inputs. Inactive S inputs are left floating. The number of bias inputs to drive, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e132" xlink:type="simple"/></inline-formula>, is fixed or a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e133" xlink:type="simple"/></inline-formula> and driving voltage sources <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e134" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e135" xlink:type="simple"/></inline-formula> are set to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e136" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e137" xlink:type="simple"/></inline-formula> respectively for all bias pairs. The combined conductance of the active inputs and biases produce an output voltage on electrode y. This analog signal contains useful confidence information and can be digitized via the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e138" xlink:type="simple"/></inline-formula> function to either a logical 1 or a 0, if desired.</p>
<p>During the write phase, driving voltage source F is set to either <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e139" xlink:type="simple"/></inline-formula> (unsupervised) or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e140" xlink:type="simple"/></inline-formula> (supervised), where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e141" xlink:type="simple"/></inline-formula> is an externally applied teaching signal. The polarity of the driving voltage sources <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e142" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e143" xlink:type="simple"/></inline-formula> are inverted to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e144" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e145" xlink:type="simple"/></inline-formula>. The polarity switch causes all active memristors to be driven to a less conductive state, counteracting the read phase. If this dynamic counteraction did not take place, the memristors would quickly saturate into their maximally conductive states, rendering the synapses useless.</p>
<p>A more intuitive explanation of the above feedback cycle is that “the winning pathway is rewarded by not getting decayed.” Each synapse can be thought of as two competing energy dissipating pathways (positive or negative evaluations) that are building structure (differential conductance). We may apply reinforcing Hebbian feedback by (1) allowing the winning pathway to dissipate more energy or (2) forcing the decay of the losing pathway. If we chose method (1) then we must at some future time ensure that we decay the conductance before device saturation is reached. If we chose method (2) then we achieve both decay and reinforcement at the same time.</p>
</sec><sec id="s2j">
<title>AHaH Rule from Circuit Derivation</title>
<p>Without significant demonstrations of utility there is little motivation to pursue a new form of computing. Our functional model abstraction is necessary to reduce the computational overhead associated with simulating circuits and enable large scale simulations that tackle benchmark problems with real world utility. In this section, we derive the AHaH plasticity rule again, but instead of basing it on statistical independent components as in the derivation of <xref ref-type="disp-formula" rid="pone.0085175.e106">Equation 5</xref>, we derive it from simple circuit physics.</p>
<p>During the read phase, simple circuit analysis shows that the voltage on the electrode labeled y in the circuit shown in <xref ref-type="fig" rid="pone-0085175-g005">Figure 5</xref> is:<disp-formula id="pone.0085175.e146"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e146" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e147" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e148" xlink:type="simple"/></inline-formula> are the conductances of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e149" xlink:type="simple"/></inline-formula> memristors for the positive and negative dissipative pathways, respectively. The driving voltage sources <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e150" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e151" xlink:type="simple"/></inline-formula> as well as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e152" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e153" xlink:type="simple"/></inline-formula> are set to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e154" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e155" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e156" xlink:type="simple"/></inline-formula> active inputs and bias pairs.</p>
<p>During the write phase the driving voltage source F is set according to either a supervisory signal or in the unsupervised case, the anti-signum of the previous read voltage:<disp-formula id="pone.0085175.e157"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e157" xlink:type="simple"/><label>(8)</label></disp-formula></p>
<p>We may adapt <xref ref-type="disp-formula" rid="pone.0085175.e064">Equation 2</xref> by replacing pressure with voltage:<disp-formula id="pone.0085175.e158"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e158" xlink:type="simple"/><label>(9)</label></disp-formula></p>
<p>Using <xref ref-type="disp-formula" rid="pone.0085175.e158">Equation 9</xref>, the change to memristor conductances over the read and write phases is given in <xref ref-type="table" rid="pone-0085175-t002">Table 2</xref> and corresponds to the circuits of <xref ref-type="fig" rid="pone-0085175-g006">Figure 6</xref>. There are a total of four possibilities because of the two phases and the fact that the polarities of the bias memristors are inverted relative to the spike input memristors. Driving voltage source F is set to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e159" xlink:type="simple"/></inline-formula> during the write phase for both spike and bias inputs. The terms in <xref ref-type="table" rid="pone-0085175-t002">Table 2</xref> can be combined to show the total update to the input memristors over the read and write cycle:<disp-formula id="pone.0085175.e160"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e160" xlink:type="simple"/><label>(10)</label></disp-formula>and likewise for the bias memristors:</p>
<fig id="pone-0085175-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g006</object-id><label>Figure 6</label><caption>
<title>Circuit voltages across memristors during the read and write phases.</title>
<p>A) Voltages during read phase across spike input memristors. B) Voltages during write phase across spike input memristors. C) Voltages during read phase across bias memristors. D) Voltages during write phase across bias memristors.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g006" position="float" xlink:type="simple"/></fig><table-wrap id="pone-0085175-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.t002</object-id><label>Table 2</label><caption>
<title>Memristor conductance updates during the read and write cycle.</title>
</caption><alternatives><graphic id="pone-0085175-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.t002" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td colspan="2" align="left" rowspan="1">Input Memristors</td>
<td colspan="2" align="left" rowspan="1">Bias Memristors</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Read</td>
<td align="left" rowspan="1" colspan="1">Write</td>
<td align="left" rowspan="1" colspan="1">Read</td>
<td align="left" rowspan="1" colspan="1">Write</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e030" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e031" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e032" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e033" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Accumulate</td>
<td align="left" rowspan="1" colspan="1">Decay</td>
<td align="left" rowspan="1" colspan="1">Decay</td>
<td align="left" rowspan="1" colspan="1">Accumulate</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e034" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e035" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e036" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e037" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e038" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e039" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e040" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e041" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e042" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e043" xlink:type="simple"/></inline-formula></td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt102"><label/><p>Both input and bias memristors are updated during one read/write cycle. During the read phase the active input memristors increase in conductance (accumulate) while the bias memristors decrease in conductance (decay). During the write phase the active input memristors decrease in conductance while the bias memristors increase in conductance. The changes in memristor conductances, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e044" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e045" xlink:type="simple"/></inline-formula>, for the memristor pairs are listed for all four cases.</p></fn></table-wrap-foot></table-wrap>
<p><disp-formula id="pone.0085175.e161"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e161" xlink:type="simple"/><label>(11)</label></disp-formula>The quantity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e162" xlink:type="simple"/></inline-formula>, which we call the weight conjugate, remains constant due to competition for limited feedback:<disp-formula id="pone.0085175.e163"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e163" xlink:type="simple"/><label>(12)</label></disp-formula></p>
<p>The output voltage during the read phase reduces to:<disp-formula id="pone.0085175.e164"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e164" xlink:type="simple"/><label>(13)</label></disp-formula>where we have used the substitution:</p>
<p><disp-formula id="pone.0085175.e165"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e165" xlink:type="simple"/><label>(14)</label></disp-formula>We identify the quantity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e166" xlink:type="simple"/></inline-formula> as the standard linear sum over the active weights of the node (<xref ref-type="disp-formula" rid="pone.0085175.e094">Equation 4</xref>). Furthermore, we identify the change of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e167" xlink:type="simple"/></inline-formula> weight as:<disp-formula id="pone.0085175.e168"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e168" xlink:type="simple"/><label>(15)</label></disp-formula></p>
<p>By absorbing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e169" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e170" xlink:type="simple"/></inline-formula> and the two constant 2s into the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e171" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e172" xlink:type="simple"/></inline-formula> constants we arrive at the functional form <italic>Model A</italic> of the AHaH rule:<disp-formula id="pone.0085175.e173"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e173" xlink:type="simple"/><label>(16)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e174" xlink:type="simple"/></inline-formula> is the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e175" xlink:type="simple"/></inline-formula> spike input weight, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e176" xlink:type="simple"/></inline-formula> is the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e177" xlink:type="simple"/></inline-formula> bias weight, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e178" xlink:type="simple"/></inline-formula> is the total number of biases. To shorten the notation we make the substitution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e179" xlink:type="simple"/></inline-formula>. Also note that the quantity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e180" xlink:type="simple"/></inline-formula> is intended to denote the sum over the active (spiking) inputs. The noise variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e181" xlink:type="simple"/></inline-formula> (normal Gaussian) and the decay variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e182" xlink:type="simple"/></inline-formula> account for the underlying stochastic nature of the memristive devices.</p>
<p>Model A is an approximation that is derived by making simplifying assumptions that include linearization of the update and non-saturation of the memristors. However, when a weight reaches saturation, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e183" xlink:type="simple"/></inline-formula>, it becomes resistant to Hebbian modification since the weight differential can no longer be increased, only decreased. This has the desirable effect of inhibiting null state occupation. However, it also means that functional Model A is not sufficient to account for these anti-Hebbian forces that grow increasingly stronger as weights near saturation. The result is that Model A leads to strange attractor dynamics and weights that can (but may not) grow without bound, a condition that is clearly unacceptable for a functional model and is not congruent with the circuit.</p>
<p>To account for the growing effect of anti-Hebbian forces we can make a modification to the bias weight update, and we call the resulting form functional <italic>Model B</italic>:<disp-formula id="pone.0085175.e184"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e184" xlink:type="simple"/><label>(17)</label></disp-formula></p>
<p>The purpose of a functional model is to capture equivalent function with minimal computational overhead so that we may pursue large scale application development on existing technology without incurring the computational cost of circuit simulations. We justify the use of Model B because simulations prove it is a close functional match to the circuit, and it is computationally less expensive than Model A. However, it can be expected that better functional forms exist. Henceforth, any reference to the <italic>functional</italic> model refers to Model B.</p>
<p>Finally, in cases where supervision is desired, the sign of the Hebbian feedback may be modulated by an external supervisory signal, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e185" xlink:type="simple"/></inline-formula>, rather than the evaluation state y:<disp-formula id="pone.0085175.e186"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e186" xlink:type="simple"/><label>(18)</label></disp-formula></p>
<p>Compare <xref ref-type="disp-formula" rid="pone.0085175.e184">Equation 17</xref> to <xref ref-type="disp-formula" rid="pone.0085175.e106">Equation 5</xref>. Both our functional models as well as the form of <xref ref-type="disp-formula" rid="pone.0085175.e106">Equation 5</xref> converge to functionally similar attractor states. The common characteristic between both forms is a transition from Hebbian to anti-Hebbian learning, as the magnitude of node activation, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e187" xlink:type="simple"/></inline-formula>, grows large. This transition insures stable AHaH attractor states.</p>
</sec><sec id="s2k">
<title>Generalized Memristive Device Model</title>
<p>Note that AHaH computing is not constrained to just one particular memristive device; any memristive device can be used as long as it meets the following criteria: (1) it is incremental and (2) its state change is voltage dependent. In order to simulate the proposed AHaH node circuit shown in <xref ref-type="fig" rid="pone-0085175-g005">Figure 5</xref>, a memristive device model is therefore needed. An effective memristive device model for our use should satisfy several requirements. It should accurately model the device behavior, it should be computationally efficient, and it should model as many different devices as possible. Many memristive device models exist, but we felt compelled to create another one which modeled a wider range of devices and, in particular, shows a transition from stochastic binary to incremental analog properties. Any device that can be manufactured to have electronic behavioral characteristics fitting to our model should be considered a viable component for building AHaH computing devices.</p>
<p>In our proposed semi-empirical model, the total current through the device comes from both a memory-dependent current component, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e188" xlink:type="simple"/></inline-formula>, and a Schottky diode current, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e189" xlink:type="simple"/></inline-formula> in parallel:<disp-formula id="pone.0085175.e190"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e190" xlink:type="simple"/><label>(19)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e191" xlink:type="simple"/></inline-formula>. A value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e192" xlink:type="simple"/></inline-formula> represents a device that contains no Schottky diode effects.</p>
<p>The Schottky component, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e193" xlink:type="simple"/></inline-formula>, follows from the fact that many memristive devices contain a Schottky barrier formed at a metal–semiconductor junction <xref ref-type="bibr" rid="pone.0085175-Yang1">[48]</xref>, <xref ref-type="bibr" rid="pone.0085175-Chang1">[63]</xref>, <xref ref-type="bibr" rid="pone.0085175-Jo1">[68]</xref>, <xref ref-type="bibr" rid="pone.0085175-Shang1">[94]</xref>. The Schottky component is modeled by forward bias and reverse bias components as follows:<disp-formula id="pone.0085175.e194"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e194" xlink:type="simple"/><label>(20)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e195" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e196" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e197" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e198" xlink:type="simple"/></inline-formula> are positive valued parameters setting the exponential behavior of the forward and reverse biases exponential current flow across the Schottky barrier.</p>
<p>The memory component of our model, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e199" xlink:type="simple"/></inline-formula>, arises from the notion that memristors can be represented as a collection of conducting channels that switch between states of differing resistance. The channels could be formed from molecular switches, atoms, ions, nanoparticles or more complex composite structures. Modification of device resistance is attained through the application of an external voltage gradient that causes the channels to transition between conducting and non-conducting states. As the number of channels increases, the memristor will become more incremental as it acquires the ability to access more states. By modifying the number of channels we may cover a range of devices from binary to incremental. We treat each channel as a <italic>metastable switch</italic> (MSS) and the conductance of a collection of metastable switches capture the memory effect of the memristor.</p>
<p>An MSS possesses two states, A and B, separated by a potential energy barrier as shown in <xref ref-type="fig" rid="pone-0085175-g007">Figure 7</xref>. Let the barrier potential be the reference potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e200" xlink:type="simple"/></inline-formula>. The probability that the MSS will transition from the B state to the A state is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e201" xlink:type="simple"/></inline-formula>, while the probability that the MSS will transition from the A state to the B state is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e202" xlink:type="simple"/></inline-formula>. The transition probabilities are modeled as:<disp-formula id="pone.0085175.e203"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e203" xlink:type="simple"/><label>(21)</label></disp-formula>and<disp-formula id="pone.0085175.e204"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e204" xlink:type="simple"/><label>(22)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e205" xlink:type="simple"/></inline-formula>. Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e206" xlink:type="simple"/></inline-formula> is the thermal voltage and is equal to approximately 26 <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e207" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e208" xlink:type="simple"/></inline-formula> K, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e209" xlink:type="simple"/></inline-formula> is the ratio of the time step period <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e210" xlink:type="simple"/></inline-formula> to the characteristic time scale of the device, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e211" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e212" xlink:type="simple"/></inline-formula> is the voltage across the switch. The probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e213" xlink:type="simple"/></inline-formula> is defined as the positive-going direction, so that a positive applied voltage increases the chances of occupying the A state. An MSS possesses utility in an electrical circuit as an adaptive element so long as these conductances differ. Each state has an intrinsic electrical conductance given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e214" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e215" xlink:type="simple"/></inline-formula>. The convention is that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e216" xlink:type="simple"/></inline-formula>. Note that the logistic function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e217" xlink:type="simple"/></inline-formula> is similar to the hyperbolic-sign function used in other memristive device models including the nonlinear ion-drift, the Simmons tunnel barrier, the threshold adaptive models, and physics-based models <xref ref-type="bibr" rid="pone.0085175-Sheridan1">[64]</xref>, <xref ref-type="bibr" rid="pone.0085175-Kvatinsky1">[95]</xref>–<xref ref-type="bibr" rid="pone.0085175-Williams1">[98]</xref>. Our use of the logistic function follows simply from the requirement that probabilities must be bounded between 0 and 1.</p>
<fig id="pone-0085175-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g007</object-id><label>Figure 7</label><caption>
<title>Generalized Metastable Switch (MSS).</title>
<p>An MSS is an idealized two-state element that switches probabilistically between its two states as a function of applied voltage bias and temperature. The probability that the MSS will transition from the B state to the A state is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e046" xlink:type="simple"/></inline-formula>, while the probability that the MSS will transition from the A state to the B state is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e047" xlink:type="simple"/></inline-formula>. We model a memristor as a collection of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e048" xlink:type="simple"/></inline-formula> MSSs evolving over discrete time steps.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g007" position="float" xlink:type="simple"/></fig>
<p>We model a memristor as a collection of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e218" xlink:type="simple"/></inline-formula> MSSs evolving in discrete time steps, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e219" xlink:type="simple"/></inline-formula>. The total memristor conductance is given by the sum over each MSS:<disp-formula id="pone.0085175.e220"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e220" xlink:type="simple"/><label>(23)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e221" xlink:type="simple"/></inline-formula> is the number of MSSs in the A state, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e222" xlink:type="simple"/></inline-formula> is the number of MSSs in the B state and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e223" xlink:type="simple"/></inline-formula>.</p>
<p>At each time step some subpopulation of the MSSs in the A state will transition to the B state, while some subpopulation in the B state will transition to the A state. The probability that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e224" xlink:type="simple"/></inline-formula> MSSs will transition out of a population of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e225" xlink:type="simple"/></inline-formula> MSSs is given by the binomial distribution:<disp-formula id="pone.0085175.e226"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e226" xlink:type="simple"/><label>(24)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e227" xlink:type="simple"/></inline-formula> is the probability a MSS will transition states. As <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e228" xlink:type="simple"/></inline-formula> becomes large we may approximate the binomial distribution with a normal distribution:<disp-formula id="pone.0085175.e229"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e229" xlink:type="simple"/><label>(25)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e230" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e231" xlink:type="simple"/></inline-formula>.</p>
<p>We model the change in conductance of a memristor as a probabilistic process where the number of switches that transition between A and B states is picked from a normal distribution with a center at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e232" xlink:type="simple"/></inline-formula> and variance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e233" xlink:type="simple"/></inline-formula>, and where the state transition probabilities are given by <xref ref-type="disp-formula" rid="pone.0085175.e203">Equations 21</xref> and <xref ref-type="disp-formula" rid="pone.0085175.e204">22</xref>.</p>
<p>The update to the memristor conductance is given by the contribution from two random variables picked from two normal distributions:<disp-formula id="pone.0085175.e234"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e234" xlink:type="simple"/><label>(26)</label></disp-formula></p>
<p>The final update to the conductance of the memristor is then given by:<disp-formula id="pone.0085175.e235"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e235" xlink:type="simple"/><label>(27)</label></disp-formula></p>
<p>Reducing the number of MSSs in the model will reduce the averaging effects and cause the memristor to behave in a more stochastic way. As the number of MSSs becomes small, the normal approximation to the binomial distribution breaks down. However, our desired operating regime of many metastable switches, and hence incremental behavior, is within the acceptable bounds of the approximation.</p>
</sec></sec><sec id="s3" sec-type="methods">
<title>Methods</title>
<p>All experiments are software based, and they involve the simulation of AHaH nodes in various configurations to perform various adaptive learning tasks. The source code for the experiments is written in the Java programming language and can be obtained from a Git repository linked to from Xeiam LLC’s main web page at <ext-link ext-link-type="uri" xlink:href="http://xeiam.com" xlink:type="simple">http://xeiam.com</ext-link> under the AHaH! project. The code used for the experiments in this paper is tagged as <italic>PLOS_AHAH</italic> on the <italic>master</italic> branch giving a pointer to the exact code used for this paper. The specific programs for each experiment are clearly identified at the end of each experiment description in the methods section. Further details about the programs and the relevant program parameters can be found in the source code itself in the form of comments.</p>
<p>There are two distinct models used for the simulation experiments: functional and circuit. The simulations based on the functional model use functional Model B as described above. The simulations based on the circuit model use ideal electrical circuit components and the generalized model for memristive devices. Nonideal behaviors such as parasitic impedances are not included in the circuit simulation experiments. We want to emphasize that at this stage we are attempting to cross the considerable divide between memristive electronics and general machine learning by defining a theoretical methodology for computing with dissipative attractor states. By focusing on nonideal circuit behavior at this stage we risk obfuscating what is otherwise a theory with minimal complexity.</p>
<sec id="s3a">
<title>Generalized Memristive Device Model</title>
<p>By adjusting the free variables in the generalized memristive device model and comparing the subsequent current-voltage hysteresis loops to four real world memristive device I–V data, matching model parameters were determined as shown in <xref ref-type="table" rid="pone-0085175-t003">Table 3</xref>. The devices include the Ag-chalcogenide <xref ref-type="bibr" rid="pone.0085175-Oblea1">[55]</xref>, AIST <xref ref-type="bibr" rid="pone.0085175-Zhang1">[99]</xref>, GST <xref ref-type="bibr" rid="pone.0085175-Li1">[70]</xref>, and WO<italic><sub>x</sub></italic> <xref ref-type="bibr" rid="pone.0085175-Chang1">[63]</xref> devices, and they represent a wide spectrum of incremental memristive devices found in recent publications exhibiting diverse characteristics. All simulations in this paper involving AHaH node circuitry use the memristor model parameters of the Ag-chalcogenide device, unless otherwise noted. The remaining three are presented in support of our general model.</p>
<table-wrap id="pone-0085175-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.t003</object-id><label>Table 3</label><caption>
<title>General memristive device model parameters fit to various devices.</title>
</caption><alternatives><graphic id="pone-0085175-t003-3" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.t003" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Device</td>
<td align="left" rowspan="1" colspan="1"><italic>t</italic><sub>c</sub> [ms]</td>
<td align="left" rowspan="1" colspan="1"><italic>G</italic><sub>A</sub> [mS]</td>
<td align="left" rowspan="1" colspan="1"><italic>G</italic><sub>B</sub> [mS]</td>
<td align="left" rowspan="1" colspan="1"><italic>V</italic><sub>A</sub> [V]</td>
<td align="left" rowspan="1" colspan="1"><italic>V</italic><sub>B</sub> [V]</td>
<td align="left" rowspan="1" colspan="1"><italic>φ</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>α</italic><sub>f</sub></td>
<td align="left" rowspan="1" colspan="1"><italic>β</italic><sub>f</sub></td>
<td align="left" rowspan="1" colspan="1"><italic>α</italic><sub>r</sub></td>
<td align="left" rowspan="1" colspan="1"><italic>β</italic><sub>r</sub></td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Ag-chalc</td>
<td align="left" rowspan="1" colspan="1">0.32</td>
<td align="left" rowspan="1" colspan="1">8.7</td>
<td align="left" rowspan="1" colspan="1">0.91</td>
<td align="left" rowspan="1" colspan="1">0.17</td>
<td align="left" rowspan="1" colspan="1">0.22</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">–</td>
<td align="left" rowspan="1" colspan="1">–</td>
<td align="left" rowspan="1" colspan="1">–</td>
<td align="left" rowspan="1" colspan="1">–</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">AIST</td>
<td align="left" rowspan="1" colspan="1">0.15</td>
<td align="left" rowspan="1" colspan="1">40</td>
<td align="left" rowspan="1" colspan="1">10</td>
<td align="left" rowspan="1" colspan="1">.23</td>
<td align="left" rowspan="1" colspan="1">.25</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">–</td>
<td align="left" rowspan="1" colspan="1">–</td>
<td align="left" rowspan="1" colspan="1">–</td>
<td align="left" rowspan="1" colspan="1">–</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">GST</td>
<td align="left" rowspan="1" colspan="1">0.42</td>
<td align="left" rowspan="1" colspan="1">.12</td>
<td align="left" rowspan="1" colspan="1">1.2</td>
<td align="left" rowspan="1" colspan="1">.9</td>
<td align="left" rowspan="1" colspan="1">0.6</td>
<td align="left" rowspan="1" colspan="1">0.7</td>
<td align="left" rowspan="1" colspan="1">5×10<sup>−3</sup></td>
<td align="left" rowspan="1" colspan="1">3.0</td>
<td align="left" rowspan="1" colspan="1">5×10<sup>−3</sup></td>
<td align="left" rowspan="1" colspan="1">3.0</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">WO<italic><sub>x</sub></italic></td>
<td align="left" rowspan="1" colspan="1">0.80</td>
<td align="left" rowspan="1" colspan="1">.025</td>
<td align="left" rowspan="1" colspan="1">0.004</td>
<td align="left" rowspan="1" colspan="1">0.8</td>
<td align="left" rowspan="1" colspan="1">1.0</td>
<td align="left" rowspan="1" colspan="1">.55</td>
<td align="left" rowspan="1" colspan="1">1×10<sup>−9</sup></td>
<td align="left" rowspan="1" colspan="1">8.5</td>
<td align="left" rowspan="1" colspan="1">22×10<sup>−9</sup></td>
<td align="left" rowspan="1" colspan="1">6.2</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt103"><label/><p>The devices used to test our general memristive device model include the Ag-chalcogenide, AIST, GST, and WO<italic><sub>x</sub></italic> devices. The parameters in this table were determined by comparing the model response to a simulated sinusoidal or triangle-wave voltage to real I–V data of physical devices.</p></fn></table-wrap-foot></table-wrap>
<p><xref ref-type="fig" rid="pone-0085175-g008">Figure 8A</xref> shows the hysteresis curve of the model and raw Ag-chalcogenide device data driven at 100 Hz with a sinusoidal voltage of 0.25 V amplitude. Additional 1000 Hz and 10 kHz simulations are also shown. The predicted behavior of the model shows a good fit to the physical Ag-chalcogenide device. In fact the model is arguably better than other models (linear ion drift and nonlinear ion drift) tested for a similar device in <xref ref-type="bibr" rid="pone.0085175-Pino1">[61]</xref>. <xref ref-type="fig" rid="pone-0085175-g008">Figure 8B</xref> shows the predicted response of two series-connected arbitrary memristive devices with differing parameters driven by the sinusoidal voltage as in 8A. The simulation of two devices in series (<xref ref-type="fig" rid="pone-0085175-g004">Figure 4</xref>) as shown in <xref ref-type="fig" rid="pone-0085175-g008">Figure 8B</xref> also displayed expected characteristics and agrees with results in <xref ref-type="bibr" rid="pone.0085175-Mladenov1">[100]</xref> where the linear ion drift model was used. Experiments have not yet been carried out on physical devices to verify this. <xref ref-type="fig" rid="pone-0085175-g008">Figure 8C</xref> shows the incremental pulsed resistance change of a single Ag-chalcogenide modeled device for three different pulse train configurations. The three different pulse trains were chosen to show that by changing both the pulse width or the pulse voltage, the modeled behavior is predicted as expected. <xref ref-type="fig" rid="pone-0085175-g008">Figure 8D</xref> shows the time response of the Ag-chalcogenide modeled device at frequencies of 100 Hz, 150 Hz, and 200 Hz. <xref ref-type="fig" rid="pone-0085175-g008">Figure 8E</xref> shows the simulated response of the Ag-chalcogenide modeled device to a triangle wave of both +0.1 V and −0.1 V amplitude at 100 Hz designed to show the expected incremental prediction of the model. <xref ref-type="fig" rid="pone-0085175-g008">Figure 8F</xref> shows additional model fits to the AIST, GST, and WO<italic><sub>x</sub></italic> devices. As demonstrated, our model can be applied to a wide range of memristive devices from Chalcogenides to metal-oxides and more. The source code for these simulations is in <italic>AgChalcogenideHysteresisPlotA.java</italic>, <italic>AgChalcogenideHysteresisPlotB.java</italic>, <italic>AgChalcogenidePulseTrainPlotC</italic>, <italic>AgChalcogenideTimePlotD</italic>, <italic>AgChalcogenideTrianglePlotE</italic>, <italic>AgInSbTeHysteresisPlot.java</italic>, <italic>GSTHysteresisPlot.java</italic>, and <italic>PdWO3WHysteresisPlot.java</italic>.</p>
<fig id="pone-0085175-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g008</object-id><label>Figure 8</label><caption>
<title>Generalized memristive device model simulations.</title>
<p>A) Solid line represents the model simulated at 100 Hz and dots represent the measurements from a physical Ag-chalcogenide device from Boise State University. Physical and predicted device current resulted from driving a sinusoidal voltage of 0.25 V amplitude at 100 Hz across the device. B) Simulation of two series-connected arbitrary devices with differing model parameter values. C) Simulated response to pulse trains of {10 <italic>μ</italic>s, 0.2 V, −0.5 V}, {10 <italic>μ</italic>s, 0.8 V, −2.0 V}, and {5 <italic>μ</italic>s, 0.8 V, −2.0 V} showing the incremental change in resistance in response to small voltage pulses. D) Simulated time response of model from driving a sinusoidal voltage of 0.25 V amplitude at 100 Hz, 150 Hz, and 200 Hz. E) Simulated response to a triangle wave of 0.1 V amplitude at 100 Hz showing the expected incremental behavior of the model. F) Simulated and scaled hysteresis curves for the AIST, GST, and WO<italic><sub>x</sub></italic> devices (not to scale).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g008" position="float" xlink:type="simple"/></fig>
<p>When it comes time to manufacture AHaH node circuitry, an ideal memristor will be chosen taking into consideration many properties. It is likely that some types of memristors will be better candidates, some will not be suitable at all, and that the best device has yet to be fabricated. Based on our current understanding, the ideal device would have low thresholds of adaptation (&lt;0.2 V), on-state resistance of ∼100 kΩ or greater, high dynamic range, durability, the capability of incremental operation with very short pulse widths and long retention times of a week or more. However, even devices that deviate considerably from these parameters will be useful in more specific applications. As an example, short retention times on the order of seconds are perfectly compatible with combinatorial optimizers.</p>
</sec><sec id="s3b">
<title>AHaH Circuit Simulation</title>
<p>Circuit simulations were carried out by solving for the voltage at node y in each AHaH node (<xref ref-type="fig" rid="pone-0085175-g005">Figure 5</xref>) using Kirchhoff’s Current law (KCL) during the read phase followed by updating all memristor conductance values according to the generalized MSS model given the voltage drop across each memristor and the read period length. During the write phase, the memristor conductance values were individually updated according to the generalized MSS model given the voltage drop across each memristor and the write period length. The source code for the circuit is available in AHaH21Circuit.java. Parameters for operation of the circuit were set as follows: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e236" xlink:type="simple"/></inline-formula> = 0.5 V, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e237" xlink:type="simple"/></inline-formula> = −0.5 V, read period (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e238" xlink:type="simple"/></inline-formula>) = 1 <italic>μ</italic>s, and write period (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e239" xlink:type="simple"/></inline-formula>) = 1 <italic>μ</italic>s. The number of input and bias memristors differed depending on the simulation task, as noted in each section below or in the source code.</p>
</sec><sec id="s3c">
<title>Spike Encoding</title>
<p>All machine learning applications built from AHaH nodes have one thing in common: the inputs to the AHaH nodes take as input a spike pattern. A spike pattern is a set of integers that specify which synapses in the AHaH node are coactive. In terms of a circuit, this is a description of what physical input lines are being driven by the driving voltage (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e240" xlink:type="simple"/></inline-formula>). All other inputs remain floating (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e241" xlink:type="simple"/></inline-formula>). Any data source can be converted into a spike encoding with a spike encoder. As an example, the eye converts electromagnetic radiation into spikes, the ear converts sound waves into spikes, and the skin converts pressure into spikes. Each of these may be considered a spike encoder and each is optimized for a specific data source.</p>
<p>A simple example makes spike encoding for an AHaH node clear. Suppose a dataset is available where the colors of a person’s clothes are associated with the sex of the person. The entire dataset consists of several colors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e242" xlink:type="simple"/></inline-formula> sex associations. For each person, the colors are mapped to an integer and added to a vector of variable length:<disp-formula id="pone.0085175.e243"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e243" xlink:type="simple"/><label>(28)</label></disp-formula>where red maps to 1, blue maps to 2, yellow maps to 3, etc. The spike patterns for this dataset are then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e244" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e245" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e246" xlink:type="simple"/></inline-formula>. In order to accommodate the range of spikes, the AHaH nodes would require at least five inputs or a spike space of five.</p>
<p>In the case of real-value numbers, a simple recursive method for producing a spike encoding can also conveniently be realized through strictly anti-Hebbian learning via a binary decision tree with AHaH nodes at each tree node. Starting from the root node and proceeding to the leaf node, the input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e247" xlink:type="simple"/></inline-formula> is summed with a bias <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e248" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e249" xlink:type="simple"/></inline-formula>. Depending on the sign of the result <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e250" xlink:type="simple"/></inline-formula>, it is routed in one direction or another toward the leaf nodes. The bias is updated according to anti-Hebbian learning, the practical result being a subtraction of an adaptive average:<disp-formula id="pone.0085175.e251"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e251" xlink:type="simple"/><label>(29)</label></disp-formula></p>
<p>If we then assign a unique integer to each node in the decision tree, the path that was taken from the root to the leaf becomes the spike encoding. This process is an adaptive analog to digital conversion. The source code used to generate this spike encoding is in <italic>AHaHA2D.java</italic>. This adaptive binning procedure can be extended to sparse-spike encoded patterns if.<disp-formula id="pone.0085175.e252"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e252" xlink:type="simple"/><label>(30)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e253" xlink:type="simple"/></inline-formula> is sampled randomly from the set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e254" xlink:type="simple"/></inline-formula> with equal frequency.</p>
</sec><sec id="s3d">
<title>Circuit and Functional Model Correspondence</title>
<p>We demonstrate that both the functional and circuit implementation of the AHaH node are equivalent and functioning correctly in order to establish a link between our benchmark results and the physical circuit. The source code for these experiments can be found in <italic>AHaHRuleFunctionalApp.java</italic> and <italic>AHaHRuleCircuitApp.java</italic> for both the functional and circuit form respectively. In both applications a four-input AHaH node receives the spike patterns from the set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e255" xlink:type="simple"/></inline-formula>, and the change in the synaptic weights is measured as a function of the output activation, y. Recall that we must encode the nonlinearly separable two-input channels into four-input linearly separable <italic>spike logic</italic> channels so that we can achieve all logic functions (XOR) directly with AHaH attractor states. For both the functional and circuit form of the AHaH node, a bias synapse is included in addition to the normal inputs.</p>
<p>In the derivation of the functional model, the assumption was made that the quantity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e256" xlink:type="simple"/></inline-formula> was constant (<xref ref-type="disp-formula" rid="pone.0085175.e163">Equation 12</xref>). This enabled the treatment of the output voltage as a sum over the input and bias weights. This condition of conservation of adaptive resources is also required in the thermodynamic model (<xref ref-type="disp-formula" rid="pone.0085175.e063">Equation 1</xref>). To demonstrate we have attained this conservation, the quantities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e257" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e258" xlink:type="simple"/></inline-formula> (<xref ref-type="disp-formula" rid="pone.0085175.e163">Equations 12</xref> and <xref ref-type="disp-formula" rid="pone.0085175.e165">14</xref>) are plotted for five different four-input AHaH nodes receiving the spike patterns from the set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e259" xlink:type="simple"/></inline-formula> for 1100 time steps. The source code for this experiment is in <italic>DifferentialWeightApp.java</italic>.</p>
</sec><sec id="s3e">
<title>AHaH Logic</title>
<p>A two input AHaH node will receive three possible spike patterns <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e260" xlink:type="simple"/></inline-formula> and converge to multiple attractor states. Each decision boundary plotted in <xref ref-type="fig" rid="pone-0085175-g002">Figure 2</xref> represents a state and its anti-state (i.e. an AHaH bit), since two solutions exist for each stable decision boundary. The 6 possible states are labeled A, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e261" xlink:type="simple"/></inline-formula>, B, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e262" xlink:type="simple"/></inline-formula>, C, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e263" xlink:type="simple"/></inline-formula>. Fifty two-input AHaH nodes with Ag-chalcogenide memristors were simulated. All AHaH nodes were initialized with random weights picked from a Gaussian distribution with low weight saturation. That is, the memristors were initialized close to their minimally conductive states. Each node was given a stream of 500 inputs randomly picked with equal probability from the set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e264" xlink:type="simple"/></inline-formula>. The source code for this experiment is in a file called <italic>TwoInputAttractorsApp.java</italic>, and there exists a functional form and a circuit form version to show correspondence between the two.</p>
<p>As stated earlier, the attractor states A, B, and C can be viewed as logic functions. It was earlier demonstrated how NAND gates can be used to make these attractor states computationally complete. It was also described how a spike encoding consisting of two input lines per channel can be used to achieve completeness directly with AHaH attractor states. To investigate this, 5000 AHaH nodes were initialized with random weights with zero mean. Each AHaH node was driven with 1000 spikes randomly selected from the set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e265" xlink:type="simple"/></inline-formula>. Finally, each AHaH node’s logic function was tested, and the distribution of logic functions was measured. The source code for this experiment is in <italic>SpikeLogicStateOccupationFrequencyApp.java</italic>, and there exists functional form and circuit form versions to show correspondence between the two.</p>
<p>To demonstrate that the attractor states and hence logic functions are stable over time, the above experiment can be repeated. However, the number of time steps can be significantly increased and the logic state of each AHaH node can be recorded at each time step. For this experiment, 100 AHaH nodes were randomly initialized, and their logic functions were tested over 50,000 time steps. The source code for this experiment is in <italic>SpikeLogicFuntionVsTimeApp.java</italic>, and there exists functional form and circuit form versions to show correspondence between the two.</p>
</sec><sec id="s3f">
<title>AHaH Clustering</title>
<p>Clustering is a method of knowledge discovery which automatically tries to find hidden structure in data in an unsupervised manner <xref ref-type="bibr" rid="pone.0085175-Jain1">[101]</xref>. Centroid-based clustering methods like <italic>k</italic>-means <xref ref-type="bibr" rid="pone.0085175-Lloyd1">[102]</xref> require that the user define the number of cluster centers ahead of time. Density-based methods can be used without predefining cluster centers, but can fail if the clusters are of various densities <xref ref-type="bibr" rid="pone.0085175-Kriegel1">[103]</xref>. <xref ref-type="sec" rid="s3">Methods</xref> such as OPTICS <xref ref-type="bibr" rid="pone.0085175-Ankerst1">[104]</xref> address the problem of variable cluster densities, but introduce the problem that they expect some kind of density drop, which leads to arbitrary cluster borders. On datasets consisting of a mixture of known cluster distributions, density-based clustering algorithms are outperformed by distribution-based methods such as expectation maximization (EM) clustering <xref ref-type="bibr" rid="pone.0085175-Dempster1">[105]</xref>. However, EM clustering assumes that the data is a mixture of a known distribution and as such is not able to model density-based clusters. It is furthermore prone to over-fitting.</p>
<p>An AHaH node converges to attractor states that cleanly partition its input space by maximizing the margin between opposing data distributions. The set of AHaH attractor states are furthermore computationally complete. These two properties enable a sufficiently large collective of AHaH nodes to assign unique labels to unique input data distributions while maintaining a high level of tolerance to noise. If a collective of AHaH nodes are allowed to randomly fall into attractor states, the binary output vector is a label for the input feature. For example, a four node collective with outputs (0,0,0,1) would encode the output ‘0001’ and, if converted to base-10 integers, be assigned the cluster ID ‘1’. The collective node output (1,1,1,1) would encode the output ‘1111’ and be assigned the cluster ID ‘15’. Such a collective is called an AHaH clusterer.</p>
<p>The total number of possible output labels from the AHaH collective is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e266" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e267" xlink:type="simple"/></inline-formula> is the number of AHaH nodes in the collective. The collective may output the same label for different spike patterns if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e268" xlink:type="simple"/></inline-formula> is small and/or the number of patterns, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e269" xlink:type="simple"/></inline-formula>, is high. However, as the number of AHaH nodes increases, the probability of this occurring drops exponentially. Under the assumption that all attractor states are equally likely, the probability that any two unique spike patterns, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e270" xlink:type="simple"/></inline-formula>, will be assigned the same binary label is:<disp-formula id="pone.0085175.e271"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e271" xlink:type="simple"/><label>(31)</label></disp-formula></p>
<p>For example, given 64 spike patterns and 16 AHaH nodes, the probability of the collective assigning the same label is 3%. By increasing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e272" xlink:type="simple"/></inline-formula> to 32, the probability falls to less than one in a million.</p>
<p>We developed a quantitative metric to characterize the performance of our AHaH clusterer. Given a unique spike pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e273" xlink:type="simple"/></inline-formula> we would ideally like a unique label <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e274" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e275" xlink:type="simple"/></inline-formula>). This is complicated by the presence of noise, occlusion, and non-stationary data or drift. Failure can occur in two ways. First, if the same underlying pattern is given more than one label, we may say that the AHaH clusterer is <italic>diverging</italic>. We measure the divergence, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e276" xlink:type="simple"/></inline-formula>, as the inverse of the average labels per pattern. Second, if two different patterns are given the same label, we may say that it is <italic>converging</italic>. We measure convergence, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e277" xlink:type="simple"/></inline-formula>, as the inverse of the average patterns per label.</p>
<p>Divergence and convergence may be combined to form a composite measure we call <italic>vergence</italic>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e278" xlink:type="simple"/></inline-formula>:<disp-formula id="pone.0085175.e279"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e279" xlink:type="simple"/><label>(32)</label></disp-formula></p>
<p>Perfect clustering extraction will occur with a vergence value of 1. The code used to encapsulate the vergence measurement can be found in <italic>VergenceEvaluator.java</italic>.</p>
<p>To investigate the AHaH clusterer’s performance as measured by our vergence metric, we swept the following parameters individually while holding the others constant: learning rate (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e280" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e281" xlink:type="simple"/></inline-formula>), number of AHaH nodes, number of noise bits, spike pattern length, and number of spike patterns. The applications used to perform the sweeps can be found in the files <italic>SweepLearningRateApp.java</italic>, <italic>SweepNumAhahNodesApp.java</italic>, <italic>SweepNumNoiseBitsVsSpikePatternLengthApp.java</italic>, <italic>SweepSpikePatternLengthApp.java</italic>, and <italic>SweepNumSpikePatternsApp.java</italic>, respectively.</p>
<p>The number of inputs to the AHaH nodes making up the AHaH clusterer was 256. Synthetic spike patterns were created with a random spike pattern generator. Given a spike pattern length, the number of inputs available on the AHaH nodes and the number of unique spike patterns, a set of spike patterns was generated. Noise is generated by taking random input lines and activating them, or, if the input line is already active, deactivating it. The number of patterns that can be distinguished by the AHaH clusterer before vergence falls is a function of the input pattern sparsity, number of total patterns and the pattern noise. Both functional-based and circuit-based AHaH clusterers were investigated and showed good correspondence.</p>
<p>While the vergence experiments provide a quantitative measure of the characteristics of the AHaH clusterer, we also designed a program to qualitatively visualize the clustering capabilities. The basic idea is to create several spatial clusters in two-dimensional space and let the clusterer automatically determine the boundaries between clusters in an unsupervised manner. We used a <italic>k</italic>-nearest neighbor algorithm to translate the spatial location of cluster points into a spike representation, although other spike encoding methods are of course possible. The AHaH clusterer converges to attractor states that map spike patterns to integer, which is in turn mapped to a unique color. The visualizations give the observer a sense of how tolerant the AHaH clusterer is to variations in cluster type, size and temporal stability. The code for the clustering visualization is in <italic>ClusteringApp.java</italic>. There are several different visualizations including clusters of various sizes, arrangements, and numbers, both stationary and non-stationary.</p>
</sec><sec id="s3g">
<title>AHaH Classification</title>
<p>Linear classification is a useful tool used in the field of machine learning to characterize and apply labels to samples from datasets. State of the art approaches to classification include algorithms such as decision trees, random forests, support vector machines (SVM) and naïve Bayes <xref ref-type="bibr" rid="pone.0085175-Kotsiantis1">[106]</xref>. These approaches are used in real world applications such as image recognition, data mining, spam filtering, voice recognition, and fraud detection. Our AHaH-based linear classifier is different from these techniques mainly in that it is not just another algorithm; it can be realized as a physically adaptive circuit. This presents several competitive advantages, the main one being that such a circuit would increase the speed and reduce power consumption dramatically while eliminating the problems associated with disk I/O bottlenecks experienced in large scale data mining applications <xref ref-type="bibr" rid="pone.0085175-Yu1">[107]</xref>.</p>
<p>The AHaH classifier consists of one or more AHaH nodes, each node assigned to a classification label and each operating the supervised form of the AHaH rule of <xref ref-type="disp-formula" rid="pone.0085175.e184">Equation 17</xref>. In cases where a supervisory signal is not available, the unsupervised form of the rule (<xref ref-type="disp-formula" rid="pone.0085175.e186">Equation 18</xref>) may be used. Larger magnitude AHaH node output activations are interpreted as a higher confidence. There are multiple ways to interpret the output of the classifier depending on the situation. First, one can order all AHaH node outputs and choose the most positive. This method is ideal when only one label per pattern is needed and an output must always be generated. Second, one can choose all AHaH node outputs that exceed a confidence threshold. This method can be used when multiple labels exist for each input pattern. Finally, only the most positive AHaH node output is chosen if it exceeds a threshold, otherwise nothing is returned. This method can be used when only one label per pattern is needed, but rejection of a pattern is allowed.</p>
<p>To compare the AHaH classifier to other state of the art classification algorithms, we chose four popular classifier benchmark data sets: the Breast Cancer Wisconsin (Original), Census Income, MNIST Handwritten Digits, and the Reuters-21578 data sets. The source code for these classification experiments is found in <italic>BreastCancerFunctionalApp.java</italic>, <italic>CensusIncomeApp.java</italic>, <italic>MnistApp.java</italic>, and <italic>Reuters21578App.java</italic>, respectively.</p>
<p>We scored the classifiers’ performance using standard classification metrics: precision, recall, F1, and accuracy. Information on these metrics and how they are used is widely available. The standard training and test sets were used for learning and testing respectively. More information about these benchmark datasets is widely available, and a large amount of classification algorithms have been benchmarked against them including SVM, naïve Bayes, and decision trees.</p>
<p>To further validate an AHaH classifier implemented with circuit AHaH nodes against functional AHaH nodes, we use the Breast Cancer Wisconsin (Original) benchmark dataset. This dataset is relatively small allowing the circuit level simulations to complete quickly. Each sample is either labeled <italic>benign</italic> or <italic>malignant</italic>. There were a total of 683 samples. The first 500 were designated as the training set and the last 183 as the test set. Our spike encoder for this data set produced a total of 70 unique spikes requiring 70 inputs for this particular classifier. The source code for the circuit form of the Breast Cancer Wisconsin experiment is in <italic>BreastCancerCircuitApp.java</italic>.</p>
<p>Continuous valued inputs were converted using the adaptive decision tree method of <xref ref-type="disp-formula" rid="pone.0085175.e251">Equation 29</xref>. Text was converted to a bag-of-words representation where each unique word was representative of a unique spike. MNIST image data was first thresholded to produce a spike list of active pixels. The spike list in each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e282" xlink:type="simple"/></inline-formula> image patch was converted to a single spike via the method of <xref ref-type="disp-formula" rid="pone.0085175.e252">Equation 30</xref>. The image patch was convolved and pooled over an <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e283" xlink:type="simple"/></inline-formula> pixel region. The result of this procedure is a list of spikes with moderate translational invariance, which was fed to the AHaH classifier. The source code for this procedure is available in <italic>MnistSpikeEncoder.java</italic>.</p>
<p>The AHaH classifier is capable of unsupervised learning by evoking <xref ref-type="disp-formula" rid="pone.0085175.e184">Equation 17</xref>. If no supervised labels are given but the classifier is able to output labels with high confidence, the output can be assumed to be correct and used as the supervised signal. The result is a continued convergence into the attractor state, which represent a point of maximal margin. This has application in any domain where large volumes of unlabeled data exist such as image recognition. By allowing the classifier to process these unlabeled examples, it can continue to improve its performance (bootstrap) without supervised labels.</p>
<p>To demonstrate this unsupervised learning capability we used the Reuters-21578 dataset. The entire training and test sets were lumped together and the classifier was given the first 25% inputs in a supervised manner. For the remaining 75% of the news articles, the classifier was run in an unsupervised manner. Only when the confidence was 1.0, which indicates high certainty of a correct answer, did the classifier use its own classification as a supervised training signal. The F1 score was recorded after each story for the following most frequent labels: <italic>earn</italic>, <italic>acq</italic>, <italic>money-fx</italic>, <italic>grain</italic>, <italic>crude</italic>, <italic>trade</italic>, <italic>interest</italic>, <italic>ship</italic>, <italic>wheat</italic>, and <italic>corn</italic>, a common label set used in many benchmarking exercises using this dataset. The source code for this experiment is in <italic>Reuters21578SemiSupervisedApp.java</italic>.</p>
</sec><sec id="s3h">
<title>AHaH Signal Prediction</title>
<p>Complex signal prediction involves using the prior history of a signal or group of signals to predict the future state of the signal. Signal prediction, also known as signal forecasting, is used in adaptive filters, resource planning and action selection. Some real world examples include production estimating, retail inventory planning, inflation prediction, insurance risk assessment, and weather forecasting. Current prediction algorithms include principle component analysis and regression and Kalman filtering <xref ref-type="bibr" rid="pone.0085175-Ndong1">[108]</xref>, artificial neural networks <xref ref-type="bibr" rid="pone.0085175-Zhang2">[109]</xref> and Bayesian model averaging <xref ref-type="bibr" rid="pone.0085175-Hoeting1">[110]</xref>.</p>
<p>By posing signal prediction as a multi-label classification problem, complex signals can be learned and predicted using the AHaH classifier. As a simplified proof of concept exercise to demonstrate this, a complex temporal signal prediction experiment was designed. For each moment of time, the real-valued signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e284" xlink:type="simple"/></inline-formula> is converted into a sparse feature representation using the method of <xref ref-type="disp-formula" rid="pone.0085175.e251">Equation 29</xref>. These features are buffered to form a feature set:<disp-formula id="pone.0085175.e285"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e285" xlink:type="simple"/><label>(33)</label></disp-formula></p>
<p>This feature set is then used to make predictions of the current feature <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e286" xlink:type="simple"/></inline-formula>, and the spikes of the current feature are used as supervised labels. After learning, the output prediction may be used in lieu of the actual input and run forward recursively in time. In this way, extended predictions about the future are possible. The source code for the experiment is available in <italic>ComplexSignalPredictionApp.java</italic>. The signal was generated from the summation of five sinusoidal signals with randomly chosen amplitudes, periods, and phases. The experiment ran for a total of 10,000 time steps. During the last 300 time steps, recursive prediction occurred.</p>
</sec><sec id="s3i">
<title>AHaH Motor Control</title>
<p>Motor control is the process by which sensory information about the world and the current state of the body is used to execute actions to generate movement. Stabilizing Hebbian feedback applied to an AHaH node can occur any time after the Anti-Hebbian read, which opens the interesting possibility of using AHaH nodes for reinforcement-based learning. Here we show that a small collective of AHaH nodes, an AHaH motor controller, can be used in autonomous robotic control. As a proof-of-concept experiment we use an AHaH motor controller to guide a multi-jointed robotic arm to a target based on a value signal or cost function.</p>
<p>A virtual environment in which an AHaH motor controller controls the angles of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e287" xlink:type="simple"/></inline-formula> connected fixed length rods in order to make contact with a target was created as shown in <xref ref-type="fig" rid="pone-0085175-g009">Figure 9</xref>. The arm rests on a plane with its base anchored at the center, and all the joints have 360 degrees of freedom to rotate. New targets are dropped randomly within the robotic arm’s reach radius after it captures a target. The robotic arm virtual environment is part of an open source project called Proprioceptron, also available at <ext-link ext-link-type="uri" xlink:href="http://xeiam.com" xlink:type="simple">http://xeiam.com</ext-link>. Proprioceptron builds upon a 3D gaming library and offers virtual worlds and challenges for testing motor control algorithms. The robotic arm challenge offers 5 levels of difficulty starting with stationary targets, increasing target lateral speed as the levels increase.</p>
<fig id="pone-0085175-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g009</object-id><label>Figure 9</label><caption>
<title>Unsupervised robotic arm challenge.</title>
<p>The robotic arm challenge involves a multi-jointed robotic arm that moves to capture a target. Each joint on the arm has 360 degrees of rotation, and the base joint is anchored to the floor. Using only a value signal relating the distance from the head to the target and an AHaH motor controller taking as input sensory stimuli in a closed-loop configuration, the robotic arm autonomously learns to capture stationary and moving targets. New targets are dropped within the arm’s reach radius after each capture, and the number of discrete angular joint actuations required for each catch is recorded to asses capture efficiency.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g009" position="float" xlink:type="simple"/></fig>
<p>Sensors measure the relative joint angles of each segment of the robot arm as well as the distance from the target ball to each of two “eyes” located on the side of the arm’s “head”. Sensor measurements are converted into a sparse spiking representation using the method of <xref ref-type="disp-formula" rid="pone.0085175.e251">Equation 29</xref>. A value signal is computed as the inverse distance of the head to the target:<disp-formula id="pone.0085175.e288"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e288" xlink:type="simple"/><label>(34)</label></disp-formula></p>
<p>Opposing “muscles” actuate each joint. Each muscle is formed of many “fibers” and a single AHaH node controls each fiber. The number of discrete angular steps that move each joint, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e289" xlink:type="simple"/></inline-formula>, is given by:<disp-formula id="pone.0085175.e290"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e290" xlink:type="simple"/><label>(35)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e291" xlink:type="simple"/></inline-formula> is the number of muscle fibers, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e292" xlink:type="simple"/></inline-formula> is the post-synaptic activation of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e293" xlink:type="simple"/></inline-formula> AHaH node controlling the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e294" xlink:type="simple"/></inline-formula> muscle fiber of the primary muscle, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e295" xlink:type="simple"/></inline-formula> is the post-synaptic activation of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e296" xlink:type="simple"/></inline-formula> AHaH node controlling the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e297" xlink:type="simple"/></inline-formula> muscle fiber of the opposing muscle, and H is the Heaviside step function. The number of discrete angular steps moved in each joint at each time step is then given by the difference in these two values.</p>
<p>Given a movement we can say if a fiber (AHaH node) acted for or against it. We can further determine if the movement was good or bad by observing the change in the value signal. If, at a later time, the value increased after a movement, then each fiber responsible for the movement receive rewarding Hebbian feedback. Likewise, if the fiber acted in support of a movement and later the value signal dropped, then the fiber is denied a Hebbian update. As the duration of time between movement and reward increases, so does the difficulty of the problem since many movements can be taken during the interval. A reinforcement scheme can be implemented in a number of ways over a number of timescales and may even be combined. For example, we may integrate over a number of time scales to determine if the value increased or decreased.</p>
<p>Experimental observation led to constant values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e298" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e299" xlink:type="simple"/></inline-formula> for the AHaH rule, although generally good performance was observed for a wide range of values. The choice of these parameters is influenced by the complexity of the problem and the need to learn complex compound sequences, as well as the duration between action (anti-Hebbian) and reward (Hebbian).</p>
<p>We measured the robotic arm’s efficiency in catching targets by summing the total number of discrete angular joint actuations from the time the target was placed until capture. As a control, the same challenge was carried out using a simple random actuator. The challenge was carried out for both AHaH-controlled and random-controlled robotic arm actuation for different robotic arm lengths ranging from 3 to 21 joints in increments of three. The total joint actuation is the average amount of discrete joint actuation over the 100 captured targets. The source code for this experiment is available in <italic>RoboticArmApp.java</italic>.</p>
</sec><sec id="s3j">
<title>AHaH Combinatorial Optimization</title>
<p>An AHaH node will descend into a probabilistic output state if the Hebbian feedback is withheld. As the magnitude of the synaptic weight falls closer to zero, the chance that state transitions will occur rises from 0% to 50%. This property can be exploited in probabilistic search and optimization tasks. Consider a combinatorial optimization task such as the traveling salesman problem where the city-to-city path is encoded as a binary vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e300" xlink:type="simple"/></inline-formula>. The space of all possible paths can be visualized as the leaves of a binary tree of depth <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e301" xlink:type="simple"/></inline-formula>. The act of constructing a path can be seen as a routing procedure traversing the tree from trunk to leaf. By allowing prior attempted solutions to modify the routing probabilities, an initial uniform routing distribution can collapse into a subspace of more optimal solutions.</p>
<p>This can be accomplished by utilizing an AHaH node with a single input as a node within a virtual routing tree. As a route progresses from the trunk to a leaf, each AHaH node is evaluated for its state and receives the anti-Hebbian update. Should the route result in a solution that is better than the average solution, all nodes along the routing path receive a Hebbian update. By repeating the procedure over and over again, a positive feedback loop is created such that more optimal routes result in higher route probabilities that, in turn, result in more optimal routes. The net effect is a collapse of the route probabilities from the trunk to the leaves as a path is locked in. The process is intuitively similar to the formation of a lighting strike searching for a path to ground and as such we call it a <italic>strike search</italic>.</p>
<p>To evaluate the AHaH combinatorial optimizer, we used the functional model (<xref ref-type="disp-formula" rid="pone.0085175.e184">Equation 17</xref>), setting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e302" xlink:type="simple"/></inline-formula> and making it a free parameter we call the <italic>learning rate</italic>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e303" xlink:type="simple"/></inline-formula>:<disp-formula id="pone.0085175.e304"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e304" xlink:type="simple"/><label>(36)</label></disp-formula></p>
<p>The experiment consists of 200 strike searches, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e305" xlink:type="simple"/></inline-formula> is set to a value chosen randomly from between 0.00015 and 0.0035 at the start of each trial. The noise variable, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e306" xlink:type="simple"/></inline-formula>, is picked from a random Gaussian distribution with zero mean and 0.025 variance. After every 10,000 solution attempts, branches with synaptic weight magnitudes less than 0.01 are pruned. A 64-city network is created where each city is directly connected to every other city, and the city coordinates are picked from a random Gaussian distribution with zero mean and a variance of one. The city path is encoded as a bit sequence such that the first city is encoded with 6 bits, and each successive city with only as many bits needed to resolve the remaining cities such that the second-to-last city requires one bit. The value of the solution is the path distance, which we are attempting to minimize. The strike process is terminated when the same solution is generated 5 successive times, indicating convergence. A random search is used as a control, where each new solution attempt is picked from a uniform random distribution. The code for this experiment is in <italic>StrikeSearchApp.java</italic>.</p>
</sec></sec><sec id="s4">
<title>Results and Discussion</title>
<sec id="s4a">
<title>AHaH Rule</title>
<p>The AHaH rule reconstructions for the functional and circuit forms of the AHaH node are shown in <xref ref-type="fig" rid="pone-0085175-g010">Figures 10A and 10B</xref> respectively. In both cases, the AHaH rule is clearly represented and there is congruence between both forms. However, <xref ref-type="fig" rid="pone-0085175-g010">Figure 10B</xref> hides complexity in the circuit that arises from the differential aspect of the weights and their limited dynamic range. Because of this, depending on the saturation state of a weight, the form of weight update may change over time. The AHaH rule reconstruction of <xref ref-type="fig" rid="pone-0085175-g010">Figure 10B</xref> is thus for a specific weight initialization for a specific time interval.</p>
<fig id="pone-0085175-g010" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g010</object-id><label>Figure 10</label><caption>
<title>The AHaH rule reconstructed from simulations.</title>
<p>Each data point represents the change in a synaptic weight as a function of AHaH node activation, y. Blue data points correspond to input synapses and red data points to bias inputs. There is good congruence between the A) functional and B) circuit implementations of the AHaH rule.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g010" position="float" xlink:type="simple"/></fig>
<p>As part of our functional model derivation (<xref ref-type="disp-formula" rid="pone.0085175.e163">Equation 12</xref>) and the connection to thermodynamics (<xref ref-type="disp-formula" rid="pone.0085175.e063">Equation 1</xref>) we require that the quantity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e307" xlink:type="simple"/></inline-formula> remains constant. As can be seen in <xref ref-type="fig" rid="pone-0085175-g011">Figure 11</xref>, the quantity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e308" xlink:type="simple"/></inline-formula> does indeed asymptote and remains constant.</p>
<fig id="pone-0085175-g011" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g011</object-id><label>Figure 11</label><caption>
<title>Justification of constant weight conjugate.</title>
<p>Multiple AHaH nodes receive spike patterns from the set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e049" xlink:type="simple"/></inline-formula> while the weight and weight conjugate is measured. Blue = weight conjugate (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e050" xlink:type="simple"/></inline-formula>), Red = weight (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e051" xlink:type="simple"/></inline-formula>). The quantity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e052" xlink:type="simple"/></inline-formula> has a much lower variance than the quantity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e053" xlink:type="simple"/></inline-formula> over multiple trials, justifying the assumption that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e054" xlink:type="simple"/></inline-formula> is a constant factor.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g011" position="float" xlink:type="simple"/></fig></sec><sec id="s4b">
<title>AHaH Logic</title>
<p>The 2-input AHaH node receiving 500 consecutive inputs randomly chosen from the set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e309" xlink:type="simple"/></inline-formula> at 50 different initial synaptic weights evolves into one of the six attractor basins as shown in <xref ref-type="fig" rid="pone-0085175-g012">Figure 12</xref>. Labels A, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e310" xlink:type="simple"/></inline-formula>, B, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e311" xlink:type="simple"/></inline-formula>, C, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e312" xlink:type="simple"/></inline-formula> indicate the attractor basins in these weight-space plots and correspond to the equivalent decision boundaries shown in <xref ref-type="fig" rid="pone-0085175-g002">Figure 2</xref>. The same experiment was performed with the functional form and the circuit form of the AHaH node (<xref ref-type="fig" rid="pone-0085175-g012">Figure 12A and 12B</xref> respectively) and close correspondence can be seen.</p>
<fig id="pone-0085175-g012" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g012</object-id><label>Figure 12</label><caption>
<title>Attractor states of a two-input AHaH node under the three-pattern input.</title>
<p>The AHaH rule naturally forms decision boundaries that maximize the margin between data distributions. Weight space plots show the initial weight coordinate (green circle), the final weight coordinate (red circle) and the path between (blue line). Evolution of weights from a random normal initialization to attractor basins can be clearly seen for both the functional model (A) and circuit model (B).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g012" position="float" xlink:type="simple"/></fig>
<p>After being initialized with random synaptic weights, the occupation of logic states of AHaH nodes receiving the spike logic patterns of <xref ref-type="table" rid="pone-0085175-t001">Table 1</xref> are shown in <xref ref-type="fig" rid="pone-0085175-g013">Figure 13</xref> for both functional and circuit models. Each logic function was assigned a unique integer value as in <xref ref-type="table" rid="pone-0085175-t004">Table 4</xref>. Experimental results show congruence between the functional form and circuit form simulations. All linear functions are represented by distinct AHaH attractor states. Absent are the expected nonlinear XOR functions 6 and 9. These functions are possible through combinations of other logic functions, meaning a multi-stage AHaH node network is capable of achieving any logic function. Since any algorithm or program can be reduced to successive utilizations of logic gates, the attractor states of AHaH nodes support universal computation. Logic functions remain stable over time, as indicated by <xref ref-type="fig" rid="pone-0085175-g013">Figure 13B</xref>.</p>
<fig id="pone-0085175-g013" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g013</object-id><label>Figure 13</label><caption>
<title>AHaH attractor states as logic functions.</title>
<p>A) Logic state occupation frequency after 5000 time steps for both functional model and circuit model. All logic functions can be attained directly from attractor states except for XOR functions, which can be attained via multi-stage circuits. B) The logic functions are stable over time for both functional model and circuit model, indicating stable attractor dynamics.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g013" position="float" xlink:type="simple"/></fig><table-wrap id="pone-0085175-t004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.t004</object-id><label>Table 4</label><caption>
<title>Logic functions.</title>
</caption><alternatives><graphic id="pone-0085175-t004-4" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.t004" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">SP⇓, LF⇒</td>
<td align="left" rowspan="1" colspan="1">15</td>
<td align="left" rowspan="1" colspan="1">14</td>
<td align="left" rowspan="1" colspan="1">13</td>
<td align="left" rowspan="1" colspan="1">12</td>
<td align="left" rowspan="1" colspan="1">11</td>
<td align="left" rowspan="1" colspan="1">10</td>
<td align="left" rowspan="1" colspan="1">9</td>
<td align="left" rowspan="1" colspan="1">8</td>
<td align="left" rowspan="1" colspan="1">7</td>
<td align="left" rowspan="1" colspan="1">6</td>
<td align="left" rowspan="1" colspan="1">5</td>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">(<italic>z</italic>, 1, <italic>z</italic>, 1)</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">0</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">(<italic>z</italic>, 1, 1, <italic>z</italic>)</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">0</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">(1, <italic>z</italic>, <italic>z</italic>, 1)</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">0</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">(1, <italic>z</italic>, 1, <italic>z</italic>)</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">0</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt104"><label/><p>The table defines all 16 possible logic functions (LF) for the four spike encoded input patterns (SP).</p></fn></table-wrap-foot></table-wrap>
<p>Logic functions 0 and 15 represent the null state and their occupation is inhibited through the action of the bias. By increasing the number of bias inputs from 1 to 3 we can collapse the stable attractor logic states down to 3, 5, 10 and 12. These functions represent the pure independent component states and act to pass or invert each of the two input channels. Although these states are not computationally complete, they can be made so via the use of NAND gates as we discussed in the theory section. The advantage of using states 3, 5, 10 and 12 is that they are very stable. The disadvantage is that we must now rely on external circuitry (i.e. NAND gates) to achieve computational universality.</p>
</sec><sec id="s4c">
<title>AHaH Clustering</title>
<p>The AHaH clusterer parameter sweep experiment results are summarized in <xref ref-type="table" rid="pone-0085175-t005">Table 5</xref>. While setting the free parameters at their default values and sweeping the parameter under investigation, the range of that parameter that resulted in a vergence value greater than 0.90 was determined. The performance of the AHaH clusterer proved to be robust to input pattern noise. For example, the clusterer can achieve perfect performance with up to 18% noise under a 100% pattern load. A full pattern load occurs when the number of patterns (16) multiplied by the pattern size (16) is equal to the total number of input lines (256 in this case). The clusterer can achieve greater than 90% vergence with up to 44% noise, meaning 7 of the 16 spike input pattern’s bits are reassigned random values.</p>
<table-wrap id="pone-0085175-t005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.t005</object-id><label>Table 5</label><caption>
<title>AHaH clusterer sweep results.</title>
</caption><alternatives><graphic id="pone-0085175-t005-5" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.t005" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Learning Rate</td>
<td align="left" rowspan="1" colspan="1">Number ofAHaH nodes</td>
<td align="left" rowspan="1" colspan="1">Number ofNoise Bits</td>
<td align="left" rowspan="1" colspan="1">Spike PatternLength</td>
<td align="left" rowspan="1" colspan="1">Number ofSpike Patterns</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Default Value</td>
<td align="left" rowspan="1" colspan="1">0.0005</td>
<td align="left" rowspan="1" colspan="1">20</td>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">16</td>
<td align="left" rowspan="1" colspan="1">16</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Range</td>
<td align="left" rowspan="1" colspan="1">.0002–.0012</td>
<td align="left" rowspan="1" colspan="1">&gt;7</td>
<td align="left" rowspan="1" colspan="1">&lt; = 7</td>
<td align="left" rowspan="1" colspan="1">&lt; = 36</td>
<td align="left" rowspan="1" colspan="1">&lt; = 28</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt105"><label/><p>While sweeping each parameter of the AHaH clusterer and holding the others constant at their default values, the reported range is where the vergence remained greater than 90%.</p></fn></table-wrap-foot></table-wrap>
<p>The results shown in <xref ref-type="fig" rid="pone-0085175-g014">Figure 14</xref> illustrate that the performance as measured by vergence degrades as the number of spike patterns increase. This result is explained by the fact that AHaH plasticity is acting to maximize the margin between data distributions or patterns. As the number of patterns increases, the margin must decrease and hence becomes more susceptible to noise. For example, under a 200% pattern load (32 patterns), vergence falls below 90% after 12.5% noise (2 noise bits). Comparing <xref ref-type="fig" rid="pone-0085175-g014">Figure 14A and 14B</xref> further demonstrates that circuit and functional models produce similar results. Without noise, the clusterer has impressive capacity and can reliably assign labels to spike patterns with load factors that exceed 400%.</p>
<fig id="pone-0085175-g014" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g014</object-id><label>Figure 14</label><caption>
<title>AHaH clusterer.</title>
<p>Functional (A) and circuit (B) simulation results of an AHaH clusterer formed of twenty AHaH nodes. Spike patterns were encoded over 16 active input lines from a total spike space of 256. The number of noise bits was swept from 1 (6.25%) to 10 (62.5%) while the vergence was measured. The performance is a function of the total number of spike patterns. Blue = 16 (100% load), Orange = 20 (125% load), Purple = 24 (150% load), Green = 32 (200% load), Red = 64 (400% load).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g014" position="float" xlink:type="simple"/></fig>
<p><xref ref-type="fig" rid="pone-0085175-g015">Figure 15</xref> shows screen shots of three different two-dimensional clustering visualizations. The AHaH clusterer performs well for clusters of various sizes and numbers as well as non-Gaussian clusters even though it does not need to know the number of clusters ahead of time or the expected cluster forms. Videos of similar clustering tasks shown in <xref ref-type="fig" rid="pone-0085175-g015">Figure 15</xref> can be viewed in the online Supporting Information section (<xref ref-type="supplementary-material" rid="pone.0085175.s001">Videos S1</xref>– <xref ref-type="supplementary-material" rid="pone.0085175.s003">S3</xref>). <xref ref-type="supplementary-material" rid="pone.0085175.s004">Video S4</xref> shows a two-dimensional clustering visualization with moving clusters.</p>
<fig id="pone-0085175-g015" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g015</object-id><label>Figure 15</label><caption>
<title>Two-dimensional spatial clustering demonstrations.</title>
<p>The AHaH clusterer performs well across a wide range of different 2D spatial cluster types, all without predefining the number of clusters or the expected cluster types. A) Gaussian B) non-Gaussian C) random Gaussian size and placement.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g015" position="float" xlink:type="simple"/></fig>
<p>The results show that the AHaH clusterer is able to handle a spectrum of cluster types. We demonstrate the ability to detect Gaussian and non-Gaussian clusters, clusters of non-equal size, as well as non-stationary clusters. Whereas other methods have intrinsic failure modes for certain types of clusters, our method can apparently handle a wide range of cluster types. Although more work must be done to fully compare our methods to existing clustering methods, our results thus far indicate that our method offers a genuinely new clustering mechanism with a number of distinct advantages. The most significant advantage is that we can implement the AHaH clusterer in physically adaptive AHaH circuits. In other words, clustering can now become an adaptive hardware resource.</p>
</sec><sec id="s4d">
<title>AHaH Classification</title>
<p>Our AHaH classifier benchmark scores for the Breast Cancer Wisconsin (Original), Census Income, MNIST Handwritten Digits, and the Reuters-21578 data sets are shown in <xref ref-type="table" rid="pone-0085175-t006">Table 6</xref> along with results from other published studies using their respective classification methods. Our results compare well to published benchmarks and consistently match or exceed SVM performance. This is surprising given the simplicity of the approach, which amounts to simple sparse spike encoding followed by classification with independent AHaH nodes.</p>
<table-wrap id="pone-0085175-t006" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.t006</object-id><label>Table 6</label><caption>
<title>Benchmark classification results.</title>
</caption><alternatives><graphic id="pone-0085175-t006-6" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.t006" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td colspan="2" align="left" rowspan="1">Breast Cancer Wisconsin (Original)</td>
<td colspan="2" align="left" rowspan="1">Census Income</td>
<td colspan="2" align="left" rowspan="1">MNIST Handwritten Digits</td>
<td colspan="2" align="left" rowspan="1">Reuters-21578</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">AHaH</td>
<td align="left" rowspan="1" colspan="1">.997</td>
<td align="left" rowspan="1" colspan="1">AHaH</td>
<td align="left" rowspan="1" colspan="1">.853</td>
<td align="left" rowspan="1" colspan="1">AHaH</td>
<td align="left" rowspan="1" colspan="1">.98–.99</td>
<td align="left" rowspan="1" colspan="1">AHaH</td>
<td align="left" rowspan="1" colspan="1">.92</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">RS-SVM <xref ref-type="bibr" rid="pone.0085175-Chen1">[115]</xref></td>
<td align="left" rowspan="1" colspan="1">1.0</td>
<td align="left" rowspan="1" colspan="1">NBTree <xref ref-type="bibr" rid="pone.0085175-Kohavi1">[116]</xref></td>
<td align="left" rowspan="1" colspan="1">.86</td>
<td align="left" rowspan="1" colspan="1">deep convex net <xref ref-type="bibr" rid="pone.0085175-Deng1">[117]</xref></td>
<td align="left" rowspan="1" colspan="1">.992</td>
<td align="left" rowspan="1" colspan="1">SVM <xref ref-type="bibr" rid="pone.0085175-Joachims1">[118]</xref></td>
<td align="left" rowspan="1" colspan="1">.864</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">SVM <xref ref-type="bibr" rid="pone.0085175-Bennett1">[119]</xref></td>
<td align="left" rowspan="1" colspan="1">.972</td>
<td align="left" rowspan="1" colspan="1">naïve-Bayes <xref ref-type="bibr" rid="pone.0085175-Kohavi1">[116]</xref></td>
<td align="left" rowspan="1" colspan="1">.84</td>
<td align="left" rowspan="1" colspan="1">large conv. net <xref ref-type="bibr" rid="pone.0085175-Ranzato1">[120]</xref></td>
<td align="left" rowspan="1" colspan="1">.991</td>
<td align="left" rowspan="1" colspan="1">C4.5 <xref ref-type="bibr" rid="pone.0085175-Joachims1">[118]</xref></td>
<td align="left" rowspan="1" colspan="1">.794</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">C4.5 <xref ref-type="bibr" rid="pone.0085175-Quinlan1">[121]</xref></td>
<td align="left" rowspan="1" colspan="1">.9474</td>
<td align="left" rowspan="1" colspan="1">C4.5 <xref ref-type="bibr" rid="pone.0085175-Kohavi1">[116]</xref></td>
<td align="left" rowspan="1" colspan="1">.858</td>
<td align="left" rowspan="1" colspan="1">polynomial SVM <xref ref-type="bibr" rid="pone.0085175-SchlkopfSimard1">[42]</xref></td>
<td align="left" rowspan="1" colspan="1">.986</td>
<td align="left" rowspan="1" colspan="1">naïve-Bayes <xref ref-type="bibr" rid="pone.0085175-Joachims1">[118]</xref></td>
<td align="left" rowspan="1" colspan="1">.72</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt106"><label/><p>AHaH classifier classification scores for the Breast Cancer, Census Income, MNIST Handwritten Digits and Reuters-21578 classification benchmark datasets. The AHaH classifier results compare favorably with other methods. Higher scores on the MNIST dataset are possible by increasing the resolution of the spike encoding.</p></fn></table-wrap-foot></table-wrap>
<p>In comparing our MNIST results with other methods, it is important to account for data preprocessing and artificial inflation of the training data set through transformations of training samples. We do not inflate the training set; our results are achievable with only one online training epoch. Both the training and test are completed on a standard desktop computer processor in a few minutes to less than an hour, depending on the resolution of the spike encoding. The current state of the art achieves a recognition rate of 99.65% and “took a few days” to train on a desktop computer with GPU acceleration <xref ref-type="bibr" rid="pone.0085175-Cirean1">[111]</xref>. Another study determined that human performance on this task is 97.27% <xref ref-type="bibr" rid="pone.0085175-Chaaban1">[112]</xref>.</p>
<p>The Reuters-21578, Census Income and Breast Cancer datasets cover a range of data types from strings to integers to continuous real-valued signals. The Census Income dataset furthermore contains mixed data types as well as exemplars with missing attributes. In all cases the AHaH classifier combined with the simple spike encoder of <xref ref-type="disp-formula" rid="pone.0085175.e251">Equation 29</xref> matched or exceeded state of the art classifiers. This is significant primarily for the reason that both spike encoding and classification functions can be attained via AHaH learning and support the idea that a generic adaptive learning hardware resource is possible.</p>
<p><xref ref-type="fig" rid="pone-0085175-g016">Figure 16</xref> provides a more detailed look at the individual classification experiments. Typical for all benchmark data sets, as the confidence threshold of the AHaH classifier is increased, the precision increases while recall drops (<xref ref-type="fig" rid="pone-0085175-g016">Figure 16A and 16B</xref>). In other words, the classifier makes fewer mistakes at the expense of not being able to answer some queries. The circuit-level simulation using the Ag-chalcogenide device model yielded a classification score as a function of confidence threshold similar to the functional simulations as shown in <xref ref-type="fig" rid="pone-0085175-g016">Figures 16C and 16D</xref> respectively. Similar circuit-level simulation results were obtained using the AIST and WO<italic><sub>x</sub></italic> device models. The results of the MNIST experiment are shown in <xref ref-type="fig" rid="pone-0085175-g016">Figures 16E and 16F</xref>. While <xref ref-type="fig" rid="pone-0085175-g016">Figure 16E</xref> shows the average over all digits, <xref ref-type="fig" rid="pone-0085175-g016">Figure 16F</xref> shows the scores of the individual digits.</p>
<fig id="pone-0085175-g016" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g016</object-id><label>Figure 16</label><caption>
<title>Classification benchmarks results.</title>
<p>A) Reuters-21578. Using the top ten most frequent labels associated with the news articles in the Reuters-21578 data set, the AHaH classifier’s accuracy, precision, recall, and F1 score was determined as a function of its confidence threshold. As the confidence threshold increases, the precision increases while recall drops. An optimal confidence threshold can be chosen depending on the desired results and can be dynamically changed. The peak F1 score is 0.92. B) Census Income. The peak F1 score is 0.853 C) Breast Cancer. The peak F1 score is 0.997. D) Breast Cancer repeated but using the circuit model rather than the functional model. The peak F1 score and the shape of the curves are similar to functional model results. E) MNIST. The peak F1 score is 0.98–.99, depending on the resolution of the spike encoding. F) The individual F1 classification scores of the hand written digits.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g016" position="float" xlink:type="simple"/></fig>
<p>Using the confidence threshold as a guide, the AHaH classifier can also be used in a semi-supervised mode. Starting in supervised mode and learning over a range of training data, the classifier can then switch to unsupervised mode. In unsupervised mode we may activate Hebbian learning if the confidence exceeds a value. Results are shown in <xref ref-type="fig" rid="pone-0085175-g017">Figure 17</xref>, which shows continued improved F1 score without supervision.</p>
<fig id="pone-0085175-g017" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g017</object-id><label>Figure 17</label><caption>
<title>Semi-supervised operation of the AHaH classifier.</title>
<p>For the first 30% of samples from the Reuters-21578 data set, the AHaH classifier was operated in supervised mode followed by operation in unsupervised mode for the remaining samples. A confidence threshold of 1.0 was set for unsupervised application of a learn signal. The F1 score for the top ten most frequently occurring labels in the Reuters-21578 data set were tracked. These results show that the AHaH classifier is capable of continuously improving its performance without supervised feedback.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g017" position="float" xlink:type="simple"/></fig>
<p>Results to date indicate that the AHaH classifier is an efficient incremental optimal linear classifier. The AHaH classifier displays a range of desirable classifier characteristics hinting that it may be an ideal general classifier capable of handling a wide range of classification applications. The classifier can learn online in a feed-forward manner. This is important for large data sets and applications that require constant adaptation such as prediction, anomaly detection and motor control. The classifier can associate an unlimited number of labels to a pattern, where the addition of a label is simply the addition of another AHaH node. By allowing the classifier to process unlabeled data it can improve over time. This has practical implications in any situation where substantial quantities of unlabeled data exist. Through the use of spike encoders, the classifier can handle mixed data types such as discrete or continuous numbers and strings. The classifier tolerates missing values, noise, and irrelevant attributes and is computationally efficient. The most significant advantage, however, is that the circuit can be mapped to physically adaptive hardware. Optimal incremental classification can now become a hardware resource.</p>
</sec><sec id="s4e">
<title>AHaH Signal Prediction</title>
<p>The results of the temporal signal prediction experiment are shown in <xref ref-type="fig" rid="pone-0085175-g018">Figure 18</xref>. The solid line drawn on top of the true signal represents the predictor’s accurate prediction of the true complex waveform after a period of supervised learning (mostly not shown). One advantage of the recursive prediction is that the forward-looking time window can be dynamically chosen. Although the predictor was trained to predict only the next time step, the recursive prediction can be carried forward to the desired point in the future for which the prediction should be made, which was 300 time steps in this example. At some point forward the prediction will degrade if the signal is not deterministic and cyclical. Not all applications require the recursive prediction, and a simpler statically set forward-looking time window could be set.</p>
<fig id="pone-0085175-g018" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g018</object-id><label>Figure 18</label><caption>
<title>Complex signal prediction with the AHaH classifier.</title>
<p>By posing prediction as a multi-label classification problem, the AHaH classifier can learn complex temporal waveforms and make extended predictions via recursion. Here, the temporal signal (dots) is a summation of five sinusoidal signals with randomly chosen amplitudes, periods, and phases. The classifier is trained for 10,000 time steps (last 100 steps shown, dotted line) and then tested for 300 time steps (solid line).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g018" position="float" xlink:type="simple"/></fig>
<p>While this temporal signal prediction demonstration is not by any means an exhaustive comparison of AHaH signal prediction to other forecasting algorithms, it demonstrates the utility and flexibility of the AHaH classifier and provides the first glimpse of using AHaH nodes in the large application space of signal forecasting. These results also shed light on how AHaH node supervisory signals could be generated in a completely self-organizing system with zero human intervention. Time is the supervisor and prediction is the Hebbian reward. From the practical perspective, prediction provides the ability to prepare or optimize for the future. It also provides the ability to detect when a system is changing. If a prediction fails to meet with reality, an anomaly has occurred.</p>
</sec><sec id="s4f">
<title>AHaH Motor Control</title>
<p>The results of the motorized robotic arm experiment are shown in <xref ref-type="fig" rid="pone-0085175-g019">Figure 19</xref>. The performance of the AHaH-guided robotic arm is compared with a random-guided robotic arm by measuring the average total joint actuation needed to capture 100 moving targets. The results demonstrate that the collective of AHaH nodes are performing a gradient descent of the value function and can rapidly guide the arm to its target, independent of the number of joints. Videos of AHaH-controlled 3-, 6-, 9-, 12-, and 15-joint robotic arms performing the capture challenge can be viewed in the online Supporting Information section (<xref ref-type="supplementary-material" rid="pone.0085175.s005">Videos S5</xref>–<xref ref-type="supplementary-material" rid="pone.0085175.s009">S9</xref>).</p>
<fig id="pone-0085175-g019" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g019</object-id><label>Figure 19</label><caption>
<title>Unsupervised robotic arm challenge.</title>
<p>The average total joint actuation required for the robot arm to capture the target remains constant as the number of arm joints increases for actuation using the AHaH motor controller. For random actuation, the required actuation grows exponentially.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g019" position="float" xlink:type="simple"/></fig>
<p>Our results show that populations of independent AHaH nodes can effectively control multiple degrees of freedom so as to ascend (or descend) a value function. This process is spontaneous and results from the emergent behavior of many AHaH nodes acting as <italic>self configuring classifiers</italic> competing for Hebbian reward. Real world applications of this effect could of course include actuation of robotic appendages as well as autonomous controllers. Our results are significant primarily because the controller can be reduced to physically adaptive circuits and hence can be made to consume very little power and space, an important consideration in mobile platforms.</p>
</sec><sec id="s4g">
<title>AHaH Combinatorial Optimization</title>
<p>The results of the traveling salesman problem experiment are shown in <xref ref-type="fig" rid="pone-0085175-g020">Figure 20</xref>. Our experiment demonstrates that an AHaH combinatorial optimizer performing a strike search can outperform a strike search backed by a random path chooser (<xref ref-type="fig" rid="pone-0085175-g020">Figure 20A</xref>). This result shows that the strike is performing a directed search as expected. Trials with higher convergence times resulted from cases where the optimizer was given a relatively lower learning rate. Recall, a lower learning rate allows for a finer-grained search resulting in the longer convergence times. <xref ref-type="fig" rid="pone-0085175-g020">Figure 20B</xref> shows the relationship between the learning rate and the solution value (distance), while <xref ref-type="fig" rid="pone-0085175-g020">Figure 20C</xref> shows the relationship between the learning rate and the convergence time. Lowering the learning rate causes more evidence to be accumulated before positive-feedback forces selection and the solution proceeds from the trunk to leaf node, one bit at a time.</p>
<fig id="pone-0085175-g020" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.g020</object-id><label>Figure 20</label><caption>
<title>64-city traveling salesman experiment.</title>
<p>By using single-input AHaH nodes as nodes in a routing tree to perform a strike search, combinatorial optimization problems such as the traveling salesman problem can be solved. Adjusting the learning rate can control the speed and quality of the solution. A) The distance between the 64 cities versus the convergences time for the AHaH-based and random-based strike search. B) Lower learning rates lead to better solutions. C) Higher learning rates decrease convergence time.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.g020" position="float" xlink:type="simple"/></fig>
<p>A strike evolves in time as bits are sequentially locked in via the positive feedback selection mechanism after a period of evidence accumulation. The lower the learning rate, the more evidence is accumulated before a path is locked in. In this way, a strike search appears to be a relatively generic method to accelerate the search for a procedure. Using the traveling salesman problem as an example, we could just as easily encode the strike path as a relative procedure for re-ordering a list of cities rather than an absolute ordering. For example, we could swap the cities at indices A and B, then swap the cities at indices C and D, and so on. Furthermore, we could utilize the strike procedure in a recursive manner. In the case of the traveling salesman problem we could assign lower-level strikes to find optimal sub-paths and higher-order strikes to assemble larger paths from the sub-paths. Most generally, if (1) a problem can be represented as a bit configuration and (2) the configuration can be assigned a value in an efficient manner, then a strike can be used as an adaptive learning hardware resource for optimization tasks. The ability to change the convergence times allows dynamic choices to be made in the time available.</p>
</sec><sec id="s4h">
<title>Synaptic Power Consumption</title>
<p>Both static and dynamic power consumption pathways must be considered when calculating the energy dissipation of neuromorphic chips containing AHaH circuit architecture. The static power component is dominated by the current flowing through the AHaH node synapse arrays during the read and write phases. The dynamic power component is dominated by the charging and discharging of the capacitive components of the circuitry. This capacitance includes parasitics from circuit elements and interconnect wires. Industry best practices can optimize dynamic power consumption. Here we focus on an estimation of static power consumption. Note that by not including the dynamic power consumption in this estimation, these values represent only a lower bounds on the synaptic power consumption of a neuromorphic chip. Dynamic power consumption, which is heavily dependent on chip design and architecture may have a significant power contribution. Recall that one of the major motivations of AHaH computing is the elimination of the von Neumann bottleneck for machine learning applications. Considering static and dynamic power consumption together with the elimination of this bottleneck, the net gain in power efficiency compared to modern digital electronics will most likely increase.</p>
<p>Static power dissipation of a single AHaH node is equal to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e313" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e314" xlink:type="simple"/></inline-formula> is the voltage drop across the memristor pairs and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e315" xlink:type="simple"/></inline-formula> is the equivalent conductance of the combined active input and bias memristor pairs in <xref ref-type="fig" rid="pone-0085175-g005">Figure 5</xref>. Since each synapse only dissipates energy when it is active (it remains floating otherwise), and since only a small number of synapses are active at any given time (given the sparse spike encoding), the current flowing through the AHaH node during the read and write phases is very low. The total dissipative energy per synaptic event is.<disp-formula id="pone.0085175.e316"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0085175.e316" xlink:type="simple"/><label>(37)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e317" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e318" xlink:type="simple"/></inline-formula> are the energy of the read and write phases, respectively, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e319" xlink:type="simple"/></inline-formula> is the pulse width of the read and write phases and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e320" xlink:type="simple"/></inline-formula> denotes the three possible outcomes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e321" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e322" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e323" xlink:type="simple"/></inline-formula>. That is, the synapse could select the <italic>A Path</italic>, the <italic>B Path</italic> or else feedback is withheld. Utilizing <xref ref-type="disp-formula" rid="pone.0085175.e063">Equation 1</xref>, it is straightforward to show the conductance under maximum power dissipation for each condition, as shown in <xref ref-type="table" rid="pone-0085175-t007">Table 7</xref>.</p>
<table-wrap id="pone-0085175-t007" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.t007</object-id><label>Table 7</label><caption>
<title>Maximum power and corresponding synaptic weights.</title>
</caption><alternatives><graphic id="pone-0085175-t007-7" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.t007" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Condition</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e095" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e096" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Maximum Power</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Path A Selected</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e097" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e098" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e099" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Path B Selected</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e100" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e101" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e102" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">No Feedback</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e103" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e104" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e105" xlink:type="simple"/></inline-formula></td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt107"><label/><p>The maximum power dissipation of a differential synaptic weight changes depending on whether feedback is present or not. In the absence of feedback, the power is maximized when the conductance of each path is the same and the output descends into randomness. When feedback is present the synapse may converge to one of two possible configurations, and the power dissipation increases by a factor of four.</p></fn></table-wrap-foot></table-wrap>
<p>From <xref ref-type="disp-formula" rid="pone.0085175.e316">Equation 37</xref> it is clear that lower operating voltages, shorter pulse widths, and lower conductance memristors will reduce static power consumption. Developing an ideal candidate memristor for AHaH computing will play an important role in static power reduction. Given an operating resistance of the Ag-chalcogenide memristor of 250Ω, a pulse with of 10 <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0085175.e324" xlink:type="simple"/></inline-formula>s and a voltage of 1 V, the estimated per-synapse static energy consumption is 40 nJ. Several synapse-like memristors achieving ultra-low power pulsed synaptic updates have recently been reported <xref ref-type="bibr" rid="pone.0085175-Jo1">[68]</xref>, <xref ref-type="bibr" rid="pone.0085175-Li1">[70]</xref>, <xref ref-type="bibr" rid="pone.0085175-Kuzum1">[113]</xref>. The energy consumption per synapse using roughly average voltage, pulse width, and resistance values taken from the pulse-driven memristive behavior plots in the three references results in a calculated energy consumption per synapse per update of ∼12 pJ, ∼67.5 pJ, and ∼56 pJ respectively. Kuzum et al. <xref ref-type="bibr" rid="pone.0085175-Kuzum1">[113]</xref> claim that scaling trends project energy consumption for electronic synapses down to 2 pJ. As an example, a 100 kΩ device driven with 1 V, 100 ns pulse widths would consume 1.5 pJ of static energy per synapse. Such devices will play a significant role in reaching biological efficiency.</p>
<p>In all applications, the spike encoding plays an important role in reducing the number of spikes and hence the power consumption. <xref ref-type="table" rid="pone-0085175-t008">Table 8</xref> tabulates the coactive spikes, the spike space, and the number of AHaH nodes used for most of the demonstration applications and benchmarks in this paper. Different applications require different configurations; some will have few AHaH nodes and a large spike space while others may have many AHaH nodes and few inputs.</p>
<table-wrap id="pone-0085175-t008" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0085175.t008</object-id><label>Table 8</label><caption>
<title>Application spike sparsity and AHaH node count.</title>
</caption><alternatives><graphic id="pone-0085175-t008-8" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0085175.t008" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Application</td>
<td align="left" rowspan="1" colspan="1">CoactiveSpikes</td>
<td align="left" rowspan="1" colspan="1">SpikeSpace</td>
<td align="left" rowspan="1" colspan="1">Sparsity</td>
<td align="left" rowspan="1" colspan="1">AHaH NodeCount</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Breast Cancer</td>
<td align="left" rowspan="1" colspan="1">31</td>
<td align="left" rowspan="1" colspan="1">70</td>
<td align="left" rowspan="1" colspan="1">0.44</td>
<td align="left" rowspan="1" colspan="1">2</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Census Income</td>
<td align="left" rowspan="1" colspan="1">63</td>
<td align="left" rowspan="1" colspan="1">∼1800</td>
<td align="left" rowspan="1" colspan="1">∼0.035</td>
<td align="left" rowspan="1" colspan="1">2</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">MNIST</td>
<td align="left" rowspan="1" colspan="1">∼1000</td>
<td align="left" rowspan="1" colspan="1">∼27,500</td>
<td align="left" rowspan="1" colspan="1">∼0.036</td>
<td align="left" rowspan="1" colspan="1">10</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Reuters 21578</td>
<td align="left" rowspan="1" colspan="1">∼100</td>
<td align="left" rowspan="1" colspan="1">∼46,000</td>
<td align="left" rowspan="1" colspan="1">∼0.002</td>
<td align="left" rowspan="1" colspan="1">119</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Robotic Arm</td>
<td align="left" rowspan="1" colspan="1">92</td>
<td align="left" rowspan="1" colspan="1">341</td>
<td align="left" rowspan="1" colspan="1">0.27</td>
<td align="left" rowspan="1" colspan="1">345</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Comb. Opt.</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">n/a</td>
<td align="left" rowspan="1" colspan="1">∼600,000</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Clusterer</td>
<td align="left" rowspan="1" colspan="1">16</td>
<td align="left" rowspan="1" colspan="1">256</td>
<td align="left" rowspan="1" colspan="1">0.0625</td>
<td align="left" rowspan="1" colspan="1">20</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Prediction</td>
<td align="left" rowspan="1" colspan="1">300</td>
<td align="left" rowspan="1" colspan="1">9600</td>
<td align="left" rowspan="1" colspan="1">0.031</td>
<td align="left" rowspan="1" colspan="1">32</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt108"><label/><p>The applications and benchmarks presented in this paper to demonstrate various machine learning tasks using AHaH plasticity require different AHaH node configurations depending on the type of data being processed and what the desired result is. The sparsity is a function of the incoming data and is defined as the number of coactive spikes divided by the total spike space.</p></fn></table-wrap-foot></table-wrap></sec></sec><sec id="s5">
<title>Limitations of the Study, Open Questions, and Future Work</title>
<p>We have attempted to connect a low-level general statistical model of collections of metastable switches with dissipative attractor-based computation and machine learning in a physically realizable circuit. Our aim is to provide a road map for others to follow so that we may all explore and exploit this interesting and potentially useful form of computing. Our ultimate goal is to provide a physical adaptive learning hardware resource (the AHaH circuit) in much the same way as modern RAM memory provides a memory resource to computing systems. However, only when we have investigated the circuit and functional models and have demonstrated real world utility is it necessary to move toward simulation of nonideal circuits effects, such as parasitic impedances, signal delays, settling times and variations in memristor properties. These details are certainly required for the eventual construction of a neural processing unit (NPU) but to include them in this paper would obfuscate our core message that “a new type of computing is possible that appears to offer a solution of general machine learning”.</p>
<p>Our demonstrations of utility include results across the field of machine learning, from clustering and classification to prediction, control and combinatorial optimization. Given the intended broad scope of this paper it was not possible to provide much elaboration on some of our results, comparison with many other methods, nor discuss the implications. For this reason we have open-sourced all code used to generate the results of this paper. We encourage the reader to investigate our methods carefully and come to their own conclusions.</p>
<p>Although it was important to develop specific techniques to address the broad capabilities we have demonstrated, we wish to convey the idea that the AHaH node is a building block from which many higher-order adaptive algorithms may be built including many we have not yet conceived of. As an example consider our results with the AHaH motor controller and AHaH classifier. By using the classifier’s confidence estimation as the value function for the AHaH motor controller, which in turn controls the viewing position, angle and rotation of an “eye”, it should be possible to spontaneously control the gaze of a vision system to find and center previously trained objects. Alternately, by pairing the AHaH signal prediction with the AHaH combinatorial optimizer, it should be possible learn to predict a reward signal while simultaneously optimizing actions to attain reward. We can infer from our results that other capabilities are possible. Anomaly detection, for example, is the inverse of prediction. If a prediction can be made about a temporally dynamic signal, then an anomaly signal can be generated should predictions fail to match with reality. Tracking of non-stationary statistics is also a natural by-product of the attractor nature of the AHaH rule, and was slightly touched upon in the 2D clustering videos, <xref ref-type="supplementary-material" rid="pone.0085175.s004">Video S4</xref> in particular. Attractor points of the AHaH rule are created by the data structure. It follows logically that these same states will shift as the structure of the information changes. It also follows that a system built of components locked in attractor states will spontaneously heal if damaged <xref ref-type="bibr" rid="pone.0085175-Nugent1">[43]</xref>, <xref ref-type="bibr" rid="pone.0085175-Nugent2">[44]</xref>. This property could provide new developments in fault-tolerant electronics.</p>
</sec><sec id="s6">
<title>Conclusions</title>
<p>We have introduced the concept of AHaH computing. We have shown how the simple process of particles dissipating into containers through adaptive channels competing for conduction resources leads to AHaH plasticity. We have shown that memristive devices can arise from metastable switches, how differential synaptic weights may be built of two or more memristors, and how an AHaH node may be built of arrays of synapses. A simple read and write cycle driving an AHaH circuit results in physical devices implementing AHaH plasticity. We have demonstrated that the attractor states of the AHaH rule can configure computationally complete logic functions, and have shown their use in supervised and unsupervised classification, clustering, complex signal prediction, unsupervised robotic arm actuation and combinatorial optimization. We have demonstrated unsupervised clustering and supervised classification in circuit simulations, and have further shown a correspondence between our functional and circuit forms of the AHaH node.</p>
<p>The AHaH node may offer us a building block for a new type of computing with likely application in the field of machine learning. Indeed, we hope that our work demonstrates that functions needed to enable perception (clustering, classification), planning (combinatorial optimization, prediction), control (robotic actuation) and generic computation (universal logic) are possible with a simple circuit that, technologically speaking, may be very close at hand.</p>
</sec><sec id="s7">
<title>Supporting Information</title>
<supplementary-material id="pone.0085175.s001" mimetype="video/mp4" xlink:href="info:doi/10.1371/journal.pone.0085175.s001" position="float" xlink:type="simple"><label>Video S1</label><caption>
<p><bold>AHaH clustering demonstration with three Gaussian clusters.</bold></p>
<p>(MP4)</p>
</caption></supplementary-material><supplementary-material id="pone.0085175.s002" mimetype="video/mp4" xlink:href="info:doi/10.1371/journal.pone.0085175.s002" position="float" xlink:type="simple"><label>Video S2</label><caption>
<p><bold>AHaH clustering demonstration with one Gaussian cluster and one non-Gaussian cluster.</bold></p>
<p>(MP4)</p>
</caption></supplementary-material><supplementary-material id="pone.0085175.s003" mimetype="video/mp4" xlink:href="info:doi/10.1371/journal.pone.0085175.s003" position="float" xlink:type="simple"><label>Video S3</label><caption>
<p><bold>AHaH clustering demonstration with many Gaussian clusters of various sizes.</bold></p>
<p>(MP4)</p>
</caption></supplementary-material><supplementary-material id="pone.0085175.s004" mimetype="video/mp4" xlink:href="info:doi/10.1371/journal.pone.0085175.s004" position="float" xlink:type="simple"><label>Video S4</label><caption>
<p><bold>AHaH clustering demonstration with non-stationary clusters.</bold></p>
<p>(MP4)</p>
</caption></supplementary-material><supplementary-material id="pone.0085175.s005" mimetype="video/mp4" xlink:href="info:doi/10.1371/journal.pone.0085175.s005" position="float" xlink:type="simple"><label>Video S5</label><caption>
<p><bold>AHaH motor control demonstration with 3-joint robotic arm.</bold></p>
<p>(MP4)</p>
</caption></supplementary-material><supplementary-material id="pone.0085175.s006" mimetype="video/mp4" xlink:href="info:doi/10.1371/journal.pone.0085175.s006" position="float" xlink:type="simple"><label>Video S6</label><caption>
<p><bold>AHaH motor control demonstration with 6-joint robotic arm.</bold></p>
<p>(MP4)</p>
</caption></supplementary-material><supplementary-material id="pone.0085175.s007" mimetype="video/mp4" xlink:href="info:doi/10.1371/journal.pone.0085175.s007" position="float" xlink:type="simple"><label>Video S7</label><caption>
<p><bold>AHaH motor control demonstration with 9-joint robotic arm.</bold></p>
<p>(MP4)</p>
</caption></supplementary-material><supplementary-material id="pone.0085175.s008" mimetype="video/mp4" xlink:href="info:doi/10.1371/journal.pone.0085175.s008" position="float" xlink:type="simple"><label>Video S8</label><caption>
<p><bold>AHaH motor control demonstration with 12-joint robotic arm.</bold></p>
<p>(MP4)</p>
</caption></supplementary-material><supplementary-material id="pone.0085175.s009" mimetype="video/mp4" xlink:href="info:doi/10.1371/journal.pone.0085175.s009" position="float" xlink:type="simple"><label>Video S9</label><caption>
<p><bold>AHaH motor control demonstration with 15-joint robotic arm.</bold></p>
<p>(MP4)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>The Breast Cancer Wisconsin, Reuters-21578 Distribution 1.0, and Census Income classification benchmark datasets were obtained from the UCI Machine Learning Repository <xref ref-type="bibr" rid="pone.0085175-Bache1">[114]</xref>.</p>
<p>The authors would like to thank Kristy A. Campbell from Boise State University for graciously providing us with memristor device data.</p>
<p>Special thanks to Air Force Research Laboratory’s (AFRL) Information Directorate.</p>
<p>Alex Nugent would like to personally thank Hillary Riggs, Kermit Lopez, Luis Ortiz, and Todd Hylton for their support over the years. This work would definitely not have existed without them.</p>
<p>This manuscript has been approved for public release; distribution unlimited. Case Number: 88ABW-2014-0103.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0085175-Grime1"><label>1</label>
<mixed-citation publication-type="other" xlink:type="simple">Grime JP, Crick JC, Rincon JE (1985) The ecological significance of plasticity. In: Proc. 1985 Symposia of the Society for Experimental Biology. volume 40, 5–29.</mixed-citation>
</ref>
<ref id="pone.0085175-Desbiez1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Desbiez</surname><given-names>MO</given-names></name>, <name name-style="western"><surname>Kergosien</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Champagnat</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Thellier</surname><given-names>M</given-names></name> (<year>1984</year>) <article-title>Memorization and delayed expression of regulatory messages in plants</article-title>. <source>Planta</source> <volume>160</volume>: <fpage>392</fpage>–<lpage>399</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Aphalo1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Aphalo</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Ballaré</surname><given-names>CL</given-names></name> (<year>1995</year>) <article-title>On the importance of information-acquiring systems in plant–plant interactions</article-title>. <source>Functional Ecology</source> <volume>9</volume>: <fpage>5</fpage>–<lpage>14</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Falik1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Falik</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Reides</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Gersani</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Novoplansky</surname><given-names>A</given-names></name> (<year>2003</year>) <article-title>Self/non-self discrimination in roots</article-title>. <source>Journal of Ecology</source> <volume>91</volume>: <fpage>525</fpage>–<lpage>531</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Scialdone1"><label>5</label>
<mixed-citation publication-type="other" xlink:type="simple">Scialdone A, Mugford ST, Feike D, Skeffington A, Borrill P, <etal>et al</etal>.. (2013) Arabidopsis plants perform arithmetic division to prevent starvation at night. eLife 2 doi: 10.7554/eLife.00669.</mixed-citation>
</ref>
<ref id="pone.0085175-vonBodman1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>von Bodman</surname><given-names>SB</given-names></name>, <name name-style="western"><surname>Bauer</surname><given-names>WD</given-names></name>, <name name-style="western"><surname>Coplin</surname><given-names>DL</given-names></name> (<year>2003</year>) <article-title>Quorum sensing in plant-pathogenic bacteria</article-title>. <source>Annual Review of Phytopathology</source> <volume>41</volume>: <fpage>455</fpage>–<lpage>482</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Nakagaki1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nakagaki</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Yamada</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Tóth</surname><given-names>Á</given-names></name> (<year>2000</year>) <article-title>Intelligence: Maze-solving by an amoeboid organism</article-title>. <source>Nature</source> <volume>407</volume>: <fpage>470</fpage>–<lpage>470</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Bonabeau1"><label>8</label>
<mixed-citation publication-type="other" xlink:type="simple">Bonabeau E, Dorigo M, Theraulaz G (1999) Swarm Intelligence: from Natural to 1172 Artificial Systems, volume 4. Oxford University press New York.</mixed-citation>
</ref>
<ref id="pone.0085175-Choudhary1"><label>9</label>
<mixed-citation publication-type="other" xlink:type="simple">Choudhary S, Sloan S, Fok S, Neckar A, Trautmann E, <etal>et al</etal>.. (2012) Silicon neurons that compute. In: Artificial Neural Networks and Machine Learning – ICANN 2012, Springer Berlin Heidelberg, volume 7552 of Lecture Notes in Computer Science. 121–128.</mixed-citation>
</ref>
<ref id="pone.0085175-Izhikevich1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Izhikevich</surname><given-names>EM</given-names></name> (<year>2003</year>) <article-title>Simple model of spiking neurons</article-title>. <source>IEEE Transactions on Neural Networks</source> <volume>14</volume>: <fpage>1569</fpage>–<lpage>1572</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Eliasmith1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eliasmith</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Stewart</surname><given-names>TC</given-names></name>, <name name-style="western"><surname>Choo</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Bekolay</surname><given-names>T</given-names></name>, <name name-style="western"><surname>DeWolf</surname><given-names>T</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>A large scale model of the functioning brain</article-title>. <source>Science</source> <volume>338</volume>: <fpage>1202</fpage>–<lpage>1205</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Boser1"><label>12</label>
<mixed-citation publication-type="other" xlink:type="simple">Boser BE, Guyon IM, Vapnik VN (1992) A training algorithm for optimal margin classifiers. In: Proc. 1992 ACM 5th Annual Workshop on Computational Learning Theory. 144–152.</mixed-citation>
</ref>
<ref id="pone.0085175-MacQueen1"><label>13</label>
<mixed-citation publication-type="other" xlink:type="simple">MacQueen J (1967) Some methods for classification and analysis of multivariate observations. In: Proc. 1967 5th Berkeley Symposium on Mathematical Statistics and Probability. 281–297, p.14.</mixed-citation>
</ref>
<ref id="pone.0085175-Breiman1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Breiman</surname><given-names>L</given-names></name> (<year>2001</year>) <article-title>Random forests</article-title>. <source>Machine Learning</source> <volume>45</volume>: <fpage>5</fpage>–<lpage>32</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-SerranoGotarredona1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Serrano-Gotarredona</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Oster</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Lichtsteiner</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Linares-Barranco</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Paz-Vicente</surname><given-names>R</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>CAVIAR: A 45k neuron, 5M synapse, 12G connects/s AER hardware sensory–processing–learning–actuating system for high-speed visual object recognition and tracking</article-title>. <source>IEEE Transactions on Neural Networks</source> <volume>20</volume>: <fpage>1417</fpage>–<lpage>1438</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Sardar1"><label>16</label>
<mixed-citation publication-type="other" xlink:type="simple">Sardar S, Tewari G, Babu KA (2011) A hardware/software co-design model for face recognition using cognimem neural network chip. In: Proc. 2011 IEEE International Conference on Image Information Processing (ICIIP). 1–6.</mixed-citation>
</ref>
<ref id="pone.0085175-Arthur1"><label>17</label>
<mixed-citation publication-type="other" xlink:type="simple">Arthur JV, Merolla PA, Akopyan F, Alvarez R, Cassidy A, <etal>et al</etal>.. (2012) Building block of a programmable neuromorphic substrate: A digital neurosynaptic core. In: Proc. 2012 IEEE International Joint Conference on Neural Networks (IJCNN). 1–8.</mixed-citation>
</ref>
<ref id="pone.0085175-Ananthanarayanan1"><label>18</label>
<mixed-citation publication-type="other" xlink:type="simple">Ananthanarayanan R, Esser SK, Simon HD, Modha DS (2009) The cat is out of the bag: cortical simulations with 109 neurons, 1013 synapses. In: Proc. 2009 IEEE Conference on High Performance Computing Networking, Storage and Analysis. 1–12.</mixed-citation>
</ref>
<ref id="pone.0085175-Le1"><label>19</label>
<mixed-citation publication-type="other" xlink:type="simple">Le QV, Ranzato M, Monga R, Devin M, Chen K, <etal>et al</etal>.. (2011) Building high-level features using large scale unsupervised learning. preprint arXiv cs.LG/1112.6209.</mixed-citation>
</ref>
<ref id="pone.0085175-Pakkenberg1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pakkenberg</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Pelvig</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Marner</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Bundgaard</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Gundersen</surname><given-names>HJG</given-names></name>, <etal>et al</etal>. (<year>2003</year>) <article-title>Aging and the human neocortex</article-title>. <source>Experimental Gerontology</source> <volume>38</volume>: <fpage>95</fpage>–<lpage>99</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Turing1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Turing</surname><given-names>AM</given-names></name> (<year>1936</year>) <article-title>On computable numbers, with an application to the entscheidungsproblem</article-title>. <source>Proceedings of the London Mathematical Society</source> <volume>42</volume>: <fpage>230</fpage>–<lpage>265</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Turing2"><label>22</label>
<mixed-citation publication-type="other" xlink:type="simple">Turing A (1948) Intelligent machinery. Report, National Physical Laboratory.</mixed-citation>
</ref>
<ref id="pone.0085175-Schrdinger1"><label>23</label>
<mixed-citation publication-type="other" xlink:type="simple">Schrödinger E (1992) What is Life?: With Mind and Matter and Autobiographical Sketches. Cambridge University Press.</mixed-citation>
</ref>
<ref id="pone.0085175-Hebb1"><label>24</label>
<mixed-citation publication-type="other" xlink:type="simple">Hebb DO (2002) The Organization of Behavior: A Neuropsychological Theory. Psychology Press.</mixed-citation>
</ref>
<ref id="pone.0085175-Rosenblatt1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rosenblatt</surname><given-names>F</given-names></name> (<year>1958</year>) <article-title>The perceptron: a probabilistic model for information storage and organization in the brain</article-title>. <source>Psychological Review</source> <volume>65</volume>: <fpage>386</fpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Barlow1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barlow</surname><given-names>HB</given-names></name> (<year>1953</year>) <article-title>Summation and inhibition in the frog’s retina</article-title>. <source>The Journal of Physiology</source> <volume>119</volume>: <fpage>69</fpage>–<lpage>88</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Hubel1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hubel</surname><given-names>DH</given-names></name>, <name name-style="western"><surname>Wiesel</surname><given-names>TN</given-names></name> (<year>1959</year>) <article-title>Receptive fields of single neurones in the cat’s striate cortex</article-title>. <source>The Journal of Physiology</source> <volume>148</volume>: <fpage>574</fpage>–<lpage>591</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Widrow1"><label>28</label>
<mixed-citation publication-type="other" xlink:type="simple">Widrow B (1987) The original adaptive neural net broom-balancer. In: Proc. 1987 IEEE International Symposium on Circuits and Systems. volume 2, 351–357.</mixed-citation>
</ref>
<ref id="pone.0085175-Minsky1"><label>29</label>
<mixed-citation publication-type="other" xlink:type="simple">Minsky M, Seymour P (1969) Perceptrons. MIT press.</mixed-citation>
</ref>
<ref id="pone.0085175-Hopfield1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name> (<year>1982</year>) <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>79</volume>: <fpage>2554</fpage>–<lpage>2558</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Rumelhart1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rumelhart</surname><given-names>DE</given-names></name>, <name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name>, <name name-style="western"><surname>Williams</surname><given-names>RJ</given-names></name> (<year>1986</year>) <article-title>Learning representations by back-propagating errors</article-title>. <source>Nature</source> <volume>323</volume>: <fpage>533</fpage>–<lpage>536</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Grossberg1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grossberg</surname><given-names>S</given-names></name> (<year>1987</year>) <article-title>Competitive learning: from interactive activation to adaptive resonance</article-title>. <source>Cognitive Science</source> <volume>11</volume>: <fpage>23</fpage>–<lpage>63</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Chua1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chua</surname><given-names>L</given-names></name> (<year>1971</year>) <article-title>Memristor–the missing circuit element</article-title>. <source>IEEE Transactions on Circuit Theory</source> <volume>18</volume>: <fpage>507</fpage>–<lpage>519</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Chua2"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chua</surname><given-names>LO</given-names></name>, <name name-style="western"><surname>Kang</surname><given-names>SM</given-names></name> (<year>1976</year>) <article-title>Memristive devices and systems</article-title>. <source>Proceedings of the IEEE</source> <volume>64</volume>: <fpage>209</fpage>–<lpage>223</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Mead1"><label>35</label>
<mixed-citation publication-type="other" xlink:type="simple">Mead C, Conway L (1980) Introduction to VLSI Systems. Boston, MA, USA: Addison-Wesley Longman Publishing Co., Inc.</mixed-citation>
</ref>
<ref id="pone.0085175-Mead2"><label>36</label>
<mixed-citation publication-type="other" xlink:type="simple">Mead C, Ismail M (1989) Analog VLSI Implementation of Neural Systems. Springer.</mixed-citation>
</ref>
<ref id="pone.0085175-Bienenstock1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bienenstock</surname><given-names>EL</given-names></name>, <name name-style="western"><surname>Cooper</surname><given-names>LN</given-names></name>, <name name-style="western"><surname>Munro</surname><given-names>PW</given-names></name> (<year>1982</year>) <article-title>Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex</article-title>. <source>The Journal of Neuroscience</source> <volume>2</volume>: <fpage>32</fpage>–<lpage>48</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Barlow2"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barlow</surname><given-names>HB</given-names></name> (<year>1989</year>) <article-title>Unsupervised learning</article-title>. <source>Neural Computation</source> <volume>1</volume>: <fpage>295</fpage>–<lpage>311</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Bell1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bell</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name> (<year>1997</year>) <article-title>The independent components of natural scenes are edge filters</article-title>. <source>Vision Research</source> <volume>37</volume>: <fpage>3327</fpage>–<lpage>3338</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Hyvrinen1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hyvärinen</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Oja</surname><given-names>E</given-names></name> (<year>1997</year>) <article-title>A fast fixed-point algorithm for independent component analysis</article-title>. <source>Neural Computation</source> <volume>9</volume>: <fpage>1483</fpage>–<lpage>1492</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Comon1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Comon</surname><given-names>P</given-names></name> (<year>1994</year>) <article-title>Independent component analysis, a new concept?</article-title> <source>Signal Processing</source> <volume>36</volume>: <fpage>287</fpage>–<lpage>314</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-SchlkopfSimard1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schölkopf, Simard</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Vapnik</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Smola</surname><given-names>AJ</given-names></name> (<year>1997</year>) <article-title>Improving the accuracy and speed of support vector machines</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>9</volume>: <fpage>375</fpage>–<lpage>381</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Nugent1"><label>43</label>
<mixed-citation publication-type="other" xlink:type="simple">Nugent A, Kenyon G, Porter R (2004) Unsupervised adaptation to improve fault tolerance of neural network classifiers. In: Proc. 2004 IEEE NASA/DoD Conference on Evolvable Hardware. 146–149.</mixed-citation>
</ref>
<ref id="pone.0085175-Nugent2"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nugent</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Porter</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Kenyon</surname><given-names>GT</given-names></name> (<year>2008</year>) <article-title>Reliable computing with unreliable components: using separable environments to stabilize long-term information storage</article-title>. <source>Physica D: Nonlinear Phenomena</source> <volume>237</volume>: <fpage>1196</fpage>–<lpage>1206</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Nugent3"><label>45</label>
<mixed-citation publication-type="other" xlink:type="simple">Nugent A (2008). Plasticity-induced self organizing nanotechnology for the extraction of independent components from a data stream. US Patent 7,409,375.</mixed-citation>
</ref>
<ref id="pone.0085175-Nugent4"><label>46</label>
<mixed-citation publication-type="other" xlink:type="simple">Nugent A (2008). Universal logic gate utilizing nanotechnology. US Patent 7,420,396.</mixed-citation>
</ref>
<ref id="pone.0085175-Nugent5"><label>47</label>
<mixed-citation publication-type="other" xlink:type="simple">Nugent A (2009). Methodology for the configuration and repair of unreliable switching elements. US Patent 7,599,895.</mixed-citation>
</ref>
<ref id="pone.0085175-Yang1"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yang</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Pickett</surname><given-names>MD</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Ohlberg</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Stewart</surname><given-names>DR</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Memristive switching mechanism for metal/oxide/metal nanodevices</article-title>. <source>Nature Nanotechnology</source> <volume>3</volume>: <fpage>429</fpage>–<lpage>433</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Snider1"><label>49</label>
<mixed-citation publication-type="other" xlink:type="simple">Snider GS (2008) Spike-timing-dependent learning in memristive nanodevices. In: Proc. 2008 IEEE International Symposium on Nanoscale Architectures (NANOARCH). 85–92.</mixed-citation>
</ref>
<ref id="pone.0085175-Stewart1"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stewart</surname><given-names>DR</given-names></name>, <name name-style="western"><surname>Ohlberg</surname><given-names>DAA</given-names></name>, <name name-style="western"><surname>Beck</surname><given-names>PA</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Williams</surname><given-names>RS</given-names></name>, <etal>et al</etal>. (<year>2004</year>) <article-title>Molecule-independent electrical switching in Pt/organic monolayer/Ti devices</article-title>. <source>Nano Letters</source> <volume>4</volume>: <fpage>133</fpage>–<lpage>136</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Kozicki1"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kozicki</surname><given-names>MN</given-names></name>, <name name-style="western"><surname>Gopalan</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Balakrishnan</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Mitkova</surname><given-names>M</given-names></name> (<year>2006</year>) <article-title>A low-power nonvolatile switching element based on copper-tungsten oxide solid electrolyte</article-title>. <source>IEEE Transactions on Nanotechnology</source> <volume>5</volume>: <fpage>535</fpage>–<lpage>544</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Szot1"><label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Szot</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Speier</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Bihlmayer</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Waser</surname><given-names>R</given-names></name> (<year>2006</year>) <article-title>Switching the electrical resistance of individual dislocations in single-crystalline SrTiO3</article-title>. <source>Nature Materials</source> <volume>5</volume>: <fpage>312</fpage>–<lpage>320</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Dong1"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dong</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Xiang</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Oh</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Seong</surname><given-names>D</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>Reproducible hysteresis and resistive switching in metal-Cu<sub>x</sub>O-metal heterostructures</article-title>. <source>Applied Physics Letters</source> <volume>90</volume>: <fpage>042107</fpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Tsubouchi1"><label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsubouchi</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Ohkubo</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Kumigashira</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Oshima</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Matsumoto</surname><given-names>Y</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>High-throughput characterization of metal electrode performance for electric-field-induced resistance switching in metal/Pr<sub>0.7</sub>Ca<sub>0.3</sub>M<sub>n</sub>O<sub>3</sub>/metal structures</article-title>. <source>Advanced Materials</source> <volume>19</volume>: <fpage>1711</fpage>–<lpage>1713</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Oblea1"><label>55</label>
<mixed-citation publication-type="other" xlink:type="simple">Oblea AS, Timilsina A, Moore D, Campbell KA (2010) Silver chalcogenide based memristor devices. In: Proc. 2010 IEEE International Joint Conference on Neural Networks (IJCNN). 1–3.</mixed-citation>
</ref>
<ref id="pone.0085175-Yang2"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yang</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Sheridan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Lu</surname><given-names>W</given-names></name> (<year>2012</year>) <article-title>Complementary resistive switching in tantalum oxide-based resistive memory devices</article-title>. <source>Applied Physics Letters</source> <volume>100</volume>: <fpage>203112</fpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Valov1"><label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Valov</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Kozicki</surname><given-names>MN</given-names></name> (<year>2013</year>) <article-title>Cation-based resistance change memory</article-title>. <source>Journal of Physics D: Applied Physics</source> <volume>46</volume>: <fpage>074005</fpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Hasegawa1"><label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hasegawa</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Nayak</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Ohno</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Terabe</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Tsuruoka</surname><given-names>T</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Memristive operations demonstrated by gap-type atomic switches</article-title>. <source>Applied Physics A</source> <volume>102</volume>: <fpage>811</fpage>–<lpage>815</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Jackson1"><label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jackson</surname><given-names>BL</given-names></name>, <name name-style="western"><surname>Rajendran</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Corrado</surname><given-names>GS</given-names></name>, <name name-style="western"><surname>Breitwisch</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Burr</surname><given-names>GW</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>Nanoscale electronic synapses using phase change devices</article-title>. <source>ACM Journal on Emerging Technologies in Computing Systems (JETC)</source> <volume>9</volume>: <fpage>12</fpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Choi1"><label>60</label>
<mixed-citation publication-type="other" xlink:type="simple">Choi S, Ambrogio S, Balatti S, Nardi F, Ielmini D (2012) Resistance drift model for conductivebridge (CB) RAM by filament surface relaxation. In: Proc. 2012 IEEE 4th International Memory Workshop (IMW). 1–4.</mixed-citation>
</ref>
<ref id="pone.0085175-Pino1"><label>61</label>
<mixed-citation publication-type="other" xlink:type="simple">Pino RE, Bohl JW, McDonald N, Wysocki B, Rozwood P, <etal>et al</etal>.. (2010) Compact method for modeling and simulation of memristor devices: ion conductor chalcogenide-based memristor devices. In: Proc. 2010 IEEE/ACM International Symposium on Nanoscale Architectures (NANOARCH). 1–4.</mixed-citation>
</ref>
<ref id="pone.0085175-Menzel1"><label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Menzel</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Bottger</surname><given-names>U</given-names></name>, <name name-style="western"><surname>Waser</surname><given-names>R</given-names></name> (<year>2012</year>) <article-title>Simulation of multilevel switching in electrochemical metallization memory cells</article-title>. <source>Journal of Applied Physics</source> <volume>111</volume>: <fpage>014501</fpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Chang1"><label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chang</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Jo</surname><given-names>SH</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>KH</given-names></name>, <name name-style="western"><surname>Sheridan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Gaba</surname><given-names>S</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Synaptic behaviors and modeling of a metal oxide memristive device</article-title>. <source>Applied Physics A</source> <volume>102</volume>: <fpage>857</fpage>–<lpage>863</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Sheridan1"><label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sheridan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>KH</given-names></name>, <name name-style="western"><surname>Gaba</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Chang</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>L</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Device and SPICE modeling of RRAM devices</article-title>. <source>Nanoscale</source> <volume>3</volume>: <fpage>3833</fpage>–<lpage>3840</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Biolek1"><label>65</label>
<mixed-citation publication-type="other" xlink:type="simple">Biolek D, Biolek Z, Biolkova V (2009) SPICE modeling of memristive, memcapacitative and meminductive systems. In: Proc. 2009 IEEE European Conference on Circuit Theory and Design (ECCTD). 249–252.</mixed-citation>
</ref>
<ref id="pone.0085175-Chang2"><label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chang</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Jo</surname><given-names>SH</given-names></name>, <name name-style="western"><surname>Lu</surname><given-names>W</given-names></name> (<year>2011</year>) <article-title>Short-term memory to long-term memory transition in a nanoscale memristor</article-title>. <source>ACS Nano</source> <volume>5</volume>: <fpage>7669</fpage>–<lpage>7676</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-MerrikhBayat1"><label>67</label>
<mixed-citation publication-type="other" xlink:type="simple">Merrikh-Bayat F, Shouraki SB, Afrakoti IEP (2010) Bottleneck of using single memristor as a synapse and its solution. preprint arXiv: cs.NE/1008.3450.</mixed-citation>
</ref>
<ref id="pone.0085175-Jo1"><label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jo</surname><given-names>SH</given-names></name>, <name name-style="western"><surname>Chang</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Ebong</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Bhadviya</surname><given-names>BB</given-names></name>, <name name-style="western"><surname>Mazumder</surname><given-names>P</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Nanoscale memristor device as synapse in neuromorphic systems</article-title>. <source>Nano Letters</source> <volume>10</volume>: <fpage>1297</fpage>–<lpage>1301</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Hasegawa2"><label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hasegawa</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Ohno</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Terabe</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Tsuruoka</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Nakayama</surname><given-names>T</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Learning abilities achieved by a single solid-state atomic switch</article-title>. <source>Advanced Materials</source> <volume>22</volume>: <fpage>1831</fpage>–<lpage>1834</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Li1"><label>70</label>
<mixed-citation publication-type="other" xlink:type="simple">Li Y, Zhong Y, Xu L, Zhang J, Xu X, <etal>et al</etal>.. (2013) Ultrafast synaptic events in a chalcogenide memristor. Scientific Reports 3 doi:10.1038/srep01619.</mixed-citation>
</ref>
<ref id="pone.0085175-MerrikhBayat2"><label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Merrikh-Bayat</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Shouraki</surname><given-names>SB</given-names></name> (<year>2011</year>) <article-title>Memristor-based circuits for performing basic arithmetic operations</article-title>. <source>Procedia Computer Science</source> <volume>3</volume>: <fpage>128</fpage>–<lpage>132</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-MerrikhBayat3"><label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Merrikh-Bayat</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Shouraki</surname><given-names>SB</given-names></name> (<year>2013</year>) <article-title>Memristive neuro-fuzzy system</article-title>. <source>IEEE Transactions on Cybernetics</source> <volume>43</volume>: <fpage>269</fpage>–<lpage>285</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Morabito1"><label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morabito</surname><given-names>FC</given-names></name>, <name name-style="western"><surname>Andreou</surname><given-names>AG</given-names></name>, <name name-style="western"><surname>Chicca</surname><given-names>E</given-names></name> (<year>2013</year>) <article-title>Neuromorphic engineering: from neural systems to brain-like engineered systems</article-title>. <source>Neural Networks</source> <volume>45</volume>: <fpage>1</fpage>–<lpage>3</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Klimo1"><label>74</label>
<mixed-citation publication-type="other" xlink:type="simple">Klimo M, Such O (2011) Memristors can implement fuzzy logic. preprint arXiv cs.ET/1110.2074.</mixed-citation>
</ref>
<ref id="pone.0085175-Klimo2"><label>75</label>
<mixed-citation publication-type="other" xlink:type="simple">Klimo M, Such O (2012) Fuzzy computer architecture based on memristor circuits. In: Proc. 2012 4th International Conference on Future Computational Technologies and Applications. 84–87.</mixed-citation>
</ref>
<ref id="pone.0085175-Kavehei1"><label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kavehei</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Al-Sarawi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Cho</surname><given-names>KR</given-names></name>, <name name-style="western"><surname>Eshraghian</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Abbott</surname><given-names>D</given-names></name> (<year>2012</year>) <article-title>An analytical approach for memristive nanoarchitectures</article-title>. <source>IEEE Transactions on Nanotechnology</source> <volume>11</volume>: <fpage>374</fpage>–<lpage>385</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Rosezin1"><label>77</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rosezin</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Linn</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Nielen</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Kugeler</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Bruchhaus</surname><given-names>R</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Integrated complementary resistive switches for passive high-density nanocrossbar arrays</article-title>. <source>IEEE Electron Device Letters</source> <volume>32</volume>: <fpage>191</fpage>–<lpage>193</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Kim1"><label>78</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kim</surname><given-names>KH</given-names></name>, <name name-style="western"><surname>Gaba</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Wheeler</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Cruz-Albrecht</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Hussain</surname><given-names>T</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>A functional hybrid memristor crossbar-array/CMOS system for data storage and neuromorphic applications</article-title>. <source>Nano Letters</source> <volume>12</volume>: <fpage>389</fpage>–<lpage>395</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Jo2"><label>79</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jo</surname><given-names>SH</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>KH</given-names></name>, <name name-style="western"><surname>Lu</surname><given-names>W</given-names></name> (<year>2009</year>) <article-title>High-density crossbar arrays based on a Si memristive system</article-title>. <source>Nano Letters</source> <volume>9</volume>: <fpage>870</fpage>–<lpage>874</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Xia1"><label>80</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Xia</surname><given-names>Q</given-names></name>, <name name-style="western"><surname>Robinett</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Cumbie</surname><given-names>MW</given-names></name>, <name name-style="western"><surname>Banerjee</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Cardinali</surname><given-names>TJ</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Memristor-1327 tor-CMOS hybrid integrated circuits for reconfigurable logic</article-title>. <source>Nano Letters</source> <volume>9</volume>: <fpage>3640</fpage>–<lpage>3645</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Strukov1"><label>81</label>
<mixed-citation publication-type="other" xlink:type="simple">Strukov DB, Stewart DR, Borghetti J, Li X, Pickett M, <etal>et al</etal>.. (2010) Hybrid CMOS/memristor circuits. In: Proc. 2010 IEEE International Symposium on Circuits and Systems (ISCAS). 1967–1970.</mixed-citation>
</ref>
<ref id="pone.0085175-Snider2"><label>82</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Snider</surname><given-names>G</given-names></name> (<year>2011</year>) <article-title>Instar and outstar learning with memristive nanodevices</article-title>. <source>Nanotechnology</source> <volume>22</volume>: <fpage>015201</fpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Thomas1"><label>83</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thomas</surname><given-names>A</given-names></name> (<year>2013</year>) <article-title>Memristor-based neural networks</article-title>. <source>Journal of Physics D: Applied Physics</source> <volume>46</volume>: <fpage>093001</fpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Indiveri1"><label>84</label>
<mixed-citation publication-type="other" xlink:type="simple">Indiveri G, Linares-Barranco B, Legenstein R, Deligeorgis G, Prodromakis T (2013) Integration of nanoscale memristor synapses in neuromorphic computing architectures. preprint arXiv cs.ET/1302.7007.</mixed-citation>
</ref>
<ref id="pone.0085175-Turing3"><label>85</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Turing</surname><given-names>AM</given-names></name> (<year>1952</year>) <article-title>The chemical basis of morphogenesis</article-title>. <source>Philosophical Transactions of the Royal Society of London Series B, Biological Sciences</source> <volume>237</volume>: <fpage>37</fpage>–<lpage>72</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Getling1"><label>86</label>
<mixed-citation publication-type="other" xlink:type="simple">Getling AV (1998) Rayleigh-Bénard Convection: Structures and Dynamics. World Scientific.</mixed-citation>
</ref>
<ref id="pone.0085175-Athelogou1"><label>87</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Athelogou</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Merté</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Deisz</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Hübler</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Lüscher</surname><given-names>E</given-names></name> (<year>1989</year>) <article-title>Extremal properties of dendritic patterns: biological applications</article-title>. <source>Helvetica Physica Acta</source> <volume>62</volume>: <fpage>250</fpage>–<lpage>253</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Swenson1"><label>88</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Swenson</surname><given-names>R</given-names></name> (<year>1989</year>) <article-title>Emergent attractors and the law of maximum entropy production: foundations to a theory of general evolution</article-title>. <source>Systems Research</source> <volume>6</volume>: <fpage>187</fpage>–<lpage>197</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Bejan1"><label>89</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bejan</surname><given-names>A</given-names></name> (<year>1997</year>) <article-title>Constructal-theory network of conducting paths for cooling a heat generating volume</article-title>. <source>International Journal of Heat and Mass Transfer</source> <volume>40</volume>: <fpage>799</fpage>–<lpage>816</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Jorgensen1"><label>90</label>
<mixed-citation publication-type="other" xlink:type="simple">Jorgensen SE, Svirezhev YM (2004) Towards a Thermodynamic Theory for Ecological Systems. Elsevier.</mixed-citation>
</ref>
<ref id="pone.0085175-Schneider1"><label>91</label>
<mixed-citation publication-type="other" xlink:type="simple">Schneider ED, Sagan D (2005) Into the Cool: Energy Flow, Thermodynamics, and Life. University of Chicago Press.</mixed-citation>
</ref>
<ref id="pone.0085175-Lotka1"><label>92</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lotka</surname><given-names>AJ</given-names></name> (<year>1922</year>) <article-title>Contribution to the energetics of evolution</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>8</volume>: <fpage>147</fpage>–<lpage>151</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Hatsopoulos1"><label>93</label>
<mixed-citation publication-type="other" xlink:type="simple">Hatsopoulos GN, Keenan JH (1981) Principles of General Thermodynamics. RE Krieger Publishing Company.</mixed-citation>
</ref>
<ref id="pone.0085175-Shang1"><label>94</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shang</surname><given-names>DS</given-names></name>, <name name-style="western"><surname>Shi</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Sun</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Shen</surname><given-names>BG</given-names></name>, <name name-style="western"><surname>Zhuge</surname><given-names>F</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Improvement of reproducible resistance switching in polycrystalline tungsten oxide films by in situ oxygen annealing</article-title>. <source>Applied Physics Letters</source> <volume>96</volume>: <fpage>072103</fpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Kvatinsky1"><label>95</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kvatinsky</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Friedman</surname><given-names>EG</given-names></name>, <name name-style="western"><surname>Kolodny</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Weiser</surname><given-names>UC</given-names></name> (<year>2013</year>) <article-title>TEAM: ThrEshold Adaptive Memristor model</article-title>. <source>IEEE Transactions on Circuits and Systems I: Regular Papers</source> <volume>60</volume>: <fpage>211</fpage>–<lpage>221</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Strukov2"><label>96</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Strukov</surname><given-names>DB</given-names></name>, <name name-style="western"><surname>Williams</surname><given-names>RS</given-names></name> (<year>2009</year>) <article-title>Exponential ionic drift: fast switching and low volatility of thin-film memristors</article-title>. <source>Applied Physics A</source> <volume>94</volume>: <fpage>515</fpage>–<lpage>519</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Pickett1"><label>97</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pickett</surname><given-names>MD</given-names></name>, <name name-style="western"><surname>Strukov</surname><given-names>DB</given-names></name>, <name name-style="western"><surname>Borghetti</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Yang</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Snider</surname><given-names>GS</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Switching dynamics in titanium dioxide memristive devices</article-title>. <source>Journal of Applied Physics</source> <volume>106</volume>: <fpage>074508</fpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Williams1"><label>98</label>
<mixed-citation publication-type="other" xlink:type="simple">Williams RS, Pickett MD, Strachan JP (2013) Physics–based memristor models. In: 1365 Proc. 2013 IEEE International Symposium on Circuits and Systems (ISCAS). 217–220.</mixed-citation>
</ref>
<ref id="pone.0085175-Zhang1"><label>99</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Sun</surname><given-names>HJ</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>Q</given-names></name>, <name name-style="western"><surname>Xu</surname><given-names>XH</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>AgInSbTe memristor with gradual resistance tuning</article-title>. <source>Applied Physics Letters</source> <volume>102</volume>: <fpage>183513</fpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Mladenov1"><label>100</label>
<mixed-citation publication-type="other" xlink:type="simple">Mladenov VM, Kirilov SM (2012) Analysis of a serial circuit with two memristors and voltage source at sine and impulse regime. In: Proc. 2012 IEEE 13th International Workshop on Cellular Nanoscale Networks and Their Applications (CNNA). 1–6.</mixed-citation>
</ref>
<ref id="pone.0085175-Jain1"><label>101</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jain</surname><given-names>AK</given-names></name>, <name name-style="western"><surname>Murty</surname><given-names>MN</given-names></name>, <name name-style="western"><surname>Flynn</surname><given-names>PJ</given-names></name> (<year>1999</year>) <article-title>Data clustering: a review</article-title>. <source>ACM Computing Surveys (CSUR)</source> <volume>31</volume>: <fpage>264</fpage>–<lpage>323</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Lloyd1"><label>102</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lloyd</surname><given-names>S</given-names></name> (<year>1982</year>) <article-title>Least squares quantization in PCM</article-title>. <source>IEEE Transactions on Information Theory</source> <volume>28</volume>: <fpage>129</fpage>–<lpage>137</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Kriegel1"><label>103</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegel</surname><given-names>HP</given-names></name>, <name name-style="western"><surname>Kröger</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Sander</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Zimek</surname><given-names>A</given-names></name> (<year>2011</year>) <article-title>Density-based clustering</article-title>. <source>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</source> <volume>1</volume>: <fpage>231</fpage>–<lpage>240</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Ankerst1"><label>104</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ankerst</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Breunig</surname><given-names>MM</given-names></name>, <name name-style="western"><surname>Kriegel</surname><given-names>HP</given-names></name>, <name name-style="western"><surname>Sander</surname><given-names>J</given-names></name> (<year>1999</year>) <article-title>OPTICS: ordering points to identify the clustering structure</article-title>. <source>ACM SIGMOD Record</source> <volume>28</volume>: <fpage>49</fpage>–<lpage>60</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Dempster1"><label>105</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dempster</surname><given-names>AP</given-names></name>, <name name-style="western"><surname>Laird</surname><given-names>NM</given-names></name>, <name name-style="western"><surname>Rubin</surname><given-names>DB</given-names></name> (<year>1977</year>) <article-title>Maximum likelihood from incomplete data via the EM algorithm</article-title>. <source>Journal of the Royal Statistical Society: Series B</source> <volume>39</volume>: <fpage>1</fpage>–<lpage>38</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Kotsiantis1"><label>106</label>
<mixed-citation publication-type="other" xlink:type="simple">Kotsiantis SB (2007) Supervised Machine Learning: a Review of Classification Techniques. IOS Press. 3–24 pp.</mixed-citation>
</ref>
<ref id="pone.0085175-Yu1"><label>107</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yu</surname><given-names>HF</given-names></name>, <name name-style="western"><surname>Hsieh</surname><given-names>CJ</given-names></name>, <name name-style="western"><surname>Chang</surname><given-names>KW</given-names></name>, <name name-style="western"><surname>Lin</surname><given-names>CJ</given-names></name> (<year>2012</year>) <article-title>Large linear classification when data cannot fit in memory</article-title>. <source>ACM Transactions on Knowledge Discovery from Data (TKDD)</source> <volume>5</volume>: <fpage>23</fpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Ndong1"><label>108</label>
<mixed-citation publication-type="other" xlink:type="simple">Ndong J, Salamatian K (2011) Signal processing-based anomaly detection techniques: a comparative analysis. In: Proc. 2011 3rd International Conference on Evolving Internet. 32–39.</mixed-citation>
</ref>
<ref id="pone.0085175-Zhang2"><label>109</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Patuwo</surname><given-names>BE</given-names></name>, <name name-style="western"><surname>Hu</surname><given-names>MY</given-names></name> (<year>1998</year>) <article-title>Forecasting with artificial neural networks: The state of the art</article-title>. <source>International Journal of Forecasting</source> <volume>14</volume>: <fpage>35</fpage>–<lpage>62</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Hoeting1"><label>110</label>
<mixed-citation publication-type="other" xlink:type="simple">Hoeting JA, Madigan D, Raftery AE, Volinsky CT (1999) Bayesian model averaging: a tutorial. Statistical Science: 382–401.</mixed-citation>
</ref>
<ref id="pone.0085175-Cirean1"><label>111</label>
<mixed-citation publication-type="other" xlink:type="simple">Cireşan DC, Meier U, Gambardella LM, Schmidhuber J (2012) Deep big multilayer perceptrons for digit recognition. In: Neural Networks: Tricks of the Trade, Springer Berlin Heidelberg, volume 7700 of Lecture Notes in Computer Science. 581–598.</mixed-citation>
</ref>
<ref id="pone.0085175-Chaaban1"><label>112</label>
<mixed-citation publication-type="other" xlink:type="simple">Chaaban I, Scheessele MR (2007) Human performance on the USPS database. Report, Indiana University South Bend.</mixed-citation>
</ref>
<ref id="pone.0085175-Kuzum1"><label>113</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kuzum</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Jeyasingh</surname><given-names>RGD</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Wong</surname><given-names>HSP</given-names></name> (<year>2011</year>) <article-title>Nanoelectronic programmable synapses based on phase change materials for brain-inspired computing</article-title>. <source>Nano Letters</source> <volume>12</volume>: <fpage>2179</fpage>–<lpage>2186</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Bache1"><label>114</label>
<mixed-citation publication-type="other" xlink:type="simple">Bache K, Lichman M (2013). UCI machine learning repository. Available: <ext-link ext-link-type="uri" xlink:href="http://archive.ics.uci.edu/ml" xlink:type="simple">http://archive.ics.uci.edu/ml</ext-link>.</mixed-citation>
</ref>
<ref id="pone.0085175-Chen1"><label>115</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chen</surname><given-names>HL</given-names></name>, <name name-style="western"><surname>Yang</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>DY</given-names></name> (<year>2011</year>) <article-title>A support vector machine classifier with rough set-based feature selection for breast cancer diagnosis</article-title>. <source>Expert Systems with Applications</source> <volume>38</volume>: <fpage>9014</fpage>–<lpage>9022</lpage>.</mixed-citation>
</ref>
<ref id="pone.0085175-Kohavi1"><label>116</label>
<mixed-citation publication-type="other" xlink:type="simple">Kohavi R (1996) Scaling up the accuracy of naïve-Bayes classifiers: a decision-tree hybrid. In: Proc. 1996 2nd International Conference on Knowledge Discovery and Data Mining. 202–207.</mixed-citation>
</ref>
<ref id="pone.0085175-Deng1"><label>117</label>
<mixed-citation publication-type="other" xlink:type="simple">Deng L, Yu D (2011) Deep convex net: A scalable architecture for speech pattern classification. In: Proc. 2011 Interspeech. 2285–2288.</mixed-citation>
</ref>
<ref id="pone.0085175-Joachims1"><label>118</label>
<mixed-citation publication-type="other" xlink:type="simple">Joachims T (1998) Text Categorization with Support Vector Machines: Learning with Many Relevant Features. Springer.</mixed-citation>
</ref>
<ref id="pone.0085175-Bennett1"><label>119</label>
<mixed-citation publication-type="other" xlink:type="simple">Bennett KP, Blue JA (1998) A support vector machine approach to decision trees. In: Proc. 1998 IEEE International Joint Conference on Neural Networks Proceedings. The 1998 IEEE World Congress on Computational Intelligence. volume 3, 2396–2401.</mixed-citation>
</ref>
<ref id="pone.0085175-Ranzato1"><label>120</label>
<mixed-citation publication-type="other" xlink:type="simple">Ranzato M, Huang FJ, Boureau YL, Lecun Y (2007) Unsupervised learning of invariant feature hierarchies with applications to object recognition. In: Proc. 2007 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 1–8.</mixed-citation>
</ref>
<ref id="pone.0085175-Quinlan1"><label>121</label>
<mixed-citation publication-type="other" xlink:type="simple">Quinlan JR (1996) Improved use of continuous attributes in C4.5. preprint arXiv cs.AI/9603103.</mixed-citation>
</ref>
</ref-list></back>
</article>