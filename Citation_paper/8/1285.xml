<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-18-00055</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1006711</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Speech</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuronal tuning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Speech signal processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Modulation</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject><subj-group><subject>Acoustic signals</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>A Gestalt inference model for auditory scene segregation</article-title>
<alt-title alt-title-type="running-head">A Gestalt inference model for auditory scene segregation</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Chakrabarty</surname> <given-names>Debmalya</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2597-738X</contrib-id>
<name name-style="western">
<surname>Elhilali</surname> <given-names>Mounya</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"/>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<addr-line>Laboratory for Computational Audio Processing, Center for Speech and Language Processing, Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Theunissen</surname> <given-names>Frédéric E.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University of California at Berkeley, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">mounya@jhu.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>1</month>
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>22</day>
<month>1</month>
<year>2019</year>
</pub-date>
<volume>15</volume>
<issue>1</issue>
<elocation-id>e1006711</elocation-id>
<history>
<date date-type="received">
<day>11</day>
<month>1</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>12</day>
<month>12</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Chakrabarty, Elhilali</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1006711"/>
<abstract>
<p>Our current understanding of how the brain segregates auditory scenes into meaningful objects is in line with a Gestaltism framework. These Gestalt principles suggest a theory of how different attributes of the soundscape are extracted then bound together into separate groups that reflect different objects or streams present in the scene. These cues are thought to reflect the underlying statistical structure of natural sounds in a similar way that statistics of natural images are closely linked to the principles that guide figure-ground segregation and object segmentation in vision. In the present study, we leverage inference in stochastic neural networks to learn emergent grouping cues directly from natural soundscapes including speech, music and sounds in nature. The model learns a hierarchy of local and global spectro-temporal attributes reminiscent of simultaneous and sequential Gestalt cues that underlie the organization of auditory scenes. These mappings operate at multiple time scales to analyze an incoming complex scene and are then fused using a Hebbian network that binds together coherent features into perceptually-segregated auditory objects. The proposed architecture successfully emulates a wide range of well established auditory scene segregation phenomena and quantifies the complimentary role of segregation and binding cues in driving auditory scene segregation.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author summary</title>
<p>In every day life, our brain is able to effortlessly make sense of the cacophony of sounds that constantly enter our ears and organize them into meaningful sound objects. In this work, we use an architecture based on stochastic neural networks to ‘learn’ from natural sounds which cues are crucial to the process of auditory scene organization. The computational model delivers a hierarchical architecture that mimics multistage processing in the biological auditory system. It learns a rich hierarchy of spectral and temporal features that allow the decomposition of an auditory scene into informative components. These features are then grouped together into coherent objects based on Hebbian learning principles. Though trained on unrelated datasets of natural sounds, the model is able to replicate human perception of auditory scenes in a wide variety of soundscapes ranging from simple tone sequences to complex speech-in-noise scenes.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000050</institution-id>
<institution>National Heart, Lung, and Blood Institute</institution>
</institution-wrap>
</funding-source>
<award-id>R01HL133043</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2597-738X</contrib-id>
<name name-style="western">
<surname>Elhilali</surname> <given-names>Mounya</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000006</institution-id>
<institution>Office of Naval Research</institution>
</institution-wrap>
</funding-source>
<award-id>N000141612045</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2597-738X</contrib-id>
<name name-style="western">
<surname>Elhilali</surname> <given-names>Mounya</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000049</institution-id>
<institution>National Institute on Aging</institution>
</institution-wrap>
</funding-source>
<award-id>U01AG058532</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2597-738X</contrib-id>
<name name-style="western">
<surname>Elhilali</surname> <given-names>Mounya</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award004">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000006</institution-id>
<institution>Office of Naval Research</institution>
</institution-wrap>
</funding-source>
<award-id>N000141712736</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-2597-738X</contrib-id>
<name name-style="western">
<surname>Elhilali</surname> <given-names>Mounya</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This research was supported by National Institutes of Health grants R01HL133043 and U01AG058532 and Office of Naval Research grants N000141612045 and N000141712736. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="5"/>
<table-count count="0"/>
<page-count count="33"/>
</counts>
<custom-meta-group>
<custom-meta>
<meta-name>PLOS Publication Stage</meta-name>
<meta-value>vor-update-to-uncorrected-proof</meta-value>
</custom-meta>
<custom-meta>
<meta-name>Publication Update</meta-name>
<meta-value>2019-02-01</meta-value>
</custom-meta>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>No experimental data is generated in this study. An implementation of the model is available at <ext-link ext-link-type="uri" xlink:href="https://engineering.jhu.edu/lcap/" xlink:type="simple">https://engineering.jhu.edu/lcap/</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>We live in busy environments, and our surrounds continuously flood our sensory system with complex information that needs to be analyzed in order to make sense of the world around us. This process, labeled scene analysis, is common across all sensory modalities including vision, audition and olfaction [<xref ref-type="bibr" rid="pcbi.1006711.ref001">1</xref>]. It refers to the ability of humans, animals and machines alike to parse the mixture of cues impinging on our senses, organize them into meaningful groups and map them onto relevant foreground and background objects. Our brain relies on innate dispositions that aid this process and help guide the organization of patterns into perceived objects [<xref ref-type="bibr" rid="pcbi.1006711.ref002">2</xref>]. These dispositions, referred to as Gestalt principles, inform our current understanding of the perceptual organization of scenes [<xref ref-type="bibr" rid="pcbi.1006711.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref004">4</xref>].</p>
<p>In most theoretical accounts, the role of Gestalt principles in parsing a scene can be conceptualized in two stages: segregation (or analysis) and grouping (or fusion) [<xref ref-type="bibr" rid="pcbi.1006711.ref005">5</xref>]. In the first stage, the sensory mixture is decomposed into feature elements, believed to be the building blocks of the scene. These features reflect the physical nature of sources in the scene, the state and structure of the environment itself, as well as perceptual mappings of these attributes as viewed by the sensory system. These features vary in complexity along a continuum from basic attributes (e.g. edges or frequency components) to more complex characteristics of the scene (e.g. shapes or timbral profiles). The ubiquitous nature of these profiles often conceals the multiplexed structures that underlie this analysis of scene features in the brain. In most computational accounts, this segregation stage is modeled using feature analyses which map the sensory signal into its building blocks ranging from simple components (e.g. frequency channels) to dimensionally-complex kernels [<xref ref-type="bibr" rid="pcbi.1006711.ref006">6</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref007">7</xref>].</p>
<p>Processing the distinctive features of a scene is generally followed by a fusion stage which integrates the state and behavior of the scene’s building blocks using grouping mechanisms that reflect the local and global distribution and dynamics of the features. This stage employs ‘rules’ that guide how grouped elements give rise to perceptually coherent structures forming <italic>objects</italic> or <italic>streams</italic> [<xref ref-type="bibr" rid="pcbi.1006711.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref009">9</xref>]. In many mathematical models, these grouping cues are often leveraged in back-end classifiers that are tuned to capture patterns and relationships within specific object classes (e.g. speech, music, faces, etc) [<xref ref-type="bibr" rid="pcbi.1006711.ref010">10</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref013">13</xref>]. In doing so, these models effectively capture the inter-dependencies between object attributes and learn their mapping onto an integrated representational space [<xref ref-type="bibr" rid="pcbi.1006711.ref014">14</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref016">16</xref>]. Ultimately, success in tackling scene analysis depends on two key components [<xref ref-type="bibr" rid="pcbi.1006711.ref017">17</xref>]: (i) obtaining a rich and robust feature representation that can capture object specific details present in the scene; (ii) grouping the feature elements such that their spatial and temporal associations match the dynamics of objects within the scene.</p>
<p>Vision models have been very successful in mining these two aspects of scene analysis. Intricate hierarchical systems have leveraged inherent structure in static and dynamic images to extract increasingly elaborate features from a scene that are then used to segment it, interpret its objects or track them over time [<xref ref-type="bibr" rid="pcbi.1006711.ref018">18</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref020">20</xref>]. Data-driven approaches have shown that high dimensional feature spaces are very effective in extracting meaningful semantics from arbitrary natural images [<xref ref-type="bibr" rid="pcbi.1006711.ref020">20</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref022">22</xref>]; while hand-engineered features like scale-invariant feature transform (SIFT) [<xref ref-type="bibr" rid="pcbi.1006711.ref023">23</xref>], histogram of oriented gradients (HOG) [<xref ref-type="bibr" rid="pcbi.1006711.ref024">24</xref>], and Bag-of-visual-word descriptor [<xref ref-type="bibr" rid="pcbi.1006711.ref025">25</xref>] among others have also enjoyed a great deal of success in tackling computer vision problems like image classification and object detection. Recent advances in deep layered architectures have resulted in a flurry of rich representational spaces showing selectivity to contours, corners, angles and surface boundaries in images [<xref ref-type="bibr" rid="pcbi.1006711.ref026">26</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref029">29</xref>]. The deep nature of these architectures has also led to a natural evolution from low-level features to more complex, higher-level embeddings that capture scene semantics or syntax [<xref ref-type="bibr" rid="pcbi.1006711.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref031">31</xref>].</p>
<p>In audition, computational approaches to tackle auditory scene organization have mostly taken advantage of physiological and perceptual underpinnings of sound processing [<xref ref-type="bibr" rid="pcbi.1006711.ref017">17</xref>]. A large body of work has built on knowledge of the auditory pathway, particularly the peripheral system to build sophisticated analysis models of auditory scenes. These systems extract relevant cues from a scene, such as its spectral content, spatial structure as well as temporal dynamics; hence allowing sound events with uncorrelated acoustic behavior to occupy different subspaces in the analysis stage. These models are quite effective in replicating perceptual results of stream segregation especially using simple tone and noise stimuli [<xref ref-type="bibr" rid="pcbi.1006711.ref032">32</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref037">37</xref>]. Some models also extend beyond early acoustic features to examine feature binding mechanisms that can be used as an effective strategy in segregating wide range of stimuli from simple tone sequences to spectro-temporally complex sounds like speech and music [<xref ref-type="bibr" rid="pcbi.1006711.ref038">38</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref040">40</xref>]. In most approaches however, the models are built around hand-crafted feature representations, hence limiting their scope to specific mappings of the acoustic space. With the emergence of deep belief architectures, recent efforts started learning rich feature spaces from natural soundscapes in a data driven fashion, and subsequently using these spaces in domains like music genre classification, phoneme classification and speaker identification [<xref ref-type="bibr" rid="pcbi.1006711.ref041">41</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref044">44</xref>]. Applications of deep learning have also successfully tackled the problem of speech separation even with monaural inputs by learning embeddings of a speaker’s time-frequency dynamics against other speakers [<xref ref-type="bibr" rid="pcbi.1006711.ref045">45</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref046">46</xref>].</p>
<p>The current study also leverages neural network theory to ‘learn’ Gestalt principles directly from sound. The work examines what kind of cues can one <italic>infer</italic> from natural sounds; how well do these learned cue reflect the known Gestalt components of auditory streams; and how effective are these cues in explaining perceptual organization of auditory scenes with varying degrees of complexity. The model is devised as a hierarchical structure that generally follows the two-stage pipeline of analysis then fusion, in line with prototypical scene analysis theories [<xref ref-type="bibr" rid="pcbi.1006711.ref005">5</xref>]. This system analyzes the incoming acoustic signal with a multitude of granularities, hence allowing both local and global acoustic attributes to emerge. The short-term analysis performs a local tiling of the spectro-temporal space; hence inferring <italic>simultaneous</italic> grouping cues [<xref ref-type="bibr" rid="pcbi.1006711.ref047">47</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref049">49</xref>]. A longer-range analysis extends the segregation stage to examine temporal dependencies across acoustic attributes over different time scales; hence exploring emergence of <italic>sequential</italic> grouping cues [<xref ref-type="bibr" rid="pcbi.1006711.ref050">50</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref054">54</xref>]. Finally, a fusion stage binds the cues together based on how strongly they correlate with each other across multiple time scales. This integration is achieved using <italic>Hebbian</italic> learning which reinforces activity across coherent channels and suppresses activity across incoherent ones [<xref ref-type="bibr" rid="pcbi.1006711.ref055">55</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref057">57</xref>]. Apart from the basic layout and choice of analysis window sizes, the network is trained in an unsupervised fashion on a rich sound dataset including speech and nature sounds hence offering a general inference architecture of auditory Gestalt cues that are common across many sound environments.</p>
<p>The overall system is tested with a wide range of stimuli where we can quantify the role of each and every component of the network in driving stream segregation processes. We also contrast the system performance with a set of control experiments where different components of the model are deliberately switched on/off in order to examine their impact on the organization of different acoustic scenes. These control experiments aim not only to dissect the role of various system components. They also shed light on how necessary and/or sufficient different grouping cues are to anchor the analysis of different stimuli structures and sound types. The paper first presents an in-depth description of the proposed architecture, followed by an analysis of the emergent properties of the trained network and their potential neural correlates in the auditory pathway. The experimental results outline how the network replicates human psychoacoustic behavior in stream segregation and speech intelligibility paradigms. Finally, we present control experiments that dissect the network architecture and examine the contribution its component. We discuss the implications of this network in shedding light on ties between observed perceptual performance in various complex auditory scenes and the neural underpinnings of this behavior as implemented in networks of neurons along the auditory pathway.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>A Gestalt inference model for auditory scene segregation</title>
<p>A number of Gestalt principles have been posited as indispensable anchors used by the brain to guide the segregation of auditory scenes into perceptually meaningful objects [<xref ref-type="bibr" rid="pcbi.1006711.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref047">47</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref058">58</xref>]. These comprise a wide variety of cues; for instance harmonicity which couples harmonically-related frequency channels together, common fate which favors sound elements that co-vary in amplitude, and common onsets which groups components that share a similar starting time and to a lesser degree a common ending time. Most of these cues are thought to be innate in our auditory system, and evidence for their role is found across many species [<xref ref-type="bibr" rid="pcbi.1006711.ref059">59</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref063">63</xref>]. These processes likely take advantage of statistical regularities of sounds in natural environments and reflect the physical constraints of sound generation and propagation (e.g. two sound sources rarely start at the exactly the same time; periodic vibrations induce resonant modes at integer multiples of the fundamental frequency). Here, we examine whether a statistical inference model can learn these cues directly from natural sounds; and if so, how effective are these learned cues relative to existing hand-tailored segregation systems.</p>
<p>The proposed model is designed as a hierarchical system that explicitly mimics an ‘analysis-then-fusion’ processing pipeline. The analysis stage is itself laid out in two stages. First, an analysis of local spectrotemporal cues aims to learn <italic>simultaneous</italic> Gestalt cues believed to operate over short-time scales in order to locally segregate sound elements. Second, an analysis of more global cues operates over longer time-scales and aims to learn <italic>sequential</italic> Gestalt cues that enable tracking dynamics of elements from the first stage at a temporal or melodic level [<xref ref-type="bibr" rid="pcbi.1006711.ref008">8</xref>]. Following these stages is a fusion step that combines together segregated elements that constitute different auditory objects, using principles of <italic>temporal coherence</italic> [<xref ref-type="bibr" rid="pcbi.1006711.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref064">64</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref065">65</xref>]. The Gestalt analysis stages are learned directly from natural sounds in a generative fashion, allowing each component of the model to represent natural sounds from its own vantage point following principles of stochastic neural networks, as detailed next. The fusion stage merely organizes or fuses these learned patterns following the concept of temporal coherence, as also detailed later.</p>
<p><xref ref-type="fig" rid="pcbi.1006711.g001">Fig 1</xref> depicts a schematic of the overall model. It takes as input the acoustic waveform of an auditory scene <italic>u</italic>(<italic>t</italic>) and maps it onto a time-frequency representation, using a biomimetic peripheral model from Yang <italic>et al</italic>. [<xref ref-type="bibr" rid="pcbi.1006711.ref066">66</xref>]. Briefly, this transformation analyzes the acoustic signal <italic>u</italic>(<italic>t</italic>) using a bank of logarithmically-spaced cochlear filters whose outputs are further sharpened via a first order derivative along the frequency axis, followed by half wave rectification and short term integration over 10ms frames (see <xref ref-type="sec" rid="sec015">Methods</xref> for details). This filterbank analysis results in an auditory spectrogram represented by <italic>S</italic>(<italic>t</italic>, <italic>f</italic>).</p>
<fig id="pcbi.1006711.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006711.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Schematic of the proposed model.</title>
<p>An acoustic signal <italic>u</italic>(<italic>t</italic>) undergoes a series of transformations starting with a mapping to a time-frequency spectrogram, followed by two-layers of stochastic neural networks (local analysis <inline-formula id="pcbi.1006711.e001"><alternatives><graphic id="pcbi.1006711.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> and long-range analysis <inline-formula id="pcbi.1006711.e002"><alternatives><graphic id="pcbi.1006711.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>), then a fusion stage <inline-formula id="pcbi.1006711.e003"><alternatives><graphic id="pcbi.1006711.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>3</mml:mn></mml:msub></mml:math></alternatives></inline-formula>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006711.g001" xlink:type="simple"/>
</fig>
<p>The following stage (called <inline-formula id="pcbi.1006711.e004"><alternatives><graphic id="pcbi.1006711.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>) is structured as a two-layer sparse Restricted Boltzmann Machine (sparse RBM) with a fully connected visible and hidden layer [<xref ref-type="bibr" rid="pcbi.1006711.ref067">67</xref>]. It takes as input 3 consecutive frames of the spectrogram and learns a probability distribution over the set of these short tokens. RBMs are powerful stochastic neural networks that are conceptually similar to autoencoders but can infer statistical distributions over their input set [<xref ref-type="bibr" rid="pcbi.1006711.ref068">68</xref>]. A RBM layer is chosen for this stage in order to explore the space of local spectrotemporal tokens and learn latent cues that represent statistical structures in natural sounds over short time scales. The visible layer units {<italic>x</italic><sub><italic>k</italic></sub>} are real-valued and characterized by a Gaussian distribution fitted over the input spectrogram <italic>S</italic>(<italic>t</italic>, <italic>f</italic>); while hidden units {<italic>h</italic><sub><italic>k</italic></sub>} are sampled from a Bernoulli distribution for <italic>k</italic> = 1, 2, …, <italic>K</italic> where <italic>K</italic> is the number of nodes in each layer. The network is parameterized by Θ = {<italic>W</italic>, <italic>A</italic>, <italic>B</italic>} where <italic>W</italic> represents the interconnected weights between visible and hidden units, and <italic>A</italic> (<italic>B</italic>) represents the visible (hidden) bias, respectively. The network is trained using a Contrastive Divergence (CD) algorithm with the objective to minimize the reconstruction error between <italic>x</italic> and <inline-formula id="pcbi.1006711.e005"><alternatives><graphic id="pcbi.1006711.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mi>h</mml:mi> <mml:mi>W</mml:mi> <mml:mo>+</mml:mo> <mml:mi>A</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> [<xref ref-type="bibr" rid="pcbi.1006711.ref069">69</xref>].</p>
<p>By learning the regularities in local spectrotemporal tokens of natural sounds, the connection weights <italic>W</italic> effectively span an array of latent cues that reflect the structure of soundscapes. Our hypothesis is that these latent factors represent the so-called simultaneous cues used as Gestalt principles for sound analysis. After training, connection weights are transformed into a 2D filter <inline-formula id="pcbi.1006711.e006"><alternatives><graphic id="pcbi.1006711.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:mi mathvariant="script">F</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, akin to spectro-temporal receptive fields derived from neural activity of biological neurons in the auditory system [<xref ref-type="bibr" rid="pcbi.1006711.ref070">70</xref>]. These learned filters are then applied in a convolutional fashion over the incoming spectrogram <italic>S</italic>(<italic>t</italic>, <italic>f</italic>) to derive the outputs of layer <inline-formula id="pcbi.1006711.e007"><alternatives><graphic id="pcbi.1006711.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> nodes. These responses are further subjected to a neural adaptation stage which imposes a dynamic regulation of the response of each filter hence suppressing units with weak activation (see <xref ref-type="sec" rid="sec015">Methods</xref> for details).</p>
<p>
<inline-formula id="pcbi.1006711.e008">
<alternatives>
<graphic id="pcbi.1006711.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e008" xlink:type="simple"/>
<mml:math display="inline" id="M8">
<mml:msub>
<mml:mi mathvariant="script">L</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
</mml:math>
</alternatives>
</inline-formula> responses are then processed by the next layer in the model which completes the analysis stage to infer possible sequential cues that extend over longer time constants. This second layer <inline-formula id="pcbi.1006711.e009"><alternatives><graphic id="pcbi.1006711.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> is devised as an array of conditional RBMs (cRBMs), which are extended versions of RBMs designed to model temporal dependencies [<xref ref-type="bibr" rid="pcbi.1006711.ref071">71</xref>]. Similar to a RBM, a cRBM consists of a visible layer with units {<italic>x</italic><sub><italic>k</italic></sub>}, assumed to arise from a Gaussian distribution fitted over the input, and a hidden layer with {<italic>h</italic><sub><italic>k</italic></sub>} units sampled from a Bernoulli distribution. Unlike a RBM, a cRBM acts as a dynamical system operating over an entire input history <italic>τ</italic> taking as input occurrences at times {<italic>t</italic>, <italic>t</italic> − 1, …, <italic>t</italic> − <italic>τ</italic>} in order to capture dynamics in the input space over context <italic>τ</italic>. In the current model, we explore sequential cues over a range of temporal contexts and construct an array of parallel cRBM networks over multiple histories ranging in temporal resolutions from <italic>τ</italic> ∼ (30–600 <italic>ms</italic>). <inline-formula id="pcbi.1006711.e010"><alternatives><graphic id="pcbi.1006711.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> is parameterized by Θ = {<italic>W</italic>, <italic>A</italic><sup><italic>τ</italic></sup>, <italic>B</italic><sup><italic>τ</italic></sup>, <italic>C</italic><sup><italic>τ</italic></sup>, <italic>D</italic><sup><italic>τ</italic></sup>} where <italic>W</italic> represents the interconnected weights between visible and hidden units and capture the interactions across input features over an extended temporal history <italic>τ</italic>, <italic>A</italic><sup><italic>τ</italic></sup> and <italic>B</italic><sup><italic>τ</italic></sup> represent the visible and hidden biases, respectively, while <italic>C</italic><sup><italic>τ</italic></sup> and <italic>D</italic><sup><italic>τ</italic></sup> quantify autoregressive weights between past inputs and the current input (or current hidden unit, respectively). Just like the localized layer <inline-formula id="pcbi.1006711.e011"><alternatives><graphic id="pcbi.1006711.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>, the contextual layer <inline-formula id="pcbi.1006711.e012"><alternatives><graphic id="pcbi.1006711.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> is trained in a generative fashion using contrastive divergence (CD) in order to best capture the dynamics in natural sounds using the same dataset of realistic sounds spanning speech, music and natural sounds. Here again, our hypothesis is that the stochastic cRBM learns latent parameters Θ that reflect the sequential cues underlying dynamics of natural sounds over a wide range of temporal contexts. Once trained, the model parameters are applied to incoming <inline-formula id="pcbi.1006711.e013"><alternatives><graphic id="pcbi.1006711.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> filter responses in a linear fashion, yielding a multi-resolution output which is then passed over to the next stage in the hierarchy (see <xref ref-type="sec" rid="sec015">Methods</xref> for details).</p>
<p>The next layer in the hierarchy focuses on a fusion operation to facilitate the grouping of perceptually-coherent objects. This binding stage explores co-activations across all <inline-formula id="pcbi.1006711.e014"><alternatives><graphic id="pcbi.1006711.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> channels within a given context <italic>τ</italic> and binds together the units that exhibit strong temporal coherence [<xref ref-type="bibr" rid="pcbi.1006711.ref064">64</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref072">72</xref>]. The <italic>‘temporal coherence’</italic> theory posits that emergence of perceptual representations of auditory objects depends upon <italic>strong</italic> coherence across cues emanating from same object and <italic>weaker</italic> co-activation across cues from competing objects. This coherence is not an instantaneous correlation but one that is accumulated over longer time scales, commensurate with the contextual windows explored in the <inline-formula id="pcbi.1006711.e015"><alternatives><graphic id="pcbi.1006711.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> layer. We implement this concept in a biologically-plausible fashion via mechanisms of Hebbian learning, which suggests that when two neurons fire together, their synaptic connection gets stronger [<xref ref-type="bibr" rid="pcbi.1006711.ref073">73</xref>]. Effectively, Hebbian interactions operate by reinforcing activity across coherent channels, hence grouping them into putative objects and inhibiting activity across incoherent channels [<xref ref-type="bibr" rid="pcbi.1006711.ref074">74</xref>]. We implement a synaptic interaction across output channels from layer <inline-formula id="pcbi.1006711.e016"><alternatives><graphic id="pcbi.1006711.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> by introducing a coherence synaptic weight matrix <italic>V</italic>. If two units <italic>i</italic> and <italic>j</italic> are co-activated at a given time <italic>t</italic>, their corresponding synaptic connection <italic>V</italic><sub><italic>ij</italic></sub> is reinforced over time. If the correlation between their activity is weak, the corresponding synaptic weight <italic>V</italic><sub><italic>ij</italic></sub> is reduced accordingly. These synaptic weights are applied to the output of each channel in a dynamic fashion, hence modulating the activity across an entire ensemble of neurons within each context in layer <inline-formula id="pcbi.1006711.e017"><alternatives><graphic id="pcbi.1006711.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>. The net effect gives emergence to perceptual coherent groups that represent auditory objects in a scene. A final read-out stage is then appended to the model to extract responses to different stimuli and test the degree of segregation of different objects, as viewed by the model outputs (see <xref ref-type="sec" rid="sec015">Methods</xref> for details).</p>
</sec>
<sec id="sec004">
<title>Model characterization</title>
<p>In order to examine the emergent sensitivity of learned layers in the network, we derive the tuning characteristics of individual nodes or neurons and explore their filtering properties in the modulation domain [<xref ref-type="bibr" rid="pcbi.1006711.ref075">75</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref076">76</xref>]. Modulation tuning reflects stimulus cues that best drive individual nodes in the model both in terms of temporal variations and dynamics (i.e. temporal modulations or rates) as well as spectral span and bandwidth (i.e. spectral modulations or scales). This approach follows common empirical techniques used in electrophyisology and psychophysics to probe the tuning of a system to specific acoustic cues. It is specifically used to characterize spectro-temporal receptive fields (STRFs) which offer 2-dimensional profiles of filtering characteristics of neurons [<xref ref-type="bibr" rid="pcbi.1006711.ref070">70</xref>].</p>
<p>First, we employ a classic transfer function method using probe stimuli in order to derive the tuning of both <inline-formula id="pcbi.1006711.e018"><alternatives><graphic id="pcbi.1006711.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006711.e019"><alternatives><graphic id="pcbi.1006711.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> layers of the network [<xref ref-type="bibr" rid="pcbi.1006711.ref077">77</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref079">79</xref>]. We present modulated noise signals (called ripples) as input to the model with varying spectro-temporal modulation parameters (<xref ref-type="fig" rid="pcbi.1006711.g002">Fig 2E</xref>) and characterize the fidelity of the ripple encoding at various stages of the network as the ripple modulation parameters are varied [<xref ref-type="bibr" rid="pcbi.1006711.ref080">80</xref>]. Each ripple is constructed as a broadband noise signal whose envelope is modulated both in time and frequency, with temporal modulation parameter <italic>ω</italic> (in Hz) and spectral modulation parameter Ω (in cyc/oct) (see <xref ref-type="sec" rid="sec015">Methods</xref> for details).</p>
<fig id="pcbi.1006711.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006711.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Modulation characteristics of the network.</title>
<p>(A,B) Normalized modulation transfer function for layers <inline-formula id="pcbi.1006711.e020"><alternatives><graphic id="pcbi.1006711.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> -left- and <inline-formula id="pcbi.1006711.e021"><alternatives><graphic id="pcbi.1006711.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> -right- displayed in axes of rate (temporal modulations in Hz)—scale (spectral modulations in cycles per octave). (C,D) Overlaid on each transfer function is a contour plot of agglomerative clusters in spectro-temporal modulation space for layers <inline-formula id="pcbi.1006711.e022"><alternatives><graphic id="pcbi.1006711.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> -left- and <inline-formula id="pcbi.1006711.e023"><alternatives><graphic id="pcbi.1006711.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> -right-.(E) Noise ripples are used to analyze the spectro-temporal tuning of the model at different stages. They are noise signals that are modulated in time and frequency. (F) Example filter tuning <inline-formula id="pcbi.1006711.e024"><alternatives><graphic id="pcbi.1006711.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mrow><mml:mi mathvariant="script">F</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> from layer <inline-formula id="pcbi.1006711.e025"><alternatives><graphic id="pcbi.1006711.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> for 4 nodes exhibiting tuning to harmonicity (top, left), onset (top, right), a slow neuron (bottom, left) and a fast neuron (bottom, right). The filter response profiles have been interpolated using a cubic function for display purposes.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006711.g002" xlink:type="simple"/>
</fig>
<p>By sweeping through a range of ripple parameters, we compute a normalized modulation transfer function (MTF) from the response of layers <inline-formula id="pcbi.1006711.e026"><alternatives><graphic id="pcbi.1006711.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006711.e027"><alternatives><graphic id="pcbi.1006711.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> which quantifies the synchronized response of each layer to the corresponding dynamics in the ripple stimulus (see <xref ref-type="sec" rid="sec015">Methods</xref> for details). <inline-formula id="pcbi.1006711.e028"><alternatives><graphic id="pcbi.1006711.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>3</mml:mn></mml:msub></mml:math></alternatives></inline-formula> is not a trained layer and hence is not subject to this analysis. <xref ref-type="fig" rid="pcbi.1006711.g002">Fig 2A and 2B</xref> depict the MTF derived from both <inline-formula id="pcbi.1006711.e029"><alternatives><graphic id="pcbi.1006711.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006711.e030"><alternatives><graphic id="pcbi.1006711.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>. The functions highlight that both layers exhibit a general low-pass behavior both along temporal and spectral modulations. As expected, layer <inline-formula id="pcbi.1006711.e031"><alternatives><graphic id="pcbi.1006711.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> is trained over shorter time-scales and does exhibit faster temporal dynamics along the rate axis, while the contextual layer <inline-formula id="pcbi.1006711.e032"><alternatives><graphic id="pcbi.1006711.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> is mostly tuned to slower dynamics &lt; 30<italic>Hz</italic> with a slightly tighter spectral selectivity mostly concentrated below 1 cycles/oct. This outcome is very reminiscent of similar transfer functions obtained from neurophysiological data showing contrasting tuning characterizations in the midbrain, auditory thalamus and auditory cortex [<xref ref-type="bibr" rid="pcbi.1006711.ref081">81</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref083">83</xref>], whereby selectivity of individual neurons along the mammalian auditory hierarchy evolves from faster to slower temporal dynamics and from more refined to broader spectral spans along frequency.</p>
<p>We further examine the selectivity of <italic>individual</italic> neurons and compare emergent tuning characteristics common across nodes in the network by employing an agglomerative clustering algorithm (see <xref ref-type="sec" rid="sec015">Methods</xref> for details). This approach clusters nodes exhibiting similar tuning profiles into common groups hence providing insight into the underlying acoustic cues being processed by each cluster. <xref ref-type="fig" rid="pcbi.1006711.g002">Fig 2C and 2D</xref> show contour plots from the resulting clusters overlaid on the MTF profiles for layers <inline-formula id="pcbi.1006711.e033"><alternatives><graphic id="pcbi.1006711.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006711.e034"><alternatives><graphic id="pcbi.1006711.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>. The array of clusters indicates that neurons in each of these layers do indeed exhibit a wide variety of selectivity to spectral and temporal dynamics in the input signal. We specifically note a cluster of <inline-formula id="pcbi.1006711.e035"><alternatives><graphic id="pcbi.1006711.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> neurons that is more sensitive to fast transients or ‘onsets’. This group is labeled ‘O’ in <xref ref-type="fig" rid="pcbi.1006711.g002">Fig 2C</xref>. An example time-frequency profile <inline-formula id="pcbi.1006711.e036"><alternatives><graphic id="pcbi.1006711.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:mrow><mml:mi mathvariant="script">F</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> of a neuron in the ‘O’ cluster is shown in <xref ref-type="fig" rid="pcbi.1006711.g002">Fig 2F</xref> (upper-right). We also note a spectrally-structured cluster (labeled ‘H’) centered around spectral modulations ∈ [1-2] cyc/oct corresponding to harmonic peaks present in natural sounds. An example neuron from this cluster is shown in <xref ref-type="fig" rid="pcbi.1006711.g002">Fig 2F</xref> (upper-left) and highlights the selectivity to specific frequency bands in the input spectrogram. The clustering procedure also reveals the presence of oriented spectro-temporally selective clusters, likely tuned to detect frequency-modulated sweeps in the signal over different spectrotemporal scales; as well as other clusters with special selectivity to spectral or temporal features. <xref ref-type="fig" rid="pcbi.1006711.g002">Fig 2F</xref> (lower panels) shows an example of two <inline-formula id="pcbi.1006711.e037"><alternatives><graphic id="pcbi.1006711.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> neurons with different temporal dynamics contrasting a slow neuron ‘S’ and a fast neuron ‘F’.</p>
</sec>
<sec id="sec005">
<title>Stream segregation experiments</title>
<p>We test the model’s behavior with a variety of acoustic scenes ranging from classic streaming paradigms using simple tones to experiments using speech signals. Crucially, all experiments are tested on the <italic>same model</italic> (after all layers have been trained), without any adjustment to model parameters. The stimulus parameters are carefully chosen to closely replicate previously published human perceptual experiments hence allowing a direct comparison between the model and human perception. All stream segregation results are shown in <xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3</xref> organized in 3 columns: the stimulus on the left, a replica of human perception of the same stimulus reproduced from the corresponding publication in the center, and the model performance on the right.</p>
<fig id="pcbi.1006711.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006711.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Primary results of stream segregation using proposed model.</title>
<p>Leftmost panel shows the stimuli sequence used for each experiment. Middle panel shows human listening performance and rightmost panel shows the model performance. Row (A) replicates experiments from [<xref ref-type="bibr" rid="pcbi.1006711.ref086">86</xref>], row (B) replicates experiments from [<xref ref-type="bibr" rid="pcbi.1006711.ref090">90</xref>], row (C) replicates experiments from [<xref ref-type="bibr" rid="pcbi.1006711.ref092">92</xref>], rows (D) and (E) replicate experiments from [<xref ref-type="bibr" rid="pcbi.1006711.ref093">93</xref>], and rows (F) and (G) replicate experiments from [<xref ref-type="bibr" rid="pcbi.1006711.ref095">95</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref096">96</xref>].</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006711.g003" xlink:type="simple"/>
</fig>
<sec id="sec006">
<title>Simple tones</title>
<p>The first experiment employs the classic two-tone paradigm with sequences of high and low notes, commonly used in streaming experiments [<xref ref-type="bibr" rid="pcbi.1006711.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref084">84</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref085">85</xref>]. The sequences are produced by presenting two tones of different frequencies, <italic>A</italic> and <italic>B</italic>, repeatedly and in alternation (<italic>ABAB</italic>−). When the frequency separation Δ<italic>F</italic> between the <italic>A</italic> and <italic>B</italic> tones is relatively small (&lt;10%), listeners perceive the sequence as grouped or fused and report hearing one stream. As the frequency separation Δ<italic>F</italic> increases, listeners hear two separate streams consisting of only low notes (<italic>A</italic> − <italic>A</italic> −) or only high notes (−<italic>B</italic> − <italic>B</italic>-). In contrast, when the two <italic>A</italic> and <italic>B</italic> notes are presented synchronously (<xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3A</xref>-left), listeners tend to hear the sequence as grouped regardless of the frequency separation Δ<italic>F</italic>, in a process reminiscent of temporal coherence which fuses together channels that are co-activated together [<xref ref-type="bibr" rid="pcbi.1006711.ref064">64</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref072">72</xref>]. <xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3A</xref>-middle replicates results from a study by Micheyl <italic>et al</italic>. [<xref ref-type="bibr" rid="pcbi.1006711.ref086">86</xref>]. The study shows that an alternating tone sequence is perceived as a single stream when the frequency separation Δ<italic>F</italic> is small and is segregated into two streams when Δ<italic>F</italic> is large. When the two tones are presented synchronously, they are always perceived as grouped regardless of frequency separation. The fused percept is objectively measured using d’ [<xref ref-type="bibr" rid="pcbi.1006711.ref087">87</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref088">88</xref>]; where listeners are asked to detect a change in one of the tones presented in the final burst (see <xref ref-type="sec" rid="sec015">Methods</xref> for details). <xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3A</xref>-right shows that the model replicates the same behavior using the same tone sequences presented in alternation or synchrony. As the frequency separation Δ<italic>F</italic> increases between the <italic>A</italic> and <italic>B</italic> tones, the model is more likely to perceive them as segregated in the alternating condition but tends to fuse them in the synchronous condition.</p>
<p>The two-tone paradigm is also often used to probe the phenomenon of buildup of streaming [<xref ref-type="bibr" rid="pcbi.1006711.ref008">8</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref089">89</xref>]. The buildup highlights that streaming is a dynamic process, whereby the segregation of the two notes into separate streams is not instantaneous; but builds-up over time taking up to several seconds to emerge. In a study by Micheyl et al. [<xref ref-type="bibr" rid="pcbi.1006711.ref090">90</xref>], buildup was assessed using a variation of the two-tone paradigm using tone triplets (<italic>ABA</italic> − <italic>ABA</italic>), as shown in <xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3B</xref>-left. <xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3B</xref>-middle replicates results from this study [<xref ref-type="bibr" rid="pcbi.1006711.ref090">90</xref>] whereby listeners <italic>continuously</italic> report perception of one or two streams for different frequency separations Δ<italic>F</italic>. The behavioral data shows that when the frequency separation Δ<italic>F</italic> is large, both <italic>A</italic> and <italic>B</italic> tones are perceived as segregated streams relatively quickly. As Δ<italic>F</italic> decreases, the segregated percept takes longer to emerge lasting over many seconds. <xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3B</xref>-right replicates the same behavior using the model and shows that the sequences gradually segregate into separate streams with different time constants. The model faithfully replicates human performance; demonstrating a faster buildup at large Δ<italic>F</italic>, slower buildup at intermediate Δ<italic>F</italic>, and no buildup at very small Δ<italic>F</italic>.</p>
</sec>
<sec id="sec007">
<title>Complex tones</title>
<p>Next, we explore stream segregation using complex tones. These complexes highlight the wide range of acoustic cues that aid in the segregation of auditory scenes; including frequency separation (as shown earlier), as well as amplitude modulations (AM), harmonicity, temporal synchrony, etc. [<xref ref-type="bibr" rid="pcbi.1006711.ref003">3</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref058">58</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref091">91</xref>]. In this simulation, we focus on the role of modulation cues in stream segregation by replicating a classic study by Grimault et al. [<xref ref-type="bibr" rid="pcbi.1006711.ref092">92</xref>] where alternating noise bursts with different AM rates are presented (<xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3C</xref>-left). As the difference in modulation rate Δ<italic>AM</italic> increases, noise bursts tend to segregate into two streams with distinct AM rates. Once the rate difference Δ<italic>AM</italic> reaches about 2 octaves, the modulated noises fully segregate into two distinct streams. <xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3C</xref>-middle shows human perception of segregated streams as a function of Δ<italic>AM</italic> replicating the results from the study by [<xref ref-type="bibr" rid="pcbi.1006711.ref092">92</xref>]; while <xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3C</xref>-right shows the performance of the model on the same stimuli. As shown in the Figure, the model closely replicates human perception as reflected by increase in probability of stream segregation. The model appears to leverage the explicit encoding of amplitude information in its trained layers to facilitate the segregation of noise sequences into corresponding streams.</p>
<p>Next, we examine the role of harmonicity and temporal synchrony as putative grouping cues. Both these cues are believed to exert strong grouping, acting as a bond that fuses sound elements together as shown in a study by Micheyl et al. [<xref ref-type="bibr" rid="pcbi.1006711.ref093">93</xref>]. In this work, a target tone at frequency 1000 Hz is masked by background tones that are either harmonically related or in temporal synchrony with the the target tone. The study examines two kinds of stimuli: ‘MBS’ -multiple burst same- stimuli (<xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3D</xref>-left) have the same burst of tones presented every time; while ‘MBD’ -multiple burst different- stimuli (<xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3E</xref>-left) vary the harmonicity relationship between target and background tones at every burst based on different fundamental frequencies (see <xref ref-type="sec" rid="sec015">Methods</xref> for more details about the stimuli). <xref ref-type="fig" rid="pcbi.1006711.g003">Fig [3D] and [3E]</xref>-middle replicate the results from the study by Micheyl et al. [<xref ref-type="bibr" rid="pcbi.1006711.ref093">93</xref>] in which listeners detect a change in the final burst of the target tone. The study shows that when target and background tones are either harmonically related or in temporal synchrony with each other, d’ is low indicating a strong background-target fusion. Listeners’ ability to segregate the target improves when either harmonicity or sychrony is perturbed. <xref ref-type="fig" rid="pcbi.1006711.g003">Fig [3D] and [3E]</xref>-right show the model performance on the same MBS and MBD stimuli respectively. When target and background tones are harmonically-related or in synchrony, the model favors fusion and results in a small d’. In contrast, when perturbing harmonicity by shifting the harmonics, the model favors a segregated interpretation resulting in increased d’. Similarly, when target and background tones are asynchronous, there is a significant increase in d’, again suggesting strong segregation.</p>
</sec>
<sec id="sec008">
<title>Speech intelligibility</title>
<p>Next, we examine the model’s behavior using complex sounds such as speech in presence of competing noise. In all experiments, a speech utterance is presented to the network either in clean or masked by background noise that includes speech modulated noise, babble noise, cafe noise or an interfering speech utterance. All speech utterances are part of the CRM corpus where each utterance consists of a call sign and a color–number combination, all embedded in a carrier phrase [<xref ref-type="bibr" rid="pcbi.1006711.ref094">94</xref>]. A typical sentence would be “Ready baron, go to red four now,” where ‘baron’ is the call sign, and ‘red’-‘four’ is the color–number combination. <xref ref-type="fig" rid="pcbi.1006711.g003">Fig [3F] and [3G]</xref>-left show spectrograms of speech utterances from the CRM corpus mixed with speech modulated noise and an interfering speech utterance respectively.</p>
<p><xref ref-type="fig" rid="pcbi.1006711.g003">Fig [3F] and [3G]</xref>-middle replicate the results from two behavioral studies using the CRM corpus in a dichotic listening paradigm where subjects identified the “number” and “color” mentioned in the target utterance under different noise conditions [<xref ref-type="bibr" rid="pcbi.1006711.ref095">95</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref096">96</xref>]. The behavioral data yield a measure of speech intelligibility (in word percent correct) as a function of signal to noise ratio (SNR) with different noise maskers. <xref ref-type="fig" rid="pcbi.1006711.g003">Fig [3F] and [3G]</xref>-right depict the model’s performance replicating the same paradigm as closely as possible (see <xref ref-type="sec" rid="sec015">Methods</xref> for details). The model yields a correct identification of speech tokens (numbers, colors, or both) that is closely related to the SNR condition following an S-shaped curve typical of similar measures of speech intelligibility in noise. The model performance plateaus at about 98% correct identification at SNRs above 3dB (<xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3F</xref>-right); whereas it degrades quite rapidly from -3 to -9 dB before reaching chance performance at -18 dB. When comparing effects of noise type, both human and model performance are poorer in presence of an interfering utterance, relative to babble and cafe noise conditions.</p>
</sec>
</sec>
<sec id="sec009">
<title>Model function and malfunction</title>
<p>As outlined earlier, <xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3</xref> contrasts the model’s performance against reported human perceptual results in a range of stream segregation experiments. Next, we reexamine our initial hypotheses; namely that the model is able to infer simultaneous and sequential grouping cues by learning statistical regularities in natural soundscapes. The experimental results shown in the previous section suggest that simultaneous cues (tonotopic organization, AM rate, harmonicity, temporal synchrony, etc), sequential cues and grouping mechanisms play an important role in streaming paradigms. In order to shed light on their individual contributions, we run a series of <italic>control</italic> experiments where we look at malfunctions in the model if certain components of the system are disrupted individually.</p>
<sec id="sec010">
<title>Role of simultaneous cues</title>
<p>The tuning characteristics of layer <inline-formula id="pcbi.1006711.e038"><alternatives><graphic id="pcbi.1006711.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> show that model neurons naturally cluster around specific modulation regions, hence, revealing a wide selectivity to different acoustic cues that emerge in natural sounds. Here, we focus on four <inline-formula id="pcbi.1006711.e039"><alternatives><graphic id="pcbi.1006711.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> neuron clusters with particular selectivity to harmonicity, onsets, fast and slow temporal modulations. We individually ‘turn off’ each of these clusters from the system and replicate all stream segregation experiments shown earlier. <xref ref-type="fig" rid="pcbi.1006711.g004">Fig 4</xref> shows the model performance as follows: The leftmost column shows the model performance when <inline-formula id="pcbi.1006711.e040"><alternatives><graphic id="pcbi.1006711.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> harmonicity-neurons are turned off, the middle column with <inline-formula id="pcbi.1006711.e041"><alternatives><graphic id="pcbi.1006711.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> onset neurons turned off, and the rightmost column with fast and slow <inline-formula id="pcbi.1006711.e042"><alternatives><graphic id="pcbi.1006711.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> units turned off respectively. In these experiments, <inline-formula id="pcbi.1006711.e043"><alternatives><graphic id="pcbi.1006711.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> is not altered but is retrained based on a modified input (i.e. its input dimensionality is reduced because harmonicity, onset, slow or fast channels are removed).</p>
<fig id="pcbi.1006711.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006711.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Control experiments introducing malfunction in layer <inline-formula id="pcbi.1006711.e044"><alternatives><graphic id="pcbi.1006711.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e044" xlink:type="simple"/><mml:math display="inline" id="M44"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>.</title>
<p>The layout of the figure in each column is similar to that of <xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3</xref> showing model response to different stimuli. The leftmost column remove <inline-formula id="pcbi.1006711.e045"><alternatives><graphic id="pcbi.1006711.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>-harmonic neurons, middle column removes <inline-formula id="pcbi.1006711.e046"><alternatives><graphic id="pcbi.1006711.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>-onset neurons and rightmost column contrasts only fast or slow <inline-formula id="pcbi.1006711.e047"><alternatives><graphic id="pcbi.1006711.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e047" xlink:type="simple"/><mml:math display="inline" id="M47"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> neurons.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006711.g004" xlink:type="simple"/>
</fig>
<p>Switching off harmonicity-<inline-formula id="pcbi.1006711.e048"><alternatives><graphic id="pcbi.1006711.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> nodes has no effect on the system’s performance in a two tone paradigm (<xref ref-type="fig" rid="pcbi.1006711.g004">Fig 4A</xref>-left) or sinusoidally amplitude-modulated noise bursts (<xref ref-type="fig" rid="pcbi.1006711.g004">Fig 4B</xref>-left). In contrast, the ability to segregate MBS and MBD sequences in case of mistuned harmonics is drastically affected by the absence of harmonicity-tuned nodes in the network (<xref ref-type="fig" rid="pcbi.1006711.g004">Fig 4C and 4D</xref>-left). Similarly, the network’s ability to detect speech (both colors and numbers in the CRM corpus) is severely impacted in absence of harmonicity-tuned nodes (<xref ref-type="fig" rid="pcbi.1006711.g004">Fig 4E</xref>-left). Taking a closer look at the behavior of the network in detecting numbers, we note a systematic drop in performance across all digits which all contain prominent voiced phonemes (<xref ref-type="fig" rid="pcbi.1006711.g004">Fig 4F</xref>-left).</p>
<p>Disabling <inline-formula id="pcbi.1006711.e049"><alternatives><graphic id="pcbi.1006711.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>-onset nodes results in its own malfunction of the model. Streaming two-tone sequences and sinusoidally amplitude-modulated noise bursts is not affected by switching off onset units (<xref ref-type="fig" rid="pcbi.1006711.g004">Fig 4A and 4B</xref>-middle). However, the MBD and MBS stimuli appear to be affected in an interesting way (<xref ref-type="fig" rid="pcbi.1006711.g004">Fig 4c and 4D</xref>-middle) where we note an improvement of segregation in case of mistuned harmonics. The design of these stimuli puts temporal synchrony and harmonicity in conflict. Free of onset-detectors, the model is able to judge segregation mostly driven by harmonicity or lack thereof in the case of mistuning. Conversely, in case of temporal asynchrony, there is a drop in segregation performance in absence of onset-detectors, though the model is able to exploit the harmonic relationship between target and background tones to induce streaming. A comparable drop in speech intelligibility performance is also noted (<xref ref-type="fig" rid="pcbi.1006711.g004">Fig 4E</xref>-middle), attesting to the important role of onsets in speech perception. Taking a closer look at the model performance with individual digits (<xref ref-type="fig" rid="pcbi.1006711.g004">Fig 4F</xref>-middle), we note severe drops for tokens like “three”, “six” and “seven” that contain prominent fricative and plosive unvoiced phonemes.</p>
<p>Selectivity to temporal dynamics plays a complementary role in the model’s ability to perform stream segregation. We manipulate the selectivity of <inline-formula id="pcbi.1006711.e050"><alternatives><graphic id="pcbi.1006711.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e050" xlink:type="simple"/><mml:math display="inline" id="M50"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> neurons to different range of amplitude modulations by testing only-slow (&lt; 25 Hz) or only-fast neurons (&gt; 25 Hz). The segregation of two-tone sequences appears to be unaffected by presence or absence of slow or fast units alone, and is likely mostly driven by the tonotopic organization of the nodes in the network (<xref ref-type="fig" rid="pcbi.1006711.g004">Fig 4A</xref>-right). In contrast, streaming of sinusoidally-modulated noise bursts is heavily affected when <inline-formula id="pcbi.1006711.e051"><alternatives><graphic id="pcbi.1006711.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e051" xlink:type="simple"/><mml:math display="inline" id="M51"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> units tuned to faster modulations are turned off, though only mild changes are noted when slower-units are turned off (<xref ref-type="fig" rid="pcbi.1006711.g004">Fig 4B</xref>-right). Streaming of MBD and MBS sequences appears unaffected by the time-constants of temporal modulations left in the <inline-formula id="pcbi.1006711.e052"><alternatives><graphic id="pcbi.1006711.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> layer; and we observe no changes to the model behavior (<xref ref-type="fig" rid="pcbi.1006711.g004">Fig 4C and 4D</xref>-right). Interestingly, speech intelligibility is also unaffected when faster <inline-formula id="pcbi.1006711.e053"><alternatives><graphic id="pcbi.1006711.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e053" xlink:type="simple"/><mml:math display="inline" id="M53"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> units are turned off (<xref ref-type="fig" rid="pcbi.1006711.g004">Fig 4E</xref>-right). In contrast, switching off slower units drastically affects the model’s ability to separate speech from noise, especially at low SNRs, strongly corroborating the role of midrange-modulations in speech perception [<xref ref-type="bibr" rid="pcbi.1006711.ref076">76</xref>].</p>
</sec>
<sec id="sec011">
<title>Role of sequential temporal dynamics</title>
<p>Next, we examine the impact of model parameters responsible for temporal integration on stream segregation over longer time scales. First, we observe the model’s behavior if we switch off neural adaptation at the output of <inline-formula id="pcbi.1006711.e054"><alternatives><graphic id="pcbi.1006711.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e054" xlink:type="simple"/><mml:math display="inline" id="M54"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> nodes. This mechanism aims to adjust the dynamics of neurons’ responses by eliminating nodes with moderate activation over time. <xref ref-type="fig" rid="pcbi.1006711.g005">Fig 5</xref>-left contrasts the model’s performance with and without this neural adaptation. <xref ref-type="fig" rid="pcbi.1006711.g005">Fig 5A</xref>-left shows that neural adaptation is important for segregating alternating two-tone sequences. Adaptation appears to aid the temporal coherence layer in ‘shutting down’ neurons from competing streams which facilitates segregation. In its absence, both tones in the stimulus continue to compete at the output of the model hence affecting the ability to segregate. Furthermore, this continued competition appears to slow-down the buildup process (<xref ref-type="fig" rid="pcbi.1006711.g005">Fig 5B</xref>-left compared to the original model behavior in <xref ref-type="fig" rid="pcbi.1006711.g005">Fig 5B</xref>-right). As noted in the figure, a tone sequence with frequency separation of Δ<italic>F</italic> = 9 semitones takes many seconds to eventually reach a segregated percept with modified model as compared to 1-2 secs in the original model, owing to the continued competition between the two tones. While the temporal coherence model is able to note the out-phase relationship between the streams, this process is assisted by neural adaptation which supresses activity from competing streams hence speeding up stream segregation in line with observed behavioral responses (<xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3B</xref>-middle). A similar behavior is observed in case of sinusoidally amplitude-modulated noise bursts in <xref ref-type="fig" rid="pcbi.1006711.g005">Fig 5C</xref>-left. Here again, removing adaptation from the network allows competition across channels to linger longer hence hampering the role of temporal coherence in detecting consistent incoherent activity across competing streams. In the case of MBD and MBS sequences, adaptation appears to have a mild effect with the exception of mistuned harmonics in the case of MBD sequences and temporal asynchrony for MBS sequences (<xref ref-type="fig" rid="pcbi.1006711.g005">Fig 5D and 5E</xref>-left).</p>
<fig id="pcbi.1006711.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1006711.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Control experiments introducing malfunction in temporal dynamics of the network.</title>
<p>The layout of the figure in each column is similar to that of <xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3</xref> showing model response to different stimuli. The leftmost column remove neural adaptation, middle column removes <inline-formula id="pcbi.1006711.e055"><alternatives><graphic id="pcbi.1006711.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e055" xlink:type="simple"/><mml:math display="inline" id="M55"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>-slow neurons and rightmost column removes the <inline-formula id="pcbi.1006711.e056"><alternatives><graphic id="pcbi.1006711.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e056" xlink:type="simple"/><mml:math display="inline" id="M56"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>3</mml:mn></mml:msub></mml:math></alternatives></inline-formula> temporal coherence.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1006711.g005" xlink:type="simple"/>
</fig>
<p>We next explore the role of temporal dynamics in cue extraction, particularly the role of slower time-constants which are thought to play a crucial role in sequential integration of acoustic cues as the scene evolves. We probe this role in a control experiment by switching off the <inline-formula id="pcbi.1006711.e057"><alternatives><graphic id="pcbi.1006711.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e057" xlink:type="simple"/><mml:math display="inline" id="M57"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> units with strong selectivity to very slow modulation rates (&lt; 15 Hz) and compare this modified network against the full architecture. The results comparing the two models are shown in <xref ref-type="fig" rid="pcbi.1006711.g005">Fig 5</xref>-middle and reveal wide spread after-effects across all streaming experiments. In the case of the two-tone paradigm, removing slower neurons from <inline-formula id="pcbi.1006711.e058"><alternatives><graphic id="pcbi.1006711.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e058" xlink:type="simple"/><mml:math display="inline" id="M58"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> significantly impairs the network’s ability to segregate 2 streams as Δ<italic>F</italic> increases (<xref ref-type="fig" rid="pcbi.1006711.g005">Fig 5A</xref>-middle). Also of note is that the streaming buildup is severely affected and quickly settles on final assessment of segregation between streams regardless of Δ<italic>F</italic> value likely reflecting the inherent spectral-based separation across the neurons in the network but failing to track how activity across the neural population evolves over time (<xref ref-type="fig" rid="pcbi.1006711.g005">Fig 5B</xref>-middle). Segregation of modulated noise bursts is also severely affected (<xref ref-type="fig" rid="pcbi.1006711.g005">Fig 5C</xref>-middle). The probability of perceiving 2 streams drops dramatically, indicating a poor integration of neural activity across differentiated neurons. The same effect is observed in the case of MBS and MBD sequences, where the network fails to segregate the target tone from background masker tones even in presence of mistuned harmonic relationships (<xref ref-type="fig" rid="pcbi.1006711.g005">Fig 5D and 5E</xref>-middle). This drop is also noted for both stimuli in the case of asynchrony, even though the drop is not as dramatic, suggesting the network still relied on some degree of temporal alignment across the fast neurons remaining in the <inline-formula id="pcbi.1006711.e059"><alternatives><graphic id="pcbi.1006711.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e059" xlink:type="simple"/><mml:math display="inline" id="M59"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> network to judge relationship between tone bursts. Finally, in the case of speech in noise experiments, the network containing ‘faster’ neurons is severely impaired across all SNR values (<xref ref-type="fig" rid="pcbi.1006711.g005">Fig 5F</xref>-middle). The drop in performance is clear across all digits (<xref ref-type="fig" rid="pcbi.1006711.g005">Fig 5G</xref>-middle). The absence of slow <inline-formula id="pcbi.1006711.e060"><alternatives><graphic id="pcbi.1006711.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e060" xlink:type="simple"/><mml:math display="inline" id="M60"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> units clearly affects the network’s ability to match the slow changes in temporal structure of speech tokens even in presence of simultaneous cues hence failing to facilitate stream segregation. This result reinforces the joint role of both spectral and temporal (local and global) attributes in speech encoding and comprehension [<xref ref-type="bibr" rid="pcbi.1006711.ref076">76</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref097">97</xref>].</p>
<p>Finally, the role of temporal fusion across channels is examined by testing the model’s performance without the temporal coherence mechanism in layer <inline-formula id="pcbi.1006711.e061"><alternatives><graphic id="pcbi.1006711.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e061" xlink:type="simple"/><mml:math display="inline" id="M61"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>3</mml:mn></mml:msub></mml:math></alternatives></inline-formula>. Much like earlier control experiments, removing temporal coherence has sweeping effects on the model’s ability to perform stream segregation. In the two-tone paradigm, the model treats the synchronous and alternating notes similarly as it fails to judge the phase relationship across spectral channels (<xref ref-type="fig" rid="pcbi.1006711.g005">Fig 5A</xref>-right). The buildup of streaming is also completely annihilated regardless of frequency separation across channels strongly suggesting that integration over time and across frequency channels plays an important role in the brain’s ability to consolidate information spectrally and temporally while it examines possible configurations or interpretations of the scene (<xref ref-type="fig" rid="pcbi.1006711.g005">Fig 5B</xref>-right). This process is very much what the temporal coherence stage contributes and is clearly impaired without coherence. Segregation of modulated noise bursts is also affected although the probability of segregation does increase with increased AM rate difference Δ<italic>AM</italic> albeit with reduced probability suggesting poorer segregation performance of the modified network (<xref ref-type="fig" rid="pcbi.1006711.g005">Fig 5C</xref>-right). In the case of noise complexes in the MBD and MBS paradigm, the network completely fails to achieve any form of segregation (<xref ref-type="fig" rid="pcbi.1006711.g005">Fig 5D and 5E</xref>-right) suggesting that the presence of simultaneous cues (e.g. harmonicity) is not sufficient. Complex noise patterns tend to activate a wide range of channels which require an integration mechanism such as <inline-formula id="pcbi.1006711.e062"><alternatives><graphic id="pcbi.1006711.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e062" xlink:type="simple"/><mml:math display="inline" id="M62"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>3</mml:mn></mml:msub></mml:math></alternatives></inline-formula> temporal coherence to interpret the across-channel consistency and phase relationships. Speech segregation is slightly affected by disabling temporal coherence (<xref ref-type="fig" rid="pcbi.1006711.g005">Fig 5F</xref>-right) and more noticeably at lower SNR values for both colors and numbers. <xref ref-type="fig" rid="pcbi.1006711.g005">Fig 5G</xref>-right) highlights these mild reductions in segregation that are observed consistently across all digits.</p>
</sec>
</sec>
</sec>
<sec id="sec012" sec-type="conclusions">
<title>Discussion</title>
<p>The current study presents a biologically-plausible model of stream segregation that leverages the multiplexed and non-linear representation of sounds along an auditory hierarchy. While the model is formulated to focus on local and global cues in everyday sounds, it is structured to ‘learn’ these cues directly from the data. The unsupervised nature of the architecture yields physiologically and perceptually meaningful tuning of model neurons that support the organization of sounds into distinct auditory objects. The three key components of the architecture as shown in <xref ref-type="fig" rid="pcbi.1006711.g001">Fig 1</xref> are: (1) A stochastic network <italic>RBM layer</italic> that encodes two-dimensional input spectrogram into <italic>localized</italic> specto-temporal bases based on short term feature analysis; (2) A dynamic <italic>cRBM layer</italic> that captures the long-term temporal dependencies across spectro-temporal bases characterizing the transformation of sound from fast changing details to slower dynamics. (3) A <italic>temporal coherence layer</italic> that mimics the hebbian process of binding local and global details together to mediate the mapping from feature space to formation of auditory objects.</p>
<p>The layout of the model closely replicates the physiological layout of auditory processing in the brain where an acoustic signal undergoes a series of transformations from the cochlea all the way to auditory cortex (A1), effectively extracting a rich feature representation that forms the basis for perceptual grouping of sound objects [<xref ref-type="bibr" rid="pcbi.1006711.ref098">98</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref103">103</xref>]. The sound transformations in the biological system evolve in temporal and spectral resolutions from temporally fast, spectrally refined as is typically observed in the periphery and levels of the midbrain to markedly slower and spectrally broader in cortical networks [<xref ref-type="bibr" rid="pcbi.1006711.ref081">81</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref104">104</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref105">105</xref>]. The current model ‘learns’ similar structures as can be seen from the modulation transfer functions for both layers <inline-formula id="pcbi.1006711.e063"><alternatives><graphic id="pcbi.1006711.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e063" xlink:type="simple"/><mml:math display="inline" id="M63"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006711.e064"><alternatives><graphic id="pcbi.1006711.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e064" xlink:type="simple"/><mml:math display="inline" id="M64"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> (<xref ref-type="fig" rid="pcbi.1006711.g002">Fig 2</xref>). The fact that the model evolves in temporal resolution from short to longer analyses is not surprising as it is one of the structural designs of the system. However, the detailed analyses learned in each layer are intriguing and suggest a close connection between neural selectivity along the auditory pathway and the progression of processes underlying Gestalt principles from a local analysis of simultaneous cues to global sequential cues [<xref ref-type="bibr" rid="pcbi.1006711.ref008">8</xref>]. This connection has in fact been postulated in a number of studies of auditory neurophysiology, particularly contrasting the differences in tuning characteristics between individual neurons in the midbrain (particularly the inferior colliculus) and cortex [<xref ref-type="bibr" rid="pcbi.1006711.ref081">81</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref106">106</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref107">107</xref>]. The current model does appear to also exhibit a similar variety of tuning characteristics and it is tempting to interpret the modulation profiles emergent from layers <inline-formula id="pcbi.1006711.e065"><alternatives><graphic id="pcbi.1006711.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e065" xlink:type="simple"/><mml:math display="inline" id="M65"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006711.e066"><alternatives><graphic id="pcbi.1006711.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e066" xlink:type="simple"/><mml:math display="inline" id="M66"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> as potentially aligned with a midbrain/cortex hierarchy. However, we should also entertain the possibility that both layers <inline-formula id="pcbi.1006711.e067"><alternatives><graphic id="pcbi.1006711.e067g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e067" xlink:type="simple"/><mml:math display="inline" id="M67"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006711.e068"><alternatives><graphic id="pcbi.1006711.e068g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e068" xlink:type="simple"/><mml:math display="inline" id="M68"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> could map to different sub-populations in auditory cortex. Cortical substructures have been reported to exhibit a variety of heterogeneous behaviors and variability in encoding temporal details about an incoming sound by multiplexing temporal and rate representations [<xref ref-type="bibr" rid="pcbi.1006711.ref108">108</xref>]. Interpreting the model output based on such dichotomy in integration mechanisms raises an interesting possibility attributing statistical-constraints of Gestalt cues solely to cortical networks in the brain. This alternative merits further examination in future work especially considering more intricate network architectures that extend across more layers including extra hidden layers in a true tradition of deep learning [<xref ref-type="bibr" rid="pcbi.1006711.ref068">68</xref>]. Follow-up analyses should also examine the encoding of stimulus features across an even wider array of temporal resolutions that span the contribution of finer details including temporal fine structure to even longer multi-second time dynamics [<xref ref-type="bibr" rid="pcbi.1006711.ref109">109</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref110">110</xref>].</p>
<p><italic>Role of simultaneous layer</italic>. Extracting relevant information from incoming acoustic waves is the backbone of any processing and sound interpretation system. The model replicates this feature analysis in a data-driven fashion by employing a diverse dataset of natural sounds including human speech, animal vocalizations and street ambient sounds. Structuring the local layer using a RBM architecture allows the model to learn a rich tiling of spectro-temporal basis functions. The results indicate that these bases capture fine details in the acoustic stimulus, as suggested by the modulation transfer function (<xref ref-type="fig" rid="pcbi.1006711.g002">Fig 2</xref>). The tuning of individual model neurons is itself well-structured and localized in this spectro-temporal space with clear organization of subsets to a wide range of acoustic cues spanning frequency proximity, harmonicity, onset, and AM rate among others, as shown by the clustering analysis.</p>
<p>Traditionally, biomimetic computational models of stream segregation have attempted to replicate some or all of these cues to enable stream segregation. Often, this process is achieved by hand-selecting specific axes of feature analysis that best suit the auditory scenes of interest in these specific studies [<xref ref-type="bibr" rid="pcbi.1006711.ref111">111</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref113">113</xref>]. One of the drawbacks to feature selectivity in model design is confining the testable signals to those that take advantage of these specific features. By employing an unsupervised approach to feature selection, the current model not only replicates known simultaneous cues in auditory scene analysis, but also nonlinearly spans multitudes of features given the fully-connected nature of the Restricted Boltzman Machine (RBM) used in layer <inline-formula id="pcbi.1006711.e069"><alternatives><graphic id="pcbi.1006711.e069g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e069" xlink:type="simple"/><mml:math display="inline" id="M69"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>. Across-feature integration is in line with recent findings suggesting that many auditory neurons are driven by a multitude of stimulus features [<xref ref-type="bibr" rid="pcbi.1006711.ref114">114</xref>]. This feature integration is particularly crucial in case of complex sounds where a multitude of dimensions provide the perceptual system with converging evidence about the organization of the scene [<xref ref-type="bibr" rid="pcbi.1006711.ref115">115</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref116">116</xref>]. The complementary value of this cross-feature mapping is clearly visible in control experiments where dropping different components of the simultaneous layer have different effects on the model’s ability to perform stream segregation (<xref ref-type="fig" rid="pcbi.1006711.g004">Fig 4</xref>).</p>
<p><italic>Role of sequential layer</italic>. Along the same lines, the sequential layer provides an integrated non-linear mapping of the feature space from localized details to slowly evolving spectro-temporal patterns. The use of a cRBM layer allows the model to ‘learn’ tuning from natural sounds along slower time-constants. The transfer function analysis reveals a strong selectivity to slow temporal modulations present in natural sounds typically in the range ∼ 2–32 Hz as shown in <xref ref-type="fig" rid="pcbi.1006711.g002">Fig 2</xref>. This tuning is reminiscent of modulation transfer functions derived from the mammalian auditory cortex revealing neurons that are slightly broader spectrally and slower temporally [<xref ref-type="bibr" rid="pcbi.1006711.ref081">81</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref082">82</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref104">104</xref>]. This global analysis has not been extensively investigated in models of auditory scene analysis, though few models have leveraged cortical-like processing to complement local feature analysis [<xref ref-type="bibr" rid="pcbi.1006711.ref113">113</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref117">117</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref119">119</xref>]. Engineering approaches have also leveraged this global analysis especially in the case of speech processing systems. Approaches such as RASTA (relative spectra), high-pass and band-pass filtered modulation spectra take advantage of slow articulatory structures of speech production as well as the sensitivity of human perception to such slow dynamics to offer a more robust processing of speech sounds in presence of noise [<xref ref-type="bibr" rid="pcbi.1006711.ref120">120</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref122">122</xref>].</p>
<p><italic>Role of temporal coherence layer</italic>. While feature analysis is a crucial ingredient in auditory scene analysis, fusing the relevant cues together is an equally important complementary stage to group the features into meaningful objects. Perceptual and physiological data have strongly suggested that temporal coherence achieves the feature fusion needed for object formation [<xref ref-type="bibr" rid="pcbi.1006711.ref064">64</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref123">123</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref124">124</xref>]. While its exact neural underpinnings are not well understood yet, empirical evidence strongly suggest that it plays an important role in scene organization by the auditory system [<xref ref-type="bibr" rid="pcbi.1006711.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref065">65</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref125">125</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref126">126</xref>]. Indirect neurophysiological evidence suggest that coherence mechanisms operate beyond auditory cortex likely in a network engaging the intraparietal sulcus and superior temporal sulcus [<xref ref-type="bibr" rid="pcbi.1006711.ref065">65</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref126">126</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref128">128</xref>]. The current model employs a rather simple biologically-plausible Hebbian interaction across channels to rapidly adapt co-operative and competitive interactions between coherent and non-coherent responses [<xref ref-type="bibr" rid="pcbi.1006711.ref072">72</xref>]. Effectively, channels that exhibit a high degree of temporal correlation across feature dynamics are mutually strengthened while incoherent channels are gradually weakened hence facilitating segregation of target signals from background interference. Naturally, the Hebbian-based approach is not the only implementation for this fusion stage and numerous techniques for such fusion have been explored in areas of data mining and analytics. In fact, feature fusion has become an important topic of research in the deep learning literature particularly when applied to computer vision and sensor networks. Ultimately, the architecture used to implement such grouping stage will have to infer relationships between activities of model sub-components based on some pre-defined loss function (in case of unsupervised learning). In the current model, we reduced this learning function to the basic principle of temporal coherence [<xref ref-type="bibr" rid="pcbi.1006711.ref064">64</xref>].</p>
<sec id="sec013">
<title>Scene segregation and fusion</title>
<p>The analysis of control experiments quantifies the complementarity of rich feature representation and grouping mechanisms in driving scene segregation. The proposed architecture faithfully replicates human psychoacoustic behavior on steaming paradigms over wide range of stimuli ranging from simple tones to speech utterances as demonstrated in <xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3</xref>. In case of two tone streaming paradigm shown in (<xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3A</xref>), the network exhibits stream segregation when two alternating tones are widely separated across tonotopic frequency axis. This behavior is consistent with well established psychophysical and physiological findings of stream segregation induced by differences in tonotopic cues [<xref ref-type="bibr" rid="pcbi.1006711.ref129">129</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref132">132</xref>]; and relies heavily on the activation of different groups of neurons with distinct frequency selectivities as captured in <inline-formula id="pcbi.1006711.e070"><alternatives><graphic id="pcbi.1006711.e070g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e070" xlink:type="simple"/><mml:math display="inline" id="M70"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>. In absence of temporal correlation between these two groups, the temporal coherence layer aided by the adaptation mechanism suppresses the anti-correlated groups of units, hence inducing stream segregation in the final stage of the network. However when Δ<italic>F</italic> is small enough, there is high degree of overlap resulting in a single stream percept. This segregation/integration effect is strongly maintained regardless of a number of manipulations to the model architecture. The key components crucial to the organization of tone sequences are the presence of tonotopic or frequency selectivity combined with temporal integration that examines activity across neural channels at relatively longer time-scales. This observation is very much in line with the spatio-temporal view of auditory stream segregation which requires neural channels to be widely separated in addition to temporal asynchrony across these channels [<xref ref-type="bibr" rid="pcbi.1006711.ref133">133</xref>].</p>
<p>The interaction of spectral and temporal dynamics during the organization of tone sequences supports the view of stream segregation as a dynamic process. The buildup effect reported in the current model (<xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3B</xref>) is in line with established psychoacoustic behaviors [<xref ref-type="bibr" rid="pcbi.1006711.ref090">90</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref134">134</xref>–<xref ref-type="bibr" rid="pcbi.1006711.ref136">136</xref>] and suggests that segregation of two streams is not instantaneous; but strengthens over time and can lead to segregation when frequency difference (Δ<italic>F</italic>) is large enough. The current model highlights that this effect is in fact reflecting the competition across neural channels as viewed by the temporal coherence layer. The binding of correlated groups of neurons strengthens over time while suppressing the anti-correlated units over time in the same process. Interaction across multiple features is also noted in other simulations that pit against each other harmonicity, onsets and temporal dynamics (<xref ref-type="fig" rid="pcbi.1006711.g003">Fig 3[C], 3[D] and 3[E]</xref>). Simulations using complex tones directly examine the role of localized spectro-temporal tuning in <inline-formula id="pcbi.1006711.e071"><alternatives><graphic id="pcbi.1006711.e071g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e071" xlink:type="simple"/><mml:math display="inline" id="M71"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> as an encoding of simultaneous cues such as harmonicity, onset and fast amplitude modulations among others. Sequential cues emergent in <inline-formula id="pcbi.1006711.e072"><alternatives><graphic id="pcbi.1006711.e072g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e072" xlink:type="simple"/><mml:math display="inline" id="M72"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> are crucial in tracking the activity emerging in the localized layer over longer amplitude modulations; which are then fused together in the last <inline-formula id="pcbi.1006711.e073"><alternatives><graphic id="pcbi.1006711.e073g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e073" xlink:type="simple"/><mml:math display="inline" id="M73"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>3</mml:mn></mml:msub></mml:math></alternatives></inline-formula> layer.</p>
<p>Through this rich selectivity learned directly from natural sounds, the network offers a wide span of selectivity across the spectrotemporal space. This tuning proves effective in tackling complex auditory scenes composed of speech with various interferers. In line with human perceptual data, the model shows that speech tokens are harder to identify in presence of utterances from same corpus compared to babble and cafe noise as the signal-to-noise ratio gets smaller. The model highlights that this variable response is largely caused by the dominance of neural activity from the interfering set relative to the target. The distinct activation between target and interferer is further blurred in absence of of slow sequential cues which integrate information about the speech utterance beyond just that target number/color. As shown in the control experiments, a network that lacks slow sequential cues is further impaired in making a judgment about the identity of the target token, likely due a to an enhanced confusion between its representation and that of the interferer. Once this activity reaches the temporal coherence layer, the weakly responsive neurons get suppressed, hence resulting in the actual number/color token getting wrongly identified as the one in the interfering utterance.</p>
</sec>
<sec id="sec014">
<title>Concluding remarks</title>
<p>Overall, the proposed model highlights three key results: (i) Using the right configuration, we are able to infer a wide-range of Gestalt cues directly from natural sounds. The proposed RBM architecture offers a cooperative and nonlinear integration of these cues to result in a multiplexed representation of auditory scenes across various granularities in time and frequency. By using an unsupervised learning approach, the network is not being optimized for a specific application; rather, it is reflecting the inherent variety of local and global dynamics present in natural sounds. Possibly, an even deeper neural architecture extending beyond just a few layers could extend the rich feature analysis and fill in the spectrum from local to global hence adding a more refined mapping along with the nonlinear integration naturally offered by the RBM architecture. (ii) Grouping acoustic features is effectively an outlook across <italic>all</italic> active nodes that allows to piece together the pieces of each auditory object. This process effectively plays 2 key roles: a grouping role by putting together pieces of a sound object (effectively integrating together pitch, timbre, rhythm and possibly space information that reflect a common object); and an elimination role by suppressing channels that are irrelevant to the emergence of the foreground object, hence enhancing the signal-to-noise ratio in the network. Temporal coherence is one such fusion mechanism that has been garnering stronger neural and perceptual evidence [<xref ref-type="bibr" rid="pcbi.1006711.ref039">39</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref065">65</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref125">125</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref126">126</xref>]. The current work employs Hebbian learning, a biological simple mechanism that affords such fusion over the rightly chosen time-scales. (iii) Auditory scene segregation is a balancing act of the proper feature analysis along with mechanisms for fusion that give rise of auditory object representations. While both stages are necessary, neither one is sufficient. The proposed model offers a unified platform that integrates together these different mechanisms and strategies. It also bridges the existing physiological theories of scene organization with perceptual accounts of auditory scene analysis.</p>
</sec>
</sec>
<sec id="sec015" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec016">
<title>Network architecture</title>
<p>The proposed model is structured along 4 key stages: initial data pre-processing by transforming the acoustic signal to a time-frequency representation, a local analysis over short time-scales, a global analysis over an array of longer time-scales, then a fusion stage using temporal coherence. A final readout of the network activity is implemented to extract information from specific streaming experiments to probe segregation of individual streams in the input scene. Details of each component of the model are outlined next:</p>
<p>The acoustic signal is first analyzed through a model of peripheral processing in the mammalian auditory system, following the model by Yang et al. [<xref ref-type="bibr" rid="pcbi.1006711.ref066">66</xref>]. Briefly, it transforms the acoustic stimulus sampled at 8KHz into a joint time-frequency representation referred to as auditory spectrogram. The stage starts with a bank of 128 asymmetric constant-Q filters equally-spaced on a logarithmic axis over 5.3 octaves spanning the range 180 Hz to 4000 Hz (<italic>Q</italic><sub><italic>ERB</italic></sub> ≈ 4) [<xref ref-type="bibr" rid="pcbi.1006711.ref137">137</xref>]. By its very nature, the peripheral model uses a non-parametric set of cochlear filters that are fixed over a span of 5.3 octaves (see [<xref ref-type="bibr" rid="pcbi.1006711.ref066">66</xref>] for details). In the current model, we cap our sampling rate to 8KHz in order to provide ample coverage over lower frequency regions. After cochlear filtering, the outputs undergo spectral sharpening via first order derivative along frequency, followed by half-wave rectification then short term integration with <italic>e</italic><sup>−<italic>t</italic>/<italic>τ</italic></sup> where <italic>τ</italic> = 10 ms. This filterbank analysis results in a time-frequency auditory spectrogram represented by <italic>S</italic>(<italic>t</italic>, <italic>f</italic>). Three consecutive frames are then grouped together to form a one dimensional vector <italic>x</italic> such that <italic>x</italic> ∈ <italic>R</italic><sup><italic>n</italic></sup> and <italic>n</italic> = 384. This process is repeated for all the audio samples in the dataset to form a set of <italic>N</italic> sampled patches given by <italic>X</italic> = <italic>x</italic><sup>1</sup>, <italic>x</italic><sup>2</sup>, …, <italic>x</italic><sup><italic>N</italic></sup>. This set of time-frequency patches (<italic>X</italic>) constitutes the input to second component of the network.</p>
<sec id="sec017">
<title>Simultaneous layer</title>
<p>The simultaneous layer <inline-formula id="pcbi.1006711.e074"><alternatives><graphic id="pcbi.1006711.e074g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e074" xlink:type="simple"/><mml:math display="inline" id="M74"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> is structured as a Sparse Restricted Boltzmann machines (RBM), which is chosen to discover features from an unlabeled set in an unsupervised fashion [<xref ref-type="bibr" rid="pcbi.1006711.ref067">67</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref138">138</xref>]. Sparse RBMs are undirected graphical models with <italic>K</italic> binary hidden variables. The energy function of a RBM is defined as:
<disp-formula id="pcbi.1006711.e075"><alternatives><graphic id="pcbi.1006711.e075g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e075" xlink:type="simple"/><mml:math display="block" id="M75"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>h</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>k</mml:mi></mml:munder> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>−</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>−</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>l</mml:mi></mml:munder> <mml:msub><mml:mi>B</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:msub><mml:mi>h</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>−</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi>l</mml:mi></mml:mrow></mml:munder> <mml:msub><mml:mi>x</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:msub><mml:mi>h</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where, <italic>x</italic><sub><italic>k</italic></sub> and <italic>h</italic><sub><italic>l</italic></sub> denote the states of <italic>k</italic><sup><italic>th</italic></sup> visible unit and <italic>l</italic><sup><italic>th</italic></sup> hidden unit, while <italic>w</italic><sub><italic>kl</italic></sub> represents the strength of connection between them and <italic>A</italic> (and <italic>B</italic>) are the visible (and hidden) biases, respectively. The joint energy distribution of (<italic>x</italic>, <italic>h</italic>) is defined as:
<disp-formula id="pcbi.1006711.e076"><alternatives><graphic id="pcbi.1006711.e076g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e076" xlink:type="simple"/><mml:math display="block" id="M76"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>h</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>Z</mml:mi></mml:mfrac> <mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>{</mml:mo> <mml:mo>−</mml:mo> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>h</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula> <disp-formula id="pcbi.1006711.e077"><alternatives><graphic id="pcbi.1006711.e077g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e077" xlink:type="simple"/><mml:math display="block" id="M77"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>Z</mml:mi> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>h</mml:mi></mml:mrow></mml:munder> <mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>{</mml:mo> <mml:mo>−</mml:mo> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>h</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
where, <italic>Z</italic> is a normalizing partition function which is obtained by summing the energy function <italic>E</italic>(<italic>x</italic>, <italic>h</italic>) over all possible combinations of visible and hidden units.</p>
<p>Given observed data, the states of hidden units are conditionally independent. Their activation probabilities are,
<disp-formula id="pcbi.1006711.e078"><alternatives><graphic id="pcbi.1006711.e078g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e078" xlink:type="simple"/><mml:math display="block" id="M78"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>p</mml:mi> <mml:mo>{</mml:mo> <mml:mo>−</mml:mo> <mml:msup><mml:mi>x</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:msub><mml:mi>w</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>}</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
where <italic>w</italic><sub><italic>l</italic></sub> denotes the <italic>l</italic><sup><italic>th</italic></sup> column of <italic>W</italic> and represents connection weights between the <italic>l</italic><sup><italic>th</italic></sup> hidden unit and all visible units. We incorporate sparsity into the hidden layer representation to ensure that hidden activations are more selective to specific characteristics of the training data. A sparsity penalty is imposed on the activation of hidden units such that the probability of a hidden unit being active, denoted by <italic>q</italic> should be as close as possible to a specified “sparsity target”, given by <italic>p</italic>. The penalty term is chosen to be the cross entropy between the desired and actual distributions given by: <italic>p</italic> log <italic>q</italic> + (1 − <italic>p</italic>) log(1 − <italic>q</italic>) [<xref ref-type="bibr" rid="pcbi.1006711.ref068">68</xref>]. The measure imposes a ‘sparsity-cost’ that allows to adjust both the bias and weights of each hidden unit in the network.</p>
<p>Given an input signal, a hidden unit is said to be <italic>representing</italic> a particular data sample when it is activated. The objective of generative training of RBMs is to maximize the marginal distribution of visible units <italic>P</italic>(<italic>x</italic>) which is typically done using <italic>Contrastive Divergence</italic> (CD) [<xref ref-type="bibr" rid="pcbi.1006711.ref069">69</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref139">139</xref>]. This algorithm updates the feature of the <italic>k</italic>-th hidden unit seeing the training data <italic>x</italic><sub><italic>l</italic></sub> such that:
<disp-formula id="pcbi.1006711.e079"><alternatives><graphic id="pcbi.1006711.e079g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e079" xlink:type="simple"/><mml:math display="block" id="M79"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mo>−</mml:mo> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>h</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo> <mml:mo>−</mml:mo></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>l</mml:mi> <mml:mo>)</mml:mo> <mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
where <italic>x</italic><sub>(<italic>l</italic>)−</sub> is sampled from <italic>P</italic>(<italic>x</italic>|<italic>h</italic><sub><italic>l</italic></sub>). The algorithm learns the distribution of hidden activations <italic>h</italic><sub><italic>k</italic></sub> such that <italic>x</italic><sub>(<italic>l</italic>)−</sub> -when sampled from the hidden activations- come close to the real distribution of visible units <italic>x</italic>. As hidden activations <italic>h</italic><sub><italic>k</italic></sub> keep on learning the representation of visible units <italic>x</italic>, the update rule Δ<italic>w</italic><sub><italic>k</italic></sub> keeps decreasing. The learning process only stops when the reconstruction is close to perfect i.e. (<italic>x</italic><sub>(<italic>l</italic>)</sub> − <italic>x</italic><sub>(<italic>l</italic>)−</sub>) approaches 0.</p>
<p>The model used here employs 400 hidden units in the simultaneous layer <inline-formula id="pcbi.1006711.e080"><alternatives><graphic id="pcbi.1006711.e080g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e080" xlink:type="simple"/><mml:math display="inline" id="M80"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>. Once trained, the weights <italic>W</italic> yield unique spectro-temporal basis functions. We then transform the weights <italic>W</italic> into two-dimensional functions <inline-formula id="pcbi.1006711.e081"><alternatives><graphic id="pcbi.1006711.e081g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e081" xlink:type="simple"/><mml:math display="inline" id="M81"><mml:mrow><mml:mi mathvariant="script">F</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> where <italic>t</italic> denotes a patch of 30 ms and <italic>f</italic> corresponds to the frequency axis of auditory spectrogram. These 2D filters are then applied in a convolutional fashion onto the time-frequency patch <italic>S</italic>(<italic>t</italic>, <italic>f</italic>) to obtain the filter response <inline-formula id="pcbi.1006711.e082"><alternatives><graphic id="pcbi.1006711.e082g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e082" xlink:type="simple"/><mml:math display="inline" id="M82"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> given by:
<disp-formula id="pcbi.1006711.e083"><alternatives><graphic id="pcbi.1006711.e083g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e083" xlink:type="simple"/><mml:math display="block" id="M83"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>f</mml:mi></mml:munder> <mml:mo>∫</mml:mo> <mml:msub><mml:mi>S</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>τ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi mathvariant="script">F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>−</mml:mo> <mml:mi>τ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>d</mml:mi> <mml:mi>τ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
These responses <inline-formula id="pcbi.1006711.e084"><alternatives><graphic id="pcbi.1006711.e084g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e084" xlink:type="simple"/><mml:math display="inline" id="M84"><mml:mrow><mml:mo>{</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> then undergo an adaptation process that allows to strengthen the contrast between foreground and background units. This mechanism follows a classic closed-loop synaptic adaptation proposed by Tsodyks et al. [<xref ref-type="bibr" rid="pcbi.1006711.ref140">140</xref>] given by:
<disp-formula id="pcbi.1006711.e085"><alternatives><graphic id="pcbi.1006711.e085g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e085" xlink:type="simple"/><mml:math display="block" id="M85"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mfrac><mml:mrow><mml:mi>δ</mml:mi> <mml:mi>a</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn> <mml:mo>−</mml:mo> <mml:mi>a</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>τ</mml:mi> <mml:mi>a</mml:mi></mml:msub></mml:mfrac> <mml:mo>−</mml:mo> <mml:mi>α</mml:mi> <mml:mi>a</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula> <disp-formula id="pcbi.1006711.e086"><alternatives><graphic id="pcbi.1006711.e086g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e086" xlink:type="simple"/><mml:math display="block" id="M86"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold">r</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>a</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
with time constant <italic>τ</italic><sub><italic>a</italic></sub> = 300 ms and synaptic utilization parameter <italic>α</italic> = 1<italic>e</italic><sup>−5</sup>. This operation yields output responses {<bold>r</bold><sub><italic>k</italic></sub>(<italic>t</italic>)} that are then processed through the next layer in the hierarchy. A range of other adaptation parameters <italic>τ</italic><sub><italic>a</italic></sub> and <italic>α</italic> (around the chosen values) were explored with qualitatively similar results.</p>
</sec>
<sec id="sec018">
<title>Sequential layer</title>
<p>The next layer <inline-formula id="pcbi.1006711.e087"><alternatives><graphic id="pcbi.1006711.e087g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e087" xlink:type="simple"/><mml:math display="inline" id="M87"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> is structured as an array of conditional RBMs (cRBM) [<xref ref-type="bibr" rid="pcbi.1006711.ref071">71</xref>]. cRBMs are non-linear generative models for time series data that employ undirected models with visible units {<italic>x</italic><sub><italic>k</italic></sub>} connected to a layer of binary latent variables {<italic>h</italic><sub><italic>k</italic></sub>}. In the present model, the visible units {<italic>x</italic><sub><italic>k</italic></sub>} are represented by a Gaussian distribution fitted over <inline-formula id="pcbi.1006711.e088"><alternatives><graphic id="pcbi.1006711.e088g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e088" xlink:type="simple"/><mml:math display="inline" id="M88"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> responses. At each time step <italic>t</italic>, the model maintains a history of the last <italic>τ</italic> time steps and stores the visible variables corresponding to these time steps in a <italic>history</italic> vector referred to as <italic>x</italic><sub><italic>τ</italic></sub>. Each visible input {<italic>x</italic><sub><italic>k</italic></sub>} and hidden unit {<italic>h</italic><sub><italic>k</italic></sub>} at a particular time step <italic>t</italic> receives directed connections from the history vector <italic>x</italic><sub><italic>τ</italic></sub> so as to capture long term temporal dependencies across visible units. This dynamical model is defined by a joint distribution:
<disp-formula id="pcbi.1006711.e089"><alternatives><graphic id="pcbi.1006711.e089g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e089" xlink:type="simple"/><mml:math display="block" id="M89"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi>h</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>τ</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>{</mml:mo> <mml:mo>−</mml:mo> <mml:mi mathvariant="bold-italic">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi>h</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>τ</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:mi>Z</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>τ</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
where <italic>x</italic>(<italic>t</italic>) is a Gaussian fitted representation of <inline-formula id="pcbi.1006711.e090"><alternatives><graphic id="pcbi.1006711.e090g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e090" xlink:type="simple"/><mml:math display="inline" id="M90"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> filter responses over time, <italic>h</italic>(<italic>t</italic>) is a collection of binary hidden units such that <italic>h</italic>(<italic>t</italic>) ∈ (0, 1), <italic>x</italic><sub><italic>τ</italic></sub> contains the history of past <italic>τ</italic> filter responses, and <italic>Z</italic> is the partition function as explained in the previous section. The energy function <italic><bold>E</bold></italic> is given by:
<disp-formula id="pcbi.1006711.e091"><alternatives><graphic id="pcbi.1006711.e091g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e091" xlink:type="simple"/><mml:math display="block" id="M91"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold-italic">E</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi>h</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>τ</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>k</mml:mi></mml:munder> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>−</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>l</mml:mi></mml:munder> <mml:msub><mml:mi>h</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>,</mml:mo> <mml:mi>l</mml:mi></mml:mrow></mml:munder> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>h</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
where <italic>W</italic> captures the connections between input and hidden variables. The dynamical terms <inline-formula id="pcbi.1006711.e092"><alternatives><graphic id="pcbi.1006711.e092g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e092" xlink:type="simple"/><mml:math display="inline" id="M92"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006711.e093"><alternatives><graphic id="pcbi.1006711.e093g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e093" xlink:type="simple"/><mml:math display="inline" id="M93"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> are linear functions of previous <italic>τ</italic> filter responses <italic>x</italic><sub><italic>τ</italic></sub>, given by:
<disp-formula id="pcbi.1006711.e094"><alternatives><graphic id="pcbi.1006711.e094g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e094" xlink:type="simple"/><mml:math display="block" id="M94"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>m</mml:mi></mml:munder> <mml:msub><mml:mi>C</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>K</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mspace width="2.em"/><mml:msub><mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>B</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>m</mml:mi></mml:munder> <mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>h</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>K</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
where <italic>A</italic> and <italic>B</italic> are static biases and <italic>C</italic> and <italic>D</italic> are autoregressive model parameters. The dynamic biases <inline-formula id="pcbi.1006711.e095"><alternatives><graphic id="pcbi.1006711.e095g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e095" xlink:type="simple"/><mml:math display="inline" id="M95"><mml:mover accent="true"><mml:mi>a</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006711.e096"><alternatives><graphic id="pcbi.1006711.e096g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e096" xlink:type="simple"/><mml:math display="inline" id="M96"><mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> integrate the input over past <italic>τ</italic> time steps and apply them as a bias to the visible unit <italic>x</italic><sub><italic>k</italic></sub>(<italic>t</italic>) and hidden unit <italic>h</italic><sub><italic>l</italic></sub>(<italic>t</italic>) at current time step <italic>t</italic>. The parameter set Θ = {<italic>W</italic>, <italic>A</italic><sup><italic>τ</italic></sup>, <italic>B</italic><sup><italic>τ</italic></sup>, <italic>C</italic><sup><italic>τ</italic></sup>, <italic>D</italic><sup><italic>τ</italic></sup>} of cRBM networks are learned using contrastive divergence (CD) similar to layer <inline-formula id="pcbi.1006711.e097"><alternatives><graphic id="pcbi.1006711.e097g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e097" xlink:type="simple"/><mml:math display="inline" id="M97"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> [<xref ref-type="bibr" rid="pcbi.1006711.ref069">69</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref139">139</xref>].</p>
<p>Layer <inline-formula id="pcbi.1006711.e098"><alternatives><graphic id="pcbi.1006711.e098g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e098" xlink:type="simple"/><mml:math display="inline" id="M98"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> is structured as an array of cRBM networks spanning various time histories. In the current model, we define networks with time constants <italic>τ</italic> ranging between 30–600 ms. For each time constant, a matching number of instances of layer <inline-formula id="pcbi.1006711.e099"><alternatives><graphic id="pcbi.1006711.e099g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e099" xlink:type="simple"/><mml:math display="inline" id="M99"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> responses are grouped and analyzed in parallel. The same training data (as outlined later) is used to train the RBM in layer <inline-formula id="pcbi.1006711.e100"><alternatives><graphic id="pcbi.1006711.e100g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e100" xlink:type="simple"/><mml:math display="inline" id="M100"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> as well as the cRBM in <inline-formula id="pcbi.1006711.e101"><alternatives><graphic id="pcbi.1006711.e101g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e101" xlink:type="simple"/><mml:math display="inline" id="M101"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>, though training occurs individually for each layer. Here, we employ 300 nodes for each layer of each cRBM network.</p>
</sec>
<sec id="sec019">
<title>Temporal coherence layer</title>
<p>The activations from layer <inline-formula id="pcbi.1006711.e102"><alternatives><graphic id="pcbi.1006711.e102g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e102" xlink:type="simple"/><mml:math display="inline" id="M102"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> are further processed using a Hebbian network, which implement a Storkey learning rule [<xref ref-type="bibr" rid="pcbi.1006711.ref074">74</xref>], written as:
<disp-formula id="pcbi.1006711.e103"><alternatives><graphic id="pcbi.1006711.e103g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e103" xlink:type="simple"/><mml:math display="block" id="M103"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>r</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
where <italic>v</italic><sub><italic>ij</italic></sub>(<italic>t</italic>) is a coherence synaptic connection weight between two neurons <italic>i</italic><sup><italic>th</italic></sup> and <italic>j</italic><sup><italic>th</italic></sup> in <inline-formula id="pcbi.1006711.e104"><alternatives><graphic id="pcbi.1006711.e104g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e104" xlink:type="simple"/><mml:math display="inline" id="M104"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> at time <italic>t</italic>, <italic>r</italic><sub><italic>i</italic></sub>(<italic>t</italic>) and <italic>r</italic><sub><italic>j</italic></sub>(<italic>t</italic>) are the responses of <italic>i</italic><sup><italic>th</italic></sup> and <italic>j</italic><sup><italic>th</italic></sup> <inline-formula id="pcbi.1006711.e105"><alternatives><graphic id="pcbi.1006711.e105g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e105" xlink:type="simple"/><mml:math display="inline" id="M105"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> neurons respectively and <italic>v</italic><sub><italic>ij</italic></sub>(<italic>t</italic> − 1) is the connection weight between the same two neurons at time <italic>t</italic> − 1. The equation above shows that if both <italic>r</italic><sub><italic>i</italic></sub>(<italic>t</italic>) and <italic>r</italic><sub><italic>j</italic></sub>(<italic>t</italic>) are ‘coherent’, the synaptic connection between them becomes stronger whereas the synaptic connections gets weaker for anti-correlated responses. Given that this stage occurs after the sequential integration layer, the coherence is indeed assessed over time histories used in each cRBM network in <inline-formula id="pcbi.1006711.e106"><alternatives><graphic id="pcbi.1006711.e106g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e106" xlink:type="simple"/><mml:math display="inline" id="M106"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>. This stage effectively applies a time-dependent Hebbian weight to the output of the model resulting in <inline-formula id="pcbi.1006711.e107"><alternatives><graphic id="pcbi.1006711.e107g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e107" xlink:type="simple"/><mml:math display="inline" id="M107"><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>R</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:msub><mml:mi>R</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:msub><mml:mi>V</mml:mi> <mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</sec>
<sec id="sec020">
<title>Model dataset</title>
<p>An ensemble of natural sounds comprising of speech and natural sounds are assembled together into a single dataset. It includes speech segments from the TIMIT database [<xref ref-type="bibr" rid="pcbi.1006711.ref141">141</xref>] that include both male and female speakers, as well as various accents and styles and approximately amounts to 4 hours of data. It also comprises the BBC sound effects database [<xref ref-type="bibr" rid="pcbi.1006711.ref142">142</xref>] which contains environmental sounds like ambient and outdoor noises (e.g. street, office, warfare and transportation) as well as animal vocalizations (e.g. barking dogs, bleating goats, and chattering monkeys). The BBC database has total of 2400 recordings, amounting to 68 hours of data. All signals are analyzed over 3-sec segments. Speech utterances are approximately 3 seconds in length, while animal vocalizations and ambient sounds are broken into 3 seconds, and windowed using a raised cosine window to avoid transient effects. All segments are down sampled to 8 kHz and standardized to be zero-mean and unit variance.</p>
</sec>
</sec>
<sec id="sec021">
<title>Model characterization</title>
<sec id="sec022">
<title>Ripple stimuli</title>
<p>The modulation transfer function (MTF) for each layer is characterized using ripple stimuli [<xref ref-type="bibr" rid="pcbi.1006711.ref143">143</xref>]. They are broadband noises consisting of 280 tones, equally spaced along the logarithmic frequency axis, over a range of 5 octaves. The spectral envelope of these stimuli forms sinusoids whose amplitude is modulated by an amount Δ<italic>A</italic> that ranges from 0 to 100%. This construction forms a drifting sinusoidally shaped spectrum along the frequency axis. The envelope of a ripple stimulus is given by:
<disp-formula id="pcbi.1006711.e108"><alternatives><graphic id="pcbi.1006711.e108g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e108" xlink:type="simple"/><mml:math display="block" id="M108"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>,</mml:mo> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>L</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>A</mml:mi> <mml:mi>s</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:mo>(</mml:mo> <mml:mi>ω</mml:mi> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mo>Ω</mml:mo> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mi>ϕ</mml:mi> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula>
where <italic>L</italic> denotes the overall level of the stimulus, <italic>t</italic> is time, and <italic>f</italic> is the tonotopic axis, defined as <inline-formula id="pcbi.1006711.e109"><alternatives><graphic id="pcbi.1006711.e109g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e109" xlink:type="simple"/><mml:math display="inline" id="M109"><mml:mrow><mml:mi>f</mml:mi> <mml:mo>=</mml:mo> <mml:mi>l</mml:mi> <mml:mi>o</mml:mi> <mml:msub><mml:mi>g</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mi mathvariant="monospace">f</mml:mi> <mml:mo>/</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, with <italic>f</italic><sub>0</sub> being the lower edge of the spectrum and <monospace>f</monospace> the linear frequency index. <italic>ω</italic> is the ripple velocity (in Hz), Ω is the ripple density (in cyc/oct), and <italic>ϕ</italic> is phase of the ripple.</p>
</sec>
<sec id="sec023">
<title>Measurement of modulation transfer function (MTF)</title>
<p>The MTF for each layer is measured using individual ripples at rate-scale (<italic>ω</italic>, Ω) combinations over a range of Ω = [0.25, 16] (cyc/oct), and <italic>ω</italic> = [−50, 50] (Hz), with negative rates denoting upward moving ripples. The MTF calculation procedure is as follows: For each combination of (<italic>ω</italic><sub>0</sub>, Ω<sub>0</sub>), we generate a ripple stimulus with contrast Δ<italic>A</italic> = 100% and a corresponding ripple with contrast Δ<italic>A</italic> = 0% that provides a base level or noise floor to the model’s response. The response of units in <inline-formula id="pcbi.1006711.e111"><alternatives><graphic id="pcbi.1006711.e111g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e111" xlink:type="simple"/><mml:math display="inline" id="M111"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006711.e112"><alternatives><graphic id="pcbi.1006711.e112g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e112" xlink:type="simple"/><mml:math display="inline" id="M112"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> to each ripple pair is then obtained (note that responses obtained from the layer <inline-formula id="pcbi.1006711.e113"><alternatives><graphic id="pcbi.1006711.e113g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e113" xlink:type="simple"/><mml:math display="inline" id="M113"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> are used as input for layer <inline-formula id="pcbi.1006711.e114"><alternatives><graphic id="pcbi.1006711.e114g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e114" xlink:type="simple"/><mml:math display="inline" id="M114"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>). Then, an estimate of modulation-synchronized activity <italic>M</italic> at exactly <italic>ω</italic><sub>0</sub> is obtained from each response then converted to a normalized tuning estimate, given by:
<disp-formula id="pcbi.1006711.e115"><alternatives><graphic id="pcbi.1006711.e115g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e115" xlink:type="simple"/><mml:math display="block" id="M115"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mn>10</mml:mn> <mml:mi>l</mml:mi> <mml:mi>o</mml:mi> <mml:msub><mml:mi>g</mml:mi> <mml:mn>10</mml:mn></mml:msub> <mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi>M</mml:mi> <mml:mrow><mml:mn>100</mml:mn> <mml:mo>%</mml:mo></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>ω</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow> <mml:mrow><mml:mrow><mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:msub><mml:mi>M</mml:mi> <mml:mrow><mml:mn>0</mml:mn> <mml:mo>%</mml:mo></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>ω</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>|</mml:mo> <mml:mo>|</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula></p>
</sec>
<sec id="sec024">
<title>Agglomerative clustering</title>
<p>We employ a hierarchical clustering to explore emergent groupings in the structure of filters in layers <inline-formula id="pcbi.1006711.e116"><alternatives><graphic id="pcbi.1006711.e116g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e116" xlink:type="simple"/><mml:math display="inline" id="M116"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006711.e117"><alternatives><graphic id="pcbi.1006711.e117g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e117" xlink:type="simple"/><mml:math display="inline" id="M117"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>. The procedure follows classic clustering techniques used in data mining to partition a dataset into subsets that share some similarity [<xref ref-type="bibr" rid="pcbi.1006711.ref144">144</xref>]. We build a hierarchy from individual <inline-formula id="pcbi.1006711.e118"><alternatives><graphic id="pcbi.1006711.e118g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e118" xlink:type="simple"/><mml:math display="inline" id="M118"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1006711.e119"><alternatives><graphic id="pcbi.1006711.e119g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e119" xlink:type="simple"/><mml:math display="inline" id="M119"><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula> filters by employing pair-wise Euclidean distance between rate-scale tuning of the filters. The agglomerative clustering approach gradually merges individual clusters together based on a distance measure (e.g. Euclidean distance). The number of clusters employed here is heuristically determined based on visual inspection of emerging groups. The two clusters of particular interest in control experiments are harmonicity and onset groups, which occupy a region centered around [1-2] cyc/oct and fast temporal modulations, respectively. We visual inspect the time-frequency profiles of each group to confirm its consistency. We also confirm that neurons grouped in the group labeled onsets (O) are indeed transient filters with an onset response (rather than offset). No apparent offset detectors emerged in the trained filters.</p>
</sec>
</sec>
<sec id="sec025">
<title>Stimuli for stream segregation experiments</title>
<p>We test the model on stream segregation paradigms spanning tones, complexes and speech; as detailed next.</p>
<sec id="sec026">
<title>Two tone sequences</title>
<p>The two-tone stimuli consist of a sequence of repeating pure tones. The tone sequences replicate the stimulus structure used in [<xref ref-type="bibr" rid="pcbi.1006711.ref086">86</xref>] and consist of 100 ms tones, half of which are fixed at 1000 Hz, referred to as “A” tones. The other half, denoted as “B” tones, have a frequency 1, 3, 6, 9, or 15 semitones below 1000 Hz, i.e. at 943.9, 840.9, 707.1, 594.6 or 420 Hz. The A and B tones are separated by a silent gaps of 100 ms and are presented either alternately or synchronously. Each stimulus consists of a total of 24 tones, twelve A tones and twelve B tones. The total duration of sequence is 2.3 seconds for the synchronous case and 2.4 seconds for the alternating case.</p>
<p><italic>Buildup effect on stream segregation</italic>. In order to probe streaming buildup, we use tone triplet sequences, ABA, following the stimulus paradigm used in [<xref ref-type="bibr" rid="pcbi.1006711.ref090">90</xref>]. Tone A is randomly selected from a set of 3 different frequencies (500 Hz, 1000 Hz and 2000 Hz) across different experiments and the other tone B is placed at 1, 3, 6 or 9 semitones above A in each of the experiments. Tones are 125 ms in length with no silence between triplets, though there is a silent gap of 125 ms between consecutive triplets. The buildup effect is demonstrated by varying the duration of entire stimuli sequence from 1 second to 10 seconds. The results are averaged across all the experiments and compared against the psychophysical results reported in [<xref ref-type="bibr" rid="pcbi.1006711.ref090">90</xref>].</p>
</sec>
<sec id="sec027">
<title>Amplitude-modulated noise sequences</title>
<p>The stimulus paradigm used for the amplitude-modulated (AM) noise sequences closely follows the structure used in [<xref ref-type="bibr" rid="pcbi.1006711.ref092">92</xref>]. The noise sequence consists of repeating sinusoidally amplitude-modulated bursts of broadband noise, in a repeating ABA pattern, where A and B correspond to noise bursts having different modulation rates. The modulation depth is maintained at 100% throughout the experiment. Each burst is of 100 ms in duration with no silent gap in between, however there is a silent gap of 20 ms between each of the triplets. The modulation rate of A noise is kept constant at 100 Hz throughout all experiments whereas the modulation rate of B noise is varied from 100 Hz to 800 Hz across different sequences. The modulation rates of B noise are spanned such that they are 0, 0.3, 0.5, 0.7, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 2.5 or 3 octaves above fixed modulation rate of A noise in each sequence. The duration of all sequences is kept constant at 6.4 s.</p>
</sec>
<sec id="sec028">
<title>Tone complexes with harmonicity and onset variations</title>
<p>For this experiment, we use the same stimuli sequence as used in [<xref ref-type="bibr" rid="pcbi.1006711.ref093">93</xref>]. The stimulus consists of 8 target tones denoted by A. Tone A is kept at a constant frequency of 1000 Hz throughout the sequence. Target tones are accompanied by background tones, where each of background tones are 100 ms in duration. The background tone are presented either synchronously with 100 ms targets (referred to as <bold>sync</bold>), or 40 ms before each 60 ms target (referred to as <bold>async</bold>). The offsets of the target and background tones are synchronous in all cases. In synchronous condition, we present different patterns of target and background tones along the tonotopic frequency axis, namely harmonic, shifted and mistuned condition. In “harmonic condition” (H), the background and target tones are placed harmonically in the frequency axis, where each of the tones are harmonics of fundamental frequency <italic>f</italic><sub>0</sub> set to be 1000/<italic>N</italic> and N is randomly set to 3, 4, 5, or 6, with equal probability. All harmonics with frequencies lower than 2000 Hz are included in the stimulus. According to conditions being tested, N is set to be constant for every burst in the sequence denoted by “multiple bursts same” (MBS), or is varied randomly across bursts within trial denoted by “multiple bursts different” (MBD), with the constraint that two consecutive N cannot be the same. In “shifted” condition, the bursts are constructed by shifting all the harmonics by 25% of the <italic>f</italic><sub>0</sub> in either direction except the target tone. In “mistuned” condition, the stimulus is generated by shifting only the target tone by 4% relative to its reference H position. In asynchronous condition, we present the target and background at specific harmonics just like the ‘H’ condition; however in this case, there is an onset difference of 40 ms between the target and background tones.</p>
</sec>
<sec id="sec029">
<title>Speech intelligibility</title>
<p>This experiment replicates the paradigm used in [<xref ref-type="bibr" rid="pcbi.1006711.ref095">95</xref>, <xref ref-type="bibr" rid="pcbi.1006711.ref096">96</xref>]. Speech sentences from multiple speakers are taken from CRM corpus [<xref ref-type="bibr" rid="pcbi.1006711.ref094">94</xref>] that contains an utterance like “Ready Baron [call sign] go to <italic>blue</italic> [color] <italic>eight</italic> [number] now”. The dataset includes four colors (blue, red, green, white) and eight numbers (1-8) in different combinations yielding 256 different sentences recorded for eight different talkers. The task is to identify the target color or number in the sentence under different SNR conditions for various noise types ranging from -18 dB to 18 dB in 3 dB steps. In order to maintain consistency with the perceptual experiments, we use speech modulated noise, babble noise, cafe noise and two-talker interferer from the actual corpus. In case of speech modulated noise, the noise signal is spectrally shaped (with a 512 point FIR filter) to match the average spectrum of 2048 sentences in the CRM corpus. The babble and cafe noise is taken from BBC sound database [<xref ref-type="bibr" rid="pcbi.1006711.ref142">142</xref>] whereas two talker interferer is taken from CRM corpus in such a way that the color and number in interferer sentences are different from target color or number.</p>
</sec>
</sec>
<sec id="sec030">
<title>Readout of model segregation results</title>
<p>For all non-speech simulations, the final readout compares the model response to a given stimulus and to a slight variation of that stimulus in order to probe whether their respective outputs exhibit noticeable differences, which would indicate a segregated or grouped percept. Ultimately, the model readout quantifies the response difference between these signals (as a relative measure) as we sweep through the input parameters. This approach is consistent with classic techniques used to objectively probe stream segregation in human listeners (see [<xref ref-type="bibr" rid="pcbi.1006711.ref086">86</xref>] for more discussion). In the present study, a threshold is chosen empirically to quantify the difference between the stimulus and its variant in order to label it as 1 stream (small enough difference) or 2 streams (large enough difference). In all cases, we confirm that the results are qualitatively similar when we vary the choice of thresholds within a reasonable range. Details of this comparison procedure are outlined below. The procedure for segregation of speech signals is different, as specified in the speech intelligibility section.</p>
<p><italic>Two tone sequences</italic>. In order to determine whether the tones in the ABA tone sequence are grouped into a single stream or multiple streams, we alter the last burst of the A tone by 4% of its actual frequency in either direction (upward or downward) in one sequence (represented by A’) and keep the A tone the same in another sequence. We pass both sequences through the model and compute the Euclidean distance between final responses obtained for the sequence with change and sequence with no change. As the separation between A and B tones increases, we notice that this Euclidean distance increases. We determine an empirically chosen threshold over this distance measure to indicate whether tones A and B are grouped into a single stream or form segregated streams. A d’ measure is then used to quantify correct (hit rate) and false detection (false alarm) of A for both alternating and synchronous sequence; which is computed as:
<disp-formula id="pcbi.1006711.e110"><alternatives><graphic id="pcbi.1006711.e110g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1006711.e110" xlink:type="simple"/><mml:math display="block" id="M110"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>d</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mi>z</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>H</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:mi>z</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>F</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula>
where z() represents the z-score. The d’ score determines the strength of auditory streaming, in line with the approach used in the psychophysical results reported in [<xref ref-type="bibr" rid="pcbi.1006711.ref086">86</xref>].</p>
<p><italic>Buildup effect on stream segregation</italic>. The analysis of buildup also alters the the final burst of the sequence as either tone A or A’ as explained earlier. If the network can report any difference between A and A’ based on a thresholded Euclidean distance, we consider A as a single stream, otherwise both A and B are grouped into single stream. Here, we used the percentage of correct detection of tone A as metric to determine streaming, consistent with results reported in [<xref ref-type="bibr" rid="pcbi.1006711.ref090">90</xref>].</p>
<p><italic>Amplitude-modulated noise sequences</italic>. In the noise sequences, the final burst is comprised of either noise A having the modulation rate of 100 Hz or noise A’ with slight alteration of 10% to actual modulation rate. The results are then reported in terms of percentage of correct detection of noise A following a similar thresholded Euclidean measure approach.</p>
<p><italic>Tone complexes with harmonicity and onset variations</italic>. Just like previous experiments, the final burst of the sequence in each trial comprise of either target tone A or an alteration of 4% to the actual frequency of A in random direction represented by A’. A d’ analysis based on correct (hit rate) and false detection (false alarm) of A for all possible combinations is reported following the procedure described earlier.</p>
<p><italic>Speech intelligibility</italic>. The model’s performance is assessed based on a simplified speech identification task that only employs a readout of the encoding of the target speech segments in the model. First, we divide all utterances belonging to a particular target (either number or color) into training and test sets. Each of the utterances is passed through the entire network to obtain an output response. Frames belonging to the target token are collected together and their corresponding output responses are averaged out to get a single mean response for each utterance. We collect all such responses across the entire training set and build GMM models [<xref ref-type="bibr" rid="pcbi.1006711.ref145">145</xref>] for each target. The test utterance is then passed trough the network to obtain the corresponding output response and averaged across the frames corresponding to the target token similar to training paradigm. This average response is then analyzed through each of the GMM models to obtain the log likelihood score relative to each target <italic>P</italic>(<italic>target</italic>|<italic>θ</italic>) where <italic>θ</italic> represents the GMM parameters for each target class. Based on a predetermined threshold defined empirically, a decision is made as to whether the system identifies the correct target token or not. We repeat the experiments for all the colors and numbers in the CRM corpus and report the accuracy of the system in terms of percentage correct identification of color, number and both color and number.</p>
</sec>
</sec>
</body>
<back>
<ack>
<p>We are very grateful for the tremendous support provided by Sandeep Kothinti. We would also like to thank Dr. Monty Escabi and two other anonymous reviewers for their insightful comments and suggestions.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1006711.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lewicki</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Olshausen</surname> <given-names>BA</given-names></name>, <name name-style="western"><surname>Surlykke</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Moss</surname> <given-names>CF</given-names></name>. <article-title>Scene analysis in the natural environment</article-title>. <source>Frontiers in psychology</source>. <year>2014</year>;<volume>5</volume>:<fpage>199</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2014.00199" xlink:type="simple">10.3389/fpsyg.2014.00199</ext-link></comment> <object-id pub-id-type="pmid">24744740</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref002">
<label>2</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Wolfe</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Kluender</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Levi</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Bartoshuk</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Herz</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Klatzky</surname> <given-names>RL</given-names></name>, <etal>et al</etal>. <chapter-title>Perceiving and Recognizing Objects</chapter-title>. In: <source>Sensation and Perception</source>. <edition>4th ed</edition>. <publisher-name>Sinauer Associates</publisher-name>; <year>2015</year>. p. <fpage>120</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Darwin</surname> <given-names>CJ</given-names></name>. <article-title>Auditory grouping</article-title>. <source>Trends in Cognitive Sciences</source>. <year>1997</year>;<volume>1</volume>(<issue>9</issue>):<fpage>327</fpage>–<lpage>333</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S1364-6613(97)01097-8" xlink:type="simple">10.1016/S1364-6613(97)01097-8</ext-link></comment> <object-id pub-id-type="pmid">21223942</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wagemans</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Elder</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Kubovy</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Palmer</surname> <given-names>SE</given-names></name>, <name name-style="western"><surname>Peterson</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Singh</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>A century of Gestalt psychology in visual perception: I. Perceptual grouping and figure–ground organization</article-title>. <source>Psychological Bulletin</source>. <year>2012</year>;<volume>138</volume>(<issue>6</issue>):<fpage>1172</fpage>–<lpage>1217</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0029333" xlink:type="simple">10.1037/a0029333</ext-link></comment> <object-id pub-id-type="pmid">22845751</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Haykin</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>Z</given-names></name>. <article-title>The cocktail party problem</article-title>. <source>Neural computation</source>. <year>2005</year>;<volume>17</volume>(<issue>9</issue>):<fpage>1875</fpage>–<lpage>1902</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/0899766054322964" xlink:type="simple">10.1162/0899766054322964</ext-link></comment> <object-id pub-id-type="pmid">15992485</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref006">
<label>6</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Duda</surname> <given-names>RO</given-names></name>, <name name-style="western"><surname>Hart</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Stork</surname> <given-names>DG</given-names></name>. <source>Pattern Classification</source>. <publisher-name>Wiley</publisher-name>; <year>2000</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref007">
<label>7</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Bishop</surname> <given-names>CM</given-names></name>. <source>Pattern Recognition and Machine Learning</source>. <publisher-name>Springer</publisher-name>; <year>2006</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref008">
<label>8</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Bregman</surname> <given-names>AS</given-names></name>. <source>Auditory scene analysis: the perceptual organization of sound</source>. <publisher-loc>Cambridge, Mass</publisher-loc>.: <publisher-name>MIT Press</publisher-name>; <year>1990</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Griffiths</surname> <given-names>TD</given-names></name>, <name name-style="western"><surname>Warren</surname> <given-names>JD</given-names></name>. <article-title>What is an auditory object?</article-title> <source>Nature neuroscreviews</source>. <year>2004</year>;<volume>5</volume>(<issue>11</issue>):<fpage>887</fpage>–<lpage>892</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Buchler</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Allegro</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Launer</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Dillier</surname> <given-names>N</given-names></name>. <article-title>Sound classification in hearing aids inspired by auditory scene analysis</article-title>. <source>Eurasip Journal on Applied Signal Processing</source>. <year>2005</year>;<volume>2005</volume>(<issue>18</issue>):<fpage>2991</fpage>–<lpage>3002</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref011">
<label>11</label>
<mixed-citation publication-type="other" xlink:type="simple">Ellis DPW, Weiss RJ. Model-based monaural source separation using vector-quantized phase-vocoder representation. In: Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing. vol. 5; 2006. p. 957–960.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jang</surname> <given-names>GJ</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>TW</given-names></name>. <article-title>A Maximum Likelihood Approach to Single-channel Source Separation</article-title>. <source>Journal of Machine Learning Research</source>. <year>2003</year>;<volume>4</volume>(<issue>7-8</issue>):<fpage>1365</fpage>–<lpage>1392</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Couvreur</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Fontaine</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Gaunard</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Mubikangiey</surname> <given-names>CG</given-names></name>. <article-title>Automatic classification of environmental noise events by hidden Markov models</article-title>. <source>Applied Acoustics</source>. <year>1998</year>;<volume>54</volume>(<issue>3</issue>):<fpage>187</fpage>–<lpage>206</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0003-682X(97)00105-9" xlink:type="simple">10.1016/S0003-682X(97)00105-9</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Grossberg</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Govindarajan</surname> <given-names>KK</given-names></name>, <name name-style="western"><surname>Wyse</surname> <given-names>LL</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>MA</given-names></name>. <article-title>ARTSTREAM: a neural network model of auditory scene analysis and source segregation</article-title>. <source>Neural networks</source>. <year>2004</year>;<volume>17</volume>(<issue>4</issue>):<fpage>511</fpage>–<lpage>536</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neunet.2003.10.002" xlink:type="simple">10.1016/j.neunet.2003.10.002</ext-link></comment> <object-id pub-id-type="pmid">15109681</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Winkler</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Denham</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>. <article-title>Modeling the auditory scene: predictive regularity representations and perceptual objects</article-title>. <source>Trends in cognitive sciences</source>. <year>2009</year>;<volume>13</volume>(<issue>12</issue>):<fpage>40</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2009.09.003" xlink:type="simple">10.1016/j.tics.2009.09.003</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nix</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hohmann</surname> <given-names>V</given-names></name>. <article-title>Combined estimation of spectral envelopes and sound source direction of concurrent voices by multidimensional statistical filtering</article-title>. <source>IEEE Transactions on Audio, Speech and Language Processing</source>. <year>2007</year>;<volume>15</volume>(<issue>3</issue>):<fpage>995</fpage>–<lpage>1008</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TASL.2006.889788" xlink:type="simple">10.1109/TASL.2006.889788</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref017">
<label>17</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Elhilali</surname> <given-names>M</given-names></name>. <chapter-title>Modeling the cocktail party problem</chapter-title>. In: <name name-style="western"><surname>Middlebrooks</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Simon</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Popper</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Fay</surname> <given-names>R</given-names></name>, editors. <source>The auditory system at the cocktail party</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2017</year>. p. <fpage>111</fpage>–<lpage>135</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Riesenhuber</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname> <given-names>T</given-names></name>. <article-title>Hierarchical models of object recognition in cortex</article-title>. <source>Nature neuroscience</source>. <year>1999</year>;<volume>2</volume>(<issue>11</issue>):<fpage>1019</fpage>–<lpage>1025</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/14819" xlink:type="simple">10.1038/14819</ext-link></comment> <object-id pub-id-type="pmid">10526343</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref019">
<label>19</label>
<mixed-citation publication-type="other" xlink:type="simple">Bo L, Ren X, Fox D. Kernel descriptors for visual recognition. In: NIPS; 2010. p. 1–9.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Li</surname> <given-names>Lj</given-names></name>, <name name-style="western"><surname>Su</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Xing</surname> <given-names>EP</given-names></name>, <name name-style="western"><surname>Fei-fei</surname> <given-names>L</given-names></name>. <article-title>Object Bank: A High-Level Image Representation for Scene Classification &amp; Semantic Feature Sparsification</article-title>. <source>Nips</source>. <year>2010</year>; p. <fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>LeCun</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Boser</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Denker</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Henderson</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Howard</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Hubbard</surname> <given-names>W</given-names></name>, <etal>et al</etal>. <article-title>Backpropagation Applied to Handwritten Zip Code Recognition</article-title>. <source>Neural Computation</source>. <year>1989</year>;<volume>1</volume>(<issue>4</issue>):<fpage>541</fpage>–<lpage>551</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.1989.1.4.541" xlink:type="simple">10.1162/neco.1989.1.4.541</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vincent</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Larochelle</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Manzagol</surname> <given-names>Pa</given-names></name>. <article-title>Deep Learning with Denoising Autoencoders</article-title>. <source>Journal of Machine Learning</source>. <year>2008</year>;<volume>27</volume>:<fpage>49</fpage>–<lpage>50</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zhou</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Yuan</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Shi</surname> <given-names>C</given-names></name>. <article-title>Object tracking using SIFT features and mean shift</article-title>. <source>Computer Vision and Image Understanding</source>. <year>2009</year>;<volume>113</volume>(<issue>3</issue>):<fpage>345</fpage>–<lpage>352</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cviu.2008.08.006" xlink:type="simple">10.1016/j.cviu.2008.08.006</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tatler</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>Hayhoe</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Land</surname> <given-names>MF</given-names></name>, <name name-style="western"><surname>Ballard</surname> <given-names>DH</given-names></name>. <article-title>Eye guidance in natural vision: Reinterpreting salience</article-title>. <source>Journal of Vision</source>. <year>2011</year>;<volume>11</volume>(<issue>5</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/11.5.5" xlink:type="simple">10.1167/11.5.5</ext-link></comment> <object-id pub-id-type="pmid">21622729</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref025">
<label>25</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Nowak</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Jurie</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Triggs</surname> <given-names>B</given-names></name>. <chapter-title>Sampling strategies for bag-of-features image classification</chapter-title>. In: <source>Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</source>. <volume>vol. 3954</volume> <publisher-name>LNCS</publisher-name>; <year>2006</year>. p. <fpage>490</fpage>–<lpage>503</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref026">
<label>26</label>
<mixed-citation publication-type="other" xlink:type="simple">Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition. International Conference on Learning Representations (ICRL). 2015; p. 1–14.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref027">
<label>27</label>
<mixed-citation publication-type="other" xlink:type="simple">Lee H, Grosse R, Ranganath R, Ng AY. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In: Proceedings of the 26th Annual International Conference on Machine Learning. ACM; 2009. p. 1–8.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref028">
<label>28</label>
<mixed-citation publication-type="other" xlink:type="simple">Radford A, Metz L, Chintala S. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. arXiv. 2015; p. 1–15.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref029">
<label>29</label>
<mixed-citation publication-type="other" xlink:type="simple">Coates A, Carpenter B, Case C, Satheesh S, Suresh B, Wang T, et al. Text Detection and Character Recognition in Scene Images with Unsupervised Feature Learning. 2011 International Conference on Document Analysis and Recognition. 2011; p. 440–445.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>, <name name-style="western"><surname>Osindero</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Teh</surname> <given-names>YW</given-names></name>. <article-title>A Fast Learning Algorithm for Deep Belief Nets</article-title>. <source>Neural Computation</source>. <year>2006</year>;<volume>18</volume>(<issue>7</issue>):<fpage>1527</fpage>–<lpage>1554</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.2006.18.7.1527" xlink:type="simple">10.1162/neco.2006.18.7.1527</ext-link></comment> <object-id pub-id-type="pmid">16764513</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref031">
<label>31</label>
<mixed-citation publication-type="other" xlink:type="simple">Taylor GW, Sigal L, Fleet DJ, Hinton GE. Dynamical binary latent variable models for 3D human pose tracking. Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. 2010; p. 631–638.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hartmann</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>D</given-names></name>. <article-title>Stream segregation and peripheral channeling</article-title>. <source>Music Perception</source>. <year>1991</year>;<volume>9</volume>(<issue>2</issue>):<fpage>155</fpage>–<lpage>184</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2307/40285527" xlink:type="simple">10.2307/40285527</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Beauvois</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Meddis</surname> <given-names>R</given-names></name>. <article-title>A computer model of auditory stream segregation</article-title>. <source>Human experimental psychology</source>. <year>1991</year>;<volume>43</volume>(<issue>3</issue>):<fpage>517</fpage>–<lpage>541</lpage>. <object-id pub-id-type="pmid">1775655</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>McCabe</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Denham</surname> <given-names>MJ</given-names></name>. <article-title>A model of auditory streaming</article-title>. <source>Journal of the Acoustical Society of America</source>. <year>1997</year>;<volume>101</volume>(<issue>3</issue>):<fpage>1611</fpage>–<lpage>1621</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.418176" xlink:type="simple">10.1121/1.418176</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wang</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>P</given-names></name>. <article-title>An oscillatory correlation model of auditory streaming</article-title>. <source>Cognitive neurodynamics</source>. <year>2008</year>;<volume>2</volume>(<issue>1</issue>):<fpage>7</fpage>–<lpage>19</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s11571-007-9035-8" xlink:type="simple">10.1007/s11571-007-9035-8</ext-link></comment> <object-id pub-id-type="pmid">19003469</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hu</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>D</given-names></name>. <article-title>A tandem algorithm for pitch estimation and voiced speech segregation</article-title>. <source>IEEE Transactions on Audio, Speech and Language Processing</source>. <year>2010</year>;<volume>18</volume>(<issue>8</issue>):<fpage>2067</fpage>–<lpage>2079</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TASL.2010.2041110" xlink:type="simple">10.1109/TASL.2010.2041110</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wang</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>GJ</given-names></name>. <article-title>Separation of speech from interfering sounds based on oscillatory correlation</article-title>. <source>IEEE Transactions on Neural Networks</source>. <year>1999</year>;<volume>10</volume>(<issue>3</issue>):<fpage>684</fpage>–<lpage>697</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/72.761727" xlink:type="simple">10.1109/72.761727</ext-link></comment> <object-id pub-id-type="pmid">18252568</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Krishnan</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Elhilali</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Shamma</surname> <given-names>S</given-names></name>. <article-title>Segregating complex sound sources through temporal coherence</article-title>. <source>PLoS computational biology</source>. <year>2014</year>;<volume>10</volume>(<issue>12</issue>):<fpage>e1003985</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003985" xlink:type="simple">10.1371/journal.pcbi.1003985</ext-link></comment> <object-id pub-id-type="pmid">25521593</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Elhilali</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Micheyl</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Oxenham</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Shamma</surname> <given-names>SA</given-names></name>. <article-title>Temporal Coherence in the Perceptual Organization and Cortical Representation of Auditory Scenes</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>61</volume>(<issue>2</issue>):<fpage>317</fpage>–<lpage>329</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2008.12.005" xlink:type="simple">10.1016/j.neuron.2008.12.005</ext-link></comment> <object-id pub-id-type="pmid">19186172</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Thakur</surname> <given-names>CS</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Afshar</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hamilton</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Tapson</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Shamma</surname> <given-names>SA</given-names></name>, <etal>et al</etal>. <article-title>Sound stream segregation: A neuromorphic approach to solve the “cocktail party problem” in real-time</article-title>. <source>Frontiers in Neuroscience</source>. <year>2015</year>;<volume>9</volume>(<issue>SEP</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnins.2015.00309" xlink:type="simple">10.3389/fnins.2015.00309</ext-link></comment> <object-id pub-id-type="pmid">26388721</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref041">
<label>41</label>
<mixed-citation publication-type="other" xlink:type="simple">Lee H, Pham P, Largman Y, Ng AY. Unsupervised feature learning for audio classification using convolutional deep belief networks. In: Advances in neural information processing systems; 2009. p. 1096–1104.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref042">
<label>42</label>
<mixed-citation publication-type="other" xlink:type="simple">Simpson AJR, Roma G, Plumbley MD. Deep karaoke: Extracting vocals from musical mixtures using a convolutional deep neural network. In: Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). vol. 9237; 2015. p. 429–436.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Xu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>Q</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Foster</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Sigtia</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Jackson</surname> <given-names>PJB</given-names></name>, <etal>et al</etal>. <article-title>Unsupervised Feature Learning Based on Deep Models for Environmental Audio Tagging</article-title>. <source>IEEE/ACM Transactions on Audio Speech and Language Processing</source>. <year>2017</year>;<volume>25</volume>(<issue>6</issue>):<fpage>1230</fpage>–<lpage>1241</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TASLP.2017.2690563" xlink:type="simple">10.1109/TASLP.2017.2690563</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref044">
<label>44</label>
<mixed-citation publication-type="other" xlink:type="simple">Sainath TN, Kanevsky D, Iyengar G. Unsupervised audio segmentation using extended Baum-Welch transformations. In: ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing—Proceedings. vol. 1; 2007. p. 209–212.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref045">
<label>45</label>
<mixed-citation publication-type="other" xlink:type="simple">Hershey JR, Chen Z, Le Roux J, Watanabe S. Deep clustering: Discriminative embeddings for segmentation and separation. In: Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE; 2016. p. 31–35.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Luo</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Mesgarani</surname> <given-names>N</given-names></name>. <article-title>Speaker-Independent Speech Separation with Deep Attractor Network</article-title>. <source>IEEE/ACM Transactions on Audio Speech and Language Processing</source>. <year>2018</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TASLP.2018.2795749" xlink:type="simple">10.1109/TASLP.2018.2795749</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref047">
<label>47</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Darwin</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Carlyon</surname> <given-names>RP</given-names></name>. <chapter-title>Auditory Grouping</chapter-title>. In: <name name-style="western"><surname>Moore</surname> <given-names>BCJ</given-names></name>, editor. <source>Hearing</source>. <volume>vol. 6</volume> of Hearing. <publisher-loc>Orlando, FL</publisher-loc>: <publisher-name>Elsevier</publisher-name>; <year>1995</year>. p. <fpage>387</fpage>–<lpage>424</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shinn-Cunningham</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>AKC</given-names></name>, <name name-style="western"><surname>Oxenham</surname> <given-names>AJ</given-names></name>. <article-title>A sound element gets lost in perceptual competition</article-title>. <source>ProcNatAcadSci</source>. <year>2007</year>;<volume>104</volume>(<issue>29</issue>):<fpage>12223</fpage>–<lpage>12227</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0704641104" xlink:type="simple">10.1073/pnas.0704641104</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Darwin</surname> <given-names>CJ</given-names></name>. <article-title>Simultaneous Grouping and Auditory Continuity</article-title>. <source>Perception &amp; Psychoacoustics</source>. <year>2005</year>;<volume>67</volume>(<issue>8</issue>):<fpage>1384</fpage>–<lpage>1390</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BF03193643" xlink:type="simple">10.3758/BF03193643</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Darwin</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Hukin</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Al-Khatib</surname> <given-names>BY</given-names></name>. <article-title>Grouping in pitch perception: Evidence for sequential constraints</article-title>. <source>Journal of the Acoustical Society of America</source>. <year>1995</year>;<volume>98</volume>(<issue>2 I</issue>):<fpage>880</fpage>–<lpage>885</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.413513" xlink:type="simple">10.1121/1.413513</ext-link></comment> <object-id pub-id-type="pmid">7642826</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Moore</surname> <given-names>BCJ</given-names></name>, <name name-style="western"><surname>Gockel</surname> <given-names>H</given-names></name>. <article-title>Factors influencing sequential stream segregation</article-title>. <source>Acta Acustica</source>. <year>2002</year>;<volume>88</volume>:<fpage>320</fpage>–<lpage>333</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>van Zuijen</surname> <given-names>TL</given-names></name>, <name name-style="western"><surname>Sussman</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Winkler</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Näätänen</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Tervaniemi</surname> <given-names>M</given-names></name>. <article-title>Grouping of Sequential Sounds—An Event-Related Potential Study Comparing Musicians and Nonmusicians</article-title>. <source>Journal of Cognitive Neuroscience</source>. <year>2004</year>;<volume>16</volume>(<issue>2</issue>):<fpage>331</fpage>–<lpage>338</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089892904322984607" xlink:type="simple">10.1162/089892904322984607</ext-link></comment> <object-id pub-id-type="pmid">15068601</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Best</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Gallun</surname> <given-names>FJ</given-names></name>, <name name-style="western"><surname>Carlile</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Shinn-Cunningham</surname> <given-names>BG</given-names></name>. <article-title>Binaural interference and auditory grouping</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>2007</year>;<volume>121</volume>(<issue>2</issue>):<fpage>1070</fpage>–<lpage>1076</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.2407738" xlink:type="simple">10.1121/1.2407738</ext-link></comment> <object-id pub-id-type="pmid">17348529</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hamaoui</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Deutsch</surname> <given-names>D</given-names></name>. <article-title>The perceptual grouping of musical Sequences: Pitch and timing as competing cues</article-title>. <source>Proceedings of the 11th International Conference on Music Perception and Cognition</source>. <year>2011</year>;<volume>11</volume>:<fpage>81</fpage>–<lpage>87</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Luciw</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Weng</surname> <given-names>J</given-names></name>. <article-title>Top-down connections in self-organizing hebbian networks: Topographic class grouping</article-title>. <source>IEEE Transactions on Autonomous Mental Development</source>. <year>2010</year>;<volume>2</volume>(<issue>3</issue>):<fpage>248</fpage>–<lpage>261</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TAMD.2010.2072150" xlink:type="simple">10.1109/TAMD.2010.2072150</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Falconbridge</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Stamps</surname> <given-names>RL</given-names></name>, <name name-style="western"><surname>Badcock</surname> <given-names>DR</given-names></name>. <article-title>A simple Hebbian/anti-Hebbian network learns the sparse, independent components of natural images</article-title>. <source>Neural computation</source>. <year>2006</year>;<volume>18</volume>(<issue>2</issue>):<fpage>415</fpage>–<lpage>29</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976606775093891" xlink:type="simple">10.1162/089976606775093891</ext-link></comment> <object-id pub-id-type="pmid">16378520</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Xie</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Seung</surname> <given-names>HS</given-names></name>. <article-title>Equivalence of Backpropagation and Contrastive Hebbian Learning in a Layered Network</article-title>. <source>Neural Computation</source>. <year>2003</year>;<volume>15</volume>(<issue>2</issue>):<fpage>441</fpage>–<lpage>454</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976603762552988" xlink:type="simple">10.1162/089976603762552988</ext-link></comment> <object-id pub-id-type="pmid">12590814</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Carlyon</surname> <given-names>RP</given-names></name>. <article-title>How the brain separates sounds</article-title>. <source>Trends in cognitive sciences</source>. <year>2004</year>;<volume>8</volume>(<issue>10</issue>):<fpage>465</fpage>–<lpage>471</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2004.08.008" xlink:type="simple">10.1016/j.tics.2004.08.008</ext-link></comment> <object-id pub-id-type="pmid">15450511</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hulse</surname> <given-names>SH</given-names></name>, <name name-style="western"><surname>MacDougall-Shackleton</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Wisniewski</surname> <given-names>AB</given-names></name>. <article-title>Auditory scene analysis by songbirds: stream segregation of birdsong by European starlings (Sturnus vulgaris)</article-title>. <source>Journal of computational psychology</source>. <year>1997</year>;<volume>111</volume>(<issue>1</issue>):<fpage>3</fpage>–<lpage>13</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0735-7036.111.1.3" xlink:type="simple">10.1037/0735-7036.111.1.3</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fay</surname> <given-names>RR</given-names></name>. <article-title>Auditory stream segregation in goldfish (Carassius auratus)</article-title>. <source>Hearing research</source>. <year>1998</year>;<volume>120</volume>(<issue>1-2</issue>):<fpage>69</fpage>–<lpage>76</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0378-5955(98)00058-6" xlink:type="simple">10.1016/S0378-5955(98)00058-6</ext-link></comment> <object-id pub-id-type="pmid">9667432</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Izumi</surname> <given-names>A</given-names></name>. <article-title>Auditory stream segregation in Japanese monkeys</article-title>. <source>Cognition</source>. <year>2002</year>;<volume>82</volume>(<issue>3</issue>):<fpage>113</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0010-0277(01)00161-5" xlink:type="simple">10.1016/S0010-0277(01)00161-5</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Aubin</surname> <given-names>T</given-names></name>. <article-title>Penguins and their noisy world</article-title>. <source>Annals of the Brazilian Academy of Sciences</source>. <year>2004</year>;<volume>76</volume>(<issue>2</issue>):<fpage>279</fpage>–<lpage>283</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1590/S0001-37652004000200015" xlink:type="simple">10.1590/S0001-37652004000200015</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref063">
<label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Itatani</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Klump</surname> <given-names>GM</given-names></name>. <article-title>Auditory streaming of amplitude-modulated sounds in the songbird forebrain</article-title>. <source>Journal of neurophysiology</source>. <year>2009</year>;<volume>101</volume>(<issue>6</issue>):<fpage>3212</fpage>–<lpage>3225</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.91333.2008" xlink:type="simple">10.1152/jn.91333.2008</ext-link></comment> <object-id pub-id-type="pmid">19357341</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref064">
<label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shamma</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Elhilali</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Micheyl</surname> <given-names>C</given-names></name>. <article-title>Temporal coherence and attention in auditory scene analysis</article-title>. <source>Trends in neurosciences</source>. <year>2011</year>;<volume>34</volume>(<issue>3</issue>):<fpage>114</fpage>–<lpage>23</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tins.2010.11.002" xlink:type="simple">10.1016/j.tins.2010.11.002</ext-link></comment> <object-id pub-id-type="pmid">21196054</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref065">
<label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lu</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Xu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Yin</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Oxenham</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Fritz</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Shamma</surname> <given-names>SA</given-names></name>. <article-title>Temporal coherence structure rapidly shapes neuronal interactions</article-title>. <source>Nature communications</source>. <year>2017</year>;<volume>8</volume>:<fpage>13900</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/ncomms13900" xlink:type="simple">10.1038/ncomms13900</ext-link></comment> <object-id pub-id-type="pmid">28054545</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref066">
<label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yang</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Shamma</surname> <given-names>SA</given-names></name>. <article-title>Auditory representations of acoustic signals</article-title>. <source>IEEE Trans Inf Theory</source>. <year>1992</year>;<volume>38</volume>(<issue>2</issue>):<fpage>824</fpage>–<lpage>839</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/18.119739" xlink:type="simple">10.1109/18.119739</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref067">
<label>67</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>. <chapter-title>A Practical Guide to Training Restricted Boltzmann Machines</chapter-title>. In: <source>Neural Networks: Tricks of the Trade</source>. <volume>vol. 7700</volume>. <publisher-name>springer</publisher-name>; <year>2012</year>. p. <fpage>599</fpage>–<lpage>619</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref068">
<label>68</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Goodfellow</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Courville</surname> <given-names>A</given-names></name>. <source>Deep Learning</source>. <publisher-name>MIT press</publisher-name>; <year>2016</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref069">
<label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hinton</surname> <given-names>GE</given-names></name>. <article-title>Training Products of Experts by Minimizing Contrastive Divergence</article-title>. <source>Neural Computation</source>. <year>2002</year>;<volume>14</volume>(<issue>8</issue>):<fpage>1771</fpage>–<lpage>1800</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976602760128018" xlink:type="simple">10.1162/089976602760128018</ext-link></comment> <object-id pub-id-type="pmid">12180402</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref070">
<label>70</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Elhilali</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Shamma</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Simon</surname> <given-names>JZ</given-names></name>, <name name-style="western"><surname>Fritz</surname> <given-names>JB</given-names></name>. <chapter-title>A linear systems view to the concept of STRF</chapter-title>. In: <name name-style="western"><surname>Depireux</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Elhilali</surname> <given-names>M</given-names></name>, editors. <source>Handbook of Modern Techniques in Auditory Cortex</source>. <publisher-name>Nova Science Pub Inc</publisher-name>; <year>2013</year>. p. <fpage>33</fpage>–<lpage>60</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref071">
<label>71</label>
<mixed-citation publication-type="other" xlink:type="simple">Taylor GW, Hinton GE. Factored conditional restricted Boltzmann Machines for modeling motion style. In: Proceedings of the 26th Annual International Conference on Machine Learning—ICML’09; 2009. p. 1–8.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref072">
<label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shamma</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Elhilali</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Micheyl</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Oxenham</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Pressnitzer</surname> <given-names>D</given-names></name>, <etal>et al</etal>. <article-title>Temporal Coherence and the Streaming of Complex Sounds</article-title>. <source>Advances in experimental medicine and biology</source>. <year>2013</year>;<volume>787</volume>:<fpage>535</fpage>–<lpage>543</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/978-1-4614-1590-9_59" xlink:type="simple">10.1007/978-1-4614-1590-9_59</ext-link></comment> <object-id pub-id-type="pmid">23716261</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref073">
<label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hopfield</surname> <given-names>JJ</given-names></name>. <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1982</year>;<volume>79</volume>(<issue>8</issue>):<fpage>2554</fpage>–<lpage>2558</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.79.8.2554" xlink:type="simple">10.1073/pnas.79.8.2554</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref074">
<label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Storkey</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Valabregue</surname> <given-names>R</given-names></name>. <article-title>The basins of attraction of a new Hopfield learning rule</article-title>. <source>Neural Networks</source>. <year>1999</year>;<volume>12</volume>(<issue>6</issue>):<fpage>869</fpage>–<lpage>876</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0893-6080(99)00038-6" xlink:type="simple">10.1016/S0893-6080(99)00038-6</ext-link></comment> <object-id pub-id-type="pmid">12662662</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref075">
<label>75</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Singh</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Theunissen</surname> <given-names>F</given-names></name>. <article-title>Modulation spectra of natural sounds and ethological theories of auditory processing</article-title>. <source>Journal of the Acoustical Society of America</source>. <year>2003</year>;<volume>106</volume>:<fpage>3394</fpage>–<lpage>3411</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.1624067" xlink:type="simple">10.1121/1.1624067</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref076">
<label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Elliott</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Theunissen</surname> <given-names>FE</given-names></name>. <article-title>The modulation transfer function for speech intelligibility</article-title>. <source>PLoS computational biology</source>. <year>2009</year>;<volume>5</volume>(<issue>3</issue>):<fpage>e1000302</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1000302" xlink:type="simple">10.1371/journal.pcbi.1000302</ext-link></comment> <object-id pub-id-type="pmid">19266016</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref077">
<label>77</label>
<mixed-citation publication-type="other" xlink:type="simple">Shamma SA, Versnel H, Kowalski N. Ripple Analysis in Ferret Primary Auditory Cortex. I. Response Characteristics of Single Units to Sinusoidally Rippled Spectra. Institute for Systems Research Technical Reports. 1994.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref078">
<label>78</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schreiner</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Calhoun</surname> <given-names>B</given-names></name>. <article-title>Spectral envelope coding in cat primary auditory cortex: Properties of ripple transfer functions</article-title>. <source>Journal of Auditory Neuroscience</source>. <year>1995</year>;<volume>1</volume>:<fpage>39</fpage>–<lpage>61</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref079">
<label>79</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schonwiesner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Zatorre</surname> <given-names>RJ</given-names></name>. <article-title>Spectro-temporal modulation transfer function of single voxels in the human auditory cortex measured with high-resolution fMRI</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2009</year>;<volume>106</volume>(<issue>34</issue>):<fpage>14611</fpage>–<lpage>14616</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0907682106" xlink:type="simple">10.1073/pnas.0907682106</ext-link></comment> <object-id pub-id-type="pmid">19667199</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref080">
<label>80</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Depireux</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Simon</surname> <given-names>JZ</given-names></name>, <name name-style="western"><surname>Klein</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Shamma</surname> <given-names>SA</given-names></name>. <article-title>Spectro-temporal response field characterization with dynamic ripples in ferret primary auditory cortex</article-title>. <source>Journal of neurophysiology</source>. <year>2001</year>;<volume>85</volume>(<issue>3</issue>):<fpage>1220</fpage>–<lpage>1234</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.2001.85.3.1220" xlink:type="simple">10.1152/jn.2001.85.3.1220</ext-link></comment> <object-id pub-id-type="pmid">11247991</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref081">
<label>81</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Miller</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Escabí</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Read</surname> <given-names>HL</given-names></name>, <name name-style="western"><surname>Schreiner</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Escabi</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Read</surname> <given-names>HL</given-names></name>, <etal>et al</etal>. <article-title>Spectrotemporal receptive fields in the lemniscal auditory thalamus and cortex</article-title>. <source>Journal of neurophysiology</source>. <year>2002</year>;<volume>87</volume>(<issue>1</issue>):<fpage>516</fpage>–<lpage>527</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00395.2001" xlink:type="simple">10.1152/jn.00395.2001</ext-link></comment> <object-id pub-id-type="pmid">11784767</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref082">
<label>82</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Escabi</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Read</surname> <given-names>HL</given-names></name>, <name name-style="western"><surname>Schreiner</surname> <given-names>CE</given-names></name>. <article-title>Naturalistic auditory contrast improves spectrotemporal coding in the cat inferior colliculus</article-title>. <source>Journal of Neuroscience</source>. <year>2003</year>;<volume>23</volume>(<issue>37</issue>):<fpage>11489</fpage>–<lpage>11504</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.23-37-11489.2003" xlink:type="simple">10.1523/JNEUROSCI.23-37-11489.2003</ext-link></comment> <object-id pub-id-type="pmid">14684853</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref083">
<label>83</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sharpee</surname> <given-names>TO</given-names></name>, <name name-style="western"><surname>Atencio</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Schreiner</surname> <given-names>CE</given-names></name>. <article-title>Hierarchical representations in the auditory cortex</article-title>. <source>Current opinion in neurobiology</source>. <year>2011</year>;<volume>21</volume>(<issue>5</issue>):<fpage>761</fpage>–<lpage>767</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2011.05.027" xlink:type="simple">10.1016/j.conb.2011.05.027</ext-link></comment> <object-id pub-id-type="pmid">21704508</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref084">
<label>84</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bregman</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Rudnicky</surname> <given-names>AI</given-names></name>. <article-title>Auditory segregation: stream or streams?</article-title> <source>Journal of Experimental Psychology-Human Perception and Performance</source>. <year>1975</year>;<volume>1</volume>(<issue>3</issue>):<fpage>263</fpage>–<lpage>267</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0096-1523.1.3.263" xlink:type="simple">10.1037/0096-1523.1.3.263</ext-link></comment> <object-id pub-id-type="pmid">1202149</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref085">
<label>85</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>van Noorden</surname> <given-names>LP</given-names></name>, <name name-style="western"><surname>van Noorden</surname> <given-names>LP</given-names></name>. <article-title>Minimun differences of level and frequency for perceptual fission of tone sequences ABAB</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>1977</year>;<volume>61</volume>(<issue>4</issue>):<fpage>1041</fpage>–<lpage>1045</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.381388" xlink:type="simple">10.1121/1.381388</ext-link></comment> <object-id pub-id-type="pmid">864091</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref086">
<label>86</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Micheyl</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Hanson</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Demany</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Shamma</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Oxenham</surname> <given-names>AJ</given-names></name>. <article-title>Auditory stream segregation for alternating and synchronous tones</article-title>. <source>Journal of experimental psychologyHuman perception and performance</source>. <year>2013</year>;<volume>39</volume>(<issue>6</issue>):<fpage>1568</fpage>–<lpage>1580</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0032241" xlink:type="simple">10.1037/a0032241</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref087">
<label>87</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Green</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Swets</surname> <given-names>JA</given-names></name>. <source>Signal detection theory and psychophysics</source>. <volume>vol. 1</volume>. <publisher-name>Wiley</publisher-name> <publisher-loc>New York</publisher-loc>; <year>1966</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref088">
<label>88</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Macmillan</surname> <given-names>NA</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>M</given-names></name>. <article-title>A probe-signal investigation of uncertain-frequency detection</article-title>. <source>Journal of the Acoustical Society of America</source>. <year>1975</year>;<volume>58</volume>(<issue>5</issue>):<fpage>1051</fpage>–<lpage>1058</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.380764" xlink:type="simple">10.1121/1.380764</ext-link></comment> <object-id pub-id-type="pmid">1194556</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref089">
<label>89</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Naatanen</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Tervaniemi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sussman</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Paavilainen</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Winkler</surname> <given-names>I</given-names></name>. <article-title>“Primitive intelligence” in the auditory cortex</article-title>. <source>Trends in neurosciences</source>. <year>2001</year>;<volume>24</volume>(<issue>5</issue>):<fpage>283</fpage>–<lpage>288</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0166-2236(00)01790-2" xlink:type="simple">10.1016/S0166-2236(00)01790-2</ext-link></comment> <object-id pub-id-type="pmid">11311381</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref090">
<label>90</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Micheyl</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Tian</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Carlyon</surname> <given-names>RP</given-names></name>, <name name-style="western"><surname>Rauschecker</surname> <given-names>JP</given-names></name>. <article-title>Perceptual organization of tone sequences in the auditory cortex of awake macaques</article-title>. <source>Neuron</source>. <year>2005</year>;<volume>48</volume>(<issue>1</issue>):<fpage>139</fpage>–<lpage>148</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2005.08.039" xlink:type="simple">10.1016/j.neuron.2005.08.039</ext-link></comment> <object-id pub-id-type="pmid">16202714</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref091">
<label>91</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ciocca</surname> <given-names>V</given-names></name>. <article-title>The auditory organization of complex sounds</article-title>. <source>Frontiers in bioscience: a journal and virtual library</source>. <year>2008</year>;<volume>13</volume>:<fpage>148</fpage>–<lpage>169</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2741/2666" xlink:type="simple">10.2741/2666</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref092">
<label>92</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Grimault</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Bacon</surname> <given-names>SP</given-names></name>, <name name-style="western"><surname>Micheyl</surname> <given-names>C</given-names></name>. <article-title>Auditory stream segregation on the basis of amplitude-modulation rate</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>2002</year>;<volume>111</volume>(<issue>3</issue>):<fpage>1340</fpage>–<lpage>1348</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.1452740" xlink:type="simple">10.1121/1.1452740</ext-link></comment> <object-id pub-id-type="pmid">11931311</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref093">
<label>93</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Micheyl</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Kreft</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Shamma</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Oxenham</surname> <given-names>AJ</given-names></name>. <article-title>Temporal coherence versus harmonicity in auditory stream formation</article-title>. <source>Journal of the Acoustical Society of America</source>. <year>2013</year>;<volume>133</volume>(<issue>3</issue>):<fpage>EL188</fpage>–<lpage>EL194</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.4789866" xlink:type="simple">10.1121/1.4789866</ext-link></comment> <object-id pub-id-type="pmid">23464127</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref094">
<label>94</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bolia</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>WT</given-names></name>, <name name-style="western"><surname>Ericson</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Simpson</surname> <given-names>BD</given-names></name>. <article-title>A Speech Corpus for Multitalker Communications Research</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>2000</year>;<volume>107</volume>(<issue>2</issue>):<fpage>1065</fpage>–<lpage>1066</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.428288" xlink:type="simple">10.1121/1.428288</ext-link></comment> <object-id pub-id-type="pmid">10687719</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref095">
<label>95</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brungart</surname> <given-names>DS</given-names></name>. <article-title>Evaluation of speech intelligibility with the coordinate response measure</article-title>. <source>Journal of the Acoustical Society of America</source>. <year>2001</year>;<volume>109</volume>(<issue>5</issue>):<fpage>2276</fpage>–<lpage>2279</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.1357812" xlink:type="simple">10.1121/1.1357812</ext-link></comment> <object-id pub-id-type="pmid">11386582</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref096">
<label>96</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Eddins</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>C</given-names></name>. <article-title>Psychometric properties of the coordinate response measure corpus with various types of background interference</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>2012</year>;<volume>131</volume>(<issue>2</issue>):<fpage>EL177</fpage>–<lpage>EL183</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.3678680" xlink:type="simple">10.1121/1.3678680</ext-link></comment> <object-id pub-id-type="pmid">22352619</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref097">
<label>97</label>
<mixed-citation publication-type="other" xlink:type="simple">Elhilali M, Shamma S. Information-bearing components of speech intelligibility under babble-noise and bandlimiting distortions. In: Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP); 2008. p. 4205–4208.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref098">
<label>98</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Pickles</surname> <given-names>JO</given-names></name>. <source>An Introduction to the Physiology of Hearing</source>. <publisher-name>Academic Press</publisher-name>; <year>1988</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref099">
<label>99</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Middlebrooks</surname> <given-names>JC</given-names></name>. <article-title>Auditory cortex cheers the overture and listens through the finale</article-title>. <source>Nature neuroscience</source>. <year>2005</year>;<volume>8</volume>(<issue>7</issue>):<fpage>851</fpage>–<lpage>852</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn0705-851" xlink:type="simple">10.1038/nn0705-851</ext-link></comment> <object-id pub-id-type="pmid">16136671</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref100">
<label>100</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>. <article-title>Hierarchical models in the brain</article-title>. <source>PLoS computational biology</source>. <year>2008</year>;<volume>4</volume>(<issue>11</issue>):<fpage>e1000211</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1000211" xlink:type="simple">10.1371/journal.pcbi.1000211</ext-link></comment> <object-id pub-id-type="pmid">18989391</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref101">
<label>101</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Scholl</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Gao</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Wehr</surname> <given-names>M</given-names></name>. <article-title>Nonoverlapping Sets of Synapses Drive On Responses and Off Responses in Auditory Cortex</article-title>. <source>Neuron</source>. <year>2010</year>;<volume>65</volume>(<issue>3</issue>):<fpage>412</fpage>–<lpage>421</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2010.01.020" xlink:type="simple">10.1016/j.neuron.2010.01.020</ext-link></comment> <object-id pub-id-type="pmid">20159453</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref102">
<label>102</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wang</surname> <given-names>X</given-names></name>. <article-title>The harmonic organization of auditory cortex</article-title>. <source>Frontiers in Systems Neuroscience</source>. <year>2013</year>;<volume>7</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnsys.2013.00114" xlink:type="simple">10.3389/fnsys.2013.00114</ext-link></comment> <object-id pub-id-type="pmid">24381544</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref103">
<label>103</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Bizley</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Nodal</surname> <given-names>FR</given-names></name>, <name name-style="western"><surname>Ahmed</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>King</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Schnupp</surname> <given-names>JW</given-names></name>. <article-title>Responses of auditory cortex to complex stimuli: functional organization revealed using intrinsic optical signals</article-title>. <source>Journal of neurophysiology</source>. <year>2008</year>;<volume>99</volume>(<issue>4</issue>):<fpage>1928</fpage>–<lpage>1941</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00469.2007" xlink:type="simple">10.1152/jn.00469.2007</ext-link></comment> <object-id pub-id-type="pmid">18272880</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref104">
<label>104</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Escabi</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Schreiner</surname> <given-names>CE</given-names></name>. <article-title>Nonlinear spectrotemporal sound analysis by neurons in the auditory midbrain</article-title>. <source>Journal of Neuroscience</source>. <year>2002</year>;<volume>22</volume>(<issue>10</issue>):<fpage>4114</fpage>–<lpage>31</lpage>. <object-id pub-id-type="pmid">12019330</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref105">
<label>105</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Woolley</surname> <given-names>SMN</given-names></name>, <name name-style="western"><surname>Fremouw</surname> <given-names>TE</given-names></name>, <name name-style="western"><surname>Hsu</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Theunissen</surname> <given-names>FE</given-names></name>. <article-title>Tuning for spectro-temporal modulations as a mechanism for auditory discrimination of natural sounds</article-title>. <source>Nature Neurosci</source>. <year>2005</year>;<volume>8</volume>(<issue>10</issue>):<fpage>1371</fpage>–<lpage>1379</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1536" xlink:type="simple">10.1038/nn1536</ext-link></comment> <object-id pub-id-type="pmid">16136039</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref106">
<label>106</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Fishbach</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Las</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Ulanovsky</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Farkas</surname> <given-names>D</given-names></name>. <article-title>Primary auditory cortex of cats: feature detection or something else?</article-title> <source>Biological Cybernetics</source>. <year>2003</year>;<volume>89</volume>(<issue>5</issue>):<fpage>397</fpage>–<lpage>406</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00422-003-0445-3" xlink:type="simple">10.1007/s00422-003-0445-3</ext-link></comment> <object-id pub-id-type="pmid">14669020</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref107">
<label>107</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Escabi</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Read</surname> <given-names>HL</given-names></name>. <article-title>Neural mechanisms for spectral analysis in the auditory midbrain, thalamus, and cortex</article-title>. <source>International review of neurobiology</source>. <year>2005</year>;<volume>70</volume>:<fpage>207</fpage>–<lpage>252</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0074-7742(05)70007-6" xlink:type="simple">10.1016/S0074-7742(05)70007-6</ext-link></comment> <object-id pub-id-type="pmid">16472636</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref108">
<label>108</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lu</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Liang</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>X</given-names></name>. <article-title>Temporal and rate representations of time-varying signals in the auditory cortex of awake primates</article-title>. <source>Nature neuroscience</source>. <year>2001</year>;<volume>4</volume>(<issue>11</issue>):<fpage>1131</fpage>–<lpage>1138</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn737" xlink:type="simple">10.1038/nn737</ext-link></comment> <object-id pub-id-type="pmid">11593234</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref109">
<label>109</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ulanovsky</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Las</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Farkas</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Nelken</surname> <given-names>I</given-names></name>. <article-title>Multiple time scales of adaptation in auditory cortex neurons</article-title>. <source>J Neurosci</source>. <year>2004</year>;<volume>24</volume>(<issue>46</issue>):<fpage>10440</fpage>–<lpage>10453</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1905-04.2004" xlink:type="simple">10.1523/JNEUROSCI.1905-04.2004</ext-link></comment> <object-id pub-id-type="pmid">15548659</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref110">
<label>110</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Moore</surname> <given-names>BCJ</given-names></name>. <source>Auditory Processing of Temporal Fine Structure: Effects of age and hearing loss</source>. <edition>1st ed</edition>. <publisher-name>World Scientific Publishing, Co</publisher-name>.; <year>2014</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref111">
<label>111</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Goldstein</surname> <given-names>JL</given-names></name>. <article-title>An optimum processor theory for the central formation of the pitch of complex tones</article-title>. <source>Journal of the Acoustical Society of America</source>. <year>1973</year>;<volume>54</volume>:<fpage>1496</fpage>–<lpage>1516</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.1914448" xlink:type="simple">10.1121/1.1914448</ext-link></comment> <object-id pub-id-type="pmid">4780803</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref112">
<label>112</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Oxenham</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Bernstein</surname> <given-names>JGW</given-names></name>, <name name-style="western"><surname>Penagos</surname> <given-names>H</given-names></name>. <article-title>Correct tonotopic representation is necessary for complex pitch perception</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2004</year>;<volume>101</volume>(<issue>5</issue>):<fpage>1421</fpage>–<lpage>1425</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0306958101" xlink:type="simple">10.1073/pnas.0306958101</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref113">
<label>113</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chi</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Ru</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Shamma</surname> <given-names>SA</given-names></name>. <article-title>Multiresolution spectrotemporal analysis of complex sounds</article-title>. <source>Journal of the Acoustical Society of America</source>. <year>2005</year>;<volume>118</volume>(<issue>2</issue>):<fpage>887</fpage>–<lpage>906</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.1945807" xlink:type="simple">10.1121/1.1945807</ext-link></comment> <object-id pub-id-type="pmid">16158645</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref114">
<label>114</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kozlov</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Gentner</surname> <given-names>TQ</given-names></name>. <article-title>Central auditory neurons have composite receptive fields</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2016</year>;<volume>113</volume>(<issue>5</issue>):<fpage>1441</fpage>–<lpage>1446</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1506903113" xlink:type="simple">10.1073/pnas.1506903113</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref115">
<label>115</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Middlebrooks</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Dykes</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Merzenich</surname> <given-names>MM</given-names></name>. <article-title>Binaural response-specific bands in primary auditory cortex (AI) of the cat: topographical organization orthogonal to isofrequency contours</article-title>. <source>Brain research</source>. <year>1980</year>;<volume>181</volume>(<issue>1</issue>):<fpage>31</fpage>–<lpage>48</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0006-8993(80)91257-3" xlink:type="simple">10.1016/0006-8993(80)91257-3</ext-link></comment> <object-id pub-id-type="pmid">7350963</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref116">
<label>116</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schreiner</surname> <given-names>CE</given-names></name>. <article-title>Spatial distribution of responses to simple and complex sounds in the primary auditory cortex</article-title>. <source>Audiology and Neuro-otology</source>. <year>1998</year>;<volume>3</volume>(<issue>2-3</issue>):<fpage>104</fpage>–<lpage>122</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1159/000013785" xlink:type="simple">10.1159/000013785</ext-link></comment> <object-id pub-id-type="pmid">9575380</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref117">
<label>117</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Elhilali</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Shamma</surname> <given-names>SA</given-names></name>. <article-title>A cocktail party with a cortical twist: how cortical mechanisms contribute to sound segregation</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>2008</year>;<volume>124</volume>(<issue>6</issue>):<fpage>3751</fpage>–<lpage>71</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.3001672" xlink:type="simple">10.1121/1.3001672</ext-link></comment> <object-id pub-id-type="pmid">19206802</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref118">
<label>118</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Klein</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Konig</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Kording</surname> <given-names>KP</given-names></name>. <article-title>Sparse spectrotemporal coding of sounds</article-title>. <source>EURASIP JApplSigProc</source>. <year>2003</year>;<volume>2003</volume>(<issue>7</issue>):<fpage>659</fpage>–<lpage>667</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref119">
<label>119</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Carlson</surname> <given-names>NL</given-names></name>, <name name-style="western"><surname>Vivienne L Ming</surname> <given-names>MRD</given-names></name>. <article-title>Sparse Codes for Speech Predict Spectrotemporal Receptive Fields in the Inferior Colliculus</article-title>. <source>PLoS CompBio</source>. <year>2012</year>;<volume>8</volume>(<issue>7</issue>):<fpage>e1002594</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002594" xlink:type="simple">10.1371/journal.pcbi.1002594</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref120">
<label>120</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hermansky</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Morgan</surname> <given-names>N</given-names></name>. <article-title>RASTA Processing of Speech</article-title>. <source>IEEE TransSpeech and Audio Process</source>. <year>1994</year>;<volume>2</volume>(<issue>4</issue>):<fpage>382</fpage>–<lpage>395</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref121">
<label>121</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nemala</surname> <given-names>SK</given-names></name>, <name name-style="western"><surname>Patil</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Elhilali</surname> <given-names>M</given-names></name>. <article-title>A Multistream Feature Framework Based on Bandpass Modulation Filtering for Robust Speech Recognition</article-title>. <source>IEEE Transactions on Audio, Speech, and Language Processing</source>. <year>2013</year>;<volume>21</volume>(<issue>2</issue>):<fpage>416</fpage>–<lpage>426</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TASL.2012.2219526" xlink:type="simple">10.1109/TASL.2012.2219526</ext-link></comment> <object-id pub-id-type="pmid">29928166</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref122">
<label>122</label>
<mixed-citation publication-type="other" xlink:type="simple">Pearse D, Hirsch H. The Aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions. In: ICSLP 2000 (6th International Conference on Spokel Language Processing). vol. 6; 2000. p. 16–19.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref123">
<label>123</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>van Noorden</surname> <given-names>LP</given-names></name>, <name name-style="western"><surname>van Noorden</surname> <given-names>LP</given-names></name>. <source>Temporal coherence in the perception of tone sequences</source>; <year>1975</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref124">
<label>124</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Blake</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>SH</given-names></name>. <article-title>The role of temporal structure in human vision</article-title>. <source>Behavioral and cognitive neuroscience review</source>. <year>2005</year>;<volume>4</volume>(<issue>1</issue>):<fpage>21</fpage>–<lpage>42</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/1534582305276839" xlink:type="simple">10.1177/1534582305276839</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref125">
<label>125</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>O’Sullivan</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Shamma</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Lalor</surname> <given-names>EC</given-names></name>. <article-title>Evidence for Neural Computations of Temporal Coherence in an Auditory Scene and Their Enhancement during Active Listening</article-title>. <source>The Journal of neuroscience</source>. <year>2015</year>;<volume>35</volume>(<issue>18</issue>):<fpage>7256</fpage>–<lpage>7263</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4973-14.2015" xlink:type="simple">10.1523/JNEUROSCI.4973-14.2015</ext-link></comment> <object-id pub-id-type="pmid">25948273</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref126">
<label>126</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Teki</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Barascud</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Picard</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Payne</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Griffiths</surname> <given-names>TD</given-names></name>, <name name-style="western"><surname>Chait</surname> <given-names>M</given-names></name>. <article-title>Neural Correlates of Auditory Figure-Ground Segregation Based on Temporal Coherence</article-title>. <source>Cerebral cortex</source>. <year>2016</year>;<volume>26</volume>(<issue>9</issue>):<fpage>3669</fpage>–<lpage>3680</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhw173" xlink:type="simple">10.1093/cercor/bhw173</ext-link></comment> <object-id pub-id-type="pmid">27325682</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref127">
<label>127</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Liang</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Bressler</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Ding</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Desimone</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Fries</surname> <given-names>P</given-names></name>. <article-title>Temporal dynamics of attention-modulated neuronal synchronization in macaque V4</article-title>. <source>Neurocomputing</source>. <year>2003</year>;<volume>52-54</volume>:<fpage>481</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0925-2312(02)00741-5" xlink:type="simple">10.1016/S0925-2312(02)00741-5</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref128">
<label>128</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zeitler</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fries</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Gielen</surname> <given-names>S</given-names></name>. <article-title>Assessing neuronal coherence with single-unit, multi-unit, and local field potentials</article-title>. <source>Neural Comp</source>. <year>2006</year>;<volume>18</volume>(<issue>9</issue>):<fpage>2256</fpage>–<lpage>2281</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/neco.2006.18.9.2256" xlink:type="simple">10.1162/neco.2006.18.9.2256</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref129">
<label>129</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Beauvois</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Meddis</surname> <given-names>R</given-names></name>. <article-title>Computer simulation of auditory stream segregation in alternating-tone sequences</article-title>. <source>The Journal of the Acoustical Society of America</source>. <year>1996</year>;<volume>99</volume>(<issue>4</issue>):<fpage>2270</fpage>–<lpage>2280</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.415414" xlink:type="simple">10.1121/1.415414</ext-link></comment> <object-id pub-id-type="pmid">8730073</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref130">
<label>130</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bee</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Klump</surname> <given-names>GM</given-names></name>. <article-title>Primitive auditory stream segregation: a neurophysiological study in the songbird forebrain</article-title>. <source>Journal of neurophysiology</source>. <year>2004</year>;<volume>92</volume>(<issue>2</issue>):<fpage>1088</fpage>–<lpage>1104</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1152/jn.00884.2003" xlink:type="simple">10.1152/jn.00884.2003</ext-link></comment> <object-id pub-id-type="pmid">15044521</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref131">
<label>131</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pressnitzer</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Sayles</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Micheyl</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Winter</surname> <given-names>IM</given-names></name>. <article-title>Perceptual organization of sound begins in the auditory periphery</article-title>. <source>Current Biology</source>. <year>2008</year>;<volume>18</volume>(<issue>15</issue>):<fpage>1124</fpage>–<lpage>1128</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cub.2008.06.053" xlink:type="simple">10.1016/j.cub.2008.06.053</ext-link></comment> <object-id pub-id-type="pmid">18656355</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref132">
<label>132</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Micheyl</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Carlyon</surname> <given-names>RP</given-names></name>, <name name-style="western"><surname>Gutschalk</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Melcher</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Oxenham</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Rauschecker</surname> <given-names>JP</given-names></name>, <etal>et al</etal>. <article-title>The role of auditory cortex in the formation of auditory streams</article-title>. <source>Hearing Research</source>. <year>2007</year>;<volume>229</volume>(<issue>1-2</issue>):<fpage>116</fpage>–<lpage>131</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.heares.2007.01.007" xlink:type="simple">10.1016/j.heares.2007.01.007</ext-link></comment> <object-id pub-id-type="pmid">17307315</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref133">
<label>133</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Elhilali</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Micheyl</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Oxenham</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Shamma</surname> <given-names>SA</given-names></name>. <chapter-title>Rate vs. temporal code? A spatio-temporal coherence model of the cortical basis of streaming</chapter-title>. In: <name name-style="western"><surname>Lopez-Poveda</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Palmer</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Meddis</surname> <given-names>R</given-names></name>, editors. <source>Auditory Physiology, Perception and Models</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2010</year>. p. <fpage>497</fpage>–<lpage>506</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref134">
<label>134</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Roberts</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Glasberg</surname> <given-names>BR</given-names></name>, <name name-style="western"><surname>Moore</surname> <given-names>BCJ</given-names></name>. <article-title>Effects of the build-up and resetting of auditory stream segregation on temporal discrimination</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>. <year>2007</year>;<volume>34</volume>(<issue>4</issue>):<fpage>992</fpage>–<lpage>1006</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref135">
<label>135</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Haywood</surname> <given-names>NR</given-names></name>, <name name-style="western"><surname>Roberts</surname> <given-names>B</given-names></name>. <article-title>Build-up of the tendency to segregate auditory streams: Resetting effects evoked by a single deviant tone</article-title>. <source>Journal of the Acoustical Society of America</source>. <year>2010</year>;<volume>128</volume>(<issue>5</issue>):<fpage>3019</fpage>–<lpage>3031</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.3488675" xlink:type="simple">10.1121/1.3488675</ext-link></comment> <object-id pub-id-type="pmid">21110597</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref136">
<label>136</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Deike</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Heil</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Böckmann-Barthel</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Brechmann</surname> <given-names>A</given-names></name>. <article-title>The build-up of auditory stream segregation: a different perspective</article-title>. <source>Frontiers in Psychology</source>. <year>2012</year>;<volume>3</volume>:<fpage>1</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2012.00461" xlink:type="simple">10.3389/fpsyg.2012.00461</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref137">
<label>137</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Ngo</surname> <given-names>TD</given-names></name>. <source>Biomimetic Technologies: Principles and Applications</source>. <publisher-name>Woodhead Publishing</publisher-name>; <year>2015</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref138">
<label>138</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Liu</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Zeng</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Alsaadi</surname> <given-names>FE</given-names></name>. <article-title>A survey of deep neural network architectures and their applications</article-title>. <source>Neurocomputing</source>. <year>2017</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref139">
<label>139</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cho</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Raiko</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Ilin</surname> <given-names>A</given-names></name>. <article-title>Enhanced Gradient for Training Restricted Boltzmann Machines</article-title>. <source>Neural Computation</source>. <year>2013</year>;<volume>25</volume>(<issue>3</issue>):<fpage>805</fpage>–<lpage>831</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/NECO_a_00397" xlink:type="simple">10.1162/NECO_a_00397</ext-link></comment> <object-id pub-id-type="pmid">23148412</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref140">
<label>140</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tsodyks</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Pawelzik</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Markram</surname> <given-names>H</given-names></name>. <article-title>Neural Networks with Dynamic Synapses</article-title>. <source>Neural computation</source>. <year>1998</year>;<volume>10</volume>(<issue>4</issue>):<fpage>821</fpage>–<lpage>835</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089976698300017502" xlink:type="simple">10.1162/089976698300017502</ext-link></comment> <object-id pub-id-type="pmid">9573407</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref141">
<label>141</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Garofolo</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Lamel</surname> <given-names>LF</given-names></name>, <name name-style="western"><surname>Fisher</surname> <given-names>WM</given-names></name>, <name name-style="western"><surname>Fiscus</surname> <given-names>JG</given-names></name>, <name name-style="western"><surname>Pallett</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Dahlgren</surname> <given-names>NL</given-names></name>, <etal>et al</etal>. <source>DARPA TIMIT Acoustic Phonetic Continuous Speech Corpus</source>; <year>1993</year>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref142">
<label>142</label>
<mixed-citation publication-type="other" xlink:type="simple">BBC. The BBC Sound Effects Library; 1990.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref143">
<label>143</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chi</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Gao</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Guyton</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Ru</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Shamma</surname> <given-names>S</given-names></name>. <article-title>Spectro-temporal modulation transfer functions and speech intelligibility</article-title>. <source>Journal of the Acoustical Society of America</source>. <year>1999</year>;<volume>106</volume>(<issue>5</issue>):<fpage>2719</fpage>–<lpage>2732</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1121/1.428100" xlink:type="simple">10.1121/1.428100</ext-link></comment> <object-id pub-id-type="pmid">10573888</object-id></mixed-citation>
</ref>
<ref id="pcbi.1006711.ref144">
<label>144</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Rokach</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Maimon</surname> <given-names>O</given-names></name>. <chapter-title>Clustering Methods</chapter-title>. In: <name name-style="western"><surname>Maimon</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Rokach</surname> <given-names>L</given-names></name>, editors. <source>Data Mining and Knowledge Discovery Handbook</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>; <year>2005</year>. p. <fpage>321</fpage>–<lpage>352</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1006711.ref145">
<label>145</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Reynolds</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Rose</surname> <given-names>RC</given-names></name>. <article-title>Robust Text-Independent Speaker Identification Using Gaussian Mixture Speaker Models</article-title>. <source>IEEE Transactions on Speech and Audio Processing</source>. <year>1995</year>;<volume>3</volume>(<issue>1</issue>):<fpage>72</fpage>–<lpage>83</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/89.365379" xlink:type="simple">10.1109/89.365379</ext-link></comment></mixed-citation>
</ref>
</ref-list>
</back>
</article>