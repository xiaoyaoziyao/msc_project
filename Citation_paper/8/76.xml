<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pone.0196391</article-id>
<article-id pub-id-type="publisher-id">PONE-D-17-28472</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Speech</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Face</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Face</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject><subj-group><subject>Fear</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject><subj-group><subject>Fear</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Research assessment</subject><subj-group><subject>Research validity</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Music cognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Music cognition</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Music cognition</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Musculoskeletal system</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Musculoskeletal system</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Animal signaling and communication</subject><subj-group><subject>Vocalization</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Animal signaling and communication</subject><subj-group><subject>Vocalization</subject></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English</article-title>
<alt-title alt-title-type="running-head">The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6364-6410</contrib-id>
<name name-style="western">
<surname>Livingstone</surname>
<given-names>Steven R.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Russo</surname>
<given-names>Frank A.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Resources</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Psychology, Ryerson University, Toronto, Canada</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Department of Computer Science and Information Systems, University of Wisconsin-River Falls, Wisconsin, WI, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Najbauer</surname>
<given-names>Joseph</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Pécs Medical School, HUNGARY</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The second author holds a research chair sponsored by a commercial source: SONOVA/Phonak. Research funding related to the chair partly supported the development of the database presented in this paper. The agreement with the commercial sponsor does not entail restrictions on sharing of data and/or materials, and does not alter our adherence to PLOS ONE policies on sharing data and materials. In addition, neither of the authors are or have been on the editorial board of PLOS ONE, nor acted as an expert witness in relevant legal proceedings, nor sat or currently sit on a committee for an organization that may benefit from publication of the paper. Both authors declare, that to the best of their knowledge, there are no other competing interests.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">steven.livingstone@uwrf.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>16</day>
<month>5</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<year>2018</year>
</pub-date>
<volume>13</volume>
<issue>5</issue>
<elocation-id>e0196391</elocation-id>
<history>
<date date-type="received">
<day>31</day>
<month>7</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>12</day>
<month>4</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Livingstone, Russo</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0196391"/>
<abstract>
<p>The RAVDESS is a validated multimodal database of emotional speech and song. The database is gender balanced consisting of 24 professional actors, vocalizing lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity, with an additional neutral expression. All conditions are available in face-and-voice, face-only, and voice-only formats. The set of 7356 recordings were each rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity and test-retest intrarater reliability were reported. Corrected accuracy and composite "goodness" measures are presented to assist researchers in the selection of stimuli. All recordings are made freely available under a Creative Commons license and can be downloaded at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1188976" xlink:type="simple">https://doi.org/10.5281/zenodo.1188976</ext-link>.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100002790</institution-id>
<institution>Canadian Network for Research and Innovation in Machining Technology, Natural Sciences and Engineering Research Council of Canada</institution>
</institution-wrap>
</funding-source>
<award-id>2012-341583</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Russo</surname>
<given-names>Frank A.</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100004228</institution-id>
<institution>Phonak</institution>
</institution-wrap>
</funding-source>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6364-6410</contrib-id>
<name name-style="western">
<surname>Livingstone</surname>
<given-names>Steven R.</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was supported by FAR—Discovery Grant (2012-341583) from the Natural Sciences and Engineering Research Council of Canada, <ext-link ext-link-type="uri" xlink:href="http://www.nserc-crsng.gc.ca/" xlink:type="simple">http://www.nserc-crsng.gc.ca/</ext-link>; FAR—Hear the world research chair in music and emotional speech from Phonak, <ext-link ext-link-type="uri" xlink:href="https://www.phonak.com" xlink:type="simple">https://www.phonak.com</ext-link>.</funding-statement>
</funding-group>
<counts>
<fig-count count="4"/>
<table-count count="7"/>
<page-count count="35"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The study of emotion has advanced rapidly over the last decade, driven by low-cost smart technologies and broad interest from researchers in neuroscience, psychology, psychiatry, audiology, and computer science. Integral to these studies is the availability of validated and reliable expressions of emotion. To meet these needs, a growing number of emotion stimulus sets have become available. Most sets contain either static facial expressions or voice recordings. Few contain audiovisual recordings of speakers in North American English. Clinically, there is growing recognition for the role of singing in understanding neurological disorders and facilitating rehabilitation. Yet there are few validated sets of sung emotional expression. To address these needs, we developed the RAVDESS, a large validated set of audiovisual speech and song in North American English. This paper describes the creation of the RAVDESS, and reports validity and reliability data based on ratings from healthy, adult participants.</p>
<sec id="sec002">
<title>The importance of multimodal communication</title>
<p>A trend in emotion research has been the use of affective stimuli that depicts emotion in a single modality, primarily through facial expressions. However, in the natural world emotional communication is temporal and multimodal. Studies have highlighted the importance of multisensory integration when processing affective stimuli [<xref ref-type="bibr" rid="pone.0196391.ref001">1</xref>–<xref ref-type="bibr" rid="pone.0196391.ref013">13</xref>]. The absence of validated multimodal sets has motivated researchers to create their own multimodal stimuli [<xref ref-type="bibr" rid="pone.0196391.ref005">5</xref>, <xref ref-type="bibr" rid="pone.0196391.ref007">7</xref>, <xref ref-type="bibr" rid="pone.0196391.ref014">14</xref>–<xref ref-type="bibr" rid="pone.0196391.ref018">18</xref>]. Researchers have also created multimodal stimuli by combining two independent unimodal sets [<xref ref-type="bibr" rid="pone.0196391.ref019">19</xref>], or joining self-created stimuli with an existing unimodal set [<xref ref-type="bibr" rid="pone.0196391.ref020">20</xref>]. This ad hoc approach may complicate the comparison of findings across studies, as each set varies in features, technical quality, and expressive intensity. Thus, divergent findings may be partially attributable to variations in stimulus sets.</p>
</sec>
<sec id="sec003">
<title>The need for dynamic facial expressions</title>
<p>Normal conversation contains a variety of expressions, and faces are rarely, if ever, static. Yet most sets contain only static facial images [<xref ref-type="bibr" rid="pone.0196391.ref021">21</xref>–<xref ref-type="bibr" rid="pone.0196391.ref033">33</xref>]. There is now substantial evidence that facial movement facilitates affective processing [<xref ref-type="bibr" rid="pone.0196391.ref034">34</xref>–<xref ref-type="bibr" rid="pone.0196391.ref044">44</xref>]. Imaging studies have revealed that dynamic expressions evoke differential and enhanced patterns of neural activation relative to static expressions [<xref ref-type="bibr" rid="pone.0196391.ref045">45</xref>–<xref ref-type="bibr" rid="pone.0196391.ref047">47</xref>]. Electromyographic studies have shown that dynamic stimuli elicit larger mimicry responses in the facial muscles of observers than those elicited by static expressions [<xref ref-type="bibr" rid="pone.0196391.ref048">48</xref>, <xref ref-type="bibr" rid="pone.0196391.ref049">49</xref>]. Thus, dynamic facial expressions may provide a more ecologically valid representation of emotion than static facial expressions.</p>
</sec>
<sec id="sec004">
<title>Distinguishing features of the RAVDESS</title>
<p>There are five distinguishing features of the RAVDESS that build on popular existing sets.</p>
<sec id="sec005">
<title>Scope</title>
<p>First, whereas many sets contain fewer than 200 clips [<xref ref-type="bibr" rid="pone.0196391.ref021">21</xref>–<xref ref-type="bibr" rid="pone.0196391.ref024">24</xref>, <xref ref-type="bibr" rid="pone.0196391.ref026">26</xref>, <xref ref-type="bibr" rid="pone.0196391.ref028">28</xref>, <xref ref-type="bibr" rid="pone.0196391.ref030">30</xref>, <xref ref-type="bibr" rid="pone.0196391.ref050">50</xref>], the RAVDESS contains 7356 clips. The factorial design of the RAVDESS is visualized in <xref ref-type="supplementary-material" rid="pone.0196391.s001">S1</xref> and <xref ref-type="supplementary-material" rid="pone.0196391.s002">S2</xref> Figs. To our knowledge, only three other sets contain over 1000 recordings of dynamic, multimodal conversation [<xref ref-type="bibr" rid="pone.0196391.ref051">51</xref>–<xref ref-type="bibr" rid="pone.0196391.ref053">53</xref>]. The RAVDESS consists of 24 professional actors, each performing 104 unique vocalizations with emotions that include: happy, sad, angry, fearful, surprise, disgust, calm, and neutral. Each recorded production of an actor is available in three modality formats: audio-visual (AV), video-only (VO), and audio-only (AO). This diversity may be useful in repeated measures designs, as imaging studies have shown that key brain structures habituate to repeated presentations of the same stimulus [<xref ref-type="bibr" rid="pone.0196391.ref054">54</xref>, <xref ref-type="bibr" rid="pone.0196391.ref055">55</xref>]. A large corpus of recordings is also useful for researchers in machine learning. The validated database is particularly well suited to machine learning approaches involving supervised learning, such as emotion classifiers [<xref ref-type="bibr" rid="pone.0196391.ref056">56</xref>], as they provide a large set for training and testing different algorithms.</p>
</sec>
<sec id="sec006">
<title>Emotional intensity</title>
<p>Second, all emotions have been performed at two levels of emotional intensity, normal and strong. To our knowledge, only two other sets provide a controlled manipulation of intensity [<xref ref-type="bibr" rid="pone.0196391.ref057">57</xref>, <xref ref-type="bibr" rid="pone.0196391.ref058">58</xref>]. Intensity is one of the most salient aspects of emotion [<xref ref-type="bibr" rid="pone.0196391.ref059">59</xref>], and has a prominent role in several theories of emotion [<xref ref-type="bibr" rid="pone.0196391.ref060">60</xref>–<xref ref-type="bibr" rid="pone.0196391.ref064">64</xref>]; note, the terms ‘intensity’ and ‘activation’ have been used interchangeably in these works. In these models, intensity often forms one of several orthogonal axes in a multidimensional emotional space. Perceptually, intense facial and vocal expressions are identified more accurately than their less intense counterparts [<xref ref-type="bibr" rid="pone.0196391.ref065">65</xref>, <xref ref-type="bibr" rid="pone.0196391.ref066">66</xref>]. Intense facial expressions are also identified more quickly than their less intense counterparts [<xref ref-type="bibr" rid="pone.0196391.ref067">67</xref>], and elicit stronger facial mimicry responses in observers [<xref ref-type="bibr" rid="pone.0196391.ref068">68</xref>]. Thus, intense displays may be useful when researchers seek clear, unambiguous emotional exemplars. In contrast, normal intensity expressions may be required when investigating subtle differences in emotional perception [<xref ref-type="bibr" rid="pone.0196391.ref069">69</xref>], or for researchers seeking portrayals similar to those found in everyday life.</p>
</sec>
<sec id="sec007">
<title>Two baseline emotions</title>
<p>Third, the RAVDESS includes two baseline emotions, neutral and calm. Many studies incorporate a neutral or “no emotion” control condition. However, neutral expressions have produced mixed perceptual results [<xref ref-type="bibr" rid="pone.0196391.ref070">70</xref>], at times conveying a negative emotional valence [<xref ref-type="bibr" rid="pone.0196391.ref071">71</xref>]. Researchers have suggested that this may be due to uncertainty on the part of the performer as to how neutral should be conveyed [<xref ref-type="bibr" rid="pone.0196391.ref066">66</xref>]. To compensate for this a calm baseline condition has been included, which is perceptually like neutral, but may be perceived as having a mild positive valence. To our knowledge, the calm expression is not contained in any other set of dynamic conversational expressions, and is present in one static facial image set [<xref ref-type="bibr" rid="pone.0196391.ref029">29</xref>].</p>
</sec>
<sec id="sec008">
<title>North-American actors and raters</title>
<p>Fourth, the RAVDESS provides audiovisual recordings of vocal communication in North American English. Three existing sets present validated, audiovisual expressions of vocal emotional communication: the Geneva Multimodal Emotion Portrayal (GEMEP) [<xref ref-type="bibr" rid="pone.0196391.ref057">57</xref>, <xref ref-type="bibr" rid="pone.0196391.ref072">72</xref>], CREMA-D [<xref ref-type="bibr" rid="pone.0196391.ref052">52</xref>], and MSP-IMPROV [<xref ref-type="bibr" rid="pone.0196391.ref051">51</xref>]. The GEMEP consists of 10 French-speaking actors, expressing a range of emotions, at three levels of intensity, in three vocal conditions (improvised sentences, pseudo-speech, and nonverbal affect bursts). The GEMEP is an exemplary and detailed set. However, the geographic origin of the GEMEP may pose issues for researchers in North America.</p>
<p>The pseudo-speech and improvised sentences of the GEMEP are spoken with a French accent. This may be unsuitable for researchers who require vocal content from the same geographic region or language as their participants. The facial expressions of the GEMEP actors may also signal a different geographical region due to the presence of nonverbal accents [<xref ref-type="bibr" rid="pone.0196391.ref073">73</xref>]. These accents can be subtle enough to distinguish cultures that share a common language, such as Australia and the United States [<xref ref-type="bibr" rid="pone.0196391.ref074">74</xref>]. Finally, the GEMEP stimuli have been validated by individuals of the same cultural region as the GEMEP actors. However, there is significant evidence of an ‘in-group’ advantage for emotional recognition, where accuracy is higher for emotions expressed and recognized by members of the same cultural group [<xref ref-type="bibr" rid="pone.0196391.ref075">75</xref>]. Reported accuracy rates of the GEMEP stimuli may differ when used with North American participants.</p>
<p>The CREMA-D consists of 91 English-speaking actors, expressing six spoken emotions. One sentence was produced at three levels of intensity, the other 11 sentences with unspecified intensity. This extensive set of 7442 recordings was validated by 2443 raters using crowd-sourced participants (Survey Sampling International) in an Internet-presented format, providing approximately 10 ratings per clip.</p>
<p>The MSP-IMPROV consists of 12 English-speaking actors, expressing four spoken emotions in a novel dyadic-conversational scenario. Fifteen sentences were produced with unspecified intensity. This large set of 7818 recordings was validated by over 50000 raters using crowd-sourced participants (Amazon Mechanical Turk) in an Internet-presented format. A core set of 652 clips were each rated 28 times, while remaining clips were each rated approximately 5 times.</p>
<p>A fourth set, the eNTERFACE’05, also provides audiovisual expressions of vocal emotional communication [<xref ref-type="bibr" rid="pone.0196391.ref076">76</xref>]. The set consists of 42 English-speaking lay-expressers from different countries, expressing six emotions in scenario-elicited format. Five distinct sentences for each emotion were produced with unspecified intensity. Recordings were included based on the judgements of two trained investigators. However, no measures of accuracy or reliability were provided. As such, the set cannot be assessed and compared against the performance of the RAVDESS or other existing sets.</p>
<p>The RAVDESS was designed for researchers and participants located in North America. It consists of 24 English-speaking actors, drawn from the Toronto area of Ontario, Canada. The 319 raters chosen to evaluate the RAVDESS stimuli were drawn from the same region.</p>
</sec>
<sec id="sec009">
<title>Singing corpus</title>
<p>The final distinctive feature of the RAVDESS is that it includes a validated corpus of emotional song. Music is increasingly being used to understand cognitive and neural function in healthy and disordered populations [<xref ref-type="bibr" rid="pone.0196391.ref077">77</xref>–<xref ref-type="bibr" rid="pone.0196391.ref083">83</xref>]. Music has been used as a nonpharmacological treatment in the rehabilitation of neurological and motor disorders [<xref ref-type="bibr" rid="pone.0196391.ref084">84</xref>], including: cognitive recovery following stroke [<xref ref-type="bibr" rid="pone.0196391.ref077">77</xref>], mood improvement in depression [<xref ref-type="bibr" rid="pone.0196391.ref085">85</xref>], reduction of anxiety in obsessive compulsive disorder [<xref ref-type="bibr" rid="pone.0196391.ref086">86</xref>], recognition of speech emotion in children with cochlear implants [<xref ref-type="bibr" rid="pone.0196391.ref087">87</xref>], language function in aphasia [<xref ref-type="bibr" rid="pone.0196391.ref088">88</xref>], and motor rehabilitation in Parkinson’s disease [<xref ref-type="bibr" rid="pone.0196391.ref089">89</xref>, <xref ref-type="bibr" rid="pone.0196391.ref090">90</xref>]. The RAVDESS offers clinical therapists a set of validated expressions of sung musical emotion from which to develop rehabilitative and diagnostic options. The RAVDESS is lexically-matched in song and speech. This feature may be beneficial for understanding processing differences in speech and song, or for examining disorders in which speech-music overlaps play a central role [<xref ref-type="bibr" rid="pone.0196391.ref091">91</xref>–<xref ref-type="bibr" rid="pone.0196391.ref093">93</xref>]. Specifically, the use of lexically matched utterances removes a confounding factor in studies seeking to compare speech with song or music [<xref ref-type="bibr" rid="pone.0196391.ref094">94</xref>, <xref ref-type="bibr" rid="pone.0196391.ref095">95</xref>].</p>
</sec>
</sec>
<sec id="sec010">
<title>Creation and validation of a new multimodal set</title>
<p>In the following sections, we present validation and reliability data in support of the RAVDESS. For the validation task, 247 participants each rated a subset of the 7356 files. For the reliability task, a further 72 participants provided intra-participant test-retest data. Validation was achieved by asking participants to label the expressed emotion. In several existing databases of facial emotion, an alternate rating method of validation has been implemented using a limited number of highly-trained participants to identify specific facial muscle contractions, or action units, which are then used to indicate a target emotion [<xref ref-type="bibr" rid="pone.0196391.ref096">96</xref>–<xref ref-type="bibr" rid="pone.0196391.ref098">98</xref>]. These systems were developed for nonverbal expressions of emotion, which involve relatively still faces. In contrast, vocal production involves significant orofacial movement, where movements tied to lexical content interact with movements related to emotional expression [<xref ref-type="bibr" rid="pone.0196391.ref013">13</xref>]. Thus, traditional muscle coding systems are unsuitable for validating the RAVDESS.</p>
<p>The validity task presents measures of emotional accuracy, intensity, and genuineness for all stimuli. These data, presented in <xref ref-type="supplementary-material" rid="pone.0196391.s004">S1 Table</xref>, provide a granular view of the RAVDESS stimuli. To assist researchers in the selection of appropriate stimuli, we include a composite “goodness” score, see also [<xref ref-type="bibr" rid="pone.0196391.ref033">33</xref>]. Goodness scores range between 0 and 10, and are a weighted sum of mean accuracy, intensity, and genuineness measures. The equation is defined such that stimuli receiving higher measures of accuracy, intensity, and genuineness, are assigned higher goodness scores.</p>
</sec>
</sec>
<sec id="sec011" sec-type="materials|methods">
<title>Method</title>
<sec id="sec012">
<title>Ethics declaration</title>
<p>The RAVDESS and validation experiment used human volunteers. Informed written consent was obtained prior to any experiment or recording from all participants. Facial images of several actors are displayed later in this manuscript. These individuals gave written informed consent, as outlined in the PLOS consent form, to publish these case details. Participants and data from participants were treated according to the Declaration of Helsinki. The recording methods of the database and the subsequent validation experiment were approved by the local ethics committee of Ryerson University, Canada.</p>
</sec>
<sec id="sec013">
<title>Development of the RAVDESS stimuli</title>
<sec id="sec014">
<title>Actors</title>
<p>Twenty-four professional actors, working in Toronto, Ontario, Canada were hired for stimulus creation (M = 26.0 years; SD = 3.75; age range = 21–33; 12 males and 12 females). Actors self-identified as Caucasian (N = 20), East-Asian (N = 2), and Mixed (N = 2, East-Asian Caucasian, and Black-Canadian First nations Caucasian). To be eligible, actors needed to have English as their first language, speak with a neutral North American accent, and to not possess any distinctive features (e.g., beards, facial tattoos, hair colorings, facial piercings). Participants were also required to identify text presented at 1.5 m distance without wearing glasses.</p>
<p>Professional actors were selected over lay expressers for several reasons. Studies have shown that actor portrayals of emotion are identified more readily than those of lay expressers [<xref ref-type="bibr" rid="pone.0196391.ref099">99</xref>]. While one recent study found that vocal expressions of actors are only marginally more accurate than those of lay-expressers [<xref ref-type="bibr" rid="pone.0196391.ref100">100</xref>], it is unknown if the same holds true for facial expressions or dynamic audio-visual expressions. A growing number of emotion sets have successfully used professional or trained actors [<xref ref-type="bibr" rid="pone.0196391.ref029">29</xref>, <xref ref-type="bibr" rid="pone.0196391.ref032">32</xref>, <xref ref-type="bibr" rid="pone.0196391.ref033">33</xref>, <xref ref-type="bibr" rid="pone.0196391.ref050">50</xref>, <xref ref-type="bibr" rid="pone.0196391.ref057">57</xref>]. As with the RAVDESS, the creation of FACS-posed expressions was not the goal of these sets. Finally, the use of trained individuals is common in psychological tasks, such as music performance [<xref ref-type="bibr" rid="pone.0196391.ref101">101</xref>]. Actors are often recruited for studies of emotional expression [<xref ref-type="bibr" rid="pone.0196391.ref095">95</xref>], as they have received extensive training on the realistic portrayal of emotion.</p>
<p>The Toronto accent is a good example of the Standard North American English commonly found in Hollywood movies. The most notable exception is what has come to be known as Canadian raising, whereby diphthongs are raised when occurring before a voiceless consonant. For example, the /aʊ/ found in “house” or “about” will be somewhat raised to /^ʊ/. Canadian raising can be found in most parts of Canada, as well as northeastern New England, the Pacific Northwest, and the Upper Midwest. Critically, this accent feature is not prominent in the Toronto region and it is not found in the RAVDESS stimulus statements.</p>
</sec>
<sec id="sec015">
<title>Stimuli</title>
<p>Two neutral statements were used (“Kids are talking by the door”, “Dogs are sitting by the door”). Statements were seven syllables in length and were matched in word frequency and familiarity using the MRC psycholinguistic database [<xref ref-type="bibr" rid="pone.0196391.ref102">102</xref>]. For the singing trials, statements were associated with melodies that were sounded using piano MIDI tones of fixed acoustic intensity, consisting of six eighth notes (300 ms) and ending with a quarter note (600 ms). The tonality of melodies associated with each emotion was tailored to be consistent with emotional association [<xref ref-type="bibr" rid="pone.0196391.ref103">103</xref>, <xref ref-type="bibr" rid="pone.0196391.ref104">104</xref>]. The melody associated with the positively valenced emotions calm and happy was in the major mode (F3, F3, A3, A3, F3, E3, F3). The melody associated with the negatively valenced emotions sad, angry, and fearful was in the minor mode (F3, F3, A<sup>b</sup>3, A<sup>b</sup>3, F3, E3, F3). The melody associated with neutral emotion did not contain the third scale degree (F3, F3, G3, G3, F3, E3, F3) and was designed to be ambiguous in terms of major or minor mode.</p>
<p>The perceived valence of song melodies was validated in a separate a perceptual task. Eight participants (5 female, 3 male, mean age = 27.4, SD = 9.2), from Ryerson University, Toronto volunteered to participate. Raters had varied amounts of private musical instruction (mean = 9.0 years, SD = 7.1). Participants were asked to rate the perceived valence of each of the three melodies (major-mode, neutral, minor-mode), using a 9-point valence scale from the self-assessment-manikin (SAM) [<xref ref-type="bibr" rid="pone.0196391.ref105">105</xref>]. Results confirmed that the major-mode melody (M = 7.88, SD = 1.13) was rated as more positive than the neutral melody (M = 5.13, SD = 1.55), which in turn was rated as more positive than the negative melody (M = 3.0, SD = 1.77).</p>
<p>The stimulus timeline consisted of three main epochs: Task presentation (4500 ms), Count-in (2400 ms), and Vocalization (4800 ms). In the Task presentation epoch, the statement and emotion to be produced by the vocalist were presented on screen as text for 4500 ms. In the song condition, the melody to be used by the vocalist was sounded (2400 ms) after the text had been on screen for 1000ms. The Count-in epoch presented a visual count-in timer (‘1’, ‘2’, ‘3’, ‘4’) at an interonset interval of 600ms. The start of the Vocalization epoch was signaled with a green circle that was displayed for 2400 ms. The stimulus timeline began with an auditory beep (500 ms) and 1000ms of silence, and ended with an auditory beep (500 ms). The total duration of the stimulus trial was 13700 ms.</p>
</sec>
<sec id="sec016">
<title>Selection of emotions</title>
<p>Eight emotions were selected for speech: neutral, calm, happy, sad, angry, fearful, surprise, and disgust. Calm and neutral were selected as baseline conditions, while the remaining states constitute the set of six basic or fundamental emotions that are thought to be culturally universal [<xref ref-type="bibr" rid="pone.0196391.ref106">106</xref>]. The concept of primary emotions has a long history in science and philosophy [<xref ref-type="bibr" rid="pone.0196391.ref107">107</xref>–<xref ref-type="bibr" rid="pone.0196391.ref109">109</xref>], with modern proponents [<xref ref-type="bibr" rid="pone.0196391.ref110">110</xref>–<xref ref-type="bibr" rid="pone.0196391.ref112">112</xref>]. While the discrete model of emotion has been criticized [<xref ref-type="bibr" rid="pone.0196391.ref113">113</xref>–<xref ref-type="bibr" rid="pone.0196391.ref115">115</xref>], it is a practical choice in the creation and labelling of emotion sets. Consequently, these six emotion labels can be found in most existing sets [<xref ref-type="bibr" rid="pone.0196391.ref021">21</xref>, <xref ref-type="bibr" rid="pone.0196391.ref024">24</xref>–<xref ref-type="bibr" rid="pone.0196391.ref027">27</xref>, <xref ref-type="bibr" rid="pone.0196391.ref029">29</xref>–<xref ref-type="bibr" rid="pone.0196391.ref031">31</xref>, <xref ref-type="bibr" rid="pone.0196391.ref050">50</xref>, <xref ref-type="bibr" rid="pone.0196391.ref057">57</xref>, <xref ref-type="bibr" rid="pone.0196391.ref116">116</xref>–<xref ref-type="bibr" rid="pone.0196391.ref119">119</xref>]. The categorization of surprise as a basic emotion has been questioned by some theorists [<xref ref-type="bibr" rid="pone.0196391.ref114">114</xref>], while others have argued for its inclusion as a primary emotion [<xref ref-type="bibr" rid="pone.0196391.ref112">112</xref>]. As the debate remains unsettled, and as surprise is included in many existing sets, surprise was included in the speech set of the RAVDESS.</p>
<p>For song, six emotions were selected: neutral, calm, happy, sad, angry, and fearful. These emotions were selected as they are representative of expressions often conveyed in music [<xref ref-type="bibr" rid="pone.0196391.ref104">104</xref>, <xref ref-type="bibr" rid="pone.0196391.ref120">120</xref>, <xref ref-type="bibr" rid="pone.0196391.ref121">121</xref>]. Surprise and disgust were not included as they are rarely expressed in music and exhibit poor rates of reliability in listener studies [<xref ref-type="bibr" rid="pone.0196391.ref122">122</xref>–<xref ref-type="bibr" rid="pone.0196391.ref124">124</xref>].</p>
</sec>
<sec id="sec017">
<title>Emotional elicitation</title>
<p>The RAVDESS was created using induced emotional expressions. These expressions have been variously referred to as simulated, posed, portrayed, enacted, instructed, or “felt experience acting” [<xref ref-type="bibr" rid="pone.0196391.ref057">57</xref>, <xref ref-type="bibr" rid="pone.0196391.ref125">125</xref>–<xref ref-type="bibr" rid="pone.0196391.ref127">127</xref>]. In this type of elicitation procedure, actors used trained techniques to induce the desired emotional state prior to expression.</p>
<p>In the RAVDESS, actors were told that they could use whatever techniques they were trained in to induce the desired state, such as method acting or Stanislavski’s emotional memory techniques [<xref ref-type="bibr" rid="pone.0196391.ref128">128</xref>]. Actors were told that they would be given as much time as was needed for them to enter the desired emotional state, and that once achieved, they would signal their readiness. It was emphasized that actors were to provide genuine expressions of emotion, and that they were to be physically and mentally experiencing the intended emotion. Actors were told not to “indicate”—a pejorative acting term that refers to a non-truthful performance [<xref ref-type="bibr" rid="pone.0196391.ref129">129</xref>].</p>
</sec>
<sec id="sec018">
<title>Procedure and design</title>
<p>The RAVDESS was created following the procedure outlined in <xref ref-type="fig" rid="pone.0196391.g001">Fig 1</xref>. Actors were recruited through postings made to online casting services, and contacts at the Toronto Fringe Festival. Fifty-eight actors auditioned, during which they were recorded while performing one example of each emotional condition in speech and song. Audition videos were reviewed by the first author and two research assistants (hereon, three investigators), with expressions rated in terms of accuracy, intensity, and genuineness. From this set, the 24 actors with the highest aggregate ratings were asked to return for a second recording. Actors were booked for a 4-hour recording session and were paid for their time.</p>
<fig id="pone.0196391.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0196391.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Flowchart of RAVDESS creation and validation.</title>
<p>Flowchart illustrating the method of stimulus recording, editing, and validation.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.g001" xlink:type="simple"/>
</fig>
<p>Recordings took place in a professional recording studio at Ryerson University. Actors wore a black t-shirt, had minimal makeup, were clean shaven, wore contact lenses (if required), and had no distinctive jewelry. Actors were standing during all productions, with a seat provided to allow actors to rest and prepare between conditions. Microphone levels were set by having the actor produce several very angry expressions. Actors began with several practice trials of each emotional expression, and then completed all speech trials. Actors were given a 60-minute break in between blocks. Following the break, actors began with singing practice trials and then completed all singing trials. Recordings always began with speech to prevent any metrical influence of the singing condition. Trials were blocked by emotion, with low-intensity emotions followed by their very intense counterparts. This ordering allowed actors to enter and remain within the desired state for all productions of that emotional category.</p>
<p>A dialog script was used with all actors. A description of each emotional condition was provided. To ensure that actors understood what emotion was requested, emotional labels taken from the prototype model of emotion were used in the description [<xref ref-type="bibr" rid="pone.0196391.ref130">130</xref>]. A vignette describing a scenario involving that emotion was provided for each level of intensity. Actors were then given time to prepare their emotional state using their desired induction technique. For the song condition, actors were told to sing the basic notated pitches, but that they were free to vary acoustic characteristics to convey the desired emotion.</p>
<p>Actors could repeat a given trial until they were comfortable with their production. Actors were observed in an adjacent control room via video and audio feeds. Feedback was given if a production was felt to be ambiguous by both operators. No instruction was given as to how an emotion should be expressed. Multiple takes of each production were recorded. All takes were later reviewed by three investigators. Clips containing hand movements or gestures were removed, as were trials that contained lexical errors. After the removal of erroneous clips, the criteria for selection were productions that clearly conveyed the specified emotion and intensity through the face and the voice. The best two takes as agreed through consensus were selected for inclusion.</p>
</sec>
<sec id="sec019">
<title>Technical information</title>
<p>Actors were recorded individually in a professional recording studio, as illustrated in <xref ref-type="fig" rid="pone.0196391.g002">Fig 2</xref>. Actors stood in front of a Westcott digital green screen cloth and were recorded with a Sony Handycam HDR-SR11. Actors were recorded at 1080i with a scan resolution of 1920x1080 pixels at 30 fps, with files saved in AVCHD format. The camera was placed 1.4 m from the actor and zoomed to provide a fixed-width field of view of 0.5 m. Only the actor and green screen cloth were visible in the frame. The camera’s height was adjusted to ensure the actor fit within the scene, capturing their head and upper shoulders (see <xref ref-type="fig" rid="pone.0196391.g003">Fig 3</xref>). Arms and hands were not visible. Actors were illuminated by ceiling fluorescent lighting and three 28W 5200k CRI 82 bulbs, fitted in 10” reflectors with 38” white parabolic umbrellas. This setup provided full spectrum lighting while minimizing facial shadows. Voice recordings were captured by a Rode NTK vacuum tube condenser microphone, fitted with a Stedman proscreen XL pop filter, placed 20 cm from the actor. Microphone output was recorded using Pro Tools 8 and a Digidesign 003 mixing workstation, at a sampling rate of 48 kHz, 16 bit, with files saved in uncompressed wave format.</p>
<fig id="pone.0196391.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0196391.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Physical setup of the recording studio.</title>
<p>The physical layout of the recording studio used to record RAVDESS stimuli. All measurements refer to horizontal distances unless otherwise specified.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.g002" xlink:type="simple"/>
</fig>
<fig id="pone.0196391.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0196391.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Examples of the eight RAVDESS emotions.</title>
<p>Still frame examples of the eight emotions contained in the RAVDESS, in speech and song.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.g003" xlink:type="simple"/>
</fig>
<p>Stimuli were presented visually on a 15” Macbook Pro and auditorily over KRK Rocket 5 speakers, controlled by Matlab 2009b and the Psychophysics Toolbox [<xref ref-type="bibr" rid="pone.0196391.ref131">131</xref>]. Temporal accuracy of the presentation software was confirmed with the Black Box Toolkit [<xref ref-type="bibr" rid="pone.0196391.ref132">132</xref>]. Operator feedback was provided over speakers, with audio feeds controlled by Mackie Big Knob studio command system.</p>
</sec>
<sec id="sec020">
<title>Post-processing and standardization of recordings</title>
<p>Recordings were edited using Adobe Premiere Pro CS6. The microphone stream was imported and aligned to the camera’s audio channel using predefined markers. Chroma key compositing was used to replace the green screen backdrop with a solid white background (RGB 255, 255, 255). Trials that had been selected for inclusion were marked and unwanted trials were removed from the session.</p>
<p>The microphone track for each actor was peak-normalized to -3 dBFS using Adobe Audition CS6. Peak normalization was chosen to retain the natural variation in loudness between emotional conditions [<xref ref-type="bibr" rid="pone.0196391.ref095">95</xref>, <xref ref-type="bibr" rid="pone.0196391.ref126">126</xref>, <xref ref-type="bibr" rid="pone.0196391.ref133">133</xref>]. The singing audio track was imported into Melodyne for pitch adjustment to ensure that the three melodies remained perceptually distinct. Intervals are perceived as “in tune” when mistuned by up to 35 cents [<xref ref-type="bibr" rid="pone.0196391.ref134">134</xref>, <xref ref-type="bibr" rid="pone.0196391.ref135">135</xref>], and “out of tune” when mistuned by 50–100 cents [<xref ref-type="bibr" rid="pone.0196391.ref136">136</xref>]. Notes that were mistuned by more than 35 cents were adjusted to within ±35 cents of the target frequency.</p>
<p>Trials were exported using Adobe Premiere Pro CS6. Full audio-video and video-only trials were exported as MPEG-4 format (H.264, AAC) with a resolution of 1280x720 pixels at 30 fps (HD format, 720p). Audio-only files were exported as lossless wave format, at 48 kHz.</p>
</sec>
</sec>
<sec id="sec021">
<title>Description of RAVDESS files</title>
<sec id="sec022" sec-type="materials|methods">
<title>Experimental design</title>
<p>The RAVDESS contains 7356 recordings of 24 actors (12 male, 12 female). All actors produced 104 distinct vocalizations, consisting of 60 spoken utterances and 44 sung utterances. Each of the 104 vocalizations was exported to create three separate modality conditions: audio-video (face and voice), video-only (face, but no voice), and audio-only (voice, but no face). This produced 312 files per actor (104 × 3). The song recordings of one female participant were lost due to technical issues (132 files). Thus, 24 × 312–132 = 7356 files. This set is composed of 4320 speech recordings and 3036 song recordings.</p>
<p>Actors vocalized two distinct statements in the speech and song conditions. The two statements were each spoken with eight emotional intentions (neutral, calm, happy, sad, angry, fearful, surprise, and disgust), and sung with six emotional intentions (neutral, calm, happy, sad, angry, and fearful). All emotional conditions except neutral were vocalized at two levels of emotional intensity, normal and strong. Actors repeated each vocalization twice. The factorial design of the RAVDESS is visualized in <xref ref-type="supplementary-material" rid="pone.0196391.s001">S1</xref> and <xref ref-type="supplementary-material" rid="pone.0196391.s002">S2</xref> Figs.</p>
<p>The full design of speech trials includes: Emotional [Vocalist (12) × Gender (2) × Statement (2) × Emotion (7) × Intensity (2) × Repetition (2) × Modality (3)] + Neutral [Vocalist (12) × Gender (2) × Statement (2) × Repetition (2) × Modality (3)] = 4320 recordings. The full design of emotional and neutral trials in song was: Emotional [Vocalist (11) × Gender (2) × Statement (2) × Emotion (5) × Intensity (2) × Repetition (2) × Modality (3)] + Neutral [Vocalist (11) × Gender (2) × Statement (2) × Repetition (2) × Modality (3)] = 3036 recordings.</p>
<p>Still-image frames showing examples of each of the emotional expressions are illustrated in <xref ref-type="fig" rid="pone.0196391.g003">Fig 3</xref>. Full audio-video movies showing examples of each emotional expression for speech and song are presented in <xref ref-type="supplementary-material" rid="pone.0196391.s009">S1</xref> and <xref ref-type="supplementary-material" rid="pone.0196391.s010">S2</xref> Files respectively.</p>
</sec>
<sec id="sec023">
<title>Filename convention</title>
<p>Each RAVDESS file has a unique filename. The filename consists of seven two-digit numerical identifiers, separated by hyphens (e.g., 02-01-06-01-02-01-12.mp4). Each two-digit numerical identifier defines the level of a different experimental factor. The identifiers are ordered: Modality–Channel–Emotion–Intensity–Statement–Repetition–Actor.mp4 or .wav. The numerical coding of levels is described in <xref ref-type="table" rid="pone.0196391.t001">Table 1</xref>. For example, the filename “02-01-06-01-02-01-12.mp4” refers to: Video-only (02)–Speech (01)–Fearful (06)–Intensity normal (01)–Statement “dogs” (02)–First repetition (01)–Twelfth actor, female (12). The gender of the actor is coded by the actor’s number, where odd numbered actors are male, even numbered actors are female.</p>
<table-wrap id="pone.0196391.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0196391.t001</object-id>
<label>Table 1</label> <caption><title>Description of factor-level coding of RAVDESS filenames.</title></caption>
<alternatives>
<graphic id="pone.0196391.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Identifier</th>
<th align="left">Coding description of factor levels</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Modality</td>
<td align="left">01 = Audio-video, 02 = Video-only, 03 = Audio-only</td>
</tr>
<tr>
<td align="left">Channel</td>
<td align="left">01 = Speech, 02 = Song</td>
</tr>
<tr>
<td align="left">Emotion</td>
<td align="left">01 = Neutral, 02 = Calm, 03 = Happy, 04 = Sad, 05 = Angry, 06 = Fearful, <break/>07 = Disgust, 08 = Surprised</td>
</tr>
<tr>
<td align="left">Intensity</td>
<td align="left">01 = Normal, 02 = Strong</td>
</tr>
<tr>
<td align="left">Statement</td>
<td align="left">01 = "Kids are talking by the door", 02 = "Dogs are sitting by the door"</td>
</tr>
<tr>
<td align="left">Repetition</td>
<td align="left">01 = First repetition, 02 = Second repetition</td>
</tr>
<tr>
<td align="left">Actor</td>
<td align="left">01 = First actor, …, 24 = Twenty-fourth actor</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec024">
<title>Download and accessibility</title>
<p>A main goal of the RAVDESS was to provide researchers and interested parties with a validated stimulus set that is free and accessible. To meet this goal, the RAVDESS database is released under a Creative Commons Attribution-NonCommerical-ShareAlike 4.0 license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-sa/4.0/" xlink:type="simple">CC BY-NA-SC 4.0</ext-link>). The database can be downloaded free of charge and without restriction from the open access repository Zenodo (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1188976" xlink:type="simple">https://doi.org/10.5281/zenodo.1188976</ext-link>). This manuscript and its associated validation datasets are published in PLOS ONE, an open access journal that applies the Creative Commons Attribution (CC BY) license to its articles.</p>
</sec>
</sec>
<sec id="sec025">
<title>Validation of RAVDESS stimuli</title>
<sec id="sec026">
<title>Participants</title>
<p>Three hundred and nineteen undergraduate students (76% female, 24% male, mean age = 20.55 years, SD = 4.65) from Ryerson University, Toronto, Canada, participated in exchange for course credit. Raters had varied amounts of education (M = 13.97 years, SD = 2.24), private music instruction (M = 3.46 years, SD = 3.69), singing experience (M = 1.88 years, SD = 2.69), and drama experience (M = 2.35 years, SD = 2.81). All participants were fluent in English, with 75.2% identifying English as their L1. Participants identified themselves as being right-handed (91.5%), left-handed (7.84%), or ambidextrous (0.6%). No raters had taken part in stimulus creation.</p>
</sec>
<sec id="sec027">
<title>Stimuli, apparatus, and procedure</title>
<p>The stimuli consisted of 7356 audio-visual (AV), video-only (VO), and audio-only (AO) recordings of emotional speech and song. Participants were tested individually in IAC double-walled sound-attenuated booths. Stimuli were presented visually on a 27” iMac, at a resolution of 2560x1440 pixels, and auditorily over Sennheiser HD 518 headphones, controlled by custom Matlab software and the Psychophysics Toolbox [<xref ref-type="bibr" rid="pone.0196391.ref131">131</xref>]. Volume settings were kept constant across all participants.</p>
</sec>
<sec id="sec028">
<title>Validity task</title>
<p>Two hundred and forty-seven raters took part in the validity task. Raters were presented a pseudo-randomly chosen set of 298 stimuli, consisting of 174 speech and 124 song presentations. Trials were blocked and counterbalanced by Channel. Raters were seated approximately 60 cm from the computer display. In addition to verbal instruction, the following on-screen instructions were presented: “You will now be presented with recordings of people speaking and singing with different emotions. Recordings will be either: sound alone, video alone, or sound and video. After each recording, you will be asked to make three judgements: <italic>category</italic> of the emotion, <italic>strength</italic> of the emotion, and <italic>genuineness</italic> of the emotion. Category is the type of emotion (e.g., happy or sad). Strength is how intense the emotion was (e.g., weak or strong). Genuineness is whether you thought the person was physically, mentally, and emotionally feeling what they expressed (e.g., not genuine or very genuine).” Three practice trials preceded each Channel block, which used stimuli that were not contained in the rater’s subset.</p>
<p>Raters were asked to identify the category of emotion using a forced-choice response format. Speech options were: neutral, calm, happy, sad, angry, fearful, disgust, and surprise. Song options were: neutral, calm, happy, sad, angry, and fearful. The escape option “None of these are correct” was also provided [<xref ref-type="bibr" rid="pone.0196391.ref137">137</xref>]. Two orderings of emotion labels were used and was counterbalanced across raters. Emotion labels were listed vertically, next to a numbered box that was shaded according to Plutchik’s wheel of emotion [<xref ref-type="bibr" rid="pone.0196391.ref062">62</xref>]. Raters then evaluated the strength of the emotion using a 5-point Likert scale ranging from very weak (1) to very strong (5). Raters then evaluated the genuineness of the presentation using a 5-point Likert scale ranging from not genuine (1) to very genuine (5). The response rating screens are shown in <xref ref-type="supplementary-material" rid="pone.0196391.s003">S3 Fig</xref>.</p>
<p>Rater responses could only be provided once the feedback screen was displayed, ensuring participants viewed the entire clip. This process prevented participants from moving quickly through the task. It also eliminated any confounding effects of skipping stimuli of longer duration, as duration is known to vary consistently with emotion and intensity [<xref ref-type="bibr" rid="pone.0196391.ref066">66</xref>]. Raters also completed a background questionnaire. Participation in the experiment took approximately 60 minutes. All 7356 stimuli were each rated 10 times on emotional category, intensity, and genuineness, yielding 73560 ratings for each of the three measurement scales, or 220680 ratings in total.</p>
</sec>
<sec id="sec029">
<title>Test-retest reliability task</title>
<p>Seventy-two raters took part in the test-retest reliability task. No participant from the validity task took part in the test-retest task. Raters began with a subset of 102 trials, consisting of 60 speech and 42 song trials. Raters were then given a 20-minute break outside the testing booth, during which time they filled out a background questionnaire. Raters then re-entered the booth and were presented the same 102 files. Trials were blocked and counterbalanced by Channel within each presentation, with different random orderings used in the first and second blocks. All other aspects of the reliability task were the same as those used in the validity task.</p>
</sec>
<sec id="sec030">
<title>Analysis of validity task</title>
<p>Emotion category ratings were coded as correct (1) when the category selected by the rater matched the category that the actor had intended to express, and incorrect (0) otherwise. We use the term “proportion correct” to refer to the proportion of responses that were coded as correct, see also [<xref ref-type="bibr" rid="pone.0196391.ref029">29</xref>]. As proportion correct scores do not correct for response bias or false alarms, unbiased hit rates (H<sub>u</sub>) were also calculated [<xref ref-type="bibr" rid="pone.0196391.ref138">138</xref>]. Unbiased hit rates are proportion scores (0–1), and yield a smaller value than their corresponding proportion correct scores, except in the case of perfect unbiased accuracy. Unbiased hit rates were calculated as the product of Uncorrected hit rate and Differential accuracy [<xref ref-type="bibr" rid="pone.0196391.ref138">138</xref>]; as defined by <xref ref-type="disp-formula" rid="pone.0196391.e001">Eq 1</xref> where <italic>i</italic> is the i<sup>th</sup> stimulus of interest, <italic>n</italic> is number of stimuli of that intended emotional category, and <italic>N</italic> is the total number of stimuli for that channel (speech or song).</p>
<disp-formula id="pone.0196391.e001">
<alternatives>
<graphic id="pone.0196391.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mrow><mml:mi>U</mml:mi><mml:mi>H</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder></mml:mstyle><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Responses</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Responses</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Responses</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:munderover></mml:mstyle><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Responses</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Responses</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mi>i</mml:mi><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Responses</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
<p>Interrater reliability is assessed with Fleiss’ kappa [<xref ref-type="bibr" rid="pone.0196391.ref139">139</xref>], a chance-corrected measure of inter-rater agreement for m-raters on nominal data. Kappa scores were calculated to estimate the degree of agreement between raters’ emotion category responses. These scores reflect the degree of agreement in classification over that which would be expected by chance. Kappa scores were generated for each factor of interest (reported in <xref ref-type="table" rid="pone.0196391.t002">Table 2</xref>). These calculations involved separate n*m matrices, consisting of ‘n’ RAVDESS files and ‘m’ raters (m = 10). Category-wise kappa scores were also generated, and represent interrater reliability scores for each emotional category (reported in <xref ref-type="table" rid="pone.0196391.t003">Table 3</xref>). It was not expected that calm and neutral expressions would be identified as distinct emotions due to their perceptual similarities. Therefore, responses of neutral or calm were accepted as correct for both neutral and calm expressions, see also [<xref ref-type="bibr" rid="pone.0196391.ref029">29</xref>]. Hypothesis tests were conducted during the calculation of kappa values to determine if the observed interrater agreement rates were different to those expected by chance. All tests achieved p-values &lt; 0.001, suggesting that observed interrater agreement rates were not due to chance. For conciseness, kappa test p-values are omitted from the manuscript. Kappa values are interpreted according to the guidelines established by Landis and Koch [<xref ref-type="bibr" rid="pone.0196391.ref140">140</xref>], where values &lt; 0 indicate poor agreement, 0.01–0.20 slight agreement, 0.21–0.40 fair agreement, 0.41–0.60 moderate agreement, 0.61–0.80 substantial agreement, and 0.81–1 indicate almost perfect agreement.</p>
<table-wrap id="pone.0196391.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0196391.t002</object-id>
<label>Table 2</label> <caption><title>Validity task accuracy measures across channel, modality, and intensity.</title></caption>
<alternatives>
<graphic id="pone.0196391.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.t002" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Channel</th>
<th align="left">Modality</th>
<th align="left">Intensity</th>
<th align="left">N</th>
<th align="left">Mean (SD) Proportion correct</th>
<th align="left">Mean (SD) Unbiased hit rate</th>
<th align="left">Mean (SD) Intensity</th>
<th align="left">Mean (SD) Genu.</th>
<th align="left">Kappa</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="6">Speech</td>
<td align="left" rowspan="2">AV</td>
<td align="left">Normal</td>
<td align="left">768</td>
<td align="left">0.77 (0.23)</td>
<td align="left">0.57 (0.17)</td>
<td align="left">3.44 (0.51)</td>
<td align="left">3.47 (0.44)</td>
<td align="left">0.62</td>
</tr>
<tr>
<td align="left">Strong</td>
<td align="left">672</td>
<td align="left">0.83 (0.19)</td>
<td align="left">0.62 (0.15)</td>
<td align="left">4.01 (0.56)</td>
<td align="left">3.56 (0.56)</td>
<td align="left">0.71</td>
</tr>
<tr>
<td align="left" rowspan="2">VO</td>
<td align="left">Normal</td>
<td align="left">768</td>
<td align="left">0.70 (0.25)</td>
<td align="left">0.52 (0.19)</td>
<td align="left">3.40 (0.54)</td>
<td align="left">3.42 (0.46)</td>
<td align="left">0.53</td>
</tr>
<tr>
<td align="left">Strong</td>
<td align="left">672</td>
<td align="left">0.75 (0.25)</td>
<td align="left">0.56 (0.19)</td>
<td align="left">3.88 (0.60)</td>
<td align="left">3.55 (0.48)</td>
<td align="left">0.62</td>
</tr>
<tr>
<td align="left" rowspan="2">AO</td>
<td align="left">Normal</td>
<td align="left">758</td>
<td align="left">0.58 (0.30)</td>
<td align="left">0.43 (0.22)</td>
<td align="left">3.14 (0.42)</td>
<td align="left">3.12 (0.41)</td>
<td align="left">0.41</td>
</tr>
<tr>
<td align="left">Strong</td>
<td align="left">672</td>
<td align="left">0.67 (0.27)</td>
<td align="left">0.50 (0.21)</td>
<td align="left">3.71 (0.62)</td>
<td align="left">3.51 (0.46)</td>
<td align="left">0.52</td>
</tr>
<tr>
<td align="left" rowspan="6">Song</td>
<td align="left" rowspan="2">AV</td>
<td align="left">Normal</td>
<td align="left">552</td>
<td align="left">0.77 (0.23)</td>
<td align="left">0.57 (0.19)</td>
<td align="left">3.37 (0.49)</td>
<td align="left">3.33 (0.48)</td>
<td align="left">0.61</td>
</tr>
<tr>
<td align="left">Strong</td>
<td align="left">460</td>
<td align="left">0.84 (0.20)</td>
<td align="left">0.63 (0.18)</td>
<td align="left">3.91 (0.58)</td>
<td align="left">3.46 (0.51)</td>
<td align="left">0.72</td>
</tr>
<tr>
<td align="left" rowspan="2">VO</td>
<td align="left">Normal</td>
<td align="left">552</td>
<td align="left">0.75 (0.25)</td>
<td align="left">0.55 (0.21)</td>
<td align="left">3.41 (0.53)</td>
<td align="left">3.36 (0.46)</td>
<td align="left">0.61</td>
</tr>
<tr>
<td align="left">Strong</td>
<td align="left">460</td>
<td align="left">0.79 (0.23)</td>
<td align="left">0.59 (0.20)</td>
<td align="left">3.89 (0.61)</td>
<td align="left">3.54 (0.51)</td>
<td align="left">0.67</td>
</tr>
<tr>
<td align="left" rowspan="2">AO</td>
<td align="left">Normal</td>
<td align="left">552</td>
<td align="left">0.53 (0.28)</td>
<td align="left">0.39 (0.21)</td>
<td align="left">3.13 (0.39)</td>
<td align="left">3.24 (0.37)</td>
<td align="left">0.31</td>
</tr>
<tr>
<td align="left">Strong</td>
<td align="left">460</td>
<td align="left">0.62 (0.28)</td>
<td align="left">0.47 (0.23)</td>
<td align="left">3.55 (0.57)</td>
<td align="left">3.37 (0.40)</td>
<td align="left">0.44</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t002fn001"><p>Description of validity ratings for spoken and sung expressions, across channel, modality, and emotional intensity (N = 247 participants, each rating 298 stimuli). AV = audio-video; VO = video only; AO = audio only. As neutral had no intensity manipulation, neutral scores were collapsed into the ‘normal’ intensity category.</p></fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="pone.0196391.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0196391.t003</object-id>
<label>Table 3</label> <caption><title>Validity task accuracy measures across emotion and channel.</title></caption>
<alternatives>
<graphic id="pone.0196391.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.t003" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Emotion</th>
<th align="left">N</th>
<th align="left">Mean (SD) Proportion correct</th>
<th align="left">Mean (SD) Unbiased hit rate</th>
<th align="left">Mean (SD) Intensity</th>
<th align="left">Mean (SD) Genuineness</th>
<th align="left">Kappa</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Neutral (speech)</td>
<td align="left">288</td>
<td align="left">0.87 (0.14)</td>
<td align="left">0.60 (0.10)</td>
<td align="left">3.16 (0.44)</td>
<td align="left">3.36 (0.45)</td>
<td align="left">0.58</td>
</tr>
<tr>
<td align="left">Neutral (song)</td>
<td align="left">276</td>
<td align="left">0.78 (0.18)</td>
<td align="left">0.53 (0.12)</td>
<td align="left">3.03 (0.36)</td>
<td align="left">3.22 (0.40)</td>
<td align="left">0.49</td>
</tr>
<tr>
<td align="left">Calm (speech)</td>
<td align="left">576</td>
<td align="left">0.70 (0.24)</td>
<td align="left">0.48 (0.16)</td>
<td align="left">3.26 (0.41)</td>
<td align="left">3.39 (0.39)</td>
<td align="left">0.58</td>
</tr>
<tr>
<td align="left">Calm (song)</td>
<td align="left">552</td>
<td align="left">0.63 (0.25)</td>
<td align="left">0.43 (0.17)</td>
<td align="left">3.24 (0.40)</td>
<td align="left">3.38 (0.40)</td>
<td align="left">0.49</td>
</tr>
<tr>
<td align="left">Happy (speech)</td>
<td align="left">576</td>
<td align="left">0.68 (0.32)</td>
<td align="left">0.49 (0.23)</td>
<td align="left">3.68 (0.58)</td>
<td align="left">3.51 (0.45)</td>
<td align="left">0.63</td>
</tr>
<tr>
<td align="left">Happy (song)</td>
<td align="left">552</td>
<td align="left">0.75 (0.29)</td>
<td align="left">0.55 (0.21)</td>
<td align="left">3.68 (0.59)</td>
<td align="left">3.40 (0.50)</td>
<td align="left">0.65</td>
</tr>
<tr>
<td align="left">Sad (speech)</td>
<td align="left">576</td>
<td align="left">0.61 (0.30)</td>
<td align="left">0.42 (0.21)</td>
<td align="left">3.33 (0.61)</td>
<td align="left">3.37 (0.45)</td>
<td align="left">0.53</td>
</tr>
<tr>
<td align="left">Sad (song)</td>
<td align="left">552</td>
<td align="left">0.68 (0.28)</td>
<td align="left">0.43 (0.18)</td>
<td align="left">3.41 (0.55)</td>
<td align="left">3.34 (0.46)</td>
<td align="left">0.51</td>
</tr>
<tr>
<td align="left">Angry (speech)</td>
<td align="left">576</td>
<td align="left">0.81 (0.22)</td>
<td align="left">0.64 (0.17)</td>
<td align="left">3.96 (0.67)</td>
<td align="left">3.71 (0.55)</td>
<td align="left">0.67</td>
</tr>
<tr>
<td align="left">Angry (song)</td>
<td align="left">552</td>
<td align="left">0.83 (0.22)</td>
<td align="left">0.73 (0.19)</td>
<td align="left">3.83 (0.62)</td>
<td align="left">3.45 (0.51)</td>
<td align="left">0.75</td>
</tr>
<tr>
<td align="left">Fearful (speech)</td>
<td align="left">576</td>
<td align="left">0.71 (0.24)</td>
<td align="left">0.56 (0.19)</td>
<td align="left">3.76 (0.66)</td>
<td align="left">3.46 (0.49)</td>
<td align="left">0.60</td>
</tr>
<tr>
<td align="left">Fearful (song)</td>
<td align="left">552</td>
<td align="left">0.65 (0.29)</td>
<td align="left">0.51 (0.22)</td>
<td align="left">3.70 (0.58)</td>
<td align="left">3.37 (0.47)</td>
<td align="left">0.57</td>
</tr>
<tr>
<td align="left">Disgust (speech)</td>
<td align="left">576</td>
<td align="left">0.70 (0.27)</td>
<td align="left">0.55 (0.21)</td>
<td align="left">3.73 (0.57)</td>
<td align="left">3.43 (0.46)</td>
<td align="left">0.60</td>
</tr>
<tr>
<td align="left">Surprise (speech)</td>
<td align="left">552</td>
<td align="left">0.72 (0.24)</td>
<td align="left">0.55 (0.19)</td>
<td align="left">3.53 (0.49)</td>
<td align="left">3.47 (0.45)</td>
<td align="left">0.60</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t003fn001"><p>Description of validity ratings and interrater reliability values for emotional expressions in speech and song.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>Measures of inter-rater reliability were calculated for emotional intensity and genuineness scales. Separate intra-class correlations (ICC) were calculated for speech and song. ICC one-way random effects, consistency, single rater/measurement ICC(1,1) and one-way random effects, consistency, multiple raters/measurements ICC(1,k) were calculated [<xref ref-type="bibr" rid="pone.0196391.ref141">141</xref>]. The higher indices of ICC(2,1) and ICC(2,k) that partial out variance due to specific raters and rater × stimuli interaction were not calculated, as all raters were not presented the full set of stimuli. As one-way random-effects models generally give a smaller ICC estimate than 2-way models [<xref ref-type="bibr" rid="pone.0196391.ref142">142</xref>], our reliability indices are probably lower than the actual reliability of the stimulus. ICC values are reported according to the guidelines set forth recently by Koo and Li [<xref ref-type="bibr" rid="pone.0196391.ref142">142</xref>]. ICC values are interpreted according to the guidelines established by Cicchetti [<xref ref-type="bibr" rid="pone.0196391.ref143">143</xref>], where values &lt; 0.40 indicate poor agreement, 0.40–0.59 fair agreement, 0.60–0.74 good agreement, and 0.75–1 indicate excellent agreement.</p>
<p>For individual stimuli, a composite “goodness” score was derived to facilitate researchers’ selection of stimuli for their research paradigm, see also [<xref ref-type="bibr" rid="pone.0196391.ref033">33</xref>]. Goodness values range between 0 and 10; as defined by <xref ref-type="disp-formula" rid="pone.0196391.e002">Eq 2</xref>, where <italic>i</italic> is the i<sup>th</sup> stimulus of interest, <italic>P</italic> refers to Proportion correct, <italic>I</italic> refers to the Intensity rating, and <italic>G</italic> refers to the Genuineness rating. As the neutral emotion category does not have a meaningful intensity or genuineness rating, goodness scores for these stimuli are determined only by their proportion correct scores.</p>
<disp-formula id="pone.0196391.e002">
<alternatives>
<graphic id="pone.0196391.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e002" xlink:type="simple"/>
<mml:math display="block" id="M2">
<mml:mrow><mml:mi>G</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>G</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width="0.50em"/><mml:mi mathvariant="normal">if</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">Emotion</mml:mi><mml:mo>≠</mml:mo><mml:mi mathvariant="normal">neutral</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>10</mml:mn><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mspace width="7.5em"/><mml:mi mathvariant="normal">otherwise</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
<p>Response times for emotion category, intensity, and genuineness ratings were also calculated. Response times were defined as the duration of time between the display of the response option screen and the recording of a valid keystroke by the participant. Response times that exceeded 2.2 times the inter-quartile range above the upper quartile were excluded from the data [<xref ref-type="bibr" rid="pone.0196391.ref144">144</xref>, <xref ref-type="bibr" rid="pone.0196391.ref145">145</xref>]. That is, RT &gt; = F<sub>U</sub> + 2.2 * (F<sub>U</sub>—F<sub>L</sub>), where F<sub>U</sub> and F<sub>L</sub> refer to upper and lower fourth respectively. This process removed response times of unusually long duration (e.g., participant had become distracted). This process removed the response times of 4.2% of category responses (n = 3088), 2.6% of intensity responses (n = 1944), and 1.9% of genuineness responses (n = 1430).</p>
<p>The measures proportion correct, emotional intensity, and emotional genuineness, were examined with repeated measures analyses of variance (ANOVA). As participants were presented a subset of all stimuli, participants did not see all levels of all factors (cells). To analyze these data, responses across missing cells were collapsed to create valid factorial designs. This collapsing precluded a full-factorial ANOVA, but did permit examinations by Channel(2), Modality(3), Intensity(2), and by Emotion (7, 5) separately for each channel. Proportion scores (0–1) were arcsine transformed prior to analysis [<xref ref-type="bibr" rid="pone.0196391.ref146">146</xref>]. For readability, pre-transformed means are reported in the manuscript. When Mauchly’s sphericity test was significant, Greenhouse–Geisser’s correction was applied when ε &lt; .75, and Huynh-Feldt correction when ε ≥ .75 [<xref ref-type="bibr" rid="pone.0196391.ref147">147</xref>]. Effect sizes are reported with partial eta-squared values. Means are accompanied by 95% confidence intervals in square brackets. Pairwise comparisons were adjusted using Bonferroni correction. All reported ANOVAs were exploratory in nature with no explicit a-priori hypotheses. As exploratory ANOVAs suffer from hidden multiplicity [<xref ref-type="bibr" rid="pone.0196391.ref148">148</xref>], <italic>p</italic>-values were corrected by controlling the false discovery rate with the Benjamini–Hochberg procedure [<xref ref-type="bibr" rid="pone.0196391.ref149">149</xref>]. Statistical tests were conducted in Matlab 2015a and SPSS v22.0.0.2. Reliability measures were calculated in R v3.4.3 [<xref ref-type="bibr" rid="pone.0196391.ref150">150</xref>] with RStudio [<xref ref-type="bibr" rid="pone.0196391.ref151">151</xref>], using the irr package [<xref ref-type="bibr" rid="pone.0196391.ref152">152</xref>], and data manipulation tools from the tidyverse package [<xref ref-type="bibr" rid="pone.0196391.ref153">153</xref>].</p>
</sec>
<sec id="sec031">
<title>Analysis of test-retest reliability task</title>
<p>Intrarater reliability was assessed with Cohen’s kappa, a chance-corrected measure of agreement for 2 raters on nominal data. As with the validity task, kappa scores were calculated to estimate the degree of agreement between raters’ chosen emotion category responses. Separate kappa scores were generated for speech and song. These calculations involved separate n*2 matrices, consisting of ‘n’ RAVDESS files and 2 ratings. As in the validity task, responses of neutral or calm were accepted as correct for both neutral and calm expressions. Measures of intrarater reliability for emotional intensity and genuineness scales were assessed with intra-class correlations (ICC), as described above in the validity task.</p>
</sec>
</sec>
</sec>
<sec id="sec032" sec-type="results">
<title>Results</title>
<sec id="sec033">
<title>Validity task</title>
<sec id="sec034">
<title>Accuracy measures</title>
<p>There were two measures of correctness in the validity task (proportion correct and unbiased hit rate) for each stimulus, resulting in 7356 proportion correct and unbiased hit rate scores. These scores are presented individually in <xref ref-type="supplementary-material" rid="pone.0196391.s004">S1 Table</xref>, along with the measures Intensity, Genuineness, their respective response times, Goodness scores, and stimulus file duration. For conciseness, these scores are presented in <xref ref-type="table" rid="pone.0196391.t002">Table 2</xref> by Channel, Modality, and Intensity, along with their respective Kappa scores.</p>
<p>The overall proportion correct was high for speech (mean = .72, SD = .27, median = .8), as well as for song (mean = .71, SD = .27, median = .8). The overall unbiased hit rate for speech was good (mean H<sub>u</sub> = .53, SD = .20, median = .57), as well as for song (mean H<sub>u</sub> = .53, SD = .21, median = .55). Kappa values indicated “substantial” interrater agreement for strong expressions in speech (κ = .62, n = 2016), and song (mean κ = .61, n = 1380), and “moderate” agreement for normal expressions in speech (κ = .53, n = 2304) and song (κ = .52, n = 1656). These validity ratings are also presented in aggregate form by Emotion, for speech and song, in <xref ref-type="table" rid="pone.0196391.t003">Table 3</xref>.</p>
<p>To assess the effect of presentation mode on raters’ identification accuracy, a three-way repeated measures ANOVA was conducted on raters’ proportion correct scores by Channel (2 levels: speech, song), Modality (3 levels: audio-video, video, audio), and Intensity (2 levels: normal, strong). No effect of Channel was found, F(1, 246) = 2.31, <italic>p</italic> = .15. A main effect of Modality was found, F (1.94, 477.53) = 941.68, p &lt; 0.001, <inline-formula id="pone.0196391.e003"><alternatives><graphic id="pone.0196391.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> = .79. Pairwise comparisons confirmed that Audio-Video presentations <italic>M</italic> = .81, 95% CI [.80, .81] &gt; Video presentations <italic>M</italic> = .75, [.74, .76], &gt; Audio presentation <italic>M</italic> = .60, [.59, .61]. These findings are in line with existing research suggesting a face-bias in emotional recognition tasks [<xref ref-type="bibr" rid="pone.0196391.ref075">75</xref>, <xref ref-type="bibr" rid="pone.0196391.ref126">126</xref>]. A main effect of Intensity was also found, F (1, 246) = 402.39, <italic>p</italic> &lt; 0.001, <inline-formula id="pone.0196391.e004"><alternatives><graphic id="pone.0196391.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> = .62. Pairwise comparisons confirmed that stronger intensity expressions <italic>M</italic> = .75, [.74, .76] &gt; normal intensity expressions <italic>M</italic> = .68, [.68, .69]. Comparable findings have been previously reported for face and voice recognition tasks [<xref ref-type="bibr" rid="pone.0196391.ref065">65</xref>, <xref ref-type="bibr" rid="pone.0196391.ref066">66</xref>]. A two-way interaction of Channel × Modality was reported, F (1.92, 972.9) = 59.08, <italic>p</italic> &lt; 0.001, <inline-formula id="pone.0196391.e005"><alternatives><graphic id="pone.0196391.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> = .19. Posthoc comparisons (Tukey’s HSD = .02, α = .05) confirmed that Video-Song <italic>M</italic> = .77, [.76, .78] &gt; Video-Speech, <italic>M</italic> = .72, [.71, .73], yet Audio-Song <italic>M</italic> = .58, [.56, .59] &lt; Audio-Speech <italic>M</italic> = .63, [.62, .64], suggesting a role in the interaction. These results are partially supported by recent findings that emotion expressed through the voice is identified less accurately in song than in speech [<xref ref-type="bibr" rid="pone.0196391.ref013">13</xref>]. Finally, a significant two-way interaction of Modality × Intensity was reported, F (2, 492) = 9.38, <italic>p</italic> &lt; 0.001, <inline-formula id="pone.0196391.e006"><alternatives><graphic id="pone.0196391.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> = .04. Given the small effect size, we do not report posthoc comparisons.</p>
<p>To assess the effect of emotion on raters’ identification accuracy, separate one-way repeated measures ANOVA were conducted on raters’ proportion correct scores by Emotion in Speech (8 levels: neutral, calm, happy, sad, angry, fearful, disgust, surprise), and Song (6 levels: neutral, calm, happy, sad, angry, fearful, disgust, surprise). For speech, a main effect of Emotion was found, <italic>F</italic> (5.87, 1443.83) = 108.03, p &lt; 0.001, <inline-formula id="pone.0196391.e007"><alternatives><graphic id="pone.0196391.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> = .31. Pairwise comparisons confirmed that Neutral <italic>M</italic> = .87, 95% CI [.85, .88] &gt; Angry <italic>M</italic> = .81, [.80, .83] &gt; Calm <italic>M</italic> = .70, [.68, .72] ~ Fearful <italic>M</italic> = .71, [.69, .73] ~ Surprise <italic>M</italic> = .72, [.71, .74] &gt; Happy <italic>M</italic> = .68, [.67, .70] ~ Disgust <italic>M</italic> = .70, [.68, .71] &gt; Sadness <italic>M</italic> = .61, [.59, .63]. These results are generally in line with recognition rates commonly reported in the literature [<xref ref-type="bibr" rid="pone.0196391.ref126">126</xref>]. For song, a main effect of Emotion was found, <italic>F</italic> (3.79, 932.27) = 81.33, p &lt; 0.001, <inline-formula id="pone.0196391.e008"><alternatives><graphic id="pone.0196391.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> = .25. Pairwise comparisons confirmed that Neutral <italic>M</italic> = .78, 95% CI [.76, .80] ~ Angry <italic>M</italic> = .84, [.82, .85] &gt; Happy <italic>M</italic> = .75, [.73, .76] &gt; Sad <italic>M</italic> = .68, [.66, .70] &gt; Fearful <italic>M</italic> = .65, [.64, .67] ~ Calm <italic>M</italic> = .63, [.61, .65]. To provide a more nuanced understanding of these results, proportion correct scores by Emotion, Channel, Modality, and Intensity are presented in aggregate in <xref ref-type="table" rid="pone.0196391.t004">Table 4</xref>.</p>
<table-wrap id="pone.0196391.t004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0196391.t004</object-id>
<label>Table 4</label> <caption><title>Validity task mean proportion correct scores.</title></caption>
<alternatives>
<graphic id="pone.0196391.t004g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.t004" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center" colspan="4">Speech</th>
<th align="center" colspan="4">Song</th>
<th align="left"/>
</tr>
<tr>
<th align="center"/>
<th align="center">AV</th>
<th align="center">VO</th>
<th align="center">AO</th>
<th align="center">Total</th>
<th align="center">AV</th>
<th align="center">VO</th>
<th align="center">AO</th>
<th align="center">Total</th>
<th align="center">Channel Total</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><italic>Strong Intensity</italic></td>
<td align="right"/>
<td align="right"/>
<td align="right"/>
<td align="right"/>
<td align="right"/>
<td align="right"/>
<td align="right"/>
<td align="right"/>
<td align="right"/>
</tr>
<tr>
<td align="left">Calm</td>
<td align="left">0.72</td>
<td align="left">0.58</td>
<td align="left">0.75</td>
<td align="left">0.68</td>
<td align="left">0.66</td>
<td align="left">0.59</td>
<td align="left">0.64</td>
<td align="left">0.63</td>
<td align="left">0.66</td>
</tr>
<tr>
<td align="left">Happy</td>
<td align="left">0.84</td>
<td align="left">0.89</td>
<td align="left">0.44</td>
<td align="left">0.72</td>
<td align="left">0.93</td>
<td align="left">0.90</td>
<td align="left">0.50</td>
<td align="left">0.78</td>
<td align="left">0.75</td>
</tr>
<tr>
<td align="left">Sad</td>
<td align="left">0.81</td>
<td align="left">0.77</td>
<td align="left">0.62</td>
<td align="left">0.73</td>
<td align="left">0.85</td>
<td align="left">0.83</td>
<td align="left">0.51</td>
<td align="left">0.73</td>
<td align="left">0.73</td>
</tr>
<tr>
<td align="left">Angry</td>
<td align="left">0.94</td>
<td align="left">0.92</td>
<td align="left">0.91</td>
<td align="left">0.92</td>
<td align="left">0.93</td>
<td align="left">0.90</td>
<td align="left">0.86</td>
<td align="left">0.89</td>
<td align="left">0.91</td>
</tr>
<tr>
<td align="left">Fearful</td>
<td align="left">0.79</td>
<td align="left">0.70</td>
<td align="left">0.73</td>
<td align="left">0.74</td>
<td align="left">0.83</td>
<td align="left">0.75</td>
<td align="left">0.59</td>
<td align="left">0.72</td>
<td align="left">0.73</td>
</tr>
<tr>
<td align="left">Disgust</td>
<td align="left">0.88</td>
<td align="left">0.68</td>
<td align="left">0.54</td>
<td align="left">0.70</td>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left">0.70</td>
</tr>
<tr>
<td align="left">Surprise</td>
<td align="left">0.86</td>
<td align="left">0.69</td>
<td align="left">0.74</td>
<td align="left">0.76</td>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left">0.76</td>
</tr>
<tr>
<td align="left"><bold>Total for Strong Intensity</bold></td>
<td align="left"><bold>0.83</bold></td>
<td align="left"><bold>0.75</bold></td>
<td align="left"><bold>0.67</bold></td>
<td align="left"><bold>0.75</bold></td>
<td align="left"><bold>0.84</bold></td>
<td align="left"><bold>0.79</bold></td>
<td align="left"><bold>0.62</bold></td>
<td align="left"><bold>0.75</bold></td>
<td align="left"><bold>0.75</bold></td>
</tr>
<tr>
<td align="left"><italic>Normal Intensity</italic></td>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left"/>
</tr>
<tr>
<td align="left">Calm</td>
<td align="left">0.73</td>
<td align="left">0.62</td>
<td align="left">0.79</td>
<td align="left">0.71</td>
<td align="left">0.61</td>
<td align="left">0.58</td>
<td align="left">0.68</td>
<td align="left">0.62</td>
<td align="left">0.67</td>
</tr>
<tr>
<td align="left">Happy</td>
<td align="left">0.80</td>
<td align="left">0.85</td>
<td align="left">0.29</td>
<td align="left">0.65</td>
<td align="left">0.86</td>
<td align="left">0.88</td>
<td align="left">0.40</td>
<td align="left">0.72</td>
<td align="left">0.68</td>
</tr>
<tr>
<td align="left">Sad</td>
<td align="left">0.56</td>
<td align="left">0.56</td>
<td align="left">0.34</td>
<td align="left">0.49</td>
<td align="left">0.73</td>
<td align="left">0.74</td>
<td align="left">0.40</td>
<td align="left">0.63</td>
<td align="left">0.55</td>
</tr>
<tr>
<td align="left">Angry</td>
<td align="left">0.75</td>
<td align="left">0.78</td>
<td align="left">0.59</td>
<td align="left">0.71</td>
<td align="left">0.88</td>
<td align="left">0.88</td>
<td align="left">0.57</td>
<td align="left">0.77</td>
<td align="left">0.74</td>
</tr>
<tr>
<td align="left">Fearful</td>
<td align="left">0.77</td>
<td align="left">0.66</td>
<td align="left">0.59</td>
<td align="left">0.67</td>
<td align="left">0.71</td>
<td align="left">0.64</td>
<td align="left">0.40</td>
<td align="left">0.58</td>
<td align="left">0.63</td>
</tr>
<tr>
<td align="left">Disgust</td>
<td align="left">0.89</td>
<td align="left">0.69</td>
<td align="left">0.50</td>
<td align="left">0.70</td>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left">0.70</td>
</tr>
<tr>
<td align="left">Surprise</td>
<td align="left">0.82</td>
<td align="left">0.61</td>
<td align="left">0.62</td>
<td align="left">0.68</td>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left">0.68</td>
</tr>
<tr>
<td align="left"><bold>Total for Normal Intensity</bold></td>
<td align="left"><bold>0.76</bold></td>
<td align="left"><bold>0.68</bold></td>
<td align="left"><bold>0.53</bold></td>
<td align="left"><bold>0.66</bold></td>
<td align="left"><bold>0.76</bold></td>
<td align="left"><bold>0.75</bold></td>
<td align="left"><bold>0.49</bold></td>
<td align="left"><bold>0.66</bold></td>
<td align="left"><bold>0.66</bold></td>
</tr>
<tr>
<td align="left"><italic>No Intensity</italic></td>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left"/>
<td align="left"/>
</tr>
<tr>
<td align="left">Neutral</td>
<td align="left">0.88</td>
<td align="left">0.81</td>
<td align="left">0.91</td>
<td align="left">0.87</td>
<td align="left">0.83</td>
<td align="left">0.76</td>
<td align="left">0.75</td>
<td align="left">0.78</td>
<td align="left">0.82</td>
</tr>
<tr>
<td align="left"><bold>Total for all intensities</bold></td>
<td align="left"><bold>0.80</bold></td>
<td align="left"><bold>0.72</bold></td>
<td align="left"><bold>0.62</bold></td>
<td align="left"><bold>0.72</bold></td>
<td align="left"><bold>0.80</bold></td>
<td align="left"><bold>0.77</bold></td>
<td align="left"><bold>0.57</bold></td>
<td align="left"><bold>0.71</bold></td>
<td align="left"><bold>0.71</bold></td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t004fn001"><p>Validity task Mean proportion correct scores across channel, modality, emotion, and intensity, for speech and song.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>Mean scores by actor for proportion correct, unbiased hit rates, intensity, genuineness, response times, goodness, and file duration are provided in <xref ref-type="supplementary-material" rid="pone.0196391.s005">S2 Table</xref>, separately for speech and song. The actors which achieved a mean proportion correct score &gt; = 0.75 in speech were: A6, A18, A8, A2, A7, and A12. The actors which achieved a mean proportion correct score &gt; = 0.75 in song were: A8, A7, A4, and A15. These scores represent aggregate scores only and researchers are encouraged to select files individually based on their specific requirements.</p>
<p>Confusion matrices showing the average proportion of target and non-target labels selected by raters for each intended emotional expression are presented in <xref ref-type="supplementary-material" rid="pone.0196391.s006">S3 Table</xref>. These confusion matrix data are visualized in <xref ref-type="fig" rid="pone.0196391.g004">Fig 4</xref>. The data reveal that the pattern of errors was relatively consistent across both channels.</p>
<fig id="pone.0196391.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0196391.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Confusion matrices of emotional validity.</title>
<p>The confusion matrices present mean proportion correct scores for actors’ intended emotions as per rater chosen emotion labels for: (A) Speech (N = 43200 ratings), and (B) Song (N = 30360 ratings). Proportion scores that equal or exceed 15% are notated on the corresponding bar.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec035">
<title>Intensity and genuineness measures</title>
<p>Interrater reliability of the ratings provided for emotional intensity (five levels, labeled 1 to 5 from least intense to the most intense) and emotional genuineness (five levels, labeled 1 to 5 from not genuine to very genuine) were estimated with intraclass correlations, separately for speech and song, and are presented in <xref ref-type="table" rid="pone.0196391.t005">Table 5</xref>.</p>
<table-wrap id="pone.0196391.t005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0196391.t005</object-id>
<label>Table 5</label> <caption><title>Validity task ICC calculations for intensity and genuineness using single- and multiple-rating, consistency-agreement, 1-way random-effects models.</title></caption>
<alternatives>
<graphic id="pone.0196391.t005g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.t005" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="2">Response Scale</th>
<th align="left" rowspan="2">ICC test</th>
<th align="left" rowspan="2">Value</th>
<th align="left" colspan="2">95% Conf. Interval</th>
<th align="left" colspan="4">F-test with True Value 0</th>
</tr>
<tr>
<th align="left">Lower bound</th>
<th align="left">Upper bound</th>
<th align="left">Value</th>
<th align="left">df1</th>
<th align="left">df2</th>
<th align="left">Sig</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="2">Intensity (speech)</td>
<td align="left">Single (1, 1)</td>
<td align="left">0.22</td>
<td align="left">0.21</td>
<td align="left">0.23</td>
<td align="left">3.84</td>
<td align="left">4319</td>
<td align="left">38880</td>
<td align="left">0.000</td>
</tr>
<tr>
<td align="left">Average (1, k)</td>
<td align="left">0.74</td>
<td align="left">0.73</td>
<td align="left">0.75</td>
<td align="left">3.84</td>
<td align="left">4319</td>
<td align="left">38880</td>
<td align="left">0.000</td>
</tr>
<tr>
<td align="left" rowspan="2">Intensity (song)</td>
<td align="left">Single (1, 1)</td>
<td align="left">0.21</td>
<td align="left">0.20</td>
<td align="left">0.22</td>
<td align="left">3.63</td>
<td align="left">3035</td>
<td align="left">27324</td>
<td align="left">0.000</td>
</tr>
<tr>
<td align="left">Average (1, k)</td>
<td align="left">0.72</td>
<td align="left">0.71</td>
<td align="left">0.74</td>
<td align="left">3.63</td>
<td align="left">3035</td>
<td align="left">27324</td>
<td align="left">0.000</td>
</tr>
<tr>
<td align="left" rowspan="2">Genuineness (speech)</td>
<td align="left">Single (1, 1)</td>
<td align="left">0.07</td>
<td align="left">0.06</td>
<td align="left">0.08</td>
<td align="left">1.73</td>
<td align="left">4319</td>
<td align="left">38880</td>
<td align="left">0.000</td>
</tr>
<tr>
<td align="left">Average (1, k)</td>
<td align="left">0.42</td>
<td align="left">0.40</td>
<td align="left">0.45</td>
<td align="left">1.73</td>
<td align="left">4319</td>
<td align="left">38880</td>
<td align="left">0.000</td>
</tr>
<tr>
<td align="left" rowspan="2">Genuineness (song)</td>
<td align="left">Single (1, 1)</td>
<td align="left">0.07</td>
<td align="left">0.06</td>
<td align="left">0.07</td>
<td align="left">1.71</td>
<td align="left">3035</td>
<td align="left">27324</td>
<td align="left">0.000</td>
</tr>
<tr>
<td align="left">Average (1, k)</td>
<td align="left">0.42</td>
<td align="left">0.38</td>
<td align="left">0.45</td>
<td align="left">1.71</td>
<td align="left">3035</td>
<td align="left">27324</td>
<td align="left">0.000</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t005fn001"><p>Validity task intraclass correlations of the response scales emotional intensity and genuineness, for speech and song.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>Intraclass correlation single-rater values indicated “poor” agreement in speech and song for both intensity and genuineness response scales. Intraclass correlation multiple-rater values indicated “good” agreement in speech and song for both intensity and genuineness response scales. These values are comparable to those of existing sets. For intensity, the GEMEP corpus reported an average ICC(1,1) of 0.33, and ICC(1,k) of 0.9, while the Radbound Faces Database reported an ICC(1,1) of 0.20 and ICC(1,k) of 0.83. For genuineness ratings, the Radbound reported an ICC(1,1) of 0.13 and ICC(1,k) of 0.75.</p>
<p>To assess the effect of presentation mode on intensity ratings, a three-way repeated measures ANOVA was conducted on raters’ intensity scores by Channel (2 levels: speech, song), Modality (3 levels: audio-video, video, audio), and Intensity (2 levels: normal, strong). All statistical tests were significant. For conciseness, we only report posthoc tests for <inline-formula id="pone.0196391.e009"><alternatives><graphic id="pone.0196391.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> &gt; .10, see also [<xref ref-type="bibr" rid="pone.0196391.ref031">31</xref>]. A main effect of Channel was found, <italic>F</italic> (1, 246) = 9.33, p = 0.003, <inline-formula id="pone.0196391.e010"><alternatives><graphic id="pone.0196391.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> = .04. Pairwise comparisons confirmed that Speech <italic>M</italic> = 3.6, 95% CI [3.55, 3.65] &gt; Song <italic>M</italic> = 3.55, [3.50, 3.60]. A main effect of Modality was found, <italic>F</italic> (1.76, 433.67) = 239.86, p &lt; 0.001, <inline-formula id="pone.0196391.e011"><alternatives><graphic id="pone.0196391.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> = .49. Pairwise comparisons confirmed that Audio-Visual presentations <italic>M</italic> = 3.68, [3.64, 3.733] &gt; Video <italic>M</italic> = 3.65, [3.60, 3.70] &gt; Audio <italic>M</italic> = 3.38, [3.30, 3.44]. A main effect of Intensity was also found, <italic>F</italic> (1, 246) = 1202.26, p &lt; 0.001, <inline-formula id="pone.0196391.e012"><alternatives><graphic id="pone.0196391.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> = .83. Pairwise comparisons confirmed that Strong intensity presentations <italic>M</italic> = 3.83, [3.78, 3.87] &gt; Normal intensity presentations <italic>M</italic> = 3.31, [3.26, 3.37]. Significant interactions were also found for Channel × Modality, F (2, 492) = 18.88, p &lt; 0.001, <inline-formula id="pone.0196391.e013"><alternatives><graphic id="pone.0196391.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> = .07; Channel × Intensity, F (1, 246) = 16.08, p &lt; 0.001, <inline-formula id="pone.0196391.e014"><alternatives><graphic id="pone.0196391.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> = .06; Modality × Intensity, F (2, 492) = 10.82, p &lt; 0.001, <inline-formula id="pone.0196391.e015"><alternatives><graphic id="pone.0196391.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> = .04; and Channel ×Modality × Intensity, F (2, 492) = 11.01, p &lt; 0.001, <inline-formula id="pone.0196391.e016"><alternatives><graphic id="pone.0196391.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> = .04.</p>
<p>To assess the effect of presentation mode on genuineness ratings, a three-way repeated measures ANOVA was conducted on raters’ proportion correct scores by Channel (2 levels: speech, song), Modality (3 levels: audio-video, video, audio), and Intensity (2 levels: normal, strong). A main effect of Channel was found, <italic>F</italic> (1, 246) = 22.35, p &lt; 0.001, <inline-formula id="pone.0196391.e017"><alternatives><graphic id="pone.0196391.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> = .08. Pairwise comparisons confirmed that Speech <italic>M</italic> = 3.47, 95% CI [3.4, 3.54] &gt; Song <italic>M</italic> = 3.38, [3.31, 3.45]. A main effect of Modality was found, <italic>F</italic> (1.81, 444.68) = 19.89, p &lt; 0.001, <inline-formula id="pone.0196391.e018"><alternatives><graphic id="pone.0196391.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> = .08. Pairwise comparisons confirmed that Video <italic>M</italic> = 3.47, [3.40, 3.54] ~ Audio-Video <italic>M</italic> = 3.46, [3.38, 3.53] &gt; Audio <italic>M</italic> = 3.36, [3.29, 3.42]. A main effect of Intensity was found, <italic>F</italic> (1, 246) = 47.0, p &lt; 0.001, <inline-formula id="pone.0196391.e019"><alternatives><graphic id="pone.0196391.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> = .16. Pairwise comparisons confirmed that Strong intensity presentations <italic>M</italic> = 3.50, [3.42, 3.57] &gt; Normal intensity presentations <italic>M</italic> = 3.36, [3.29, 3.42]. Significant interactions were also found for Channel × Modality, F (2, 492) = 14.38, p &lt; 0.001, <inline-formula id="pone.0196391.e020"><alternatives><graphic id="pone.0196391.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> = .06 and Channel × Modality × Intensity, F (2, 492) = 4.71, p = 0.01, <inline-formula id="pone.0196391.e021"><alternatives><graphic id="pone.0196391.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196391.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">η</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> = .02.</p>
</sec>
</sec>
<sec id="sec036">
<title>Test-retest reliability task</title>
<p>The overall proportion correct for speech at Time 1 was high (mean = .70, SD = .46), and was comparable to accuracy rates at Time 2 (mean = .72, SD = .45). The overall proportion correct for song at Time 1 was also high (mean = .71, SD = .46), and was comparable to accuracy rates at Time 2 (mean = .71, SD = .45). Intrarater reliability scores were calculated to quantify test—retest reliability of the stimuli. Kappa values indicated “substantial” intrarater reliability for strong expressions in speech (κ = .76, n = 2016), and song (mean κ = .77, n = 1380), and “substantial” reliability for normal expressions in speech (κ = .70, n = 2304) and song (κ = .68, n = 1656). Category-wise values are reported by Emotion, for speech and song, in <xref ref-type="table" rid="pone.0196391.t006">Table 6</xref>.</p>
<table-wrap id="pone.0196391.t006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0196391.t006</object-id>
<label>Table 6</label> <caption><title>Test-retest task intrarater reliability ratings by emotion and channel.</title></caption>
<alternatives>
<graphic id="pone.0196391.t006g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.t006" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Emotion</th>
<th align="left">Mean Proportion correct Time 1 (SD)</th>
<th align="left">Mean Proportion correct Time 2 (SD)</th>
<th align="left">Kappa</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Neutral (speech)</td>
<td align="left">0.85 (0.36)</td>
<td align="left">0.89 (0.31)</td>
<td align="left">0.75</td>
</tr>
<tr>
<td align="left">Neutral (song)</td>
<td align="left">0.78 (0.42)</td>
<td align="left">0.82 (0.39)</td>
<td align="left">0.67</td>
</tr>
<tr>
<td align="left">Calm (speech)</td>
<td align="left">0.73 (0.45)</td>
<td align="left">0.72 (0.45)</td>
<td align="left">0.75</td>
</tr>
<tr>
<td align="left">Calm (song)</td>
<td align="left">0.64 (0.48)</td>
<td align="left">0.62 (0.49)</td>
<td align="left">0.67</td>
</tr>
<tr>
<td align="left">Happy (speech)</td>
<td align="left">0.67 (0.47)</td>
<td align="left">0.67 (0.47)</td>
<td align="left">0.77</td>
</tr>
<tr>
<td align="left">Happy (song)</td>
<td align="left">0.73 (0.44)</td>
<td align="left">0.75 (0.44)</td>
<td align="left">0.79</td>
</tr>
<tr>
<td align="left">Sad (speech)</td>
<td align="left">0.62 (0.49)</td>
<td align="left">0.61 (0.49)</td>
<td align="left">0.73</td>
</tr>
<tr>
<td align="left">Sad (song)</td>
<td align="left">0.69 (0.46)</td>
<td align="left">0.67 (0.47)</td>
<td align="left">0.70</td>
</tr>
<tr>
<td align="left">Angry (speech)</td>
<td align="left">0.77 (0.42)</td>
<td align="left">0.80 (0.40)</td>
<td align="left">0.77</td>
</tr>
<tr>
<td align="left">Angry (song)</td>
<td align="left">0.82 (0.38)</td>
<td align="left">0.83 (0.37)</td>
<td align="left">0.83</td>
</tr>
<tr>
<td align="left">Fearful (speech)</td>
<td align="left">0.71 (0.45)</td>
<td align="left">0.73 (0.44)</td>
<td align="left">0.75</td>
</tr>
<tr>
<td align="left">Fearful (song)</td>
<td align="left">0.62 (0.49)</td>
<td align="left">0.64 (0.48)</td>
<td align="left">0.72</td>
</tr>
<tr>
<td align="left">Disgust (speech)</td>
<td align="left">0.68 (0.47)</td>
<td align="left">0.71 (0.45)</td>
<td align="left">0.73</td>
</tr>
<tr>
<td align="left">Surprise (speech)</td>
<td align="left">0.68 (0.44)</td>
<td align="left">0.73 (0.44)</td>
<td align="left">0.73</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t006fn001"><p>Ratings from the test-retest intrarater reliability task across emotions, in speech and song (N = 72 participants, each rating 102 stimuli twice).</p></fn>
</table-wrap-foot>
</table-wrap>
<p>Intrarater reliability of the ratings provided for emotional intensity and emotional genuineness at Time 1 and Time 2 were estimated with intraclass correlations, separately for speech and song, and are presented in <xref ref-type="table" rid="pone.0196391.t007">Table 7</xref>.</p>
<table-wrap id="pone.0196391.t007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0196391.t007</object-id>
<label>Table 7</label> <caption><title>Test-retest task intrarater ICC calculations for intensity and genuineness using single- and multiple-rating, consistency-agreement, 1-way random-effects models.</title></caption>
<alternatives>
<graphic id="pone.0196391.t007g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.t007" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="2">Response Scale</th>
<th align="left" rowspan="2">ICC test</th>
<th align="left" rowspan="2">Value</th>
<th align="left" colspan="2">95% Conf. Interval</th>
<th align="left" colspan="4">F-test with True Value 0</th>
</tr>
<tr>
<th align="left">Lower bound</th>
<th align="left">Upper bound</th>
<th align="left">Value</th>
<th align="left">df1</th>
<th align="left">df2</th>
<th align="left">Sig</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="2">Intensity (speech)</td>
<td align="left">Single (1, 1)</td>
<td align="left">0.46</td>
<td align="left">0.44</td>
<td align="left">0.49</td>
<td align="left">2.71</td>
<td align="left">4319</td>
<td align="left">4320</td>
<td align="left">0.000</td>
</tr>
<tr>
<td align="left">Average (1, k)</td>
<td align="left">0.63</td>
<td align="left">0.61</td>
<td align="left">0.65</td>
<td align="left">2.71</td>
<td align="left">4319</td>
<td align="left">4320</td>
<td align="left">0.000</td>
</tr>
<tr>
<td align="left" rowspan="2">Intensity (song)</td>
<td align="left">Single (1, 1)</td>
<td align="left">0.46</td>
<td align="left">0.43</td>
<td align="left">0.49</td>
<td align="left">2.70</td>
<td align="left">3035</td>
<td align="left">3036</td>
<td align="left">0.000</td>
</tr>
<tr>
<td align="left">Average (1, k)</td>
<td align="left">0.63</td>
<td align="left">0.60</td>
<td align="left">0.66</td>
<td align="left">2.70</td>
<td align="left">3035</td>
<td align="left">3036</td>
<td align="left">0.000</td>
</tr>
<tr>
<td align="left" rowspan="2">Genuineness (speech)</td>
<td align="left">Single (1, 1)</td>
<td align="left">0.42</td>
<td align="left">0.39</td>
<td align="left">0.44</td>
<td align="left">2.42</td>
<td align="left">4319</td>
<td align="left">4320</td>
<td align="left">0.000</td>
</tr>
<tr>
<td align="left">Average (1, k)</td>
<td align="left">0.59</td>
<td align="left">0.56</td>
<td align="left">0.61</td>
<td align="left">2.42</td>
<td align="left">4319</td>
<td align="left">4320</td>
<td align="left">0.000</td>
</tr>
<tr>
<td align="left" rowspan="2">Genuineness (song)</td>
<td align="left">Single (1, 1)</td>
<td align="left">0.43</td>
<td align="left">0.40</td>
<td align="left">0.45</td>
<td align="left">2.48</td>
<td align="left">3035</td>
<td align="left">3036</td>
<td align="left">0.000</td>
</tr>
<tr>
<td align="left">Average (1, k)</td>
<td align="left">0.60</td>
<td align="left">0.57</td>
<td align="left">0.62</td>
<td align="left">2.48</td>
<td align="left">3035</td>
<td align="left">3036</td>
<td align="left">0.000</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Intrarater intraclass correlation single-rater values indicated “fair” agreement in speech and song for both intensity and genuineness response scales. Intraclass correlation multiple-rater values indicated “fair” to “good” agreement in speech and song for both intensity and genuineness response scales.</p>
</sec>
</sec>
<sec id="sec037" sec-type="conclusions">
<title>Discussion</title>
<p>In this paper, we described the construction and validation of the RAVDESS, a set of emotional expressions that are dynamic and multimodal. The RAVDESS has several important features that lend itself for use by scientists, engineers, and clinicians: it is large in number, it contains visual and auditory depictions of spoken and sung expressions, it consists of professional actors from North America, it has a variety of emotional expressions at two levels of emotional intensity, and it is made freely available under a Creative Commons non-commercial license.</p>
<p>Validation of the RAVDESS was performed with 247 raters from North America. Validity referred to the accuracy with which participants correctly identified the actors’ intended emotions. We examined proportion correct scores, as is commonly reported in the literature. Overall scores were high, achieving 80% for audio-video, 75% for video-only, and 60% for audio-only. These scores are comparable to the CREMA-D, the only other validated English database of audio-visual vocal emotion [<xref ref-type="bibr" rid="pone.0196391.ref052">52</xref>], which achieved 64%, 58%, and 41% respectively for the three modalities, and those of the GEMEP [<xref ref-type="bibr" rid="pone.0196391.ref057">57</xref>], consisting of French audio-visual vocal expressions, which achieved 73%, 59%, and 44% respectively for the three modalities. Audio-only productions also fared well against batteries of affective vocal productions, including the Montreal affective voices [<xref ref-type="bibr" rid="pone.0196391.ref050">50</xref>] at 69%, Portuguese sentences [<xref ref-type="bibr" rid="pone.0196391.ref117">117</xref>] at 75%, and German sentences [<xref ref-type="bibr" rid="pone.0196391.ref099">99</xref>] at 85% (calculated from files received from the author, Dr. Burkhardt, through personal communication). As proportion correct scores do not correct for false alarms, unbiased hit rate were also reported, as were Fleiss’ kappa, a chance-corrected measure of interrater reliability. According to the guidelines provided by Landis and Koch [<xref ref-type="bibr" rid="pone.0196391.ref140">140</xref>], strong expressions of emotion fell within the substantial range of inter-rater agreement with a mean kappa of 0.61, while normal intensity expressions fell within the moderate range of inter-rater reliability with a mean kappa of 0.53.</p>
<p>Test-retest reliability of the RAVDESS was assessed with an additional 72 raters from North America. Reliability referred to the likelihood of participants selecting the same emotional category for a given stimulus presented twice. Cohen’s kappa scores were used to quantify the degree of intrarater agreement. Test-retest reliability was high, with a mean kappa of 0.73, falling well within substantial range of intrarater agreement. We are not aware of any other database that has provided test-retest kappa scores. Collectively, these results confirm that the RAVDESS has good validity and test-retest reliability.</p>
<p>Validity measures revealed variations in accuracy across emotional categories and presentation modalities. These variations are common in studies of emotional perception and reflect the nature of emotion as a complex form of communication, one that is strongly affected by the mode of presentation [<xref ref-type="bibr" rid="pone.0196391.ref007">7</xref>, <xref ref-type="bibr" rid="pone.0196391.ref075">75</xref>, <xref ref-type="bibr" rid="pone.0196391.ref127">127</xref>]. Strong intensity audiovisual displays were identified with 83% accuracy, which is comparable to the mean accuracy rates reported for the Pictures of Facial Affect at 88% [<xref ref-type="bibr" rid="pone.0196391.ref021">21</xref>], JACFEE at 74% [<xref ref-type="bibr" rid="pone.0196391.ref154">154</xref>], and NimStim at 79% [<xref ref-type="bibr" rid="pone.0196391.ref029">29</xref>]. These same displays presented in audio-only achieved 65% accuracy. The effect of modality appeared to vary with emotion; disgust and happiness achieved 88% and 84% accuracy in audio-visual speech yet 54% and 44% in audio-only speech; while anger achieved over 90% in both modalities. A recent review of multimodal emotional perception similarly found a face-bias for happiness and disgust, but not anger [<xref ref-type="bibr" rid="pone.0196391.ref127">127</xref>].</p>
<p>Sung expressions of emotion performed comparably with spoken expressions, achieving 71% and 72% respectively. The inclusion of a lexically-matched set of emotional song is an important distinguishing feature of the RAVDESS. To our knowledge, the RAVDESS is the only validated set of emotional song, and is one of only a handful of validated sets of musical emotion [<xref ref-type="bibr" rid="pone.0196391.ref155">155</xref>, <xref ref-type="bibr" rid="pone.0196391.ref156">156</xref>]. The scope of the song set, at 3036 files, is significantly larger than existing sets, which contain fewer than 250 clips. The RAVDESS is the only set that includes audio-visual and video-only displays of musical emotion. There is significant research highlighting the importance of visual information in the expression of musical emotion [<xref ref-type="bibr" rid="pone.0196391.ref157">157</xref>–<xref ref-type="bibr" rid="pone.0196391.ref159">159</xref>] and the coordination of music performance [<xref ref-type="bibr" rid="pone.0196391.ref160">160</xref>]. The RAVDESS may therefore be of interest to researchers in music cognition and computer music performance.</p>
<p>The intensity of actors’ productions had a large effect on participant ratings. Strong intensity productions were identified more accurately, were rated as more emotionally intense, and rated as more genuine that normal intensity productions. These results are in line with research which has shown that strongly intense displays are identified more accurately in faces and voices [<xref ref-type="bibr" rid="pone.0196391.ref053">53</xref>, <xref ref-type="bibr" rid="pone.0196391.ref065">65</xref>, <xref ref-type="bibr" rid="pone.0196391.ref066">66</xref>]. Production studies have revealed differences in the facial and vocal expressions of intense emotions. Facial expressions with increased muscle contraction are rated as more emotionally intense [<xref ref-type="bibr" rid="pone.0196391.ref065">65</xref>]. Head movements of vocalists exhibit larger and faster movements, and greater rotational turning when expressing intense emotions [<xref ref-type="bibr" rid="pone.0196391.ref161">161</xref>]. Acoustic profiles of the voice also show clear differences in emotional intensity [<xref ref-type="bibr" rid="pone.0196391.ref066">66</xref>, <xref ref-type="bibr" rid="pone.0196391.ref162">162</xref>]. These findings suggest that intense expressions, like those in the RAVDESS, have facial and vocal features that are more readily identified than their less intense counterparts.</p>
<p>Validation measures revealed a pattern of confusions between several emotion categories. Calm was misidentified as happy for 19% of responses, sad as neutral or calm at 17%, and happy as neutral or calm at 14%. Previous research has found that neutral productions convey a mildly negative emotional valence [<xref ref-type="bibr" rid="pone.0196391.ref071">71</xref>]. Raters misidentification of sadness with neutral/calm support this finding. Calm was included as a second baseline expression to convey a mild, positively valenced emotion. Misidentification rates suggests that raters confused happy with the mildly positively valenced calm expressions.</p>
<p>Ratings of emotional intensity and genuineness were also reported. Both inter-rater and intrarater reliability of these scales was assessed with intraclass correlations. According to the guidelines provided by Cicchetti [<xref ref-type="bibr" rid="pone.0196391.ref143">143</xref>], inter-rater single-measure ratings of intensity and genuineness fell within the poor range of reliability, and good-to-fair range respectively for average-measure ratings. These results suggest that there was little-to-moderate consistency between raters in their evaluations of intensity and genuineness. Interestingly, test-retest intrarater reliability fell within the fair range for single-measures, and good-to-fair range respectively for average-measure ratings. These results suggest that ratings of intensity and genuineness were more consistent in the context of test-retest than in the context of between raters. That is, raters were more consistent in their own ratings across multiple presentations, but that these ratings were more variable between raters. Collectively, this suggests that while intensity had a strong effect on raters’ accuracy of emotional identification, the emotional properties of intensity and genuineness were not identified consistently by raters. To our knowledge, there has been no investigation assessing the accuracy with which emotional intensity or genuineness can be identified, as these measures are typically assessed using a continuous Likert-scale response paradigm. Thus, it is unclear if the reported reliability values are a function of the RAVDESS stimuli or a more general property of these emotional concepts. This topic warrants further study. Regardless, investigators should interpret measures of intensity and genuineness with caution when selecting appropriate stimuli.</p>
<p>The RAVDESS included a set of six basic emotions that are thought to be culturally universal. This decision was based partly on the design goal of providing a set of emotions with high discriminability. A criticism of universal emotions is that there are few positively-valenced states [<xref ref-type="bibr" rid="pone.0196391.ref112">112</xref>, <xref ref-type="bibr" rid="pone.0196391.ref163">163</xref>, <xref ref-type="bibr" rid="pone.0196391.ref164">164</xref>]. Several sets have sought to overcome this issue [<xref ref-type="bibr" rid="pone.0196391.ref050">50</xref>, <xref ref-type="bibr" rid="pone.0196391.ref057">57</xref>, <xref ref-type="bibr" rid="pone.0196391.ref165">165</xref>]. Two of these sets developed non-verbal utterances, including pleasure [<xref ref-type="bibr" rid="pone.0196391.ref050">50</xref>], and pleasure, triumph, amusement, relief [<xref ref-type="bibr" rid="pone.0196391.ref165">165</xref>]. While these audio-only, non-verbal utterances were accurately identified, to our knowledge there has been no validation of these states in facial-only or facial-verbal modalities. Recent research also suggests that the acoustic expression of these states may not be culturally universal [<xref ref-type="bibr" rid="pone.0196391.ref166">166</xref>]. We chose not to include these states as face-and-voice and face-only are both integral modalities of expression in the RAVDESS. The GEMEP also included a broader range of positive emotions (pleasure, elated, joy, pride, amusement, relief, and interest). However, most of these states achieved recognition rates at or below 40%. As the authors note, empirical evidence on their expressive characteristics is scarce. As a primary goal of the RAVDESS was to provide emotions with high discriminability, we opted not to include additional “positive” emotional states.</p>
<p>The construction and validation of the RAVDESS used aspects of both the discrete and continuous theories of emotion. The division of emotions into distinct categories with verbal labels (e.g., happy, sad) is grounded in discrete emotion theory. Emotions were also produced and rated in terms of their emotional intensity–a continuous scale which draws from dimensional models of emotion. Dimensional models began with the works of Spencer [<xref ref-type="bibr" rid="pone.0196391.ref167">167</xref>] and Wundt [<xref ref-type="bibr" rid="pone.0196391.ref168">168</xref>], and classify emotions as existing within a multidimensional space, generally defined by the orthogonal dimensions of arousal and valence [<xref ref-type="bibr" rid="pone.0196391.ref064">64</xref>, <xref ref-type="bibr" rid="pone.0196391.ref169">169</xref>–<xref ref-type="bibr" rid="pone.0196391.ref174">174</xref>]. Perceptual ratings of emotional stimuli often involve ratings along the dimensions of arousal and valence [<xref ref-type="bibr" rid="pone.0196391.ref105">105</xref>]. An important avenue for future work with the RAVDESS will be to provide dimensional ratings of arousal and valence.</p>
<p>Stimuli were validated using a forced-choice emotion response format. A criticism of forced-choice emotion paradigms is that they can bias the participant towards a hypothesis, leading to artificially forced agreement [<xref ref-type="bibr" rid="pone.0196391.ref113">113</xref>]. To address this criticism, our response paradigm included the “None of these are correct” option, proposed by Frank and Stennett [<xref ref-type="bibr" rid="pone.0196391.ref137">137</xref>]. Participants selected this option less than 3% of the time (see <xref ref-type="supplementary-material" rid="pone.0196391.s006">S3 Table</xref>), providing further support for the contention that RAVDESS actors provided clear exemplars of emotion.</p>
<p>The RAVDESS was constructed using induced expressions of emotion. These expressions were elicited using techniques the actors had been trained in, including method acting or emotional memory techniques. This form of elicitation has been used successfully in previous studies with the goal of producing more authentic displays [<xref ref-type="bibr" rid="pone.0196391.ref057">57</xref>, <xref ref-type="bibr" rid="pone.0196391.ref125">125</xref>, <xref ref-type="bibr" rid="pone.0196391.ref161">161</xref>, <xref ref-type="bibr" rid="pone.0196391.ref162">162</xref>]. Other methods of inducing an emotional state including presentation of films or music, mental imagery methods, or stressor scenarios. However, these procedures can produce relatively weak effects, and there may be uncertainty as to the emotion that was elicited [<xref ref-type="bibr" rid="pone.0196391.ref175">175</xref>, <xref ref-type="bibr" rid="pone.0196391.ref176">176</xref>]. For these reasons, we opted for induction techniques that our actors had used throughout their careers.</p>
<p>The use of induced emotional expressions contrasts with sets that use naturally occurring spontaneous or “in the wild” expressions. In these sets, recordings of individuals in real-life situations are taken from a variety of sources, such as television, clinical interviews, lost baggage offices, and online video streaming services [<xref ref-type="bibr" rid="pone.0196391.ref177">177</xref>]. Both approaches have strengths and weaknesses. A criticism of induced expressions is that they can be exaggerated, leading to inflated rates of observer agreement relative to spontaneous displays [<xref ref-type="bibr" rid="pone.0196391.ref075">75</xref>, <xref ref-type="bibr" rid="pone.0196391.ref178">178</xref>–<xref ref-type="bibr" rid="pone.0196391.ref180">180</xref>]. There may also be fewer individuals in induced sets, which commonly use a within-subjects design. This contrasts with spontaneous sets that may have hundreds or thousands of different individuals in a between-subjects format. However, induced expressions offer several important advantages over spontaneous expressions. First, experimenters have confidence in the emotion being expressed. This contrasts with naturalistic recordings in which the emotional category of the expression is labelled after the fact by the experimenter or participant ratings. This labelling procedure raises serious concerns about the reliability of the assigned categories, as well whether the expressions reflect truly natural emotions [<xref ref-type="bibr" rid="pone.0196391.ref127">127</xref>, <xref ref-type="bibr" rid="pone.0196391.ref181">181</xref>]. Second, induced expressions are intended to convey a single emotional category, for example “happy” or “sad”. Naturalistic recordings however are often given mixed labels by raters, with all but a few given a single clear category [<xref ref-type="bibr" rid="pone.0196391.ref182">182</xref>]. Finally, induced sets maintain good experimental control where actors can be recorded expressing every emotional category, using repeated lexical material, while environmental aspects including lighting, clothing, recording equipment, and background setting can remain constant. This contrasts with naturalistic sets where individuals appear in only one or two clips, and the recording situation, material, and quality can vary substantially [<xref ref-type="bibr" rid="pone.0196391.ref126">126</xref>].</p>
<p>The RAVDESS is large in scope, containing 7356 validated presentations of emotion. During construction, several sets removed stimuli that were identified at or below defined accuracy levels [<xref ref-type="bibr" rid="pone.0196391.ref050">50</xref>, <xref ref-type="bibr" rid="pone.0196391.ref183">183</xref>], while others produced core sets containing “optimal” stimuli [<xref ref-type="bibr" rid="pone.0196391.ref057">57</xref>, <xref ref-type="bibr" rid="pone.0196391.ref119">119</xref>]. In this initial iteration of the RAVDESS, we chose to include the full corpus of recordings. These recordings and their ratings provide researchers with a rich dataset of highly accurate, mixed, and inaccurate expressions of emotion. A significant body of literature has been dedicated to identifying what features lead to an accurate emotional expression. However, much can be learned from why particular expressions are conveyed inaccurately. These recordings provide researchers with a large data set from which to examine questions related to both the accuracy and inaccuracy of emotional expressions.</p>
<p>There are several shortcomings of the RAVDESS. Firstly, the scope of the database precluded the use of a fully within-subjects rating methodology. The validity task presented a subset of 298 clips to each rater. We opted for this design as it provided greater representative statistical variance over the use of a limited pool of fully within-subjects raters–an approach that has been criticized [<xref ref-type="bibr" rid="pone.0196391.ref113">113</xref>]. This design choice however limited the range of statistical tests that could be reported. Despite this, the reported measures provided evidence of high validity and reliability. Relatedly, each recording was only rated 10 times. While several sets have used similar numbers of ratings [<xref ref-type="bibr" rid="pone.0196391.ref032">32</xref>, <xref ref-type="bibr" rid="pone.0196391.ref052">52</xref>], this may not be sufficient for certain paradigms. Another shortcoming was the exclusion of “self-conscious” emotions, such as shame, pride, and embarrassment [<xref ref-type="bibr" rid="pone.0196391.ref184">184</xref>–<xref ref-type="bibr" rid="pone.0196391.ref186">186</xref>]. We chose not to include these expressions as there is limited evidence that these states can be conveyed effectively through vocal communication, as prior research has focused on facial expressions and body postures. As with the exclusion of surprise and disgust in the song corpus, we opted to include emotions that are known to be reliably and accurately expressed through vocal communication. Another limitation was the use of trained investigators for the review and selection of actors’ raw productions, rather than using large-scale perceptual tests. This decision reflected the need to remove problematic stimuli (e.g., presence of hand movements and gestures, lexical errors, microphone peaking and pops), and to select the clearest exemplars of emotion. The use of expert investigators for an initial review of raw productions during recording or post-recording is common in emotion sets [<xref ref-type="bibr" rid="pone.0196391.ref032">32</xref>, <xref ref-type="bibr" rid="pone.0196391.ref033">33</xref>, <xref ref-type="bibr" rid="pone.0196391.ref057">57</xref>, <xref ref-type="bibr" rid="pone.0196391.ref058">58</xref>, <xref ref-type="bibr" rid="pone.0196391.ref076">76</xref>, <xref ref-type="bibr" rid="pone.0196391.ref119">119</xref>, <xref ref-type="bibr" rid="pone.0196391.ref156">156</xref>, <xref ref-type="bibr" rid="pone.0196391.ref165">165</xref>, <xref ref-type="bibr" rid="pone.0196391.ref187">187</xref>–<xref ref-type="bibr" rid="pone.0196391.ref190">190</xref>]. However, a consequence of this procedure may have been a selection bias driven by investigators’ prior expectations for prototypical expressions. A final limitation was the inclusion of only two statements, limiting the lexical variability of the database. While increased lexical variability would have been beneficial, we chose to prioritize diversity in actors, emotions, and intensities, while matching speech-and-song productions. As adding a third statement would have increased the size of the database by 3678 files, and required an additional 125 raters, we opted to use only two statements in the RAVDESS.</p>
</sec>
<sec id="sec038" sec-type="conclusions">
<title>Conclusion</title>
<p>In this paper, we presented the Ryerson Audio-Visual Database of Emotional Speech and Song, a set of multimodal, dynamic expressions of basic emotions. The RAVDESS is one of only two databases of audiovisual vocal expressions presented in North American English. The set consists of a large number unique speech and song recordings, each available in audio-visual, video-only, and audio-only high-definition formats. Participant testing involving untrained research participants revealed high rates of emotional validity and test-retest reliability. We believe this set will be of interest to a wide variety of researchers and engineers. The RAVDESS is made freely available under a Creative Commons non-commercial license, and can be downloaded at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1188976" xlink:type="simple">https://doi.org/10.5281/zenodo.1188976</ext-link>.</p>
</sec>
<sec id="sec039">
<title>Supporting information</title>
<supplementary-material id="pone.0196391.s001" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Tree diagram of experimental design of speech corpus.</title>
<p>Breakdown of experimental factor-levels and number of recordings per factor-level. Square brackets report [number of files for that specific internal-node, and total number of files for that factor-level]. For example, female-vocalist-1 has 60 face-video recordings, while there are 1440 face-video recordings in the set. A double outlined box indicates a leaf-node.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0196391.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.s002" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Tree diagram of experimental design of song corpus.</title>
<p>Breakdown of experimental factor-levels and number of recordings per factor-level. Square brackets report [number of files for that specific internal-node, and total number of files for that factor-level]. For example, female-vocalist-1 has 44 face-video recordings, while there are 1012 face-video recordings in the set. A double outlined box indicates a leaf-node. Note, the song corpus contains 11 females and 12 males, and differs to the speech corpus which is matched on gender (12 each).</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0196391.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.s003" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Response task option screens.</title>
<p>Response option screens presented to participants during the validity and reliability tasks, showing: (a) Emotion category (b) Emotional intensity (c) Genuineness.</p>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0196391.s004" mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.s004" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Validity task measures for all stimuli.</title>
<p>Proportion correct scores, unbiased hit rates, intensity ratings, genuineness ratings, their respective response times, and goodness scores, for all 7356 RAVDESS stimuli.</p>
<p>(XLSX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0196391.s005" mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.s005" xlink:type="simple">
<label>S2 Table</label>
<caption>
<title>Validity task measures summarized by actor.</title>
<p>Mean scores by actor for proportion correct, unbiased hit rates, intensity, genuineness, response times, goodness, and file duration, separately for speech and song.</p>
<p>(XLSX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0196391.s006" mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.s006" xlink:type="simple">
<label>S3 Table</label>
<caption>
<title>Confusion matrices of proportion correct measures.</title>
<p>Confusion matrices showing the average proportion of target and non-target labels selected by raters for each intended emotional expression, for speech and song.</p>
<p>(XLSX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0196391.s007" mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.s007" xlink:type="simple">
<label>S4 Table</label>
<caption>
<title>Raw test-retest response data for all stimuli.</title>
<p>Raw response data from the test-retest reliability task for all 7356 stimuli. Includes rater identified emotional category (neutral 1, calm 2, happy 3, sad 4, angry 5, fearful 6, surprise 7, disgust 8, none 9), and coded raw accuracy (incorrect 0, correct 1), for presentations at Time 1 and Time 2.</p>
<p>(XLSX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0196391.s008" mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.s008" xlink:type="simple">
<label>S5 Table</label>
<caption>
<title>Raw validity response data for all stimuli.</title>
<p>Raw response data from the validity task for all 7356 stimuli. Includes rater identified emotional category (neutral 1, calm 2, happy 3, sad 4, angry 5, fearful 6, surprise 7, disgust 8, none 9), coded raw accuracy (incorrect 0, correct 1), emotional intensity (very weak 1, to very strong 5), and emotional genuineness (not genuine 1, to very genuine 5).</p>
<p>(XLSX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0196391.s009" mimetype="video/mp4" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.s009" xlink:type="simple">
<label>S1 File</label>
<caption>
<title>Audiovisual examples of RAVDESS speech stimuli.</title>
<p>Movie file presenting strong intensity displays of eight speech emotions: neutral, calm, happy, sad, angry, fearful, disgust, and surprise.</p>
<p>(MP4)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0196391.s010" mimetype="video/mp4" position="float" xlink:href="info:doi/10.1371/journal.pone.0196391.s010" xlink:type="simple">
<label>S2 File</label>
<caption>
<title>Audiovisual examples of RAVDESS song stimuli.</title>
<p>Movie file presenting strong intensity displays of six speech emotions: neutral, calm, happy, sad, angry, and fearful.</p>
<p>(MP4)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The authors thank Katlyn Peck, Nicola Dove, James McGrath, Gabe Nespoli, Saul Moshberg, and Mia Saadon and for their contributions and assistance, and all the participants who enabled us to create and validate this database.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0196391.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>de Gelder</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Vroomen</surname> <given-names>J</given-names></name>. <article-title>The perception of emotions by ear and by eye</article-title>. <source>Cognition &amp; Emotion</source>. <year>2000</year>;<volume>14</volume>(<issue>3</issue>):<fpage>289</fpage>–<lpage>311</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/026999300378824" xlink:type="simple">10.1080/026999300378824</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dolan</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Morris</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>de Gelder</surname> <given-names>B</given-names></name>. <article-title>Crossmodal binding of fear in voice and face</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2001</year>;<volume>98</volume>(<issue>17</issue>):<fpage>10006</fpage>–<lpage>10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.171288598" xlink:type="simple">10.1073/pnas.171288598</ext-link></comment> <object-id pub-id-type="pmid">11493699</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pourtois</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>de Gelder</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Vroomen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Rossion</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Crommelinck</surname> <given-names>M</given-names></name>. <article-title>The time‐course of intermodal binding between seeing and hearing affective information</article-title>. <source>NeuroReport</source>. <year>2000</year>;<volume>11</volume>(<issue>6</issue>):<fpage>1329</fpage>–<lpage>33</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1097/00001756-200004270-00036" xlink:type="simple">10.1097/00001756-200004270-00036</ext-link></comment> <object-id pub-id-type="pmid">10817616</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>de Gelder</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Vroomen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>de Jong</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Masthoff</surname> <given-names>ED</given-names></name>, <name name-style="western"><surname>Trompenaars</surname> <given-names>FJ</given-names></name>, <name name-style="western"><surname>Hodiamont</surname> <given-names>P</given-names></name>. <article-title>Multisensory integration of emotional faces and voices in schizophrenics</article-title>. <source>Schizophrenia Research</source>. <year>2005</year>;<volume>72</volume>(<issue>2–3</issue>):<fpage>195</fpage>–<lpage>203</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.schres.2004.02.013" xlink:type="simple">10.1016/j.schres.2004.02.013</ext-link></comment> <object-id pub-id-type="pmid">15560964</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kreifelts</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Ethofer</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Grodd</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Erb</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wildgruber</surname> <given-names>D</given-names></name>. <article-title>Audiovisual integration of emotional signals in voice and face: an event-related fMRI study</article-title>. <source>NeuroImage</source>. <year>2007</year>;<volume>37</volume>(<issue>4</issue>):<fpage>1445</fpage>–<lpage>56</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2007.06.020" xlink:type="simple">10.1016/j.neuroimage.2007.06.020</ext-link></comment> <object-id pub-id-type="pmid">17659885</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Massaro</surname> <given-names>DW</given-names></name>, <name name-style="western"><surname>Egan</surname> <given-names>PB</given-names></name>. <article-title>Perceiving affect from the voice and the face</article-title>. <source>Psychonomic Bulletin &amp; Review</source>. <year>1996</year>;<volume>3</volume>(<issue>2</issue>):<fpage>215</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/bf03212421" xlink:type="simple">10.3758/bf03212421</ext-link></comment> <object-id pub-id-type="pmid">24213870</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Collignon</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Girard</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gosselin</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Roy</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Saint-Amour</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Lassonde</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Audio-visual integration of emotion expression</article-title>. <source>Brain Research</source>. <year>2008</year>;<volume>1242</volume>:<fpage>126</fpage>–<lpage>35</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.brainres.2008.04.023" xlink:type="simple">10.1016/j.brainres.2008.04.023</ext-link></comment> <object-id pub-id-type="pmid">18495094</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>de Gelder</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Bertelson</surname> <given-names>P</given-names></name>. <article-title>Multisensory integration, perception and ecological validity</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2003</year>;<volume>7</volume>(<issue>10</issue>):<fpage>460</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2003.08.014" xlink:type="simple">10.1016/j.tics.2003.08.014</ext-link></comment> <object-id pub-id-type="pmid">14550494</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tanaka</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Koizumi</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Imai</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Hiramatsu</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hiramoto</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>de Gelder</surname> <given-names>B</given-names></name>. <article-title>I feel your voice: Cultural differences in the multisensory perception of emotion</article-title>. <source>Psychological Science</source>. <year>2010</year>;<volume>21</volume>(<issue>9</issue>):<fpage>1259</fpage>–<lpage>62</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0956797610380698" xlink:type="simple">10.1177/0956797610380698</ext-link></comment> <object-id pub-id-type="pmid">20713633</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Campanella</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Belin</surname> <given-names>P</given-names></name>. <article-title>Integrating face and voice in person perception</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2007</year>;<volume>11</volume>(<issue>12</issue>):<fpage>535</fpage>–<lpage>43</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2007.10.001" xlink:type="simple">10.1016/j.tics.2007.10.001</ext-link></comment> <object-id pub-id-type="pmid">17997124</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Regenbogen</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Schneider</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Gur</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Schneider</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Habel</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Kellermann</surname> <given-names>T</given-names></name>. <article-title>Multimodal human communication—targeting facial expressions, speech content and prosody</article-title>. <source>NeuroImage</source>. <year>2012</year>;<volume>60</volume>(<issue>4</issue>):<fpage>2346</fpage>–<lpage>56</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2012.02.043" xlink:type="simple">10.1016/j.neuroimage.2012.02.043</ext-link></comment> <object-id pub-id-type="pmid">22487549</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thompson</surname> <given-names>WF</given-names></name>, <name name-style="western"><surname>Russo</surname> <given-names>FA</given-names></name>, <name name-style="western"><surname>Quinto</surname> <given-names>L</given-names></name>. <article-title>Audio-visual integration of emotional cues in song</article-title>. <source>Cognition and Emotion</source>. <year>2008</year>;<volume>22</volume>(<issue>8</issue>):<fpage>1457</fpage>–<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/02699930701813974" xlink:type="simple">10.1080/02699930701813974</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Livingstone</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Thompson</surname> <given-names>WF</given-names></name>, <name name-style="western"><surname>Wanderley</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Palmer</surname> <given-names>C</given-names></name>. <article-title>Common cues to emotion in the dynamic facial expressions of speech and song</article-title>. <source>The Quarterly Journal of Experimental Psychology</source>. <year>2015</year>;<volume>68</volume>(<issue>5</issue>):<fpage>952</fpage>–<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/17470218.2014.971034" xlink:type="simple">10.1080/17470218.2014.971034</ext-link></comment> <object-id pub-id-type="pmid">25424388</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balconi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Carrera</surname> <given-names>A</given-names></name>. <article-title>Cross-modal integration of emotional face and voice in congruous and incongruous pairs: The P2 ERP effect</article-title>. <source>Journal of Cognitive Psychology</source>. <year>2011</year>;<volume>23</volume>(<issue>1</issue>):<fpage>132</fpage>–<lpage>9</lpage>.</mixed-citation></ref>
<ref id="pone.0196391.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paulmann</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pell</surname> <given-names>MD</given-names></name>. <article-title>Is there an advantage for recognizing multi-modal emotional stimuli?</article-title> <source>Motivation and Emotion</source>. <year>2011</year>;<volume>35</volume>(<issue>2</issue>):<fpage>192</fpage>–<lpage>201</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s11031-011-9206-0" xlink:type="simple">10.1007/s11031-011-9206-0</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Flom</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bahrick</surname> <given-names>LE</given-names></name>. <article-title>The development of infant discrimination of affect in multimodal and unimodal stimulation: The role of intersensory redundancy</article-title>. <source>Developmental Psychology</source>. <year>2007</year>;<volume>43</volume>(<issue>1</issue>):<fpage>238</fpage>–<lpage>52</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0012-1649.43.1.238" xlink:type="simple">10.1037/0012-1649.43.1.238</ext-link></comment> <object-id pub-id-type="pmid">17201522</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Regenbogen</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Schneider</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Finkelmeyer</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Kohn</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Derntl</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Kellermann</surname> <given-names>T</given-names></name>, <etal>et al</etal>. <article-title>The differential contribution of facial expressions, prosody, and speech content to empathy</article-title>. <source>Cognition &amp; Emotion</source>. <year>2012</year>;<volume>26</volume>(<issue>6</issue>):<fpage>995</fpage>–<lpage>1014</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/02699931.2011.631296" xlink:type="simple">10.1080/02699931.2011.631296</ext-link></comment> <object-id pub-id-type="pmid">22214265</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sestito</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Umiltà</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>De Paola</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Fortunati</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Raballo</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Leuci</surname> <given-names>E</given-names></name>, <etal>et al</etal>. <article-title>Facial reactions in response to dynamic emotional stimuli in different modalities in patients suffering from schizophrenia: a behavioral and EMG study</article-title>. <source>Frontiers in Human Neuroscience</source>. <year>2013</year>;<volume>7</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnhum.2013.00368" xlink:type="simple">10.3389/fnhum.2013.00368</ext-link></comment> <object-id pub-id-type="pmid">23888132</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Delle-Vigne</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kornreich</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Verbanck</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Campanella</surname> <given-names>S</given-names></name>. <article-title>Subclinical alexithymia modulates early audio-visual perceptive and attentional event-related potentials</article-title>. <source>Frontiers in Human Neuroscience</source>. <year>2014</year>;<volume>8</volume>(<issue>106</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnhum.2014.00106" xlink:type="simple">10.3389/fnhum.2014.00106</ext-link></comment> <object-id pub-id-type="pmid">24624070</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zvyagintsev</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Parisi</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Chechko</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Nikolaev</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Mathiak</surname> <given-names>K</given-names></name>. <article-title>Attention and multisensory integration of emotions in schizophrenia</article-title>. <source>Frontiers in Human Neuroscience</source>. <year>2013</year>;<volume>7</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnhum.2013.00674" xlink:type="simple">10.3389/fnhum.2013.00674</ext-link></comment> <object-id pub-id-type="pmid">24151459</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref021"><label>21</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Ekman</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Friesen</surname> <given-names>WV</given-names></name>. <chapter-title>Pictures of Facial Affect</chapter-title>. <publisher-loc>Palo Alto, CA</publisher-loc>.: <publisher-name>Consulting Psychologists Press</publisher-name>; <year>1976</year>.</mixed-citation></ref>
<ref id="pone.0196391.ref022"><label>22</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Matsumoto</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ekman</surname> <given-names>P</given-names></name>. <chapter-title>Japanese and Caucasian Facial Expressions of Emotion (JACFEE) [Slides]</chapter-title>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Intercultural and Emotion Research Laboratory, Department of Psychology, San Francisco State University</publisher-name>.; <year>1988</year>.</mixed-citation></ref>
<ref id="pone.0196391.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Erwin</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Gur</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Gur</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Skolnick</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Mawhinney-Hee</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Smailis</surname> <given-names>J</given-names></name>. <article-title>Facial emotion discrimination: I. Task construction and behavioral findings in normal subjects</article-title>. <source>Psychiatry Research</source>. <year>1992</year>;<volume>42</volume>(<issue>3</issue>):<fpage>231</fpage>–<lpage>40</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0165-1781(92)90115-J" xlink:type="simple">10.1016/0165-1781(92)90115-J</ext-link></comment> <object-id pub-id-type="pmid">1496055</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mazurski</surname> <given-names>EJ</given-names></name>, <name name-style="western"><surname>Bond</surname> <given-names>NW</given-names></name>. <article-title>A new series of slides depicting facial expressions of affect: a comparison with the pictures of facial affect series</article-title>. <source>Australian Journal of Psychology</source>. <year>1993</year>;<volume>45</volume>(<issue>1</issue>):<fpage>41</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/00049539308259117" xlink:type="simple">10.1080/00049539308259117</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref025"><label>25</label><mixed-citation publication-type="other" xlink:type="simple">Lundqvist D, Flykt A, Öhman A. The Karolinska directed emotional faces [Database of standardized facial images]: (Available from Psychology section, Department of Clinical Neuroscience, Karolinska Hospital, S-171 76 Stockholm, Sweden); 1998.</mixed-citation></ref>
<ref id="pone.0196391.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Markham</surname> <given-names>R</given-names></name>. <article-title>The development of a series of photographs of Chinese facial expressions of emotion</article-title>. <source>Journal of Cross-Cultural Psychology</source>. <year>1999</year>;<volume>30</volume>(<issue>4</issue>):<fpage>397</fpage>–<lpage>410</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0022022199030004001" xlink:type="simple">10.1177/0022022199030004001</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref027"><label>27</label><mixed-citation publication-type="other" xlink:type="simple">Kanade T, Cohn JF, Tian Y, editors. Comprehensive database for facial expression analysis. Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat No PR00580); 2000; Los Alamitos, CA: IEEE Computer Society Conference Publishing Services.</mixed-citation></ref>
<ref id="pone.0196391.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beaupré</surname> <given-names>MG</given-names></name>, <name name-style="western"><surname>Hess</surname> <given-names>U</given-names></name>. <article-title>Cross-cultural emotion recognition among Canadian ethnic groups</article-title>. <source>Journal of Cross-Cultural Psychology</source>. <year>2005</year>;<volume>36</volume>(<issue>3</issue>):<fpage>355</fpage>–<lpage>70</lpage>.</mixed-citation></ref>
<ref id="pone.0196391.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tottenham</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Tanaka</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Leon</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>McCarry</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Nurse</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hare</surname> <given-names>TA</given-names></name>, <etal>et al</etal>. <article-title>The NimStim set of facial expressions: judgments from untrained research participants</article-title>. <source>Psychiatry Research</source>. <year>2009</year>;<volume>168</volume>(<issue>3</issue>):<fpage>242</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.psychres.2008.05.006" xlink:type="simple">10.1016/j.psychres.2008.05.006</ext-link></comment> <object-id pub-id-type="pmid">19564050</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tracy</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Robins</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>Schriber</surname> <given-names>RA</given-names></name>. <article-title>Development of a FACS-verified set of basic and self-conscious emotion expressions</article-title>. <source>Emotion</source>. <year>2009</year>;<volume>9</volume>(<issue>4</issue>):<fpage>554</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0015766" xlink:type="simple">10.1037/a0015766</ext-link></comment> <object-id pub-id-type="pmid">19653779</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Langner</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Dotsch</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bijlstra</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Wigboldus</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Hawk</surname> <given-names>ST</given-names></name>, <name name-style="western"><surname>van Knippenberg</surname> <given-names>A</given-names></name>. <article-title>Presentation and validation of the Radboud Faces Database</article-title>. <source>Cognition and Emotion</source>. <year>2010</year>;<volume>24</volume>(<issue>8</issue>):<fpage>1377</fpage>–<lpage>88</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/02699930903485076" xlink:type="simple">10.1080/02699930903485076</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ebner</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Riediger</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lindenberger</surname> <given-names>U</given-names></name>. <article-title>FACES—A database of facial expressions in young, middle-aged, and older women and men: Development and validation</article-title>. <source>Behavior Research Methods</source>. <year>2010</year>;<volume>42</volume>(<issue>1</issue>):<fpage>351</fpage>–<lpage>62</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BRM.42.1.351" xlink:type="simple">10.3758/BRM.42.1.351</ext-link></comment> <object-id pub-id-type="pmid">20160315</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Egger</surname> <given-names>HL</given-names></name>, <name name-style="western"><surname>Pine</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Leibenluft</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Ernst</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Towbin</surname> <given-names>KE</given-names></name>, <etal>et al</etal>. <article-title>The NIMH Child Emotional Faces Picture Set (NIMH‐ChEFS): A new set of children's facial emotion stimuli</article-title>. <source>International Journal of Methods in Psychiatric Research</source>. <year>2011</year>;<volume>20</volume>(<issue>3</issue>):<fpage>145</fpage>–<lpage>56</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/mpr.343" xlink:type="simple">10.1002/mpr.343</ext-link></comment> <object-id pub-id-type="pmid">22547297</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bassili</surname> <given-names>JN</given-names></name>. <article-title>Emotion recognition: The role of facial movement and the relative importance of upper and lower areas of the face</article-title>. <source>Journal of Personality and Social Psychology</source>. <year>1979</year>;<volume>37</volume>:<fpage>2049</fpage>–<lpage>58</lpage>. <object-id pub-id-type="pmid">521902</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cunningham</surname> <given-names>DW</given-names></name>, <name name-style="western"><surname>Wallraven</surname> <given-names>C</given-names></name>. <article-title>Dynamic information for the recognition of conversational expressions</article-title>. <source>Journal of Vision</source>. <year>2009</year>;<volume>9</volume>(<issue>13</issue>)(7):<fpage>1</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/9.13.7" xlink:type="simple">10.1167/9.13.7</ext-link></comment> <object-id pub-id-type="pmid">20055540</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ambadar</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Schooler</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Cohn</surname> <given-names>JF</given-names></name>. <article-title>Deciphering the enigmatic face the importance of facial dynamics in interpreting subtle facial expressions</article-title>. <source>Psychological Science</source>. <year>2005</year>;<volume>16</volume>(<issue>5</issue>):<fpage>403</fpage>–<lpage>10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.0956-7976.2005.01548.x" xlink:type="simple">10.1111/j.0956-7976.2005.01548.x</ext-link></comment> <object-id pub-id-type="pmid">15869701</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wehrle</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kaiser</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Schmidt</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>. <article-title>Studying the dynamics of emotional expression using synthesized facial muscle movements</article-title>. <source>Journal of Personality and Social Psychology</source>. <year>2000</year>;<volume>78</volume>(<issue>1</issue>):<fpage>105</fpage>–<lpage>19</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037//0022-3514.78.1.105" xlink:type="simple">10.1037//0022-3514.78.1.105</ext-link></comment> <object-id pub-id-type="pmid">10653509</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Biele</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Grabowska</surname> <given-names>A</given-names></name>. <article-title>Sex differences in perception of emotion intensity in dynamic and static facial expressions</article-title>. <source>Experimental Brain Research</source>. <year>2006</year>;<volume>171</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00221-005-0254-0" xlink:type="simple">10.1007/s00221-005-0254-0</ext-link></comment> <object-id pub-id-type="pmid">16628369</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bould</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Morris</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Wink</surname> <given-names>B</given-names></name>. <article-title>Recognising subtle emotional expressions: The role of facial movements</article-title>. <source>Cognition and Emotion</source>. <year>2008</year>;<volume>22</volume>(<issue>8</issue>):<fpage>1569</fpage>–<lpage>87</lpage>.</mixed-citation></ref>
<ref id="pone.0196391.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Krumhuber</surname> <given-names>EG</given-names></name>, <name name-style="western"><surname>Kappas</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Manstead</surname> <given-names>ASR</given-names></name>. <article-title>Effects of dynamic aspects of facial expressions: A review</article-title>. <source>Emotion Review</source>. <year>2013</year>;<volume>5</volume>(<issue>1</issue>):<fpage>41</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/1754073912451349" xlink:type="simple">10.1177/1754073912451349</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Krumhuber</surname> <given-names>EG</given-names></name>, <name name-style="western"><surname>Kappas</surname> <given-names>A</given-names></name>. <article-title>Moving smiles: The role of dynamic components for the perception of the genuineness of smiles</article-title>. <source>Journal of Nonverbal Behavior</source>. <year>2005</year>;<volume>29</volume>(<issue>1</issue>):<fpage>3</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s10919-004-0887-x" xlink:type="simple">10.1007/s10919-004-0887-x</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pollick</surname> <given-names>FE</given-names></name>, <name name-style="western"><surname>Hill</surname> <given-names>HC</given-names></name>, <name name-style="western"><surname>Calder</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Paterson</surname> <given-names>H</given-names></name>. <article-title>Recognising facial expression from spatially and temporally modified movements</article-title>. <source>Perception</source>. <year>2003</year>;<volume>32</volume>(<issue>7</issue>):<fpage>813</fpage>–<lpage>26</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1068/p3319" xlink:type="simple">10.1068/p3319</ext-link></comment> <object-id pub-id-type="pmid">12974567</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sato</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Yoshikawa</surname> <given-names>S</given-names></name>. <article-title>The dynamic aspects of emotional facial expressions</article-title>. <source>Cognition and Emotion</source>. <year>2004</year>;<volume>18</volume>(<issue>5</issue>):<fpage>701</fpage>–<lpage>10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/02699930341000176" xlink:type="simple">10.1080/02699930341000176</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nelson</surname> <given-names>NL</given-names></name>, <name name-style="western"><surname>Russell</surname> <given-names>JA</given-names></name>. <article-title>Dynamic facial expressions allow differentiation of displays intended to convey positive and hubristic pride</article-title>. <source>Emotion</source>. <year>2014</year>;<volume>14</volume>(<issue>5</issue>):<fpage>857</fpage>–<lpage>64</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0036789" xlink:type="simple">10.1037/a0036789</ext-link></comment> <object-id pub-id-type="pmid">24866524</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pitcher</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Dilks</surname> <given-names>DD</given-names></name>, <name name-style="western"><surname>Saxe</surname> <given-names>RR</given-names></name>, <name name-style="western"><surname>Triantafyllou</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name>. <article-title>Differential selectivity for dynamic versus static information in face-selective cortical regions</article-title>. <source>NeuroImage</source>. <year>2011</year>;<volume>56</volume>(<issue>4</issue>):<fpage>2356</fpage>–<lpage>63</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2011.03.067" xlink:type="simple">10.1016/j.neuroimage.2011.03.067</ext-link></comment> <object-id pub-id-type="pmid">21473921</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Trautmann</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Fehr</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Herrmann</surname> <given-names>M</given-names></name>. <article-title>Emotions in motion: dynamic compared to static facial expressions of disgust and happiness reveal more widespread emotion-specific activations</article-title>. <source>Brain Research</source>. <year>2009</year>;<volume>1284</volume>:<fpage>100</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.brainres.2009.05.075" xlink:type="simple">10.1016/j.brainres.2009.05.075</ext-link></comment> <object-id pub-id-type="pmid">19501062</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Recio</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Sommer</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Schacht</surname> <given-names>A</given-names></name>. <article-title>Electrophysiological correlates of perceiving and evaluating static and dynamic facial emotional expressions</article-title>. <source>Brain Research</source>. <year>2011</year>;<volume>1376</volume>:<fpage>66</fpage>–<lpage>75</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.brainres.2010.12.041" xlink:type="simple">10.1016/j.brainres.2010.12.041</ext-link></comment> <object-id pub-id-type="pmid">21172314</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sato</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Yoshikawa</surname> <given-names>S</given-names></name>. <article-title>Spontaneous facial mimicry in response to dynamic facial expressions</article-title>. <source>Cognition</source>. <year>2007</year>;<volume>104</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>18</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cognition.2006.05.001" xlink:type="simple">10.1016/j.cognition.2006.05.001</ext-link></comment> <object-id pub-id-type="pmid">16780824</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Weyers</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Mühlberger</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hefele</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Pauli</surname> <given-names>P</given-names></name>. <article-title>Electromyographic responses to static and dynamic avatar emotional facial expressions</article-title>. <source>Psychophysiology</source>. <year>2006</year>;<volume>43</volume>(<issue>5</issue>):<fpage>450</fpage>–<lpage>3</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1469-8986.2006.00451.x" xlink:type="simple">10.1111/j.1469-8986.2006.00451.x</ext-link></comment> <object-id pub-id-type="pmid">16965606</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Belin</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Fillion-Bilodeau</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gosselin</surname> <given-names>F</given-names></name>. <article-title>The Montreal Affective Voices: a validated set of nonverbal affect bursts for research on auditory affective processing</article-title>. <source>Behavior Research Methods</source>. <year>2008</year>;<volume>40</volume>(<issue>2</issue>):<fpage>531</fpage>–<lpage>9</lpage>. <object-id pub-id-type="pmid">18522064</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Busso</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Parthasarathy</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Burmania</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>AbdelWahab</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sadoughi</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Provost</surname> <given-names>EM</given-names></name>. <article-title>MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception</article-title>. <source>IEEE Transactions on Affective Computing</source>. <year>2017</year>;<volume>8</volume>(<issue>1</issue>):<fpage>67</fpage>–<lpage>80</lpage>.</mixed-citation></ref>
<ref id="pone.0196391.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cao</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Cooper</surname> <given-names>DG</given-names></name>, <name name-style="western"><surname>Keutmann</surname> <given-names>MK</given-names></name>, <name name-style="western"><surname>Gur</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Nenkova</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Verma</surname> <given-names>R</given-names></name>. <article-title>CREMA-D: Crowd-sourced emotional multimodal actors dataset</article-title>. <source>IEEE Transactions on Affective Computing</source>. <year>2014</year>;<volume>5</volume>(<issue>4</issue>):<fpage>377</fpage>–<lpage>90</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TAFFC.2014.2336244" xlink:type="simple">10.1109/TAFFC.2014.2336244</ext-link></comment> <object-id pub-id-type="pmid">25653738</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bänziger</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Grandjean</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>. <article-title>Emotion recognition from expressions in face, voice, and body: the Multimodal Emotion Recognition Test (MERT)</article-title>. <source>Emotion</source>. <year>2009</year>;<volume>9</volume>(<issue>5</issue>):<fpage>691</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0017088" xlink:type="simple">10.1037/a0017088</ext-link></comment> <object-id pub-id-type="pmid">19803591</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Breiter</surname> <given-names>HC</given-names></name>, <name name-style="western"><surname>Etcoff</surname> <given-names>NL</given-names></name>, <name name-style="western"><surname>Whalen</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Kennedy</surname> <given-names>WA</given-names></name>, <name name-style="western"><surname>Rauch</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Buckner</surname> <given-names>RL</given-names></name>, <etal>et al</etal>. <article-title>Response and habituation of the human amygdala during visual processing of facial expression</article-title>. <source>Neuron</source>. <year>1996</year>;<volume>17</volume>(<issue>5</issue>):<fpage>875</fpage>–<lpage>87</lpage>. <object-id pub-id-type="pmid">8938120</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thomas</surname> <given-names>KM</given-names></name>, <name name-style="western"><surname>Drevets</surname> <given-names>WC</given-names></name>, <name name-style="western"><surname>Whalen</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Eccard</surname> <given-names>CH</given-names></name>, <name name-style="western"><surname>Dahl</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Ryan</surname> <given-names>ND</given-names></name>, <etal>et al</etal>. <article-title>Amygdala response to facial expressions in children and adults</article-title>. <source>Biological Psychiatry</source>. <year>2001</year>;<volume>49</volume>(<issue>4</issue>):<fpage>309</fpage>–<lpage>16</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/s0006-3223(00)01066-0" xlink:type="simple">10.1016/s0006-3223(00)01066-0</ext-link></comment> <object-id pub-id-type="pmid">11239901</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref056"><label>56</label><mixed-citation publication-type="other" xlink:type="simple">Zhang B, Provost EM, Essi G, editors. Cross-corpus acoustic emotion recognition from singing and speaking: A multi-task learning approach. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP); 2016; Shanghai, China: IEEE.</mixed-citation></ref>
<ref id="pone.0196391.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bänziger</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Mortillaro</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>. <article-title>Introducing the Geneva Multimodal expression corpus for experimental research on emotion perception</article-title>. <source>Emotion</source>. <year>2012</year>;<volume>12</volume>:<fpage>1161</fpage>–<lpage>79</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0025827" xlink:type="simple">10.1037/a0025827</ext-link></comment> <object-id pub-id-type="pmid">22081890</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kaulard</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Cunningham</surname> <given-names>DW</given-names></name>, <name name-style="western"><surname>Bülthoff</surname> <given-names>HH</given-names></name>, <name name-style="western"><surname>Wallraven</surname> <given-names>C</given-names></name>. <article-title>The MPI Facial Expression Database—A validated database of emotional and conversational facial expressions</article-title>. <source>PloS one</source>. <year>2012</year>;<volume>7</volume>(<issue>3</issue>):<fpage>e32321</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0032321" xlink:type="simple">10.1371/journal.pone.0032321</ext-link></comment> <object-id pub-id-type="pmid">22438875</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sonnemans</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Frijda</surname> <given-names>NH</given-names></name>. <article-title>The structure of subjective emotional intensity</article-title>. <source>Cognition &amp; Emotion</source>. <year>1994</year>;<volume>8</volume>(<issue>4</issue>):<fpage>329</fpage>–<lpage>50</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/02699939408408945" xlink:type="simple">10.1080/02699939408408945</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Diener</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Larsen</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Levine</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Emmons</surname> <given-names>RA</given-names></name>. <article-title>Intensity and frequency: Dimensions underlying positive and negative affect</article-title>. <source>Journal of Personality and Social Psychology</source>. <year>1985</year>;<volume>48</volume>(<issue>5</issue>):<fpage>1253</fpage>–<lpage>65</lpage>. <object-id pub-id-type="pmid">3998989</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frijda</surname> <given-names>NH</given-names></name>. <article-title>The laws of emotion</article-title>. <source>American Psychologist</source>. <year>1988</year>;<volume>43</volume>(<issue>5</issue>):<fpage>349</fpage>–<lpage>58</lpage>. 0.1037/0003-066X.43.5.349. <object-id pub-id-type="pmid">3389582</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref062"><label>62</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Plutchik</surname> <given-names>R</given-names></name>. <chapter-title>Emotion, a psychoevolutionary synthesis</chapter-title>: <publisher-name>Harper &amp; Row</publisher-name> <publisher-loc>New York</publisher-loc>; <year>1980</year>.</mixed-citation></ref>
<ref id="pone.0196391.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reisenzein</surname> <given-names>R</given-names></name>. <article-title>Pleasure-arousal theory and the intensity of emotions</article-title>. <source>Journal of Personality and Social Psychology</source>. <year>1994</year>;<volume>67</volume>(<issue>3</issue>):<fpage>525</fpage>–<lpage>39</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037//0022-3514.67.3.525" xlink:type="simple">10.1037//0022-3514.67.3.525</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schlosberg</surname> <given-names>H</given-names></name>. <article-title>Three dimensions of emotion</article-title>. <source>Psychological Review</source>. <year>1954</year>;<volume>61</volume>(<issue>2</issue>):<fpage>81</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/h0054570" xlink:type="simple">10.1037/h0054570</ext-link></comment> <object-id pub-id-type="pmid">13155714</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hess</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Blairy</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kleck</surname> <given-names>RE</given-names></name>. <article-title>The intensity of emotional facial expressions and decoding accuracy</article-title>. <source>Journal of Nonverbal Behavior</source>. <year>1997</year>;<volume>21</volume>(<issue>4</issue>):<fpage>241</fpage>–<lpage>57</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1023/A:1024952730333" xlink:type="simple">10.1023/A:1024952730333</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Juslin</surname> <given-names>PN</given-names></name>, <name name-style="western"><surname>Laukka</surname> <given-names>P</given-names></name>. <article-title>Impact of intended emotion intensity on cue utilization and decoding accuracy in vocal expression of emotion</article-title>. <source>Emotion</source>. <year>2001</year>;<volume>1</volume>:<fpage>381</fpage>–<lpage>412</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/1528-3542.1.4.381" xlink:type="simple">10.1037/1528-3542.1.4.381</ext-link></comment> <object-id pub-id-type="pmid">12901399</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Palermo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Coltheart</surname> <given-names>M</given-names></name>. <article-title>Photographs of facial expression: Accuracy, response times, and ratings of intensity</article-title>. <source>Behavior Research Methods, Instruments, &amp; Computers</source>. <year>2004</year>;<volume>36</volume>(<issue>4</issue>):<fpage>634</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/bf03206544" xlink:type="simple">10.3758/bf03206544</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cacioppo</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Petty</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Losch</surname> <given-names>ME</given-names></name>, <name name-style="western"><surname>Kim</surname> <given-names>HS</given-names></name>. <article-title>Electromyographic activity over facial muscle regions can differentiate the valence and intensity of affective reactions</article-title>. <source>Journal of Personality and Social Psychology</source>. <year>1986</year>;<volume>50</volume>(<issue>2</issue>):<fpage>260</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0022-3514.50.2.260" xlink:type="simple">10.1037/0022-3514.50.2.260</ext-link></comment> <object-id pub-id-type="pmid">3701577</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hess</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Blairy</surname> <given-names>S</given-names></name>. <article-title>Facial mimicry and emotional contagion to dynamic emotional facial expressions and their influence on decoding accuracy</article-title>. <source>International Journal of Psychophysiology</source>. <year>2001</year>;<volume>40</volume>(<issue>2</issue>):<fpage>129</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0167-8760(00)00161-6" xlink:type="simple">10.1016/S0167-8760(00)00161-6</ext-link></comment> <object-id pub-id-type="pmid">11165351</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Banse</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Wallbott</surname> <given-names>HG</given-names></name>, <name name-style="western"><surname>Goldbeck</surname> <given-names>T</given-names></name>. <article-title>Vocal cues in emotion encoding and decoding</article-title>. <source>Motivation and Emotion</source>. <year>1991</year>;<volume>15</volume>(<issue>2</issue>):<fpage>123</fpage>–<lpage>48</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/bf00995674" xlink:type="simple">10.1007/bf00995674</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Donegan</surname> <given-names>NH</given-names></name>, <name name-style="western"><surname>Sanislow</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Blumberg</surname> <given-names>HP</given-names></name>, <name name-style="western"><surname>Fulbright</surname> <given-names>RK</given-names></name>, <name name-style="western"><surname>Lacadie</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Skudlarski</surname> <given-names>P</given-names></name>, <etal>et al</etal>. <article-title>Amygdala hyperreactivity in borderline personality disorder: implications for emotional dysregulation</article-title>. <source>Biological Psychiatry</source>. <year>2003</year>;<volume>54</volume>(<issue>11</issue>):<fpage>1284</fpage>–<lpage>93</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0006-3223(03)00636-X" xlink:type="simple">10.1016/S0006-3223(03)00636-X</ext-link></comment> <object-id pub-id-type="pmid">14643096</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref072"><label>72</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Bänziger</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>. <chapter-title>Using actor portrayals to systematically study multimodal emotion expression: The GEMEP corpus</chapter-title>. <source>Affective computing and intelligent interaction</source>: <collab>Springer</collab>; <year>2007</year>. p. <fpage>476</fpage>–<lpage>87</lpage>.</mixed-citation></ref>
<ref id="pone.0196391.ref073"><label>73</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marsh</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Elfenbein</surname> <given-names>HA</given-names></name>, <name name-style="western"><surname>Ambady</surname> <given-names>N</given-names></name>. <article-title>Nonverbal “accents” cultural differences in facial expressions of emotion</article-title>. <source>Psychological Science</source>. <year>2003</year>;<volume>14</volume>(<issue>4</issue>):<fpage>373</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/1467-9280.24461" xlink:type="simple">10.1111/1467-9280.24461</ext-link></comment> <object-id pub-id-type="pmid">12807413</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref074"><label>74</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marsh</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Elfenbein</surname> <given-names>HA</given-names></name>, <name name-style="western"><surname>Ambady</surname> <given-names>N</given-names></name>. <article-title>Separated by a Common Language Nonverbal Accents and Cultural Stereotypes About Americans and Australians</article-title>. <source>Journal of Cross-Cultural Psychology</source>. <year>2007</year>;<volume>38</volume>(<issue>3</issue>):<fpage>284</fpage>–<lpage>301</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0022022107300275" xlink:type="simple">10.1177/0022022107300275</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref075"><label>75</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Elfenbein</surname> <given-names>HA</given-names></name>, <name name-style="western"><surname>Ambady</surname> <given-names>N</given-names></name>. <article-title>On the universality and cultural specificity of emotion recognition: A meta-analysis</article-title>. <source>Psychological Bulletin</source>. <year>2002</year>;<volume>128</volume>(<issue>2</issue>):<fpage>203</fpage>–<lpage>35</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0033-2909.128.2.203" xlink:type="simple">10.1037/0033-2909.128.2.203</ext-link></comment> <object-id pub-id-type="pmid">11931516</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref076"><label>76</label><mixed-citation publication-type="other" xlink:type="simple">Martin O, Kotsia I, Macq B, Pitas I, editors. The enterface’05 audio-visual emotion database. Proceedings of the 22nd International Conference on Data Engineering Workshops (ICDEW'06); 2006: IEEE.</mixed-citation></ref>
<ref id="pone.0196391.ref077"><label>77</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Särkämö</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Tervaniemi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Laitinen</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Forsblom</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Soinila</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Mikkonen</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Music listening enhances cognitive recovery and mood after middle cerebral artery stroke</article-title>. <source>Brain</source>. <year>2008</year>;<volume>131</volume>(<issue>3</issue>):<fpage>866</fpage>–<lpage>76</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/brain/awn013" xlink:type="simple">10.1093/brain/awn013</ext-link></comment> <object-id pub-id-type="pmid">18287122</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref078"><label>78</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cuddy</surname> <given-names>LL</given-names></name>, <name name-style="western"><surname>Duffin</surname> <given-names>J</given-names></name>. <article-title>Music, memory, and Alzheimer’s disease: is music recognition spared in dementia, and how can it be assessed?</article-title> <source>Medical hypotheses</source>. <year>2005</year>;<volume>64</volume>(<issue>2</issue>):<fpage>229</fpage>–<lpage>35</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.mehy.2004.09.005" xlink:type="simple">10.1016/j.mehy.2004.09.005</ext-link></comment> <object-id pub-id-type="pmid">15607545</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref079"><label>79</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hébert</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Racette</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Gagnon</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Peretz</surname> <given-names>I</given-names></name>. <article-title>Revisiting the dissociation between singing and speaking in expressive aphasia</article-title>. <source>Brain</source>. <year>2003</year>;<volume>126</volume>(<issue>8</issue>):<fpage>1838</fpage>–<lpage>50</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/brain/awg186" xlink:type="simple">10.1093/brain/awg186</ext-link></comment> <object-id pub-id-type="pmid">12821526</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref080"><label>80</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koelsch</surname> <given-names>S</given-names></name>. <article-title>Brain correlates of music-evoked emotions</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2014</year>;<volume>15</volume>(<issue>3</issue>):<fpage>170</fpage>–<lpage>80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn3666" xlink:type="simple">10.1038/nrn3666</ext-link></comment> <object-id pub-id-type="pmid">24552785</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref081"><label>81</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Caria</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Venuti</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>de Falco</surname> <given-names>S</given-names></name>. <article-title>Functional and dysfunctional brain circuits underlying emotional processing of music in autism spectrum disorders</article-title>. <source>Cerebral Cortex</source>. <year>2011</year>;<volume>21</volume>(<issue>12</issue>):<fpage>2838</fpage>–<lpage>49</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhr084" xlink:type="simple">10.1093/cercor/bhr084</ext-link></comment> <object-id pub-id-type="pmid">21527791</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref082"><label>82</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hsieh</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hornberger</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Piguet</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Hodges</surname> <given-names>JR</given-names></name>. <article-title>Neural basis of music knowledge: evidence from the dementias</article-title>. <source>Brain</source>. <year>2011</year>;<volume>134</volume>(<issue>9</issue>):<fpage>2523</fpage>–<lpage>34</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/brain/awr190" xlink:type="simple">10.1093/brain/awr190</ext-link></comment> <object-id pub-id-type="pmid">21857031</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref083"><label>83</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Punkanen</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Eerola</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Erkkilä</surname> <given-names>J</given-names></name>. <article-title>Biased emotional recognition in depression: Perception of emotions in music by depressed patients</article-title>. <source>Journal of Affective Disorders</source>. <year>2011</year>;<volume>130</volume>(<issue>1–2</issue>):<fpage>118</fpage>–<lpage>26</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jad.2010.10.034" xlink:type="simple">10.1016/j.jad.2010.10.034</ext-link></comment> <object-id pub-id-type="pmid">21071094</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref084"><label>84</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wan</surname> <given-names>CY</given-names></name>, <name name-style="western"><surname>Rüber</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Hohmann</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schlaug</surname> <given-names>G</given-names></name>. <article-title>The therapeutic effects of singing in neurological disorders. Music Perception</article-title>: <source>An Interdisciplinary Journal</source>. <year>2010</year>;<volume>27</volume>(<issue>4</issue>):<fpage>287</fpage>–<lpage>95</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/9781119155195.ch16" xlink:type="simple">10.1002/9781119155195.ch16</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref085"><label>85</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Aalbers</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Fusar-Poli</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Freeman</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Spreen</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Ket</surname> <given-names>JCF</given-names></name>, <name name-style="western"><surname>Vink</surname> <given-names>AC</given-names></name>, <etal>et al</etal>. <article-title>Music therapy for depression</article-title>. <source>Cochrane Database of Systematic Reviews</source>. <year>2017</year>;(<issue>11</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/14651858.CD004517.pub3" xlink:type="simple">10.1002/14651858.CD004517.pub3</ext-link></comment> <object-id pub-id-type="pmid">29144545</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref086"><label>86</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bidabadi</surname> <given-names>SS</given-names></name>, <name name-style="western"><surname>Mehryar</surname> <given-names>A</given-names></name>. <article-title>Music therapy as an adjunct to standard treatment for obsessive compulsive disorder and co-morbid anxiety and depression: A randomized clinical trial</article-title>. <source>Journal of Affective Disorders</source>. <year>2015</year>;<volume>184</volume>:<fpage>13</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jad.2015.04.011" xlink:type="simple">10.1016/j.jad.2015.04.011</ext-link></comment> <object-id pub-id-type="pmid">26066780</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref087"><label>87</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Good</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Gordon</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Papsin</surname> <given-names>BC</given-names></name>, <name name-style="western"><surname>Nespoli</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Hopyan</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Peretz</surname> <given-names>I</given-names></name>, <etal>et al</etal>. <article-title>Benefits of music training for perception of emotional speech prosody in deaf children with cochlear implants</article-title>. <source>Ear and Hearing</source>. <year>2017</year>;<volume>38</volume>(<issue>4</issue>):<fpage>455</fpage>–<lpage>64</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1097/AUD.0000000000000402" xlink:type="simple">10.1097/AUD.0000000000000402</ext-link></comment> <object-id pub-id-type="pmid">28085739</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref088"><label>88</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schlaug</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Marchina</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Norton</surname> <given-names>A</given-names></name>. <article-title>From singing to speaking: why singing may lead to recovery of expressive language function in patients with Broca's aphasia</article-title>. <source>Music Perception: An Interdisciplinary Journal</source>. <year>2008</year>;<volume>25</volume>(<issue>4</issue>):<fpage>315</fpage>–<lpage>23</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1525/mp.2008.25.4.315" xlink:type="simple">10.1525/mp.2008.25.4.315</ext-link></comment> <object-id pub-id-type="pmid">21197418</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref089"><label>89</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thaut</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>McIntosh</surname> <given-names>GC</given-names></name>, <name name-style="western"><surname>Rice</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Rathbun</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Brault</surname> <given-names>J</given-names></name>. <article-title>Rhythmic auditory stimulation in gait training for Parkinson's disease patients</article-title>. <source>Movement Disorders</source>. <year>1996</year>;<volume>11</volume>(<issue>2</issue>):<fpage>193</fpage>–<lpage>200</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/mds.870110213" xlink:type="simple">10.1002/mds.870110213</ext-link></comment> <object-id pub-id-type="pmid">8684391</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref090"><label>90</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pacchetti</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Mancini</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Aglieri</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Fundarò</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Martignoni</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Nappi</surname> <given-names>G</given-names></name>. <article-title>Active music therapy in Parkinson’s disease: an integrative method for motor and emotional rehabilitation</article-title>. <source>Psychosomatic Medicine</source>. <year>2000</year>;<volume>62</volume>(<issue>3</issue>):<fpage>386</fpage>–<lpage>93</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1097/00006842-200005000-00012" xlink:type="simple">10.1097/00006842-200005000-00012</ext-link></comment> <object-id pub-id-type="pmid">10845352</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref091"><label>91</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ayotte</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Peretz</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Hyde</surname> <given-names>K</given-names></name>. <article-title>Congenital amusia A group study of adults afflicted with a music‐specific disorder</article-title>. <source>Brain</source>. <year>2002</year>;<volume>125</volume>(<issue>2</issue>):<fpage>238</fpage>–<lpage>51</lpage>.</mixed-citation></ref>
<ref id="pone.0196391.ref092"><label>92</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schlaug</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Marchina</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Norton</surname> <given-names>A</given-names></name>. <article-title>Evidence for Plasticity in White‐Matter Tracts of Patients with Chronic Broca's Aphasia Undergoing Intense Intonation‐based Speech Therapy</article-title>. <source>Annals of the New York Academy of Sciences</source>. <year>2009</year>;<volume>1169</volume>(<issue>1</issue>):<fpage>385</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1749-6632.2009.04587.x." xlink:type="simple">10.1111/j.1749-6632.2009.04587.x.</ext-link></comment> <object-id pub-id-type="pmid">19673813</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref093"><label>93</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thompson</surname> <given-names>WF</given-names></name>, <name name-style="western"><surname>Marin</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Stewart</surname> <given-names>L</given-names></name>. <article-title>Reduced sensitivity to emotional prosody in congenital amusia rekindles the musical protolanguage hypothesis</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2012</year>;<volume>109</volume>(<issue>46</issue>):<fpage>19027</fpage>–<lpage>32</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1210344109" xlink:type="simple">10.1073/pnas.1210344109</ext-link></comment> <object-id pub-id-type="pmid">23112175</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref094"><label>94</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ilie</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Thompson</surname> <given-names>WF</given-names></name>. <article-title>A comparison of acoustic cues in music and speech for three dimensions of affect. Music Perception</article-title>: <source>An Interdisciplinary Journal</source>. <year>2006</year>;<volume>23</volume>(<issue>4</issue>):<fpage>319</fpage>–<lpage>30</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1525/mp.2006.23.4.319" xlink:type="simple">10.1525/mp.2006.23.4.319</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref095"><label>95</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Juslin</surname> <given-names>PN</given-names></name>, <name name-style="western"><surname>Laukka</surname> <given-names>P</given-names></name>. <article-title>Communication of emotions in vocal expression and music performance: Different channels, same code?</article-title> <source>Psychological Bulletin</source>. <year>2003</year>;<volume>129</volume>(<issue>5</issue>):<fpage>770</fpage>–<lpage>814</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0033-2909.129.5.770" xlink:type="simple">10.1037/0033-2909.129.5.770</ext-link></comment> <object-id pub-id-type="pmid">12956543</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref096"><label>96</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Ekman</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Friesen</surname> <given-names>WV</given-names></name>. <chapter-title>Manual of the Facial Action Coding System (FACS)</chapter-title>. <publisher-loc>Palo Alto, CA</publisher-loc>.: <publisher-name>Consulting Psychologists Press</publisher-name>; <year>1978</year>.</mixed-citation></ref>
<ref id="pone.0196391.ref097"><label>97</label><mixed-citation publication-type="other" xlink:type="simple">Friesen WV, Ekman P. EMFACS-7: Emotional facial action coding system. Unpublished manuscript, University of California at San Francisco. 1983;2:36.</mixed-citation></ref>
<ref id="pone.0196391.ref098"><label>98</label><mixed-citation publication-type="other" xlink:type="simple">Izard CE. The maximally discriminative facial movement coding system (MAX). 1979.</mixed-citation></ref>
<ref id="pone.0196391.ref099"><label>99</label><mixed-citation publication-type="other" xlink:type="simple">Burkhardt F, Paeschke A, Rolfes M, Sendlmeier WF, Weiss B, editors. A database of German emotional speech. Ninth European Conference on Speech Communication and Technology (INTERSPEECH 2005); 2005; Lisbon, Portugal.</mixed-citation></ref>
<ref id="pone.0196391.ref100"><label>100</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jürgens</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Grass</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Drolet</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fischer</surname> <given-names>J</given-names></name>. <article-title>Effect of acting experience on emotion expression and recognition in voice: Non-actors provide better stimuli than expected</article-title>. <source>Journal of Nonverbal Behavior</source>. <year>2015</year>;<volume>39</volume>(<issue>3</issue>):<fpage>195</fpage>–<lpage>214</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s10919-015-0209-5" xlink:type="simple">10.1007/s10919-015-0209-5</ext-link></comment> <object-id pub-id-type="pmid">26246649</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref101"><label>101</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Palmer</surname> <given-names>C</given-names></name>. <article-title>Music performance</article-title>. <source>Annual Review of Psychology</source>. <year>1997</year>;<volume>48</volume>(<issue>1</issue>):<fpage>115</fpage>–<lpage>38</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev.psych.48.1.115" xlink:type="simple">10.1146/annurev.psych.48.1.115</ext-link></comment> <object-id pub-id-type="pmid">9046557</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref102"><label>102</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Coltheart</surname> <given-names>M</given-names></name>. <article-title>The MRC psycholinguistic database</article-title>. <source>The Quarterly Journal of Experimental Psychology</source>. <year>1981</year>;<volume>33</volume>(<issue>4</issue>):<fpage>497</fpage>–<lpage>505</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/14640748108400805" xlink:type="simple">10.1080/14640748108400805</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref103"><label>103</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dalla Bella</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Peretz</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Rousseau</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Gosselin</surname> <given-names>N</given-names></name>. <article-title>A developmental study of the affective value of tempo and mode in music</article-title>. <source>Cognition</source>. <year>2001</year>;<volume>80</volume>(<issue>3</issue>):<fpage>B1</fpage>–<lpage>B10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0010-0277(00)00136-0" xlink:type="simple">10.1016/S0010-0277(00)00136-0</ext-link></comment> <object-id pub-id-type="pmid">11274986</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref104"><label>104</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Gabrielsson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lindström</surname> <given-names>E</given-names></name>. <chapter-title>The influence of musical structure on emotional expression</chapter-title>. In: <name name-style="western"><surname>Juslin</surname> <given-names>PN</given-names></name>, <name name-style="western"><surname>Sloboda</surname> <given-names>JA</given-names></name>, editors. <source>Music and Emotion: Theory and Research</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>2001</year>. p. <fpage>223</fpage>–<lpage>48</lpage>.</mixed-citation></ref>
<ref id="pone.0196391.ref105"><label>105</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bradley</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Lang</surname> <given-names>PJ</given-names></name>. <article-title>Measuring emotion: the self-assessment manikin and the semantic differential</article-title>. <source>Journal of Behavior Therapy and Experimental Psychiatry</source>. <year>1994</year>;<volume>25</volume>(<issue>1</issue>):<fpage>49</fpage>–<lpage>59</lpage>. <object-id pub-id-type="pmid">7962581</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref106"><label>106</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ekman</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Sorenson</surname> <given-names>ER</given-names></name>, <name name-style="western"><surname>Friesen</surname> <given-names>WV</given-names></name>. <article-title>Pan-cultural elements in facial displays of emotion</article-title>. <source>Science</source>. <year>1969</year>;<volume>164</volume>(<issue>3875</issue>):<fpage>86</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.164.3875.86" xlink:type="simple">10.1126/science.164.3875.86</ext-link></comment> <object-id pub-id-type="pmid">5773719</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref107"><label>107</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Descartes</surname> <given-names>R</given-names></name>. <chapter-title>The passions of the soul</chapter-title>. In: <name name-style="western"><surname>Cottingham</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Stoothoff</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Murdoch</surname> <given-names>D</given-names></name>, editors. <source>The philosophical works of Descartes</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name> (Original work published 1649); <year>1984</year>.</mixed-citation></ref>
<ref id="pone.0196391.ref108"><label>108</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Darwin</surname> <given-names>C</given-names></name>. <chapter-title>The expression of emotions in man and animals</chapter-title>. <name name-style="western"><surname>John</surname> <given-names>Murray</given-names></name> ed. <publisher-name>Chicago: University Chicago Press</publisher-name> (Original work published 1872); <year>1965</year>.</mixed-citation></ref>
<ref id="pone.0196391.ref109"><label>109</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>James</surname> <given-names>W</given-names></name>. <article-title>What is an emotion?</article-title> <source>Mind</source>. <year>1884</year>;<volume>9</volume>:<fpage>188</fpage>–<lpage>205</lpage>.</mixed-citation></ref>
<ref id="pone.0196391.ref110"><label>110</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Izard</surname> <given-names>CE</given-names></name>. <article-title>Basic emotions, relations among emotions, and emotion-cognition relations</article-title>. <source>Psychological Review</source>. <year>1992</year>;<volume>99</volume>(<issue>3</issue>):<fpage>561</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0033-295X.99.3.561" xlink:type="simple">10.1037/0033-295X.99.3.561</ext-link></comment> <object-id pub-id-type="pmid">1502277</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref111"><label>111</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Tomkins</surname> <given-names>SS</given-names></name>. <chapter-title>Affect, imagery, consciousness</chapter-title>: Vol. <volume>I</volume>. <source>The positive affects</source>. <publisher-loc>New York, NY, USA</publisher-loc>: <publisher-name>Springer Publishing Co.</publisher-name>; <year>1962</year>.</mixed-citation></ref>
<ref id="pone.0196391.ref112"><label>112</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ekman</surname> <given-names>P</given-names></name>. <article-title>An argument for basic emotions</article-title>. <source>Cognition and Emotion</source>. <year>1992</year>;<volume>6</volume>(<issue>3–4</issue>):<fpage>169</fpage>–<lpage>200</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/02699939208411068" xlink:type="simple">10.1080/02699939208411068</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref113"><label>113</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Russell</surname> <given-names>JA</given-names></name>. <article-title>Is there universal recognition of emotion from facial expressions? A review of the cross-cultural studies</article-title>. <source>Psychological Bulletin</source>. <year>1994</year>;<volume>115</volume>(<issue>1</issue>):<fpage>102</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/b978-0-12-267630-7.50015–0" xlink:type="simple">10.1016/b978-0-12-267630-7.50015–0</ext-link></comment> <object-id pub-id-type="pmid">8202574</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref114"><label>114</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ortony</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Turner</surname> <given-names>TJ</given-names></name>. <article-title>What's basic about basic emotions?</article-title> <source>Psychological Review</source>. <year>1990</year>;<volume>97</volume>(<issue>3</issue>):<fpage>315</fpage>–<lpage>31</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037//0033-295x.97.3.315" xlink:type="simple">10.1037//0033-295x.97.3.315</ext-link></comment> <object-id pub-id-type="pmid">1669960</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref115"><label>115</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barrett</surname> <given-names>LF</given-names></name>. <article-title>Are emotions natural kinds?</article-title> <source>Perspectives on Psychological Science</source>. <year>2006</year>;<volume>1</volume>(<issue>1</issue>):<fpage>28</fpage>–<lpage>58</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1745-6916.2006.00003.x" xlink:type="simple">10.1111/j.1745-6916.2006.00003.x</ext-link></comment> <object-id pub-id-type="pmid">26151184</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref116"><label>116</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Simon</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Craig</surname> <given-names>KD</given-names></name>, <name name-style="western"><surname>Gosselin</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Belin</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Rainville</surname> <given-names>P</given-names></name>. <article-title>Recognition and discrimination of prototypical dynamic expressions of pain and emotions</article-title>. <source>Pain</source>. <year>2008</year>;<volume>135</volume>(<issue>1</issue>):<fpage>55</fpage>–<lpage>64</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.pain.2007.05.008" xlink:type="simple">10.1016/j.pain.2007.05.008</ext-link></comment> <object-id pub-id-type="pmid">17583430</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref117"><label>117</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Castro</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Lima</surname> <given-names>CF</given-names></name>. <article-title>Recognizing emotions in spoken language: A validated set of Portuguese sentences and pseudosentences for research on emotional prosody</article-title>. <source>Behavior Research Methods</source>. <year>2010</year>;<volume>42</volume>(<issue>1</issue>):<fpage>74</fpage>–<lpage>81</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BRM.42.1.74" xlink:type="simple">10.3758/BRM.42.1.74</ext-link></comment> <object-id pub-id-type="pmid">20160287</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref118"><label>118</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Yin</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Cohn</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Canavan</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Reale</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Horowitz</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>BP4D-Spontaneous: a high-resolution spontaneous 3D dynamic facial expression database</article-title>. <source>Image and Vision Computing</source>. <year>2014</year>;<volume>32</volume>(<issue>10</issue>):<fpage>692</fpage>–<lpage>706</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.imavis.2014.06.002" xlink:type="simple">10.1016/j.imavis.2014.06.002</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref119"><label>119</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>LoBue</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Thrasher</surname> <given-names>C</given-names></name>. <article-title>The Child Affective Facial Expression (CAFE) set: validity and reliability from untrained adults</article-title>. <source>Frontiers in Psychology</source>. <year>2014</year>;<volume>5</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2014.01532" xlink:type="simple">10.3389/fpsyg.2014.01532</ext-link></comment> <object-id pub-id-type="pmid">25610415</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref120"><label>120</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Juslin</surname> <given-names>PN</given-names></name>. <chapter-title>Communicating emotion in music performance: A review and a theoretical framework</chapter-title>. In: <name name-style="western"><surname>Juslin</surname> <given-names>PN</given-names></name>, <name name-style="western"><surname>Sloboda</surname> <given-names>JA</given-names></name>, editors. <source>Music and Emotion: Theory and Research</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>2001</year>. p. <fpage>309</fpage>–<lpage>40</lpage>.</mixed-citation></ref>
<ref id="pone.0196391.ref121"><label>121</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Livingstone</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Muhlberger</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Thompson</surname> <given-names>WF</given-names></name>. <article-title>Changing musical emotion: A computational rule system for modifying score and performance</article-title>. <source>Computer Music Journal</source>. <year>2010</year>;<volume>34</volume>(<issue>1</issue>):<fpage>41</fpage>–<lpage>64</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/comj.2010.34.1.41" xlink:type="simple">10.1162/comj.2010.34.1.41</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref122"><label>122</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eerola</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Vuoskoski</surname> <given-names>JK</given-names></name>. <article-title>A comparison of the discrete and dimensional models of emotion in music</article-title>. <source>Psychology of Music</source>. <year>2010</year>;<volume>39</volume>(<issue>1</issue>):<fpage>18</fpage>–<lpage>49</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0305735610362821" xlink:type="simple">10.1177/0305735610362821</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref123"><label>123</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kallinen</surname> <given-names>K</given-names></name>. <article-title>Emotional ratings of music excerpts in the western art music repertoire and their self-organization in the Kohonen neural network</article-title>. <source>Psychology of Music</source>. <year>2005</year>;<volume>33</volume>(<issue>4</issue>):<fpage>373</fpage>–<lpage>93</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0305735605056147" xlink:type="simple">10.1177/0305735605056147</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref124"><label>124</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Juslin</surname> <given-names>PN</given-names></name>, <name name-style="western"><surname>Liljeström</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Västfjäll</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Barradas</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Silva</surname> <given-names>A</given-names></name>. <article-title>An experience sampling study of emotional reactions to music: listener, music, and situation</article-title>. <source>Emotion</source>. <year>2008</year>;<volume>8</volume>(<issue>5</issue>):<fpage>668</fpage>–<lpage>83</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0013505" xlink:type="simple">10.1037/a0013505</ext-link></comment> <object-id pub-id-type="pmid">18837617</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref125"><label>125</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gosselin</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Kirouac</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Doré</surname> <given-names>FY</given-names></name>. <article-title>Components and recognition of facial expression in the communication of emotion by actors</article-title>. <source>Journal of Personality and Social Psychology</source>. <year>1995</year>;<volume>68</volume>(<issue>1</issue>):<fpage>83</fpage>–<lpage>96</lpage>. <object-id pub-id-type="pmid">7861316</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref126"><label>126</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>. <article-title>Vocal communication of emotion: A review of research paradigms</article-title>. <source>Speech Communication</source>. <year>2003</year>;<volume>40</volume>(<issue>1–2</issue>):<fpage>227</fpage>–<lpage>56</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/s0167-6393(02)00084-5" xlink:type="simple">10.1016/s0167-6393(02)00084-5</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref127"><label>127</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Clark‐Polner</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Mortillaro</surname> <given-names>M</given-names></name>. <article-title>In the eye of the beholder? Universality and cultural specificity in the expression and perception of emotion</article-title>. <source>International Journal of Psychology</source>. <year>2011</year>;<volume>46</volume>(<issue>6</issue>):<fpage>401</fpage>–<lpage>35</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/00207594.2011.626049" xlink:type="simple">10.1080/00207594.2011.626049</ext-link></comment> <object-id pub-id-type="pmid">22126090</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref128"><label>128</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Stanislavsky</surname> <given-names>K</given-names></name>. <chapter-title>An actor prepares</chapter-title>. <publisher-loc>New York</publisher-loc>: <publisher-name>Theatre Arts Books</publisher-name>; <year>1936</year>.</mixed-citation></ref>
<ref id="pone.0196391.ref129"><label>129</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Katselas</surname> <given-names>M</given-names></name>. <chapter-title>Acting Class: Take a Seat</chapter-title>. <publisher-loc>Beverly Hills, CA</publisher-loc>: <publisher-name>Phoenix Books, Inc.</publisher-name>; <year>2008</year>.</mixed-citation></ref>
<ref id="pone.0196391.ref130"><label>130</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shaver</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kirson</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>O'connor</surname> <given-names>C</given-names></name>. <article-title>Emotion knowledge: further exploration of a prototype approach</article-title>. <source>Journal of Personality and Social Psychology</source>. <year>1987</year>;<volume>52</volume>(<issue>6</issue>):<fpage>1061</fpage>–<lpage>86</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037//0022-3514.52.6.1061" xlink:type="simple">10.1037//0022-3514.52.6.1061</ext-link></comment> <object-id pub-id-type="pmid">3598857</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref131"><label>131</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brainard</surname> <given-names>DH</given-names></name>. <article-title>The psychophysics toolbox</article-title>. <source>Spatial Vision</source>. <year>1997</year>;<volume>10</volume>:<fpage>433</fpage>–<lpage>6</lpage>. <object-id pub-id-type="pmid">9176952</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref132"><label>132</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Plant</surname> <given-names>RR</given-names></name>, <name name-style="western"><surname>Hammond</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Whitehouse</surname> <given-names>T</given-names></name>. <article-title>Toward an experimental timing standards lab: Benchmarking precision in the real world</article-title>. <source>Behavior Research Methods, Instruments, &amp; Computers</source>. <year>2002</year>;<volume>34</volume>(<issue>2</issue>):<fpage>218</fpage>–<lpage>26</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/bf03195446" xlink:type="simple">10.3758/bf03195446</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref133"><label>133</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cowie</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Douglas-Cowie</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Tsapatsoulis</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Votsis</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Kollias</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Fellenz</surname> <given-names>W</given-names></name>, <etal>et al</etal>. <article-title>Emotion recognition in human-computer interaction</article-title>. <source>IEEE Signal Processing Magazine</source>. <year>2001</year>;<volume>18</volume>(<issue>1</issue>):<fpage>32</fpage>–<lpage>80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/79.911197" xlink:type="simple">10.1109/79.911197</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref134"><label>134</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vurma</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ross</surname> <given-names>J</given-names></name>. <article-title>Production and perception of musical intervals. Music Perception</article-title>: <source>An Interdisciplinary Journal</source>. <year>2006</year>;<volume>23</volume>(<issue>4</issue>):<fpage>331</fpage>–<lpage>44</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1525/mp.2006.23.4.331" xlink:type="simple">10.1525/mp.2006.23.4.331</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref135"><label>135</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Besouw</surname> <given-names>RMV</given-names></name>, <name name-style="western"><surname>Brereton</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Howard</surname> <given-names>DM</given-names></name>. <article-title>Range of tuning for tones with and without vibrato. Perception</article-title>: <source>An Interdisciplinary Journal</source>. <year>2008</year>;<volume>26</volume>(<issue>2</issue>):<fpage>145</fpage>–<lpage>55</lpage>.</mixed-citation></ref>
<ref id="pone.0196391.ref136"><label>136</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hutchins</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Peretz</surname> <given-names>I</given-names></name>. <article-title>A frog in your throat or in your ear? Searching for the causes of poor singing</article-title>. <source>Journal of Experimental Psychology: General</source>. <year>2012</year>;<volume>141</volume>(<issue>1</issue>):<fpage>76</fpage>–<lpage>97</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0025064" xlink:type="simple">10.1037/a0025064</ext-link></comment> <object-id pub-id-type="pmid">21875245</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref137"><label>137</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frank</surname> <given-names>MG</given-names></name>, <name name-style="western"><surname>Stennett</surname> <given-names>J</given-names></name>. <article-title>The forced-choice paradigm and the perception of facial expressions of emotion</article-title>. <source>Journal of Personality and Social Psychology</source>. <year>2001</year>;<volume>80</volume>(<issue>1</issue>):<fpage>75</fpage>–<lpage>85</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/0022-3514.80.1.75" xlink:type="simple">10.1037/0022-3514.80.1.75</ext-link></comment> <object-id pub-id-type="pmid">11195893</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref138"><label>138</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wagner</surname> <given-names>HL</given-names></name>. <article-title>On measuring performance in category judgment studies of nonverbal behavior</article-title>. <source>Journal of Nonverbal Behavior</source>. <year>1993</year>;<volume>17</volume>(<issue>1</issue>):<fpage>3</fpage>–<lpage>28</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/bf00987006" xlink:type="simple">10.1007/bf00987006</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref139"><label>139</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fleiss</surname> <given-names>JL</given-names></name>. <article-title>Measuring nominal scale agreement among many raters</article-title>. <source>Psychological bulletin</source>. <year>1971</year>;<volume>76</volume>(<issue>5</issue>):<fpage>378</fpage>.</mixed-citation></ref>
<ref id="pone.0196391.ref140"><label>140</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Landis</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>GG</given-names></name>. <article-title>The measurement of observer agreement for categorical data</article-title>. <source>Biometrics</source>. <year>1977</year>;<volume>33</volume>(<issue>1</issue>):<fpage>159</fpage>–<lpage>74</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.2307/2529310" xlink:type="simple">10.2307/2529310</ext-link></comment> <object-id pub-id-type="pmid">843571</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref141"><label>141</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shrout</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Fleiss</surname> <given-names>JL</given-names></name>. <article-title>Intraclass correlations: uses in assessing rater reliability</article-title>. <source>Psychological Bulletin</source>. <year>1979</year>;<volume>86</volume>(<issue>2</issue>):<fpage>420</fpage>–<lpage>8</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037//0033-2909.86.2.420" xlink:type="simple">10.1037//0033-2909.86.2.420</ext-link></comment> <object-id pub-id-type="pmid">18839484</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref142"><label>142</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koo</surname> <given-names>TK</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>MY</given-names></name>. <article-title>A guideline of selecting and reporting intraclass correlation coefficients for reliability research</article-title>. <source>Journal of chiropractic medicine</source>. <year>2016</year>;<volume>15</volume>(<issue>2</issue>):<fpage>155</fpage>–<lpage>63</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jcm.2016.02.012" xlink:type="simple">10.1016/j.jcm.2016.02.012</ext-link></comment> <object-id pub-id-type="pmid">27330520</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref143"><label>143</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cicchetti</surname> <given-names>DV</given-names></name>. <article-title>Guidelines, criteria, and rules of thumb for evaluating normed and standardized assessment instruments in psychology</article-title>. <source>Psychological Assessment</source>. <year>1994</year>;<volume>6</volume>(<issue>4</issue>):<fpage>284</fpage>–<lpage>90</lpage>.</mixed-citation></ref>
<ref id="pone.0196391.ref144"><label>144</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hoaglin</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Iglewicz</surname> <given-names>B</given-names></name>. <article-title>Fine-tuning some resistant rules for outlier labeling</article-title>. <source>Journal of the American Statistical Association</source>. <year>1987</year>;<volume>82</volume>(<issue>400</issue>):<fpage>1147</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/01621459.1987.10478551" xlink:type="simple">10.1080/01621459.1987.10478551</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref145"><label>145</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hoaglin</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Iglewicz</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Tukey</surname> <given-names>JW</given-names></name>. <article-title>Performance of some resistant rules for outlier labeling</article-title>. <source>Journal of the American Statistical Association</source>. <year>1986</year>;<volume>81</volume>(<issue>396</issue>):<fpage>991</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/01621459.1986.10478363" xlink:type="simple">10.1080/01621459.1986.10478363</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref146"><label>146</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Dixon</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Massey</surname> <given-names>FJ</given-names></name>. <chapter-title>Introduction to statistical analysis</chapter-title>. <edition>2nd ed</edition>. <publisher-loc>New York, NY, USA</publisher-loc>: <publisher-name>McGraw-Hill</publisher-name>; <year>1957</year>.</mixed-citation></ref>
<ref id="pone.0196391.ref147"><label>147</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Girden</surname> <given-names>ER</given-names></name>. <chapter-title>ANOVA: Repeated Measures</chapter-title>. <publisher-loc>Newbury park, CA</publisher-loc>: <publisher-name>Sage</publisher-name>; <year>1992</year>.</mixed-citation></ref>
<ref id="pone.0196391.ref148"><label>148</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cramer</surname> <given-names>AO</given-names></name>, <name name-style="western"><surname>van Ravenzwaaij</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Matzke</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Steingroever</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Wetzels</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Grasman</surname> <given-names>RP</given-names></name>, <etal>et al</etal>. <article-title>Hidden multiplicity in exploratory multiway ANOVA: Prevalence and remedies</article-title>. <source>Psychonomic Bulletin &amp; Review</source>. <year>2016</year>;<volume>23</volume>(<issue>2</issue>):<fpage>640</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13423-015-0913-5" xlink:type="simple">10.3758/s13423-015-0913-5</ext-link></comment> <object-id pub-id-type="pmid">26374437</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref149"><label>149</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Benjamini</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hochberg</surname> <given-names>Y</given-names></name>. <article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title>. <source>Journal of the Royal Statistical Society, Series B (Methodological)</source>. <year>1995</year>;<volume>57</volume>:<fpage>289</fpage>–<lpage>300</lpage>.</mixed-citation></ref>
<ref id="pone.0196391.ref150"><label>150</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Team</surname> <given-names>RC</given-names></name>. <chapter-title>R: A Language and Environment for Statistical Computing</chapter-title>. <publisher-name>R Foundation for Statistical Computing</publisher-name>, <publisher-loc>Vienna, Austria</publisher-loc>. <year>2017</year>.</mixed-citation></ref>
<ref id="pone.0196391.ref151"><label>151</label><mixed-citation publication-type="book" xlink:type="simple"><collab>Team R</collab>. <source>RStudio: Integrated Development for R</source>. <publisher-loc>Boston, MA</publisher-loc>. <edition>1.1.383 ed</edition>: <publisher-name>RStudio, Inc.</publisher-name>; <year>2016</year>.</mixed-citation></ref>
<ref id="pone.0196391.ref152"><label>152</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gamer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lemon</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Fellows</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Singh</surname> <given-names>P</given-names></name>. <article-title>irr: Various Coefficients of Interrater Reliability and Agreement</article-title>. <year>2012</year>.</mixed-citation></ref>
<ref id="pone.0196391.ref153"><label>153</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wickham</surname> <given-names>H</given-names></name>. <article-title>tidyverse: Easily Install and Load the 'Tidyverse'</article-title>. <year>2017</year>.</mixed-citation></ref>
<ref id="pone.0196391.ref154"><label>154</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Matsumoto</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ekman</surname> <given-names>P</given-names></name>. <chapter-title>Japanese and Caucasian Facial Expressions of Emotion (JACFEE)</chapter-title>. <publisher-loc>Palo Alto, CA</publisher-loc>.: <publisher-name>Consulting Psychologists Press</publisher-name>; <year>1993–2004</year>.</mixed-citation></ref>
<ref id="pone.0196391.ref155"><label>155</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vieillard</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Peretz</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Gosselin</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Khalfa</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gagnon</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Bouchard</surname> <given-names>B</given-names></name>. <article-title>Happy, sad, scary and peaceful musical excerpts for research on emotions</article-title>. <source>Cognition &amp; Emotion</source>. <year>2008</year>;<volume>22</volume>(<issue>4</issue>):<fpage>720</fpage>–<lpage>52</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/02699930701503567" xlink:type="simple">10.1080/02699930701503567</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref156"><label>156</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paquette</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Peretz</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Belin</surname> <given-names>P</given-names></name>. <article-title>The “Musical Emotional Bursts”: a validated set of musical affect bursts to investigate auditory affective processing</article-title>. <source>Frontiers in psychology</source>. <year>2013</year>;<volume>4</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2013.00509" xlink:type="simple">10.3389/fpsyg.2013.00509</ext-link></comment> <object-id pub-id-type="pmid">23964255</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref157"><label>157</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carlo</surname> <given-names>NS</given-names></name>, <name name-style="western"><surname>Guaitella</surname> <given-names>I</given-names></name>. <article-title>Facial expressions of emotion in speech and singing</article-title>. <source>Semiotica</source>. <year>2004</year>;<volume>2004</volume>(<issue>149</issue>):<fpage>37</fpage>–<lpage>55</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1515/semi.2004.036" xlink:type="simple">10.1515/semi.2004.036</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref158"><label>158</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Davidson</surname> <given-names>JW</given-names></name>. <article-title>Visual perception of performance manner in the movements of solo musicians</article-title>. <source>Psychology of Music</source>. <year>1993</year>;<volume>21</volume>(<issue>2</issue>):<fpage>103</fpage>–<lpage>13</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/030573569302100201" xlink:type="simple">10.1177/030573569302100201</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref159"><label>159</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vines</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>Krumhansl</surname> <given-names>CL</given-names></name>, <name name-style="western"><surname>Wanderley</surname> <given-names>MM</given-names></name>, <name name-style="western"><surname>Dalca</surname> <given-names>IM</given-names></name>, <name name-style="western"><surname>Levitin</surname> <given-names>DJ</given-names></name>. <source>Music to my eyes: Cross-modal interactions in the perception of emotions in musical performance</source>. <source>Cognition</source>. <year>2011</year>;<volume>118</volume>(<issue>2</issue>):<fpage>157</fpage>–<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cognition.2010.11.010" xlink:type="simple">10.1016/j.cognition.2010.11.010</ext-link></comment> <object-id pub-id-type="pmid">21146164</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref160"><label>160</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chang</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Livingstone</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Bosnyak</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Trainor</surname> <given-names>LJ</given-names></name>. <article-title>Body sway reflects leadership in joint music performance</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2017</year>;<volume>114</volume>(<issue>21</issue>):<fpage>E4134</fpage>–<lpage>E41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1617657114" xlink:type="simple">10.1073/pnas.1617657114</ext-link></comment> <object-id pub-id-type="pmid">28484007</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref161"><label>161</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Livingstone</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Palmer</surname> <given-names>C</given-names></name>. <article-title>Head movements encode emotions during speech and song</article-title>. <source>Emotion</source>. <year>2016</year>;<volume>16</volume>(<issue>3</issue>):<fpage>365</fpage>–<lpage>80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/emo0000106" xlink:type="simple">10.1037/emo0000106</ext-link></comment> <object-id pub-id-type="pmid">26501928</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref162"><label>162</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Banse</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>. <article-title>Acoustic profiles in vocal emotion expression</article-title>. <source>Journal of Personality and Social Psychology</source>. <year>1996</year>;<volume>70</volume>(<issue>3</issue>):<fpage>614</fpage>. <object-id pub-id-type="pmid">8851745</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref163"><label>163</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sauter</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Eisner</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Calder</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Scott</surname> <given-names>SK</given-names></name>. <article-title>Perceptual cues in nonverbal vocal expressions of emotion</article-title>. <source>The Quarterly Journal of Experimental Psychology</source>. <year>2010</year>;<volume>63</volume>(<issue>11</issue>):<fpage>2251</fpage>–<lpage>72</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/17470211003721642" xlink:type="simple">10.1080/17470211003721642</ext-link></comment> <object-id pub-id-type="pmid">20437296</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref164"><label>164</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sauter</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Scott</surname> <given-names>SK</given-names></name>. <article-title>More than one kind of happiness: Can we recognize vocal expressions of different positive states?</article-title> <source>Motivation and Emotion</source>. <year>2007</year>;<volume>31</volume>(<issue>3</issue>):<fpage>192</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s11031-007-9065-x" xlink:type="simple">10.1007/s11031-007-9065-x</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref165"><label>165</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lima</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>Castro</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Scott</surname> <given-names>SK</given-names></name>. <article-title>When voices get emotional: a corpus of nonverbal vocalizations for research on emotion processing</article-title>. <source>Behavior Research Methods</source>. <year>2013</year>;<volume>45</volume>(<issue>4</issue>):<fpage>1234</fpage>–<lpage>45</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13428-013-0324-3" xlink:type="simple">10.3758/s13428-013-0324-3</ext-link></comment> <object-id pub-id-type="pmid">23444120</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref166"><label>166</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sauter</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Eisner</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Ekman</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Scott</surname> <given-names>SK</given-names></name>. <article-title>Cross-cultural recognition of basic emotions through nonverbal emotional vocalizations</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2010</year>;<volume>107</volume>(<issue>6</issue>):<fpage>2408</fpage>–<lpage>12</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0908239106" xlink:type="simple">10.1073/pnas.0908239106</ext-link></comment> <object-id pub-id-type="pmid">20133790</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref167"><label>167</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Spencer</surname> <given-names>H</given-names></name>. <chapter-title>The principles of psychology</chapter-title>. <publisher-loc>London</publisher-loc>: <publisher-name>Longman, Brown, Green, and Longmans</publisher-name>; <year>1855</year>.</mixed-citation></ref>
<ref id="pone.0196391.ref168"><label>168</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Wundt</surname> <given-names>W</given-names></name>. <chapter-title>Outlines of psychology (Charles Hubbard Judd., Trans.)</chapter-title>. <publisher-loc>Oxford, England</publisher-loc>: <publisher-name>Engelman</publisher-name>; <year>1896</year>.</mixed-citation></ref>
<ref id="pone.0196391.ref169"><label>169</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Osgood</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Suci</surname> <given-names>GJ</given-names></name>, <name name-style="western"><surname>Tannenbaum</surname> <given-names>PH</given-names></name>. <chapter-title>The measurement of meaning</chapter-title>. <publisher-loc>Urbana</publisher-loc>: <publisher-name>University of Illinois Press</publisher-name>; <year>1967</year>.</mixed-citation></ref>
<ref id="pone.0196391.ref170"><label>170</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schlosberg</surname> <given-names>H</given-names></name>. <article-title>A scale for the judgment of facial expressions</article-title>. <source>Journal of experimental psychology</source>. <year>1941</year>;<volume>29</volume>(<issue>6</issue>):<fpage>497</fpage>–<lpage>510</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/h0061489" xlink:type="simple">10.1037/h0061489</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref171"><label>171</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schlosberg</surname> <given-names>H</given-names></name>. <article-title>The description of facial expressions in terms of two dimensions</article-title>. <source>Journal of Experimental Psychology</source>. <year>1952</year>;<volume>44</volume>(<issue>4</issue>):<fpage>229</fpage>–<lpage>37</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/h0055778" xlink:type="simple">10.1037/h0055778</ext-link></comment> <object-id pub-id-type="pmid">13000062</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref172"><label>172</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Russell</surname> <given-names>JA</given-names></name>. <article-title>A circumplex model of affect</article-title>. <source>Journal of Personality and Social Psychology</source>. <year>1980</year>;<volume>39</volume>(<issue>6</issue>):<fpage>1161</fpage>–<lpage>78</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/h0077714" xlink:type="simple">10.1037/h0077714</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref173"><label>173</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Russell</surname> <given-names>JA</given-names></name>. <article-title>Core affect and the psychological construction of emotion</article-title>. <source>Psychological Review</source>. <year>2003</year>;<volume>110</volume>(<issue>1</issue>):<fpage>145</fpage>–<lpage>72</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037//0033-295x.110.1.145" xlink:type="simple">10.1037//0033-295x.110.1.145</ext-link></comment> <object-id pub-id-type="pmid">12529060</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref174"><label>174</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barrett</surname> <given-names>LF</given-names></name>, <name name-style="western"><surname>Russell</surname> <given-names>JA</given-names></name>. <article-title>The structure of current affect</article-title>. <source>Current Directions in Psychological Science</source>. <year>1999</year>;<volume>8</volume>(<issue>1</issue>):<fpage>10</fpage>–<lpage>4</lpage>.</mixed-citation></ref>
<ref id="pone.0196391.ref175"><label>175</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laukka</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Neiberg</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Forsell</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Karlsson</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Elenius</surname> <given-names>K</given-names></name>. <article-title>Expression of affect in spontaneous speech: Acoustic correlates and automatic detection of irritation and resignation</article-title>. <source>Computer Speech &amp; Language</source>. <year>2011</year>;<volume>25</volume>(<issue>1</issue>):<fpage>84</fpage>–<lpage>104</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.csl.2010.03.004" xlink:type="simple">10.1016/j.csl.2010.03.004</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref176"><label>176</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tcherkassof</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bollon</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Dubois</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Pansu</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Adam</surname> <given-names>JM</given-names></name>. <article-title>Facial expressions of emotions: A methodological contribution to the study of spontaneous and dynamic emotional faces</article-title>. <source>European Journal of Social Psychology</source>. <year>2007</year>;<volume>37</volume>(<issue>6</issue>):<fpage>1325</fpage>–<lpage>45</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/ejsp.427" xlink:type="simple">10.1002/ejsp.427</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref177"><label>177</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Cowie</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Douglas-Cowie</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Sneddon</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Batliner</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pelachaud</surname> <given-names>C</given-names></name>. <chapter-title>Principles and History</chapter-title>. In: <name name-style="western"><surname>Cowie</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Pelachaud</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Petta</surname> <given-names>P</given-names></name>, editors. <source>Emotion-Oriented Systems</source>. <publisher-loc>Berlin, Heidelberg</publisher-loc>: <collab>Springer</collab>; <year>2011</year>. p. <fpage>167</fpage>–<lpage>96</lpage>.</mixed-citation></ref>
<ref id="pone.0196391.ref178"><label>178</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cohn</surname> <given-names>JF</given-names></name>, <name name-style="western"><surname>Schmidt</surname> <given-names>KL</given-names></name>. <article-title>The timing of facial motion in posed and spontaneous smiles</article-title>. <source>International Journal of Wavelets, Multiresolution and Information Processing</source>. <year>2004</year>;<volume>2</volume>:<fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation></ref>
<ref id="pone.0196391.ref179"><label>179</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Motley</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Camden</surname> <given-names>CT</given-names></name>. <article-title>Facial expression of emotion: A comparison of posed expressions versus spontaneous expressions in an interpersonal communication setting</article-title>. <source>Western Journal of Speech Communication</source>. <year>1988</year>;<volume>52</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>22</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/10570318809389622" xlink:type="simple">10.1080/10570318809389622</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref180"><label>180</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wagner</surname> <given-names>HL</given-names></name>, <name name-style="western"><surname>MacDonald</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Manstead</surname> <given-names>A</given-names></name>. <article-title>Communication of individual emotions by spontaneous facial expressions</article-title>. <source>Journal of Personality and Social Psychology</source>. <year>1986</year>;<volume>50</volume>(<issue>4</issue>):<fpage>737</fpage>–<lpage>43</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037//0022-3514.50.4.737" xlink:type="simple">10.1037//0022-3514.50.4.737</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref181"><label>181</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Cowie</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Douglas-Cowie</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>McRorie</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sneddon</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Devillers</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Amir</surname> <given-names>N</given-names></name>. <chapter-title>Issues in data collection</chapter-title>. In: <name name-style="western"><surname>R</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>C</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>P</surname> <given-names>P</given-names></name>, editors. <source>Emotion-Oriented Systems</source>. <publisher-loc>Berlin, Heidelberg</publisher-loc>: <collab>Springer</collab>; <year>2011</year>. p. <fpage>197</fpage>–<lpage>212</lpage>.</mixed-citation></ref>
<ref id="pone.0196391.ref182"><label>182</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cowie</surname> <given-names>R</given-names></name>. <article-title>Perceiving emotion: towards a realistic understanding of the task</article-title>. <source>Philosophical Transactions of the Royal Society of London B: Biological Sciences</source>. <year>2009</year>;<volume>364</volume>(<issue>1535</issue>):<fpage>3515</fpage>–<lpage>25</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2009.0139" xlink:type="simple">10.1098/rstb.2009.0139</ext-link></comment> <object-id pub-id-type="pmid">19884146</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref183"><label>183</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liu</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Pell</surname> <given-names>MD</given-names></name>. <article-title>Recognizing vocal emotions in Mandarin Chinese: A validated database of Chinese vocal emotional stimuli</article-title>. <source>Behavior Research Methods</source>. <year>2012</year>;<volume>44</volume>(<issue>4</issue>):<fpage>1042</fpage>–<lpage>51</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13428-012-0203-3" xlink:type="simple">10.3758/s13428-012-0203-3</ext-link></comment> <object-id pub-id-type="pmid">22539230</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref184"><label>184</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Keltner</surname> <given-names>D</given-names></name>. <article-title>Signs of appeasement: Evidence for the distinct displays of embarrassment, amusement, and shame</article-title>. <source>Journal of Personality and Social Psychology</source>. <year>1995</year>;<volume>68</volume>(<issue>3</issue>):<fpage>441</fpage>–<lpage>54</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037//0022-3514.68.3.441" xlink:type="simple">10.1037//0022-3514.68.3.441</ext-link></comment></mixed-citation></ref>
<ref id="pone.0196391.ref185"><label>185</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tracy</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Matsumoto</surname> <given-names>D</given-names></name>. <article-title>The spontaneous expression of pride and shame: Evidence for biologically innate nonverbal displays</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2008</year>;<volume>105</volume>(<issue>33</issue>):<fpage>11655</fpage>–<lpage>60</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0802686105" xlink:type="simple">10.1073/pnas.0802686105</ext-link></comment> <object-id pub-id-type="pmid">18695237</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref186"><label>186</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tracy</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Robins</surname> <given-names>RW</given-names></name>. <article-title>Show your pride evidence for a discrete emotion expression</article-title>. <source>Psychological Science</source>. <year>2004</year>;<volume>15</volume>(<issue>3</issue>):<fpage>194</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.0956-7976.2004.01503008.x" xlink:type="simple">10.1111/j.0956-7976.2004.01503008.x</ext-link></comment> <object-id pub-id-type="pmid">15016291</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref187"><label>187</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Garrido</surname> <given-names>MV</given-names></name>, <name name-style="western"><surname>Lopes</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Prada</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Rodrigues</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Jerónimo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Mourão</surname> <given-names>RP</given-names></name>. <article-title>The many faces of a face: Comparing stills and videos of facial expressions in eight dimensions (SAVE database)</article-title>. <source>Behavior Research Methods</source>. <year>2017</year>;<volume>49</volume>(<issue>4</issue>):<fpage>1343</fpage>–<lpage>60</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13428-016-0790-5" xlink:type="simple">10.3758/s13428-016-0790-5</ext-link></comment> <object-id pub-id-type="pmid">27573005</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref188"><label>188</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilhelm</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Hildebrandt</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Manske</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Schacht</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sommer</surname> <given-names>W</given-names></name>. <article-title>Test battery for measuring the perception and recognition of facial expressions of emotion</article-title>. <source>Frontiers in Psychology</source>. <year>2014</year>;<volume>5</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2014.00404" xlink:type="simple">10.3389/fpsyg.2014.00404</ext-link></comment> <object-id pub-id-type="pmid">24860528</object-id></mixed-citation></ref>
<ref id="pone.0196391.ref189"><label>189</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bänziger</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>. <article-title>Introducing the geneva multimodal emotion portrayal (gemep) corpus</article-title>. <source>Blueprint for affective computing: A sourcebook</source>. <year>2010</year>:<fpage>271</fpage>–<lpage>94</lpage>.</mixed-citation></ref>
<ref id="pone.0196391.ref190"><label>190</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van Der Schalk</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Hawk</surname> <given-names>ST</given-names></name>, <name name-style="western"><surname>Fischer</surname> <given-names>AH</given-names></name>, <name name-style="western"><surname>Doosje</surname> <given-names>B</given-names></name>. <article-title>Moving faces, looking places: validation of the Amsterdam Dynamic Facial Expression Set (ADFES)</article-title>. <source>Emotion</source>. <year>2011</year>;<volume>11</volume>(<issue>4</issue>):<fpage>907</fpage>–<lpage>20</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0023853" xlink:type="simple">10.1037/a0023853</ext-link></comment> <object-id pub-id-type="pmid">21859206</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>