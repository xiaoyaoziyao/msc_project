<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PCOMPBIOL-D-11-00886</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1002303</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Computational biology</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subj-group>
                <subject>Circuit models</subject>
                <subject>Coding mechanisms</subject>
                <subject>Sensory systems</subject>
              </subj-group>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Neurophysiology</subject>
              <subj-group>
                <subject>Motor systems</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Neural networks</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Nonlinear dynamics</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Computational Biology</subject>
          <subject>Neuroscience</subject>
          <subject>Mathematics</subject>
        </subj-group>
      </article-categories><title-group><article-title>A Hierarchical Neuronal Model for Generation and Online Recognition of Birdsongs</article-title><alt-title alt-title-type="running-head">A Model for Generation and Recognition of Birdsong</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Yildiz</surname>
            <given-names>Izzet B.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Kiebel</surname>
            <given-names>Stefan J.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
      </contrib-group><aff id="aff1">          <addr-line>Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Sporns</surname>
            <given-names>Olaf</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">Indiana University, United States of America</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">yildiz@cbs.mpg.de</email></corresp>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: IBY SJK. Performed the experiments: IBY SJK. Analyzed the data: IBY. Contributed reagents/materials/analysis tools: IBY SJK. Wrote the paper: IBY SJK.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <month>12</month>
        <year>2011</year>
      </pub-date><pub-date pub-type="epub">
        <day>15</day>
        <month>12</month>
        <year>2011</year>
      </pub-date><volume>7</volume><issue>12</issue><elocation-id>e1002303</elocation-id><history>
        <date date-type="received">
          <day>20</day>
          <month>6</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>29</day>
          <month>10</month>
          <year>2011</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2011</copyright-year><copyright-holder>Yildiz, Kiebel</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>The neuronal system underlying learning, generation and recognition of song in birds is one of the best-studied systems in the neurosciences. Here, we use these experimental findings to derive a neurobiologically plausible, dynamic, hierarchical model of birdsong generation and transform it into a functional model of birdsong recognition. The generation model consists of neuronal rate models and includes critical anatomical components like the premotor song-control nucleus HVC (proper name), the premotor nucleus RA (robust nucleus of the arcopallium), and a model of the syringeal and respiratory organs. We use Bayesian inference of this dynamical system to derive a possible mechanism for how birds can efficiently and robustly recognize the songs of their conspecifics in an online fashion. Our results indicate that the specific way birdsong is generated enables a listening bird to robustly and rapidly perceive embedded information at multiple time scales of a song. The resulting mechanism can be useful for investigating the functional roles of auditory recognition areas and providing predictions for future birdsong experiments.</p>
      </abstract><abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>How do birds communicate via their songs? Investigating this question may not only lead to a better understanding of communication via birdsong, but many believe that the answer will also give us hints about how humans decode speech from complex sound wave modulations. In birds, the output and neuronal responses of the song generation system can be measured precisely and this has resulted in a considerable body of experimental findings. We used these findings to assemble a complete model of birdsong generation and use it as the basis for constructing a potentially neurobiologically plausible, artificial recognition system based on state-of-the-art Bayesian inference techniques. Our artificial system resembles the real birdsong system when performing recognition tasks and may be used as a functional model to explain and predict experimental findings in song recognition.</p>
      </abstract><funding-group><funding-statement>IBY and SJK are funded by the Max Planck Society (<ext-link ext-link-type="uri" xlink:href="http://www.mpg.de/en" xlink:type="simple">http://www.mpg.de/en</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="18"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Songbirds are able to repeat the same, often complex songs with amazing precision. When male birds sing to a female repeatedly, there is on average a 1% temporal deviation across the whole song <xref ref-type="bibr" rid="pcbi.1002303-Chi1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Yu1">[2]</xref>. This combination of complexity and precision is remarkable. Studying the neuronal basis of birdsong generation may lead to an understanding of the mechanism underlying how sequences of song syllables are expressed as complex and temporally precise sound wave modulations. More generally, such a mechanism may also be useful for understanding how action sequences at a relatively slow time-scale (e.g. the words in a sentence) can be generated by a neuronal system while a high degree of precision is maintained in the output at a fast time-scale (e.g. the sound wave modulations necessary to form speech sounds).</p>
      <p>Recent findings <xref ref-type="bibr" rid="pcbi.1002303-Chi1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1002303-Hahnloser1">[3]</xref> have shown that the song generation mechanism in birds is hierarchical where neurons in one particular high-level structure, HVC, fire in a specific sequence with high temporal precision and drive neurons in the lower level structure RA (robust nucleus of the arcopallium).</p>
      <p>Female birds, at which the songs are typically directed, are expert in registering variables like the speed of the song and the precision and the repertoire of the singer <xref ref-type="bibr" rid="pcbi.1002303-OLochlen1">[4]</xref>–<xref ref-type="bibr" rid="pcbi.1002303-Ballentine1">[8]</xref>. Unfortunately, the study of song recognition is more challenging than song generation because experimental indicators for recognition, such as the subsequent behavior of a female bird, are more difficult to measure than indicators for song generation. This has led to a long list of experimental and theoretical findings on song generation and learning while the mechanisms of song recognition remain relatively elusive.</p>
      <p>Here, we propose that the functional mechanism of song <italic>recognition</italic> can be obtained from the song <italic>generation</italic> mechanism. The basic idea underlying this novel modeling approach is that female birds are optimal in song recognition because their mating choice critically depends on the optimal recognition of valuable features of the male which are revealed by subtle indicators in his song. Similarly, male birds should be able to distinguish the songs of their neighbors from the songs of strangers to protect their territories <xref ref-type="bibr" rid="pcbi.1002303-Stoddard1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Godard1">[10]</xref>. Using a recently established Bayesian inference technique for nonlinear dynamical systems <xref ref-type="bibr" rid="pcbi.1002303-Friston1">[11]</xref>, we can emulate this optimal recognition: the key ingredient is a generative model (a nonlinear dynamical system) which can generate a specific song. Usually, generative models for complex sensory dynamics, such as the sound wave or spectrum of birdsong, are difficult to derive because it is hard to describe a complex multi-scale structure like birdsong using only differential equations. Fortunately, since the hierarchical birdsong generating system is so well-studied, parts of such a model already exist, in particular at the level of the HVC, RA and vocal tract dynamics <xref ref-type="bibr" rid="pcbi.1002303-Li1">[12]</xref>–<xref ref-type="bibr" rid="pcbi.1002303-Abarbanel1">[18]</xref>. We have combined these parts into a coherent whole, guided by key experimental results, to form a generative model that can play complex songs. In particular, we combined sequence-generating dynamics, attractor dynamics and a model of vocal tract dynamics <xref ref-type="bibr" rid="pcbi.1002303-Laje1">[17]</xref> in a three-level, hierarchical nonlinear dynamical system. This dynamic model is based on neuronal rate models, thereby describing the biological system at a mesoscopic level. We then used Bayesian inference to derive another set of hierarchical, nonlinear differential equations (recognition system) which is, by way of construction, Bayes-optimal in recognizing this song and can be compared to the real birdsong recognition system. To do this, we exposed the agent to several tasks and found that the agent's dynamics and performance were reminiscent of song recognition in real bird brains in aspects such as sensitivity to speed changes <xref ref-type="bibr" rid="pcbi.1002303-Nagel1">[19]</xref> and song perturbations <xref ref-type="bibr" rid="pcbi.1002303-Gill1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Keller1">[21]</xref>. Thus, by harnessing rich experimental and theoretical results in birdsong generation, we were able to derive a novel, functional model of birdsong recognition. We discuss the experimental evidence that the identified mechanism is indeed used for song recognition by birds. We suggest that the present model may be useful for understanding the functional and computational roles of auditory recognition areas. In addition, the identified recognition mechanism can be used as a novel machine learning tool to recognize sequential behavior from fast sensory input, e.g. in artificial speech recognition.</p>
    </sec>
    <sec id="s2">
      <title>Model</title>
      <p>In this section, we will briefly summarize relevant experimental findings, motivate and describe the present model for birdsong generation and briefly give the mathematical details.</p>
      <p>A birdsong consists of small units called notes (analogous to phonetic units in speech) which can be grouped together to form syllables <xref ref-type="bibr" rid="pcbi.1002303-Doupe1">[22]</xref>. A combination of identical or different syllables forms motifs. This hierarchical structure of song units is produced by two highly specialized song pathways (<xref ref-type="fig" rid="pcbi-1002303-g001">Figure 1</xref>, see <xref ref-type="bibr" rid="pcbi.1002303-Bolhuis1">[23]</xref> for a review). In the motor pathway, the forebrain nucleus HVC includes specific neurons called HVC<sub>(RA)</sub> that project to nucleus RA. RA neurons innervate the vocal and respiratory nuclei to produce vocal output. The anterior forebrain pathway is involved in learning new songs and producing variability for the song structure <xref ref-type="bibr" rid="pcbi.1002303-Kao1">[24]</xref>.</p>
      <fig id="pcbi-1002303-g001" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002303.g001</object-id>
        <label>Figure 1</label>
        <caption>
          <title>The schematic diagram of a songbird brain with the motor pathway (red arrows), which is considered in the model of song generation, and the anterior forebrain pathway (AFP, black arrows).</title>
          <p>In the motor pathway RA neurons that are driven by the HVC control the motor (nXIIts innervates the syrinx) and respiratory (DM) areas. The anterior forebrain pathway communicates with the motor pathway through the LMAN area that provides direct input to the RA region. Abbreviations: DLM, nucleus dorsolateralis anterior, pars medialis; DM, dorsomedial nucleus; HVC, a letter based name; LMAN, lateral magnocellular nucleus of the anterior nidopallium; nXIIts, tracheosyringeal portion of the nucleus hypoglossus; RA, robust nucleus of the arcopallium. Adapted from <xref ref-type="bibr" rid="pcbi.1002303-Nottebohm1">[87]</xref>.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.g001" xlink:type="simple"/>
      </fig>
      <p>Our modeling approach is based on the following key experimental observations: During birdsong generation, HVC<sub>(RA)</sub> neurons fire sequentially at temporally precise moments where each element of this sequence fires only once during the song to control a group of RA neurons <xref ref-type="bibr" rid="pcbi.1002303-Yu1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Hahnloser1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Fee1">[25]</xref>. This suggests that bursting HVC<sub>(RA)</sub> neurons select and drive the activity of subsets of RA neurons <xref ref-type="bibr" rid="pcbi.1002303-Fee1">[25]</xref>. In particular, each RA neuron can be driven by more than one HVC<sub>(RA)</sub> neuron <xref ref-type="bibr" rid="pcbi.1002303-Leonardo1">[26]</xref>, see <xref ref-type="fig" rid="pcbi-1002303-g002">Figure 2</xref>.</p>
      <fig id="pcbi-1002303-g002" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002303.g002</object-id>
        <label>Figure 2</label>
        <caption>
          <title>The scheme of HVC and RA dynamics.</title>
          <p>Five RA ensembles are controlled by eight sequentially activated HVC<sub>(RA)</sub> ensembles. The horizontal axis denotes time and the arrows describe the specific HVC ensemble that activates the corresponding RA ensembles. The color scheme matches the dynamics shown in <xref ref-type="fig" rid="pcbi-1002303-g005">Figure 5</xref>. The part of the song obtained from the first three RA-patterns (i.e., ensemble combinations 2–4, 1-3-5 and 1–4) is shown as a sonogram in <xref ref-type="fig" rid="pcbi-1002303-g003">Figure 3</xref>.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.g002" xlink:type="simple"/>
      </fig>
      <p>How can one model such a mechanism? There have been several approaches to model the sequential activation of HVC<sub>(RA)</sub> neurons using single neuron models <xref ref-type="bibr" rid="pcbi.1002303-Li1">[12]</xref>–<xref ref-type="bibr" rid="pcbi.1002303-Gibb1">[15]</xref>. Here, we follow an alternative way by capturing the neuronal mass activity using firing rate models, i.e. we consider model neurons that can be thought of as the synchronized firing activity of an ensemble of neurons. This is motivated by experimental evidence suggesting that there are about 200 co-active HVC<sub>(RA)</sub> neurons at a specific time during song generation <xref ref-type="bibr" rid="pcbi.1002303-Fee1">[25]</xref>. One of the well established ways for modeling the sequential activation of neuronal ensembles is the winnerless competition using Lotka-Volterra type dynamics <xref ref-type="bibr" rid="pcbi.1002303-Afraimovich1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Rabinovich1">[28]</xref>. This approach aims at modeling activity at a mesoscopic level, e.g. activity that may be expressed in local field potentials.</p>
      <p>Another benefit of using ensemble dynamics appears at the RA level where each ensemble controls the vocal tract muscles in a specific way. Different than HVC<sub>(RA)</sub> ensembles, one or more RA ensembles can activate simultaneously (synchronize) <xref ref-type="bibr" rid="pcbi.1002303-Fee1">[25]</xref> (<xref ref-type="fig" rid="pcbi-1002303-g002">Figure 2</xref>). We hypothesize that the complex sound wave modulations that can be observed in many birdsongs are generated by this network of RA ensembles using spatiotemporal coding (see also <xref ref-type="bibr" rid="pcbi.1002303-Leonardo1">[26]</xref>). This coding requires the activation of different sets of RA ensembles (spatial coding) when the proper signals are received from the corresponding HVC<sub>(RA)</sub> ensembles (temporal coding). This spatiotemporal coding can be modeled with network states which are driven from one attractor to another where each of these attractors specifies the currently active RA ensembles. In other words, when HVC<sub>(RA)</sub> ensembles undergo sequential activations, the RA level is driven from one attractor to the next. Such networks with attractor dynamics (so called Hopfield networks <xref ref-type="bibr" rid="pcbi.1002303-Hopfield1">[29]</xref>) can encode a large number of potential attractors because the forcing input from the HVC level effectively recombines subsets of RA ensembles in distinct assemblies.</p>
      <p>Note that since the activity of each unit represents the average firing rate of an ensemble of neurons, some features of neural activity at the level of individual neurons are not considered. Here, we focus on capturing the two key features of the hierarchy, which are the sequential firing of HVC ensembles and the spatiotemporal coding at the RA level. Therefore, the choice of parameters in the computational model below are motivated by capturing the specific dynamics inferred by experiments <xref ref-type="bibr" rid="pcbi.1002303-Fee1">[25]</xref>.</p>
      <p>At the lowest level, we map the dynamical RA states onto motor neurons. To do this, we compute linear combinations of oscillators at different frequencies which represent the effect of currently active RA ensembles and create dynamical control signals (<xref ref-type="fig" rid="pcbi-1002303-g003">Figure 3</xref>) for a model of the vocal organ, the syrinx <xref ref-type="bibr" rid="pcbi.1002303-Laje1">[17]</xref>. This mathematical model of the syrinx has been used previously to model several birdsongs <xref ref-type="bibr" rid="pcbi.1002303-Gardner1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Laje2">[31]</xref>.</p>
      <fig id="pcbi-1002303-g003" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002303.g003</object-id>
        <label>Figure 3</label>
        <caption>
          <title>Motor control signals and resulting power spectra generated by the model.</title>
          <p>Left: The motor control signals are obtained by a linear combination of sine waves (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e001" xlink:type="simple"/></inline-formula>) with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e002" xlink:type="simple"/></inline-formula> (<italic>x</italic>-axis) and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e003" xlink:type="simple"/></inline-formula> (<italic>y</italic>-axis) where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e004" xlink:type="simple"/></inline-formula> in (<bold>A</bold>), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e005" xlink:type="simple"/></inline-formula> in (<bold>B</bold>) and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e006" xlink:type="simple"/></inline-formula> in (<bold>C</bold>). Right: These <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e007" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e008" xlink:type="simple"/></inline-formula> dynamics are used in the syrinx equations (4) to obtain sound waves with the corresponding sonograms (time (sec) vs. frequency (kHz)). <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e009" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e010" xlink:type="simple"/></inline-formula> control the amplitude and frequency of the sound waves, respectively. When <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e011" xlink:type="simple"/></inline-formula>, no phonation is produced (mini-breaths, <xref ref-type="bibr" rid="pcbi.1002303-Hartley1">[44]</xref>). The fluctuations in the fundamental frequencies in the sonograms on the right can be traced by moving in counter clock-wise direction on the ellipse-like curves on the left starting from the blue arrows at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e012" xlink:type="simple"/></inline-formula>.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.g003" xlink:type="simple"/>
      </fig>
      <p>In summary, the present three-level hierarchical model generates sequences at its top (HVC) level, which are transformed into sequences of multi-dimensional attractors at the RA level. Each of these attractors encodes a mixture of oscillations. These oscillatory dynamics enter a syrinx model as a control signal to produce a birdsong sonogram. In the following, we describe the equations used at each level in detail (see <xref ref-type="fig" rid="pcbi-1002303-g004">Figure 4</xref> for an overview). The Bayesian recognition of dynamics generated by this birdsong model is described at the end of the section.</p>
      <fig id="pcbi-1002303-g004" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002303.g004</object-id>
        <label>Figure 4</label>
        <caption>
          <title>Summary of nonlinear differential equations (1), (3), (4) and (5) (see <xref ref-type="supplementary-material" rid="pcbi.1002303.s006">Text S1</xref> for Eq. (5)) that are used in the hierarchical model for birdsong generation.</title>
          <p> Notice that the output at each level is used as an input to the lower level. Typical dynamics of HVC, RA and oscillator (Osc.) levels are given in <xref ref-type="fig" rid="pcbi-1002303-g005">Figure 5A, 5B and 5C</xref>, respectively. Finally, the output of the oscillator level is used in the syrinx equations to produce appropriate sound waves (<xref ref-type="fig" rid="pcbi-1002303-g006">Figure 6</xref>). See <xref ref-type="table" rid="pcbi-1002303-t001">Table 1</xref> for the parameters.</p>
        </caption>
        <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.g004" xlink:type="simple"/>
      </fig>
      <sec id="s2a">
        <title>HVC Sequential Dynamics</title>
        <p>Lotka-Volterra equations are well known in population biology to describe the competition between species <xref ref-type="bibr" rid="pcbi.1002303-May1">[32]</xref>. Rabinovich et al. (see <xref ref-type="bibr" rid="pcbi.1002303-Rabinovich2">[33]</xref> for a review) applied this idea more generally to neuronal dynamics under the name of winnerless competition, see <xref ref-type="bibr" rid="pcbi.1002303-Rabinovich1">[28]</xref> and <xref ref-type="bibr" rid="pcbi.1002303-Varona1">[34]</xref> for applications. In the following, we will describe how one can apply this idea to model sequential HVC activity by a nonlinear dynamical system. In the winnerless competition setting, there are <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e013" xlink:type="simple"/></inline-formula> equilibrium points which are saddles of a nonlinear dynamical system. Each of these equilibrium points has a single unstable direction and all other directions are stable. One can think of these saddle points as the beads on a string where the unstable manifold of one saddle point is the stable manifold of the next saddle point and this sequence continues in a circular fashion forming a heteroclinic chain. Under some conditions <xref ref-type="bibr" rid="pcbi.1002303-Afraimovich1">[27]</xref>, this sequence is stable, i.e. a solution of the system that starts from a neighborhood of the chain, stays in this neighborhood at all times while traveling through all saddle points. This stable sequential behavior is what we exploit to model the experimentally established sequential activities of HVC<sub>(RA)</sub> ensembles at the highest level. As the solution of the system moves along the string, it visits all saddle points, i.e. each HVC<sub>(RA)</sub> ensemble, one by one thereby activating each ensemble for a brief period until it is deactivated as the next ensemble becomes active.</p>
        <p>These dynamics can be obtained from a neural mass model of mean membrane potential and action firing potential <xref ref-type="bibr" rid="pcbi.1002303-Fukai1">[35]</xref>, reviewed in <xref ref-type="bibr" rid="pcbi.1002303-Rabinovich2">[33]</xref>. We use the equations:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e014" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e015" xlink:type="simple"/></inline-formula> is the hidden-state vector (e.g., mean membrane potentials) at the third (HVC) level, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e016" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e017" xlink:type="simple"/></inline-formula> are scalars, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e018" xlink:type="simple"/></inline-formula> is the sigmoid function applied component-wise and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e019" xlink:type="simple"/></inline-formula> is the connectivity matrix with entries <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e020" xlink:type="simple"/></inline-formula> giving the strength of inhibition from state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e021" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e022" xlink:type="simple"/></inline-formula>. The second equation describes the output vector (or causal-state vector; e.g., neural firing rates) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e023" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e024" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e025" xlink:type="simple"/></inline-formula>, is a normalizing function. We also add normally distributed noise vectors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e026" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e027" xlink:type="simple"/></inline-formula> to render the model stochastic. With an appropriately chosen connectivity matrix, one can obtain a system with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e028" xlink:type="simple"/></inline-formula> saddle points forming a stable heteroclinic chain <xref ref-type="bibr" rid="pcbi.1002303-Afraimovich1">[27]</xref>. For the entries of the connectivity matrix, one chooses high inhibition from the previously active neuron to the currently active neuron and low inhibition from the current active neuron to the next neuron which will become active:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e029" xlink:type="simple"/></disp-formula>(Here <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e030" xlink:type="simple"/></inline-formula> when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e031" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e032" xlink:type="simple"/></inline-formula> when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e033" xlink:type="simple"/></inline-formula>).</p>
        <p>Note that, theoretically, one can generate arbitrarily long sequences of HVC activation using the above connectivity matrix. The stability region around the heteroclinic chain will persist for much longer sequences than the one modeled here. For our illustrative simulations described below, we use <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e034" xlink:type="simple"/></inline-formula>, i.e., there are 8 HVC<sub>(RA)</sub> neuronal ensembles but the model works robustly with more HVC<sub>(RA)</sub> ensembles as well (see <xref ref-type="supplementary-material" rid="pcbi.1002303.s002">Figure S1</xref>). A real bird brain has many more HVC<sub>(RA)</sub> ensembles but here we are interested in presenting a general mechanism for which a small selection of HVC and RA ensembles is sufficient. See the third level dynamics in <xref ref-type="fig" rid="pcbi-1002303-g005">Figure 5A</xref> for typical dynamics generated by this system.</p>
        <fig id="pcbi-1002303-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002303.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Generated dynamics for the first simulation ‘Ideal communication’.</title>
            <p>The causal states are shown on the left and hidden states on the right with arbitrary units both in time and neuronal activation. There are three levels: <bold>A</bold>) HVC (third) level, Eq. (1), <bold>B</bold>) RA (second) level, Eq. (3) and <bold>C</bold>) Oscillator (first) level, Eq. (5) in <xref ref-type="supplementary-material" rid="pcbi.1002303.s006">Text S1</xref>. At the third level, there are eight HVC ensembles (each represented with a different color) which are activated for a short amount of time to control the dynamics of the five RA ensembles, see also <xref ref-type="fig" rid="pcbi-1002303-g002">Figure 2</xref>. At the second level (left column), the solid lines represent <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e035" xlink:type="simple"/></inline-formula> and dashed lines represent <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e036" xlink:type="simple"/></inline-formula>, see Eq. (3). At the first level (right), we only show <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e037" xlink:type="simple"/></inline-formula> since <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e038" xlink:type="simple"/></inline-formula> is a shifted version of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e039" xlink:type="simple"/></inline-formula>, see Eq. (5) in <xref ref-type="supplementary-material" rid="pcbi.1002303.s006">Text S1</xref>. At the first level (left), the blue line is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e040" xlink:type="simple"/></inline-formula> and the red line is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e041" xlink:type="simple"/></inline-formula> (which are mostly overlapping because of phase-locking). These output dynamics control the syrinx to obtain synthetic birdsong (<xref ref-type="fig" rid="pcbi-1002303-g006">Figure 6</xref>).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.g005" xlink:type="simple"/>
        </fig>
        <p>We control the dynamics of RA ensembles by letting the <italic>k</italic>th HVC<sub>(RA)</sub> ensemble send a signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e042" xlink:type="simple"/></inline-formula> to the lower level during its activation time. See the next subsection for details of how this signal vector is computed. The total signal sent to the lower level by all HVC<sub>(RA)</sub> ensembles at any time is a linear combination of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e043" xlink:type="simple"/></inline-formula>'s: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e044" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e045" xlink:type="simple"/></inline-formula> is the output vector in Eq. (1). Note that for typical sequential dynamics at the HVC level, except for the transition times, only one entry in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e046" xlink:type="simple"/></inline-formula> is active (i.e., only one entry is close to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e047" xlink:type="simple"/></inline-formula>), see <xref ref-type="fig" rid="pcbi-1002303-g005">Figure 5A</xref>.</p>
      </sec>
      <sec id="s2b">
        <title>RA Attractor Dynamics</title>
        <p>Experimental findings suggest that activation of different HVC<sub>(RA)</sub> ensembles drives the activation of different <italic>combinations</italic> of RA ensembles <xref ref-type="bibr" rid="pcbi.1002303-Fee1">[25]</xref>. In the present model, we capture this by forming a network of RA neuronal ensembles whose dynamics converge to one of several attractors depending on the input from the HVC level (see <xref ref-type="fig" rid="pcbi-1002303-g002">Figure 2</xref>). This means that the RA level receives input from the HVC level and produces output which encodes the level of activity of each RA ensemble at a given time. Since we are working with continuous systems, the notion of attractors comes up naturally as the RA ensemble activity flows from one activity pattern to another one. To achieve this smooth flow between RA attractors, we have to use a nonlinear network because otherwise the RA level would simply copy the dynamics of the HVC level. Note that, similar to the HVC level, the intrinsic neuronal dynamics of the RA are not established well experimentally. In this situation, we aim at describing underlying population dynamics which give rise to the experimentally observed key features of RA dynamics <xref ref-type="bibr" rid="pcbi.1002303-Fee1">[25]</xref>. To implement these dynamics, we use a well-established type of an attractor-based network described by Hopfield <xref ref-type="bibr" rid="pcbi.1002303-Hopfield1">[29]</xref>. Hopfield networks have been mostly used as a model of associative memory where each memory item is encoded by an attractor. When such a system receives noisy sensory input, i.e. it is started at some nearby initial state, it evolves to an attractor (the memory to be retrieved) <xref ref-type="bibr" rid="pcbi.1002303-Hopfield1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Hopfield2">[36]</xref>. Here, we use this idea to encode the activities of RA ensembles by attractors. As the attractor of the network changes continuously due to driving HVC input, the activities of RA ensembles also changes such that some RA ensembles activate and some others deactivate. This gives us the spatiotemporal coding that drives the syrinx dynamics described in the next subsection. We use a Hopfield network with <italic>asymmetric</italic> connectivity matrices <xref ref-type="bibr" rid="pcbi.1002303-Chen1">[37]</xref>–<xref ref-type="bibr" rid="pcbi.1002303-Xu1">[39]</xref> given by the following equation:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e048" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e049" xlink:type="simple"/></inline-formula> is the ensemble state vector with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e050" xlink:type="simple"/></inline-formula> ensembles, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e051" xlink:type="simple"/></inline-formula> is a diagonal positive matrix which governs the rate of change of each ensemble's state, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e052" xlink:type="simple"/></inline-formula> is a synaptic connectivity matrix with entries <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e053" xlink:type="simple"/></inline-formula> denoting the strength of connection from ensemble <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e054" xlink:type="simple"/></inline-formula> to ensemble <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e055" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e056" xlink:type="simple"/></inline-formula> is the activation function which we take as <italic>tanh</italic> function applied component-wise and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e057" xlink:type="simple"/></inline-formula> is the direct input from the HVC level. This equation is similar to Eq. (1), i.e. both are continuous-time recurrent neural networks, but in Eq. (2) we have an additional input vector and different conditions on the connectivity matrix as described below. In addition, the use of the nonlinear activation function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e058" xlink:type="simple"/></inline-formula> brings more plausibility to the network, as compared to linear dynamics, since the effect of one RA ensemble to another one does not increase linearly but saturates.</p>
        <p>The input vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e059" xlink:type="simple"/></inline-formula> should be chosen such that RA ensembles get quickly attracted to a desired attractor. An attractor means that a subset of the RA ensembles are ‘active’ (taking the value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e060" xlink:type="simple"/></inline-formula>) while all other RA ensembles are inactive (taking the value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e061" xlink:type="simple"/></inline-formula>). The goal is to establish conditions for the network in Eq. (2) to have a globally asymptotically stable equilibrium point (a vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e062" xlink:type="simple"/></inline-formula> that makes the right hand side of Eq. (2) zero and attracts all the solutions regardless of the initial state). These conditions and the proper choice of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e063" xlink:type="simple"/></inline-formula> for the desired attractor have been described in <xref ref-type="bibr" rid="pcbi.1002303-Matsuoka1">[38]</xref> and <xref ref-type="bibr" rid="pcbi.1002303-Zheng1">[40]</xref> (see Theorem 1 in <xref ref-type="supplementary-material" rid="pcbi.1002303.s006">Text S1</xref>).</p>
        <p>Using this technique, we can employ a small number of RA ensembles to encode a larger number of desired attractors to control the lowest level, the motor output. Each HVC<sub>(RA)</sub> ensemble provides a different <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e064" xlink:type="simple"/></inline-formula>-vector to the RA level thereby driving the RA ensembles into a unique attractor. The application of this is that each RA level attractor will drive the motor output in a specific way thereby producing a different part of the song. We obtain the equations for the second level by combining the Hopfield network, Eq. (2), with the two output equations (state vectors) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e065" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e066" xlink:type="simple"/></inline-formula> where superscripts denote the specific level of a variable:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e067" xlink:type="simple"/><label>(3)</label></disp-formula>where the exact form of the connectivity matrices <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e068" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e069" xlink:type="simple"/></inline-formula> and the HVC input vector <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e070" xlink:type="simple"/></inline-formula> are described in <xref ref-type="supplementary-material" rid="pcbi.1002303.s006">Text S1</xref>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e071" xlink:type="simple"/></inline-formula> is a scalar and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e072" xlink:type="simple"/></inline-formula> are normally distributed noise vectors. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e073" xlink:type="simple"/></inline-formula> is the normalizing function as in Eq. (1). Note that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e074" xlink:type="simple"/></inline-formula> squeezes the entries of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e075" xlink:type="simple"/></inline-formula> into the interval <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e076" xlink:type="simple"/></inline-formula> but <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e077" xlink:type="simple"/></inline-formula> may return values smaller than 1 since more than one entry of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e078" xlink:type="simple"/></inline-formula> can be active (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e079" xlink:type="simple"/></inline-formula>) at a given time. The vectors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e080" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e081" xlink:type="simple"/></inline-formula> carry the output of the second level to the first level (oscillator level) as described in the next subsection.</p>
        <p>In the present model, we use <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e082" xlink:type="simple"/></inline-formula> (i.e., five RA ensembles, <xref ref-type="fig" rid="pcbi-1002303-g005">Figure 5B</xref>). Note that there are <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e083" xlink:type="simple"/></inline-formula> different ways to activate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e084" xlink:type="simple"/></inline-formula> RA ensembles to produce motor output. We use 7 of these 31 combinations (one occurring twice) in <xref ref-type="fig" rid="pcbi-1002303-g005">Figure 5</xref> for generation of an example song (with 8 HVC<sub>(RA)</sub> ensembles at the higher level). In the figures, we used arbitrary units for both time (x-axis) and neuronal activation (y-axis) because we consider neuronal ensembles.</p>
      </sec>
      <sec id="s2c">
        <title>Model of the Avian Vocal Organ</title>
        <p>The avian vocal organ, the <italic>syrinx</italic>, is located at the base of the trachea (windpipe) where the trachea divides into the bronchi. A set of soft tissues within the syrinx, the <italic>labia</italic>, which are similar to human vocal folds, oscillate with the airstream propelled from the air sacs. Sound waves generated from these oscillations propagate through the trachea and beak. Therefore, these sound waves are modeled as the oscillations of the labia which are produced by the vocal control signals: the air sac pressure, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e085" xlink:type="simple"/></inline-formula>, and the stiffness of the labia, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e086" xlink:type="simple"/></inline-formula>. Such a mathematical model of the vocal fold oscillations was first given by Titze <xref ref-type="bibr" rid="pcbi.1002303-Titze1">[41]</xref>, and similar oscillations were experimentally observed in the bird syrinx <xref ref-type="bibr" rid="pcbi.1002303-Larsen1">[42]</xref>. A simplified version of this model (using a polynomial approximation for the nonlinear dissipation) can be given as follows <xref ref-type="bibr" rid="pcbi.1002303-Laje1">[17]</xref>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e087" xlink:type="simple"/><label>(4)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e088" xlink:type="simple"/></inline-formula> is the position of the labia from the midpoint of the syrinx, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e089" xlink:type="simple"/></inline-formula> denotes the air sac pressure, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e090" xlink:type="simple"/></inline-formula> is the linear dissipation constant, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e091" xlink:type="simple"/></inline-formula> is the stiffness of the labia and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e092" xlink:type="simple"/></inline-formula> is a dissipation term to prevent the big amplitude oscillations when the labia meet each other or the walls of the syrinx <xref ref-type="bibr" rid="pcbi.1002303-Mindlin1">[43]</xref>. The fundamental frequency of the sound wave increases or decreases proportional to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e093" xlink:type="simple"/></inline-formula>. Note that there is a critical value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e094" xlink:type="simple"/></inline-formula> for the pressure such that if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e095" xlink:type="simple"/></inline-formula>, no phonation is produced. This region in the parameter space corresponds to the mini breaths between syllables <xref ref-type="bibr" rid="pcbi.1002303-Hartley1">[44]</xref>. Using this simple model, one can obtain accurate copies of some birdsongs such as canary <xref ref-type="bibr" rid="pcbi.1002303-Gardner1">[30]</xref>, chingolo sparrow <xref ref-type="bibr" rid="pcbi.1002303-Laje1">[17]</xref>, white-crowned sparrow <xref ref-type="bibr" rid="pcbi.1002303-Laje2">[31]</xref> and cardinal <xref ref-type="bibr" rid="pcbi.1002303-Mindlin2">[45]</xref> by choosing appropriate vocal control signals for the syrinx (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e096" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e097" xlink:type="simple"/></inline-formula>) as described next.</p>
      </sec>
      <sec id="s2d">
        <title>Vocal Control Signals</title>
        <p>Oscillators as in the present model have been widely used to model movement patterns in animals and humans. Central pattern generators are a well-known example of neural networks that are used to generate periodic motor commands such as locomotion <xref ref-type="bibr" rid="pcbi.1002303-Ijspeert1">[46]</xref>. We use the same principle here, and use five oscillators with different frequencies (one for each RA ensemble) to let the RA dynamics drive the vocal output (syrinx) mechanism, see Eq. (4). Note that it is experimentally not well established how the RA level controls the syrinx muscles; our approach is a natural extension of the phenomenological syrinx model described above <xref ref-type="bibr" rid="pcbi.1002303-Laje1">[17]</xref>. The main point here is that the oscillator level (first level) is assumed to generate mixtures of oscillations (hidden states) where the RA level activity at the supraordinate level controls which oscillations should be produced at a given time. Each RA ensemble is assumed to control the activity of a single oscillator at the level below Therefore, the spatiotemporal coding of the RA level is transformed into the oscillatory activity of the first level which generates the final <italic>p(t)</italic> and <italic>k(t)</italic> dynamics necessary to control the syrinx.</p>
        <p>As oscillators, we choose simple sine wave equations where the lowest frequency oscillator <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e098" xlink:type="simple"/></inline-formula> corresponds to the slowest-changing dynamics of the birdsong. We choose the remaining four oscillators such that their frequencies are integer multiples of this first oscillator's frequency (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e099" xlink:type="simple"/></inline-formula>): <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e100" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e101" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e102" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e103" xlink:type="simple"/></inline-formula>. Each one of these sine waves represents faster changing dynamics of the song; <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e104" xlink:type="simple"/></inline-formula> being the fastest. In this way, we can model effects in the birdsong which express themselves on different time-scales.</p>
        <p>We include these five oscillators in the present model at the first level, where each of the five ensembles at the RA level controls the amplitude of one of the oscillators (through <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e105" xlink:type="simple"/></inline-formula>, Eq. (5) in <xref ref-type="supplementary-material" rid="pcbi.1002303.s006">Text S1</xref>). The observable output is obtained by taking a linear combination of these amplitude-modulated sine waves. To drive the vocal model appropriately, we produce two outputs <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e106" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e107" xlink:type="simple"/></inline-formula> (the second output is simply a time-shifted copy of the first one), which are involved in producing air sac pressure <italic>p(t)</italic> and the stiffness of the labia <italic>k(t)</italic>. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e108" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e109" xlink:type="simple"/></inline-formula> are described in detail in <xref ref-type="supplementary-material" rid="pcbi.1002303.s006">Text S1</xref>.</p>
        <p>Laje et al. <xref ref-type="bibr" rid="pcbi.1002303-Laje2">[31]</xref> chose <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e110" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e111" xlink:type="simple"/></inline-formula> to form several ellipses in the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e112" xlink:type="simple"/></inline-formula> parameter space where each ellipse corresponds to a different syllable. However, this parameterization may not support complicated syllables which have more fluctuations on the sonogram. Here, we extend their model to increase the complexity of the generated songs by using the linear combination of different frequency sine waves (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e113" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e114" xlink:type="simple"/></inline-formula> described above) to parameterize these two functions and obtain a variety of ellipse-like curves in the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e115" xlink:type="simple"/></inline-formula> parameter space (see <xref ref-type="fig" rid="pcbi-1002303-g003">Figure 3</xref>):<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e116" xlink:type="simple"/></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e117" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e118" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e119" xlink:type="simple"/></inline-formula> are the outputs of the first level and the scalars are given in <xref ref-type="table" rid="pcbi-1002303-t001">Table 1</xref>. These ellipse-like curves can be plugged into Eq. (4) to obtain synthetic birdsongs. See <xref ref-type="fig" rid="pcbi-1002303-g006">Figure 6</xref> for the sonogram obtained using the first level output of the generation process shown in <xref ref-type="fig" rid="pcbi-1002303-g005">Figure 5C</xref>. The sonogram can be played and is reminiscent of a birdsong (<xref ref-type="supplementary-material" rid="pcbi.1002303.s001">Audio S1</xref>). Note that in the real system, longer HVC<sub>(RA)</sub> sequences would be required to produce a song with 6.5 seconds duration since HVC<sub>(RA)</sub> bursts last only about 6–10 ms <xref ref-type="bibr" rid="pcbi.1002303-Fee1">[25]</xref>. Here, we assume that each HVC<sub>(RA)</sub> ensemble in the model is a collection of at least 80 HVC<sub>(RA)</sub> neurons that fires sequentially and controls the timing of the song for about 800 ms.</p>
        <fig id="pcbi-1002303-g006" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002303.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>The sound wave and sonogram of a generated song.</title>
            <p>We plugged the air sac pressure (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e120" xlink:type="simple"/></inline-formula>) and stiffness (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e121" xlink:type="simple"/></inline-formula>) parameters obtained from the first level output of the generative model (<xref ref-type="fig" rid="pcbi-1002303-g005">Figure 5C</xref>) into the syrinx equations (4). <bold>A</bold>) The solution of the syrinx equations, i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e122" xlink:type="simple"/></inline-formula> in Eq. (4), arbitrary units. The mini-breaths where no phonation is produced can be clearly observed. <bold>B</bold>) The sonogram of the soundwave in A (time (sec) vs. frequency (Hz)) is given with a sampling frequency of 12000 Hz. The first ∼3 sec of this sonogram can also be viewed in separate chunks in <xref ref-type="fig" rid="pcbi-1002303-g003">Figure 3</xref>.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.g006" xlink:type="simple"/>
        </fig>
        <table-wrap id="pcbi-1002303-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1002303.t001</object-id><label>Table 1</label><caption>
            <title>Variables used in the generative and recognition models.</title>
          </caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1002303-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.t001" xlink:type="simple"/><table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <tbody>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e123" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="2" rowspan="1">Hidden states, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e124" xlink:type="simple"/></inline-formula>,<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e125" xlink:type="simple"/></inline-formula> and causal states, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e126" xlink:type="simple"/></inline-formula>,<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e127" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e128" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="2" rowspan="1">Normally distributed noise vectors at the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e129" xlink:type="simple"/></inline-formula>th level</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e130" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="2" rowspan="1">Sigmoid (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e131" xlink:type="simple"/></inline-formula>) and normalizing (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e132" xlink:type="simple"/></inline-formula>) functions</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e133" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="2" rowspan="1">Rate constants: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e134" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e135" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e136" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e137" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="2" rowspan="1">Decay rate: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e138" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e139" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="2" rowspan="1">Connectivity matrix of the HVC level</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e140" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="2" rowspan="1">Diagonal matrix with diagonal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e141" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e142" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="2" rowspan="1">Connectivity matrix of the RA level</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e143" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="2" rowspan="1">Direct input from the HVC to the RA level</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e144" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="2" rowspan="1">Number of HVC (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e145" xlink:type="simple"/></inline-formula>) and RA (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e146" xlink:type="simple"/></inline-formula>) ensembles</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e147" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="2" rowspan="1">Angular frequencies: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e148" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e149" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e150" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="2" rowspan="1">Phase-shift: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e151" xlink:type="simple"/></inline-formula> time units.</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e152" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="2" rowspan="1">Syrinx control parameters:</td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e153" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e154" xlink:type="simple"/></inline-formula>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e155" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e156" xlink:type="simple"/></inline-formula>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e157" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e158" xlink:type="simple"/></inline-formula>
                </td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e159" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="2" rowspan="1">Covariance matrices for the noise in generation: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e160" xlink:type="simple"/></inline-formula></td>
              </tr>
              <tr>
                <td align="left" colspan="1" rowspan="1"/>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e161" xlink:type="simple"/></inline-formula>
                </td>
                <td align="left" colspan="1" rowspan="1">
                  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e162" xlink:type="simple"/></inline-formula>
                </td>
              </tr>
            </tbody>
          </table></alternatives><table-wrap-foot>
            <fn id="nt101">
              <label/>
              <p>This table lists the variables of the equations shown in <xref ref-type="fig" rid="pcbi-1002303-g004">Figure 4</xref> and Eqs. 1 to 5.</p>
            </fn>
          </table-wrap-foot></table-wrap>
      </sec>
      <sec id="s2e">
        <title>Online Bayesian Recognition</title>
        <p>In this subsection, we will briefly describe the present recognition scheme for the generated songs. This scheme is a model of vocal communication between conspecific birds but may also serve as a functional model to explain experimental findings along the auditory processing pathway which is less understood than the song pathway. Here, we describe a potential mapping of this Bayesian inference framework to neuronal dynamics at a population level, see <xref ref-type="bibr" rid="pcbi.1002303-Friston2">[47]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Friston3">[48]</xref>. The inference is based on hierarchical message passing and implements a predictive coding scheme for dynamics. As summarized below, all the update equations of the recognition system (to reconstruct the hidden states) consist of differential equations (as in the generation model) and therefore may be implemented by neuronal populations and their network interactions via forward, backward and lateral connections <xref ref-type="bibr" rid="pcbi.1002303-Friston2">[47]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Friston3">[48]</xref>.</p>
        <p>How can a bird recognize a conspecific's song and decode the information contained in the song? This decoding is important as it is known that female birds select their mates according to criteria such as the complexity of the male's repertoire <xref ref-type="bibr" rid="pcbi.1002303-Hasselquist1">[7]</xref> or the precision of the vocal performance <xref ref-type="bibr" rid="pcbi.1002303-Ballentine1">[8]</xref> and they show preference for the songs of their mates or fathers compared to the songs of strangers <xref ref-type="bibr" rid="pcbi.1002303-OLochlen1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-OLoghlen1">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Riebel1">[50]</xref>. In general, this suggests that listening birds may have certain expectations (priors) about the type of the song they expect to hear. In general, we assume that listening birds have internal models for the songs they have learned before and the generative model of the heard songs should fit to this internal model.</p>
        <p>Using this concept, we model optimal recognition using Bayesian inference for hierarchical, nonlinear dynamical systems <xref ref-type="bibr" rid="pcbi.1002303-Friston2">[47]</xref>.</p>
        <p>For the sensory input, we assume that the vocal control signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e163" xlink:type="simple"/></inline-formula>, given the sound wave, can be readily extracted by the listening bird (agent) from the spectrotemporal dynamics, see <xref ref-type="fig" rid="pcbi-1002303-g003">Figure 3</xref>. Here, we consider the <italic>p(t)</italic> and <italic>k(t)</italic> dynamics, in the recognition step, as an abstract representation of the song spectrum and therefore a phenomenological approximation to the highly nonlinear features of the singing bird's syrinx. This means that we assume that the listening bird has access to these dynamics via some low-level recognition process. For the present implementation of the inference framework, the full inference from the soundwave (<xref ref-type="fig" rid="pcbi-1002303-g007">Figure 7</xref>) would currently be computationally too expensive because this would require a high temporal resolution, e.g. at 12 kHz, and long time-series. However, once an optimized (parallel) implementation of the present framework becomes available, the present model can be extended in a straightforward fashion to model recognition that receives a soundwave as sensory input by adding another level that transformed the <italic>p(t)</italic> and <italic>k(t)</italic> dynamics to soundwaves.</p>
        <fig id="pcbi-1002303-g007" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002303.g007</object-id>
          <label>Figure 7</label>
          <caption>
            <title>First simulation ‘Ideal communication’: The dynamics of song generation (left two columns) and song recognition (right two columns) with arbitrary units.</title>
            <p>The format and the generated dynamics are the same as shown in <xref ref-type="fig" rid="pcbi-1002303-g005">Figure 5</xref>. The recognition scheme receives only the output of the first level (bottom left) and reconstructs states at all levels using the online Bayesian inference scheme. It can be seen that the reconstruction is successful as there are only tiny deviations between the true (left) and the reconstructed (right) dynamics.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.g007" xlink:type="simple"/>
        </fig>
        <p>Given this vocal control signal, we infer the spatiotemporal RA dynamics and the sequential HVC<sub>(RA)</sub> dynamics. The proposed Bayesian inference scheme provides, under some assumptions, optimal inference to decode the RA and HVC<sub>(RA)</sub> dynamics, i.e. to recognize the hidden messages embedded into the vocal control signal.</p>
        <p>The mathematical description is provided below and can be conceptualized as follows: At each time step <italic>t</italic>, the recognition system receives sensory input, here the current amplitudes of the <italic>p(t)</italic> and <italic>k(t)</italic> dynamics. Like the generative model, the recognition system has three levels as well. Each of these three levels consists of interacting neuronal populations, which encode predictions, i.e. expectations, about how their internal dynamics will evolve during a song. At the same time, each level receives input from the subordinate level. For the first level, this is the sensory input, which is compared with the internal prediction. The prediction error is forwarded to the second level, where again predictions are used to generate prediction errors, which are forwarded to the third level. Critically, each level adjusts its internal predictions to minimize its prediction error weighted by the prior precision of the internal prediction. At each level, the updated predictions are sent to the subordinate levels to guide their internal predictions by higher level predictions. In summary, each level minimizes its prediction error by a fusion of internal dynamics with top-down (predictions) and bottom-up (prediction error) messages. The overall result is that a listening bird fuses its dynamic and hierarchically arranged expectations about a song with the actual sensory input. Importantly, due to this dynamic fusion, the recognition is robust against deviations from its expectations by explaining away errors of the singing bird by internal precision-weighted prediction error. The derivation of the update equations to achieve Bayes-optimal online recognition solutions is non-trivial, see Friston et al. <xref ref-type="bibr" rid="pcbi.1002303-Friston1">[11]</xref>. Note that this modeling approach implies that generation and recognition models are fundamentally different from each other in the sense that generation is a top-down process where recognition consists of both top-down and bottom-up processes. Although some of the computations in the generation and recognition model are the same and may provide a computational explanation for mirror neuron accounts <xref ref-type="bibr" rid="pcbi.1002303-Kilner1">[51]</xref>, this is not a central issue in the present paper and we assume here that recognition is performed by neuronal populations different from those that generated the song. Clearly, this remains an open question that can only be settled experimentally.</p>
        <p>For sensory input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e164" xlink:type="simple"/></inline-formula> and a given model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e165" xlink:type="simple"/></inline-formula>, the probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e166" xlink:type="simple"/></inline-formula> is called the <italic>model evidence</italic> or <italic>marginal likelihood of</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e167" xlink:type="simple"/></inline-formula> and is an important quantity for model comparison among different models. In our case, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e168" xlink:type="simple"/></inline-formula> is the vocal control signal for the syrinx which we take as the input and the model <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e169" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1002303-g004">Figure 4</xref>) includes all the parameters and equations together with causal and hidden states at all levels. We take <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e170" xlink:type="simple"/></inline-formula> to be the set of all hidden states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e171" xlink:type="simple"/></inline-formula> and causal states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e172" xlink:type="simple"/></inline-formula> at all levels of hierarchy. The task for the agent is to infer the states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e173" xlink:type="simple"/></inline-formula> from the sensory input under model <italic>m</italic>. We assume that the parameters (such as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e174" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e175" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e176" xlink:type="simple"/></inline-formula>, see <xref ref-type="fig" rid="pcbi-1002303-g004">Figure 4</xref>) have been learned previously by the listening bird and are fixed (<xref ref-type="table" rid="pcbi-1002303-t001">Table 1</xref>).</p>
        <p>Our goal is to approximate the <italic>posterior density</italic> <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e177" xlink:type="simple"/></inline-formula> which will give us both the posterior mean of the dynamical states and the uncertainty about this mean. To get a good approximation for the posterior density, we follow a rather indirect way using the marginal likelihood.</p>
        <p>The marginal likelihood of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e178" xlink:type="simple"/></inline-formula> can be written as  <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e179" xlink:type="simple"/></inline-formula>. Here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e180" xlink:type="simple"/></inline-formula> is defined in terms of the likelihood <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e181" xlink:type="simple"/></inline-formula> and the prior <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e182" xlink:type="simple"/></inline-formula>. Except for a few analytical cases, this integral is usually intractable and needs to be approximated. One way for this approximation is to introduce a <italic>free-energy</italic> term which is a lower bound for the marginal likelihood. It is not hard to show that:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e183" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e184" xlink:type="simple"/></inline-formula> is the free-energy, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e185" xlink:type="simple"/></inline-formula> is the Kullback-Leibler divergence and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e186" xlink:type="simple"/></inline-formula> is the <italic>recognition density</italic>. Note that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e187" xlink:type="simple"/></inline-formula> is an auxiliary function that we will use to approximate the posterior density. It is easy to show that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e188" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e189" xlink:type="simple"/></inline-formula> if and only if <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e190" xlink:type="simple"/></inline-formula>. This means <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e191" xlink:type="simple"/></inline-formula> is a lower bound for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e192" xlink:type="simple"/></inline-formula>, and if we can maximize <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e193" xlink:type="simple"/></inline-formula>, this will minimize <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e194" xlink:type="simple"/></inline-formula> giving an approximation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e195" xlink:type="simple"/></inline-formula> for the posterior density.</p>
        <p>To maximize <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e196" xlink:type="simple"/></inline-formula> with respect to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e197" xlink:type="simple"/></inline-formula>, we make the assumption of normally distributed error terms and write <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e198" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e199" xlink:type="simple"/></inline-formula> consists of the mode <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e200" xlink:type="simple"/></inline-formula> and the variance <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e201" xlink:type="simple"/></inline-formula>. Then the problem turns to a maximization problem of the free energy with respect to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e202" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e203" xlink:type="simple"/></disp-formula>which gives the approximation for the posterior density <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e204" xlink:type="simple"/></inline-formula>. For the details of this variational process and its extension to time-dependent states, see <xref ref-type="bibr" rid="pcbi.1002303-Friston1">[11]</xref>.</p>
        <p>Since we apply the variational scheme in a hierarchical setting, we write the equations in our model (see <xref ref-type="fig" rid="pcbi-1002303-g004">Figure 4</xref>) in a generic hierarchical form <xref ref-type="bibr" rid="pcbi.1002303-Friston1">[11]</xref>. We use the same set of equations as in the generative model since we assumed the singing and listening birds have the same internal models. We denote <italic>all</italic> hidden and causal states at level <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e205" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e206" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e207" xlink:type="simple"/></inline-formula>, respectively. In particular, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e208" xlink:type="simple"/></inline-formula> stands for all the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e209" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e210" xlink:type="simple"/></inline-formula> outputs of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e211" xlink:type="simple"/></inline-formula> th level. We also write <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e212" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e213" xlink:type="simple"/></inline-formula> to describe the dynamics of the hidden and causal states in the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e214" xlink:type="simple"/></inline-formula> th level:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e215" xlink:type="simple"/></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e216" xlink:type="simple"/></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e217" xlink:type="simple"/></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e218" xlink:type="simple"/></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e219" xlink:type="simple"/></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e220" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e221" xlink:type="simple"/></inline-formula> denotes the normally distributed fluctuations at the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e222" xlink:type="simple"/></inline-formula> th level. The present model shown in <xref ref-type="fig" rid="pcbi-1002303-g004">Figure 4</xref> follows this generic form. The causal states (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e223" xlink:type="simple"/></inline-formula>) provide input to the subordinate level while the hidden states (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e224" xlink:type="simple"/></inline-formula>) are intrinsic to each level.</p>
        <p>Note that the Gaussian fluctuations <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e225" xlink:type="simple"/></inline-formula> in the above hierarchical form quantify different amounts of noise at each level of the singing bird. We list the covariance matrices used in the “Ideal Communication” simulation in <xref ref-type="table" rid="pcbi-1002303-t001">Table 1</xref>. Note that sensory input <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e226" xlink:type="simple"/></inline-formula> enters the recognition system at the first level: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e227" xlink:type="simple"/></inline-formula>. The optimization process of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e228" xlink:type="simple"/></inline-formula> (i.e. the estimated mode of causal and hidden states) can be implemented in a message passing scheme <xref ref-type="bibr" rid="pcbi.1002303-Friston1">[11]</xref> which involves passing predictions down and passing prediction errors up from one level to another. Prediction errors can be written as<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e229" xlink:type="simple"/></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e230" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e231" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e232" xlink:type="simple"/></inline-formula> denote the predictions from level above for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e233" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e234" xlink:type="simple"/></inline-formula>, respectively. In this scheme, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e235" xlink:type="simple"/></inline-formula> is optimized through gradient descent on prediction errors at each level of the hierarchy. Importantly, the computations required for this gradient descent could be implemented by interacting neuronal populations at each level: Each population comprises causal and hidden <italic>state-units</italic> that encode the expected states and the <italic>error-units</italic>, with one matching error-unit for each state-unit, which encode the prediction errors. The estimated mode of the states, i.e. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e236" xlink:type="simple"/></inline-formula>, is described by the activity of the state-units. The error units compare the estimated modes with predictions sent via backward and lateral connections and compute prediction errors, which are passed on via forward and lateral connections. This message passing has been shown to minimize precision-weighted prediction errors and optimize predictions at all levels efficiently (see <xref ref-type="bibr" rid="pcbi.1002303-Friston2">[47]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Mumford1">[52]</xref> for further details).</p>
        <p><italic>Software Note:</italic> The routines (including commented Matlab source code) implementing this dynamic inversion, which were also used for the simulations in this paper, are available as academic freeware (Statistical Parametric Mapping package (SPM8) from <ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm/" xlink:type="simple">http://www.fil.ion.ucl.ac.uk/spm/</ext-link>; Dynamic Expectation Maximization (DEM) Toolbox).</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Results</title>
      <p>To illustrate the behavior of the described generation and recognition schemes, we exposed our recognition model to four different tasks. Since the neuronal structures for song generation and song recognition are mostly different (see <xref ref-type="sec" rid="s4">Discussion</xref>), we refer in the following to the levels in both the generation and recognition models as the first, second and third levels instead of ‘Oscillator’, ‘RA’ and ‘HVC’ levels, respectively.</p>
      <p>We first show the case of ‘ideal communication’, i.e. the recognition scheme described above can appropriately infer about the states at all three levels from sensory input that describes a veridical song. In a second simulation, we show the case when the sensory input is not as expected, i.e. when, for the listening bird, there is an unexpected deviation in the song (a single syllable). We will demonstrate how the listening bird detects this deviation and what neuronal correlates are observed in presence of this deviation. In the third simulation, we show that the recognition mechanism is robust against differences in the anatomical connectivity pattern in the second layer. This robustness is a consequence of the hierarchical setup of the generative model. This is an important finding because it explains how different birds can decode the same song although their individual anatomical connectivity within some layers may differ. In our final simulation, we replicate the experimental findings of a study <xref ref-type="bibr" rid="pcbi.1002303-Long2">[53]</xref> where the authors cooled HVC and observed that the song slowed down. We also show how the listening bird (e.g., female bird in a social context) can detect the minor deviations due to a speed change of the song.</p>
      <sec id="s3a">
        <title>Ideal Communication</title>
        <p>Here, we simulate the ideal situation in which both the ‘singing bird’ and the ‘listening bird’ have learned how exactly a song should sound. As before, we use eight third level ensembles that are each activated sequentially and, during this time, they control the activities of five second level ensembles (<xref ref-type="fig" rid="pcbi-1002303-g002">Figure 2</xref>). The third level imposes a sequence of attractors on the second level which in turn produce linear combinations of appropriate sine waves to produce the air sac pressure and labia stiffness, see <xref ref-type="fig" rid="pcbi-1002303-g004">Figure 4</xref>. To introduce noise (both internal state noise for the singing bird, and also transmission noise to the listening bird), we used normally distributed zero-mean noise with standard deviation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e237" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e238" xlink:type="simple"/></inline-formula> at all levels. To show that recognition is robust against starting condition (i.e. the state of the ongoing neuronal activity within the bird brain at song onset), the initial states of the recognition are chosen differently from the true initial values used in the generation. As expected, we find that the listening bird starts tracking the sensory input very quickly and follows it robustly during the remainder of the song, see <xref ref-type="fig" rid="pcbi-1002303-g007">Figure 7</xref>.</p>
      </sec>
      <sec id="s3b">
        <title>Deviation from Expected Song</title>
        <p>Next, we show what happens if the listening bird has a different expectation than the singing bird about how a song should sound. In the generative model (singing bird), we use the same third level ensembles and the corresponding second level combinations that we used in the ‘Ideal Communication’ case (<xref ref-type="fig" rid="pcbi-1002303-g002">Figure 2</xref>). However, the recognition system (listening bird) knows a slightly different song where there is a deviation in a single syllable. We model this by changing the effect of the third ensemble at the third level such that it activates only the first ensemble at the second level (instead of the first and fourth as in the singing bird). This means that the motor output and the sonogram look different from the prior expectation of the listening bird but only for the third syllable, see <xref ref-type="fig" rid="pcbi-1002303-g008">Figure 8</xref>. The internal recognition dynamics of the listening bird register this deviation and show two effects during the third syllable, between time points <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e239" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e240" xlink:type="simple"/></inline-formula>: (i) Prediction errors in the recognition are distributed throughout all three levels and are not only explained by changes at a single level (<xref ref-type="fig" rid="pcbi-1002303-g009">Figure 9</xref>). This makes sense since the observed deviation at the first level cannot be explained by the simple oscillatory first level dynamics. Rather, the recognition attempts to explain away the deviation at the first level by using prediction error at the second and third level as well. At the first level, this is quite successful because the recognized dynamics look very similar to the generated dynamics (see <xref ref-type="fig" rid="pcbi-1002303-g008">Figure 8</xref>, bottom row). However, at higher levels, there are obvious differences between the generated and recognized dynamics, i.e. the listening bird can infer a deviation via the prediction error at the second and third levels. (ii) When the deviation has finished, the recognition quickly locks back onto the ongoing song dynamics at all three levels and decodes the song veridically. In summary, this simulation shows that the dynamic recognition hierarchy uses all its levels to compensate for unexpected deviations in the song. This means that all levels of the hierarchy work together in concert to minimize the effects of deviations throughout the hierarchy. In other words, the activity of high-level auditory processing levels in songbirds in response to small deviations in the expected song may be most revealing for their function. This mechanism may be important in social context since the listening bird can recognize subtle variations in the singing bird by its activity in high-level areas and grade the singing bird's overall performance <xref ref-type="bibr" rid="pcbi.1002303-Nowicki1">[54]</xref>.</p>
        <fig id="pcbi-1002303-g008" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002303.g008</object-id>
          <label>Figure 8</label>
          <caption>
            <title>Second simulation ‘Deviation from expected song’: For simplicity, we only show the causal states of the generation (left column) and recognition (right column), where the format is the same as in <xref ref-type="fig" rid="pcbi-1002303-g005"><bold>Figure 5</bold></xref> with arbitrary units.</title>
            <p>The listening bird (recognition) hears a slightly deviating syllable between the time steps <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e241" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e242" xlink:type="simple"/></inline-formula> indicated by the black rectangle. During this period, the third ensemble of the HVC level (red color) in the singing bird (generation) activates the first and the fourth ensembles of the RA level (blue and cyan colors) while the listening bird expects the activation of the first ensemble of the second level (blue) only. This unexpected sensory input continues until the listening bird starts hearing and recognizing the expected syllables again after <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e243" xlink:type="simple"/></inline-formula>. See <xref ref-type="fig" rid="pcbi-1002303-g009">Figure 9</xref> for plots of the associated prediction errors.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.g008" xlink:type="simple"/>
        </fig>
        <fig id="pcbi-1002303-g009" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002303.g009</object-id>
          <label>Figure 9</label>
          <caption>
            <title>Prediction errors for the ‘Deviation from expected song’ simulation with arbitrary units.</title>
            <p>Prediction errors for all casual and hidden states during recognition are plotted using the same format as in <xref ref-type="fig" rid="pcbi-1002303-g005">Figure 5</xref>. Note that prediction errors increase during the unknown syllable (between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e244" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e245" xlink:type="simple"/></inline-formula>) and are observed at all levels.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.g009" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s3c">
        <title>Differently Wired Brains: Communication within Species</title>
        <p>Considering the anatomical complexity of the brain, genetic and developmental variability is expected in the brains of individuals of the same species. At the macro scale, the general connectivity structure of distinct brain regions may be shared, but at the micro scale, variability is found in size, location and connections between individual neurons or neuronal ensembles <xref ref-type="bibr" rid="pcbi.1002303-Ward1">[55]</xref>–<xref ref-type="bibr" rid="pcbi.1002303-Airey1">[58]</xref>. Here, we simulate a difference in the connectivity structures by using different second-level connectivity matrices <italic>W</italic> (<xref ref-type="fig" rid="pcbi-1002303-g004">Figure 4</xref> and Eq. (3)) in the generative model of the singing bird and the recognition system of the listening bird. In other words, the listening bird has a different internal model at the second level as would be prescribed by the generative model of the singing bird at the RA level. How can birds with individual variability in their internal models still extract the same information from a song?</p>
        <p>The answer is that differences in the second-level connectivity matrix <italic>W</italic> can be compensated by a different driving activity <italic>I</italic> from the third level since <italic>I</italic> depends on <italic>W</italic> (see Theorem 1 in <xref ref-type="supplementary-material" rid="pcbi.1002303.s006">Text S1</xref>). In our simulation, we assume that these driving activities have already been learned in the corresponding birds, e.g. during juvenility. As shown in <xref ref-type="fig" rid="pcbi-1002303-g010">Figure 10</xref>, the states at all three levels can be recognized successfully even though the second levels in the two birds are wired differently. This means that the internal models of generation and recognition do not have to be the same but can cope with structural variations due to anatomical variability at the micro-scale. Critically, this compensation of anatomical variability at the second level relies on the hierarchical configuration and learning of the connectivity from the third level to second level.</p>
        <fig id="pcbi-1002303-g010" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002303.g010</object-id>
          <label>Figure 10</label>
          <caption>
            <title>Third simulation ‘Differently wired brains’: For simplicity, we only show the causal states of the generation (left column) and recognition (right column), where the format is the same as in <xref ref-type="fig" rid="pcbi-1002303-g005"><bold>Figure 5</bold></xref> with arbitrary units.</title>
            <p>The connectivity matrices (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e246" xlink:type="simple"/></inline-formula>'s) at the second level are different in the singing bird (generation) and in the listening bird (recognition). Recognition still works as well as in the first simulation (‘Ideal communication’, <xref ref-type="fig" rid="pcbi-1002303-g007">Figure 7</xref>) because third level ensembles can compensate for this variability by providing different input to the second level (different <italic>I</italic> vectors).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.g010" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s3d">
        <title>Cooling of HVC</title>
        <p>In song generation, a critical question is which regions of the brain are involved in the timing of syllables or sub-syllable structures. A recent study tackled this question by manipulating the temperature of the HVC and RA regions in the singing bird <xref ref-type="bibr" rid="pcbi.1002303-Long2">[53]</xref>. Importantly, it was shown that song speed at all time scales slowed down but the acoustic structure stayed the same as the temperature of HVC dropped. In the sonogram, this corresponds to a temporal stretching of the song. Conversely, cooling of RA did not have any effect on the timing of the song. This suggests that HVC is involved in the control of the timing of the song <xref ref-type="bibr" rid="pcbi.1002303-Long2">[53]</xref>.</p>
        <p>We observed similar behavior in our model where we modeled the cooling by manipulating the rate (i.e. speed) constants <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e247" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e248" xlink:type="simple"/></inline-formula> at the three levels. Importantly, changing the rate constant for HVC slows down the song but changing the rate constant for RA does not. In the first simulation (<xref ref-type="fig" rid="pcbi-1002303-g011">Figure 11</xref>, left), we ‘cooled’ HVC by changing <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e249" xlink:type="simple"/></inline-formula> from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e250" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e251" xlink:type="simple"/></inline-formula>. This slows down the dynamics of the HVC level and immediately slows down the RA level as well since the control signals coming from HVC now last twice as long. In other words, we find as in the cooling experiment that HVC, due to its position at the top of the hierarchy, controls directly the timing of the song. To reflect this slowing down in the output we also changed <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e252" xlink:type="simple"/></inline-formula> from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e253" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e254" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e255" xlink:type="simple"/></inline-formula> is kept constant in all simulations) to adjust the frequencies which were chosen independently from the RA level for simplicity (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e256" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e257" xlink:type="simple"/></inline-formula>). In the second simulation (<xref ref-type="fig" rid="pcbi-1002303-g011">Figure 11</xref>, right), we changed the rate constant of RA, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e258" xlink:type="simple"/></inline-formula>, from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e259" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e260" xlink:type="simple"/></inline-formula>. This has no observable effect, as in the experiment <xref ref-type="bibr" rid="pcbi.1002303-Long2">[53]</xref>, on the dynamics of RA ensembles since the timing of attractor activations is controlled by the timing of HVC. A change in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e261" xlink:type="simple"/></inline-formula> only slows down the transition times which has no detectable effect in the output.</p>
        <fig id="pcbi-1002303-g011" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002303.g011</object-id>
          <label>Figure 11</label>
          <caption>
            <title>Generated dynamics for the fourth simulation ‘Cooling of HVC’: We simulated two cooling experiments, where the format is the same as in <xref ref-type="fig" rid="pcbi-1002303-g005"><bold>Figure 5</bold></xref> with arbitrary units.</title>
            <p>Left: The rate constant at the HVC (third) level, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e262" xlink:type="simple"/></inline-formula>, is decreased by half. Right: The rate constant of the RA level (second level), <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e263" xlink:type="simple"/></inline-formula>, is decreased by half. The change in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e264" xlink:type="simple"/></inline-formula> slows down the dynamics of the system, while cooling at the RA level does not have any significant effect, compare with the dynamics in <xref ref-type="fig" rid="pcbi-1002303-g005">Figure 5</xref>. The parameters used are <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e265" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e266" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e267" xlink:type="simple"/></inline-formula> on the left and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e268" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e269" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e270" xlink:type="simple"/></inline-formula> on the right.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.g011" xlink:type="simple"/>
        </fig>
        <p>Speed changes may not only have an experimentally observable effect in the generated song but also in the listening bird. Interestingly, speech changes in song also occur under natural conditions, e.g. in a social context: Male birds sing slightly faster when addressing a female bird (directed song) compared to singing towards other males or when alone (undirected song) <xref ref-type="bibr" rid="pcbi.1002303-Jarvis1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Sossinka1">[59]</xref>. Using the present model, we tested whether the listening bird can detect such small changes in the singing bird during directed song. We slowed down the song by 3%, thereby modeling an undirected song, and analyzed the prediction errors in the listening bird which expected the slightly faster, directed version. The listening bird was able to recognize the song successfully but it also reliably distinguished the subtle change in the tempo, as can be seen from the sustained prediction errors at all three levels (<xref ref-type="fig" rid="pcbi-1002303-g012">Figure 12</xref>).</p>
        <fig id="pcbi-1002303-g012" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002303.g012</object-id>
          <label>Figure 12</label>
          <caption>
            <title>Recognition results for the fourth simulation ‘Cooling of HVC’, where the format is the same as in <xref ref-type="fig" rid="pcbi-1002303-g005"><bold>Figure 5</bold></xref> with arbitrary units.</title>
            <p>Left: We slowed down the singing bird by decreasing the rate constants by 3%: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e271" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e272" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e273" xlink:type="simple"/></inline-formula>. Middle: The rate constants for the recognition are <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e274" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e275" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e276" xlink:type="simple"/></inline-formula>. Right: The listening bird can distinguish this subtle change in song speed as can be seen from the prediction errors of the causal states at all three levels. (The hidden states show similar prediction errors at all levels).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.g012" xlink:type="simple"/>
        </fig>
      </sec>
    </sec>
    <sec id="s4">
      <title>Discussion</title>
      <p>We have described a hierarchical model for generating birdsongs and introduced an online Bayesian inversion as a recognition model. The key result is that the specific anatomical, functional and hierarchical structure of birdsong generation enables Bayesian online decoding of hidden information at a slow time-scale at the HVC and RA levels. Four simulations showed that the Bayesian recognition mechanism works efficiently in several settings and its functional behavior might be helpful to understand the mechanisms of birdsong recognition. In addition, recognition is robust to noise and can be performed online. Overall, this is a unified modeling approach which handles both generation and recognition of birdsong and may serve as a model for vocal bird communication.</p>
      <p>Both generation and recognition models extend previous modeling work either by using novel techniques (e.g. Bayesian inference for hierarchical, stochastic, nonlinear dynamical systems) or by combining well-known nonlinear differential equation systems in a novel way (generative model). The model explains recognition of birdsong as continuous message passing scheme among auditory areas and explains the dynamic song recognition system of birds using Bayesian techniques. In the generation model, we combine a well-established syringeal model with the sequential HVC/RA model and describe a hierarchical and dynamical mechanism which transforms the spatiotemporal coding at the RA level into the rich, complex structure of the song power spectrum. Based on this generative model, we use Bayesian inference to model song recognition by a conspecific. This modeling strategy is a novel approach to employ experimental findings in birdsong generation for establishing a functional model of birdsong recognition. In fact, decoding of sensory input generated by hierarchical, nonlinear dynamical systems is usually technically challenging and often impossible <xref ref-type="bibr" rid="pcbi.1002303-Budhiraja1">[60]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Judd1">[61]</xref> because the sensory input may not be informative about hidden information at higher levels. However, here we found that the decoding of birdsong using hierarchical Bayesian inference based on a song generation model is feasible, robust and can be performed online. Intuitively, it may be obvious that birdsong must be generated such that conspecifics can derive information (meaning) from it. The question is how birds do this mechanistically. Here, we propose that this recognition mechanism may rest on Bayes-optimal inference given the specific hierarchical arrangement of the neuronal birdsong-generating network.</p>
      <sec id="s4a">
        <title>Neurobiological Plausibility of the Recognition Model</title>
        <p>We have derived a recognition scheme using Bayesian inference. However, bird brains may have established their recognition capabilities by evolutionary processes <xref ref-type="bibr" rid="pcbi.1002303-OLochlen1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-OLoghlen1">[49]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Price1">[62]</xref>. What are the similarities between the proposed recognition scheme and the biological one?</p>
        <p>Note that the present modeling does not suggest that the areas involved in generation and recognition are the same. Many computations during recognition are different from those in generation. The present recognition scheme consists of three hierarchical levels, thereby mirroring the hierarchical generation system. We found that three hierarchical levels are also appropriate for the recognition of a song. Interestingly, experimental findings point to a hierarchical arrangement of the auditory system in songbirds as three major functional levels of processing <xref ref-type="bibr" rid="pcbi.1002303-Amador1">[63]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Theunissen1">[64]</xref> where it is partially unclear yet how this hierarchy maps exactly onto the auditory system. Moreover, note that these areas are mostly investigated for male (zebra finch) birds and it is quite possible that there could be different areas involved in females or in other bird species.</p>
        <p>Experimental evidence suggests that HVC may be located at the highest level of this recognition system. In particular, HVC<sub>(X)</sub> neurons (HVC neurons that project to Area X, see <xref ref-type="fig" rid="pcbi-1002303-g001">Figure 1</xref>) are selectively responsive to the bird's own or a conspecific's song <xref ref-type="bibr" rid="pcbi.1002303-Prather1">[65]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Prather2">[66]</xref>. The firing of HVC<sub>(X)</sub> neurons at temporally precise times during an auditory stimulus <xref ref-type="bibr" rid="pcbi.1002303-Prather1">[65]</xref> is similar to the temporally precise activation of HVC<sub>(RA)</sub> neurons during singing. This suggests that HVC<sub>(X)</sub> neurons may be involved in the representation of the expected sequence of song dynamics. In the present model, the third level encodes both the sequence prediction but also the perceived deviation from this sequence.</p>
        <p>The circuitry of areas subordinate to HVC during song recognition is not particularly well understood. The caudal mesopallium (CM) and caudomedial nidopallium (NCM) have been shown to be selective for particular familiar songs or sounds and are involved in auditory memory <xref ref-type="bibr" rid="pcbi.1002303-Bolhuis1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Amador1">[63]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Theunissen1">[64]</xref>. Similar functions are implemented by the second level of the present recognition model: The second level encodes the expectation of specific spatiotemporal patterns, i.e. it encodes auditory memory by attractors that correspond to specific vocal tract dynamics (sounds). Note that there is a clear distinction between the third and second level in the model: While the third level encodes the expected sequence of sound dynamics, the second level encodes the repertoire of song sounds (transcribed to sound waves by the vocal tract dynamics). This functional separation is also assumed to be implemented in the real bird brain <xref ref-type="bibr" rid="pcbi.1002303-Leonardo1">[26]</xref>.</p>
        <p>In the primary auditory area, Field L, spectral-temporal receptive fields (STRF) have been proposed to explain the selective responses of neurons <xref ref-type="bibr" rid="pcbi.1002303-Sen1">[67]</xref>. These selective responses may correspond to the recognition dynamics at the first level in the model which decodes the detailed spatiotemporal structure of the auditory stimulus guided by higher level predictions. It is interesting to note that we could use the present recognition model to derive, as done experimentally <xref ref-type="bibr" rid="pcbi.1002303-Sen1">[67]</xref>, the spectral-temporal receptive fields at the first level. Alternatively, one could use experimentally acquired STRFs to adapt the first level of the present model to establish exact equivalence of the model and the real system at the level of primary auditory areas.</p>
      </sec>
      <sec id="s4b">
        <title>Relation to Other Generation Models</title>
        <p>There are several models that focus on the sequential activation of HVC<sub>(RA)</sub> neurons using single neuron models. Inhibition is believed to be a key element to generate rhythmic (sequential) activity in HVC <xref ref-type="bibr" rid="pcbi.1002303-Gibb1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Fiete1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Solis1">[68]</xref>. We used winnerless competition which relies on inhibition to sequentially activate HVC<sub>(RA)</sub> ensembles. A similar generation mechanism as described here can be obtained using the synaptic chain scheme: Li and Greenside <xref ref-type="bibr" rid="pcbi.1002303-Li1">[12]</xref> proposed a conductance-based model for HVC<sub>(RA)</sub> neurons from which they obtained sequential multi-spike bursts. Later, Jin et al. <xref ref-type="bibr" rid="pcbi.1002303-Jin1">[13]</xref> used an intrinsic bursting mechanism to obtain higher firing rates more consistent with the experimental data. This scheme was extended in <xref ref-type="bibr" rid="pcbi.1002303-Long1">[14]</xref> and was shown to produce robust and highly stereotyped sequential bursts. A learning mechanism was proposed in <xref ref-type="bibr" rid="pcbi.1002303-Buonomano1">[69]</xref> showing how a sparse temporal code can emerge from a recurrent network. The models mentioned above focus on describing possible ways for the sequential activity of HVC where the downstream areas can be regarded as driven in a feed-forward fashion by HVC. A comprehensive generative model that includes HVC, RA and motor control areas was described in <xref ref-type="bibr" rid="pcbi.1002303-Abarbanel1">[18]</xref>. This study showed that the intrinsic connectivity at the RA level can substantially influence the acoustic features of syllables. This approach is similar to the present where the common research question is which parameterization (connectivity) of a recurrent neural network will generate motor control signals that result in realistic acoustic features of birdsong. However, we additionally incorporated recent findings <xref ref-type="bibr" rid="pcbi.1002303-Leonardo1">[26]</xref> which point to a specific role of RA ensembles in encoding sound wave modulations. Furthermore, we provide evidence that the hierarchical setting of HVC and RA ensembles is the basis for robust and rapid song recognition.</p>
      </sec>
      <sec id="s4c">
        <title>Relation to Other Recognition Models</title>
        <p>Theunissen et al. <xref ref-type="bibr" rid="pcbi.1002303-Theunissen2">[70]</xref> estimated spectral-temporal receptive fields (STRF) of nonlinear auditory neurons using natural sounds as sensory input. The STRFs describe which temporal succession of acoustical features would elicit the maximal neural response and provide useful information for modeling perception of acoustic features, e.g. in the primary auditory area, Field L <xref ref-type="bibr" rid="pcbi.1002303-Sen1">[67]</xref>. A two-level model was introduced <xref ref-type="bibr" rid="pcbi.1002303-Drew1">[71]</xref> where the first level encoded frequency responses identified by an STRF analysis and the second level used these features to model song selective responses of HVC neurons. In another approach, Larson et al. <xref ref-type="bibr" rid="pcbi.1002303-Larson1">[72]</xref> proposed a model for auditory object recognition where the first level uses a distance metric to distinguish between different spike trains and the second level acts as a decision network. However, both of these models propagate auditory signals in a feed-forward fashion from the low to the high level while the present scheme uses dynamical and recurrent bottom-up and top-down message passing thereby providing a more comprehensive model of the neuronal dynamics observed during song recognition.</p>
        <p>Learning models such as <xref ref-type="bibr" rid="pcbi.1002303-Doya1">[73]</xref> and <xref ref-type="bibr" rid="pcbi.1002303-Fiete2">[74]</xref> were proposed which also include birdsong production and evaluation. These models mainly focus on the neural mechanisms of learning but they also provide mechanisms for song evaluation.</p>
        <p>There have been also attempts for the automated recognition of birdsongs using machine learning methods, e.g. <xref ref-type="bibr" rid="pcbi.1002303-Anderson1">[75]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Kogan1">[76]</xref>. However, these models are not concerned with neurobiological plausibility but rather use ad-hoc techniques as used in automated speech recognition, i.e. hidden Markov models and template-based matching of song syllables.</p>
      </sec>
      <sec id="s4d">
        <title>Implications for Empirical Research</title>
        <p>There are several implications for future experiments which one can derive from the present model. The first is that we observe prediction errors at all levels when there is an unexpected piece of song (<xref ref-type="fig" rid="pcbi-1002303-g009">Figure 9</xref>) or a song which is slower than expected (<xref ref-type="fig" rid="pcbi-1002303-g012">Figure 12</xref>). This suggests that there may not be a single area in the auditory pathway (such as HVC<sub>(X)</sub> or LMAN in the anterior forebrain pathway) that acts as a comparator between the stimulus and previously memorized tutor song <xref ref-type="bibr" rid="pcbi.1002303-Mooney1">[77]</xref> but several levels of the auditory pathway may be involved in this comparison. Comparing the neuronal recordings from a bird that listens to a normal speed song and a slower version of the same song might reveal the locations where these prediction errors are computed. Similar experiments have been done in auditory areas Field L and caudal lateral mesopallium (CLM) where some neurons responded robustly to perturbations in vocalization or playback of the bird's own song <xref ref-type="bibr" rid="pcbi.1002303-Keller1">[21]</xref>. A functional model like the one presented here could predict what amount of activity should be expected in experiments given defined deviations, at different levels of the recognition hierarchy. Parallel to this idea, a recent experiment explained the activity in CLM by the level of <italic>surprise</italic> in the stimulus <xref ref-type="bibr" rid="pcbi.1002303-Gill1">[20]</xref>. Our model could be used to predict the amount of surprise or prediction error at different hierarchical levels. As the present model covers much of the auditory pathway, this prediction technique may be best suited for using functional MRI on birds <xref ref-type="bibr" rid="pcbi.1002303-Voss1">[78]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Voss2">[79]</xref> where one would model increased activation, relative to some baseline condition, as an increase in prediction error.</p>
      </sec>
      <sec id="s4e">
        <title>Potential Relationship between Birdsong and Human Speech</title>
        <p>As noted by several authors, human speech and birdsong have in common that both are complex, hierarchical, sequenced vocalizations which are repetitions and combinations of simple units such as phonemes and syllables <xref ref-type="bibr" rid="pcbi.1002303-Yu1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Glaze1">[80]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Rose1">[81]</xref>. Although human speech is far more complex than birdsong, the underlying anatomical and functional features show striking similarities such as the pathways for vocal production, auditory processing and learning <xref ref-type="bibr" rid="pcbi.1002303-Doupe1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Rose1">[81]</xref>. Songbirds, similar to humans, gain their vocal abilities early in life by listening to adults, memorizing, and practicing their songs <xref ref-type="bibr" rid="pcbi.1002303-Doupe1">[22]</xref>. These similarities suggest that one may derive insight about human speech recognition and learning from findings in birdsong research <xref ref-type="bibr" rid="pcbi.1002303-Kiebel1">[82]</xref>.</p>
        <p>The present results clearly point to the usefulness of a hierarchical recognition structure to decode sequences of syllables. Such hierarchical models are rarely used in automated speech recognition <xref ref-type="bibr" rid="pcbi.1002303-Deng1">[83]</xref> presumably because the standard model, the hidden Markov model, is mathematically best understood only in a non-hierarchical setting. The present scheme shows that complex spectral dynamics such as birdsong may be modeled as a sequence of nonlinear dynamics, where, in the generative model, each level drives the subordinate level in a highly non-linear fashion. To invert such a hierarchical, nonlinear, dynamical system, one requires sophisticated Bayesian inference machinery <xref ref-type="bibr" rid="pcbi.1002303-Friston1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1002303-Daunizeau1">[84]</xref>. We described such a mechanism previously for a simple auditory sequence of sounds <xref ref-type="bibr" rid="pcbi.1002303-Kiebel2">[85]</xref>. The novelty of the current approach is that we use a neurobiologically plausible generative model to derive a functional recognition model that has also the potential to recognize real and complex birdsong. In addition, we hypothesize that the specific arrangement of HVC and RA level (dynamic sequences driving attractor dynamics at a lower level) and its Bayesian online inversion will not only play a role in birdsong recognition models but may be successfully used for automated speech recognition as well.</p>
      </sec>
      <sec id="s4f">
        <title>Further Extensions to the Model, Scaling and Sensitivity Analysis</title>
        <p>The mathematical model that we used to generate birdsongs was previously shown to produce accurate copies of songs such as canary <xref ref-type="bibr" rid="pcbi.1002303-Gardner1">[30]</xref>, chingolo sparrow <xref ref-type="bibr" rid="pcbi.1002303-Laje1">[17]</xref>, white-crowned sparrow <xref ref-type="bibr" rid="pcbi.1002303-Laje2">[31]</xref> and cardinal <xref ref-type="bibr" rid="pcbi.1002303-Mindlin2">[45]</xref> songs. The vocal organs of other birds, e.g. of the zebra finch, can generate highly nonlinear, more complex, acoustic dynamics than the one considered here. For modeling such songs, one would have to replace the syrinx model of Eq. (4) by a more involved syrinx model such as the one reported in <xref ref-type="bibr" rid="pcbi.1002303-Amador2">[86]</xref>.</p>
        <p>For our purposes, we focused on one particular song to describe the generation and recognition framework. The recognition of different songs either by the same or different conspecifics could be modeled by using multiple sequences encoded at the third level, where we assume that the recognition will converge to the best fitting sequence. In addition, one could adapt the nonlinear syrinx model to endow a singing bird with its own low-level acoustic characteristics.</p>
        <p>In the present model, we used rather small numbers of ensembles for visualization and computational purposes. The generative model applies to an arbitrary number of ensembles and similar type of dynamics can be obtained with larger number of ensembles at each level (see <xref ref-type="supplementary-material" rid="pcbi.1002303.s002">Figure S1</xref> for generation with 100 HVC ensembles). For recognition, we performed similar experiments with larger numbers of HVC ensembles (32) and RA patterns (24) where the recognition results were as robust as with the reported smaller size models (see <xref ref-type="supplementary-material" rid="pcbi.1002303.s003">Figure S2</xref> for the simulation). This indicates that the model scales to larger model sizes. However, there are two main issues that one will need to address to enable recognition using hundreds of units: (i) The computational power required for the recognition quickly increases with the number of ensembles used (with complexity <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e277" xlink:type="simple"/></inline-formula> due to computing a matrix exponential, see <xref ref-type="bibr" rid="pcbi.1002303-Friston1">[11]</xref>). This can be resolved by parallelizing the ensemble-specific computations which would be a further step towards biological reality. Currently, we emulate these parallel computations using a single-process Matlab implementation. (ii) The complexity of the syrinx model must be matched by the ‘descriptive power’ of the RA level. In other words, if one wanted to increase the number of RA ensembles significantly, one also had to render the model at the syrinx level more complex so that the recognition can infer more RA ensembles from more complex sensory data. However, this increase in model complexity at the syrinx and RA levels would require a more sophisticated syrinx model and is beyond the scope of the present work, in which we provide a proof of concept and introduce the computational framework.</p>
        <p>Furthermore, we tested the sensitivity of the Bayesian recognition in response to changing specific details of the generative model: (i) We used higher noise levels (standard deviation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e278" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e279" xlink:type="simple"/></inline-formula>) as compared to the simulations above, the recognition still robustly inferred the hidden states and causes at all levels (see <xref ref-type="supplementary-material" rid="pcbi.1002303.s004">Figure S3</xref>) (ii) We found that the recognition is robust against varying the initial conditions of the states in both the generative model and recognition. We tested a wide range of random initial conditions in both generation and recognition and observed that in all simulations the recognition quickly locks into the necessary dynamics. This implies that the listening bird can recognize a song reliably whatever the initial state of itself or the singing bird at the beginning of the song. (iii) We also changed the connectivity matrices at the third level (with the constraint of high inhibition from the previous neuron and low inhibition to the next neuron) and at the second level (with the constraint that global stability conditions are satisfied, see Theorem 1 in <xref ref-type="supplementary-material" rid="pcbi.1002303.s006">Text S1</xref>) of the generative and recognition models. The recognition was still robust with these different connectivity matrices (see <xref ref-type="supplementary-material" rid="pcbi.1002303.s006">Text S1</xref> and <xref ref-type="supplementary-material" rid="pcbi.1002303.s005">Figure S4</xref>).</p>
      </sec>
      <sec id="s4g">
        <title>Conclusion</title>
        <p>We described a model to generate artificial birdsongs and a scheme for their online recognition. We constructed a model based on key experimental findings in birdsong generation. Our results show that the specific, hierarchical mechanism how birdsong is generated enables robust and rapid decoding by a hierarchical and dynamic Bayesian inference scheme. We have interpreted this as evidence that the birdsong generation mechanism is geared toward making the song robustly decodable by conspecifics and discussed the experimental evidence that songbirds use a recognition mechanism similar to the present Bayesian inference scheme.</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pcbi.1002303.s001" mimetype="audio/x-wav" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.s001" xlink:type="simple">
        <label>Audio S1</label>
        <caption>
          <p>Sound file for the generated artificial birdsong obtained by plugging the first level output of the generation scheme (<xref ref-type="fig" rid="pcbi-1002303-g005">Figure 5C</xref>) into the syrinx equations, Eq. (4).</p>
          <p>(WAV)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002303.s002" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.s002" xlink:type="simple">
        <label>Figure S1</label>
        <caption>
          <p>Generated dynamics with 100 HVC ensembles at the third level where the format is the same as in <xref ref-type="fig" rid="pcbi-1002303-g005">Figure 5</xref> with arbitrary units. We only modified the rate constants (so that all activations fit to the time-window used) of the generative model and the rest of the constants are the same and listed in <xref ref-type="table" rid="pcbi-1002303-t001">Table 1</xref>. This simulation shows that the generative model can be scaled up and similar dynamics as shown in the main text figures can be obtained with long HVC sequences.</p>
          <p>(TIFF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002303.s003" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.s003" xlink:type="simple">
        <label>Figure S2</label>
        <caption>
          <p>The dynamics of song generation (left column) and song recognition (right column) with 32 neuronal ensembles at the third levels of both generation and recognition models. The format is the same as shown in <xref ref-type="fig" rid="pcbi-1002303-g005">Figure 5</xref> with arbitrary units. We only modified the rate constants (so that all activations fit to the time-window used) and the rest of the constants are the same and listed in <xref ref-type="table" rid="pcbi-1002303-t001">Table 1</xref>. This simulation shows that the recognition model can be scaled up and similar recognition dynamics as shown in the main text figures can be obtained with long HVC sequences.</p>
          <p>(TIFF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002303.s004" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.s004" xlink:type="simple">
        <label>Figure S3</label>
        <caption>
          <p>Robustness to noise of both the generative and recognition models: We generated song dynamics (left column) and song recognition (right column) using higher noise levels than in the simulations reported in the main text. The format is the same as shown in <xref ref-type="fig" rid="pcbi-1002303-g005">Figure 5</xref> with arbitrary units. We used noise with standard deviation of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e280" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002303.e281" xlink:type="simple"/></inline-formula> for causal and hidden states, respectively, at all levels of the generative model. The recognition was still robust at these noise levels. For simplicity, we only show the causal states of the generation and recognition.</p>
          <p>(TIFF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002303.s005" mimetype="image/tiff" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.s005" xlink:type="simple">
        <label>Figure S4</label>
        <caption>
          <p>Robustness of the generative and recognition models with respect to the connectivity matrices at the third and second levels. The format is the same as shown in <xref ref-type="fig" rid="pcbi-1002303-g005">Figure 5</xref> with arbitrary units. In this simulation, we used different (randomly assigned) connectivity matrices at the third and second levels of the generative and recognition models and obtained qualitatively the same dynamics as in the simulations reported in the main text.</p>
          <p>(TIFF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002303.s006" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1002303.s006" xlink:type="simple">
        <label>Text S1</label>
        <caption>
          <p>Further details about the RA and oscillation levels and the description of the online Bayesian recognition. We describe how to choose <italic>I</italic> vectors to control the RA dynamics and explain Eq. (5) for the oscillatory dynamics in the first level. In addition, we describe the sensitivity analysis of the model with respect to the changes in the connectivity matrices.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>We would like to thank Katharina von Kriegstein and Sebastian Bitzer for their helpful comments on earlier versions of the manuscript.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002303-Chi1">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Chi</surname><given-names>ZY</given-names></name><name name-style="western"><surname>Margoliash</surname><given-names>D</given-names></name></person-group>             <year>2001</year>             <article-title>Temporal precision and temporal drift in brain and behavior of zebra finch song.</article-title>             <source>Neuron</source>             <volume>32</volume>             <fpage>899</fpage>             <lpage>910</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Yu1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>AC</given-names></name><name name-style="western"><surname>Margoliash</surname><given-names>D</given-names></name></person-group>             <year>1996</year>             <article-title>Temporal hierarchical control of singing in birds.</article-title>             <source>Science</source>             <volume>273</volume>             <fpage>1871</fpage>             <lpage>1875</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Hahnloser1">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hahnloser</surname><given-names>RHR</given-names></name><name name-style="western"><surname>Kozhevnikov</surname><given-names>AA</given-names></name><name name-style="western"><surname>Fee</surname><given-names>MS</given-names></name></person-group>             <year>2002</year>             <article-title>An ultra-sparse code underlies the generation of neural sequences in a songbird.</article-title>             <source>Nature</source>             <volume>419</volume>             <fpage>65</fpage>             <lpage>70</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-OLochlen1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>O'Lochlen</surname><given-names>AL</given-names></name><name name-style="western"><surname>Beecher</surname><given-names>MD</given-names></name></person-group>             <year>1999</year>             <article-title>Mate, neighbour and stranger songs: a female song sparrow perspective.</article-title>             <source>Anim Behav</source>             <volume>58</volume>             <fpage>13</fpage>             <lpage>20</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Searcy1">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Searcy</surname><given-names>WA</given-names></name><name name-style="western"><surname>Marler</surname><given-names>P</given-names></name><name name-style="western"><surname>Peters</surname><given-names>SS</given-names></name></person-group>             <year>1981</year>             <article-title>Species Song Discrimination in Adult Female Song and Swamp Sparrows.</article-title>             <source>Anim Behav</source>             <volume>29</volume>             <fpage>997</fpage>             <lpage>1003</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Jarvis1">
        <label>6</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jarvis</surname><given-names>ED</given-names></name><name name-style="western"><surname>Scharff</surname><given-names>C</given-names></name><name name-style="western"><surname>Grossman</surname><given-names>MR</given-names></name><name name-style="western"><surname>Ramos</surname><given-names>JA</given-names></name><name name-style="western"><surname>Nottebohm</surname><given-names>F</given-names></name></person-group>             <year>1998</year>             <article-title>For whom the bird sings: Context-dependent gene expression.</article-title>             <source>Neuron</source>             <volume>21</volume>             <fpage>775</fpage>             <lpage>788</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Hasselquist1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hasselquist</surname><given-names>D</given-names></name></person-group>             <year>1998</year>             <article-title>Polygyny in great reed warblers: A long-term study of factors contributing to male fitness.</article-title>             <source>Ecology</source>             <volume>79</volume>             <fpage>2376</fpage>             <lpage>2390</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Ballentine1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ballentine</surname><given-names>B</given-names></name><name name-style="western"><surname>Hyman</surname><given-names>J</given-names></name><name name-style="western"><surname>Nowicki</surname><given-names>S</given-names></name></person-group>             <year>2004</year>             <article-title>Vocal performance influences female response to male bird song: an experimental test.</article-title>             <source>Behav Ecol</source>             <volume>15</volume>             <fpage>163</fpage>             <lpage>168</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Stoddard1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Stoddard</surname><given-names>PK</given-names></name><name name-style="western"><surname>Beecher</surname><given-names>MD</given-names></name><name name-style="western"><surname>Horning</surname><given-names>CL</given-names></name><name name-style="western"><surname>Campbell</surname><given-names>SE</given-names></name></person-group>             <year>1991</year>             <article-title>Recognition of Individual Neighbors by Song in the Song Sparrow, a Species with Song Repertoires.</article-title>             <source>Behav Ecol Sociobiol</source>             <volume>29</volume>             <fpage>211</fpage>             <lpage>215</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Godard1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Godard</surname><given-names>R</given-names></name></person-group>             <year>1991</year>             <article-title>Long-Term-Memory of Individual Neighbors in a Migratory Songbird.</article-title>             <source>Nature</source>             <volume>350</volume>             <fpage>228</fpage>             <lpage>229</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Friston1">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name><name name-style="western"><surname>Trujillo-Barreto</surname><given-names>N</given-names></name><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name></person-group>             <year>2008</year>             <article-title>DEM: A variational treatment of dynamic systems.</article-title>             <source>Neuroimage</source>             <volume>41</volume>             <fpage>849</fpage>             <lpage>885</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Li1">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>MR</given-names></name><name name-style="western"><surname>Greenside</surname><given-names>H</given-names></name></person-group>             <year>2006</year>             <article-title>Stable propagation of a burst through a one-dimensional homogeneous excitatory chain model of songbird nucleus HVC.</article-title>             <source>Phys Rev E</source>             <volume>74</volume>             <fpage>011918</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Jin1">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jin</surname><given-names>DZ</given-names></name><name name-style="western"><surname>Ramazanoglu</surname><given-names>FM</given-names></name><name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name></person-group>             <year>2007</year>             <article-title>Intrinsic bursting enhances the robustness of a neural network model of sequence generation by avian brain area HVC.</article-title>             <source>J Comput Neurosci</source>             <volume>23</volume>             <fpage>283</fpage>             <lpage>299</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Long1">
        <label>14</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Long</surname><given-names>MA</given-names></name><name name-style="western"><surname>Jin</surname><given-names>DZZ</given-names></name><name name-style="western"><surname>Fee</surname><given-names>MS</given-names></name></person-group>             <year>2010</year>             <article-title>Support for a synaptic chain model of neuronal sequence generation.</article-title>             <source>Nature</source>             <volume>468</volume>             <fpage>394</fpage>             <lpage>399</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Gibb1">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gibb</surname><given-names>L</given-names></name><name name-style="western"><surname>Gentner</surname><given-names>TQ</given-names></name><name name-style="western"><surname>Abarbanel</surname><given-names>HDI</given-names></name></person-group>             <year>2009</year>             <article-title>Inhibition and Recurrent Excitation in a Computational Model of Sparse Bursting in Song Nucleus HVC.</article-title>             <source>J Neurophysiol</source>             <volume>102</volume>             <fpage>1748</fpage>             <lpage>1762</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Fiete1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fiete</surname><given-names>IR</given-names></name><name name-style="western"><surname>Senn</surname><given-names>W</given-names></name><name name-style="western"><surname>Wang</surname><given-names>CZH</given-names></name><name name-style="western"><surname>Hahnloser</surname><given-names>RHR</given-names></name></person-group>             <year>2010</year>             <article-title>Spike-Time-Dependent Plasticity and Heterosynaptic Competition Organize Networks to Produce Long Scale-Free Sequences of Neural Activity.</article-title>             <source>Neuron</source>             <volume>65</volume>             <fpage>563</fpage>             <lpage>576</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Laje1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Laje</surname><given-names>R</given-names></name><name name-style="western"><surname>Gardner</surname><given-names>TJ</given-names></name><name name-style="western"><surname>Mindlin</surname><given-names>GB</given-names></name></person-group>             <year>2002</year>             <article-title>Neuromuscular control of vocalizations in birdsong: A model.</article-title>             <source>Phys Rev E</source>             <volume>65</volume>             <fpage>051921</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Abarbanel1">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Abarbanel</surname><given-names>HDI</given-names></name><name name-style="western"><surname>Gibb</surname><given-names>L</given-names></name><name name-style="western"><surname>Mindlin</surname><given-names>GB</given-names></name><name name-style="western"><surname>Talathi</surname><given-names>S</given-names></name></person-group>             <year>2004</year>             <article-title>Mapping neural architectures onto acoustic features of birdsong.</article-title>             <source>J Neurophysiol</source>             <volume>92</volume>             <fpage>96</fpage>             <lpage>110</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Nagel1">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nagel</surname><given-names>KI</given-names></name><name name-style="western"><surname>McLendon</surname><given-names>HM</given-names></name><name name-style="western"><surname>Doupe</surname><given-names>AJ</given-names></name></person-group>             <year>2010</year>             <article-title>Differential influence of frequency, timing, and intensity cues in a complex acoustic categorization task.</article-title>             <source>J Neurophysiol</source>             <volume>104</volume>             <fpage>1426</fpage>             <lpage>1437</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Gill1">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gill</surname><given-names>P</given-names></name><name name-style="western"><surname>Woolley</surname><given-names>SMN</given-names></name><name name-style="western"><surname>Fremouw</surname><given-names>T</given-names></name><name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name></person-group>             <year>2008</year>             <article-title>What's that sound? Auditory area CLM encodes stimulus surprise, not intensity or intensity changes.</article-title>             <source>J Neurophysiol</source>             <volume>99</volume>             <fpage>2809</fpage>             <lpage>2820</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Keller1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Keller</surname><given-names>GB</given-names></name><name name-style="western"><surname>Hahnloser</surname><given-names>RH</given-names></name></person-group>             <year>2009</year>             <article-title>Neural processing of auditory feedback during vocal practice in a songbird.</article-title>             <source>Nature</source>             <volume>457</volume>             <fpage>187</fpage>             <lpage>190</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Doupe1">
        <label>22</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Doupe</surname><given-names>AJ</given-names></name><name name-style="western"><surname>Kuhl</surname><given-names>PK</given-names></name></person-group>             <year>1999</year>             <article-title>Birdsong and human speech: Common themes and mechanisms.</article-title>             <source>Annu Rev Neurosci</source>             <volume>22</volume>             <fpage>567</fpage>             <lpage>631</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Bolhuis1">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bolhuis</surname><given-names>JJ</given-names></name><name name-style="western"><surname>Gahr</surname><given-names>M</given-names></name></person-group>             <year>2006</year>             <article-title>Neural mechanisms of birdsong memory.</article-title>             <source>Nat Rev Neurosci</source>             <volume>7</volume>             <fpage>347</fpage>             <lpage>357</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Kao1">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kao</surname><given-names>MH</given-names></name><name name-style="western"><surname>Doupe</surname><given-names>AJ</given-names></name><name name-style="western"><surname>Brainard</surname><given-names>MS</given-names></name></person-group>             <year>2005</year>             <article-title>Contributions of an avian basal ganglia-forebrain circuit to real-time modulation of song.</article-title>             <source>Nature</source>             <volume>433</volume>             <fpage>638</fpage>             <lpage>643</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Fee1">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fee</surname><given-names>MS</given-names></name><name name-style="western"><surname>Kozhevnikov</surname><given-names>AA</given-names></name><name name-style="western"><surname>Hahnloser</surname><given-names>RHR</given-names></name></person-group>             <year>2004</year>             <article-title>Neural mechanisms of vocal sequence generation in the songbird.</article-title>             <source>Ann N Y Acad Sci</source>             <volume>1016</volume>             <fpage>153</fpage>             <lpage>170</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Leonardo1">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Leonardo</surname><given-names>A</given-names></name><name name-style="western"><surname>Fee</surname><given-names>MS</given-names></name></person-group>             <year>2005</year>             <article-title>Ensemble coding of vocal control in birdsong.</article-title>             <source>J Neurosci</source>             <volume>25</volume>             <fpage>652</fpage>             <lpage>661</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Afraimovich1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Afraimovich</surname><given-names>VS</given-names></name><name name-style="western"><surname>Rabinovich</surname><given-names>MI</given-names></name><name name-style="western"><surname>Varona</surname><given-names>P</given-names></name></person-group>             <year>2004</year>             <article-title>Heteroclinic contours in neural ensembles and the winnerless competition principle.</article-title>             <source>Int J Bifurcat Chaos</source>             <volume>14</volume>             <fpage>1195</fpage>             <lpage>1208</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Rabinovich1">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rabinovich</surname><given-names>M</given-names></name><name name-style="western"><surname>Volkovskii</surname><given-names>A</given-names></name><name name-style="western"><surname>Lecanda</surname><given-names>P</given-names></name><name name-style="western"><surname>Huerta</surname><given-names>R</given-names></name><name name-style="western"><surname>Abarbanel</surname><given-names>HD</given-names></name><etal/></person-group>             <year>2001</year>             <article-title>Dynamical encoding by networks of competing neuron groups: winnerless competition.</article-title>             <source>Phys Rev Lett</source>             <volume>87</volume>             <fpage>068102</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Hopfield1">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group>             <year>1982</year>             <article-title>Neural Networks and Physical Systems with Emergent Collective Computational Abilities.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>79</volume>             <fpage>2554</fpage>             <lpage>2558</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Gardner1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gardner</surname><given-names>T</given-names></name><name name-style="western"><surname>Cecchi</surname><given-names>G</given-names></name><name name-style="western"><surname>Magnasco</surname><given-names>M</given-names></name><name name-style="western"><surname>Laje</surname><given-names>R</given-names></name><name name-style="western"><surname>Mindlin</surname><given-names>GB</given-names></name></person-group>             <year>2001</year>             <article-title>Simple motor gestures for birdsongs.</article-title>             <source>Phys Rev Lett</source>             <volume>8720</volume>             <fpage>208101</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Laje2">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Laje</surname><given-names>R</given-names></name><name name-style="western"><surname>Mindlin</surname><given-names>GB</given-names></name></person-group>             <year>2002</year>             <article-title>Diversity within a birdsong.</article-title>             <source>Phys Rev Lett</source>             <volume>89</volume>             <fpage>288102</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-May1">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>May</surname><given-names>RM</given-names></name><name name-style="western"><surname>Leonard</surname><given-names>WJ</given-names></name></person-group>             <year>1975</year>             <article-title>Nonlinear Aspects of Competition Between Three Species.</article-title>             <source>SIAM J Appl Math</source>             <volume>29</volume>             <fpage>243</fpage>             <lpage>253</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Rabinovich2">
        <label>33</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rabinovich</surname><given-names>MI</given-names></name><name name-style="western"><surname>Varona</surname><given-names>P</given-names></name><name name-style="western"><surname>Selverston</surname><given-names>AI</given-names></name><name name-style="western"><surname>Abarbanel</surname><given-names>HDI</given-names></name></person-group>             <year>2006</year>             <article-title>Dynamical principles in neuroscience.</article-title>             <source>Rev Mod Phys</source>             <volume>78</volume>             <fpage>1213</fpage>             <lpage>1265</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Varona1">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Varona</surname><given-names>P</given-names></name><name name-style="western"><surname>Rabinovich</surname><given-names>MI</given-names></name><name name-style="western"><surname>Selverston</surname><given-names>AI</given-names></name><name name-style="western"><surname>Arshavsky</surname><given-names>YI</given-names></name></person-group>             <year>2002</year>             <article-title>Winnerless competition between sensory neurons generates chaos: A possible mechanism for molluscan hunting behavior.</article-title>             <source>Chaos</source>             <volume>12</volume>             <fpage>672</fpage>             <lpage>677</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Fukai1">
        <label>35</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fukai</surname><given-names>T</given-names></name><name name-style="western"><surname>Tanaka</surname><given-names>S</given-names></name></person-group>             <year>1997</year>             <article-title>A simple neural network exhibiting selective activation of neuronal ensembles: From winner-take-all to winners-share-all.</article-title>             <source>Neural Comput</source>             <volume>9</volume>             <fpage>77</fpage>             <lpage>97</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Hopfield2">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group>             <year>1984</year>             <article-title>Neurons with Graded Response Have Collective Computational Properties Like Those of 2-State Neurons.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>81</volume>             <fpage>3088</fpage>             <lpage>3092</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Chen1">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>TP</given-names></name><name name-style="western"><surname>Amari</surname><given-names>SI</given-names></name></person-group>             <year>2001</year>             <article-title>Stability of asymmetric Hopfield networks.</article-title>             <source>IEEE Trans Neural Netw</source>             <volume>12</volume>             <fpage>159</fpage>             <lpage>163</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Matsuoka1">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Matsuoka</surname><given-names>K</given-names></name></person-group>             <year>1992</year>             <article-title>Stability Conditions for Nonlinear Continuous Neural Networks with Asymmetric Connection Weights.</article-title>             <source>Neural Netw</source>             <volume>5</volume>             <fpage>495</fpage>             <lpage>500</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Xu1">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>ZB</given-names></name><name name-style="western"><surname>Kwong</surname><given-names>CP</given-names></name></person-group>             <year>1995</year>             <article-title>Global Convergence and Asymptotic Stability of Asymmetric Hopfield Neural Networks.</article-title>             <source>J Math Anal Appl</source>             <volume>191</volume>             <fpage>405</fpage>             <lpage>427</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Zheng1">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>PS</given-names></name><name name-style="western"><surname>Tang</surname><given-names>WS</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>JX</given-names></name></person-group>             <year>2010</year>             <article-title>Efficient Continuous-Time Asymmetric Hopfield Networks for Memory Retrieval.</article-title>             <source>Neural Comput</source>             <volume>22</volume>             <fpage>1597</fpage>             <lpage>1614</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Titze1">
        <label>41</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Titze</surname><given-names>IR</given-names></name></person-group>             <year>1988</year>             <article-title>The Physics of Small-Amplitude Oscillation of the Vocal Folds.</article-title>             <source>J Acoust Soc Am</source>             <volume>83</volume>             <fpage>1536</fpage>             <lpage>1552</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Larsen1">
        <label>42</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Larsen</surname><given-names>ON</given-names></name><name name-style="western"><surname>Goller</surname><given-names>F</given-names></name></person-group>             <year>1999</year>             <article-title>Role of syringeal vibrations in bird vocalizations.</article-title>             <source>Philos Trans R Soc Lond B Biol Sci</source>             <volume>266</volume>             <fpage>1609</fpage>             <lpage>1615</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Mindlin1">
        <label>43</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mindlin</surname><given-names>GB</given-names></name><name name-style="western"><surname>Laje</surname><given-names>R</given-names></name></person-group>             <year>2005</year>             <source>The physics of birdsong</source>             <publisher-loc>Berlin</publisher-loc>             <publisher-name>Springer</publisher-name> <comment>x, 157</comment>           </element-citation>
      </ref>
      <ref id="pcbi.1002303-Hartley1">
        <label>44</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hartley</surname><given-names>RS</given-names></name></person-group>             <year>1990</year>             <article-title>Expiratory Muscle-Activity during Song Production in the Canary.</article-title>             <source>Resp Physiol</source>             <volume>81</volume>             <fpage>177</fpage>             <lpage>188</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Mindlin2">
        <label>45</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mindlin</surname><given-names>GB</given-names></name><name name-style="western"><surname>Gardner</surname><given-names>TJ</given-names></name><name name-style="western"><surname>Goller</surname><given-names>F</given-names></name><name name-style="western"><surname>Suthers</surname><given-names>R</given-names></name></person-group>             <year>2003</year>             <article-title>Experimental support for a model of birdsong production.</article-title>             <source>Phys Rev E</source>             <volume>68</volume>             <fpage>041908</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Ijspeert1">
        <label>46</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ijspeert</surname><given-names>AJ</given-names></name></person-group>             <year>2008</year>             <article-title>Central pattern generators for locomotion control in animals and robots: A review.</article-title>             <source>Neural Netw</source>             <volume>21</volume>             <fpage>642</fpage>             <lpage>653</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Friston2">
        <label>47</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name></person-group>             <year>2008</year>             <article-title>Hierarchical Models in the Brain.</article-title>             <source>PLoS Comput Biol</source>             <volume>4</volume>             <fpage>e1000211</fpage>             <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/1000210.1001371/journal.pcbi.1000211" xlink:type="simple">1000210.1001371/journal.pcbi.1000211</ext-link></comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Friston3">
        <label>48</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name></person-group>             <year>2010</year>             <article-title>The free-energy principle: a unified brain theory?</article-title>             <source>Nat Rev Neurosci</source>             <volume>11</volume>             <fpage>127</fpage>             <lpage>138</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-OLoghlen1">
        <label>49</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>OLoghlen</surname><given-names>AL</given-names></name><name name-style="western"><surname>Beecher</surname><given-names>MD</given-names></name></person-group>             <year>1997</year>             <article-title>Sexual preferences for mate song types in female song sparrows.</article-title>             <source>Anim Behav</source>             <volume>53</volume>             <fpage>835</fpage>             <lpage>841</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Riebel1">
        <label>50</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Riebel</surname><given-names>K</given-names></name><name name-style="western"><surname>Smallegange</surname><given-names>IM</given-names></name><name name-style="western"><surname>Terpstra</surname><given-names>NJ</given-names></name><name name-style="western"><surname>Bolhuis</surname><given-names>JJ</given-names></name></person-group>             <year>2002</year>             <article-title>Sexual equality in zebra finch song preference: evidence for a dissociation between song recognition and production learning.</article-title>             <source>Philos Trans R Soc Lond B Biol Sci</source>             <volume>269</volume>             <fpage>729</fpage>             <lpage>733</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Kilner1">
        <label>51</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kilner</surname><given-names>JM</given-names></name><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name><name name-style="western"><surname>Frith</surname><given-names>CD</given-names></name></person-group>             <year>2007</year>             <article-title>The mirror-neuron system: a Bayesian perspective.</article-title>             <source>Neuroreport</source>             <volume>18</volume>             <fpage>619</fpage>             <lpage>623</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Mumford1">
        <label>52</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mumford</surname><given-names>D</given-names></name></person-group>             <year>1992</year>             <article-title>On the Computational Architecture of the Neocortex .2. The Role of Corticocortical Loops.</article-title>             <source>Biol Cybern</source>             <volume>66</volume>             <fpage>241</fpage>             <lpage>251</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Long2">
        <label>53</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Long</surname><given-names>MA</given-names></name><name name-style="western"><surname>Fee</surname><given-names>MS</given-names></name></person-group>             <year>2008</year>             <article-title>Using temperature to analyse temporal dynamics in the songbird motor pathway.</article-title>             <source>Nature</source>             <volume>456</volume>             <fpage>189</fpage>             <lpage>194</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Nowicki1">
        <label>54</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nowicki</surname><given-names>S</given-names></name><name name-style="western"><surname>Searcy</surname><given-names>WA</given-names></name></person-group>             <year>2005</year>             <article-title>Song and mate choice in birds: How the development of behavior helps us understand function.</article-title>             <source>Auk</source>             <volume>122</volume>             <fpage>1</fpage>             <lpage>14</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Ward1">
        <label>55</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ward</surname><given-names>BC</given-names></name><name name-style="western"><surname>Nordeen</surname><given-names>EJ</given-names></name><name name-style="western"><surname>Nordeen</surname><given-names>KW</given-names></name></person-group>             <year>1998</year>             <article-title>Individual variation in neuron number predicts differences in the propensity for avian vocal imitation.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>95</volume>             <fpage>1277</fpage>             <lpage>1282</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Alvarezbuylla1">
        <label>56</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Alvarezbuylla</surname><given-names>A</given-names></name><name name-style="western"><surname>Nottebohm</surname><given-names>F</given-names></name></person-group>             <year>1988</year>             <article-title>Migration of Young Neurons in Adult Avian Brain.</article-title>             <source>Nature</source>             <volume>335</volume>             <fpage>353</fpage>             <lpage>354</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Gahr1">
        <label>57</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gahr</surname><given-names>M</given-names></name><name name-style="western"><surname>Leitner</surname><given-names>S</given-names></name><name name-style="western"><surname>Fusani</surname><given-names>L</given-names></name><name name-style="western"><surname>Rybak</surname><given-names>F</given-names></name></person-group>             <year>2002</year>             <article-title>What is the adaptive role of neurogenesis in adult birds?</article-title>             <source>Prog Brain Res</source>             <volume>138</volume>             <fpage>233</fpage>             <lpage>254</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Airey1">
        <label>58</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Airey</surname><given-names>DC</given-names></name><name name-style="western"><surname>Castillo-Juarez</surname><given-names>H</given-names></name><name name-style="western"><surname>Casella</surname><given-names>G</given-names></name><name name-style="western"><surname>Pollak</surname><given-names>EJ</given-names></name><name name-style="western"><surname>DeVoogd</surname><given-names>TJ</given-names></name></person-group>             <year>2000</year>             <article-title>Variation in the volume of zebra finch song control nuclei is heritable: developmental and evolutionary implications.</article-title>             <source>Philos Trans R Soc Lond B Biol Sci</source>             <volume>267</volume>             <fpage>2099</fpage>             <lpage>2104</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Sossinka1">
        <label>59</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sossinka</surname><given-names>R</given-names></name><name name-style="western"><surname>Bohner</surname><given-names>J</given-names></name></person-group>             <year>1980</year>             <article-title>Song Types in the Zebra Finch Poephila-Guttata-Castanotis.</article-title>             <source>Z Tierpsychol</source>             <volume>53</volume>             <fpage>123</fpage>             <lpage>132</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Budhiraja1">
        <label>60</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Budhiraja</surname><given-names>A</given-names></name><name name-style="western"><surname>Chen</surname><given-names>LJ</given-names></name><name name-style="western"><surname>Lee</surname><given-names>C</given-names></name></person-group>             <year>2007</year>             <article-title>A survey of numerical methods for nonlinear filtering problems.</article-title>             <source>Physica D</source>             <volume>230</volume>             <fpage>27</fpage>             <lpage>36</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Judd1">
        <label>61</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Judd</surname><given-names>K</given-names></name><name name-style="western"><surname>Smith</surname><given-names>LA</given-names></name></person-group>             <year>2004</year>             <article-title>Indistinguishable states II - The imperfect model scenario.</article-title>             <source>Physica D</source>             <volume>196</volume>             <fpage>224</fpage>             <lpage>242</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Price1">
        <label>62</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Price</surname><given-names>T</given-names></name></person-group>             <year>1998</year>             <article-title>Sexual selection and natural selection in bird speciation.</article-title>             <source>Philos Trans R Soc Lond B Biol Sci</source>             <volume>353</volume>             <fpage>251</fpage>             <lpage>260</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Amador1">
        <label>63</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Amador</surname><given-names>A</given-names></name><name name-style="western"><surname>Margoliash</surname><given-names>D</given-names></name></person-group>             <year>2011</year>             <article-title>Auditory Memories and Feedback Processing for Vocal Learning.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Winer</surname><given-names>JA</given-names></name><name name-style="western"><surname>Schreiner</surname><given-names>CE</given-names></name></person-group>             <source>The Auditory Cortex</source>             <publisher-loc>New York</publisher-loc>             <publisher-name>Springer</publisher-name>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Theunissen1">
        <label>64</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name><name name-style="western"><surname>Amin</surname><given-names>N</given-names></name><name name-style="western"><surname>Shaevitz</surname><given-names>SS</given-names></name><name name-style="western"><surname>Woolley</surname><given-names>SMN</given-names></name><name name-style="western"><surname>Fremouw</surname><given-names>T</given-names></name><etal/></person-group>             <year>2004</year>             <article-title>Song selectivity in the song system and in the auditory forebrain.</article-title>             <source>Ann NY Acad Sci</source>             <volume>1016</volume>             <fpage>222</fpage>             <lpage>245</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Prather1">
        <label>65</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Prather</surname><given-names>JF</given-names></name><name name-style="western"><surname>Peters</surname><given-names>S</given-names></name><name name-style="western"><surname>Nowicki</surname><given-names>S</given-names></name><name name-style="western"><surname>Mooney</surname><given-names>R</given-names></name></person-group>             <year>2008</year>             <article-title>Precise auditory-vocal mirroring in neurons for learned vocal communication.</article-title>             <source>Nature</source>             <volume>451</volume>             <fpage>305-U302</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Prather2">
        <label>66</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Prather</surname><given-names>JF</given-names></name><name name-style="western"><surname>Nowicki</surname><given-names>S</given-names></name><name name-style="western"><surname>Anderson</surname><given-names>RC</given-names></name><name name-style="western"><surname>Peters</surname><given-names>S</given-names></name><name name-style="western"><surname>Mooney</surname><given-names>R</given-names></name></person-group>             <year>2009</year>             <article-title>Neural correlates of categorical perception in learned vocal communication.</article-title>             <source>Nat Neurosci</source>             <volume>12</volume>             <fpage>221</fpage>             <lpage>228</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Sen1">
        <label>67</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sen</surname><given-names>K</given-names></name><name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name><name name-style="western"><surname>Doupe</surname><given-names>AJ</given-names></name></person-group>             <year>2001</year>             <article-title>Feature analysis of natural sounds in the songbird auditory forebrain.</article-title>             <source>J Neurophysiol</source>             <volume>86</volume>             <fpage>1445</fpage>             <lpage>1458</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Solis1">
        <label>68</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Solis</surname><given-names>MM</given-names></name><name name-style="western"><surname>Perkel</surname><given-names>DJ</given-names></name></person-group>             <year>2005</year>             <article-title>Rhythmic activity in a forebrain vocal control nucleus in vitro.</article-title>             <source>J Neurosci</source>             <volume>25</volume>             <fpage>2811</fpage>             <lpage>2822</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Buonomano1">
        <label>69</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Buonomano</surname><given-names>DV</given-names></name></person-group>             <year>2005</year>             <article-title>A learning rule for the emergence of stable dynamics and timing in recurrent networks.</article-title>             <source>J Neurophysiol</source>             <volume>94</volume>             <fpage>2275</fpage>             <lpage>2283</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Theunissen2">
        <label>70</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name><name name-style="western"><surname>Sen</surname><given-names>K</given-names></name><name name-style="western"><surname>Doupe</surname><given-names>AJ</given-names></name></person-group>             <year>2000</year>             <article-title>Spectral-temporal receptive fields of nonlinear auditory neurons obtained using natural sounds.</article-title>             <source>J Neurosci</source>             <volume>20</volume>             <fpage>2315</fpage>             <lpage>2331</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Drew1">
        <label>71</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Drew</surname><given-names>PJ</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name></person-group>             <year>2003</year>             <article-title>Model of song selectivity and sequence generation in area HVc of the songbird.</article-title>             <source>J Neurophysiol</source>             <volume>89</volume>             <fpage>2697</fpage>             <lpage>2706</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Larson1">
        <label>72</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Larson</surname><given-names>E</given-names></name><name name-style="western"><surname>Billimoria</surname><given-names>CP</given-names></name><name name-style="western"><surname>Sen</surname><given-names>K</given-names></name></person-group>             <year>2009</year>             <article-title>A Biologically Plausible Computational Model for Auditory Object Recognition.</article-title>             <source>J Neurophysiol</source>             <volume>101</volume>             <fpage>323</fpage>             <lpage>331</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Doya1">
        <label>73</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Doya</surname><given-names>K</given-names></name><name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group>             <year>2000</year>             <article-title>A computational model of avian song learning.</article-title>             <person-group person-group-type="editor"><name name-style="western"><surname>Gazzaniga</surname><given-names>MS</given-names></name></person-group>             <source>The new cognitive neurosciences</source>             <publisher-loc>Cambridge, Mass.</publisher-loc>             <publisher-name>MIT Press</publisher-name> <comment>xiv, 1419</comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Fiete2">
        <label>74</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fiete</surname><given-names>IR</given-names></name><name name-style="western"><surname>Fee</surname><given-names>MS</given-names></name><name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name></person-group>             <year>2007</year>             <article-title>Model of birdsong learning based on gradient estimation by dynamic perturbation of neural conductances.</article-title>             <source>J Neurophysiol</source>             <volume>98</volume>             <fpage>2038</fpage>             <lpage>2057</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Anderson1">
        <label>75</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Anderson</surname><given-names>SE</given-names></name><name name-style="western"><surname>Dave</surname><given-names>AS</given-names></name><name name-style="western"><surname>Margoliash</surname><given-names>D</given-names></name></person-group>             <year>1996</year>             <article-title>Template-based automatic recognition of birdsong syllables from continuous recordings.</article-title>             <source>J Acoust Soc Am</source>             <volume>100</volume>             <fpage>1209</fpage>             <lpage>1219</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Kogan1">
        <label>76</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kogan</surname><given-names>JA</given-names></name><name name-style="western"><surname>Margoliash</surname><given-names>D</given-names></name></person-group>             <year>1998</year>             <article-title>Automated recognition of bird song elements from continuous recordings using dynamic time warping and hidden Markov models: A comparative study.</article-title>             <source>J Acoust Soc Am</source>             <volume>103</volume>             <fpage>2185</fpage>             <lpage>2196</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Mooney1">
        <label>77</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mooney</surname><given-names>R</given-names></name></person-group>             <year>2009</year>             <article-title>Neural mechanisms for learned birdsong.</article-title>             <source>Learn Memory</source>             <volume>16</volume>             <fpage>655</fpage>             <lpage>669</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Voss1">
        <label>78</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Voss</surname><given-names>HU</given-names></name><name name-style="western"><surname>Tabelow</surname><given-names>K</given-names></name><name name-style="western"><surname>Polzehl</surname><given-names>J</given-names></name><name name-style="western"><surname>Tchernichovski</surname><given-names>O</given-names></name><name name-style="western"><surname>Maul</surname><given-names>KK</given-names></name><etal/></person-group>             <year>2007</year>             <article-title>Functional MRI of the zebra finch brain during song stimulation suggests a lateralized response topography.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>104</volume>             <fpage>10667</fpage>             <lpage>10672</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Voss2">
        <label>79</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Voss</surname><given-names>HU</given-names></name><name name-style="western"><surname>Salgado-Commissariat</surname><given-names>D</given-names></name><name name-style="western"><surname>Helekar</surname><given-names>SA</given-names></name></person-group>             <year>2010</year>             <article-title>Altered Auditory BOLD Response to Conspecific Birdsong in Zebra Finches with Stuttered Syllables.</article-title>             <source>PLoS One</source>             <volume>5</volume>             <fpage>e14415</fpage>             <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/14410.11371/journal.pone.0014415" xlink:type="simple">14410.11371/journal.pone.0014415</ext-link></comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Glaze1">
        <label>80</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Glaze</surname><given-names>CM</given-names></name><name name-style="western"><surname>Troyer</surname><given-names>TW</given-names></name></person-group>             <year>2006</year>             <article-title>Temporal structure in zebra finch song: Implications for motor coding.</article-title>             <source>J Neurosci</source>             <volume>26</volume>             <fpage>991</fpage>             <lpage>1005</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Rose1">
        <label>81</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rose</surname><given-names>GJ</given-names></name><name name-style="western"><surname>Goller</surname><given-names>F</given-names></name><name name-style="western"><surname>Gritton</surname><given-names>HJ</given-names></name><name name-style="western"><surname>Plamondon</surname><given-names>SL</given-names></name><name name-style="western"><surname>Baugh</surname><given-names>AT</given-names></name><etal/></person-group>             <year>2004</year>             <article-title>Species-typical songs in white-crowned sparrows tutored with only phrase pairs.</article-title>             <source>Nature</source>             <volume>432</volume>             <fpage>753</fpage>             <lpage>758</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Kiebel1">
        <label>82</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name></person-group>             <year>2009</year>             <article-title>Perception and hierarchical dynamics.</article-title>             <source>Front Neuroinformatics</source>             <volume>3</volume>             <fpage>20</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Deng1">
        <label>83</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Deng</surname><given-names>L</given-names></name><name name-style="western"><surname>Yu</surname><given-names>D</given-names></name><name name-style="western"><surname>Acero</surname><given-names>A</given-names></name></person-group>             <year>2006</year>             <article-title>Structured speech modeling.</article-title>             <source>IEEE Trans Audio Speech Lang Processing</source>             <volume>14</volume>             <fpage>1492</fpage>             <lpage>1504</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Daunizeau1">
        <label>84</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name><name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name></person-group>             <year>2009</year>             <article-title>Variational Bayesian identification and prediction of stochastic nonlinear dynamic causal models.</article-title>             <source>Physica D</source>             <volume>238</volume>             <fpage>2089</fpage>             <lpage>2118</lpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Kiebel2">
        <label>85</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name></person-group>             <year>2008</year>             <article-title>A Hierarchy of Time-Scales and the Brain.</article-title>             <source>PLoS Comput Biol</source>             <volume>4</volume>             <fpage>e1000209</fpage>             <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/1000210.1001371/journal.pcbi.1000209" xlink:type="simple">1000210.1001371/journal.pcbi.1000209</ext-link></comment>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Amador2">
        <label>86</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Amador</surname><given-names>A</given-names></name><name name-style="western"><surname>Mindlin</surname><given-names>GB</given-names></name></person-group>             <year>2008</year>             <article-title>Beyond harmonic sounds in a simple model for birdsong production.</article-title>             <source>Chaos</source>             <volume>18</volume>             <fpage>043123</fpage>          </element-citation>
      </ref>
      <ref id="pcbi.1002303-Nottebohm1">
        <label>87</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Nottebohm</surname><given-names>F</given-names></name></person-group>             <year>2005</year>             <article-title>The neural basis of birdsong.</article-title>             <source>PLoS Biol</source>             <volume>3</volume>             <fpage>e164</fpage>             <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/110.1371/journal.pbio.0030164" xlink:type="simple">110.1371/journal.pbio.0030164</ext-link></comment>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>