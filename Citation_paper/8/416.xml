<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
      <journal-id journal-id-type="publisher-id">plos</journal-id>
      <journal-id journal-id-type="pmc">plosone</journal-id>
      <journal-title-group>
        <journal-title>PLoS ONE</journal-title>
      </journal-title-group>
      <issn pub-type="epub">1932-6203</issn>
      <publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">PONE-D-12-29661</article-id>
      <article-id pub-id-type="doi">10.1371/journal.pone.0059448</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Computer science</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Social and behavioral sciences</subject>
          <subj-group>
            <subject>Psychology</subject>
            <subj-group>
              <subject>Developmental psychology</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Sociology</subject>
            <subj-group>
              <subject>Social mobility</subject>
              <subject>Social research</subject>
              <subject>Social welfare</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Computer Science</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Robot-Mediated Interviews - How Effective Is a Humanoid Robot as a Tool for Interviewing Young Children?</article-title>
        <alt-title alt-title-type="running-head">Robot-Mediated Interviews</alt-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Wood</surname>
            <given-names>Luke Jai</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Dautenhahn</surname>
            <given-names>Kerstin</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Rainer</surname>
            <given-names>Austen</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Robins</surname>
            <given-names>Ben</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Lehmann</surname>
            <given-names>Hagen</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Syrdal</surname>
            <given-names>Dag Sverre</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
      </contrib-group>
      <aff id="aff1">
        <addr-line>School of Computer Science, University of Hertfordshire, Hatfield, Hertfordshire, United Kingdom</addr-line>
      </aff>
      <contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Slater</surname>
            <given-names>Mel</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group>
      <aff id="edit1">
        <addr-line>ICREA-University of Barcelona, Spain</addr-line>
      </aff>
      <author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">l.wood@herts.ac.uk</email></corresp>
        <fn fn-type="conflict">
          <p>The authors have declared that no competing interests exist.</p>
        </fn>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: LW KD AR BR. Performed the experiments: LW. Analyzed the data: LW HL DS. Contributed reagents/materials/analysis tools: LW KD. Wrote the paper: LW KD HL.</p>
        </fn>
      </author-notes>
      <pub-date pub-type="collection">
        <year>2013</year>
      </pub-date>
      <pub-date pub-type="epub">
        <day>22</day>
        <month>3</month>
        <year>2013</year>
      </pub-date>
      <volume>8</volume>
      <issue>3</issue>
      <elocation-id>e59448</elocation-id>
      <history>
        <date date-type="received">
          <day>25</day>
          <month>9</month>
          <year>2012</year>
        </date>
        <date date-type="accepted">
          <day>14</day>
          <month>2</month>
          <year>2013</year>
        </date>
      </history>
      <permissions>
        <copyright-year>2013</copyright-year>
        <copyright-holder>Wood et al</copyright-holder>
        <license xlink:type="simple">
          <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
        </license>
      </permissions>
      <abstract>
        <p>Robots have been used in a variety of education, therapy or entertainment contexts. This paper introduces the novel application of using humanoid robots for robot-mediated interviews. An experimental study examines how children’s responses towards the humanoid robot KASPAR in an interview context differ in comparison to their interaction with a human in a similar setting. Twenty-one children aged between 7 and 9 took part in this study. Each child participated in two interviews, one with an adult and one with a humanoid robot. Measures include the behavioural coding of the children’s behaviour during the interviews and questionnaire data. The questions in these interviews focused on a special event that had recently taken place in the school. The results reveal that the children interacted with KASPAR very similar to how they interacted with a human interviewer. The quantitative behaviour analysis reveal that the most notable difference between the interviews with KASPAR and the human were the duration of the interviews, the eye gaze directed towards the different interviewers, and the response time of the interviewers. These results are discussed in light of future work towards developing KASPAR as an ‘interviewer’ for young children in application areas where a robot may have advantages over a human interviewer, e.g. in police, social services, or healthcare applications.</p>
      </abstract>
      <funding-group>
        <funding-statement>The authors have no support or funding to report.</funding-statement>
      </funding-group>
      <counts>
        <page-count count="13"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>In recent years, there has been a steady increase in research exploring social robots, from robotic pets and educational aids <xref ref-type="bibr" rid="pone.0059448-Kerepesi1">[1]</xref>, <xref ref-type="bibr" rid="pone.0059448-Melson1">[2]</xref>, <xref ref-type="bibr" rid="pone.0059448-Barlett1">[3]</xref>, <xref ref-type="bibr" rid="pone.0059448-Kanda1">[4]</xref>, <xref ref-type="bibr" rid="pone.0059448-Tanaka1">[5]</xref> to therapeutic and assistive tools for children who often respond very well to such robots <xref ref-type="bibr" rid="pone.0059448-Saldien1">[6]</xref>, <xref ref-type="bibr" rid="pone.0059448-Saldien2">[7]</xref>, <xref ref-type="bibr" rid="pone.0059448-Goris1">[8]</xref>, <xref ref-type="bibr" rid="pone.0059448-Dautenhahn1">[9]</xref>, <xref ref-type="bibr" rid="pone.0059448-Robins1">[10]</xref>, <xref ref-type="bibr" rid="pone.0059448-Robins2">[11]</xref>, <xref ref-type="bibr" rid="pone.0059448-Kozima1">[12]</xref>.</p>
      <p>In our previous work we have studied extensively how humans interact with robots and how robots could be designed as acceptable enjoyable, and socially intelligent interaction partners that can provide assistance to people <xref ref-type="bibr" rid="pone.0059448-Dautenhahn2">[13]</xref>. Most relevant to this article is the minimally expressive, humanoid robot called KASPAR designed by our research group specifically for social interaction <xref ref-type="bibr" rid="pone.0059448-Dautenhahn1">[9]</xref>. The robot has been used successfully in many human-robot interaction studies involving neurotypical <xref ref-type="bibr" rid="pone.0059448-KoseBagci1">[14]</xref>, <xref ref-type="bibr" rid="pone.0059448-Robins3">[15]</xref> and autistic children <xref ref-type="bibr" rid="pone.0059448-Robins1">[10]</xref>, <xref ref-type="bibr" rid="pone.0059448-Wainer1">[16]</xref>, showing that children respond very well to the size and appearance of the robot and its human-like, but very simplified features. In this article, we explore a potential new application domain for KASPAR and humanoid robots in general, namely its use as a robotic interviewer for young children.</p>
      <p>While in therapy and education the robot is typically meant to facilitate learning and/or therapeutic changes in the children, in this novel application area robots are used as mediators between a professional human interviewer and a child, providing a simple and enjoyable interaction partner with the purpose of engaging the children in the interview for the retrieval of vital information. Exploring the possibility of using robots to interview children could reveal whether robot-mediated interviews could be a valid addition to existing methods of interviewing children by professional staff such as police or social services. However, before starting investigations in the sensitive areas of interviews with children in a social services or police context, we need to establish whether or not a humanoid robot is, in more general terms, acceptable as a robotic interviewer, e.g. will children take the interviews “seriously”, i.e. discuss factual information, as opposed to treating the situation as an entertainment activity where they use their imagination? This article establishes such baseline information using a quantitative experimental approach.</p>
      <p>Although extensive research has explored both the use of social robots with children and various approaches to interviewing children <xref ref-type="bibr" rid="pone.0059448-Docherty1">[17]</xref>, <xref ref-type="bibr" rid="pone.0059448-UK1">[18]</xref>, <xref ref-type="bibr" rid="pone.0059448-Spratling1">[19]</xref>, <xref ref-type="bibr" rid="pone.0059448-Roebers1">[20]</xref>, very little research investigates how robots could be used in an interview scenario. The most relevant work published by Bethel <italic>et al.</italic> <xref ref-type="bibr" rid="pone.0059448-Bethel1">[21]</xref> investigated if typically developing children aged 4 and 5 years old, were as likely to share a secret with a NAO robot <xref ref-type="bibr" rid="pone.0059448-NAO1">[22]</xref>, as they were with an adult. The quantitative results from this study were inconclusive. However, the qualitative results revealed that the children would readily interact with the robot and speak to it in a similar manner as they would with an adult. These results encouraged us to design a comparative experimental study to evaluate how children would respond to an interview with a robot rather than a human, and how children’s behaviour (verbally and non-verbally) may differ between the two conditions.</p>
      <p>Note, the goal of our research is not to replace human interviewers, but to provide professionals with a robotic tool as an interface that creates an enjoyable and comfortable setting for children to talk about their experiences. Robot-mediated interviews, as described in this article, could allow the professional to precisely control the robot’s behaviour (e.g. facial expressions, body language), which is often very hard to do even for professionally trained interviewers, in particular when the topic of the interview may be emotionally sensitive.</p>
      <p>The study was conducted in a local primary school with children aged between 7 and 9 in UK year groups 3 and 4. Each child was interviewed twice, once by a humanoid robot called KASPAR and once by a human. The interviews were counterbalanced and conducted in a structured and controlled manner in order to compare the results of the two conditions. We analysed and compared the interviews in terms of the children’s verbal and non-verbal behaviour, information disclosed during the interview, and the children’s answers to a questionnaire.</p>
      <p>This article is structured as follows. Firstly we review literature relating to Human-Robot Interaction (HRI) and techniques for interviewing children. This is followed by a description of the structure and methodology used in our study. The findings and results from the study are then discussed. In the final section the findings and implications are assessed and the future direction of the research proposed.</p>
    </sec>
    <sec id="s2">
      <title>Background</title>
      <sec id="s2a">
        <title>Human Robot Interaction</title>
        <p>Scientific research investigating the use of social robots, particularly with children, has steadily increased in recent years. In this section we discuss some key contributions in this domain as relevant to this article.</p>
        <p>Children are often more willing than adults to engage and readily interact with robots <xref ref-type="bibr" rid="pone.0059448-Berlyn1">[23]</xref>. Scheeff <italic>et al.</italic> <xref ref-type="bibr" rid="pone.0059448-Scheef1">[24]</xref> found that young children will actively approach robots to interact with them without any instruction and that factors such as age and gender affect the interactions. This supports the hypothesis that young children may respond well to a robot interviewing them, as they are often keen to interact with robots.</p>
        <p>Kanda <italic>et al.</italic> <xref ref-type="bibr" rid="pone.0059448-Kanda1">[4]</xref> conducted an 18-day field trial at a Japanese elementary school using two “Robovie” robots with first-grade and sixth-grade children to investigate the possibility of using robots as social partners to teach the children English. Although it was found that the majority of the children’s English did not improve, initially the children were very interested in the robot. Establishing a rapport with a child is essential when attempting to acquire information from them. Fior <italic>et al.</italic> <xref ref-type="bibr" rid="pone.0059448-Fior1">[25]</xref> investigated if children could form relationships with robots and view them as friends. Results showed that 85.9% of the children thought the robot could be their friend, 67.4% of the children would talk to the robot, and 45.7% would share a secret with the robot. These statistics support the hypothesis that children might be willing to communicate and share information with robots as the children were happy to talk to and view robots as friends, with 45.7% of the children expressing they would be willing to share a secret with a robot.</p>
        <p>Nishio <italic>et al.</italic> <xref ref-type="bibr" rid="pone.0059448-Nishio1">[26]</xref> investigated how a teleoperated android (HI-1) could be used to represent a personal presence of a real person with two young children. This research is relevant to our investigation in terms of having a robot perform conversational tasks with young children. The HI-1 robot could be deemed to fall into the uncanny valley <xref ref-type="bibr" rid="pone.0059448-MacDorman1">[27]</xref>, <xref ref-type="bibr" rid="pone.0059448-BeckerAsano1">[28]</xref>, and the children in Nishio’s study were both uncomfortable with the robot at first, although they did adjust to interacting with the robot. When interviewing children, which is the focus of the present article, it is important that they are as comfortable as possible with the robot from the start of the interview, in order to get the most of the interview. Therefore comparing how comfortable children are talking to a robot, as opposed to an adult, will be useful, as very little comparative research has been conducted in this specific area.</p>
        <p>Recent work investigated how children interact with iCat robots <xref ref-type="bibr" rid="pone.0059448-Shahid1">[29]</xref>, <xref ref-type="bibr" rid="pone.0059448-Shahid2">[30]</xref>, <xref ref-type="bibr" rid="pone.0059448-Leite1">[31]</xref>. Specifically the work by Shahid <italic>et al.</italic> <xref ref-type="bibr" rid="pone.0059448-Shahid3">[32]</xref> investigated if children perceive playing with a robot to be like playing with a friend. Results from subjective fun scores and perception tests suggested that children enjoy playing with the robot more than playing alone, but not as much as when playing with a friend. This study supports the idea that children do enjoy interacting with robots. Note, in our research, the children were always interacting with a robot or a person they had only recently met and who could be considered a stranger.</p>
        <p>A recent closely related study by Bethel <italic>et al.</italic> <xref ref-type="bibr" rid="pone.0059448-Bethel1">[21]</xref> investigated if 41 children aged 4 and 5 years old, were as likely to share a secret with a NAO robot <xref ref-type="bibr" rid="pone.0059448-NAO1">[22]</xref> as they are an adult. In this investigation a secret was shared with the child and he or she was explicitly asked not to tell anyone. Later the child took part in an interaction task with the robot and another adult separately. In the interactions the child was encouraged to tell the interaction partner the secret. The quantitative results from this study were inconclusive but the qualitative results revealed that the children would readily interact with the robot and speak to it in a similar manner as they would with an adult. Bethel et al.’s study has similarities to our research but the robot was acting as a social interaction partner rather than leading an interview. However, our investigation focused on how children would respond to a humanoid robot in a structured interview context. Also, in Bethel et al.’s study, it may not have been clear to the children that the main purpose of their interaction was to gather information from them, as they were participating in a physically interactive task. Instead, in our research it was clear to the children that the sole purpose of the interaction was for information acquisition, as there were no other tasks for them to focus on whilst having the interview.</p>
      </sec>
      <sec id="s2b">
        <title>Interviewing Children</title>
        <p>Social robotics and interviewing children are very different areas of research, and there has been little research investigating how robots could be used to interview children. Therefore, when exploring the possibility of using a robot to interview children, we spoke to specialist professionals from the Metropolitan Police that are experienced in interviewing young children. (The Metropolitan Police are the territorial police force responsible for Greater London and also have significant national responsibilities). These specialists advised us of how to conduct structured interviews with children, and also referred us to the Achieving Best Evidence in Criminal Proceedings document which the police refer to themselves <xref ref-type="bibr" rid="pone.0059448-UK1">[18]</xref>, <xref ref-type="bibr" rid="pone.0059448-UK2">[33]</xref>. The ABE was drafted for the UK’s Home Office by a team of experts from varying backgrounds including psychology, law and social services. Because the guidelines laid out in the ABE have been well researched and recognised as providing an effective structured and standardised method for interviewing young children, we followed the relevant guidelines of the documents as closely as possible, with feedback from the above mentioned professionals during the design stages of the experiments.</p>
        <p>The ABE suggests that interviews should have four phases in the following order:</p>
        <list list-type="order">
          <list-item>
            <p>Establishing a rapport</p>
          </list-item>
          <list-item>
            <p>Asking for free narrative recall</p>
          </list-item>
          <list-item>
            <p>Asking questions</p>
          </list-item>
          <list-item>
            <p>Closure</p>
          </list-item>
        </list>
        <p>This phase is used to get the child acquainted with the interviewer. The ABE suggests that this phase should be used to discuss neutral topics to relax the child and set the ground rules for the rest of the interview. In addition, the ABE states that when children have an interview with the police they instantly think that they have done something wrong and it is important to address this immediately. Although our research is not part of a criminal proceeding and the interviewers are not police officers, it was important that the children did not worry therefore we took this point into account when conducting the introduction phase as well as setting the ground rules for the interview.</p>
        <p>When interviewing children it is desirable for the children to recall as much information as possible without prompting using minimal direction. This is because information from free narrative recall is the most accurate, and would be considered more reliable as evidence in a courtroom. Although the information the children provide in our study does not need to stand up as evidence in court, it is important for the children to express themselves freely, as we are attempting to measure how much information the children freely provide to a robot compared to a human.</p>
        <p>The ABE suggests that once a child has recalled as much information on their own accord as possible a questioning phase should begin. In this phase the interviewer focuses on trying to recover key pieces of information that the child may have overlooked in their recall of the event. This allows the interviewer to maximise the amount of useful information they can recover from a child. The questions the police use in this phase should not be in any way leading, as this would compromise the integrity and legitimacy of the statement from the child. Although the interviews we were carrying out were not of a sensitive nature, and the statements did not need to be relied upon in court, we did include a questioning phase to maximise the information recovered and to adhere to the standardised interview structure that is used by the police. This was also useful for investigating any difference in the information the children provided to KASPAR compared to the human when asked more specific questions.</p>
        <p>In the closing phase of the interview we followed the advice of the recommendation of the ABE, thanking the child for their time and returning to a neutral topic of conversation, similar to establishing a rapport phase.</p>
        <p>The ABE document contains a great deal of information that is specifically useful for a police interview. In our research we followed those guidelines that were relevant to address our research questions. A lot of the criteria and information in the ABE relates to court situations and law, therefore some of the information was not applicable to our work.</p>
      </sec>
      <sec id="s2c">
        <title>Research Questions and Expectations</title>
        <p>This study aims to answer two general research questions that we identified as the first necessary step to establish whether or not a robot can be used as an interviewer for young children:</p>
        <list list-type="bullet">
          <list-item>
            <p>How do children’s non-verbal and verbal behaviour, as well as their opinions about the interaction, differ in the two experimental conditions using a robotic versus a human interviewer? (RQ1)</p>
          </list-item>
          <list-item>
            <p>In terms of content of the children’s responses, will the children disclose more information to the robot or to the human? (RQ2)</p>
          </list-item>
        </list>
        <p>Concerning RQ1, we expect that children will be more interested in KASPAR as a novel object <xref ref-type="bibr" rid="pone.0059448-Berlyn1">[23]</xref>, and would direct more behaviours that indicate interest (e.g. eye gaze) towards the robot compared to the human interviewer.</p>
        <p>In the case of RQ2, on the one hand, one may expect that children would talk more and reveal more information to the human experimenter, since the children are very used to the situation of being asked questions by a human (e.g. at home or at school) rather than talking to a robot. On the other hand, if the children experience the robot as an enjoyable and comfortable interaction partner (compared to the human experimenter who is a stranger to them) then they might disclose more information to KASPAR. We thus expected clear preferences for either the human or robotic interviewer.</p>
        <p>Note, both the robot and the human interviewer were presented as ‘strangers’ to the children in our experimental scenario. A novelty effect, in particular with regards to the robot, could therefore we expected, however, this reflects a natural situation of our targeted application area, where ‘strangers’ are interviewing young children once, or, if repeated, only a few times.</p>
      </sec>
    </sec>
    <sec id="s3" sec-type="methods">
      <title>Methods</title>
      <sec id="s3a">
        <title>Ethics Statement</title>
        <p>This research was approved by the University of Hertfordshire’s ethics committee for studies involving human participants. Informed consent was obtained in writing from all parents of the children participating in the study.</p>
      </sec>
      <sec id="s3b">
        <title>Participants</title>
        <p>The study was conducted in a local UK primary school in Hertfordshire with children aged between 7 and 9 years old (UK year groups 3 and 4). The study involved 22 children, 21 of which produced useable data, (technical difficulties meant the data from one session could not be included). Of the 21 children 10 were in year 3, 11 were in year 4, 10 were female and 11 were male. The majority of the children in year 3 were female and the majority of year 4 were male. Of the 21 children, 3 have been diagnosed with ‘some form of autism’ (according to the teachers). The results from the sessions with the children with autism were consistent with the results of the typically developing children, we therefore decided to include the data from these sessions in our dataset for this study. The adult interviewer was the first author of this article.</p>
      </sec>
      <sec id="s3c">
        <title>Procedure</title>
        <p>We conducted interviews with the children that took place on four days over a two-week period. Each child experienced two interviews, one with KASPAR and one with a human experimenter. The interviews were one week apart and the same interview structure was followed on both occasions. A two-phase counterbalancing method was implemented to reduce the chance of the interview order adversely influencing the results. Half of the children were interviewed by KASPAR first and half were interviewed by the human interviewer first. Counterbalancing was also applied in terms of gender and year group, so that each group had the same number of boys/girls and year groups. Assignment to each of the two groups was otherwise random for any particular child. In group 1 which were interviewed by KASPAR first there were 6 males and 5 females; the average age in this group was 8. Note, one of these females was the child not included in the final dataset because of technical difficulties. In group 2 the children were interviewed by the human first, there were 5 males and 6 females; the average age in this group was 8.3. The primary units of analysis were verbal communication, eye gaze and information disclosure.</p>
        <p>The children were briefly given a group introduction to both KASPAR and the human interviewer at the school one day before the interviews commenced. In this introductory session we provided information on the nature of the robot (KASPAR was explicitly introduced as a robot) and on the purpose of the study (conducting interviews). It was emphasised that they were not being assessed or graded on what they did or said in the interviews and that there were no right or wrong answers. This was explained because we did not want the children to be worried or distressed about having the interview. We ensured that the children had equal minimal contact with both KASPAR and the human experimenter prior to the interviews, as having disproportionate contact could adversely affect the results. In this introduction, it was explained that we would be talking about the Red Nose Day talent event that had recently taken place, (Red Nose Day is a bi-annual national event in the UK to raise money for charities). However, the children were not provided with any details as to what they would be asked as this could lead and influence what they might have said in the interviews.</p>
        <p>After the interviews had taken place the children were given a debrief as a group to explain how KASPAR worked. In this debrief the children were also given the opportunity to control the robot.</p>
      </sec>
      <sec id="s3d">
        <title>The Robot</title>
        <p>The robot KASPAR (<xref ref-type="fig" rid="pone-0059448-g001">Figure 1</xref>) used in this study is a child-sized, humanoid robot with a minimally expressive face and arms that are capable of gesturing. This robot has been shown to be very effective when working alongside typically developing children <xref ref-type="bibr" rid="pone.0059448-KoseBagci1">[14]</xref>, <xref ref-type="bibr" rid="pone.0059448-KoseBagci2">[34]</xref> and children with autism <xref ref-type="bibr" rid="pone.0059448-Robins1">[10]</xref>, <xref ref-type="bibr" rid="pone.0059448-Robins2">[11]</xref>, <xref ref-type="bibr" rid="pone.0059448-Wainer1">[16]</xref>, <xref ref-type="bibr" rid="pone.0059448-Robins4">[35]</xref>. Robins <italic>et al.</italic> explored how children adapt to interacting with a robot and will mirror a robot’s temporal behaviour <xref ref-type="bibr" rid="pone.0059448-Robins3">[15]</xref>. KASPAR has also been used to explore aspects of human-robot interaction relating to the role of gestures an interaction <xref ref-type="bibr" rid="pone.0059448-KoseBagci3">[36]</xref>, and how different types of embodiment affect interaction <xref ref-type="bibr" rid="pone.0059448-KoseBagci1">[14]</xref>. The previous research conducted with KASPAR would thus suggest that the robot is a suitable platform for this particular area of research investigating how children respond to a humanoid robot in an interview scenario.</p>
        <fig id="pone-0059448-g001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0059448.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>KASPAR robot.</title>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0059448.g001" position="float" xlink:type="simple"/>
        </fig>
        <p>The robot’s head and neck have 8 Degrees of Freedom along with the arms and hands that have 6 DOF <xref ref-type="bibr" rid="pone.0059448-Dautenhahn1">[9]</xref>. KASPAR is controlled via a Java based GUI which can be customised for specific applications. Once setup the GUI can activate behaviours or sequences by a key press. For using the robot to conduct an interaction, speech phrases were produced by the experimenter pressing buttons, following the Wizard-of-Oz methodology (WoZ), widely used in Human-Computer Interaction (see Gould et al. 1983 <xref ref-type="bibr" rid="pone.0059448-Gould1">[37]</xref>; Dahlback et al. 1993 <xref ref-type="bibr" rid="pone.0059448-Dahlbck1">[38]</xref> and more recently has been used in HRI <xref ref-type="bibr" rid="pone.0059448-Dautenhahn2">[13]</xref>, <xref ref-type="bibr" rid="pone.0059448-Green1">[39]</xref>. The program controlling KASPAR had been specifically tailored with pre-programmed audio clips accompanied by appropriate sequences of movements. These non-verbal or verbal behaviours (speech) were initialised by pressing specific keys, two sheets with the speech phrases and corresponding keys were in the control tent (see below) to aid the investigator in finding the correct key. The audio clips for KASPAR’s voice were generated from text-to-speech synthesis software. Text-to-speech software was used rather than recordings of a natural human voice to maintain the theme of the robot as a robot. Natural human voice coming from a clearly robotic body would most likely have impaired the perceived consistency of the robot in terms of appearance and behaviour which has been shown to be important in the human-robot interaction literature, e.g. <xref ref-type="bibr" rid="pone.0059448-Walters1">[40]</xref>, <xref ref-type="bibr" rid="pone.0059448-Goetz1">[41]</xref>. Also, using a synthesised voice helped maintain the distinction between the robot interviewer and the human interviewer. The children were unaware that KASPAR was being controlled by a human investigator. We used the WoZ methodology since in future applications that we envisage in our research, a person would speak to the child via the robot, similar to the setup used in our experiments.</p>
      </sec>
      <sec id="s3e">
        <title>Experimental Setup</title>
        <p>The interviews took place in a small room with a recessed portion that was mostly hidden from the children. The interviews took place in the main large area of the room at a table, while the recessed part of the room was used for the robot control tent. We used a small tent to fully hide the controls and monitor of KASPAR as the partition alone would not have fully hidden the equipment and controller. The control tent housed a small monitor with a wireless connection to camera #1 for viewing and listening to the children. This was essential as we needed to know what the children were saying in order to make KASPAR respond appropriately. Camera #1 was behind the interviewer to the left and camera #2 was also behind the interviewer and to the right, both of these cameras were recording the front of the children to capture eye gaze, while camera #3 was recording the front of the interviewer. The control tent also housed a laptop that controlled KASPAR via a remote connection.</p>
        <p>Both KASPAR and the human interviewed all the children on two separate occasions one week apart. The lead investigator always led the interview in person or remotely via KASPAR. This was important to maintain consistency, making sure that the responses and questions from both KASPAR and the human were the same. The children were taken to and from the interviews by a second researcher unknown to the children. This second researcher remained in the room during the interviews in case of any technical difficulties with the robot, but was as non-reactive as possible in order to avoid interferences with the experiment. Immediately after the interview the children were asked to complete a questionnaire and post it into a box located on a separate table. The second researcher answered any question’s the children had about the questionnaire. (Experimental setup and room layout shown in <xref ref-type="fig" rid="pone-0059448-g002">Figure 2</xref>).</p>
        <fig id="pone-0059448-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0059448.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Room layout and images of scenario.</title>
            <p>(A) Room setup; (B) KASPAR interviewing a child; (C) Experimenter interviewing a child. Note the individual in this manuscript (<xref ref-type="fig" rid="pone-0059448-g002">Figure 2</xref>) has given written informed consent (as outlined in PLOS consent form) to publish these case details and photograph.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0059448.g002" position="float" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s3f">
        <title>The Interview</title>
        <p>The interviews began with a short introduction of getting to know each other’s name and ascertaining other general details such as the child’s age, if they have any siblings etc. These questions were easy for the children to answer and used to establish a rapport for the rest of the interview. We then proceeded towards the main topic, the talent event that the children had been involved in. Moving towards the main topic was achieved by asking the child “what are we going to talk about today”. If the child did not remember they were reminded that they were there to talk about the talent event. Research and practice have shown that the most detailed and reliable answers are secured from open questions <xref ref-type="bibr" rid="pone.0059448-UK3">[42]</xref>, therefore the majority of interview questions were open questions. This maximised the children’s freedom to express themselves and minimised the scope for speculation. This approach might indicate who/what the child is more comfortable with based on how much they say and what they would say.</p>
        <p>The questions that focused on the main topic varied in difficulty and this was reflected in the answers that the children gave. For example, the children found the question about who won the event much easier than the question about the judges who took part in the event. Almost all of the children correctly named both of the winners of the event. However, there was a much greater variation in the number of judges the children could remember. This is possibly because during the event there would have been much more focus on the winners. Also, one of the judges of the event was unfamiliar to the children and it may have been harder for them to remember the name of this individual.</p>
        <p>The questions in this interview primarily focused on facts, similar to how the police would conduct an interview. When the police or social services are trying to gather information from an individual they are interested in the facts of an event, as it is these facts that will be used to establish what has happened before deciding on what action to take <xref ref-type="bibr" rid="pone.0059448-UK4">[43]</xref>. In addition to this, when making a prosecution, it is the facts and key points that are used to make the prosecution, rather than the feelings of the individual, because without the facts and points of proof a prosecution cannot be made.</p>
        <p>The interviews concluded by thanking the child for their time and saying that it has been nice talking to them. In these interviews we adhered to a rigid structure with set sayings in order to compare the two different conditions. The majority of the structure and questions for the interview were derived from the guidance of the ABE document, specifically the “Planning and conducting interviews with children” section <xref ref-type="bibr" rid="pone.0059448-UK1">[18]</xref>. This provided a recognised standard approach for interviewing children. When the police are asking questions it is important to try and keep the questions as open as possible. The ABE explains that “questions beginning with the phrases ‘Tell me’, or the words ‘describe’ or ‘explain’ are useful examples of this type of question” <xref ref-type="bibr" rid="pone.0059448-UK3">[42]</xref>, therefore we decided to use theses phrases and words at the beginning of our questions. See example questions below. (A full list of interview questions is shown in <xref ref-type="supplementary-material" rid="pone.0059448.s001">Figure S1</xref>):</p>
        <list list-type="bullet">
          <list-item>
            <p>Tell me about yourself.</p>
          </list-item>
          <list-item>
            <p>Tell me what we are going to talk about today.</p>
          </list-item>
          <list-item>
            <p>Describe the event to me.</p>
          </list-item>
          <list-item>
            <p>Tell me about the judges.</p>
          </list-item>
          <list-item>
            <p>Explain what happened in the final on Friday.</p>
          </list-item>
          <list-item>
            <p>Describe for me what the winner got.</p>
          </list-item>
        </list>
      </sec>
      <sec id="s3g">
        <title>Measurements</title>
        <p>The questions and interview structure were reviewed and revised several times before trialling the structure, setup and equipment in the laboratory at our University with adult volunteers. The data in the school was collected from three cameras that recorded the interviews. Two of the cameras were pointing towards the front of the child from two separate angles and the other was filming behind the child and had the interviewers face in view. In addition to the interview, the children were also asked to complete a questionnaire immediately after each interview. This was to establish what they thought of the whole experience and in particular what they thought of KASPAR. The questionnaire was kept short and simple in order not to overwhelm the children. Once all the interviews were complete, the video footage was transcribed and then coded using the Observer XT software <xref ref-type="bibr" rid="pone.0059448-Observer1">[44]</xref>. We measured verbal communication both in terms of spoken words, duration of responses and gaps between responses from the child and the interviewer. In addition, eye gaze from the child to the interviewer was coded, as well as other body language such as nodding and shaking of the head. The points of measure we used in this study were defined as follows:</p>
        <sec id="s3g1">
          <title>Interview duration</title>
          <p>Full duration of the interview from start to finish. It was used to assess if there was any difference in the time the interviews would take.</p>
        </sec>
        <sec id="s3g2">
          <title>Eye gaze duration</title>
          <p>This is defined as the child looking towards the interviewer’s face. We measured eye gaze duration to evaluate the different amounts of eye gaze towards the robot compared to the human. This measurement also allowed us to observe any relationship between eye gaze and the amount that the children spoke. The eye gaze measurement is proportionate to the duration of the interviews. Because the interviews varied in length it was important to take this into account.</p>
        </sec>
        <sec id="s3g3">
          <title>Child response duration</title>
          <p>Total amount of time the child spends speaking to the interviewer throughout the full duration of the interview.</p>
        </sec>
        <sec id="s3g4">
          <title>Interviewer response duration</title>
          <p>Total amount of time the interviewer spends speaking to the child throughout the full duration of the interview.</p>
        </sec>
        <sec id="s3g5">
          <title>Response time child&gt;interviewer</title>
          <p>Total amount of time throughout the full duration of the interview that the interviewer takes to respond to the child.</p>
        </sec>
        <sec id="s3g6">
          <title>Response time interviewer&gt;child</title>
          <p>Total amount of time throughout the full duration of the interview that the child takes to respond to the interviewer.</p>
        </sec>
        <sec id="s3g7">
          <title>Word count</title>
          <p>Total number of words spoken by the child throughout the full duration of the interview excluding filler words. We used this to measure how much the children spoke in each interview.</p>
        </sec>
        <sec id="s3g8">
          <title>Filler word count</title>
          <p>Total number of filler words spoken by the child throughout the full duration of the interview. The children would often use filler words such as “err”, “errm”, “hum”, etc. and these words were included in the transcriptions. When analysing the transcriptions for a word count these filler words were not counted in that analysis but we did perform a separate filler word analysis.</p>
        </sec>
        <sec id="s3g9">
          <title>Proportionate word count</title>
          <p>Total number of words spoken by the child throughout the full duration of the interview excluding filler words proportionate to the total number of words spoken by the interviewer throughout the full duration of the interview.</p>
        </sec>
        <sec id="s3g10">
          <title>Key word count</title>
          <p>Total number of key words spoken by the child throughout the full duration of the interview. The keywords chosen were related to the questions we asked and specifically focused on four areas:</p>
          <list list-type="bullet">
            <list-item>
              <p>Family members (brothers, sisters, etc…)</p>
            </list-item>
            <list-item>
              <p>Names of judges for the talent event</p>
            </list-item>
            <list-item>
              <p>Prizes for the winners of the event</p>
            </list-item>
            <list-item>
              <p>Names of the event winners</p>
            </list-item>
          </list>
        </sec>
        <sec id="s3g11">
          <title>Key points</title>
          <p>In this study we also logged the key points from the transcriptions. This information consisted of 6 main categories:</p>
          <list list-type="bullet">
            <list-item>
              <p>Family</p>
            </list-item>
            <list-item>
              <p>Pets</p>
            </list-item>
            <list-item>
              <p>Event acts</p>
            </list-item>
            <list-item>
              <p>Judges</p>
            </list-item>
            <list-item>
              <p>Winners</p>
            </list-item>
            <list-item>
              <p>Poster activity</p>
            </list-item>
          </list>
          <p>The questions in our study were designed to recover information about these 6 main categories. Each category had a specific information criteria defining it as a key point. This information was analysed both qualitatively and quantitatively. The qualitative aspects are the specific details of what the children are saying and the consistency of the information between the two interviews. The quantitative aspect is the numerical logging of each specific piece of information given by the child and the statistical analysis of this logged information. The latter was done in order to understand how many key points the child revealed.</p>
        </sec>
        <sec id="s3g12">
          <title>Key points - Family category</title>
          <p>Since in the interviews one of the questions asked about the children’s siblings, in this category we analysed how many family members the children mention in total throughout the duration of the interview, and how many family members they state by name. We also compared the names given in both experimental conditions, to establish the consistency of the facts disclosed in both interview conditions.</p>
        </sec>
        <sec id="s3g13">
          <title>Key points - Pets category</title>
          <p>One of the introductory rapport building questions related to pets. Similar to the family category, we analysed for both experimental conditions how many pets the children mention and how many pets they state by name.</p>
        </sec>
        <sec id="s3g14">
          <title>Key points - Event acts category</title>
          <p>The questions in the interviews were designed to acquire information about the event the children either took part in or which they witnessed. In this category we logged the number of types of acts that the children mentioned, the number of acts in the event, the number of people named that took part in the event, and a comparison of the names to check consistency. With regards to the types of acts this refers to a particular sort of act i.e. dancing or singing. If a child stated a year group and an act this was also counted as a type of act because this would be a specific type of act. The number of acts in the event refers to how many acts the child stated. For example the child may have said that there were 4 types of act (dancing, singing, acting, and magic tricks) but only referred to 2 acts that were performed (the winners and the chosen act from their own year group). We also kept a record of the number of names the child mentioned who were in an act in the event.</p>
        </sec>
        <sec id="s3g15">
          <title>Key points - Judges category</title>
          <p>The event that the questions were based around was a talent event with judges therefore we specifically asked a question about the judges. From what the children said we establish how many judges there are in the event and also record how many they judges they name with a comparison for consistency.</p>
        </sec>
        <sec id="s3g16">
          <title>Key points – Winner’s category</title>
          <p>As the event was a competition there was one winning act with two children in the act and a prize. Some of the questions in the interview were designed to find out about the winner and what they received. We logged if the children could remember the winners name and if they were aware of the prize that the winners received.</p>
        </sec>
        <sec id="s3g17">
          <title>Key points - Posters category</title>
          <p>The children that did not take part in the main event, and only watched, made some posters to support their class mates’ acts. We logged this information and recorded how many of the children mentioned this activity because this seemed to be quite an important aspect of the event that many children mentioned.</p>
        </sec>
        <sec id="s3g18">
          <title>Questionnaire</title>
          <p>The children were asked to complete a questionnaire immediately after their interviews (<xref ref-type="supplementary-material" rid="pone.0059448.s002">Figure S2</xref>) to ask the children their opinions of the interview, specifically:</p>
          <list list-type="bullet">
            <list-item>
              <p>Interest – How interesting they found the experience</p>
            </list-item>
            <list-item>
              <p>Difficulty – How difficult they found the interview</p>
            </list-item>
            <list-item>
              <p>Fun – How much fun they had participating</p>
            </list-item>
            <list-item>
              <p>Duration – How long they thought the interview took</p>
            </list-item>
          </list>
        </sec>
      </sec>
    </sec>
    <sec id="s4">
      <title>Results</title>
      <p>We performed a series of t-tests on different measurements assessed during the experiment in order to test for statistically significant differences between the human and the robotic interviewer conditions.</p>
      <p>We found that the interviews with KASPAR lasted significantly longer, on average the interviews with KASPAR lasted (minutes:seconds) 6∶53 and interviews with the human lasted 5∶22, although there was considerable variation in the durations of the interviews (see graph B in <xref ref-type="fig" rid="pone-0059448-g003">Figure 3</xref>). The interviews with KASPAR ranged from 3∶44 to 10∶45 whilst interviews with the human experimenter were between 3∶24 and 11∶43 (<xref ref-type="table" rid="pone-0059448-t001">Table 1</xref>).</p>
      <fig id="pone-0059448-g003" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0059448.g003</object-id>
        <label>Figure 3</label>
        <caption>
          <title>Interview averages comparison graphs.</title>
          <p>(A) Average Eye Gaze Duration; (B)Average Interview Duration; (C) Average Word Count; (D) Average Filler Word Count; (E) Average Key Word Count; (F) Questionnaire Averages.</p>
        </caption>
        <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0059448.g003" position="float" xlink:type="simple"/>
      </fig>
      <table-wrap id="pone-0059448-t001" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0059448.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>Overall interaction metrics (KASPAR vs. Human).</title>
        </caption>
        <alternatives>
          <graphic id="pone-0059448-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0059448.t001" xlink:type="simple"/>
          <table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td colspan="2" align="left" rowspan="1">KASPAR</td>
                <td colspan="2" align="left" rowspan="1">Human</td>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">Mean</td>
                <td align="left" rowspan="1" colspan="1">Range</td>
                <td align="left" rowspan="1" colspan="1">Mean</td>
                <td align="left" rowspan="1" colspan="1">Range</td>
                <td align="left" rowspan="1" colspan="1">Mean difference</td>
                <td align="left" rowspan="1" colspan="1">t</td>
                <td align="left" rowspan="1" colspan="1">p</td>
                <td align="left" rowspan="1" colspan="1">Confidence interval of the mean</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">Interview duration</td>
                <td align="left" rowspan="1" colspan="1">06∶53</td>
                <td align="left" rowspan="1" colspan="1">3∶44–10∶45</td>
                <td align="left" rowspan="1" colspan="1">05∶22</td>
                <td align="left" rowspan="1" colspan="1">3∶24–11∶43</td>
                <td align="left" rowspan="1" colspan="1">90.936</td>
                <td align="left" rowspan="1" colspan="1">2.947</td>
                <td align="left" rowspan="1" colspan="1">.008*</td>
                <td align="left" rowspan="1" colspan="1">26.57–155.30</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Eye gaze duration</td>
                <td align="left" rowspan="1" colspan="1">0.338</td>
                <td align="left" rowspan="1" colspan="1">.117–.807</td>
                <td align="left" rowspan="1" colspan="1">0.286</td>
                <td align="left" rowspan="1" colspan="1">.122–.717</td>
                <td align="left" rowspan="1" colspan="1">0.053</td>
                <td align="left" rowspan="1" colspan="1">2.115</td>
                <td align="left" rowspan="1" colspan="1">.047*</td>
                <td align="left" rowspan="1" colspan="1">.001–.104</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Word count</td>
                <td align="left" rowspan="1" colspan="1">359</td>
                <td align="left" rowspan="1" colspan="1">179–672</td>
                <td align="left" rowspan="1" colspan="1">373</td>
                <td align="left" rowspan="1" colspan="1">175–894</td>
                <td align="left" rowspan="1" colspan="1">−14.625</td>
                <td align="left" rowspan="1" colspan="1">−0.415</td>
                <td align="left" rowspan="1" colspan="1">0.683</td>
                <td align="left" rowspan="1" colspan="1">−148</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Proportionate word count</td>
                <td align="left" rowspan="1" colspan="1">2.42</td>
                <td align="left" rowspan="1" colspan="1">0.93–4.07</td>
                <td align="left" rowspan="1" colspan="1">2.49</td>
                <td align="left" rowspan="1" colspan="1">1.07–6.98</td>
                <td align="left" rowspan="1" colspan="1">−0.074</td>
                <td align="left" rowspan="1" colspan="1">−0.316</td>
                <td align="left" rowspan="1" colspan="1">0.755</td>
                <td align="left" rowspan="1" colspan="1">−0.979</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Filler word count</td>
                <td align="left" rowspan="1" colspan="1">19</td>
                <td align="left" rowspan="1" colspan="1">2–101</td>
                <td align="left" rowspan="1" colspan="1">19</td>
                <td align="left" rowspan="1" colspan="1">23043</td>
                <td align="left" rowspan="1" colspan="1">0</td>
                <td align="left" rowspan="1" colspan="1">0</td>
                <td align="left" rowspan="1" colspan="1">1</td>
                <td align="left" rowspan="1" colspan="1">−11.6</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>Proportionately we found that the children looked towards the face of the KASPAR significantly more (<xref ref-type="table" rid="pone-0059448-t001">Table 1</xref>). These results were normalised and calculated relative to the interview duration as the duration of the interviews varied. On average the children looked towards the face of KASPAR for 2∶19 compared to 1∶29 with the human (see graph A in <xref ref-type="fig" rid="pone-0059448-g003">Figure 3</xref>). To verify the reliability of the coding a 20% counterbalanced subset of these videos were also coded by a second independent researcher. The videos were counterbalanced in terms of interviewer, gender, year group and session. The inter-rater reliability produced a kappa value of 0.74, which is considered very good <xref ref-type="bibr" rid="pone.0059448-Landis1">[45]</xref>.</p>
      <p>There was no significant difference in the amount of words that the children spoke to the robot compared to the human interviewer. On average the children spoke 359 words to KASPAR and to the human interviewer 373 words (see graph C in <xref ref-type="fig" rid="pone-0059448-g003">Figure 3</xref>). In addition to this there was very little difference in the amount of words the children used relative to the amount of words the interviewer used (shown under proportionate word count in <xref ref-type="table" rid="pone-0059448-t001">Table 1</xref>).</p>
      <p>The number of filler words the children used was very similar in both conditions (see graph D in <xref ref-type="fig" rid="pone-0059448-g003">Figure 3</xref>). On average there was no difference in the amount of filler words the children used, and in both conditions on average 19 filler words were used. However, the number filler words used with KASPAR ranged from 2 words to 101 words, and with the human experimenter from 2 words to 63 words.</p>
      <p>There was also very little difference in the amount of keywords the children used with KASPAR compared to how many they used with the human interviewer (<xref ref-type="table" rid="pone-0059448-t002">Table 2</xref>). On average there was less than one word difference in how many keywords were used when talking to KASPAR compared to talking to a human, with the children using approximately 12 keywords on average in both conditions (see graph E in <xref ref-type="fig" rid="pone-0059448-g003">Figure 3</xref>). The number of keywords used with KASPAR ranged from 4 words to 22 words, and with the human interviewer from 2 words to 27 words. In addition to this there was very little difference between the categories (<xref ref-type="table" rid="pone-0059448-t002">Table 2</xref>).</p>
      <table-wrap id="pone-0059448-t002" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0059448.t002</object-id>
        <label>Table 2</label>
        <caption>
          <title>Key words (KASPAR vs. Human).</title>
        </caption>
        <alternatives>
          <graphic id="pone-0059448-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0059448.t002" xlink:type="simple"/>
          <table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td colspan="2" align="left" rowspan="1">KASPAR</td>
                <td colspan="2" align="left" rowspan="1">Human</td>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">Mean</td>
                <td align="left" rowspan="1" colspan="1">Range</td>
                <td align="left" rowspan="1" colspan="1">Mean</td>
                <td align="left" rowspan="1" colspan="1">Range</td>
                <td align="left" rowspan="1" colspan="1">Mean difference</td>
                <td align="left" rowspan="1" colspan="1">t</td>
                <td align="left" rowspan="1" colspan="1">p</td>
                <td align="left" rowspan="1" colspan="1">Confidence interval of the mean</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">Overall</td>
                <td align="left" rowspan="1" colspan="1">12</td>
                <td align="left" rowspan="1" colspan="1">4–22</td>
                <td align="left" rowspan="1" colspan="1">12</td>
                <td align="left" rowspan="1" colspan="1">2–27</td>
                <td align="left" rowspan="1" colspan="1">0.095</td>
                <td align="left" rowspan="1" colspan="1">0.122</td>
                <td align="left" rowspan="1" colspan="1">0.904</td>
                <td align="left" rowspan="1" colspan="1">−1.53–1.72</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">− Family members</td>
                <td align="left" rowspan="1" colspan="1">4</td>
                <td align="left" rowspan="1" colspan="1">0–12</td>
                <td align="left" rowspan="1" colspan="1">4</td>
                <td align="left" rowspan="1" colspan="1">0–11</td>
                <td align="left" rowspan="1" colspan="1">0.619</td>
                <td align="left" rowspan="1" colspan="1">1.41</td>
                <td align="left" rowspan="1" colspan="1">0.174</td>
                <td align="left" rowspan="1" colspan="1">−.30–1.54</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">− Judges names</td>
                <td align="left" rowspan="1" colspan="1">2</td>
                <td align="left" rowspan="1" colspan="1">0–4</td>
                <td align="left" rowspan="1" colspan="1">2</td>
                <td align="left" rowspan="1" colspan="1">0–2</td>
                <td align="left" rowspan="1" colspan="1">−0.19</td>
                <td align="left" rowspan="1" colspan="1">−0.608</td>
                <td align="left" rowspan="1" colspan="1">0.55</td>
                <td align="left" rowspan="1" colspan="1">−.84–.46</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">− Winners prizes</td>
                <td align="left" rowspan="1" colspan="1">1</td>
                <td align="left" rowspan="1" colspan="1">0–3</td>
                <td align="left" rowspan="1" colspan="1">1</td>
                <td align="left" rowspan="1" colspan="1">0–9</td>
                <td align="left" rowspan="1" colspan="1">−0.286</td>
                <td align="left" rowspan="1" colspan="1">−0.88</td>
                <td align="left" rowspan="1" colspan="1">0.389</td>
                <td align="left" rowspan="1" colspan="1">−.96–.39</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">− Winners names</td>
                <td align="left" rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">0–12</td>
                <td align="left" rowspan="1" colspan="1">6</td>
                <td align="left" rowspan="1" colspan="1">43497</td>
                <td align="left" rowspan="1" colspan="1">−0.048</td>
                <td align="left" rowspan="1" colspan="1">−0.062</td>
                <td align="left" rowspan="1" colspan="1">0.951</td>
                <td align="left" rowspan="1" colspan="1">−1.65–1.72</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>In our analysis we investigated the different response durations and response times for both the child and the interviewer (<xref ref-type="table" rid="pone-0059448-t003">Table 3</xref>). In particular we found that KASPAR took much longer to respond to the children than the human interviewer due to the technical limitations of the system. Throughout the full duration of the interviews KASPAR took an average of 1∶14 to respond to the children, while the human interviewer took an average of 20 seconds. However there is no significant difference in the time the interviewer and the child spend speaking. Therefore when calculating the children’s word count it was necessary to calculate this statistic relative to the interviewer word count. We found that proportionately the children spoke to both interviewers a similar amount relative to the interviewers word count (<xref ref-type="table" rid="pone-0059448-t001">Table 1</xref>).</p>
      <table-wrap id="pone-0059448-t003" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0059448.t003</object-id>
        <label>Table 3</label>
        <caption>
          <title>Response and speaking durations (KASPAR vs. Human).</title>
        </caption>
        <alternatives>
          <graphic id="pone-0059448-t003-3" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0059448.t003" xlink:type="simple"/>
          <table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td colspan="2" align="left" rowspan="1">KASPAR</td>
                <td colspan="2" align="left" rowspan="1">Human</td>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">Mean</td>
                <td align="left" rowspan="1" colspan="1">Range</td>
                <td align="left" rowspan="1" colspan="1">Mean</td>
                <td align="left" rowspan="1" colspan="1">Range</td>
                <td align="left" rowspan="1" colspan="1">Mean difference</td>
                <td align="left" rowspan="1" colspan="1">t</td>
                <td align="left" rowspan="1" colspan="1">p</td>
                <td align="left" rowspan="1" colspan="1">Confidence interval of the mean</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">Child response duration</td>
                <td align="left" rowspan="1" colspan="1">235.3</td>
                <td align="left" rowspan="1" colspan="1">96.28–472</td>
                <td align="left" rowspan="1" colspan="1">220.7</td>
                <td align="left" rowspan="1" colspan="1">97.88–618.7</td>
                <td align="left" rowspan="1" colspan="1">14.625</td>
                <td align="left" rowspan="1" colspan="1">0.577</td>
                <td align="left" rowspan="1" colspan="1">0.571</td>
                <td align="left" rowspan="1" colspan="1">−105.83</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Interviewer response duration</td>
                <td align="left" rowspan="1" colspan="1">56.2</td>
                <td align="left" rowspan="1" colspan="1">38.6–74.2</td>
                <td align="left" rowspan="1" colspan="1">54.42</td>
                <td align="left" rowspan="1" colspan="1">38.24–77.16</td>
                <td align="left" rowspan="1" colspan="1">1.803</td>
                <td align="left" rowspan="1" colspan="1">1.131</td>
                <td align="left" rowspan="1" colspan="1">0.271</td>
                <td align="left" rowspan="1" colspan="1">−6.65</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Response time child&gt;interviewer</td>
                <td align="left" rowspan="1" colspan="1">74.29</td>
                <td align="left" rowspan="1" colspan="1">22–137.6</td>
                <td align="left" rowspan="1" colspan="1">20</td>
                <td align="left" rowspan="1" colspan="1">9.04–35.8</td>
                <td align="left" rowspan="1" colspan="1">54.243</td>
                <td align="left" rowspan="1" colspan="1">8.865</td>
                <td align="left" rowspan="1" colspan="1">.000*</td>
                <td align="left" rowspan="1" colspan="1">41.479–67.007</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Response time interviewer&gt;child</td>
                <td align="left" rowspan="1" colspan="1">25.7</td>
                <td align="left" rowspan="1" colspan="1">9–61.2</td>
                <td align="left" rowspan="1" colspan="1">16.4</td>
                <td align="left" rowspan="1" colspan="1">4.8–68.4</td>
                <td align="left" rowspan="1" colspan="1">9.261</td>
                <td align="left" rowspan="1" colspan="1">2.659</td>
                <td align="left" rowspan="1" colspan="1">.015*</td>
                <td align="left" rowspan="1" colspan="1">1.997–16.526</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>In our analysis we checked for possible effects concerning the information given by the children in relation to the questions they were asked for all of the key points categories. Firstly, we compared the number of names given by the child for each category either to the human experimenter or the robot. No significant differences were found (t = −0.36; p = 0.72). Secondly, we investigated whether the names given to each interviewer were consistent in both conditions. To check this we compared the names the children gave to just the human interviewer, just the robot interviewer and both interviewers. No significant differences in the information the children provided were found, with a mean difference of 0.33 overall. (Details of the overall statistics can be found in <xref ref-type="table" rid="pone-0059448-t004">Table 4</xref>, whilst the details for each category are shown in <xref ref-type="table" rid="pone-0059448-t005">Table 5</xref>).</p>
      <table-wrap id="pone-0059448-t004" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0059448.t004</object-id>
        <label>Table 4</label>
        <caption>
          <title>Key Points - Names listed overall (KASPAR vs. Human).</title>
        </caption>
        <alternatives>
          <graphic id="pone-0059448-t004-4" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0059448.t004" xlink:type="simple"/>
          <table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td colspan="2" align="left" rowspan="1">KASPAR</td>
                <td colspan="2" align="left" rowspan="1">Human</td>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">Mean</td>
                <td align="left" rowspan="1" colspan="1">Range</td>
                <td align="left" rowspan="1" colspan="1">Mean</td>
                <td align="left" rowspan="1" colspan="1">Range</td>
                <td align="left" rowspan="1" colspan="1">Mean difference</td>
                <td align="left" rowspan="1" colspan="1">t</td>
                <td align="left" rowspan="1" colspan="1">p</td>
                <td align="left" rowspan="1" colspan="1">Confidence interval of the mean</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">All names listed total</td>
                <td align="left" rowspan="1" colspan="1">9.67</td>
                <td align="left" rowspan="1" colspan="1">4–21</td>
                <td align="left" rowspan="1" colspan="1">10</td>
                <td align="left" rowspan="1" colspan="1">3–20</td>
                <td align="left" rowspan="1" colspan="1">0.33</td>
                <td align="left" rowspan="1" colspan="1">−0.36</td>
                <td align="left" rowspan="1" colspan="1">0.72</td>
                <td align="left" rowspan="1" colspan="1">0.89</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Person names listed total</td>
                <td align="left" rowspan="1" colspan="1">1.76</td>
                <td align="left" rowspan="1" colspan="1">0–7</td>
                <td align="left" rowspan="1" colspan="1">1.48</td>
                <td align="left" rowspan="1" colspan="1">0–5</td>
                <td align="left" rowspan="1" colspan="1">0.28</td>
                <td align="left" rowspan="1" colspan="1">0.71</td>
                <td align="left" rowspan="1" colspan="1">0.49</td>
                <td align="left" rowspan="1" colspan="1">0.39</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Event names listed total</td>
                <td align="left" rowspan="1" colspan="1">7.9</td>
                <td align="left" rowspan="1" colspan="1">4–19</td>
                <td align="left" rowspan="1" colspan="1">8.52</td>
                <td align="left" rowspan="1" colspan="1">2–18</td>
                <td align="left" rowspan="1" colspan="1">0.62</td>
                <td align="left" rowspan="1" colspan="1">−0.81</td>
                <td align="left" rowspan="1" colspan="1">0.43</td>
                <td align="left" rowspan="1" colspan="1">0.75</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <table-wrap id="pone-0059448-t005" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0059448.t005</object-id>
        <label>Table 5</label>
        <caption>
          <title>Key Points - Specific categories (KASPAR vs. Human).</title>
        </caption>
        <alternatives>
          <graphic id="pone-0059448-t005-5" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0059448.t005" xlink:type="simple"/>
          <table>
            <colgroup span="1">
              <col align="left" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
              <col align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td colspan="2" align="left" rowspan="1">KASPAR</td>
                <td colspan="2" align="left" rowspan="1">Human</td>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" rowspan="1" colspan="1">Mean</td>
                <td align="left" rowspan="1" colspan="1">Range</td>
                <td align="left" rowspan="1" colspan="1">Mean</td>
                <td align="left" rowspan="1" colspan="1">Range</td>
                <td align="left" rowspan="1" colspan="1">Mean difference</td>
                <td align="left" rowspan="1" colspan="1">t</td>
                <td align="left" rowspan="1" colspan="1">p</td>
                <td align="left" rowspan="1" colspan="1">Confidence interval of the mean</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">Number of family members listedby relation</td>
                <td align="left" rowspan="1" colspan="1">2.86</td>
                <td align="left" rowspan="1" colspan="1">1–6</td>
                <td align="left" rowspan="1" colspan="1">2.19</td>
                <td align="left" rowspan="1" colspan="1">0–6</td>
                <td align="left" rowspan="1" colspan="1">0.67</td>
                <td align="left" rowspan="1" colspan="1">1.67</td>
                <td align="left" rowspan="1" colspan="1">0.109</td>
                <td align="left" rowspan="1" colspan="1">0.39</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Number of family members listedby name</td>
                <td align="left" rowspan="1" colspan="1">0.9</td>
                <td align="left" rowspan="1" colspan="1">0–6</td>
                <td align="left" rowspan="1" colspan="1">0.62</td>
                <td align="left" rowspan="1" colspan="1">0–3</td>
                <td align="left" rowspan="1" colspan="1">0.28</td>
                <td align="left" rowspan="1" colspan="1">0.95</td>
                <td align="left" rowspan="1" colspan="1">0.36</td>
                <td align="left" rowspan="1" colspan="1">0.29</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Number of pets listed</td>
                <td align="left" rowspan="1" colspan="1">3</td>
                <td align="left" rowspan="1" colspan="1">0–19</td>
                <td align="left" rowspan="1" colspan="1">5.14</td>
                <td align="left" rowspan="1" colspan="1">0–40</td>
                <td align="left" rowspan="1" colspan="1">2.14</td>
                <td align="left" rowspan="1" colspan="1">−1.11</td>
                <td align="left" rowspan="1" colspan="1">0.28</td>
                <td align="left" rowspan="1" colspan="1">1.88</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Number of pets listed by name</td>
                <td align="left" rowspan="1" colspan="1">0.86</td>
                <td align="left" rowspan="1" colspan="1">0–4</td>
                <td align="left" rowspan="1" colspan="1">0.86</td>
                <td align="left" rowspan="1" colspan="1">0–4</td>
                <td align="left" rowspan="1" colspan="1">0</td>
                <td align="left" rowspan="1" colspan="1">0</td>
                <td align="left" rowspan="1" colspan="1">1</td>
                <td align="left" rowspan="1" colspan="1">0.29</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Number of types of act listed</td>
                <td align="left" rowspan="1" colspan="1">1.67</td>
                <td align="left" rowspan="1" colspan="1">0–9</td>
                <td align="left" rowspan="1" colspan="1">1.67</td>
                <td align="left" rowspan="1" colspan="1">0–7</td>
                <td align="left" rowspan="1" colspan="1">0</td>
                <td align="left" rowspan="1" colspan="1">0</td>
                <td align="left" rowspan="1" colspan="1">1</td>
                <td align="left" rowspan="1" colspan="1">0.32</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Number of acts performing</td>
                <td align="left" rowspan="1" colspan="1">3.24</td>
                <td align="left" rowspan="1" colspan="1">1–9</td>
                <td align="left" rowspan="1" colspan="1">3.29</td>
                <td align="left" rowspan="1" colspan="1">1–8</td>
                <td align="left" rowspan="1" colspan="1">0.05</td>
                <td align="left" rowspan="1" colspan="1">−0.8</td>
                <td align="left" rowspan="1" colspan="1">0.94</td>
                <td align="left" rowspan="1" colspan="1">0.58</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Number of performing children named</td>
                <td align="left" rowspan="1" colspan="1">4.67</td>
                <td align="left" rowspan="1" colspan="1">1–13</td>
                <td align="left" rowspan="1" colspan="1">5.1</td>
                <td align="left" rowspan="1" colspan="1">2–12</td>
                <td align="left" rowspan="1" colspan="1">0.43</td>
                <td align="left" rowspan="1" colspan="1">−0.7</td>
                <td align="left" rowspan="1" colspan="1">0.51</td>
                <td align="left" rowspan="1" colspan="1">0.63</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Number of judges listed</td>
                <td align="left" rowspan="1" colspan="1">2.52</td>
                <td align="left" rowspan="1" colspan="1">1–5</td>
                <td align="left" rowspan="1" colspan="1">2.33</td>
                <td align="left" rowspan="1" colspan="1">0–6</td>
                <td align="left" rowspan="1" colspan="1">0.19</td>
                <td align="left" rowspan="1" colspan="1">0.64</td>
                <td align="left" rowspan="1" colspan="1">0.53</td>
                <td align="left" rowspan="1" colspan="1">0.29</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Number of judges listed by name</td>
                <td align="left" rowspan="1" colspan="1">1.48</td>
                <td align="left" rowspan="1" colspan="1">0–4</td>
                <td align="left" rowspan="1" colspan="1">1.52</td>
                <td align="left" rowspan="1" colspan="1">0–5</td>
                <td align="left" rowspan="1" colspan="1">0.04</td>
                <td align="left" rowspan="1" colspan="1">−0.18</td>
                <td align="left" rowspan="1" colspan="1">0.86</td>
                <td align="left" rowspan="1" colspan="1">0.26</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Winners prize stated</td>
                <td align="left" rowspan="1" colspan="1">0.86</td>
                <td align="left" rowspan="1" colspan="1">0–1</td>
                <td align="left" rowspan="1" colspan="1">0.86</td>
                <td align="left" rowspan="1" colspan="1">0–1</td>
                <td align="left" rowspan="1" colspan="1">0</td>
                <td align="left" rowspan="1" colspan="1">0</td>
                <td align="left" rowspan="1" colspan="1">1</td>
                <td align="left" rowspan="1" colspan="1">0</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Number of winners named</td>
                <td align="left" rowspan="1" colspan="1">1.76</td>
                <td align="left" rowspan="1" colspan="1">0–2</td>
                <td align="left" rowspan="1" colspan="1">1.9</td>
                <td align="left" rowspan="1" colspan="1">0–2</td>
                <td align="left" rowspan="1" colspan="1">0.14</td>
                <td align="left" rowspan="1" colspan="1">−0.83</td>
                <td align="left" rowspan="1" colspan="1">0.42</td>
                <td align="left" rowspan="1" colspan="1">0.17</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Poster activity stated</td>
                <td align="left" rowspan="1" colspan="1">0.38</td>
                <td align="left" rowspan="1" colspan="1">0–1</td>
                <td align="left" rowspan="1" colspan="1">0.48</td>
                <td align="left" rowspan="1" colspan="1">0–1</td>
                <td align="left" rowspan="1" colspan="1">0.1</td>
                <td align="left" rowspan="1" colspan="1">−0.81</td>
                <td align="left" rowspan="1" colspan="1">0.43</td>
                <td align="left" rowspan="1" colspan="1">0.11</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>The questionnaire results suggested that the only significant difference in how the children evaluated the interviews with KASPAR compared to the human was the average duration of the respective interviews. The children perceived that the interviews with KASPAR were longer (t = −2.364, p = 0.028*). It is also notable that the children seemed to find that the levels of difficulty talking to KASPAR or the human were similar (t = −0.204, p = 0.841), (see graph F in <xref ref-type="fig" rid="pone-0059448-g003">Figure 3</xref>).</p>
      <sec id="s4a">
        <title>Order Effects</title>
        <p>We also investigated the statistical effects of the order of the interviews. The results from this analysis revealed that there were no significant differences for the majority of measures. The only two measures (out of a total of 29 measures) where there were statistically significant differences were interviewer response duration (t = −2953, p = 0.008*) and the mention of the poster activity that the children took part in (t = 2.83, p = 0.01*). (Results of order effect analysis are shown in <xref ref-type="supplementary-material" rid="pone.0059448.s003">tables S1</xref>, <xref ref-type="supplementary-material" rid="pone.0059448.s004">S2</xref>, <xref ref-type="supplementary-material" rid="pone.0059448.s005">S3</xref>, <xref ref-type="supplementary-material" rid="pone.0059448.s006">S4</xref>, <xref ref-type="supplementary-material" rid="pone.0059448.s007">S5</xref>).</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Discussion</title>
      <sec id="s5a">
        <title>Findings</title>
        <p>The results from this study indicate that children were willing to interact with a robot in an interview scenario and did so in a similar way to how they interacted with a human interviewer. Furthermore, the amount of information that children provided to KASPAR was also very similar to the information they provided to the human. This was assessed by measuring the children’s use of keywords which we found to be similar in both the robot and human conditions. In addition, the analysis of the key points indicated that there were no significant differences in the information the children provided to KASPAR and the human interviewer. There were however statistically significant differences in both the duration of the interviews and the eye gaze toward the interviewer. The difference in the duration of the interviews can be explained by the additional time it took for the robot to respond, this was due to the technical limitations of the robot. In our data analysis we found that the robot took significantly longer to respond to the children and this is why the interviews with the robot took longer (<xref ref-type="table" rid="pone-0059448-t003">Table 3</xref>). To confirm this we also checked by combining the time that the children spent talking, the time that the interviewer spent talking and the time that the children took responding to the robot, and this result also confirmed that the additional time taken by the interviewing the children was due to the time it took the robot to respond. Potentially this could have influenced the results of the study if this delay had caused the children to feel a disconnection in the human-robot interaction experience. However, this is not supported by our results. Note, the robot would still blink periodically during the brief periods of delays, thus maintaining the visual appearance of movement and presence of the robot.</p>
        <p>In this study there was considerable variation in the durations of the interviews. This was due to the children all being very different in terms of how they spoke and how much information they gave. Some children were shy and would not talk much at all whilst others were very confident and would talk for a long time. Future investigations could study such individual differences in more depth, e.g. whether children’s personality traits influence their responses in interviews with a human and a robot. Previous studies have shown the influence of participants’ personality traits in human-robot interaction, e.g. <xref ref-type="bibr" rid="pone.0059448-Walters1">[40]</xref>, <xref ref-type="bibr" rid="pone.0059448-Woods1">[46]</xref>, <xref ref-type="bibr" rid="pone.0059448-Tapus1">[47]</xref>.</p>
        <p>The statistically significant difference in the durations of the interviews was due to the operation of the robot which can be confirmed from the results of the interviewer response durations (<xref ref-type="table" rid="pone-0059448-t003">Table 3</xref>). Getting KASPAR to respond to the children takes longer than it does for a human interviewer present in the room because finding the appropriate key to respond with takes longer, despite extensive training of the operator/experimenter prior to the experiment. The results show that children looked at KASPAR more than at the human (consistent with our expectations concerning RQ1), possibly because the robot was a novel object to the children and therefore they may have been more interested in KASPAR than the human interviewing them. Ascertaining that children will respond to a robot in an interview scenario as well as to a human is an important first step in establishing that robots could be a useful tool for interviewing children.</p>
        <p>The children’s verbal responses to were very similar in both conditions with regards to word count, filler words, key words and key points. Furthermore the children’s word count relative to the interviewers word count was similar. Both interviewers followed the same interview structure and asked the same questions. However, the interviewers are very different in terms of their nature (robot/human), so such a similarity in children’s responses in both conditions is very encouraging for developing robots as interviewing tools for children. Although the results from our study show that the children interacted with the robot in a similar manner to which they did with a human, and the information they provided is also similar, there are potential advantages a robot could have over a human interviewer. When the police are conducting interviews with children that have been through a stressful or traumatic ordeal it can be difficult for the human interviewer to maintain their composure without subtly and unintentionally indicating their thoughts and feelings. Sometimes the information that a child reveals in an interview can be quite shocking or surprising. The 2011 ABE states <italic>“the interviewer should not display surprise at information as this could be taken as a sign that the information is incorrect” </italic><xref ref-type="bibr" rid="pone.0059448-UK5">[<italic>48</italic>]</xref> This can be quite difficult for a human interviewer but would be easy for a robot whose expressions are explicitly controlled, and this is one of the reasons why a robotic interviewer may have an advantage over a human interviewer in certain situations. It is also important that the interviewer does not appear to assume that someone is guilty <italic>“So far as possible, the interview should be conducted in a ‘neutral’ atmosphere, with the interviewer taking care not to assume, or appear to assume, the guilt of an individual whose alleged conduct may be the subject of the interview” </italic><xref ref-type="bibr" rid="pone.0059448-UK6">[<italic>49</italic>]</xref>. Using a robot to interview a person could eliminate any of the subtle unintentional signs in body language that a human interviewer may give away, while the body language of the robot can be fully and precisely controlled by the interviewer. In addition to this the ABE states <italic>“research shows that a person’s perceived authority can have an adverse effect on the witness, especially with respect to suggestibility” </italic><xref ref-type="bibr" rid="pone.0059448-UK7">[<italic>50</italic>]</xref><italic>.</italic> Using a small child sized robot could potentially eliminate this problem because the robot is clearly not an adult and may not be viewed in the same way.</p>
        <p>The children’s similar use of filler words may indicate that the children found talking to KASPAR very similar to talking to the human in terms of comfort. In some respects measuring filler words could provide a better indicator of a child’s comfort in a particular situation than a word count. The questions in the interview were focused on an event that took place on one particular day and the interviews were one week apart therefore the amount the children would remember would inevitably change. The amount of filler words the children used is likely to be more consistent with the child’s level of comfort and the number of questions asked. Some research investigating linguistic disfluencies suggests that the use of filler words could be linked to the difficulty of planning what to say <xref ref-type="bibr" rid="pone.0059448-Boomer1">[51]</xref>, <xref ref-type="bibr" rid="pone.0059448-Shriberg1">[52]</xref>. Whereas other research suggests that filler words my serve a communicative function to help coordinate linguistic interactions <xref ref-type="bibr" rid="pone.0059448-Brennan1">[53]</xref>, for example, fillers may be used so an individual is not interrupted before they can speak their next sentence <xref ref-type="bibr" rid="pone.0059448-Clark1">[54]</xref>, <xref ref-type="bibr" rid="pone.0059448-Clark2">[55]</xref>. There is also some evidence showing that an increased number of fillers and longer pauses occur before an uncertain answer is given <xref ref-type="bibr" rid="pone.0059448-Brennan2">[56]</xref>, <xref ref-type="bibr" rid="pone.0059448-Smith1">[57]</xref>. High disfluency has been associated with anxiety <xref ref-type="bibr" rid="pone.0059448-Mahl1">[58]</xref>. The children’s equal use of filler words in the present experiment may reflect that their comfort levels were the same with both interview partners.</p>
        <p>Our analysis of the key points revealed that in our experiment there were no significant differences in the information the children provided to a robot compared to a human interviewer. However the analysis of the key points for each category does show that the questions in the interviews varied in difficulty. For example the children consistently named the winners of the event but often name fewer judges, even though there were more judges than winners (<xref ref-type="table" rid="pone-0059448-t005">Table 5</xref>). This highlights that the questions in these interviews varied in difficulty.</p>
        <p>We found no significant differences in the amount the children spoke to KASPAR, the number of keywords the children used with KASPAR, or the amount of key points the children revealed to KASPAR, compared to the human (contrary to our expectations concerning RQ2 which expected clear preferences either towards the robot or the human interviewer). However, this finding is very encouraging for the future use of robots, as it could be interpreted in such a way that children actually make no difference between human and robot interviewers in this respect and that therefore robot interviewers (i.e. robots as interviewing tools in the hands of experts remotely conducting the interview via the robot) could, with appropriate adjustments, be used as a valuable complement in interviews e.g. with social services and police.</p>
        <p>Concerning the effect of the order of the experimental conditions, only two of the twenty-nine measures contained statistically significant differences, these were the interviewer response duration and the number of children that remembered and stated that they had taken part in a poster making activity (<xref ref-type="supplementary-material" rid="pone.0059448.s005">Tables S3</xref>, <xref ref-type="supplementary-material" rid="pone.0059448.s007">S5</xref>). It is likely that the additional time in the response duration of the interviewers is because over time the lead investigator became more comfortable and used to the interview scenario and as a result took more time responding in the later stages of the study. Although there was a statistically significant difference the mean difference is only 4.05 seconds and does not appear to have affected the interactions or the results of the study. The results of the poster activity reveal that there was a significant difference in the number of children that remembered and stated taking part in the poster making activity. The results show that more children stated taking part in a poster activity in the first phase of the interviews than the second. This is possibly because the poster activity was not the main focal point of the event and the questions in the interviews did not focus on this aspect of the event.</p>
        <p>Generally, the findings from this study are consistent with the HRI literature as the children were happy to talk to and interact with KASPAR. The increased levels of eye gaze also suggest that the children were very interested in KASPAR. This study confirms and builds on the findings of the study by Bethel <italic>et al.</italic> <xref ref-type="bibr" rid="pone.0059448-Bethel1">[21]</xref> which found that children are equally likely to share a ‘secret’, or other valuable information, with a robot as they are a human. The context of the interaction and age ranges slightly differ in the two studies but the basic concept of children talking to a robot is the same.</p>
      </sec>
      <sec id="s5b">
        <title>Limitations of Study</title>
        <p>Concerning limitations of this study, all interviews were conducted with children attending the same school. Future work could consider schools in different geographical locations or different socio-economic status, or children with different ages. In this study we did not assess the children’s degree of introversion or extroversion. In future studies it may be useful to establish these characteristics of the child’s personality and see if this affects how the children respond to a robot compared to a human. The questions used in our interviews were based around a topic of which all the children had very different perceptions. For example some of the children took part in the audition for the event, some took part in the event, two of the children actually won the event as a pair, whilst many of the children only watched the event. This difference in perception would have affected the children’s responses although it would not have changed between the interviewers. Another limitation of the study is that the information the children were disclosing was not a ‘personal secret’ and there was no incentive for the child to keep anything from the interviewer. If the children had an invested interest in keeping information from KASPAR, or if the information had been of a more sensitive nature the results between the human and the robot may have been different. However conducting a study that focuses on questions of a personal matter could be intrusive and would be morally questionable. Our long-term goal is therefore to develop KASPAR further as a tool for practitioners, such as members of the police or social services, rather than conducting such studies ourselves. Such future studies ‘in the field’ are necessary to confirm the results obtained in the present study. The results from this study provide preliminary evidence that robots could be useful tools for interviewing children, and further investigative work needs to be carried out to confirm these results. In future studies it may also be useful to ask additional questions at the end of the interviews that could capture the children's subjective feelings about the experience of the interview with the interviewer to provide a detailed qualitative dimension. Our research has focused on a short-term one-off interview scenario rather than investigating long-term child-robot interactions. This is because our target application area is often a novel one-off situation and children generally do not have interviews on a regular basis, therefore, long-term child-robot interactions are less relevant in our target application domain. However future research could investigate the long-term effects for other potential interview applications, e.g. in a medical or educational context. If robots were to be used in these contexts it would be important to address questions such as: will the children’s behaviour differ if they are interviewed by the robot on a regular basis, and will their interest in and their co-operation with the robot decline due to the wearing off of the novelty effect?</p>
      </sec>
      <sec id="s5c">
        <title>Summary of Hypotheses and Implications</title>
        <p>This study investigated the difference in how children responded to a robot compared to a human in an interview scenario.</p>
        <p>RQ1: Our expectations were supported, with the children showing significantly more eye gaze directed towards the robot’s face than the human interviewer.</p>
        <p>RQ2: The results were contrary to our expectations. Rather than having a clear preference, the children behaved very similarly towards either of the interviewers (human/robot). The children used similar amounts of words, keywords and filler words when responding to both the robot and the human interviewer. There was also very little difference in the amount of words the children used relative to the amount of words the interviewer used. These findings illustrate that the children communicated with the robot in a similar way to which they did the human interviewer.</p>
        <p>This study has investigated how children respond to a robot in an interview scenario compared to a human. Our results have shown that children do respond to robots in a similar way in which they respond to a human in an interview scenario. This is important because these findings can help to uncover potential advantages a robot may have over a human interviewer, for example for use by the police or social services.</p>
      </sec>
      <sec id="s5d">
        <title>Future Work</title>
        <p>This study provides strong support for continuing the research direction of using robots in an interview scenario with young children. Further research needs to be conducted to investigate if the responses of children vary more when they have an invested interest in keeping information from the interviewer or when they are asked questions of varying difficulty or a more sensitive nature. Our next step will be to conduct a study which will investigate how children respond to questions of varying difficulty from both a human and robotic interviewer. In addition, the capabilities of KASPAR need to be enhanced to maximise the robots potential and freedom of the interactions in terms of the ability to ask a larger variety of questions rather than pre-set questions. Apart from using robot-mediated interviews in police or social services’ investigations, other potential application areas include medical contexts (e.g. finding out about the child’s medical problems), or school contexts (e.g. when teachers try to find out details about instances involving bullying or violent behaviour). Further studies investigating robot-mediated interviews that focus on questions of a more personal and sensitive nature would need to be conducted under the expertise and guidance of a specialist interviewer.</p>
      </sec>
    </sec>
    <sec id="s6">
      <title>Supporting Information</title>
      <supplementary-material id="pone.0059448.s001" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pone.0059448.s001" position="float" xlink:type="simple">
        <label>Figure S1</label>
        <caption>
          <p>Interview questions.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pone.0059448.s002" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pone.0059448.s002" position="float" xlink:type="simple">
        <label>Figure S2</label>
        <caption>
          <p>Questionnaire.</p>
          <p>(PDF)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pone.0059448.s003" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" xlink:href="info:doi/10.1371/journal.pone.0059448.s003" position="float" xlink:type="simple">
        <label>Table S1</label>
        <caption>
          <p>Overall interaction metrics (Phase 1 vs. Phase 2).</p>
          <p>(DOCX)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pone.0059448.s004" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" xlink:href="info:doi/10.1371/journal.pone.0059448.s004" position="float" xlink:type="simple">
        <label>Table S2</label>
        <caption>
          <p>Key words (Phase 1 vs. Phase 2).</p>
          <p>(DOCX)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pone.0059448.s005" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" xlink:href="info:doi/10.1371/journal.pone.0059448.s005" position="float" xlink:type="simple">
        <label>Table S3</label>
        <caption>
          <p>Response and speaking durations (Phase 1 vs. Phase 2).</p>
          <p>(DOCX)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pone.0059448.s006" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" xlink:href="info:doi/10.1371/journal.pone.0059448.s006" position="float" xlink:type="simple">
        <label>Table S4</label>
        <caption>
          <p>Key Points - Names listed overall (Phase 1 vs. Phase 2).</p>
          <p>(DOCX)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pone.0059448.s007" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" xlink:href="info:doi/10.1371/journal.pone.0059448.s007" position="float" xlink:type="simple">
        <label>Table S5</label>
        <caption>
          <p>Key Points - Specific categories (Phase 1 vs. Phase 2).</p>
          <p>(DOCX)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>The authors would like to thank Iolanda Iacono for providing assistance during the experiments in the school, and Chris Miller for help with the data coding. We would also like to thank the staff and children of the school who participated in this work, and two specialist professionals from the Metropolitan Police Ann Stuart and Toran Whybrow who provided input to this research.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pone.0059448-Kerepesi1">
        <label>1</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kerepesi</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Kubinyi</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Jonsson</surname><given-names>GK</given-names></name>, <name name-style="western"><surname>Magnusson</surname><given-names>MS</given-names></name>, <name name-style="western"><surname>Miklosi</surname><given-names>A</given-names></name> (<year>2006</year>) <article-title>Behavioral comparison of human-animal (dog) and human-robot (AIBO) interactions</article-title>. <source>Behavioural Processes</source> <volume>73</volume>: <fpage>92</fpage>–<lpage>99</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Melson1">
        <label>2</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Melson</surname><given-names>GF</given-names></name>, <name name-style="western"><surname>Kahn</surname><given-names>PHJ</given-names></name>, <name name-style="western"><surname>Beck</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Friedman</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Roberts</surname><given-names>T</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Children’s behavior toward and understanding of robotic and living dogs</article-title>. <source>Journal of Applied Developmental Psychology</source> <volume>30</volume>: <fpage>92</fpage>–<lpage>102</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Barlett1">
        <label>3</label>
        <mixed-citation publication-type="other" xlink:type="simple">Barlett B, Estivill-Castro V, Seymon S (2004) Dogs or robots–Why do children see them as robotic pets rather than canine machines? 5th Australasian User Interface Conference (AUIC2004). Dunedin.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Kanda1">
        <label>4</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kanda</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Hirano</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Eaton</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Ishiguro</surname><given-names>H</given-names></name> (<year>2004</year>) <article-title>Interactive robots as social partners and peer tutors for children: A field trial. Human-Computer Interaction</article-title>. <volume>19</volume>: <fpage>61</fpage>–<lpage>84</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Tanaka1">
        <label>5</label>
        <mixed-citation publication-type="other" xlink:type="simple">Tanaka F, Ghosh M (2011) The implementation of care-receiving robot at an English learning school for children; 2011 March 6–9; Lausanne, Switzerland.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Saldien1">
        <label>6</label>
        <mixed-citation publication-type="other" xlink:type="simple">Saldien J, Goris K, Lefeber D (2006) ANTY: the Development of an Intelligent Huggable Robot for Hospitalized Children. CLAWAR.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Saldien2">
        <label>7</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Saldien</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Goris</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Yilmazyildiz</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Verhelst</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Lefeber</surname><given-names>D</given-names></name> (<year>2008</year>) <article-title>On the design of the huggable robot probo</article-title>. <source>Journal of Physical Agents</source> <volume>2</volume>: <fpage>3</fpage>–<lpage>12</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Goris1">
        <label>8</label>
        <mixed-citation publication-type="other" xlink:type="simple">Goris K, Saldien J, Lefeber D (2009) Probo: a testbed for human robot interaction; 2009 March 11–13; La Jolla, California, USA.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Dautenhahn1">
        <label>9</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dautenhahn</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Nehaniv</surname><given-names>CL</given-names></name>, <name name-style="western"><surname>Walters</surname><given-names>ML</given-names></name>, <name name-style="western"><surname>Robins</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Kose-Bagci</surname><given-names>H</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>KASPAR: A minimally expressive humanoid robot for human–robot interaction research</article-title>. <source>Applied Bionics and Biomechanics</source> <volume>6</volume>: <fpage>369</fpage>–<lpage>397</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Robins1">
        <label>10</label>
        <mixed-citation publication-type="other" xlink:type="simple">Robins B, Dautenhahn K, Dickerson P (2009) From isolation to communication: A case study evaluation of robot assisted play for children with autism with a minimally expressive humanoid robot; 2009 February 1–7; Cancun, Mexico. IEEE Computer Society Press. pp. 205–211.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Robins2">
        <label>11</label>
        <mixed-citation publication-type="other" xlink:type="simple">Robins B, Dautenhahn K, Nehaniv CL, Mirza NA, Francois D, <etal>et al</etal>.. (2005) Sustaining interaction dynamics and engagement in dyadic child-robot interaction kinesics: Lessons learnt from an exploratory study; 2005.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Kozima1">
        <label>12</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kozima</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Michalowski</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Nakagawa</surname><given-names>C</given-names></name> (<year>2009</year>) <article-title>Keepon: A Playful Robot for Research, Therapy, and Entertainment</article-title>. <source>International Journal of Social Robotics</source> <volume>1</volume>: <fpage>3</fpage>–<lpage>18</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Dautenhahn2">
        <label>13</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dautenhahn</surname><given-names>K</given-names></name> (<year>2007</year>) <article-title>Socially intelligent robots: dimensions of human–robot interaction</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source> <volume>362</volume>: <fpage>679</fpage>–<lpage>704</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-KoseBagci1">
        <label>14</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kose-Bagci</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Ferrari</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Dautenhahn</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Syrdal</surname><given-names>DS</given-names></name>, <name name-style="western"><surname>Nehaniv</surname><given-names>CL</given-names></name> (<year>2009</year>) <article-title>Effects of Embodiment and Gestures on Social Interaction in Drumming Games with a Humanoid Robot</article-title>. <source>Advanced Robotics</source> <volume>23</volume>: <fpage>1951</fpage>–<lpage>1996</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Robins3">
        <label>15</label>
        <mixed-citation publication-type="other" xlink:type="simple">Robins B, Dautenhahn K, Te Boekhorst R, Nehaniv CL (2008) Behaviour delay and robot expressiveness in child-robot interactions: a user study on interaction kinesics; 2008. ACM. pp. 17–24.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Wainer1">
        <label>16</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wainer</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Dautenhahn</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Robins</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Amirabdollahian</surname><given-names>F</given-names></name> (<year>2010</year>) <article-title>Collaborating with Kaspar: Using an autonomous humanoid robot to foster cooperative dyadic play among children with autism; 2010 6–8 Dec</article-title>. <volume>2010</volume> pp. <fpage>631</fpage>–<lpage>638</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Docherty1">
        <label>17</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Docherty</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Sandelowski</surname><given-names>M</given-names></name> (<year>1999</year>) <article-title>Focus on Qualitative Methods Interviewing Children</article-title>. <source>Research in Nursing &amp; Health</source> <volume>22</volume>: <fpage>177</fpage>–<lpage>185</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-UK1">
        <label>18</label>
        <mixed-citation publication-type="other" xlink:type="simple">UK Government (2007) Achieving Best Evidence in Criminal Proceedings: Guidance on Interviewing Victims and Witnesses, and Using Special Measures. Home Office Criminal Justice System. pp. 1–34.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Spratling1">
        <label>19</label>
        <mixed-citation publication-type="other" xlink:type="simple">Spratling R, Coke S, Minick P (2010) Qualitative data collection with children. Applied Nursing Research.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Roebers1">
        <label>20</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roebers</surname><given-names>CM</given-names></name>, <name name-style="western"><surname>Schneider</surname><given-names>W</given-names></name> (<year>2002</year>) <article-title>Stability and consistency of children’s event recall</article-title>. <source>Cognitive Development</source> <volume>17</volume>: <fpage>1085</fpage>–<lpage>1103</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Bethel1">
        <label>21</label>
        <mixed-citation publication-type="other" xlink:type="simple">Bethel CL, Stevenson MR, Scassellati B (2011) Sharing a Secret: Interactions Between a Child, Robot, and Adult; 2011; Lausanne, Switzerland.</mixed-citation>
      </ref>
      <ref id="pone.0059448-NAO1">
        <label>22</label>
        <mixed-citation publication-type="other" xlink:type="simple">NAO Robot (2011) Aldebaran Robotics. Available: <ext-link ext-link-type="uri" xlink:href="http://www.aldebaran-robotics.com/" xlink:type="simple">http://www.aldebaran-robotics.com/</ext-link>. Accessed 2011 Dec 27.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Berlyn1">
        <label>23</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berlyn</surname><given-names>DE</given-names></name> (<year>1958</year>) <article-title>The influence of complexity and novelty in figures on orienting responses</article-title>. <source>Journal of Experimental Psychology</source> <volume>55</volume>: <fpage>289</fpage>–<lpage>296</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Scheef1">
        <label>24</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scheef</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Pinto</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Rahardja</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Snibbe</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Tow</surname><given-names>R</given-names></name> (<year>2002</year>) <article-title>Experiences with Sparky, a social robot</article-title>. <source>Socially Intelligent Agents, Multiagent Systems, Artificial Societies, And Simulated Organizations</source> <volume>3</volume>: <fpage>173</fpage>–<lpage>180</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Fior1">
        <label>25</label>
        <mixed-citation publication-type="other" xlink:type="simple">Fior M, Nugent S, Beran TN, Ramirez-Serrano A, Kuzyk R (2010) Children’s Relationships with Robots: Robot is Child’s New Friend. Journal of Physical Agents 4.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Nishio1">
        <label>26</label>
        <mixed-citation publication-type="other" xlink:type="simple">Nishio S, Ishiguro H, Anderson M, Hagita N (2007) Representing Personal Presence with a Teleoperated Android: A Case Study with Family. Association for the Advancement of Artificial Intelligence.</mixed-citation>
      </ref>
      <ref id="pone.0059448-MacDorman1">
        <label>27</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>MacDorman</surname><given-names>KF</given-names></name>, <name name-style="western"><surname>Ishiguro</surname><given-names>H</given-names></name> (<year>2006</year>) <article-title>The uncanny advantage of using androids in cognitive and social science research</article-title>. <source>Interaction Studies</source> <volume>7</volume>: <fpage>297</fpage>–<lpage>337</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-BeckerAsano1">
        <label>28</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Becker-Asano</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Ogawa</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Nishio</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Ishiguro</surname><given-names>H</given-names></name> (<year>2010</year>) <article-title>Exploring the uncanny valley with Geminoid HI-1 in a real-world application</article-title>. <volume>2010</volume> pp. <fpage>121</fpage>–<lpage>128</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Shahid1">
        <label>29</label>
        <mixed-citation publication-type="other" xlink:type="simple">Shahid S, Krahmer E, Swerts M (2010) Playing with iCat: investigating children’s appreciation of game plays with a social robot; 2010 17-NOV; Taipei, Taiwan.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Shahid2">
        <label>30</label>
        <mixed-citation publication-type="other" xlink:type="simple">Shahid S, Krahmer E, Swerts M, Mubin O (2010) Child-robot interaction during collaborative game play: effects of age and gender on emotion and experience; 2010 November 22–26; Brisbane, Australia.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Leite1">
        <label>31</label>
        <mixed-citation publication-type="other" xlink:type="simple">Leite I, Martinho C, Pereira A, Paiva A (2008) iCat: an Affective Game Buddy Based on Anticipatory Mechanisms (Short Paper); 2008 May 12–16; Estoril, Portugal. pp. 1229–1232.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Shahid3">
        <label>32</label>
        <mixed-citation publication-type="other" xlink:type="simple">Shahid S, Krahmer E, Swerts M (2011) Child-robot Interaction: Playing Alone or Together. CHI 2011. Vancouver, BC, Canada.</mixed-citation>
      </ref>
      <ref id="pone.0059448-UK2">
        <label>33</label>
        <mixed-citation publication-type="other" xlink:type="simple">UK Government (2011) Achieving Best Evidence in Criminal Proceedings: Guidance on Interviewing Victims and Witnesses, and Using Special Measures. Home Office Criminal Justice System.</mixed-citation>
      </ref>
      <ref id="pone.0059448-KoseBagci2">
        <label>34</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kose-Bagci</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Dautenhahn</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Syrdal</surname><given-names>DS</given-names></name>, <name name-style="western"><surname>Nehaniv</surname><given-names>CL</given-names></name> (<year>2010</year>) <article-title>Drum-mate: interaction dynamics and gestures in human–humanoid drumming experiments</article-title>. <source>Connection Science</source> <volume>22</volume>: <fpage>103</fpage>–<lpage>134</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Robins4">
        <label>35</label>
        <mixed-citation publication-type="other" xlink:type="simple">Robins B, Ferrari E, Dautenhahn K (2008) Developing Scenarios for Robot Assisted Play. 17th IEEE International Workshop on Robot and Human Interactive Communication - RO-MAN 2008, Munich, Germany, 1–3 August. pp. 180–186.</mixed-citation>
      </ref>
      <ref id="pone.0059448-KoseBagci3">
        <label>36</label>
        <mixed-citation publication-type="other" xlink:type="simple">Kose-Bagci H, Dautenhahn K, Syrdal DS, Nehaniv CL (2007) Drum-mate: A Human-Humanoid Drumming Experience; 2007 November 29-December 1; Pittsburgh, Pennsylvania, USA.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Gould1">
        <label>37</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gould</surname><given-names>JD</given-names></name>, <name name-style="western"><surname>Conti</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Hovanyecz</surname><given-names>T</given-names></name> (<year>1983</year>) <article-title>Composing letters with a simulated listening typewriter</article-title>. <source>Commun ACM</source> <volume>26</volume>: <fpage>295</fpage>–<lpage>308</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Dahlbck1">
        <label>38</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dahlbäck</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Jönsson</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Ahrenberg</surname><given-names>L</given-names></name> (<year>1993</year>) <article-title>Wizard of Oz studies – why and how</article-title>. <source>Knowledge-Based Systems</source> <volume>6</volume>: <fpage>258</fpage>–<lpage>266</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Green1">
        <label>39</label>
        <mixed-citation publication-type="other" xlink:type="simple">Green A, Huttenrauch H, Eklundh KS (2004) Applying the Wizard-of-Oz framework to cooperative service discovery and configuration; 2004. IEEE. pp. 575–580.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Walters1">
        <label>40</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Walters</surname><given-names>ML</given-names></name>, <name name-style="western"><surname>Syrdal</surname><given-names>DS</given-names></name>, <name name-style="western"><surname>Dautenhahn</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Te Boekhorst</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Koay</surname><given-names>KL</given-names></name> (<year>2008</year>) <article-title>Avoiding the uncanny valley: robot appearance, personality and consistency of behavior in an attention-seeking home scenario for a robot companion</article-title>. <source>Autonomous Robots</source> <volume>24</volume>: <fpage>159</fpage>–<lpage>178</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Goetz1">
        <label>41</label>
        <mixed-citation publication-type="other" xlink:type="simple">Goetz J, Kiesler S, Powers A (2003) Matching robot appearance and behavior to tasks to improve human-robot cooperation; 2003. IEEE. pp. 55–60.</mixed-citation>
      </ref>
      <ref id="pone.0059448-UK3">
        <label>42</label>
        <mixed-citation publication-type="other" xlink:type="simple">UK Government (2007) Achieving Best Evidence in Criminal Proceedings: Guidance on Interviewing Victims and Witnesses, and Using Special Measures. Home Office Criminal Justice System. 27.</mixed-citation>
      </ref>
      <ref id="pone.0059448-UK4">
        <label>43</label>
        <mixed-citation publication-type="other" xlink:type="simple">UK Government (2011) Achieving Best Evidence in Criminal Proceedings: Guidance on Interviewing Victims and Witnesses, and Using Special Measures. Home Office Criminal Justice System. 49, 56, 229.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Observer1">
        <label>44</label>
        <mixed-citation publication-type="other" xlink:type="simple">Observer XT (2011) Noldus. Available: <ext-link ext-link-type="uri" xlink:href="http://www.noldus.com/" xlink:type="simple">http://www.noldus.com/</ext-link>. Accessed 2011 Dec 28.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Landis1">
        <label>45</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Landis</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Koch</surname><given-names>GG</given-names></name> (<year>1977</year>) <article-title>The Measurement of Observer Agreement for Categorical Data</article-title>. <source>Biometrics</source> <volume>33</volume>: <fpage>159</fpage>–<lpage>174</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Woods1">
        <label>46</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Woods</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Dautenhahn</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Kaouri</surname><given-names>C</given-names></name>, <name name-style="western"><surname>te Boekhorst</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Koay</surname><given-names>KL</given-names></name>, <etal>et al</etal>. (<year>2007</year>) <article-title>Are robots like people?: Relationships between participant and robot personality traits in humanrobot interaction studies</article-title>. <source>Interaction Studies</source> <volume>8</volume>: <fpage>281</fpage>–<lpage>305</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Tapus1">
        <label>47</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tapus</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Ţăpuş</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Matarić</surname><given-names>MJ</given-names></name> (<year>2008</year>) <article-title>User–robot personality matching and assistive robot behavior adaptation for post-stroke rehabilitation therapy</article-title>. <source>Intelligent Service Robotics</source> <volume>1</volume>: <fpage>169</fpage>–<lpage>183</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-UK5">
        <label>48</label>
        <mixed-citation publication-type="other" xlink:type="simple">UK Government (2011) Achieving Best Evidence in Criminal Proceedings: Guidance on Interviewing Victims and Witnesses, and Using Special Measures. Home Office Criminal Justice System. pp. 196.</mixed-citation>
      </ref>
      <ref id="pone.0059448-UK6">
        <label>49</label>
        <mixed-citation publication-type="other" xlink:type="simple">UK Government (2011) Achieving Best Evidence in Criminal Proceedings: Guidance on Interviewing Victims and Witnesses, and Using Special Measures. Home Office Criminal Justice System. pp. 66.</mixed-citation>
      </ref>
      <ref id="pone.0059448-UK7">
        <label>50</label>
        <mixed-citation publication-type="other" xlink:type="simple">UK Government (2011) Achieving Best Evidence in Criminal Proceedings: Guidance on Interviewing Victims and Witnesses, and Using Special Measures. Home Office Criminal Justice System. pp. 56.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Boomer1">
        <label>51</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boomer</surname><given-names>DS</given-names></name> (<year>1965</year>) <article-title>Hesitation and grammatical encoding</article-title>. <source>Language and speech</source> <volume>8</volume>: <fpage>148</fpage>–<lpage>158</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Shriberg1">
        <label>52</label>
        <mixed-citation publication-type="other" xlink:type="simple">Shriberg EE, Lickley RJ (1992) Intonation of clause-internal filled pauses; 1992.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Brennan1">
        <label>53</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brennan</surname><given-names>SE</given-names></name>, <name name-style="western"><surname>Schober</surname><given-names>MF</given-names></name> (<year>2001</year>) <article-title>How listeners compensate for disfluencies in spontaneous speech</article-title>. <source>Journal of Memory and Language</source> <volume>44</volume>: <fpage>274</fpage>–<lpage>296</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Clark1">
        <label>54</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clark</surname><given-names>HH</given-names></name>, <name name-style="western"><surname>Brennan</surname><given-names>SE</given-names></name> (<year>1991</year>) <article-title>Grounding in communication</article-title>. <source>Perspectives on socially shared cognition</source> <volume>13</volume>: <fpage>127</fpage>–<lpage>149</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Clark2">
        <label>55</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clark</surname><given-names>HH</given-names></name> (<year>1994</year>) <article-title>Managing problems in speaking</article-title>. <source>Speech communication</source> <volume>15</volume>: <fpage>243</fpage>–<lpage>250</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Brennan2">
        <label>56</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brennan</surname><given-names>SE</given-names></name>, <name name-style="western"><surname>Williams</surname><given-names>M</given-names></name> (<year>1995</year>) <article-title>The Feeling of Another’s Knowing: Prosody and Filled Pauses as Cues to Listeners about the Metacognitive States of Speakers</article-title>. <source>Journal of Memory and Language</source> <volume>34</volume>: <fpage>383</fpage>–<lpage>398</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Smith1">
        <label>57</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname><given-names>VL</given-names></name>, <name name-style="western"><surname>Clark</surname><given-names>HH</given-names></name> (<year>1993</year>) <article-title>On the course of answering questions</article-title>. <source>Journal of Memory and Language</source> <volume>32</volume>: <fpage>25</fpage>–<lpage>25</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0059448-Mahl1">
        <label>58</label>
        <mixed-citation publication-type="other" xlink:type="simple">Mahl GF (1987) Explorations in nonverbal and vocal behavior: Lawrence Erlbaum Associates, Inc.</mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>