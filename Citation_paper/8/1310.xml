<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-01468</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003939</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject></subj-group></subj-group><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject></subj-group></subj-group><subj-group><subject>Cognitive psychology</subject></subj-group></subj-group><subj-group><subject>Learning and memory</subject></subj-group></subj-group><subj-group><subject>Psychology</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Social sciences</subject></subj-group></article-categories>
<title-group>
<article-title>Statistical Computations Underlying the Dynamics of Memory Updating</article-title>
<alt-title alt-title-type="running-head">Statistics of Memory Updating</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Gershman</surname><given-names>Samuel J.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Radulescu</surname><given-names>Angela</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Norman</surname><given-names>Kenneth A.</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Niv</surname><given-names>Yael</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachussetts, United States of America</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Department of Psychology and Princeton Neuroscience Institute, Princeton University, Princeton, New Jersey, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Sporns</surname><given-names>Olaf</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Indiana University, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">sjgershm@mit.edu</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: SJG KAN YN. Performed the experiments: SJG AR. Analyzed the data: SJG. Contributed reagents/materials/analysis tools: SJG. Wrote the paper: SJG AR KAN YN.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>11</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>6</day><month>11</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>11</issue>
<elocation-id>e1003939</elocation-id>
<history>
<date date-type="received"><day>19</day><month>8</month><year>2013</year></date>
<date date-type="accepted"><day>26</day><month>9</month><year>2014</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Gershman et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Psychophysical and neurophysiological studies have suggested that memory is not simply a carbon copy of our experience: Memories are modified or new memories are formed depending on the dynamic structure of our experience, and specifically, on how gradually or abruptly the world changes. We present a statistical theory of memory formation in a dynamic environment, based on a nonparametric generalization of the switching Kalman filter. We show that this theory can qualitatively account for several psychophysical and neural phenomena, and present results of a new visual memory experiment aimed at testing the theory directly. Our experimental findings suggest that humans can use temporal discontinuities in the structure of the environment to determine when to form new memory traces. The statistical perspective we offer provides a coherent account of the conditions under which new experience is integrated into an old memory versus forming a new memory, and shows that memory formation depends on inferences about the underlying structure of our experience.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>When do we modify old memories, and when do we create new ones? We suggest that this question can be answered statistically: The parsing of experience into distinct memory traces corresponds to inferences about the underlying structure of the environment. When sensory data change gradually over time, the brain infers that the environment has slowly been evolving, and the current representation of the environment (an existing memory trace) is updated. In contrast, abrupt changes indicate transitions between different structures, leading to the formation of new memories. While these ideas fall naturally out of statistical models of learning, they have not yet been directly tested in the domain of human memory. In this paper, we describe a model of statistical inference that instantiates these ideas, and test the model by asking human participants to reconstruct previously seen visual objects that have since changed gradually or abruptly. The results of this experiment support our theory of how the statistical structure of sensory experiences shapes memory formation.</p>
</abstract>
<funding-group><funding-statement>This research was supported in part by the National Institute Of Mental Health of the National Institutes of Health under Award Number R01MH098861, a graduate research fellowship from the National Science Foundation (SJG), and an Alfred P. Sloan Research Fellowship (YN). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. This publication was made possible in part through the support of a grant from the John Templeton Foundation. The opinions expressed in this publication are those of the authors and do not necessarily reflect the views of the John Templeton Foundation. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="13"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>How does the brain take a continuous stream of sensory inputs and translate it into stored memories? Theorists have offered radically different answers to this question. According to biologically inspired theories (e.g., <xref ref-type="bibr" rid="pcbi.1003939-Hopfield1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1003939-McNaughton1">[3]</xref>), input patterns are continuously assimilated into a distributed network of interconnected neurons via modification of synaptic connections. When a network trained in this fashion is allowed to run freely or with partial input, it will converge to one or more stable configurations–<italic>attractors</italic>–corresponding to blends of stored input patterns. This view of memory asserts that experiences are not stored individually, but rather overlaid on one another. Many modern psychological theories of memory (e.g., <xref ref-type="bibr" rid="pcbi.1003939-Raaijmakers1">[4]</xref>–<xref ref-type="bibr" rid="pcbi.1003939-Nosofsky1">[6]</xref>) adopt a diametrically opposed view: Input patterns are stored separately, and memory blending, if it occurs, happens at retrieval rather than during storage (though see <xref ref-type="bibr" rid="pcbi.1003939-Bower1">[7]</xref>–<xref ref-type="bibr" rid="pcbi.1003939-Shiffrin2">[9]</xref> for notable exceptions which allow memory traces to be modified by multiple input patterns).</p>
<p>One way to approach this question is to consider the information processing problem being solved by the memory system. If we were to design a brain, how would it parse experience into memory traces? This exercise in “rational analysis” <xref ref-type="bibr" rid="pcbi.1003939-Anderson1">[10]</xref> leads us to a statistical formulation of the memory storage problem. We propose that the memory system is designed to facilitate optimal predictions under a particular generative model of the environment. According to this generative model (see also <xref ref-type="bibr" rid="pcbi.1003939-Yu1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Daw1">[12]</xref>), the environment tends to change slowly over time, but occasionally jumps between completely different “modes.” For instance, while the temperature can fluctuate slowly within different parts of a building, going outside is characterized by very different (but also slowly changing) temperatures than those that were in effect indoors. Stored memories then correspond to inferences about the latent modes (e.g., we can recall the general temperature inside the building, and separately, the outdoor temperature), and input patterns are clustered together if they are inferred to have been generated by the same mode. This theory retains the idea from the cognitive psychology literature that the memory system contains multiple traces, but assumes that each trace may be a blend of several input patterns, as is the case for many neural network models.</p>
<p>Memories are no doubt stored at many resolutions: while you might have a general memory of being cold when outside and warm when inside, you will also probably remember precisely whether you wore a hat to combat the cold. Following traditional psychological models, we claim that separate traces for each input pattern are stored at the finest-grained, most “episodic” resolution. Layered on top of these episodic separate traces are more general traces that serve to organize memory retrieval and form predictions of the future. At this coarser resolution, experience must be parsed into separate traces or combined into more general traces. The goal of our theory is to illuminate the laws governing memory parsing. Depending on the statistical structure of the environment, this parsing process will produce traces that appear more or less “semantic,” in the sense that they aggregate information over individual episodes <xref ref-type="bibr" rid="pcbi.1003939-McClelland2">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Kording1">[14]</xref>. In order to avoid cumbersome terminology, we will henceforth use “traces” to refer to those traces formed as the result of parsing at the coarser-grained resolution of memory.</p>
<p>We tested our theory using a novel behavioral task that allows us to assess qualitatively whether participants store different stimuli in one or several memory traces. We presented dynamically changing visual stimuli to participants, and subsequently asked them to reconstruct one of the previously presented stimuli from memory. When the stimuli changed gradually, the reconstructions suggested that participants had, to some extent, inferred a single dynamical mode and thus formed one memory trace in which different instances interfered with each other. In contrast, when the stimuli changed abruptly, participants' behavior suggested that they had inferred two dynamical modes, one before the abrupt change and one after. This resulted in less interference between stimuli experienced before and after the change, and reconstruction of stimuli presented before the change was more accurate.</p>
<sec id="s1a">
<title>Background</title>
<p>Recent psychophysical studies have explored the dynamics of memory updating by presenting participants with sequences of stimuli and then probing their ability to discriminate between different stimuli in the sequence. The logic of these studies is that if the stimuli are assimilated into the same dynamical mode, then they will be perceived as being more similar, compared to a situation where they are segmented into different modes. For example, Wallis and Bülthoff <xref ref-type="bibr" rid="pcbi.1003939-Wallis1">[15]</xref> presented participants with a rotating face that gradually morphed into a different face. Compared to a condition in which the morphs were presented in a mixed (scrambled) order, participants in the gradual morph condition were more prone to perceive the final face as belonging to the same person as the original face. Similar findings were reported by Preminger and colleagues <xref ref-type="bibr" rid="pcbi.1003939-Preminger1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Preminger2">[17]</xref> using a variety of memory tests.</p>
<p>These psychophysical observations are complemented by neurophysiological studies of spatial representation in the rodent hippocampus. Many neurons in the CA3 subfield of the hippocampus respond selectively when the animal is in a particular region of space, and are therefore known as “place cells” <xref ref-type="bibr" rid="pcbi.1003939-Okeefe1">[18]</xref>. We can apply the same logic used in the aforementioned psychophysical studies to the hippocampal representation of space <xref ref-type="bibr" rid="pcbi.1003939-Colgin1">[19]</xref>, asking whether morphing one environment into another will lead to gradual changes in place cell firing rate (indicating a gradually changing spatial memory) or a global remapping of place fields (indicating the formation of a new memory). Leutgeb et al. <xref ref-type="bibr" rid="pcbi.1003939-Leutgeb1">[20]</xref> and Wills et al. <xref ref-type="bibr" rid="pcbi.1003939-Wills1">[21]</xref> had rats explore a set of enclosures whose shape varied from a square to a circle (including intermediate shapes). Gradually changing the enclosure shape (the “gradual” protocol) resulted in gradual changes in place fields <xref ref-type="bibr" rid="pcbi.1003939-Leutgeb1">[20]</xref>, whereas presenting the same series of enclosures in a scrambled order (the “mixed” protocol) resulted in global remapping – enclosures that were more similar to the circle than to the square tended to elicit one set of place fields, and enclosures that were more similar to the square than to the circle tended to elicit a distinct set of place fields <xref ref-type="bibr" rid="pcbi.1003939-Wills1">[21]</xref>. As with the psychophysical findings described above, these results highlight the importance of sequential structure in guiding memory organization; the same stimuli can elicit very different internal representations depending on the order in which they are presented.</p>
<p>Using a Hopfield network to encode the input patterns, Blumenfeld et al. <xref ref-type="bibr" rid="pcbi.1003939-Blumenfeld1">[22]</xref> proposed a “salience-weighted” modification of the standard Hebbian learning rule to model these findings. Intuitively, the salience weight encodes a prediction error or novelty signal that indicates the extent to which none of the network's existing attractors match the current input pattern. Formally, the salience weight is the Hamming distance between the input pattern and the network state after one step of dynamics; the salience weight is updated incrementally after each input pattern so as to smooth across recent history. A large salience weight promotes the formation of a new attractor based on the current input. For our purposes, the key idea to take away from this model is that prediction errors are useful signals for determining when to infer new memory modes (see also <xref ref-type="bibr" rid="pcbi.1003939-Redish1">[23]</xref>–<xref ref-type="bibr" rid="pcbi.1003939-Ezzyat1">[26]</xref>). In the network explored by Blumenfeld et al., a new attractor is only formed if the prediction error is sufficiently large, but how large is “sufficient”? In the next section, we place these ideas within a statistical framework, which allows us to specify the prediction error threshold in terms of probabilistic hypotheses about the environment.</p>
</sec><sec id="s1b">
<title>The statistical framework</title>
<p>The essence of our approach is captured by the following generic assumption about the environment: Properties of the environment usually change gradually, but occasionally undergo “jumps” that reflect a new underlying state of affairs <xref ref-type="bibr" rid="pcbi.1003939-Yu1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Daw1">[12]</xref>. Returning to the temperature example, when you walk around outside, you may experience gradual changes in temperature over the course of the day. If you step into a building, the temperature may change abruptly. In predicting what the temperature will be like in 5 minutes, you might then generalize from one outdoor location to another, but not between the indoor location and outdoor locations. Thus, our generalizations depend strongly on how we segment our observations; cognitively speaking, one can view each segment as a memory trace that aggregates those observations assigned to the segment. The empirical data reviewed in the previous section are consistent with the idea that the brain is attuned to abrupt changes in the state of the environment.</p>
<p>The problem of estimating the current state of a hidden variable given previous sensory measurements is known in engineering as <italic>filtering</italic>. The classic example of a filtering algorithm is the Kalman filter (KF; <xref ref-type="bibr" rid="pcbi.1003939-Kalman1">[27]</xref>), which is the Bayes-optimal estimator under the assumption that the environment evolves according to a linear-Gaussian dynamical system (LDS) –i.e., the state of the environment changes gradually and noisily over time. By design, this model cannot account for large sporadic jumps and periods of gradual change between them.</p>
<p>One way to model jumps is to posit a collection of different “dynamical modes”, each corresponding to a slowly changing LDS, and allow the generative process to switch between them stochastically. This is known as a switching LDS, and its corresponding Bayes-optimal estimator is the switching KF. However, for real-world sensory measurements, it is not reasonable to specify the number of possible modes in advance. We therefore adopt a Bayesian infinite-capacity (nonparametric) generalization of the switching LDS based on the Dirichlet process <xref ref-type="bibr" rid="pcbi.1003939-Fox1">[28]</xref>, which allows the number of modes to expand as necessary as measurements are collected (another dynamical model that could capture jumps within a single mode is a random walk with a heavy-tailed distribution on step size, such as a Lévy flight <xref ref-type="bibr" rid="pcbi.1003939-Mandelbrot1">[29]</xref>).</p>
<p>The infinite-capacity prior over modes leads to an intuitive interpretation in terms of memory traces: Each mode clusters together a number of individual observations, and thus can be identified with a temporally extended episodic memory trace such as the memory of the temperature outside. The number of such modes is essentially unlimited. However, because in our model small numbers of modes have higher probability <italic>a priori</italic>, the result is that the memory system tries to account for its observations as parsimoniously as possible by using existing modes to explain multiple observations. This leads to potential modification of existing modes each time a new observation is assigned to them, and sporadic creation of new modes. Below we describe this model formally.</p>
</sec></sec><sec id="s2">
<title>Results</title>
<p>We first propose a normative computational model that can account for the psychophysical and neural findings discussed in the <xref ref-type="sec" rid="s1">Introduction</xref>. We then describe a new psychophysical experiment that tests the predictions of our model.</p>
<sec id="s2a">
<title>Generative model</title>
<p>Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e001" xlink:type="simple"/></inline-formula> denote a set of sensory measurements at time <italic>t</italic>, arising from unobservable state variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e002" xlink:type="simple"/></inline-formula>, where <italic>k</italic> indexes modes. For instance, the observation may be the current temperature, and the state variables are the air pressure, cloud coverage, inside/outside location, air conditioner, thermostat status, and many other direct causes of temperature. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e003" xlink:type="simple"/></inline-formula> denote the mode active at time <italic>t</italic>. This mode specifies particular state-space dynamics, for instance, a mode corresponding to being indoors with the air conditioning on (which specifies the dependence of temperature on thermostat settings), another corresponding to air conditioning being off, another to being outside in the shade, etc.</p>
<p>Our model assumes that measurements (observations) are generated according to the following stochastic process. For each time point <italic>t</italic>:</p>
<list list-type="order"><list-item>
<p>Draw a mode <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e004" xlink:type="simple"/></inline-formula> from a <italic>sticky Chinese restaurant process</italic> prior <xref ref-type="bibr" rid="pcbi.1003939-Fox2">[30]</xref>:<disp-formula id="pcbi.1003939.e005"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003939.e005" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e006" xlink:type="simple"/></inline-formula> is the number of previous timepoints in which mode <italic>k</italic> was drawn, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e007" xlink:type="simple"/></inline-formula> is a stickiness parameter that governs mode persistence, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e008" xlink:type="simple"/></inline-formula> is a concentration parameter that specifies the probability of drawing a completely new mode. When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e009" xlink:type="simple"/></inline-formula>, Eq. 1 generates a partition of trials to modes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e010" xlink:type="simple"/></inline-formula> that corresponds to the distribution over partitions induced by a Dirichlet process <xref ref-type="bibr" rid="pcbi.1003939-Gershman2">[31]</xref>. This prior assigns higher probability to partitions with a small number of dynamical modes, and hence expresses a “simplicity principle” <xref ref-type="bibr" rid="pcbi.1003939-Chater1">[32]</xref> –all else equal, sensory data are more likely to be generated by a simpler environment, comprised of fewer modes. When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e011" xlink:type="simple"/></inline-formula>, modes tend to persist over multiple consecutive time points, with <italic>β</italic> controlling the strength of this persistence.</p>
</list-item><list-item>
<p>If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e012" xlink:type="simple"/></inline-formula> is a new mode, draw the state variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e013" xlink:type="simple"/></inline-formula> from a Gaussian base measure: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e014" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e015" xlink:type="simple"/></inline-formula> is the prior mean and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e016" xlink:type="simple"/></inline-formula> the covariance matrix of the state variables.</p>
</list-item><list-item>
<p>Diffuse the state variables for each active mode: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e017" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e018" xlink:type="simple"/></inline-formula> is a decay term and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e019" xlink:type="simple"/></inline-formula> is the diffusion noise covariance matrix. The diagonal terms of Q determine the rate of change: larger values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e020" xlink:type="simple"/></inline-formula> induce more rapid change along dimension <italic>d</italic>. Note that the state variable for a mode (once it is activated for the first time) evolves even when that mode is no longer active.</p>
</list-item><list-item>
<p>Emit sensory measurements <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e021" xlink:type="simple"/></inline-formula> from a Gaussian centered on the state of the currently active mode <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e022" xlink:type="simple"/></inline-formula>: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e023" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e024" xlink:type="simple"/></inline-formula> is the sensory noise covariance matrix.</p>
</list-item></list>
<p>This generative model is a simplification of the nonparametric switching LDS described in <xref ref-type="bibr" rid="pcbi.1003939-Fox1">[28]</xref>.</p>
<p>To summarize the generative model: The hidden state diffuses gradually until a jump occurs; this jump can be either to a previously activated mode, or to a new mode (in which case a new starting point is drawn for that mode, from a Gaussian prior). The concentration parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e025" xlink:type="simple"/></inline-formula> controls the probability that a new mode will be activated: Larger values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e026" xlink:type="simple"/></inline-formula> result in more modes, and if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e027" xlink:type="simple"/></inline-formula>, there are no jumps and we obtain a special case of the standard LDS formulation. The stickiness parameter <italic>β</italic> encourages modes to persist over time; when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e028" xlink:type="simple"/></inline-formula>, we recover the original Chinese restaurant process <xref ref-type="bibr" rid="pcbi.1003939-Aldous1">[33]</xref>. The diffusion variances <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e029" xlink:type="simple"/></inline-formula> control the rate of change within a mode: Larger values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e030" xlink:type="simple"/></inline-formula> result in faster change. The sensory noise variance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e031" xlink:type="simple"/></inline-formula> controls the informativeness of the observations about the hidden state: As <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e032" xlink:type="simple"/></inline-formula> increases, the sensory measurements become noisier and hence convey less information about the hidden state.</p>
</sec><sec id="s2b">
<title>Bayesian inference with an infinite-capacity switching LDS</title>
<p>Given the generative model, the filtering problem is to infer the posterior distribution over the state variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e033" xlink:type="simple"/></inline-formula> for each mode <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e034" xlink:type="simple"/></inline-formula> given the history of sensory measurements <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e035" xlink:type="simple"/></inline-formula>. This computation is given by:<disp-formula id="pcbi.1003939.e036"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003939.e036" xlink:type="simple"/><label>(2)</label></disp-formula></p>
<p>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e037" xlink:type="simple"/></inline-formula>. This corresponds to a “local” approximation <xref ref-type="bibr" rid="pcbi.1003939-Anderson2">[34]</xref>–<xref ref-type="bibr" rid="pcbi.1003939-Wang1">[36]</xref> that maintains only a single high probability partition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e038" xlink:type="simple"/></inline-formula> of previous observations to hidden causes. This partition is then used to calculate the probability of the current trial being drawn from each of the latent causes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e039" xlink:type="simple"/></inline-formula> by combining the sticky Chinese restaurant process prior (Eq. 1) and the likelihood (conditional on the partition and the previous observations) of the current state vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e040" xlink:type="simple"/></inline-formula>. Although we could have used more sophisticated methods (e.g., particle filtering) to approximate the marginalization, this method works well on the examples we consider, and is much faster, making it easier to fit to behavioral data.</p>
<p>We now describe how to compute each of the components in Eq. 2. The conditional distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e041" xlink:type="simple"/></inline-formula> is a Gaussian, with mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e042" xlink:type="simple"/></inline-formula> and covariance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e043" xlink:type="simple"/></inline-formula>, updated according to:<disp-formula id="pcbi.1003939.e044"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003939.e044" xlink:type="simple"/><label>(3)</label></disp-formula></p>
<p>for each dimension <italic>d</italic>, where the estimated mean and variance for a new mode <italic>k</italic> are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e045" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e046" xlink:type="simple"/></inline-formula> (respectively) and the step size (or learning rate) <italic>η</italic>, also known as the <italic>Kalman gain</italic>, is<disp-formula id="pcbi.1003939.e047"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003939.e047" xlink:type="simple"/><label>(4)</label></disp-formula></p>
<p>Using the local approximation described above, the posterior over mode assignments is given by:<disp-formula id="pcbi.1003939.e048"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003939.e048" xlink:type="simple"/><label>(5)</label></disp-formula></p>
<p>where the second term is the prior (Eq. 1), and the first term is the likelihood:<disp-formula id="pcbi.1003939.e049"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003939.e049" xlink:type="simple"/><label>(6)</label></disp-formula></p>
<p>where “new mode” refers to the first mode that has never been active before time <italic>i</italic>. This completes the description of our inference algorithm, which we refer to as the <italic>Dirichlet process Kalman filter</italic> (DP-KF).</p>
<p>Viewed as a mechanistic psychological model, the DP-KF assumes that the memory system keeps track of two kinds of traces: episodic traces encoding the sensory stimulus at each time point (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e050" xlink:type="simple"/></inline-formula>), and more general traces that encode summary statistics of stimuli belonging to a common mode (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e051" xlink:type="simple"/></inline-formula>). These summary statistics are updated in an incremental, psychologically plausible manner using error-driven learning. Episodes are partitioned into modes by a competitive clustering process similar to mechanisms that have been proposed in many other psychological and neural models <xref ref-type="bibr" rid="pcbi.1003939-Redish1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Gershman1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Anderson2">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Rumelhart1">[37]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Grossberg1">[38]</xref>.</p>
</sec><sec id="s2c">
<title>Model behavior</title>
<p>Eq. 6 operationalizes the idea that large prediction errors will lead to the inference of a new mode: For an old mode the Gaussian log-likelihood is inversely proportional to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e052" xlink:type="simple"/></inline-formula>, the distance between the current observation and the state when the mode was last active, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e053" xlink:type="simple"/></inline-formula> is the time at which the old mode last occurred, while for a new mode the log-likelihood is proportional to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e054" xlink:type="simple"/></inline-formula> (with the constant of proportionality scaling these distances by the variances of the modes). Thus when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e055" xlink:type="simple"/></inline-formula> is large relative to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e056" xlink:type="simple"/></inline-formula> the DP-KF will tend to assign observation <italic>t</italic> to a new mode, analogous to the process by which Blumenfeld et al. 's <xref ref-type="bibr" rid="pcbi.1003939-Blumenfeld1">[22]</xref> saliency-weighted learning rule creates a new attractor when the input pattern fails to match any of the existing attractors. (Although the likelihood for a new mode depends on the absolute scale of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e057" xlink:type="simple"/></inline-formula>, in our simulations this dependence was very weak, as the variance parameter was set to <italic>c</italic> = 1000.) Furthermore, because the variance of a mode grows with the length of time since its last occurrence (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e058" xlink:type="simple"/></inline-formula>), older modes will be more “tolerant” of prediction errors.</p>
<p><xref ref-type="fig" rid="pcbi-1003939-g001">Figure 1A</xref> illustrates the results of inference using our model with a one-dimensional sensory stimulus. Here we assumed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e059" xlink:type="simple"/></inline-formula> and <italic>α</italic> = 1. The sensory stimulus changed gradually, then underwent a jump, and then changed gradually again. On each time point we first inferred the hidden state based on past observations only (these are the model predictions). Following that, the sensory measurement was observed, thereby allowing the computation of its likelihood and updating of the posterior distribution. As a result, model predictions lag behind the jump. Nevertheless, due to inferring a new mode after the jump, the DP-KF (circles) “catches up” with the sensory evidence after one trial, whereas the regular KF model (squares) takes much longer. This occurs because the KF smooths across the jump as all observations are assumed to be generated by one slowly diffusing mode, whereas the DP-KF achieves piecewise smoothness by segmenting the time series into two modes, thereby producing better predictions.</p>
<fig id="pcbi-1003939-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003939.g001</object-id><label>Figure 1</label><caption>
<title>Simulations.</title>
<p>(<italic>A</italic>) Simulated sensory measurements and inferred state variables. For the DP-KF, the colors indicate the mode assignment with the highest posterior probability, white circles  =  mode 1, black circles  =  mode 2. (<italic>B</italic>) Posterior probability of mode 1 as a function of morph index in the gradual and mixed protocols, using the DP-KF (averaged over multiple simulation runs). See text for details.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003939.g001" position="float" xlink:type="simple"/></fig>
<p><xref ref-type="fig" rid="pcbi-1003939-g001">Figure 1B</xref> shows the results of applying the DP-KF to the “gradual” and “mixed” experimental protocols described in the <xref ref-type="sec" rid="s1">Introduction</xref> <xref ref-type="bibr" rid="pcbi.1003939-Wallis1">[15]</xref>–<xref ref-type="bibr" rid="pcbi.1003939-Preminger2">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Leutgeb1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Wills1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Hadas1">[39]</xref>. Here we used a sequence of one-dimensional measurements morphing between 0 and 1. In the gradual protocol, the sensory measurement (morphs) increased monotonically with time, whereas in the mixed protocol the morphs were presented in scrambled order. To analyze the simulated data, we re-sorted the indices from the mixed condition to match the gradual condition and calculated the posterior probability of mode 1 for each morph. Consistent with the psychophysical and neurophysiological data <xref ref-type="bibr" rid="pcbi.1003939-Wallis1">[15]</xref>–<xref ref-type="bibr" rid="pcbi.1003939-Preminger2">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Leutgeb1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Wills1">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Hadas1">[39]</xref>, the mixed protocol results in morphs being assigned to two different modes, whereas the gradual protocol results in all the morphs being predominantly assigned to a single mode.</p>
<p>Note that even if each of the modes is already firmly ingrained (through extensive experience with the morphs, as was the case in some of the experimental work we discussed), we still expect to see gradual or abrupt changes in the posterior probability of mode 1 depending on the morph sequence, since the sensory data are ambiguous with respect to the underlying dynamical mode. In other words, the time course of the posterior reflects uncertainty about which mode is currently active, and this uncertainty may change smoothly or abruptly depending on the stimulus sequence.</p>
</sec><sec id="s2d">
<title>Experiment: Memory for dynamically changing visual stimuli</title>
<p>We now describe an experiment designed to test a fundamental prediction of our model: if different modes correspond to different memories, inference of a new mode should protect the memory for old observations from retroactive interference due to new observations (see <xref ref-type="sec" rid="s5">Materials and Methods</xref> for more details). <xref ref-type="fig" rid="pcbi-1003939-g002">Figure 2</xref> illustrates the task. We exposed human participants to sequences of simple visual stimuli (lines) whose orientation and length changed from trial to trial, and asked them, at the end of the sequence, to reconstruct from memory one of the stimuli from the beginning of the sequence. To ensure that participants were encoding the stimuli, and to provide data that can be compared to the model's trial-by-trial predictions for the purpose of model fitting, we also asked participants to actively predict the orientation and length of the next line. Each participant was exposed to sequences belonging to two conditions: in the “gradual” condition, the lines changed slowly, through small perturbations in orientation/length space; in the “jump” condition, this slow change was interrupted by a large change in the middle of the sequence (<xref ref-type="fig" rid="pcbi-1003939-g003">Figure 3</xref>). Importantly, we kept the overall distance (in terms of orientation and length) between the start and end points of each sequence approximately equal in both conditions.</p>
<fig id="pcbi-1003939-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003939.g002</object-id><label>Figure 2</label><caption>
<title>Experimental task.</title>
<p>(<italic>Left</italic>) Prediction trial: participants were asked to predict the orientation and length of the next line segment (prediction shown in the center of the screen). At the bottom of the screen, a black circle superimposed on a timeline (the black bar) was used to indicate the trial's serial position in the block. At the start of each block, the black circle started out in the leftmost position; after each trial, the circle's position shifted one position to the right. (<italic>Middle</italic>) After making a prediction, participants were shown the true line segment and received a point score based on their prediction accuracy. (<italic>Right</italic>) Reconstruction trial: at the end of each block, participants were asked to reconstruct from memory the line they saw on one of the first three trials (indicated by an arrow on the timeline). No feedback was given for these reconstruction trials.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003939.g002" position="float" xlink:type="simple"/></fig><fig id="pcbi-1003939-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003939.g003</object-id><label>Figure 3</label><caption>
<title>Example trajectories and hypothetical reconstructions.</title>
<p>Each circle represents a line segment presented in a sequence, with the shaded circle indicating the first trial. The dimensions are standardized to a [0,100] range. The blue diamond represents a hypothetical reconstruction of the line segment indicated by the arrow. In solid black is the distance between the reconstruction and the starting point, while the dashed black line shows the distance between the reconstruction and the end point. (<italic>A</italic>) A gradual trajectory. Here we expected the reconstruction to be pulled away from the start point and towards the end point. (<italic>B</italic>) A jump trajectory. Here we expected the reconstruction to stay in the vicinity of the pre-jump points. As a result, we expected the distance between the reconstruction and the start point to be smaller in the jump condition as compared to the gradual condition, and the distance between the reconstruction and the end point to be smaller in the gradual condition.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003939.g003" position="float" xlink:type="simple"/></fig>
<p>We reasoned that if participants used prediction errors to segment their observations into distinct modes, then they would infer two modes in the jump condition (one for the first half and one for the second half of the sequence), but only one mode for the gradual condition. Segmenting the sequence would mean that the memory for the first half should be less biased by observations in the second half. We therefore hypothesized that reconstructions of early lines would be more veridical in the jump condition. By contrast, in the gradual condition, later observations would have been assigned to the initial mode, leading to alteration of that mode. Compared to the jump condition, reconstructions in the gradual condition should therefore be more similar to lines observed later in the block, and less similar to the target early lines. Example trajectories and reconstructions for a single participant are shown in <xref ref-type="supplementary-material" rid="pcbi.1003939.s001">Figure S1</xref>.</p>
<p>To test our hypothesis, for each sequence we calculated the Euclidean distance between the participant's reconstruction and the true line observed at the beginning of the block, as well as the distance from the line observed at the end of that block. The results, presented in <xref ref-type="fig" rid="pcbi-1003939-g004">Figure 4A</xref>, show that participants' reconstructions were closer to the last line (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e060" xlink:type="simple"/></inline-formula>), and farther from the first line (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e061" xlink:type="simple"/></inline-formula>) in the gradual condition as compared to the jump condition. A two-way (first/last × gradual/jump) ANOVA confirmed that the interaction was significant (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e062" xlink:type="simple"/></inline-formula>). We interpret this result as showing that, in the gradual condition, participants inferred one mode, thereby causing lines from the second half to influence memory for the lines from the first half; by contrast, in the jump condition participants inferred separate pre-jump and post-jump modes, thereby protecting their memory of the pre-jump lines from being distorted by the post-jump lines.</p>
<fig id="pcbi-1003939-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003939.g004</object-id><label>Figure 4</label><caption>
<title>Experimental results and model predictions.</title>
<p>(A) Euclidean distance between participants' reconstructions and the observed (true) first and last lines in a block. Error bars represent within-subject standard error of the mean. The results show that participants were more accurate in their reconstructions in the jump condition as compared to the gradual condition. (B) Stationary Kalman filter (KF) model predictions. Data in (A) are represented by black circles. (C) Non-stationary KF model predictions. (D) Stationary Dirichlet process Kalman filter (DP-KF) model predictions. (E) Non-stationary DP-KF predictions.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003939.g004" position="float" xlink:type="simple"/></fig></sec><sec id="s2e">
<title>Model-based analysis of experimental data</title>
<p>Assuming that the individual trace of each stimulus is noisy (see <xref ref-type="sec" rid="s5">Materials and Methods</xref>), it is reasonable for the memory system to use information from multiple trials to aid in reconstruction. In our model, this is accomplished at retrieval by “smoothing” over (or blurring together) the traces of trials that occurred nearby in time. This blurring removes noise under the assumption that stimuli change slowly over time and hence the underlying signal is temporally autocorrelated (whereas the noise is not). Formally, this corresponds to a form of Kalman smoothing <xref ref-type="bibr" rid="pcbi.1003939-BarShalom1">[40]</xref>. However, it is important to not smooth over instances that are very different from each other (i.e., across time points where an abrupt jump occurred and as a result the signal is no longer autocorrelated). Inference over multiple dynamical modes remedies this problem by segmenting the time series into parts that are each internally smooth; our smoothing algorithm operates within but not across these modes (note that even when there is only a single dynamical mode, smoothing can still reconstruct individual stimuli, rather than blurring them all together, because a representation of each stimulus is available to the retrieval system). A formal description of this smoothing algorithm is given in the <xref ref-type="sec" rid="s5">Materials and Methods</xref>.</p>
<p> To test how well our proposed model fit participants' data throughout the experiment, we fit several variants of the DP-KF and KF models to participants' responses on prediction trials (in which participants had to predict the next version of the line), holding out the responses on reconstruction trials for validation and comparison between the models (see <xref ref-type="sec" rid="s5">Materials and Methods</xref> for details of the model-fitting methods). Four model variants were constructed from the full model by restricting parameter values as follows:</p>
<list list-type="bullet"><list-item>
<p><bold>Stationary KF</bold>: A Kalman filter in which the hidden state is stationary (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e063" xlink:type="simple"/></inline-formula> for all <italic>d</italic>). This means all variation is attributed to the sensory and response noise. This model has five free parameters: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e064" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e065" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e066" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e067" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e068" xlink:type="simple"/></inline-formula> (where superscripts 1 and 2 refer to the two stimulus dimensions: length and angle). The <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e069" xlink:type="simple"/></inline-formula> parameters represent response noise variances (see <xref ref-type="sec" rid="s5">Materials and Methods</xref> for more details).</p>
</list-item><list-item>
<p><bold>KF</bold>: A Kalman filter in which the hidden state is allowed to diffuse over time (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e070" xlink:type="simple"/></inline-formula>). This model has seven free parameters: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e071" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e072" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p><bold>Stationary DP-KF</bold>: In this model, the hidden state can be drawn from multiple modes, where each mode's hidden state is stationary in time (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e073" xlink:type="simple"/></inline-formula> for all <italic>d</italic>). Modes tend to persist over time with the strength of persistence determined by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e074" xlink:type="simple"/></inline-formula>. This model thus has seven free parameters: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e075" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e076" xlink:type="simple"/></inline-formula> and <italic>β</italic>.</p>
</list-item><list-item>
<p><bold>DP-KF</bold>: This is the full Dirichlet process Kalman filter model. It allows multiple diffusing modes that each can change over time (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e077" xlink:type="simple"/></inline-formula>). This model has nine free parameters: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e078" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e079" xlink:type="simple"/></inline-formula>, <italic>α</italic> and <italic>β</italic>.</p>
</list-item></list>
<p><xref ref-type="fig" rid="pcbi-1003939-g004">Figure 4B-E</xref> shows the predicted reconstruction biases for each of these models. Unlike our participants, neither the KF models nor the stationary DP-KF model showed a cross-over interaction between jump/gradual and start/end. In contrast, the DP-KF model showed a cross-over interaction effect (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e080" xlink:type="simple"/></inline-formula>). Thus among the four alternatives, only the DP-KF model adequately captured the experimental results.</p>
<p>We quantitatively compared the fits of the different models in two ways. First, we performed cross-validation by splitting the blocks into two halves (even- and odd-numbered blocks), fitting the model to the trial-by-trial prediction data for one half of the blocks and computing the predictive log-likelihood of data for the other half of the blocks. <xref ref-type="fig" rid="pcbi-1003939-g005">Figure 5A</xref> shows the predictive log-likelihood of each model relative to the stationary KF model. The KF and DP-KF models performed similarly (a paired-sample <italic>t</italic>-test revealed no significant difference, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e081" xlink:type="simple"/></inline-formula>), and significantly better than their stationary variants (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e082" xlink:type="simple"/></inline-formula>).</p>
<fig id="pcbi-1003939-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003939.g005</object-id><label>Figure 5</label><caption>
<title>Model comparison.</title>
<p>(<italic>A</italic>) Predictive log-likelihood for each model, on prediction trials, relative to the stationary KF model. Larger values indicate superior performance on held-out data from prediction trials. (<italic>B</italic>) Predictive log-likelihood for each model, on the reconstruction data, relative to the stationary KF model. Error bars represent within-subject standard error of the mean.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003939.g005" position="float" xlink:type="simple"/></fig>
<p>Our second model-comparison metric was the predictive log-likelihood of participants' reconstructions. Note that the models were not fit to the reconstruction data, so there is no need to penalize for model complexity: overfitting the prediction-trials data due to too many degrees of freedom will automatically lead to poorer results when trying to predict the reconstruction trials. <xref ref-type="fig" rid="pcbi-1003939-g005">Figure 5B</xref> shows the predictive log-likelihood of each model relative to the stationary KF. According to this measure, the DP-KF model outperformed both the KF variants (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e083" xlink:type="simple"/></inline-formula>) and performed marginally better than the stationary DP-KF (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e084" xlink:type="simple"/></inline-formula>). To illustrate the DP-KF model's accuracy in predicting reconstructions, we computed the Pearson correlation coefficient between the human and model reconstructions for each participant separately, Fisher z-transformed this value, and performed a <italic>t</italic>-test against 0 for all participants. Correlations for both orientation and length were significant (each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e085" xlink:type="simple"/></inline-formula>, two-tailed <italic>t</italic>-test; <xref ref-type="fig" rid="pcbi-1003939-g006">Figure 6</xref>).</p>
<fig id="pcbi-1003939-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003939.g006</object-id><label>Figure 6</label><caption>
<title>Comparison of model and human reconstructions.</title>
<p>Histogram of z-transformed correlations between human reconstructions and model reconstructions for (A) the orientation dimension and (B) the length dimension. Vertical black line indicates a correlation of 0.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003939.g006" position="float" xlink:type="simple"/></fig>
<p>Finally, in keeping with our theoretical predictions, we found that the number of modes (<italic>K</italic>) inferred by the fitted DP-KF model was, on average, higher in the jump condition than in the gradual condition (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e086" xlink:type="simple"/></inline-formula>; <xref ref-type="fig" rid="pcbi-1003939-g007">Figure 7</xref>).</p>
<fig id="pcbi-1003939-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003939.g007</object-id><label>Figure 7</label><caption>
<title>Model-based analysis.</title>
<p>Number of modes (K) inferred by the DP-KF model for each condition.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003939.g007" position="float" xlink:type="simple"/></fig></sec></sec><sec id="s3">
<title>Discussion</title>
<p>We addressed, both theoretically and experimentally, a basic question about memory: When does new experience cause an existing memory to be modified versus a new memory to be formed? Our answer took the form of a rational analysis <xref ref-type="bibr" rid="pcbi.1003939-Anderson1">[10]</xref>. In particular, we proposed that the structure of memories reflects a process of optimal filtering in a dynamically changing environment, where each memory encodes a distinct “dynamical mode” of the environment. New modes are inferred when there are abrupt discontinuities in the temporal dynamics of sensory data that cannot be explained by existing memories. Such discontinuities are typically accompanied by a large prediction error, suggesting a biologically plausible mechanism for implementing memory-trace formation: The brain may split off new memory traces when large prediction errors are registered <xref ref-type="bibr" rid="pcbi.1003939-Redish1">[23]</xref>–<xref ref-type="bibr" rid="pcbi.1003939-Kurby1">[25]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Gershman3">[41]</xref>. Prediction errors are believed to be computed in many areas of the brain, including area CA1 of the hippocampus <xref ref-type="bibr" rid="pcbi.1003939-Vinogradova1">[42]</xref> and midbrain dopaminergic nuclei <xref ref-type="bibr" rid="pcbi.1003939-Bayer1">[43]</xref>. Indeed, predictive coding theories propose that prediction errors are computed throughout the neocortex <xref ref-type="bibr" rid="pcbi.1003939-Friston1">[44]</xref>.</p>
<p>Importantly, the specific model used here belongs to a large family of statistical models that instantiate the idea that abrupt, inexplicable changes in the environment result in inference of a new mode <xref ref-type="bibr" rid="pcbi.1003939-Fox1">[28]</xref>. The main contribution of this paper is to provide an experimental test of this principle in the domain of human reconstructive memory. In our experiment, participants were asked to reconstruct from memory a previously encountered visual stimulus, under conditions where the stimulus had since changed over time either gradually or abruptly. We envision inference over dynamic modes of the environment as giving rise to temporally extended episodic memory traces that group together individual stimulus traces, thus causing some generalization or interference between the memories of different specific observations. We thus measured the degree to which later stimuli modify the memory of earlier instances by assessing the extent to which the reconstructed stimulus shifted from the starting point of the stimulus trajectory towards the end point. We showed that gradual change resulted in greater memory modification than abrupt change, in agreement with our theoretical prediction that gradual change would favor inference of a single dynamical mode that would incorporate all stimuli in a block, whereas abrupt change would favor the inference of multiple modes, each relatively untainted by experience that is associated with the other mode.</p>
<p>The behavioral effect that we showed cannot be explained by recency or primacy biases: A recency bias does not predict a difference between the conditions, because the conditions were matched for total distance traveled and for trial-to-trial differences in the stimuli in all trials but the jump trial (which was always in the middle of the sequence). Therefore, stimuli at the end of a block, just prior to the reconstruction trial, were (on average) equally similar to the initial stimulus across conditions. Likewise, a primacy bias does not predict a difference between conditions, since the stimuli in the beginning of the block did not differ systematically between conditions.</p>
<p>The choice between modifying an existing memory versus creating a new one is formalized in our model using a nonparametric prior over partitions known as the Chinese restaurant process <xref ref-type="bibr" rid="pcbi.1003939-Aldous1">[33]</xref> (see <xref ref-type="bibr" rid="pcbi.1003939-Gershman2">[31]</xref> for an explanation of the Chinese restaurant metaphor and its origins). This prior has previously been used to model category formation <xref ref-type="bibr" rid="pcbi.1003939-Anderson2">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Sanborn1">[35]</xref>, Pavlovian conditioning in multiple contexts <xref ref-type="bibr" rid="pcbi.1003939-Gershman1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Gershman4">[45]</xref>, word segmentation <xref ref-type="bibr" rid="pcbi.1003939-Frank1">[46]</xref> and task-set learning <xref ref-type="bibr" rid="pcbi.1003939-Collins1">[47]</xref> (for a review of this literature, see <xref ref-type="bibr" rid="pcbi.1003939-Austerweil1">[48]</xref>). All of these domains have in common the problem of segmenting stimuli and actions into coherent clusters (or, in our case, modes). The Chinese restaurant process is a natural prior for segmentation because it allows an unbounded number of clusters while preferring fewer clusters. This prior thus expresses a bias towards simplicity <xref ref-type="bibr" rid="pcbi.1003939-Chater1">[32]</xref>. Even without such a prior bias, simpler segmentations are naturally favored by Bayesian inference due to the “automatic Occam's razor” phenomenon <xref ref-type="bibr" rid="pcbi.1003939-MacKay1">[49]</xref>, whereby simpler explanations of data have higher marginal likelihood than more complex explanations. While the experiment we report does not directly address whether humans exhibit a simplicity bias in memory formation, this question has been addressed by other work from our laboratory <xref ref-type="bibr" rid="pcbi.1003939-Gershman5">[50]</xref>.</p>
<p>One limitation of the current study is that it did not test a further prediction of our model: When a change occurs, an old mode can be reinvoked, rather than creating a new mode. Thus our findings could potentially be explained by a model that creates a new mode every time a large change is observed (although previous modes would still have to be maintained in memory to allow recall, unlike some models, e.g., <xref ref-type="bibr" rid="pcbi.1003939-Yu1">[11]</xref>). In future work, we will test the hypothesis that old modes can be modified in this paradigm. Using a perceptual estimation paradigm <xref ref-type="bibr" rid="pcbi.1003939-Gershman5">[50]</xref> we have shown that participants can update two modes in an alternating fashion, if these are signaled externally (in that case, by the color of the stimuli). However, unlike our current study, this earlier study did not manipulate the dynamics of stimulus trajectories and so could not address the dynamics of memory formation as a result of (abrupt vs. gradual) change in the environment.</p>
<sec id="s3a">
<title>Related work</title>
<p>Several authors have proposed neural implementations of the KF <xref ref-type="bibr" rid="pcbi.1003939-Denve1">[51]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Wilson1">[52]</xref>. Wilson and Finkel <xref ref-type="bibr" rid="pcbi.1003939-Wilson1">[52]</xref> derived an approximation of the KF that can be computed by a recurrent neural network when the prediction error is small. Intriguingly, when the prediction error is large, their approximation ‘breaks down’ by creating two bumps in the posterior distribution (rather than one as in the exact KF) with each bump implementing an independent KF. Our theory suggests a normative account of this feature, since a network that creates multiple bumps is precisely what is required by the DP-KF algorithm. Pursuing this connection is an exciting direction for future research.</p>
<p>Work on change detection <xref ref-type="bibr" rid="pcbi.1003939-Yu1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Gallistel1">[53]</xref>–<xref ref-type="bibr" rid="pcbi.1003939-Wilson3">[59]</xref> addresses a similar question: how does the brain detect a change in the statistics of sensory signals? The study of Nassar et al. <xref ref-type="bibr" rid="pcbi.1003939-Nassar1">[56]</xref>, for example, showed that humans use the recent history of prediction errors to determine when a change has occurred. This work differs from our own in several ways. First, most existing change-detection theories assume stationary sensory statistics between jumps, whereas we allow for gradual change between jumps. Second, once a jump has occurred, theories of change detection assume that the statistics of earlier epochs are no longer relevant and can be discarded; in contrast, our model assumes that participants are able to retrieve statistics from earlier modes, and in general allows for the environment to return to earlier modes (as noted above, our current experiment did not test this latter property of the model).</p>
<p>Our work also intersects with research in cognitive psychology on the reuse of existing memory traces. For example, repeating items on a list tends to aid their recognition without degrading recognition of other items (the <italic>null list-strength effect</italic> <xref ref-type="bibr" rid="pcbi.1003939-Ratcliff1">[60]</xref>). To explain this, Shiffrin et al. <xref ref-type="bibr" rid="pcbi.1003939-Shiffrin1">[8]</xref> assumed that repetition of items results in refinement of existing traces, rather than formation of new traces. Thus, there must be <italic>some</italic> reuse of memory traces. The question, then, is what counts as a repetition. Visually similar stimuli such as those used in our experiment may be judged by the memory system to be essentially the same item (i.e., a “repetition”). Our theory further asserts that small changes in these “repetitions” drive modification of existing memories, but not formation of new memories. This is similar to what Bower and Winzenz <xref ref-type="bibr" rid="pcbi.1003939-Bower1">[7]</xref> dubbed the “reallocation hypothesis,” according to which inputs are matched to memory traces and incorporated into an existing trace if the match is sufficiently high; otherwise, the input is routed to a new trace (see also <xref ref-type="bibr" rid="pcbi.1003939-Shiffrin2">[9]</xref>). Interestingly, evidence suggests that failure to recognize a new context can sometimes lead to neither outcome: using an auditory statistical learning paradigm, Gebhart et al. <xref ref-type="bibr" rid="pcbi.1003939-Gebhart1">[61]</xref> found that changes in structural information can go undetected without the aid of additional cues (e.g., sounds marking the transition between structures), preventing participants from learning new structures. This suggests that future models should incorporate a mechanism that allows some information to evade both old and new memories.</p>
<p>The dynamically updated posterior posited by our model bears some resemblance to the drifting context vector posited by several models in the memory literature <xref ref-type="bibr" rid="pcbi.1003939-Mensink1">[62]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Howard1">[63]</xref>. For example, the Temporal Context Model (TCM) introduced by Howard and Kahana <xref ref-type="bibr" rid="pcbi.1003939-Howard1">[63]</xref> assumes that list items are bound to a context vector that is essentially an average of recently experienced items. In earlier work <xref ref-type="bibr" rid="pcbi.1003939-Socher1">[64]</xref>, we operationalized the context vector as a posterior over latent “topics” that play the same role as modes in the present paper. In our current theory, items are bound to modes in much the same way that items are bound to the context vector in TCM. The connection to TCM also highlights the way in which episodic and semantic memory are deeply intertwined in contemporary theories: “episodic” traces of individual items become bound to “semantic” representations that average over multiple items <xref ref-type="bibr" rid="pcbi.1003939-Howard2">[65]</xref>. Likewise in our model, episodic and semantic components are intertwined: a separate trace for each sensory stimulus is stored, but the traces are effectively blurred together by the smoothing operation during retrieval. Although the idea of separate episodic and semantic memory systems has been very influential <xref ref-type="bibr" rid="pcbi.1003939-McClelland2">[13]</xref>, it has been known since Bartlett's investigations <xref ref-type="bibr" rid="pcbi.1003939-Bartlett1">[66]</xref> that semantic knowledge exerts strong constraints on many aspects of episodic memory <xref ref-type="bibr" rid="pcbi.1003939-Hemmer1">[67]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Hemmer2">[68]</xref>. A similar rapprochement has emerged in theories of category learning, where “episodic” (exemplar) and “semantic” (prototype) representations are combined to form varying levels of abstraction <xref ref-type="bibr" rid="pcbi.1003939-Sanborn1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Love1">[69]</xref>, <xref ref-type="bibr" rid="pcbi.1003939-Vanpaemel1">[70]</xref>.</p>
<p>Another related line of work concerns the effects of novelty on memory. Our model predicts that a novel stimulus is more likely to be encoded in a separate trace compared to a familiar stimulus, making it less likely that the novel stimulus will suffer interference from other stimuli at retrieval. This prediction has been confirmed many times in the form of the von Restorff effect <xref ref-type="bibr" rid="pcbi.1003939-VonRestorff1">[71]</xref>. Note that while the von Restorff effect reflects proactive interference (older memories interfering with the retrieval of newer memories) and our experiment tested retroactive interference (newer memories interfering with the retrieval of older memories), according to our model these are essentially due to the same process of grouping of different observations into temporally extended episodic memory traces.</p>
<p>The idea of comparing gradual and abrupt changes as a means of influencing memory updating has also been explored in the motor control literature <xref ref-type="bibr" rid="pcbi.1003939-Kagerer1">[72]</xref>–<xref ref-type="bibr" rid="pcbi.1003939-Taylor1">[74]</xref>. For example, Kagerer et al. <xref ref-type="bibr" rid="pcbi.1003939-Kagerer1">[72]</xref> had participants make arm movements to a target and then introduced a perturbation (by rotating the visual feedback) either gradually or abruptly. Participants adapted to the perturbation; following the removal of the perturbation, participants exhibited an after-effect in which movement errors were in the direction opposite to the perturbation. Kagerer et al. found that the after-effect was smaller for participants in the abrupt condition than in the gradual condition. This pattern of results is consistent with the idea that two separate motor memories were formed in the abrupt condition, thereby allowing the pre-perturbation memory to be reinstated quickly. The larger after-effect in the gradual condition suggests that in that case the gradual perturbation led to modification of the original memory. Such modifications can be long-lasting: Yamamoto et al. <xref ref-type="bibr" rid="pcbi.1003939-Yamamoto1">[75]</xref> have shown that learning a gradually changing motor task produces a motor memory that can be recovered over a year later.</p>
<p>Finally, we have recently reported related findings in the domain of Pavlovian fear conditioning <xref ref-type="bibr" rid="pcbi.1003939-Gershman3">[41]</xref>. Rats learned to associate a tone with a foot-shock. Subsequently, one group of rats were presented with the tone in the absence of shock (standard ‘extinction’ of the tone-shock association). A second group of rats experienced the same number of tones, with the the tone-shock contingency only gradually reduced to zero (that is, to full extinction). Although all rats showed similarly diminished fear of the tone at the end of the ‘extinction’ phase, rats in the standard extinction condition exhibited subsequent recovery of fear (as is typically seen after extinction training), whereas rats in the gradual condition showed no evidence of fear recovery. These findings are consistent with the idea that the fear memory is more likely to be modified by extinction training in the gradual condition, thereby reducing the probability of later recovery.</p>
</sec></sec><sec id="s4">
<title>Conclusions</title>
<p>In this paper, we empirically investigated a fundamental prediction that models of change detection make for memory. If, as we hypothesize, new experience is incorporated into old memories based on similarity, then abrupt change (i.e., dissimilar data) should prompt the creation of a new memory trace, and thus protect old memories from being modified by new data, whereas gradual change will not. Our experimental results confirm this prediction, thereby providing support for a statistical account of how continuous experience is parsed into discrete memory traces. We conclude that memories are not simply a record of our ongoing experiences; the organization of memory traces reflects our subjective inferences about the structure of the world that surrounds us.</p>
</sec><sec id="s5" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s5a">
<title>Ethics statement</title>
<p>The experiment was approved by the Institutional Review Board at Princeton University.</p>
</sec><sec id="s5b">
<title>Participants</title>
<p>32 undergraduate students received course credit or payment ($12 per hour) for participating in the experiment. The experiment was approved by the Institutional Review Board at Princeton University.</p>
</sec><sec id="s5c">
<title>Stimuli</title>
<p>The stimuli consisted of oriented line segments that changed in orientation and length on every trial. Each line segment was generated from the previous one by (randomly) adding or subtracting a fixed length (0.89 mm) and a fixed angle (14.4°), thus generating a 45° ‘move’ in an orientation/length space in which one unit was 14.4° and 0.89 mm, respectively. ‘Moves’ were restricted so that the new line segment did not overlap with the previous line segment (that is, there was no ‘backtracking’ in orientation/length space; see <xref ref-type="fig" rid="pcbi-1003939-g003">Figure 3</xref>). Jumps were also at a 45° angle, but traversed a distance 4 times as long as the other steps (i.e., 3.6 mm length and 57.6° angle). Jumps always occurred (if they did) in the middle of the trajectory (between trials 9 and 10), and were unsignaled to the participant. Finally, in generating trajectories through orientation/length space, we required the Euclidean distance between the start and end points to lie within a narrow range (60–70% of the maximum possible distance) regardless of the condition (jump or gradual). Examples of jump and gradual trajectories are shown in <xref ref-type="fig" rid="pcbi-1003939-g003">Figure 3</xref>.</p>
</sec><sec id="s5d">
<title>Procedure</title>
<p>Participants played 12 blocks of the task (6 jump trajectories and 6 gradual trajectories, randomly interleaved). Each block consisted of a sequence of 18 prediction trials. A timeline showed participants the serial position of each trial in a block. On each prediction trial, participants used a mouse to adjust the orientation and length of a line on the screen so as to predict the next observed line. After making their prediction, participants were shown the true line and awarded points based on how accurate their prediction was. The prediction task was aimed at encouraging encoding of the different line segments in memory, and also provided data for fitting our models (see below). At the end of the block, participants were given a reconstruction trial; on this trial, they were shown an arrow pointing toward a point on the timeline and asked to reconstruct the line segment they saw on that trial. Participants were always asked to reconstruct one of the first 3 trials in the block. No feedback was given on reconstruction trials.</p>
</sec><sec id="s5e">
<title>Reconstruction by smoothing</title>
<p>Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e087" xlink:type="simple"/></inline-formula> denote the estimated stimulus for time <italic>t</italic> given all observations up to the time of retrieval conditional on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e088" xlink:type="simple"/></inline-formula>. Kalman smoothing <xref ref-type="bibr" rid="pcbi.1003939-BarShalom1">[40]</xref> constructs this estimate through a backward recursion:<disp-formula id="pcbi.1003939.e089"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003939.e089" xlink:type="simple"/><label>(7)</label></disp-formula></p>
<p>for each dimension <italic>d</italic>. In essence, smoothing combines the filtered estimate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e090" xlink:type="simple"/></inline-formula> with information from the future propagated backward in time. We take <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e091" xlink:type="simple"/></inline-formula> to be the model's prediction for a participant's reconstruction of the stimulus shown at time <italic>t</italic>.</p>
</sec><sec id="s5f">
<title>Model-fitting</title>
<p>Prior to model-fitting, the stimulus values (length and orientation) were rescaled to [0, 100]. To model responses, we assumed that participants report the posterior mean, corrupted by anisotropic Gaussian noise (with variances <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e092" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e093" xlink:type="simple"/></inline-formula>, for length and orientation, respectively. Depending on the model variant, the noise variance <italic>r</italic>, the response noise variance <italic>v</italic>, the diffusion noise variance <italic>q</italic>, the stickiness parameter <italic>β</italic> and the concentration parameter <italic>α</italic> were treated as free parameters and fit to each participant's data by minimizing the negative log-likelihood of each participant's predictions using a numerical optimizer (the routine fmincon in Matlab), while constraining parameters to lie in the appropriate range. To prevent implausibly large values of <italic>v</italic>, <italic>q</italic> and <italic>r</italic>, we constrained these to be less than 10, 30 and 20, respectively, although our results do not depend on these precise values. To avoid local minima, the optimization was run from 3 randomly chosen starting points. We assumed that responses were generated from the filtered state estimate (or smoothed state estimate, in the case of retrieval), corrupted by Gaussian noise with anisotropic noise variance (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e094" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e095" xlink:type="simple"/></inline-formula>). For the KF model, <italic>α</italic> was set to 0. We set the prior covariances to be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003939.e096" xlink:type="simple"/></inline-formula>, instantiating an approximately uniform distribution over mode starting points. Reconstruction trials were not used in any of the fitting procedures. To model noise in the reconstruction process, we added a constant of 5 to the sensory noise variance (<italic>r<sup>d</sup></italic>). This value was chosen by hand, but the results were not sensitive to its precise value.</p>
</sec></sec><sec id="s6">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003939.s001" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1003939.s001" position="float" xlink:type="simple"><label>Figure S1</label><caption>
<p><bold>Example trajectories and reconstructions for a single participant.</bold> The top row shows trajectories in the “gradual” condition. The bottom row shows trajectories in the “jump” condition. The first trial is indicated by the large circle, and the blue diamond shows the reconstruction.</p>
<p>(EPS)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>We thank Robert Wilson and Jordan Taylor for illuminating discussions, and Eran Eldar for comments on the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1003939-Hopfield1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>J</given-names></name> (<year>1982</year>) <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>79</volume>: <fpage>2554</fpage>–<lpage>2558</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-McClelland1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McClelland</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Rumelhart</surname><given-names>D</given-names></name> (<year>1985</year>) <article-title>Distributed memory and the representation of general and specific information</article-title>. <source>Journal of Experimental Psychology: General</source> <volume>114</volume>: <fpage>159</fpage>–<lpage>188</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-McNaughton1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McNaughton</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Morris</surname><given-names>R</given-names></name> (<year>1987</year>) <article-title>Hippocampal synaptic enhancement and information storage within a distributed memory system</article-title>. <source>Trends in Neurosciences</source> <volume>10</volume>: <fpage>408</fpage>–<lpage>415</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Raaijmakers1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Raaijmakers</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Shiffrin</surname><given-names>R</given-names></name> (<year>1981</year>) <article-title>Search of associative memory</article-title>. <source>Psychological Review</source> <volume>88</volume>: <fpage>93</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Hintzman1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hintzman</surname><given-names>D</given-names></name> (<year>1988</year>) <article-title>Judgments of frequency and recognition memory in a multiple-trace memory model</article-title>. <source>Psychological Review</source> <volume>95</volume>: <fpage>528</fpage>–<lpage>551</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Nosofsky1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nosofsky</surname><given-names>R</given-names></name> (<year>1988</year>) <article-title>Exemplar-based accounts of relations between classification, recognition, and typicality</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>14</volume>: <fpage>700</fpage>–<lpage>708</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Bower1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bower</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Winzenz</surname><given-names>D</given-names></name> (<year>1969</year>) <article-title>Group structure, coding, and memory for digit series</article-title>. <source>Journal of Experimental Psychology</source> <volume>80</volume>: <fpage>1</fpage>–<lpage>17</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Shiffrin1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shiffrin</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Ratcliff</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Clark</surname><given-names>S</given-names></name> (<year>1990</year>) <article-title>List-strength effect: II. Theoretical mechanisms</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>16</volume>: <fpage>179</fpage>–<lpage>195</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Shiffrin2"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shiffrin</surname><given-names>RM</given-names></name>, <name name-style="western"><surname>Steyvers</surname><given-names>M</given-names></name> (<year>1997</year>) <article-title>A model for recognition memory: REMretrieving effectively from memory</article-title>. <source>Psychonomic Bulletin &amp; Review</source> <volume>4</volume>: <fpage>145</fpage>–<lpage>166</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Anderson1"><label>10</label>
<mixed-citation publication-type="other" xlink:type="simple">Anderson J (1990) The Adaptive Character of Thought. Lawrence Erlbaum.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Yu1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yu</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name> (<year>2005</year>) <article-title>Uncertainty, neuromodulation, and attention</article-title>. <source>Neuron</source> <volume>46</volume>: <fpage>681</fpage>–<lpage>692</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Daw1"><label>12</label>
<mixed-citation publication-type="other" xlink:type="simple">Daw N, Courville A (2008) The pigeon as particle filter. In: Platt J, Koller D, Singer Y, Roweis S, editors, Advances in Neural Information Processing Systems 20, Cambridge, MA: MIT Press. pp.369–376.</mixed-citation>
</ref>
<ref id="pcbi.1003939-McClelland2"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McClelland</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>McNaughton</surname><given-names>BL</given-names></name>, <name name-style="western"><surname>O′Reilly</surname><given-names>RC</given-names></name> (<year>1995</year>) <article-title>Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory</article-title>. <source>Psychological Review</source> <volume>102</volume>: <fpage>419</fpage>–<lpage>457</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Kording1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kording</surname><given-names>KP</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Shadmehr</surname><given-names>R</given-names></name> (<year>2007</year>) <article-title>The dynamics of memory as a consequence of optimal adaptation to a changing body</article-title>. <source>Nature Neuroscience</source> <volume>10</volume>: <fpage>779</fpage>–<lpage>786</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Wallis1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wallis</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Bülthoff</surname><given-names>H</given-names></name> (<year>2001</year>) <article-title>Effects of temporal association on recognition memory</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>98</volume>: <fpage>4800</fpage>–<lpage>4804</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Preminger1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Preminger</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Sagi</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>The effects of perceptual history on memory of visual objects</article-title>. <source>Vision research</source> <volume>47</volume>: <fpage>965</fpage>–<lpage>973</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Preminger2"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Preminger</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Blumenfeld</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Sagi</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>Mapping dynamic memories of gradually changing objects</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>106</volume>: <fpage>5371</fpage>–<lpage>5376</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Okeefe1"><label>18</label>
<mixed-citation publication-type="other" xlink:type="simple">O′keefe J, Nadel L (1978) The Hippocampus as a Cognitive Map. Clarendon Press Oxford.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Colgin1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Colgin</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Moser</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Moser</surname><given-names>M</given-names></name> (<year>2008</year>) <article-title>Understanding memory through hippocampal remapping</article-title>. <source>Trends in Neurosciences</source> <volume>31</volume>: <fpage>469</fpage>–<lpage>477</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Leutgeb1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leutgeb</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Leutgeb</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Treves</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Meyer</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Barnes</surname><given-names>C</given-names></name>, <etal>et al</etal>. (<year>2005</year>) <article-title>Progressive transformation of hippocampal neuronal representations in “morphed” environments</article-title>. <source>Neuron</source> <volume>48</volume>: <fpage>345</fpage>–<lpage>358</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Wills1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wills</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Lever</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Cacucci</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Burgess</surname><given-names>N</given-names></name>, <name name-style="western"><surname>O′Keefe</surname><given-names>J</given-names></name> (<year>2005</year>) <article-title>Attractor dynamics in the hippocampal representation of the local environment</article-title>. <source>Science</source> <volume>308</volume>: <fpage>873</fpage>–<lpage>876</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Blumenfeld1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Blumenfeld</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Preminger</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Sagi</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name> (<year>2006</year>) <article-title>Dynamics of memory representations in networks with novelty-facilitated synaptic plasticity</article-title>. <source>Neuron</source> <volume>52</volume>: <fpage>383</fpage>–<lpage>394</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Redish1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Redish</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Jensen</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Johnson</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Kurth-Nelson</surname><given-names>Z</given-names></name> (<year>2007</year>) <article-title>Reconciling reinforcement learning models with behavioral extinction and renewal: Implications for addiction, relapse, and problem gambling</article-title>. <source>Psychological Review</source> <volume>114</volume>: <fpage>784</fpage>–<lpage>805</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Gershman1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gershman</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Blei</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name> (<year>2010</year>) <article-title>Context, learning, and extinction</article-title>. <source>Psychological Review</source> <volume>117</volume>: <fpage>197</fpage>–<lpage>209</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Kurby1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kurby</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Zacks</surname><given-names>JM</given-names></name> (<year>2008</year>) <article-title>Segmentation in the perception and memory of events</article-title>. <source>Trends in Cognitive Sciences</source> <volume>12</volume>: <fpage>72</fpage>–<lpage>79</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Ezzyat1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ezzyat</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Davachi</surname><given-names>L</given-names></name> (<year>2011</year>) <article-title>What constitutes an episode in episodic memory?</article-title> <source>Psychological Science</source> <volume>22</volume>: <fpage>243</fpage>–<lpage>252</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Kalman1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kalman</surname><given-names>R</given-names></name> (<year>1960</year>) <article-title>A new approach to linear filtering and prediction problems</article-title>. <source>Journal of Basic Engineering</source> <volume>82</volume>: <fpage>35</fpage>–<lpage>45</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Fox1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fox</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Sudderth</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Jordan</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Willsky</surname><given-names>A</given-names></name> (<year>2011</year>) <article-title>Bayesian nonparametric inference of switching dynamic linear models</article-title>. <source>Signal Processing, IEEE Transactions on</source> <volume>59</volume>: <fpage>1569</fpage>–<lpage>1585</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Mandelbrot1"><label>29</label>
<mixed-citation publication-type="other" xlink:type="simple">Mandelbrot BB (1982) The Fractal Geometry of Nature. Macmillan.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Fox2"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fox</surname><given-names>EB</given-names></name>, <name name-style="western"><surname>Sudderth</surname><given-names>EB</given-names></name>, <name name-style="western"><surname>Jordan</surname><given-names>MI</given-names></name>, <name name-style="western"><surname>Willsky</surname><given-names>AS</given-names></name> (<year>2011</year>) <article-title>A sticky HDP-HMM with application to speaker diarization</article-title>. <source>The Annals of Applied Statistics</source> <volume>5</volume>: <fpage>1020</fpage>–<lpage>1056</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Gershman2"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gershman</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Blei</surname><given-names>D</given-names></name> (<year>2012</year>) <article-title>A tutorial on Bayesian nonparametric models</article-title>. <source>Journal of Mathematical Psychology</source> <volume>56</volume>: <fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Chater1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chater</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Vitányi</surname><given-names>P</given-names></name> (<year>2003</year>) <article-title>Simplicity: A unifying principle in cognitive science?</article-title> <source>Trends in Cognitive Sciences</source> <volume>7</volume>: <fpage>19</fpage>–<lpage>22</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Aldous1"><label>33</label>
<mixed-citation publication-type="other" xlink:type="simple">Aldous D (1985) Exchangeability and related topics. In: École d′Été de Probabilités de Saint-Flour XIII, Berlin: Springer. pp.1–198.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Anderson2"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anderson</surname><given-names>J</given-names></name> (<year>1991</year>) <article-title>The adaptive nature of human categorization</article-title>. <source>Psychological Review</source> <volume>98</volume>: <fpage>409</fpage>–<lpage>429</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Sanborn1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sanborn</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Griffiths</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Navarro</surname><given-names>D</given-names></name> (<year>2010</year>) <article-title>Rational approximations to rational models: Alternative algorithms for category learning</article-title>. <source>Psychological Review</source> <volume>117</volume>: <fpage>1144</fpage>–<lpage>1167</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Wang1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Dunson</surname><given-names>D</given-names></name> (<year>2011</year>) <article-title>Fast Bayesian inference in Dirichlet process mixture models</article-title>. <source>Journal of Computational and Graphical Statistics</source> <volume>20</volume>: <fpage>196</fpage>–<lpage>216</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Rumelhart1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rumelhart</surname><given-names>DE</given-names></name>, <name name-style="western"><surname>Zipser</surname><given-names>D</given-names></name> (<year>1985</year>) <article-title>Feature discovery by competitive learning*</article-title>. <source>Cognitive Science</source> <volume>9</volume>: <fpage>75</fpage>–<lpage>112</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Grossberg1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grossberg</surname><given-names>S</given-names></name> (<year>1987</year>) <article-title>Competitive learning: From interactive activation to adaptive resonance</article-title>. <source>Cognitive Science</source> <volume>11</volume>: <fpage>23</fpage>–<lpage>63</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Hadas1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hadas</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Intrator</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Yovel</surname><given-names>G</given-names></name> (<year>2010</year>) <article-title>Rapid object category adaptation during unlabelled classification</article-title>. <source>Perception</source> <volume>39</volume>: <fpage>1230</fpage>–<lpage>1239</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-BarShalom1"><label>40</label>
<mixed-citation publication-type="other" xlink:type="simple">Bar-Shalom Y, Li X (1993) Estimation and Tracking: Principles, Techniques, and Software. Artech House.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Gershman3"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gershman</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Jones</surname><given-names>CE</given-names></name>, <name name-style="western"><surname>Norman</surname><given-names>KA</given-names></name>, <name name-style="western"><surname>Monfils</surname><given-names>MH</given-names></name>, <name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name> (<year>2013</year>) <article-title>Gradual extinction prevents the return of fear: Implications for the discovery of state</article-title>. <source>Frontiers in Behavioral Neuroscience</source> <volume>7</volume>: <fpage>164</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Vinogradova1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vinogradova</surname><given-names>O</given-names></name> (<year>2001</year>) <article-title>Hippocampus as comparator: role of the two input and two output systems of the hippocampus in selection and registration of information</article-title>. <source>Hippocampus</source> <volume>11</volume>: <fpage>578</fpage>–<lpage>598</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Bayer1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bayer</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Glimcher</surname><given-names>P</given-names></name> (<year>2005</year>) <article-title>Midbrain dopamine neurons encode a quantitative reward prediction error signal</article-title>. <source>Neuron</source> <volume>47</volume>: <fpage>129</fpage>–<lpage>141</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Friston1"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name> (<year>2005</year>) <article-title>A theory of cortical responses</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source> <volume>360</volume>: <fpage>815</fpage>–<lpage>836</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Gershman4"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gershman</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name> (<year>2012</year>) <article-title>Exploring a latent cause theory of classical conditioning</article-title>. <source>Learning &amp; Behavior</source> <volume>40</volume>: <fpage>255</fpage>–<lpage>268</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Frank1"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frank</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Goldwater</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Griffiths</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname><given-names>J</given-names></name> (<year>2010</year>) <article-title>Modeling human performance in statistical word segmentation</article-title>. <source>Cognition</source> <volume>117</volume>: <fpage>107</fpage>–<lpage>125</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Collins1"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Collins</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Koechlin</surname><given-names>E</given-names></name> (<year>2012</year>) <article-title>Reasoning, learning, and creativity: Frontal lobe function and human decision-making</article-title>. <source>PLoS Biology</source> <volume>10</volume>: <fpage>e1001293</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Austerweil1"><label>48</label>
<mixed-citation publication-type="other" xlink:type="simple">Austerweil J, Gershman S, Tenenbaum J, Griffiths T (2014) Structure and flexibility in bayesian models of cognition. In:BusemeyerJTownsendJWangZEidelsAeditorsOxford Handbook of Computational and Mathematical PsychologyOxfordOxford University Press</mixed-citation>
</ref>
<ref id="pcbi.1003939-MacKay1"><label>49</label>
<mixed-citation publication-type="other" xlink:type="simple">MacKay DJ (2003) Information Theory, Inference and Learning Algorithms. Cambridge university press.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Gershman5"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gershman</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name> (<year>2013</year>) <article-title>Perceptual estimation obeys Occam's razor</article-title>. <source>Frontiers in Psychology</source> <volume>4</volume>: <fpage>623</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Denve1"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Denève</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Duhamel</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name> (<year>2007</year>) <article-title>Optimal sensorimotor integration in recurrent cortical networks: a neural implementation of Kalman filters</article-title>. <source>The Journal of neuroscience</source> <volume>27</volume>: <fpage>5744</fpage>–<lpage>5756</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Wilson1"><label>52</label>
<mixed-citation publication-type="other" xlink:type="simple">Wilson R, Finkel L (2009) A neural implementation of the Kalman filter. In:BengioYSchuurmansDLaffertyJWilliamsCKICulottaAeditors Advances in Neural Information Processing Systems 22.pp2062–2070.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Gallistel1"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gallistel</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Mark</surname><given-names>T</given-names></name>, <name name-style="western"><surname>King</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Latham</surname><given-names>P</given-names></name> (<year>2001</year>) <article-title>The rat approximates an ideal detector of changes in rates of reward: Implications for the law of effect</article-title>. <source>Journal of Experimental Psychology: Animal Behavior Processes</source> <volume>27</volume>: <fpage>354</fpage>–<lpage>372</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Steyvers1"><label>54</label>
<mixed-citation publication-type="other" xlink:type="simple">Steyvers M, Brown S (2005) Prediction and change detection. In: Advances in Neural Information Processing Systems. pp.1281–1288.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Brown1"><label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brown</surname><given-names>SD</given-names></name>, <name name-style="western"><surname>Steyvers</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>Detecting and predicting changes</article-title>. <source>Cognitive Psychology</source> <volume>58</volume>: <fpage>49</fpage>–<lpage>67</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Nassar1"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nassar</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Wilson</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Heasly</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Gold</surname><given-names>J</given-names></name> (<year>2010</year>) <article-title>An approximately Bayesian delta-rule model explains the dynamics of belief updating in a changing environment</article-title>. <source>The Journal of Neuroscience</source> <volume>30</volume>: <fpage>12366</fpage>–<lpage>12378</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Summerfield1"><label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Summerfield</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Behrens</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Koechlin</surname><given-names>E</given-names></name> (<year>2011</year>) <article-title>Perceptual classification in a rapidly changing environment</article-title>. <source>Neuron</source> <volume>71</volume>: <fpage>725</fpage>–<lpage>736</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Wilson2"><label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name> (<year>2011</year>) <article-title>Inferring relevance in a changing world</article-title>. <source>Frontiers in Human Neuroscience</source> <volume>5</volume>: <fpage>189</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Wilson3"><label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname><given-names>RC</given-names></name>, <name name-style="western"><surname>Nassar</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Gold</surname><given-names>JI</given-names></name> (<year>2013</year>) <article-title>A mixture of delta-rules approximation to Bayesian inference in change-point problems</article-title>. <source>PLoS Computational Biology</source> <volume>9</volume>: <fpage>e1003150</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Ratcliff1"><label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ratcliff</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Clark</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Shiffrin</surname><given-names>R</given-names></name> (<year>1990</year>) <article-title>List-strength effect: I. Data and discussion</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>16</volume>: <fpage>163</fpage>–<lpage>178</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Gebhart1"><label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gebhart</surname><given-names>AL</given-names></name>, <name name-style="western"><surname>Aslin</surname><given-names>RN</given-names></name>, <name name-style="western"><surname>Newport</surname><given-names>EL</given-names></name> (<year>2009</year>) <article-title>Changing structures in midstream: Learning along the statistical garden path</article-title>. <source>Cognitive Science</source> <volume>33</volume>: <fpage>1087</fpage>–<lpage>1116</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Mensink1"><label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mensink</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Raaijmakers</surname><given-names>J</given-names></name> (<year>1988</year>) <article-title>A model for interference and forgetting</article-title>. <source>Psychological Review</source> <volume>95</volume>: <fpage>434</fpage>–<lpage>455</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Howard1"><label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Howard</surname><given-names>MW</given-names></name>, <name name-style="western"><surname>Kahana</surname><given-names>MJ</given-names></name> (<year>2002</year>) <article-title>A distributed representation of temporal context</article-title>. <source>Journal of Mathematical Psychology</source> <volume>46</volume>: <fpage>269</fpage>–<lpage>299</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Socher1"><label>64</label>
<mixed-citation publication-type="other" xlink:type="simple">Socher R, Gershman S, Sederberg P, Norman K, Perotte AJ, et al. (2009) A Bayesian analysis of dynamics in free recall. In: Advances in Neural Information Processing Systems. pp.1714–1722.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Howard2"><label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Howard</surname><given-names>MW</given-names></name>, <name name-style="western"><surname>Shankar</surname><given-names>KH</given-names></name>, <name name-style="western"><surname>Jagadisan</surname><given-names>UK</given-names></name> (<year>2011</year>) <article-title>Constructing semantic representations from a gradually changing representation of temporal context</article-title>. <source>Topics in Cognitive Science</source> <volume>3</volume>: <fpage>48</fpage>–<lpage>73</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Bartlett1"><label>66</label>
<mixed-citation publication-type="other" xlink:type="simple">Bartlett FC (1932) Remembering: An Experimental and Social Study. Cambridge University Press.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Hemmer1"><label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hemmer</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Steyvers</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>A bayesian account of reconstructive memory</article-title>. <source>Topics in Cognitive Science</source> <volume>1</volume>: <fpage>189</fpage>–<lpage>202</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Hemmer2"><label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hemmer</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Steyvers</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>Integrating episodic memories and prior knowledge at multiple levels of abstraction</article-title>. <source>Psychonomic Bulletin &amp; Review</source> <volume>16</volume>: <fpage>80</fpage>–<lpage>87</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Love1"><label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Love</surname><given-names>BC</given-names></name>, <name name-style="western"><surname>Medin</surname><given-names>DL</given-names></name>, <name name-style="western"><surname>Gureckis</surname><given-names>TM</given-names></name> (<year>2004</year>) <article-title>Sustain: a network model of category learning</article-title>. <source>Psychological Review</source> <volume>111</volume>: <fpage>309</fpage>–<lpage>332</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Vanpaemel1"><label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vanpaemel</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Storms</surname><given-names>G</given-names></name> (<year>2008</year>) <article-title>In search of abstraction: The varying abstraction model of categorization</article-title>. <source>Psychonomic Bulletin &amp; Review</source> <volume>15</volume>: <fpage>732</fpage>–<lpage>749</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-VonRestorff1"><label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Von Restorff</surname><given-names>H</given-names></name> (<year>1933</year>) <article-title>Über die wirkung von bereichsbildungen im spurenfeld</article-title>. <source>Psychologische Forschung</source> <volume>18</volume>: <fpage>299</fpage>–<lpage>342</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Kagerer1"><label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kagerer</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Contreras-Vidal</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Stelmach</surname><given-names>G</given-names></name> (<year>1997</year>) <article-title>Adaptation to gradual as compared with sudden visuo-motor distortions</article-title>. <source>Experimental Brain Research</source> <volume>115</volume>: <fpage>557</fpage>–<lpage>561</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Pekny1"><label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pekny</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Criscimagna-Hemminger</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Shadmehr</surname><given-names>R</given-names></name> (<year>2011</year>) <article-title>Protection and expression of human motor memories</article-title>. <source>The Journal of Neuroscience</source> <volume>31</volume>: <fpage>13829</fpage>–<lpage>13839</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Taylor1"><label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Taylor</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Wojaczynski</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Ivry</surname><given-names>R</given-names></name> (<year>2011</year>) <article-title>Trial-by-trial analysis of intermanual transfer during visuomotor adaptation</article-title>. <source>Journal of Neurophysiology</source> <volume>106</volume>: <fpage>3157</fpage>–<lpage>3172</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003939-Yamamoto1"><label>75</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yamamoto</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Hoffman</surname><given-names>DS</given-names></name>, <name name-style="western"><surname>Strick</surname><given-names>PL</given-names></name> (<year>2006</year>) <article-title>Rapid and long-lasting plasticity of input-output mapping</article-title>. <source>Journal of Neurophysiology</source> <volume>96</volume>: <fpage>2797</fpage>–<lpage>2801</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>