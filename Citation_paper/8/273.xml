<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pone.0124290</article-id>
<article-id pub-id-type="publisher-id">PONE-D-14-54051</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Intra-Rater and Inter-Rater Reliability of a Medical Record Abstraction Study on Transition of Care after Childhood Cancer</article-title>
<alt-title alt-title-type="running-head">Reliability of a Medical Record Abstraction Study</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Gianinazzi</surname>
<given-names>Micòl E.</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Rueegg</surname>
<given-names>Corina S.</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Zimmerman</surname>
<given-names>Karin</given-names>
</name>
<xref rid="aff002" ref-type="aff"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Kuehni</surname>
<given-names>Claudia E.</given-names>
</name>
<xref rid="aff003" ref-type="aff"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Michel</surname>
<given-names>Gisela</given-names>
</name>
<xref rid="cor001" ref-type="corresp">*</xref>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<collab xlink:type="simple">the Swiss Paediatric Oncology Group (SPOG)</collab>
<xref rid="fn001" ref-type="fn"><sup>¶</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Health Sciences and Health Policy, University of Lucerne, Lucerne, Switzerland</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Pediatric Hematology/Oncology, University Children’s Hospital, Bern, Switzerland</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Swiss Childhood Cancer Registry, Institute of Social and Preventive Medicine, University of Bern, Bern, Switzerland</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Matsuo</surname>
<given-names>Keitaro</given-names>
</name>
<role>Academic Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Kyushu University Faculty of Medical Science, JAPAN</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: MEG CSR GM. Performed the experiments: MEG. Analyzed the data: MEG. Contributed reagents/materials/analysis tools: MEG CSR GM. Wrote the paper: MEG CSR KZ CEK GM.</p>
</fn>
<fn fn-type="other" id="fn001">
<p>¶ Membership of the Swiss Paediatric Oncology Group (SPOG) is listed in the Acknowledgments.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">gisela.michel@unilu.ch</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>22</day>
<month>5</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="collection">
<year>2015</year>
</pub-date>
<volume>10</volume>
<issue>5</issue>
<elocation-id>e0124290</elocation-id>
<history>
<date date-type="received">
<day>2</day>
<month>12</month>
<year>2014</year>
</date>
<date date-type="accepted">
<day>5</day>
<month>3</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Gianinazzi et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0124290" xlink:type="simple"/>
<abstract>
<sec id="sec001">
<title>Background</title>
<p>The abstraction of data from medical records is a widespread practice in epidemiological research. However, studies using this means of data collection rarely report reliability. Within the Transition after Childhood Cancer Study (TaCC) which is based on a medical record abstraction, we conducted a second independent abstraction of data with the aim to assess a) intra-rater reliability of one rater at two time points; b) the possible learning effects between these two time points compared to a gold-standard; and c) inter-rater reliability.</p>
</sec>
<sec id="sec002">
<title>Method</title>
<p>Within the TaCC study we conducted a systematic medical record abstraction in the 9 Swiss clinics with pediatric oncology wards. In a second phase we selected a subsample of medical records in 3 clinics to conduct a second independent abstraction. We then assessed intra-rater reliability at two time points, the learning effect over time (comparing each rater at two time-points with a gold-standard) and the inter-rater reliability of a selected number of variables. We calculated percentage agreement and Cohen’s kappa.</p>
</sec>
<sec id="sec003">
<title>Findings</title>
<p>For the assessment of the intra-rater reliability we included 154 records (80 for rater 1; 74 for rater 2). For the inter-rater reliability we could include 70 records. Intra-rater reliability was substantial to excellent (Cohen’s kappa 0-6-0.8) with an observed percentage agreement of 75%-95%. In all variables learning effects were observed. Inter-rater reliability was substantial to excellent (Cohen’s kappa 0.70-0.83) with high agreement ranging from 86% to 100%.</p>
</sec>
<sec id="sec004">
<title>Conclusions</title>
<p>Our study showed that data abstracted from medical records are reliable. Investigating intra-rater and inter-rater reliability can give confidence to draw conclusions from the abstracted data and increase data quality by minimizing systematic errors.</p>
</sec>
</abstract>
<funding-group>
<funding-statement>This work was supported by the Swiss National Science Foundation (Ambizione grant PZ00P3_121682/1 and PZ00P3-141722 to GM); the Swiss Cancer League (grant KLS-01605-10-2004, KLS-2215-02-2008, KFS-02631-08-2010, KLS-02783-02-2011); Cancer League Bern; and Stiftung zur Krebsbekämpfung. The work of the Swiss Childhood Cancer Registry is supported by the Swiss Paediatric Oncology Group (<ext-link ext-link-type="uri" xlink:href="http://www.spog.ch" xlink:type="simple">www.spog.ch</ext-link>), Schweizerische Konferenz der kantonalen Gesundheitsdirektorinnen und –direktoren (<ext-link ext-link-type="uri" xlink:href="http://www.gdk-cds.ch" xlink:type="simple">www.gdk-cds.ch</ext-link>), Swiss Cancer Research (<ext-link ext-link-type="uri" xlink:href="http://www.krebsforschung.ch" xlink:type="simple">www.krebsforschung.ch</ext-link>), Kinderkrebshilfe Schweiz (<ext-link ext-link-type="uri" xlink:href="http://www.kinderkrebshilfe.ch" xlink:type="simple">www.kinderkrebshilfe.ch</ext-link>), Ernst-Göhner Stiftung, Stiftung Domarena, and National Institute of Cancer Epidemiology and Registration (<ext-link ext-link-type="uri" xlink:href="http://www.nicer.ch" xlink:type="simple">www.nicer.ch</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="4"/>
<table-count count="6"/>
<page-count count="13"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec005" sec-type="intro">
<title>Introduction</title>
<p>The abstraction of data from patients’ medical records (MR) is a widespread practice in epidemiological research, especially in retrospective studies [<xref rid="pone.0124290.ref001" ref-type="bibr">1</xref>–<xref rid="pone.0124290.ref003" ref-type="bibr">3</xref>]. Often, however, the reliability and internal validity of such data is questionable. This has several reasons. Firstly, data written in MR have not been produced for research purposes and the adequateness of such data for the study’s research question needs to be addressed [<xref rid="pone.0124290.ref002" ref-type="bibr">2</xref>]. Secondly, poor reliability due to the potential intra and inter-rater variance limits internal validity of the results. This is particularly true for multicenter studies in which several raters are involved, data collection conditions vary, MR formats differ, data come from different time periods and the data collection leaves room for interpretation [<xref rid="pone.0124290.ref001" ref-type="bibr">1</xref>]. For this reasons it is important to report reliability of such studies. Further, this can help to improve the collection process, to reduce and correct problems or discrepancies, and, later, to gain confidence in the conclusions that will be drawn [<xref rid="pone.0124290.ref004" ref-type="bibr">4</xref>]. Despite the importance of reporting such measures, only few studies actually do so [<xref rid="pone.0124290.ref002" ref-type="bibr">2</xref>,<xref rid="pone.0124290.ref004" ref-type="bibr">4</xref>–<xref rid="pone.0124290.ref008" ref-type="bibr">8</xref>]. In general, published retrospective chart reviews which assessed reliability report good reliability levels for their abstracted data, but we have to remember that publication bias might be a problem in this type of study with only studies with positive results being published.</p>
<p>The project “Transition after Childhood Cancer (TaCC)” aims to assess the transition from pediatric to adult care of childhood cancer survivors in Switzerland by collecting data from MR in nine clinics and in three language regions. Because no previous study assessed transition using a systematic chart review for data collection we had to develop and pilot an abstraction form based on available literature and project aims. For these reasons we found it important to assess the reliability of collected data by investigating a) intra-rater reliability of two raters at two time points; b) the possible learning effects over time comparing each rater to a gold-standard at two time points; and c) inter-rater reliability.</p>
</sec>
<sec id="sec006" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec007">
<title>Ethics statement</title>
<p>Ethics approval was provided through the general cancer registry permission of the Swiss Childhood Cancer Registry (The Swiss Federal Commission of Experts for Professional Secrecy in Medical Research) and a non obstat statement was obtained from the ethics committee of the canton of Bern, stating that no additional ethics permission and no additional informed consent was necessary. All information regarding individuals was made anonymous to investigators prior to analysis.</p>
</sec>
<sec id="sec008">
<title>Study population</title>
<p>The «Transition after Childhood Cancer (TaCC) » study is a retrospective multicenter study conducted within the population-based Swiss Childhood Cancer Registry (SCCR). For nearly four decades the SCCR has been collecting data on all patients diagnosed with leukemia, lymphoma, central nervous system (CNS) tumors, malignant solid tumors or Langerhans cell histiocytosis before the age of 21 years [<xref rid="pone.0124290.ref009" ref-type="bibr">9</xref>,<xref rid="pone.0124290.ref010" ref-type="bibr">10</xref>]. The TaCC study included a stratified (by diagnosis and treating clinic) randomly selected sample of patients registered in the SCCR, who were diagnosed with childhood cancer at an age between 0 and 15 years, who survived ≥ 5 years and were aged ≥16 years at the time of this study.</p>
</sec>
<sec id="sec009">
<title>Initial data collection for the TaCC study</title>
<p>Within the TaCC study we conducted a systematic MR abstraction at the 9 clinics with pediatric oncology wards throughout Switzerland (all clinics were affiliated to the Swiss Paediatric Oncology Group). Data collection started in March 2012 and ended in April 2013. For data collection we utilized a standardized abstraction form on hardcopy, which we developed using the available literature on chart reviews [<xref rid="pone.0124290.ref011" ref-type="bibr">11</xref>]. As suggested by the guidelines, we piloted the abstraction form in three of the nine clinics, before the actual data abstraction started. We collected data on the following main categories: frequency of follow-up visits after the age 16 years, medical professionals involved, discharge (patient discharged from pediatric oncology without being transferred), date of discharge, discharge planned, date of planned discharge, transfer (patient transferred from pediatric oncology to an adult medical professional), transfer destination, date of transfer, missed follow-up appointments (the patient did not go to a visit). We digitally photographed all relevant documents as “back up” and saved them on secure servers. Following data collection, we used Epidata 3.1 to enter our data into a database. All baseline demographic or clinical information were directly extracted from the SCCR database.</p>
</sec>
<sec id="sec010">
<title>Sample for reliability assessment</title>
<p>The number of re-abstractions we carried out was based on the number of medical charts available containing information about the variables to be extracted as well as on formal sample size calculations for the kappa statistic [<xref rid="pone.0124290.ref012" ref-type="bibr">12</xref>]. Using alpha and beta error rates of 0.05 and 0.2, respectively, when testing for a statistical difference between moderate (i.e., 0.40) and high (i.e., 0.75) kappa values, sample size estimates ranged from 77 to 28 when the trait prevalence was varied between 10% and 50%. Thus, our sample sizes for intra-rater reliability and inter-rater provided the needed power to detect differences. We selected all medical records that did not have any missing values in the variables under investigation. We conducted the re-abstraction in the first three clinics of the same language region.</p>
</sec>
<sec id="sec011">
<title>Re-abstraction</title>
<p>For the re-abstraction we focused exclusively on the most important variables, namely: the variables “still in pediatric follow-up (yes, no)”, “transferred (yes, no)”, “discharged (yes, no)”, “transfer destination” (general practitioner, adult oncologist, other specialist), and the date variables “date of transfer”, “date of discharge” and “date of next visit in pediatric oncology” (<xref rid="pone.0124290.t001" ref-type="table">Table 1</xref>).</p>
<table-wrap id="pone.0124290.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0124290.t001</object-id>
<label>Table 1</label> <caption><title>Variables assessed in the re-abstraction.</title></caption>
<alternatives>
<graphic id="pone.0124290.t001g" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0124290.t001" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1">Variable</th>
<th align="left" rowspan="1" colspan="1">Description</th>
<th align="left" rowspan="1" colspan="1">Categories</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Transfer</bold></td>
<td align="left" rowspan="1" colspan="1">This variable is used to assess whether the patient has been transferred from pediatric oncology to an <italic>adult</italic> medical professional.</td>
<td align="left" rowspan="1" colspan="1">Yes/No</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Discharge</bold></td>
<td align="left" rowspan="1" colspan="1">This variable is used to assess whether the patient has been discharged from pediatric oncology without being referred to another medical professional.</td>
<td align="left" rowspan="1" colspan="1">Yes/No</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>In Follow-up</bold></td>
<td align="left" rowspan="1" colspan="1">This variable is used to assess whether the patient had regular follow-up visits in pediatric oncology at the time of data collection</td>
<td align="left" rowspan="1" colspan="1">Yes/No</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Transfer destination</bold></td>
<td align="left" rowspan="1" colspan="1">This variable is used to assess to which adult health professional the patient has been transferred to.</td>
<td align="left" rowspan="1" colspan="1">1.General Practitioner<break/>2. Adult Oncologist<break/>3. Other Specialist</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Transfer date</bold></td>
<td align="left" rowspan="1" colspan="1">Here the day, month and year of transfer had to be indicated.</td>
<td align="left" rowspan="1" colspan="1">dd, mm, yyyy</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Discharge date</bold></td>
<td align="left" rowspan="1" colspan="1">Here the day, month and year of discharge had to be indicated.</td>
<td align="left" rowspan="1" colspan="1">dd, mm, yyyy</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Date of next visit in pediatric oncology</bold></td>
<td align="left" rowspan="1" colspan="1">Here the day, month and year of the next visit at pediatric oncology have to be indicated.</td>
<td align="left" rowspan="1" colspan="1">dd, mm, yyyy</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t001fn001"><p>The categorical variables represent a more challenging collection than date variables because the corresponding information had to be found in free text and often necessitated an interpretation.</p></fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="sec012">
<title>Medical record <italic>raters</italic></title>
<p>Three study raters were chosen to carry out the abstraction in the different clinics based on their linguistic knowledge (they had to be proficient in all national languages) and on their level of education. All raters held a degree at the Master level, one in pedagogy/psychology, the second in social sciences and the third in biology. None of the raters had clinical experience, which we believed was not necessary for the purpose of this abstraction. One of these three raters (Master in Biology) had joined the research team later and was therefore excluded from the reliability study.</p>
<p>Both raters included in the reliability study were trained prior to data collection for the most important concepts assessed in the TaCC study. To measure intra-rater reliability the 2 raters abstracted a selected sample of medical records at two points in time. Both raters did not have access to the results collected at time 1.</p>
<p>To investigate possible learning effects between time point 1 and time point 2, the project manager (MEG) also abstracted the data of the same patients. These data were considered the gold standard and results of the two raters at time point 1 and 2 were than compared against the gold standard. To assess inter-rater reliability the 2 raters independently abstracted data of the same study subjects at time point 2.</p>
</sec>
<sec id="sec013">
<title>Statistical analysis</title>
<p>We performed all analyses using Stata 12.0 (StataCorp, College Station, TX). We first calculated percentage agreement, i.e. the proportion of assessments in which the two observations agreed, Cohen’s kappa and Prevalence-Adjusted Bias-Adjusted Kappa (PABAK) for all variable in intra-rater and inter-rater comparison [<xref rid="pone.0124290.ref013" ref-type="bibr">13</xref>]. For the intra-rater reliability analysis we present results per rater when possible. To assess possible learning effects between point in time 1 and point in time 2, we calculated the Cohen’s kappa between data collected by each rater at the two points in time and the data collected by the project manager MEG (gold standard).</p>
</sec>
<sec id="sec014">
<title>Cohen’s Kappa and Adjusted Kappa</title>
<p>Kappa indicates a numeric rating of the degree of agreement between two raters, taking into account the degree of agreement which would be expected by chance. The calculation of Cohen’s kappa is based on the difference between the agreement that is actually present (Pr<sub>a</sub>) and the agreement obtained by chance alone (Pr<sub>e</sub>) (Formula <xref rid="pone.0124290.e001" ref-type="disp-formula">1</xref>) [<xref rid="pone.0124290.ref014" ref-type="bibr">14</xref>]. Kappa’s values range from 0 to 1 with 0 meaning “less than chance agreement” and 1 “almost perfect agreement”.</p>
<disp-formula id="pone.0124290.e001">
<alternatives>
<graphic id="pone.0124290.e001g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0124290.e001" xlink:type="simple"/>
<mml:math display="block" id="M1" overflow="scroll">
<mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>a</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mtext>Pr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mtext>Pr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>e</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math>
</alternatives>
<label>(Formula 1)</label>
</disp-formula>
<p>Formula <xref rid="pone.0124290.e001" ref-type="disp-formula">1</xref>, however, does not take into account the bias between observers (the extent of disagreement) or the distribution of data across the categories that are used (prevalence). The following example will show how identical agreement can lead to different coefficients of kappa because of the different prevalence of data across the categories.</p>
<p>In both Tables <xref rid="pone.0124290.t002" ref-type="table">2</xref> and <xref rid="pone.0124290.t003" ref-type="table">3</xref> there is equal agreement (60 from yes and no: 25+35 and 45+ 15). However, if we apply Formula <xref rid="pone.0124290.e001" ref-type="disp-formula">1</xref> to calculate Cohen’s kappa we will end up with different results (K<sub>1</sub> = 0.1304 and K<sub>2</sub> = 0.2593). This difference in results occurs because of the different distribution of data in the 2x2 cells (the so called prevalence) [<xref rid="pone.0124290.ref012" ref-type="bibr">12</xref>].</p>
<table-wrap id="pone.0124290.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0124290.t002</object-id>
<label>Table 2</label> <caption><title>Kappa example 1.</title></caption>
<alternatives>
<graphic id="pone.0124290.t002g" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0124290.t002" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
</colgroup>
<thead>
<tr>
<th rowspan="4" align="left" colspan="1">Rater 2</th>
<th colspan="3" align="center" rowspan="1">Rater 1</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Yes</td>
<td align="left" rowspan="1" colspan="1">No</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Yes</td>
<td align="left" rowspan="1" colspan="1">45</td>
<td align="left" rowspan="1" colspan="1">15</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">No</td>
<td align="left" rowspan="1" colspan="1">25</td>
<td align="left" rowspan="1" colspan="1">15</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="pone.0124290.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0124290.t003</object-id>
<label>Table 3</label> <caption><title>Kappa example 2.</title></caption>
<alternatives>
<graphic id="pone.0124290.t003g" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0124290.t003" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
</colgroup>
<thead>
<tr>
<th rowspan="4" align="left" colspan="1">Rater 2</th>
<th colspan="3" align="center" rowspan="1">Rater 1</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Yes</td>
<td align="left" rowspan="1" colspan="1">No</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Yes</td>
<td align="left" rowspan="1" colspan="1">25</td>
<td align="left" rowspan="1" colspan="1">35</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">No</td>
<td align="left" rowspan="1" colspan="1">5</td>
<td align="left" rowspan="1" colspan="1">35</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The interpretation of kappa alone without any indication of prevalence or bias can be imprecise. To overcome this problem an alternative form of kappa has been proposed which takes into account both bias and prevalence [<xref rid="pone.0124290.ref015" ref-type="bibr">15</xref>]. This is summarized by the Prevalence-Adjusted Bias-Adjusted Kappa (PABAK). PABAK gives the proportion of agreement beyond expected chance agreement regardless of unbalanced data patterns. The interpretation of PABAK is the same as for kappa. If we consider a 2x2 table like the one in <xref rid="pone.0124290.t004" ref-type="table">Table 4</xref>, PABAK is calculated as in Formula <xref rid="pone.0124290.e002" ref-type="disp-formula">2</xref>.</p>
<table-wrap id="pone.0124290.t004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0124290.t004</object-id>
<label>Table 4</label> <caption><title>Example for the Prevalence-Adjusted Bias-Adjusted Kappa (PABAK).</title></caption>
<alternatives>
<graphic id="pone.0124290.t004g" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0124290.t004" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
</colgroup>
<thead>
<tr>
<th rowspan="4" align="left" colspan="1"> Rater 2</th>
<th colspan="3" align="center" rowspan="1">Rater 1</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Yes</td>
<td align="left" rowspan="1" colspan="1">No</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Yes</td>
<td align="left" rowspan="1" colspan="1">a</td>
<td align="left" rowspan="1" colspan="1">b</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">No</td>
<td align="left" rowspan="1" colspan="1">c</td>
<td align="left" rowspan="1" colspan="1">d</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<disp-formula id="pone.0124290.e002">
<alternatives>
<graphic id="pone.0124290.e002g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0124290.e002" xlink:type="simple"/>
<mml:math display="block" id="M2" overflow="scroll">
<mml:mrow><mml:mi>P</mml:mi><mml:mi>A</mml:mi><mml:mi>B</mml:mi><mml:mi>A</mml:mi><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:mrow></mml:math>
</alternatives>
<label>(Formula 2)</label>
</disp-formula>
</sec>
<sec id="sec015">
<title>Interpretation of Cohen’s kappa</title>
<p>To interpret our result we used as benchmark the cut-off proposed by Landis and Koch [<xref rid="pone.0124290.ref016" ref-type="bibr">16</xref>] according to whom Cohen’s kappas ≥ 0.80 represent excellent agreement, coefficients between 0.61 and 0.80 represent substantial agreement, coefficients between 0.41 and 0.61 moderate agreement and &lt;0.41 fair to poor agreement.</p>
</sec>
</sec>
<sec id="sec016" sec-type="results">
<title>Results</title>
<sec id="sec017">
<title>Sample</title>
<p>The final analysis included 154 records for the assessment of intra-rater reliability. Of these, 80 had been viewed by rater 1 and 74 by rater 2. Mean time between first (point in time 1) and second abstraction (point in time 2) was 7.6 months (SD = 2.2), range (2.1–10.3 months). For the assessment of inter-rater reliability we included 70 records (<xref rid="pone.0124290.g001" ref-type="fig">Fig 1</xref>).</p>
<fig id="pone.0124290.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0124290.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Flow chart of sample selection for reliability assessment.</title>
<p>Fig 1 shows the flow chart of our study population starting from those eligible to those included in the analysis.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0124290.g001" position="float" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec018">
<title>Intra-rater reliability</title>
<p>Overall, all variables assessed had substantial (Cohen’s kappa ≥ 0.6) to excellent agreement (Cohen’s kappa ≥ 0.8) with an observed percentage agreement ranging from 75% (date of next visit in pediatric oncology) to 95% (date of transfer) (<xref rid="pone.0124290.g002" ref-type="fig">Fig 2</xref>).</p>
<fig id="pone.0124290.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0124290.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Kappa values and their interpretation for intra-rater and inter-rater reliability.</title>
<p>Fig 2 shows the values of kappa for intra-rater (dark blue) and for inter-rater (light blue) reliability with their confidence intervals T for each variable under investigation.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0124290.g002" position="float" xlink:type="simple"/>
</fig>
<p>After taking into account prevalence and bias, PABAK was higher for all variables and ranged from 0.64 to 0.81 than the unadjusted kappa values (<xref rid="pone.0124290.t005" ref-type="table">Table 5</xref>; <xref rid="pone.0124290.g003" ref-type="fig">Fig 3</xref>).</p>
<fig id="pone.0124290.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0124290.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Kappa values and Prevalence-adjusted Bias-adjusted kappa values for intra-rater (a) and inter-rater reliability (b).</title>
<p>Fig 3a and 3b show the values of kappa compared to the the values obtained by calculating the Prevalence-adjusted Bias-adjusted kappa for intra-rater reliability (a) and inter-rater reliability (b).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0124290.g003" position="float" xlink:type="simple"/>
</fig>
<table-wrap id="pone.0124290.t005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0124290.t005</object-id>
<label>Table 5</label> <caption><title>Intra-rater reliability.</title></caption>
<alternatives>
<graphic id="pone.0124290.t005g" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0124290.t005" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1">Variable</th>
<th align="left" rowspan="1" colspan="1">Agreement</th>
<th align="left" rowspan="1" colspan="1">kappa</th>
<th align="left" rowspan="1" colspan="1">95% CI</th>
<th align="left" rowspan="1" colspan="1">PABAK</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Transfer</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>82%</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.62</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.43–0.79</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.64</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">    <italic>Rater 1</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>85%</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>67</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>43–0</italic>.<italic>91</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>73</italic></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">    <italic>Rater 2</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>77%</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>48</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>20–0</italic>.<italic>73</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>58</italic></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Discharge</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>84%</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.63</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.47–0.79</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.78</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">    <italic>Rater 1</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>90%</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>77</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>57–0</italic>.<italic>98</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>79</italic></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">    <italic>Rater 2</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>75%</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>45</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>22–0</italic>.<italic>75</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>47</italic></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>In Follow-up</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>88%</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.76</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.51–0.87</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.81</bold></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">    <italic>Rater 1</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>90%</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>77</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>64–0</italic>.<italic>91</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>80</italic></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">    <italic>Rater 2</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>86%</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>58</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>49–0</italic>.<italic>67</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>71</italic></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Transfer destination</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>88%</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.69</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.64–0.88</bold></td>
<td align="left" rowspan="1" colspan="1">n.a.<sup>d</sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">    <italic>Rater 1</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>91%</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>74</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>62–0</italic>.<italic>86</italic></td>
<td align="left" rowspan="1" colspan="1">n.a.<sup>d</sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">    <italic>Rater 2</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>82%</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>51</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>40–0</italic>.<italic>62</italic></td>
<td align="left" rowspan="1" colspan="1">n.a.<sup>d</sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Date of transfer</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>95%</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.94</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.89–1.00</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>n.a.</bold><sup><bold>d</bold></sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">    <italic>Rater 1</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>91%</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>89</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>74–1</italic>.<italic>00</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>n</italic>.<italic>a</italic>.</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">    <italic>Rater 2</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>100%</italic></td>
<td align="left" rowspan="1" colspan="1">1.00</td>
<td align="left" rowspan="1" colspan="1"><italic>-</italic></td>
<td align="left" rowspan="1" colspan="1">n.a.</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Date of discharge</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>93%</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.93</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.81–0.95</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>n.a.</bold><sup><bold>d</bold></sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">    <italic>Rater 1</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>100%</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>1</italic>.<italic>00</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>-</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>n</italic>.<italic>a</italic>.<sup><italic>d</italic></sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">    <italic>Rater 2</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>86%</italic></td>
<td align="left" rowspan="1" colspan="1">0.85</td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>66–0</italic>.<italic>91</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>n</italic>.<italic>a</italic>.<sup><italic>d</italic></sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Date of next visit</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>75%</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.70</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>0.50–0.82</bold></td>
<td align="left" rowspan="1" colspan="1"><bold>n.a.</bold><sup><bold>d</bold></sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">    <italic>Rater 1</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>78%</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>70</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>45–0</italic>.<italic>82</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>n</italic>.<italic>a</italic>.<sup><italic>d</italic></sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">    <italic>Rater 2</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>67%</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>50</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>0</italic>.<italic>40–0</italic>.<italic>81</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>n</italic>.<italic>a</italic>.<sup><italic>d</italic></sup></td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t005fn001"><p>Abbreviations: CI, Confidence Interval; kappa, Cohen’s kappa; n.a., not applicable; PABAK, Prevalence and Bias Adjusted Kappa.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>The variable “in follow-up” had the highest Cohen’s kappa (k = 0.76), while transfer and discharge had the lowest (k = 0.62 and k = 0.63) (<xref rid="pone.0124290.g002" ref-type="fig">Fig 2</xref>).</p>
<p>Date variables had Cohen’s kappas (k) above 0.9 except “date of next visit” were k was 0.7.</p>
<p>When looking at the results stratified by the two raters, we could see that Cohen’s kappas were consistently higher for rater 1 than for rater 2. Especially for the variables transfer and discharge rater 2 had Cohen’s kappas &lt; 0.5 (<xref rid="pone.0124290.t005" ref-type="table">Table 5</xref>).</p>
</sec>
<sec id="sec019">
<title>Learning effects</title>
<p>When looking at learning effects between point in time 1 and point in time 2 against the data collected by the project manager MEG we can see that kappa’s values greatly improved for both raters (all p<sub>s</sub> &lt;0.001) (<xref rid="pone.0124290.g004" ref-type="fig">Fig 4</xref>).</p>
<fig id="pone.0124290.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0124290.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Learning effect of the two raters at two points in time compared with the abstraction of the project manager.</title>
<p>Fig 4a and 4b show the comparison of the abstraction at two points in time of rater 1 (a) and rater 2 (b) compared to the chosen golden standard (abstraction of the project manager).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0124290.g004" position="float" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec020">
<title>Inter-rater reliability</title>
<p>For the variables transfer, discharge, in follow up and transfer destination the observed agreement was high ranging from 86% to 91% (<xref rid="pone.0124290.t006" ref-type="table">Table 6</xref>).</p>
<table-wrap id="pone.0124290.t006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0124290.t006</object-id>
<label>Table 6</label> <caption><title>Inter-rater reliability.</title></caption>
<alternatives>
<graphic id="pone.0124290.t006g" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0124290.t006" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1">Variable</th>
<th align="left" rowspan="1" colspan="1">Agreement</th>
<th align="left" rowspan="1" colspan="1">kappa</th>
<th align="left" rowspan="1" colspan="1">95% CI</th>
<th align="left" rowspan="1" colspan="1">PABAK</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Transfer</bold></td>
<td align="left" rowspan="1" colspan="1">88%</td>
<td align="left" rowspan="1" colspan="1"><bold>0.73</bold></td>
<td align="left" rowspan="1" colspan="1">0.68–0.78</td>
<td align="left" rowspan="1" colspan="1">0.75</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Discharge</bold></td>
<td align="left" rowspan="1" colspan="1">86%</td>
<td align="left" rowspan="1" colspan="1"><bold>0.70</bold></td>
<td align="left" rowspan="1" colspan="1">0.62–0.78</td>
<td align="left" rowspan="1" colspan="1">0.71</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>In Follow-up</bold></td>
<td align="left" rowspan="1" colspan="1">91%</td>
<td align="left" rowspan="1" colspan="1"><bold>0.74</bold></td>
<td align="left" rowspan="1" colspan="1">0.64–0.84</td>
<td align="left" rowspan="1" colspan="1">0.81</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Transfer destination</bold></td>
<td align="left" rowspan="1" colspan="1">89%</td>
<td align="left" rowspan="1" colspan="1"><bold>0.83</bold></td>
<td align="left" rowspan="1" colspan="1">0.71–0.88</td>
<td align="left" rowspan="1" colspan="1">n.a.<sup>d</sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Date of transfer</bold></td>
<td align="left" rowspan="1" colspan="1">100%</td>
<td align="left" rowspan="1" colspan="1"><bold>1.00</bold></td>
<td align="left" rowspan="1" colspan="1">-</td>
<td align="left" rowspan="1" colspan="1">n.a.<sup>d</sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Date of discharge</bold></td>
<td align="left" rowspan="1" colspan="1">100%</td>
<td align="left" rowspan="1" colspan="1"><bold>1.00</bold></td>
<td align="left" rowspan="1" colspan="1">-</td>
<td align="left" rowspan="1" colspan="1">n.a.<sup>d</sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>Date of next visit</bold></td>
<td align="left" rowspan="1" colspan="1">100%</td>
<td align="left" rowspan="1" colspan="1"><bold>1.00</bold></td>
<td align="left" rowspan="1" colspan="1">-</td>
<td align="left" rowspan="1" colspan="1">n.a.<sup>d</sup></td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t006fn001"><p>Abbreviations: CI, Confidence Interval; kappa, Cohen’s kappa; n.a., not applicable; PABAK, Prevalence and Bias Adjusted Kappa.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>Cohen’s kappas reached substantial or excellent agreement ranging from 0.70 (discharge) to 0.83 (transfer destination) (<xref rid="pone.0124290.t006" ref-type="table">Table 6</xref>; <xref rid="pone.0124290.g002" ref-type="fig">Fig 2</xref>). After adjusting for prevalence and bias, the true proportion of agreement (PABAK) was higher for all variables and ranged from 0.71 to 0.84 (<xref rid="pone.0124290.t006" ref-type="table">Table 6</xref>). Among the categorical variables, “in follow-up” had the highest Cohen’s kappa (k = 0.76), while transfer and discharge had the lowest (k = 0.62 and k = 0.63). Agreement and kappa were perfect (100%; k = 1) for the three date variables assessed.</p>
</sec>
</sec>
<sec id="sec021" sec-type="conclusions">
<title>Discussion</title>
<p>Results of our study showed that for both intra-rater and inter-rater we had substantial to excellent agreement. As expected, the variables for which no interpretation was necessary (e.g. date variables), higher and often perfect agreement was present. We found that one rater consistently had lower intra-rater agreement, but further analysis showed an improvement of judgment between point in time 1 and point in time 2 for both raters when compared to the chosen gold standard. Unexpectedly, Cohen’s kappas were higher for inter-rater reliability than for intra-rater reliability.</p>
<sec id="sec022">
<title>Strengths and Limitations</title>
<p>This is the first study assessing transition from pediatric to adult care with medical records which tested for intra-rater and inter-rater reliability of the collected data. Because the variables assessed were not always easy to find in the medical records nor easy to rate, these results give us the confidence needed to interpret the data collected. Because data collection was still ongoing, assessing reliability also gave us the opportunity to identify possible problems related to the rater’s comprehension and intervene in case we had the impression systematic errors were occurring. For data which we had already collected we performed a double control to make sure the possible mistakes of the first phase could be corrected.</p>
<p>The study has however limitations. Firstly, the sample size did not allow detection of inter-hospital differences or differences between different types of medical records (i.e. paper versos micro film) or archiving periods, which could possibly explain some of the rater variability. We also included three clinics only, while the whole study was carried out in a total of nine, in three different language regions. Caution is therefore needed in the generalization of results. In our study we only looked at documents from pediatric oncology and, even though they contain correspondence with the other specialists involved in follow-up, it was often difficult to fully understand the patients’ medical history. This was further aggravated by the fact, that none of the raters had expertise in clinical practice and was familiar with the local documentation systems. Finally, kappa has known limitations which we tried to overcome by reporting the prevalence-adjusted bias-adjusted kappa as proposed by several authors [<xref rid="pone.0124290.ref012" ref-type="bibr">12</xref>,<xref rid="pone.0124290.ref015" ref-type="bibr">15</xref>].</p>
</sec>
<sec id="sec023">
<title>Comparison with other studies</title>
<p>None of the studies that looked at assessing transition from pediatric to adult care did investigate reliability of the abstracted data. We found several studies assessing mostly inter-rater reliability of diagnostic tests (screening and detection of adverse events) which were not directly comparable to ours. Two studies [<xref rid="pone.0124290.ref004" ref-type="bibr">4</xref>, <xref rid="pone.0124290.ref008" ref-type="bibr">8</xref>] were more similar in methodology and scope to ours: in the first one the authors found that agreement was poorer for variables for which a degree of interpretability was needed (judgment data), while it was higher for data such as demographic or numeric characteristics [<xref rid="pone.0124290.ref008" ref-type="bibr">8</xref>]. The same was found in the second multicenter study using medical record abstraction in a study on community-based asthma care program [<xref rid="pone.0124290.ref004" ref-type="bibr">4</xref>]. In this study they found that the multicenter abstraction of data from medical records is reliable and conclusions could be drawn from the results. They found an overall Cohen’s kappa for intra-rater reliability of 0.81 (excellent) and an overall Cohen’s kappa of 0.75 (substantial) in the inter-rater analysis.</p>
</sec>
<sec id="sec024">
<title>Interpretation of results</title>
<p>Even though we could not reach perfection in the abstraction of the data, our results are reassuring and showed satisfactory levels of agreement. Further, the raters’ improvement in judgment between time 1 and time 2, probably due to a learning effect, allows to assume that the abstraction in the remaining 6 clinics not included in the present study, is of at least similar or higher quality and reliability. As it was expected, agreement was higher for non-judgment variables such as dates. Such information mostly does not require interpretation. Other data such as the variables “transfer” or “transfer destination” were to be looked for in free texts such as letters or medical reports and it often required a different degree of attention and interpretation. Indeed, data abstraction was difficult because in the pediatric oncology setting several other specialists are often involved in the follow-up of patients (e.g. neurologists, endocrinologists). Documents found were often from various specialists and the raters had to decide whether, for example, a patient was transferred from pediatric oncology or whether the patient was actually transferred from another specialist. A patient could namely still be in follow-up in pediatric oncology, but might have been transferred from pediatric endocrinology to adult endocrinology. This was often a source of confusion when abstracting data.</p>
<p>In contrast to other studies [<xref rid="pone.0124290.ref004" ref-type="bibr">4</xref>,<xref rid="pone.0124290.ref007" ref-type="bibr">7</xref>], we found higher inter-rater reliability than intra-rater. Because inter-rater reliability was assessed at point in time 2 only this higher reliability may be due to the learning effect we could show.</p>
</sec>
<sec id="sec025">
<title>Implications for practice</title>
<p>Despite the well-known limitations of retrospective studies using MR or other secondary data, an increasing number of studies have shown that such an approach can produce reliable results if the procedure is consistent and standardized, and if raters are appropriately trained. It would be interesting to investigate whether the archives’ organization, the documents’ age and the format of such documents (e.g. microfilm, electronic, hardcopy) influence the quality of the retrieved data. Finally, such an analysis could help detect possible problems such as rater’s comprehension difficulties or discrepancies and improve the overall quality of retrospective studies.</p>
</sec>
</sec>
<sec id="sec026" sec-type="conclusions">
<title>Conclusion</title>
<p>Our study showed that despite several limitations attributed to data abstracted from MR, our data seems to be reliable. Thanks to the assessment of learning effects, systematic errors could be corrected and general data quality improved. With good training and a standardized procedure good reliability can be achieved.</p>
</sec>
</body>
<back>
<ack>
<p>We thank the study team of the Transition after Childhood Cancer Study (Isabelle Brunner, Eliane Rupp and Samuel Wittwer) and the data managers of the Swiss Paediatric Oncology Group.</p>
<p><bold>Swiss Paediatric Oncology Group (SPOG)</bold> Scientific Committee: Prof. Dr. med. R. Ammann, Bern; Dr. med. R. Angst, Aarau; Prof. Dr. med. M. Ansari, Geneva; PD Dr. med. M. Beck Popovic, Lausanne; Dr. med. E. Bergstraesser, Zurich; Dr. med. P. Brazzola, Bellinzona; Dr. med. J. Greiner, St. Gallen; Prof. Dr. med. M. Grotzer, Zurich; Dr. med. H. Hengartner, St. Gallen; Prof. Dr. med. T. Kuehne, Basel; Prof. Dr. med. C. Kuehni, Bern; Prof. Dr. med. K. Leibundgut, Bern; Prof. Dr. med. F. Niggli, Zurich; PD Dr. med. J. Rischewski, Lucerne; Prof. Dr. med. N. von der Weid, Basel.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0124290.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eder</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Fullerton</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Benroth</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Lindsay</surname> <given-names>SP</given-names></name>. <article-title>Pragmatic strategies that enhance the reliability of data abstracted from medical records</article-title>. <source>Appl Nurs Res</source>. <year>2011</year>; <volume>18</volume>: <fpage>50</fpage>–<lpage>54</lpage>.</mixed-citation></ref>
<ref id="pone.0124290.ref002"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Allison J, Wall T, Spettell C, Calhoun J, Fargason C, Kobylinski RW, et al. The art and science of chart review. Jt Comm J Qual Improv. 2000; 115–136.</mixed-citation></ref>
<ref id="pone.0124290.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>vonKoss Krowchuk</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Moore</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Richardson</surname> <given-names>L</given-names></name>. <article-title>Using health care records as sources of data for research</article-title>. <source>J Nurs Meas</source>. <year>1995</year>; <volume>3</volume>: <fpage>3</fpage>–<lpage>12</lpage>. <object-id pub-id-type="pmid">7493186</object-id></mixed-citation></ref>
<ref id="pone.0124290.ref004"><label>4</label><mixed-citation publication-type="book" xlink:type="simple">To T, Estrabillo E, Wang C, Cicutto L. Examining intra-rater and inter-rater response agreement: A medical chart abstraction study of a community-based asthma care program. BMC Medical Research Methodology. 2008; 8–29.</mixed-citation></ref>
<ref id="pone.0124290.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gilbert</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Lowenstein</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>KoziolMcLain</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Barta</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Steiner</surname> <given-names>J</given-names></name>. <article-title>Chart reviews in emergency medicine research: Where are the methods?</article-title> <source>Annals of Emergency Medicine</source>. <year>1996</year>; <volume>27</volume>: <fpage>305</fpage>–<lpage>308</lpage>. <object-id pub-id-type="pmid">8599488</object-id></mixed-citation></ref>
<ref id="pone.0124290.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peabody</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Luck</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Glassman</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Dresselhaus</surname> <given-names>TR</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>M</given-names></name>. <article-title>Comparison of vignettes, standardized patients, and chart abstraction: a prospective validation study of 3 methods for measuring quality</article-title>. <source>Jama</source>. <year>2000</year>; <volume>283</volume>: <fpage>1715</fpage>–<lpage>1722</lpage>. <object-id pub-id-type="pmid">10755498</object-id></mixed-citation></ref>
<ref id="pone.0124290.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reeves</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Mullard</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wehner</surname> <given-names>S</given-names></name>. <article-title>Inter-rater reliability of data elements from a prototype of the Paul Coverdell National Acute Stroke Registry</article-title>. <source>BMC Neurology</source>. <year>2008</year>; <volume>8</volume>: <fpage>19</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/1471-2377-8-19" xlink:type="simple">10.1186/1471-2377-8-19</ext-link></comment> <object-id pub-id-type="pmid">18547421</object-id></mixed-citation></ref>
<ref id="pone.0124290.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yawn</surname> <given-names>BP</given-names></name>, <name name-style="western"><surname>Wollan</surname> <given-names>P</given-names></name>. <article-title>Interrater Reliability: Completing the Methods Description in Medical Records Review Studies</article-title>. <source>American Journal of Epidemiology</source>. <year>2005</year>; <volume>161</volume>: <fpage>974</fpage>–<lpage>977</lpage>. <object-id pub-id-type="pmid">15870162</object-id></mixed-citation></ref>
<ref id="pone.0124290.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Michel</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>von der Weid</surname> <given-names>NX</given-names></name>, <name name-style="western"><surname>Zwahlen</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Adam</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Rebholz</surname> <given-names>CE</given-names></name>, <name name-style="western"><surname>Kuehni</surname> <given-names>CE</given-names></name>. <article-title>The Swiss Childhood Cancer Registry: rationale, organisation and results for the years 2001–2005</article-title>. <source>Swiss Medical Weekly</source>. <year>2007</year>; <volume>137</volume>: <fpage>502</fpage>–<lpage>509</lpage>. <object-id pub-id-type="pmid">17990137</object-id></mixed-citation></ref>
<ref id="pone.0124290.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Michel</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>von der Weid</surname> <given-names>NX</given-names></name>, <name name-style="western"><surname>Zwahlen</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Redmond</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Strippoli</surname> <given-names>MPF</given-names></name>, <name name-style="western"><surname>Kuehni</surname> <given-names>CE</given-names></name>. <article-title>Incidence of childhood cancer in Switzerland: The Swiss childhood cancer registry</article-title>. <source>Pediatric Blood &amp; Cancer</source>. <year>2008</year>; <volume>50</volume>: <fpage>46</fpage>–<lpage>51</lpage>.</mixed-citation></ref>
<ref id="pone.0124290.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kottner</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Audige</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Brorson</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Donner</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Gajewski</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Hrobjartsson</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Guidelines for Reporting Reliability and Agreement Studies (GRRAS) were proposed</article-title>. <source>Int J Nurs Stud</source>. <year>2011</year>; <volume>48</volume>: <fpage>661</fpage>–<lpage>671</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.ijnurstu.2011.01.016" xlink:type="simple">10.1016/j.ijnurstu.2011.01.016</ext-link></comment> <object-id pub-id-type="pmid">21514934</object-id></mixed-citation></ref>
<ref id="pone.0124290.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sim</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Wright</surname> <given-names>CC</given-names></name>. <article-title>The Kappa Statistic in Reliability Studies: Use, Interpretation, and Sample Size Requirements</article-title>. <source>Physical Therapy</source>. <year>2005</year>; <volume>85</volume>: <fpage>257</fpage>–<lpage>268</lpage>. <object-id pub-id-type="pmid">15733050</object-id></mixed-citation></ref>
<ref id="pone.0124290.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>JL</surname> <given-names>Fleiss</given-names></name>, <name name-style="western"><surname>J. C</surname></name>, <name name-style="western"><surname>S. EB</surname></name>. <article-title>Large sample standard errors for kappaand weighted kappa</article-title>. <source>Psychological Bulletin</source>. <year>1969</year>; <volume>72</volume>: <fpage>323</fpage>–<lpage>327</lpage>.</mixed-citation></ref>
<ref id="pone.0124290.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Viera</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Garret</surname> <given-names>JM</given-names></name>. <article-title>Understanding interobserver agreement: the kappa statistics</article-title>. <source>Family medicine</source> <year>2005</year>; <volume>37</volume>: <fpage>360</fpage>–<lpage>363</lpage>. <object-id pub-id-type="pmid">15883903</object-id></mixed-citation></ref>
<ref id="pone.0124290.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Byrt</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Bishop</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Carlin</surname> <given-names>J</given-names></name>. <article-title>Bias, Prevalence and Kappa</article-title>. <source>Journal of clinical epidemiology</source>. <year>1993</year>; <volume>46</volume>: <fpage>423</fpage>–<lpage>429</lpage>. <object-id pub-id-type="pmid">8501467</object-id></mixed-citation></ref>
<ref id="pone.0124290.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Landis</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>GG</given-names></name>. <article-title>The measurement of observer agreement for categorical data</article-title>. <source>Biometrics</source>. <year>1977</year>; <volume>33</volume>: <fpage>159</fpage>–<lpage>174</lpage>. <object-id pub-id-type="pmid">843571</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>