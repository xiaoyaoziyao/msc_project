<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-00280</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003727</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subject>Neural networks</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Memory Capacity of Networks with Stochastic Binary Synapses</article-title>
<alt-title alt-title-type="running-head">Memory Capacity of Networks with Stochastic Binary Synapses</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Dubreuil</surname><given-names>Alexis M.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>Yali</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Brunel</surname><given-names>Nicolas</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>UMR 8118, CNRS, Universit√© Paris Descartes, Paris, France</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Departments of Statistics and Neurobiology, University of Chicago, Chicago, Illinois, United States of America</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Departments of Statistics and Computer Science, University of Chicago, Chicago, Illinois, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Hilgetag</surname><given-names>Claus C.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Hamburg University, Germany</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">nbrunel@galton.uchicago.edu</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Wrote the paper: AMD YA NB. Conceived and designed the model: AMD YA NB. Performed the calculations and numerical simulations: AMD YA NB.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>8</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>7</day><month>8</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>8</issue>
<elocation-id>e1003727</elocation-id>
<history>
<date date-type="received"><day>12</day><month>2</month><year>2014</year></date>
<date date-type="accepted"><day>30</day><month>5</month><year>2014</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Dubreuil et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>In standard attractor neural network models, specific patterns of activity are stored in the synaptic matrix, so that they become fixed point attractors of the network dynamics. The storage capacity of such networks has been quantified in two ways: the maximal number of patterns that can be stored, and the stored information measured in bits per synapse. In this paper, we compute both quantities in fully connected networks of N binary neurons with binary synapses, storing patterns with coding level <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e001" xlink:type="simple"/></inline-formula>, in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e002" xlink:type="simple"/></inline-formula> and sparse coding limits (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e003" xlink:type="simple"/></inline-formula>). We also derive finite-size corrections that accurately reproduce the results of simulations in networks of tens of thousands of neurons. These methods are applied to three different scenarios: (1) the classic Willshaw model, (2) networks with stochastic learning in which patterns are shown only once (one shot learning), (3) networks with stochastic learning in which patterns are shown multiple times. The storage capacities are optimized over network parameters, which allows us to compare the performance of the different models. We show that finite-size effects strongly reduce the capacity, even for networks of realistic sizes. We discuss the implications of these results for memory storage in the hippocampus and cerebral cortex.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>Two central hypotheses in neuroscience are that long-term memory is sustained by modifications of the connectivity of neural circuits, while short-term memory is sustained by persistent neuronal activity following the presentation of a stimulus. These two hypotheses have been substantiated by several decades of electrophysiological experiments, reporting activity-dependent changes in synaptic connectivity <italic>in vitro</italic>, and stimulus-selective persistent neuronal activity in delayed response tasks in behaving monkeys. They have been implemented in attractor network models, that store specific patterns of activity using Hebbian plasticity rules, which then allow retrieval of these patterns as attractors of the network dynamics. A long-standing question in the field is how many patterns (or equivalently, how much information) can be stored in such networks? Here, we compute the storage capacity of networks of binary neurons and binary synapses. Synapses store information according to a simple stochastic learning process that consists of transitions between synaptic states conditioned on the states of pre- and post-synaptic neurons. We consider this learning process in two limits: a one shot learning scenario, where each pattern is presented only once, and a slow learning scenario, where noisy versions of a set of patterns are presented multiple times, but transition probabilities are small. The two limits are assumed to represent, in a simplified way, learning in the hippocampus and neocortex, respectively. We show that in both cases, the information stored per synapse remains finite in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e004" xlink:type="simple"/></inline-formula> limit, when the coding is sparse. Furthermore, we characterize the strong finite size effects that exist in such networks.</p>
</abstract>
<funding-group><funding-statement>AMD is supported by a grant from the French Ministry of Higher Education and Research. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="15"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Attractor neural networks have been proposed as long-term memory storage devices <xref ref-type="bibr" rid="pcbi.1003727-Hopfield1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Amit1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Brunel1">[3]</xref>. In such networks, a pattern of activity (the set of firing rates of all neurons in the network) is said to be memorized if it is one of the stable states of the network dynamics. Specific patterns of activity become stable states thanks to synaptic plasticity mechanisms, including both long term potentiation and depression of synapses, that create positive feed-back loops through the network connectivity. Attractor states are consistent with the phenomenon of selective persistent activity during delay periods of delayed response tasks, which has been documented in numerous cortical areas in behaving monkeys <xref ref-type="bibr" rid="pcbi.1003727-Fuster1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Miyashita1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Fuster2">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-GoldmanRakic1">[7]</xref>. A long standing question in the field has been the question of the storage capacity of such networks. Much effort has been devoted to compute the number of attractor states that can be imprinted in the synaptic matrix, in networks of binary neurons <xref ref-type="bibr" rid="pcbi.1003727-Amit2">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Sompolinsky1">[9]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Gardner1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Tsodyks1">[11]</xref>. Models storing patterns with a covariance rule <xref ref-type="bibr" rid="pcbi.1003727-Sejnowski1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Hopfield1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Amit2">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Tsodyks1">[11]</xref> were shown to be able to store a number of patterns that scale linearly with the number of synapses per neuron. In the sparse coding limit (in which the average fraction of selective neurons per pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e005" xlink:type="simple"/></inline-formula> goes to zero in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e006" xlink:type="simple"/></inline-formula> limit), the capacity was shown to diverge as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e007" xlink:type="simple"/></inline-formula>. These scalings lead to a network storing on the order of 1 bit per synapse, in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e008" xlink:type="simple"/></inline-formula> limit, for any value of the coding level. Elizabeth Gardner <xref ref-type="bibr" rid="pcbi.1003727-Gardner1">[10]</xref> computed the maximal capacity, in the space of all possible coupling matrices, and demonstrated a similar scaling for capacity and information stored per synapse.</p>
<p>These initial studies, performed on the simplest possible networks (binary neurons, full connectivity, unrestricted synaptic weights) were followed by a second wave of studies that examined the effect of adding more neurobiological realism: random diluted connectivity <xref ref-type="bibr" rid="pcbi.1003727-Sompolinsky1">[9]</xref>, neurons characterized by analog firing rates <xref ref-type="bibr" rid="pcbi.1003727-Amit3">[13]</xref>, learning rules in which new patterns progressively erase the old ones <xref ref-type="bibr" rid="pcbi.1003727-Nadal1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Parisi1">[15]</xref>. The above mentioned modifications were shown not to affect the scaling laws described above. One particular modification however was shown to have a drastic effect on capacity. A network with binary synapses and stochastic on-line learning was shown to have a drastically impaired performance, compared to networks with continuous synapses <xref ref-type="bibr" rid="pcbi.1003727-Tsodyks2">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Amit4">[17]</xref>. For finite coding levels, the storage capacity was shown to be on the order of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e009" xlink:type="simple"/></inline-formula>, not <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e010" xlink:type="simple"/></inline-formula> stored patterns, while the information stored per synapse goes to zero in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e011" xlink:type="simple"/></inline-formula> limit. In the sparse coding limit however (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e012" xlink:type="simple"/></inline-formula>), the capacity was shown to scale as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e013" xlink:type="simple"/></inline-formula>, and therefore a similar scaling as the Gardner bound, while the information stored per synapse remains finite in this limit. These scaling laws are similar to the Willshaw model <xref ref-type="bibr" rid="pcbi.1003727-Willshaw1">[18]</xref>, which can be seen as a particular case of the Amit-Fusi <xref ref-type="bibr" rid="pcbi.1003727-Amit4">[17]</xref> rule. The model was then subsequently studied in greater detail by Huang and Amit <xref ref-type="bibr" rid="pcbi.1003727-Amit5">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Huang1">[20]</xref> who computed the storage capacity for finite values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e014" xlink:type="simple"/></inline-formula>, using numerical simulations and several approximations for the distributions of the ‚Äòlocal fields‚Äô of the neurons. However, computing the precise storage capacity of this model in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e015" xlink:type="simple"/></inline-formula> limit remains an open problem.</p>
<p>In this article we focus on a model of binary neurons where binary synapses are potentiated or depressed stochastically depending on the states of pre and post synaptic neurons <xref ref-type="bibr" rid="pcbi.1003727-Amit4">[17]</xref>. We first introduce analytical methods that allow us to compute the storage capacity in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e016" xlink:type="simple"/></inline-formula> limit, based on a binomial approximation for the synaptic inputs to the neurons. We first illustrate it on the Willshaw model and to recover the well-known result on the capacity of this model <xref ref-type="bibr" rid="pcbi.1003727-Willshaw1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Nadal2">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Knoblauch1">[22]</xref>. We then move to a stochastic learning rule, in which we study two different scenarios: (i) in which patterns are presented only once - we will refer to this model as the SP (Single Presentation) model <xref ref-type="bibr" rid="pcbi.1003727-Amit4">[17]</xref>; (ii) in which noisy versions of the patterns are presented multiple-times - the MP (Multiple presentations) model <xref ref-type="bibr" rid="pcbi.1003727-Brunel2">[23]</xref>. For both models we compute the storage capacity and the information stored per synapse in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e017" xlink:type="simple"/></inline-formula> limit, and investigate how they depend on the various parameters of the model. We then study finite size effects, and show that they have a huge effect even in networks of tens of thousands of neurons. Finally we show how capacity in finite size networks can be enhanced by introducing inhibition, as proposed in <xref ref-type="bibr" rid="pcbi.1003727-Amit5">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Huang1">[20]</xref>. In the discussion we summarize our results and discuss the relevance of the SP and MP networks to memory maintenance in the hippocampus and cortex.</p>
</sec><sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Storage capacity in the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e018" xlink:type="simple"/></inline-formula> limit</title>
<sec id="s2a1">
<title>The network</title>
<p>We consider a network of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e019" xlink:type="simple"/></inline-formula> binary <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e020" xlink:type="simple"/></inline-formula> neurons, fully connected through a binary <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e021" xlink:type="simple"/></inline-formula> synaptic connectivity matrix. The activity of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e022" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e023" xlink:type="simple"/></inline-formula>) is described by a binary variable, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e024" xlink:type="simple"/></inline-formula>. Each neuron can potentially be connected to every other neurons, through a binary connectivity matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e025" xlink:type="simple"/></inline-formula>. This connectivity matrix depends on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e026" xlink:type="simple"/></inline-formula> random uncorrelated patterns (‚Äòmemories‚Äô) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e027" xlink:type="simple"/></inline-formula> that are presented during the learning phase. The state of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e028" xlink:type="simple"/></inline-formula> in pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e029" xlink:type="simple"/></inline-formula> is<disp-formula id="pcbi.1003727.e030"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e030" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e031" xlink:type="simple"/></inline-formula> is the coding level of the memories. We study this model in the limit of low coding level, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e032" xlink:type="simple"/></inline-formula> when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e033" xlink:type="simple"/></inline-formula>. In all the models considered here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e034" xlink:type="simple"/></inline-formula> scales as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e035" xlink:type="simple"/></inline-formula> in the sparse coding limit. Thus, we introduce a parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e036" xlink:type="simple"/></inline-formula> which stays of order 1 in the sparse coding limit.</p>
<p>After the learning phase, we choose one of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e037" xlink:type="simple"/></inline-formula> presented patterns <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e038" xlink:type="simple"/></inline-formula>, and check whether it is a fixed point of the dynamics:<disp-formula id="pcbi.1003727.e039"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e039" xlink:type="simple"/><label>(2)</label></disp-formula>where<disp-formula id="pcbi.1003727.e040"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e040" xlink:type="simple"/><label>(3)</label></disp-formula>is the total synaptic input (‚Äúfield‚Äù) of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e041" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e042" xlink:type="simple"/></inline-formula> is a scaled activation threshold (constant independent of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e043" xlink:type="simple"/></inline-formula>), and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e044" xlink:type="simple"/></inline-formula> is the Heaviside function.</p>
</sec><sec id="s2a2">
<title>Field averages</title>
<p>When testing the stability of pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e045" xlink:type="simple"/></inline-formula> after learning <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e046" xlink:type="simple"/></inline-formula> patterns, we need to compute the distribution of the fields on selective neurons (sites <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e047" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e048" xlink:type="simple"/></inline-formula>), and of the fields on non-selective neurons (sites <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e049" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e050" xlink:type="simple"/></inline-formula>). The averages of those fields are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e051" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e052" xlink:type="simple"/></inline-formula> respectively, where<disp-formula id="pcbi.1003727.e053"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e053" xlink:type="simple"/><label>(4)</label></disp-formula>and<disp-formula id="pcbi.1003727.e054"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e054" xlink:type="simple"/><label>(5)</label></disp-formula></p>
<p>Pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e055" xlink:type="simple"/></inline-formula> is perfectly imprinted in the synaptic matrix if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e056" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e057" xlink:type="simple"/></inline-formula>. However, because of the storage of other patterns, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e058" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e059" xlink:type="simple"/></inline-formula> take intermediate values between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e060" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e061" xlink:type="simple"/></inline-formula>. Note that here we implicitly assume that the probability of finding a potentiated synapse between two neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e062" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e063" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e064" xlink:type="simple"/></inline-formula> is the same. This is true for the models we consider below. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e065" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e066" xlink:type="simple"/></inline-formula> are function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e067" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e068" xlink:type="simple"/></inline-formula>, and other parameters characterizing learning.</p>
</sec><sec id="s2a3">
<title>Information stored per synapse</title>
<p>One measure of the storage capability of the network is the information stored per synapse:<disp-formula id="pcbi.1003727.e069"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e069" xlink:type="simple"/><label>(6)</label></disp-formula><disp-formula id="pcbi.1003727.e070"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e070" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e071" xlink:type="simple"/></inline-formula> is the size of a set of patterns in which each pattern is a fixed point of the dynamics with probability one. When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e072" xlink:type="simple"/></inline-formula> is of order one, for the information per synapse to be of order one in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e073" xlink:type="simple"/></inline-formula> limit, we need to take <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e074" xlink:type="simple"/></inline-formula> as<disp-formula id="pcbi.1003727.e075"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e075" xlink:type="simple"/><label>(8)</label></disp-formula></p>
<p>In this case the information stored per synapse has the simple expression:<disp-formula id="pcbi.1003727.e076"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e076" xlink:type="simple"/><label>(9)</label></disp-formula></p>
</sec><sec id="s2a4">
<title>Computing the storage capacity</title>
<p>Our goal here is to compute the size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e077" xlink:type="simple"/></inline-formula> of the largest set of patterns that can be stored in the connectivity matrix. The criterion for storage that we adopt is that if one picks a pattern in this set, then this pattern is a fixed point of the dynamics with probability 1. We thus need to compute the probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e078" xlink:type="simple"/></inline-formula> of no error in retrieving a particular pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e079" xlink:type="simple"/></inline-formula>. To compute this probability, we first need to estimate the probabilities that a single selective/non-selective neuron is in its right state when the network is initialized in a state corresponding to pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e080" xlink:type="simple"/></inline-formula>. For a pattern with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e081" xlink:type="simple"/></inline-formula> selective neurons, and neglecting correlations between neurons (which is legitimate if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e082" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003727-Amit4">[17]</xref>), we have<disp-formula id="pcbi.1003727.e083"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e083" xlink:type="simple"/><label>(10)</label></disp-formula></p>
<p>Clearly, for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e084" xlink:type="simple"/></inline-formula> to go to 1 in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e085" xlink:type="simple"/></inline-formula> limit, the probabilities for the fields of single neurons to be on the wrong side of the threshold have to vanish in that limit. A first condition for this to happen is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e086" xlink:type="simple"/></inline-formula> - if these inequalities are satisfied, then the average fields of both selective and non-selective neurons are on the right side of the threshold. When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e087" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e088" xlink:type="simple"/></inline-formula> are sufficiently far from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e089" xlink:type="simple"/></inline-formula>, the tail probabilities of the distribution of the fields are<disp-formula id="pcbi.1003727.e090"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e090" xlink:type="simple"/><label>(11)</label></disp-formula><disp-formula id="pcbi.1003727.e091"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e091" xlink:type="simple"/><label>(12)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e092" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e093" xlink:type="simple"/></inline-formula> are the rate functions associated with the distributions of the fields (see Methods). Neglecting again correlations between inputs, the distributions of the fields are binomial distributions, and the rate functions are<disp-formula id="pcbi.1003727.e094"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e094" xlink:type="simple"/><label>(13)</label></disp-formula></p>
<p>Inserting Eqs. (11,12,13,8) in Eq. (10), we find that<disp-formula id="pcbi.1003727.e095"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e095" xlink:type="simple"/><label>(14)</label></disp-formula>where<disp-formula id="pcbi.1003727.e096"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e096" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003727.e097"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e097" xlink:type="simple"/><label>(15)</label></disp-formula></p>
<p>For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e098" xlink:type="simple"/></inline-formula> to go to 1 in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e099" xlink:type="simple"/></inline-formula> limit, we need both <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e100" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e101" xlink:type="simple"/></inline-formula> to go to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e102" xlink:type="simple"/></inline-formula> in that limit. This will be satisfied provided<disp-formula id="pcbi.1003727.e103"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e103" xlink:type="simple"/><label>(16)</label></disp-formula><disp-formula id="pcbi.1003727.e104"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e104" xlink:type="simple"/><label>(17)</label></disp-formula></p>
<p>These inequalities are equivalent in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e105" xlink:type="simple"/></inline-formula> limit to the inequalities<disp-formula id="pcbi.1003727.e106"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e106" xlink:type="simple"/><label>(18)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e107" xlink:type="simple"/></inline-formula> is given by the equation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e108" xlink:type="simple"/></inline-formula>.</p>
<p>The maximal information per synapse is obtained by saturating inequalities (16) and (17), and optimizing over the various parameters of the model. In practice, for given values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e109" xlink:type="simple"/></inline-formula>, and parameters of the learning process, we compute <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e110" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e111" xlink:type="simple"/></inline-formula>; we can then obtain the optimal values of the threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e112" xlink:type="simple"/></inline-formula> and the rescaled coding level <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e113" xlink:type="simple"/></inline-formula> as<disp-formula id="pcbi.1003727.e114"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e114" xlink:type="simple"/><label>(19)</label></disp-formula><disp-formula id="pcbi.1003727.e115"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e115" xlink:type="simple"/><label>(20)</label></disp-formula>and compute the information per synapse using Eq. (9). We can then find the optimum of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e116" xlink:type="simple"/></inline-formula> in the space of all parameters.</p>
<p>Before applying these methods to various models, we would like to emphasize two important features of these calculations:</p>
<list list-type="bullet"><list-item>
<p>In Eq. (16), note that the r.h.s. goes to zero extremely slowly as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e117" xlink:type="simple"/></inline-formula> goes to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e118" xlink:type="simple"/></inline-formula> (as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e119" xlink:type="simple"/></inline-formula>) - thus, we expect huge finite size effects. This will be confirmed in section ‚ÄòFinite-size networks‚Äô where these finite size effects are studied in detail.</p>
</list-item><list-item>
<p>In the sparse coding limit, a Gaussian approximation of the fields gives a poor approximation of the storage capacity, since the calculation probes the tail of the distribution.</p>
</list-item></list>
</sec></sec><sec id="s2b">
<title>Willshaw model</title>
<p>The capacity of the Willshaw model has already been studied by a number of authors <xref ref-type="bibr" rid="pcbi.1003727-Willshaw1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Nadal2">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Knoblauch1">[22]</xref>. Here, we present the application of the analysis described in the previous section to the Willshaw model, for completeness and comparison with the models described in the next sections. In this model, after presenting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e120" xlink:type="simple"/></inline-formula> patterns to the network, the synaptic matrix is described as follows: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e121" xlink:type="simple"/></inline-formula> if at least one of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e122" xlink:type="simple"/></inline-formula> presented patterns had neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e123" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e124" xlink:type="simple"/></inline-formula> co-activated, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e125" xlink:type="simple"/></inline-formula> otherwise. Thus, after the learning phase, we have,<disp-formula id="pcbi.1003727.e126"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e126" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003727.e127"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e127" xlink:type="simple"/><label>(21)</label></disp-formula></p>
<p>Saturating the inequalities (19,20) with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e128" xlink:type="simple"/></inline-formula> fixed, one obtains the information stored per synapse,<disp-formula id="pcbi.1003727.e129"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e129" xlink:type="simple"/><label>(22)</label></disp-formula></p>
<p>The information stored per synapse is shown as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e130" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pcbi-1003727-g001">Figure 1a</xref>. A maximum is reached for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e131" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e132" xlink:type="simple"/></inline-formula>, but goes to zero in both the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e133" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e134" xlink:type="simple"/></inline-formula> limits. The model has a storage capacity comparable to its maximal value, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e135" xlink:type="simple"/></inline-formula> in a large range of values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e136" xlink:type="simple"/></inline-formula> (between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e137" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e138" xlink:type="simple"/></inline-formula>). We can also optimize capacity for a given value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e139" xlink:type="simple"/></inline-formula>, as shown in <xref ref-type="fig" rid="pcbi-1003727-g001">Figure 1b</xref>. It reaches its maximum at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e140" xlink:type="simple"/></inline-formula>, and goes to zero in the small and large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e141" xlink:type="simple"/></inline-formula> limits. Again, the model has a large storage capacity for a broad range of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e142" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e143" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e144" xlink:type="simple"/></inline-formula> between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e145" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e146" xlink:type="simple"/></inline-formula>.</p>
<fig id="pcbi-1003727-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003727.g001</object-id><label>Figure 1</label><caption>
<title>Optimized information capacity of the Willshaw model in the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e147" xlink:type="simple"/></inline-formula>.</title>
<p>Information is optimized by saturating (19) (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e148" xlink:type="simple"/></inline-formula>) and (20): a. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e149" xlink:type="simple"/></inline-formula> as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e150" xlink:type="simple"/></inline-formula>, b. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e151" xlink:type="simple"/></inline-formula> as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e152" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003727.g001" position="float" xlink:type="simple"/></fig>
<p>Previous studies <xref ref-type="bibr" rid="pcbi.1003727-Willshaw1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Nadal2">[21]</xref> have found an optimal capacity of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e153" xlink:type="simple"/></inline-formula>. Those studies focused on a feed-forward network with a single output neuron, with no fluctuations in the number of selective neurons per pattern, and required that the number of errors on silent outputs is of the same order as the number of selective outputs in the whole set of patterns. In the calculations presented here, we have used a different criteria, namely that a given pattern (not all patterns) is exactly a fixed point of the dynamics of the network with a probability that goes to one in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e154" xlink:type="simple"/></inline-formula> limit. Another possible definition would be to require that <bold>all</bold> the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e155" xlink:type="simple"/></inline-formula> patterns are exact fixed points with probability one. In this case, for patterns with fixed numbers of selective neurons, the capacity drops by a factor of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e156" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e157" xlink:type="simple"/></inline-formula>, as already computed by Knoblauch et al <xref ref-type="bibr" rid="pcbi.1003727-Knoblauch1">[22]</xref>.</p>
</sec><sec id="s2c">
<title>Amit-Fusi model</title>
<p>A drawback of the Willshaw learning rule is that it only allows for synaptic potentiation. Thus, if patterns are continuously presented to the network, all synapses will eventually be potentiated and no memories can be retrieved. In <xref ref-type="bibr" rid="pcbi.1003727-Amit4">[17]</xref> Amit and Fusi introduced a new learning rule that maintains the simplicity of the Willshaw model, but allows for continuous on-line learning. The proposed learning rule includes synaptic depression. At each learning time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e158" xlink:type="simple"/></inline-formula>, a new pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e159" xlink:type="simple"/></inline-formula> with coding level <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e160" xlink:type="simple"/></inline-formula> is presented to the network, and synapses are updated stochastically:<list list-type="bullet"><list-item>
<p>for synapses such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e161" xlink:type="simple"/></inline-formula>:</p>
</list-item></list>if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e162" xlink:type="simple"/></inline-formula>, then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e163" xlink:type="simple"/></inline-formula> is potentiated to 1 with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e164" xlink:type="simple"/></inline-formula>; and if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e165" xlink:type="simple"/></inline-formula> it stays at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e166" xlink:type="simple"/></inline-formula>.<list list-type="bullet"><list-item>
<p>for synapses such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e167" xlink:type="simple"/></inline-formula>:</p>
</list-item></list>if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e168" xlink:type="simple"/></inline-formula>, then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e169" xlink:type="simple"/></inline-formula> stays at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e170" xlink:type="simple"/></inline-formula>; and if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e171" xlink:type="simple"/></inline-formula> it is depressed to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e172" xlink:type="simple"/></inline-formula> with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e173" xlink:type="simple"/></inline-formula>.</p>
<list list-type="bullet"><list-item>
<p>for synapses such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e174" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e175" xlink:type="simple"/></inline-formula>.</p>
</list-item></list>
<p>The evolution of a synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e176" xlink:type="simple"/></inline-formula> during learning can be described by the following Markov process:<disp-formula id="pcbi.1003727.e177"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e177" xlink:type="simple"/><label>(23)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e178" xlink:type="simple"/></inline-formula> is the probability that a silent synapse is potentiated upon the presentation of pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e179" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e180" xlink:type="simple"/></inline-formula> is the probability that a potentiated synapse is depressed. After a sufficient number of patterns has been presented the distribution of synaptic weights in the network reaches a stationary state. We study the network in this stationary regime.</p>
<p>For the information capacity to be of order 1, the coding level has to scale as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e181" xlink:type="simple"/></inline-formula>, as in the Willshaw model, and the effects of potentiation and depression have to be of the same order <xref ref-type="bibr" rid="pcbi.1003727-Amit4">[17]</xref>. Thus we define the <italic>depression-potentiation ratio</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e182" xlink:type="simple"/></inline-formula> as,<disp-formula id="pcbi.1003727.e183"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e183" xlink:type="simple"/><label>(24)</label></disp-formula></p>
<p>We can again use Eq. (9) and the saturated inequalities (19,20) to compute the maximal information capacity in the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e184" xlink:type="simple"/></inline-formula>. This requires computing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e185" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e186" xlink:type="simple"/></inline-formula>, defined in the previous section, as a function of the different parameters characterizing the network. We track a pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e187" xlink:type="simple"/></inline-formula> that has been presented <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e188" xlink:type="simple"/></inline-formula> time steps in the past. In the following we refer to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e189" xlink:type="simple"/></inline-formula> as the age of the pattern. In the sparse coding limit, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e190" xlink:type="simple"/></inline-formula> corresponds to the probability that a synapse is potentiated. It is determined by the depression-potentiation ratio <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e191" xlink:type="simple"/></inline-formula>,<disp-formula id="pcbi.1003727.e192"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e192" xlink:type="simple"/><label>(25)</label></disp-formula>and<disp-formula id="pcbi.1003727.e193"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e193" xlink:type="simple"/><label>(26)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e194" xlink:type="simple"/></inline-formula>. Our goal is to determine the age <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e195" xlink:type="simple"/></inline-formula> of the oldest pattern that is still a fixed point of the network dynamics, with probability one. Note that in this network, contrary to the Willshaw model in which all patterns are equivalent, here younger patterns, of age <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e196" xlink:type="simple"/></inline-formula>, are more strongly imprinted in the synaptic matrix, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e197" xlink:type="simple"/></inline-formula>, and thus also stored with probability one.</p>
<p>Choosing an activation threshold and a coding level that saturate inequalities (19) and (20), information capacity can be expressed as:<disp-formula id="pcbi.1003727.e198"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e198" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003727.e199"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e199" xlink:type="simple"/><label>(27)</label></disp-formula></p>
<p>The optimal information <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e200" xlink:type="simple"/></inline-formula> is reached for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e201" xlink:type="simple"/></inline-formula> which gives <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e202" xlink:type="simple"/></inline-formula>.</p>
<p>The dependence of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e203" xlink:type="simple"/></inline-formula> on the different parameters is shown in <xref ref-type="fig" rid="pcbi-1003727-g002">Figure 2</xref>. Panel <italic>a</italic> shows the dependence on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e204" xlink:type="simple"/></inline-formula> the fraction of activated synapses in the asymptotic learning regime. Panels <italic>b</italic>, <italic>c</italic> and <italic>d</italic> show the dependence on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e205" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e206" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e207" xlink:type="simple"/></inline-formula>. Note from panel <italic>c</italic> that there is a broad range of values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e208" xlink:type="simple"/></inline-formula> that give information capacities similar to the optimal one. One can also observe that the optimal information capacity is about <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e209" xlink:type="simple"/></inline-formula> times lower in the SP model than in the Willshaw model. This is the price one pays to have a network that is able to continuously learn new patterns. However, it should be noted that at maximal capacity, in the Willshaw model, every pattern has a vanishing basin of attraction while in the SP model, only the oldest stable patterns have vanishing basins of attraction. This feature is not captured by our measure of storage capacity.</p>
<fig id="pcbi-1003727-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003727.g002</object-id><label>Figure 2</label><caption>
<title>Optimized information capacity for the SP model in the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e210" xlink:type="simple"/></inline-formula>.</title>
<p>a. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e211" xlink:type="simple"/></inline-formula> as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e212" xlink:type="simple"/></inline-formula>, b. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e213" xlink:type="simple"/></inline-formula> as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e214" xlink:type="simple"/></inline-formula>, the ratio between the number of depressing events and potentiating events at pattern presentation, c. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e215" xlink:type="simple"/></inline-formula> as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e216" xlink:type="simple"/></inline-formula>, d. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e217" xlink:type="simple"/></inline-formula> as a function of the LTP transition probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e218" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003727.g002" position="float" xlink:type="simple"/></fig></sec><sec id="s2d">
<title>Multiple presentations of patterns, slow learning regime</title>
<p>In the SP model, patterns are presented only once. Brunel et al <xref ref-type="bibr" rid="pcbi.1003727-Brunel2">[23]</xref> studied the same network of binary neurons with stochastic binary synapses but in a different learning context, where patterns are presented multiple times. More precisely, at each learning time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e219" xlink:type="simple"/></inline-formula>, a noisy version <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e220" xlink:type="simple"/></inline-formula> of one of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e221" xlink:type="simple"/></inline-formula> prototypes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e222" xlink:type="simple"/></inline-formula> is presented to the network,<disp-formula id="pcbi.1003727.e223"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e223" xlink:type="simple"/><label>(28)</label></disp-formula></p>
<p>Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e224" xlink:type="simple"/></inline-formula> is a noise level: if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e225" xlink:type="simple"/></inline-formula>, presented patterns are identical to the prototypes, while if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e226" xlink:type="simple"/></inline-formula>, the presented patterns are uncorrelated with the prototypes. As for the SP model this model achieves a finite non-zero information capacity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e227" xlink:type="simple"/></inline-formula> in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e228" xlink:type="simple"/></inline-formula> limit if the depression-potentiation ratio <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e229" xlink:type="simple"/></inline-formula> is of order one, and if the coding level scales with network size as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e230" xlink:type="simple"/></inline-formula>. If learning is slow, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e231" xlink:type="simple"/></inline-formula>, and the number of presentations of patterns of each class becomes large the probabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e232" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e233" xlink:type="simple"/></inline-formula> are <xref ref-type="bibr" rid="pcbi.1003727-Brunel2">[23]</xref>:<disp-formula id="pcbi.1003727.e234"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e234" xlink:type="simple"/><label>(29)</label></disp-formula>and<disp-formula id="pcbi.1003727.e235"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e235" xlink:type="simple"/><label>(30)</label></disp-formula></p>
<p>We inserted those expressions in Eqs. (19,20) to study the maximal information capacity of the network under this learning protocol. The optimal information <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e236" xlink:type="simple"/></inline-formula> bits/synapse is reached at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e237" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e238" xlink:type="simple"/></inline-formula> which gives <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e239" xlink:type="simple"/></inline-formula>. In this limit, the network becomes equivalent to the Willshaw model.</p>
<p>The maximal capacity is about <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e240" xlink:type="simple"/></inline-formula> times larger than for a network that has to learn in one shot. On <xref ref-type="fig" rid="pcbi-1003727-g003">Figure 3a</xref> we plot the optimal capacity as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e241" xlink:type="simple"/></inline-formula>. The capacity of the slow learning network with multiple presentations is bounded by the capacity of the Willshaw model for all values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e242" xlink:type="simple"/></inline-formula>, and it is reached when the depression-potentiation ratio <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e243" xlink:type="simple"/></inline-formula>. For this value, no depression occurs during learning: the network loses palimpsest properties, i.e. the ability to erase older patterns to store new ones, and it is not able to learn if the presented patterns are noisy. The optimal capacity decreases with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e244" xlink:type="simple"/></inline-formula>, for instance at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e245" xlink:type="simple"/></inline-formula> (as many potentiation events as depression events at each pattern presentation), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e246" xlink:type="simple"/></inline-formula>. <xref ref-type="fig" rid="pcbi-1003727-g003">Figure 3c</xref> shows the dependence as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e247" xlink:type="simple"/></inline-formula>. In <xref ref-type="fig" rid="pcbi-1003727-g003">Figure 3d</xref>, we show the optimized capacity for different values of the noise <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e248" xlink:type="simple"/></inline-formula> in the presented patterns. This quantifies the trade-off between the storage capacity and the generalization ability of the network <xref ref-type="bibr" rid="pcbi.1003727-Brunel2">[23]</xref>.</p>
<fig id="pcbi-1003727-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003727.g003</object-id><label>Figure 3</label><caption>
<title>Optimized information capacity for the MP model in the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e249" xlink:type="simple"/></inline-formula>.</title>
<p>a. Optimal information capacity as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e250" xlink:type="simple"/></inline-formula>, the average number of activated synapses after learning. Optimal capacity is reached in the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e251" xlink:type="simple"/></inline-formula> and at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e252" xlink:type="simple"/></inline-formula> where the capacity is the same as for the Willshaw model. b. Dependence of information capacity on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e253" xlink:type="simple"/></inline-formula>, the ratio between the number of depressing events and potentiating events at pattern presentation. c. Dependence on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e254" xlink:type="simple"/></inline-formula>. d. Dependence on the noise in the presented patterns, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e255" xlink:type="simple"/></inline-formula>. This illustrates the trade-off between the storage capacity and the generalization ability of the network.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003727.g003" position="float" xlink:type="simple"/></fig></sec><sec id="s2e">
<title>Finite-size networks</title>
<p>The results we have presented so far are valid for infinite size networks. Finite-size effects can be computed for the three models we have discussed so far (see Methods). The main result of this section is that the capacity of networks of realistic sizes is very far from the large N limit. We compute capacities for finite networks in the SP and MP settings, and we validate our finite size calculations by presenting the results of simulations of large networks of sizes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e256" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e257" xlink:type="simple"/></inline-formula>.</p>
<p>We summarize the finite size calculations for the SP model (a more general and detailed analysis is given in Methods). In the finite network setting, conditional on the tested pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e258" xlink:type="simple"/></inline-formula> having <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e259" xlink:type="simple"/></inline-formula> selective neurons, the probability of no error <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e260" xlink:type="simple"/></inline-formula> is given by<disp-formula id="pcbi.1003727.e261"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e261" xlink:type="simple"/></disp-formula>with</p>
<p><disp-formula id="pcbi.1003727.e262"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e262" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003727.e263"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e263" xlink:type="simple"/><label>(31)</label></disp-formula></p>
<p>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e264" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e265" xlink:type="simple"/></inline-formula> is given by Eq. (13). In the calculations for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e266" xlink:type="simple"/></inline-formula> discussed in the previous sections we kept only the dominant term in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e267" xlink:type="simple"/></inline-formula>, which yields Eqs. (19) and (20).</p>
<p>In the above equations, the first order corrections scale as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e268" xlink:type="simple"/></inline-formula>, which has a dramatic effect on the storage capacity of finite networks. In <xref ref-type="fig" rid="pcbi-1003727-g004">Figure 4a,b</xref>, we plot <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e269" xlink:type="simple"/></inline-formula> (where the bar denotes an average over the distribution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e270" xlink:type="simple"/></inline-formula>) as a function of the age of the pattern, and compare this with numerical simulations. It is plotted for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e271" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e272" xlink:type="simple"/></inline-formula> for learning and network parameters chosen to optimize the storage capacity of the infinite-size network (see Section ‚ÄòAmit-Fusi model‚Äô). We show the result for two different approximations of the field distribution: a binomial distribution (magenta), as used in the previous calculations for infinite size networks; and a gaussian (red) approximation (see Methods for calculations) as used by previous authors <xref ref-type="bibr" rid="pcbi.1003727-Amit5">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Huang1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Leibold1">[24]</xref>. For these parameters the binomial approximation gives an accurate estimation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e273" xlink:type="simple"/></inline-formula>, while the gaussian calculation overestimates it.</p>
<fig id="pcbi-1003727-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003727.g004</object-id><label>Figure 4</label><caption>
<title>Finite size effects. Shown is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e274" xlink:type="simple"/></inline-formula>, the probability that a tested pattern of a given age is stored without errors, for the SP model.</title>
<p>a. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e275" xlink:type="simple"/></inline-formula> as a function of the age of the tested pattern. Parameters are those optimizing capacity at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e276" xlink:type="simple"/></inline-formula>, results are for simulations (blue line) and calculations with a binomial approximation of the fields distributions (magenta) and a gaussian approximation (red); <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e277" xlink:type="simple"/></inline-formula> is averaged over different value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e278" xlink:type="simple"/></inline-formula>, the number of selective neurons in the tested pattern (magenta line). b Same for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e279" xlink:type="simple"/></inline-formula>. c. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e280" xlink:type="simple"/></inline-formula> as a function of a scaled version of pattern age (see text for details), fluctuations in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e281" xlink:type="simple"/></inline-formula> are discarded on this plot. d. Same as c with an average of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e282" xlink:type="simple"/></inline-formula> over different <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e283" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003727.g004" position="float" xlink:type="simple"/></fig>
<p>The curves we get are far from the step functions predicted for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e284" xlink:type="simple"/></inline-formula> by Eq. (45). To understand why, compare Eqs. (15), and (31): finite size effects can be neglected when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e285" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e286" xlink:type="simple"/></inline-formula>. Because the finite size effects are of order <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e287" xlink:type="simple"/></inline-formula>, it is only for huge values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e288" xlink:type="simple"/></inline-formula> that the asymptotic capacity can be recovered. For instance if we choose an activation threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e289" xlink:type="simple"/></inline-formula> slightly above the optimal threshold given in Section ‚ÄòAmit-Fusi model‚Äô (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e290" xlink:type="simple"/></inline-formula>), then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e291" xlink:type="simple"/></inline-formula>, and for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e292" xlink:type="simple"/></inline-formula> we only have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e293" xlink:type="simple"/></inline-formula>. In <xref ref-type="fig" rid="pcbi-1003727-g004">Figure 4c</xref> we plot <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e294" xlink:type="simple"/></inline-formula> as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e295" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e296" xlink:type="simple"/></inline-formula> is the value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e297" xlink:type="simple"/></inline-formula> that optimizes capacity in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e298" xlink:type="simple"/></inline-formula> limit, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e299" xlink:type="simple"/></inline-formula> and the other parameters are the one that optimizes capacity. We see that we are still far from the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e300" xlink:type="simple"/></inline-formula> limit for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e301" xlink:type="simple"/></inline-formula>. Networks of sizes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e302" xlink:type="simple"/></inline-formula> have capacities which are only between 20% and 40% of the predicted capacity in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e303" xlink:type="simple"/></inline-formula> limit. Neglecting fluctuations in the number of selective neurons, we can derive an expression for the number of stored patterns <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e304" xlink:type="simple"/></inline-formula> that includes the leading finite size correction for the SP model,<disp-formula id="pcbi.1003727.e305"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e305" xlink:type="simple"/><label>(32)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e306" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e307" xlink:type="simple"/></inline-formula> are two constants (see Methods).</p>
<p>If we take fluctuations in the number of selective neurons into account, it introduces other finite-size effects as can be seen from Eqs. (43) and (44) in the Methods section. These fluctuations can be discarded if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e308" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e309" xlink:type="simple"/></inline-formula>. In <xref ref-type="fig" rid="pcbi-1003727-g004">Figure 4d</xref> we plot <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e310" xlink:type="simple"/></inline-formula> for different values of N. We see that finite size effects are even stronger in this case.</p>
<p>To plot the curves of <xref ref-type="fig" rid="pcbi-1003727-g004">Figure 4</xref>, we chose parameters to be those that optimize storage capacity for infinite network sizes. When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e311" xlink:type="simple"/></inline-formula> is finite, those parameters are no longer optimal. To optimize parameters at finite <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e312" xlink:type="simple"/></inline-formula>, since the probability of error as a function of age is no longer a step function, it is not possible to find the last pattern stored with probability one. Instead we define the capacity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e313" xlink:type="simple"/></inline-formula> as the pattern age for which <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e314" xlink:type="simple"/></inline-formula>. Using Eqs. (31) and performing an average over the distribution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e315" xlink:type="simple"/></inline-formula>, we find parameters optimizing pattern capacity for fixed values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e316" xlink:type="simple"/></inline-formula>. Results are shown on <xref ref-type="fig" rid="pcbi-1003727-g005">Figure 5a,b</xref> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e317" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e318" xlink:type="simple"/></inline-formula>. We show the results for the different approximations used to model the neural fields: the blue line is the binomial approximation, the cyan line the gaussian approximation and the magenta one is a gaussian approximation with a covariance term that takes into account correlations between synapses (see Methods and <xref ref-type="bibr" rid="pcbi.1003727-Amit5">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Huang1">[20]</xref>). For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e319" xlink:type="simple"/></inline-formula> the storage capacity of simulated networks (black crosses) is well predicted by the binomial approximation while the gaussian approximations over-estimates capacity. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e320" xlink:type="simple"/></inline-formula>, the correlations between synapses can no longer be neglected <xref ref-type="bibr" rid="pcbi.1003727-Amit4">[17]</xref>. The gaussian approximation with covariance captures the drop in capacity at large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e321" xlink:type="simple"/></inline-formula>.</p>
<fig id="pcbi-1003727-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003727.g005</object-id><label>Figure 5</label><caption>
<title>Capacity at finite <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e322" xlink:type="simple"/></inline-formula>.</title>
<p>a,b. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e323" xlink:type="simple"/></inline-formula> as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e324" xlink:type="simple"/></inline-formula> for the SP model and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e325" xlink:type="simple"/></inline-formula> Parameters are chosen to optimize capacity under the binomial approximation. Shown are the result of the gaussian approximation without covariance (cyan) and with covariance (magenta) for these parameters. c. Optimized <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e326" xlink:type="simple"/></inline-formula> as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e327" xlink:type="simple"/></inline-formula> for the SP model at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e328" xlink:type="simple"/></inline-formula>. The blue curve is for patterns with fluctuations in the number of selective neurons. The red curve is for the same number of selective neurons in all patterns. The black curve is the number of patterns that would be stored if the network were storing the same amount of information as in the case <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e329" xlink:type="simple"/></inline-formula>. d. Same for the MP model, where parameters have been optimized, but the depression-potentiation ratio is fixed at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e330" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003727.g005" position="float" xlink:type="simple"/></fig>
<p>For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e331" xlink:type="simple"/></inline-formula>, the SP model can store a maximum of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e332" xlink:type="simple"/></inline-formula> patterns at a coding level <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e333" xlink:type="simple"/></inline-formula> (see blue curve in <xref ref-type="fig" rid="pcbi-1003727-g005">figure 5c</xref>). As suggested in <xref ref-type="fig" rid="pcbi-1003727-g004">Figures 4c,d</xref>, the capacity of finite networks is strongly reduced compare to the capacity predicted for infinite size networks. More precisely, if the network of size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e334" xlink:type="simple"/></inline-formula> had the same information capacity as the infinite size network (27), it would store up to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e335" xlink:type="simple"/></inline-formula> patterns at coding level <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e336" xlink:type="simple"/></inline-formula>. Part of this decrease in capacity is avoided if we consider patterns that have a fixed number <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e337" xlink:type="simple"/></inline-formula> of selective neurons. This corresponds to the red curve in <xref ref-type="fig" rid="pcbi-1003727-g004">figure 4c</xref>. For fixed sizes the capacity is approximately twice as large. Note that finite-size effects tend to decrease as the coding level increases. In <xref ref-type="fig" rid="pcbi-1003727-g005">Figure 5c</xref>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e338" xlink:type="simple"/></inline-formula>, and the capacity is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e339" xlink:type="simple"/></inline-formula> of the value predicted by the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e340" xlink:type="simple"/></inline-formula> limit calculation. The ratio of actual to asymptotic capacities increases to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e341" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e342" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e343" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e344" xlink:type="simple"/></inline-formula>. In <xref ref-type="fig" rid="pcbi-1003727-g005">Figure 5d</xref>, we do the same analysis for the MP model with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e345" xlink:type="simple"/></inline-formula>. Here we have also optimized all the parameters, except for the depression-potentiation ratio which is set to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e346" xlink:type="simple"/></inline-formula>, ensuring that the network has the palimpsest property and the ability to deal with noisy patterns. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e347" xlink:type="simple"/></inline-formula>, the MP model with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e348" xlink:type="simple"/></inline-formula> can store up to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e349" xlink:type="simple"/></inline-formula> patterns, at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e350" xlink:type="simple"/></inline-formula> (versus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e351" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e352" xlink:type="simple"/></inline-formula> for the SP model). One can also compute the optimized capacity for a given noise level. At <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e353" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e354" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e355" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e356" xlink:type="simple"/></inline-formula> or at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e357" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e358" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e359" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e360" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s2f">
<title>Storage capacity with errors</title>
<p>So far, we have defined the storage capacity as the number of patterns that can be perfectly retrieved. However, it is quite common for attractor neural networks to have stable fixed point attractors that are close to, but not exactly equal to, patterns that are stored in the connectivity matrix. It is difficult to estimate analytically the stability of patterns that are retrieved with errors as it requires analysis of the dynamics at multiple time steps. We therefore used numerical simulations to check whether a tested pattern is retrieved as a fixed point of the dynamics at a sufficiently low error level. To quantify the degree of error, we introduce the overlap <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e361" xlink:type="simple"/></inline-formula> between the network fixed point <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e362" xlink:type="simple"/></inline-formula> and the tested pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e363" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e364" xlink:type="simple"/></inline-formula> selective neurons<disp-formula id="pcbi.1003727.e365"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e365" xlink:type="simple"/><label>(33)</label></disp-formula></p>
<p>In <xref ref-type="fig" rid="pcbi-1003727-g006">Figure 6a</xref> we show <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e366" xlink:type="simple"/></inline-formula>, the number of fixed-point attractors that have an overlap larger than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e367" xlink:type="simple"/></inline-formula> with the corresponding stored pattern, for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e368" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e369" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e370" xlink:type="simple"/></inline-formula>. Note that only a negligible number of tested patterns lead to fixed points with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e371" xlink:type="simple"/></inline-formula> smaller than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e372" xlink:type="simple"/></inline-formula>, for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e373" xlink:type="simple"/></inline-formula> neurons. Considering fixed points with errors leads to a substantial increase in capacity, e.g. for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e374" xlink:type="simple"/></inline-formula> the capacity increases from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e375" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e376" xlink:type="simple"/></inline-formula>. In <xref ref-type="fig" rid="pcbi-1003727-g006">Figure 6b</xref>, we quantify the information capacity in bits stored per synapse, defined as in Eq. (6), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e377" xlink:type="simple"/></inline-formula>. Note that in the situation when retrieval is not always perfect this expression is only an approximation of the true information content. The coding level that optimizes the information capacity in bits per synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e378" xlink:type="simple"/></inline-formula> is larger (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e379" xlink:type="simple"/></inline-formula>) than the one that optimizes the number of stored patterns <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e380" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e381" xlink:type="simple"/></inline-formula>), since the information content of individual patterns decreases with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e382" xlink:type="simple"/></inline-formula>. Finally, note that the information capacity is close to its optimum in a broad range of coding levels, up to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e383" xlink:type="simple"/></inline-formula>.</p>
<fig id="pcbi-1003727-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003727.g006</object-id><label>Figure 6</label><caption>
<title>Storage capacity with errors in the SP model.</title>
<p>Instead of counting only patterns that are perfectly retrieved, patterns that lead to fixed points of the dynamic overlapping significantly (see text for the definition of the overlap) with the tested memory pattern are also counted. Simulations are done with the same parameters as in <xref ref-type="fig" rid="pcbi-1003727-g005">Figure 5a</xref>. a. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e384" xlink:type="simple"/></inline-formula> as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e385" xlink:type="simple"/></inline-formula>. Blue crosses correspond to fixed points that are exactly the stored patterns. Red triangles correspond to fixed points that have an overlap larger than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e386" xlink:type="simple"/></inline-formula>, and brown circles an overlap larger than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e387" xlink:type="simple"/></inline-formula>. b. Same as a. but instead of quantifying storage capacity with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e388" xlink:type="simple"/></inline-formula>, it is done with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e389" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003727.g006" position="float" xlink:type="simple"/></fig></sec><sec id="s2g">
<title>Increase in capacity with inhibition</title>
<p>As we have seen above, the fluctuations in the number of selective neurons in each pattern lead to a reduction in storage capacity in networks of finite size (e.g. <xref ref-type="fig" rid="pcbi-1003727-g005">Figure 5c,d</xref>). The detrimental effects of these fluctuations can be mitigated by adding a uniform inhibition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e390" xlink:type="simple"/></inline-formula> to the network <xref ref-type="bibr" rid="pcbi.1003727-Amit5">[19]</xref>. Using a simple instantaneous and linear inhibitory feed-back, the local fields become<disp-formula id="pcbi.1003727.e391"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e391" xlink:type="simple"/><label>(34)</label></disp-formula></p>
<p>For infinite size networks, adding inhibition does not improve storage capacity since fluctuations in the number of selective neurons vanish in the large N limit. However, for finite size networks, minimizing those fluctuations leads to substantial increase in storage capacity. When testing the stability of pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e392" xlink:type="simple"/></inline-formula>, if the number of selective neurons is unknown, the variance of the field on non-selective neurons is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e393" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e394" xlink:type="simple"/></inline-formula> for selective neurons (for small <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e395" xlink:type="simple"/></inline-formula>). The variance for non-selective neurons is minimized if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e396" xlink:type="simple"/></inline-formula>, yielding the variance obtained with fixed size patterns. The same holds for selective neurons at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e397" xlink:type="simple"/></inline-formula>. Choosing a value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e398" xlink:type="simple"/></inline-formula> between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e399" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e400" xlink:type="simple"/></inline-formula> brings the network capacity towards that of fixed size patterns. In <xref ref-type="fig" rid="pcbi-1003727-g007">Figure 7a</xref>, we show the storage capacity as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e401" xlink:type="simple"/></inline-formula> for these three scenarios. Optimizing the inhibition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e402" xlink:type="simple"/></inline-formula> increases the maximal capacity by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e403" xlink:type="simple"/></inline-formula> (green curve) compared to a network with no inhibition (blue curve). Red curve is the capacity without pattern size fluctuations. Inhibition increases the capacity from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e404" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e405" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e406" xlink:type="simple"/></inline-formula>. In <xref ref-type="fig" rid="pcbi-1003727-g007">Figure 7b</xref>, information capacity measured in bits per synapse is shown as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e407" xlink:type="simple"/></inline-formula> in the same three scenarios. Note again that for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e408" xlink:type="simple"/></inline-formula>, the capacity is quite close to the optimal capacity.</p>
<fig id="pcbi-1003727-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003727.g007</object-id><label>Figure 7</label><caption>
<title>Storage capacity optimized with inhibition in the SP model.</title>
<p>Blue is for a fixed threshold and fluctuations in the number of selective neurons per pattern. Green, the fluctuations are minimized using inhibition. Red, without fluctuations in the number of selective neurons per pattern. a. Number of stored patterns as a function of the coding level <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e409" xlink:type="simple"/></inline-formula>. b. Stored information in bits per synapse, as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e410" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003727.g007" position="float" xlink:type="simple"/></fig></sec></sec><sec id="s3">
<title>Discussion</title>
<p>We have presented an analytical method to compute the storage capacity of networks of binary neurons with binary synapses in the sparse coding limit. When applied to the classic Willshaw model, in the infinite limit, we find a maximal storage capacity of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e411" xlink:type="simple"/></inline-formula>, the same than found in previous studies, although with a different definition adapted to recurrent networks, as discussed in the section ‚ÄòWillshaw model‚Äô. We then used this method to study the storage capacity of a network with binary synapses and stochastic learning, in the single presentation (SP) scenario <xref ref-type="bibr" rid="pcbi.1003727-Amit4">[17]</xref>. The main advantage of this model, compared to the Willshaw model, is its palimpsest property, that allows it to do on-line learning in an ever changing environment. Amit and Fusi showed that the optimal storage capacity was obtained in the sparse coding limit, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e412" xlink:type="simple"/></inline-formula> and with a balance between the effect of depression and potentiation. The storage capacity of this network has been further studied for finite size networks in <xref ref-type="bibr" rid="pcbi.1003727-Amit5">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Huang1">[20]</xref>. We have complemented this work by computing analytically the storage capacity in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e413" xlink:type="simple"/></inline-formula> limit. The optimal capacity of the SP model is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e414" xlink:type="simple"/></inline-formula>, which is about <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e415" xlink:type="simple"/></inline-formula> times lower than the one of the Willshaw model. This decrease in storage capacity is similar to the decrease seen in palimpsest networks with continuous synapses - for example, in the Hopfield model the capacity is about <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e416" xlink:type="simple"/></inline-formula>, while in a palimpsest version the capacity drops to about <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e417" xlink:type="simple"/></inline-formula>. The reason for this decrease is that the most recently seen patterns have large basins of attraction, while older patterns have smaller ones. In the Willshaw model, all patterns are equivalent, and therefore they all have vanishing basins of attraction at the maximal capacity.</p>
<p>We have also studied the network in a multiple presentation (MP) scenario, with in which patterns presented to the network are noisy versions of a fixed set of prototypes, in the slow learning limit in which transition probabilities go to zero <xref ref-type="bibr" rid="pcbi.1003727-Brunel2">[23]</xref>. In the extreme case in which presented patterns are the prototypes, all synaptic weights are initially at zero, and if the synapses do not experience depression, this model is equivalent to the Willshaw model with a storage capacity of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e418" xlink:type="simple"/></inline-formula>, which is about <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e419" xlink:type="simple"/></inline-formula> times larger than the capacity of the SP model. A more interesting scenario is when depression is present. In this case then the network has generalization properties (it can learn prototypes from noisy versions of them), as well as palimpsest properties (if patterns drawn from a new set of prototypes are presented it will eventually replace a previous set with the new one). We have quantified the trade-off between generalization and storage capacity (see <xref ref-type="fig" rid="pcbi-1003727-g003">Figure 3d</xref>). For instance, if the noisy patterns have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e420" xlink:type="simple"/></inline-formula> of their selective neurons in common with the prototypes to be learned, the storage capacity is decreased from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e421" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e422" xlink:type="simple"/></inline-formula>.</p>
<p>A key step in estimating storage capacity is deriving an accurate approximation for the distribution of the inputs neurons receive. These inputs are the sum of a large number of binary variables, so the distribution is a binomial if one can neglect the correlations between these variables, induced by the learning process. Amit and Fusi <xref ref-type="bibr" rid="pcbi.1003727-Amit4">[17]</xref> showed that these correlations can be neglected when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e423" xlink:type="simple"/></inline-formula>. Thus, we expect the results with the binomial approximation to be exact in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e424" xlink:type="simple"/></inline-formula> limit. We have shown that a Gaussian approximation of the binomial distribution gives inaccurate results in the sparse coding limit, because the capacity depends on the tail of the distribution, which is not well described by a Gaussian. For larger coding levels (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e425" xlink:type="simple"/></inline-formula>), the binomial approximation breaks down because it does not take into account correlations between inputs. Following <xref ref-type="bibr" rid="pcbi.1003727-Amit5">[19]</xref> and <xref ref-type="bibr" rid="pcbi.1003727-Huang1">[20]</xref>, we use a Gaussian approximation that includes the covariance of the inputs, and show that this approximation captures well the simulation results in this coding level range.</p>
<p>We computed storage capacities for two different learning scenarios. Both are unsupervised, involve a Hebbian-type plasticity rule, and allow for online learning (providing patterns are presented multiple times for the MP model). It is of interest to compare the performance of these two particular scenarios with known upper bounds on storage capacity. For networks of infinite size with binary synapses such a bound has been derived using the Gardner approach <xref ref-type="bibr" rid="pcbi.1003727-Gutfreund1">[25]</xref>. In the sparse coding limit, this bound is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e426" xlink:type="simple"/></inline-formula> with random patterns (in which fluctuations in the number of selective neurons per pattern fluctuates), and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e427" xlink:type="simple"/></inline-formula> if patterns have a fixed number of selective neurons <xref ref-type="bibr" rid="pcbi.1003727-Brunel3">[26]</xref>. We found a capacity of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e428" xlink:type="simple"/></inline-formula> for the SP model and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e429" xlink:type="simple"/></inline-formula> for the MP model, obtained both for patterns with fixed and variable number of selective neurons. The result for the MP model seems to violate the Gardner bound. However, as noticed by Nadal <xref ref-type="bibr" rid="pcbi.1003727-Nadal2">[21]</xref>, one should be cautious in comparing these results: in our calculations we have required that a given pattern is stored perfectly with probability one, while the Gardner calculation requires that <bold>all</bold> patterns are stored perfectly with probability one. As mentioned in the section ‚ÄòWillshaw model‚Äô, the capacity of the Willshaw and MP models drops to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e430" xlink:type="simple"/></inline-formula> in the case of fixed-size patterns, if one insists that <bold>all</bold> patterns should be stored perfectly, which is now consistent with the Gardner bound. This means that the MP model is able to reach a capacity which is roughly half the Gardner bound, a rather impressive feat given the simplicity of the rule. Note that supervised learning rules can get closer to these theoretical bounds <xref ref-type="bibr" rid="pcbi.1003727-Baldassi1">[27]</xref>.</p>
<p>We have also studied finite-size networks, in which we defined the capacity as the number of patterns for which the probability of exact retrieval is at least 50%. We found that networks of reasonable sizes have capacities that are far from the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e431" xlink:type="simple"/></inline-formula> limit. For networks of sizes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e432" xlink:type="simple"/></inline-formula> storage capacities are reduced by a factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e433" xlink:type="simple"/></inline-formula> or more (see <xref ref-type="fig" rid="pcbi-1003727-g004">Figure 4</xref>). These huge finite size effects can be understood by the fact that the leading order corrections in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e434" xlink:type="simple"/></inline-formula> limit are in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e435" xlink:type="simple"/></inline-formula> - and so can never be neglected unless <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e436" xlink:type="simple"/></inline-formula> is an astronomical number (see Methods). A large part of the decrease in capacity when considering finite-size networks is due to fluctuations in the number of selective neurons from pattern to pattern. In the last section, we have used inhibition to minimize the effect of these fluctuations. For instance, for a network of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e437" xlink:type="simple"/></inline-formula> neurons learning in one shot, inhibition allows to increase capacity from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e438" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e439" xlink:type="simple"/></inline-formula>. For finite size networks, memory patterns that are not perfectly retrieved can still lead to fixed points where the activity is significantly correlated with the memory patterns. We have investigated with simulations how allowing errors in the retrieved patterns modifies storage capacity. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e440" xlink:type="simple"/></inline-formula>, the capacity increases from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e441" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e442" xlink:type="simple"/></inline-formula>, i.e. by approximately 30%.</p>
<p>Our study focused on networks of binary neurons, connected through binary synapses, and storing very sparse patterns. These three assumptions allowed us to compute analytically the storage capacity of the network in two learning scenarios. An important question is how far real neural networks are from such idealized assumptions. First, the issue of whether real synapses are binary, discrete but with a larger number of states, or essentially continuous, is still unresolved, with evidence in favor of each of these scenarios <xref ref-type="bibr" rid="pcbi.1003727-Petersen1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Montgomery1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-OConnor1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Enoki1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Loewenstein1">[32]</xref>. We expect that having synapses with a finite number <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e443" xlink:type="simple"/></inline-formula> of states will not modify strongly the picture outlined here <xref ref-type="bibr" rid="pcbi.1003727-Amit4">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Barrett1">[33]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Huang1">[20]</xref>. Second, it remains to be investigated how these results will generalize to networks of more realistic neurons. In strongly connected networks of spiking neurons operating in the balanced mode <xref ref-type="bibr" rid="pcbi.1003727-VanVreeswijk1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Amit6">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-vanVreeswijk1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Brunel4">[37]</xref>, the presence of ongoing activity presents strong constraints on the viability of sparsely coded selective attractor states. This is because ‚Äònon-selective‚Äô neurons are no longer silent, but are rather active at low background rates, and the noise due to this background activity can easily wipe out the selective signal <xref ref-type="bibr" rid="pcbi.1003727-Amit6">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Roudi1">[38]</xref>. In fact, simple scaling arguments in balanced networks suggest the optimal coding level would become <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e444" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003727-Brunel1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-vanVreeswijk2">[39]</xref>. The learning rules we have considered in this paper lead to a vanishing information stored per synapse with this scaling. Finding an unsupervised learning rule that achieves a finite information capacity in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e445" xlink:type="simple"/></inline-formula> limit in networks with discrete synapses for such coding levels remains an open question. However, the results presented here show that for networks of realistic sizes, the information capacity at such coding levels is in fact not very far from the optimal one that is reached at lower coding levels (see vertical lines in <xref ref-type="fig" rid="pcbi-1003727-g005">Figure 5</xref>‚Äì<xref ref-type="fig" rid="pcbi-1003727-g007">7</xref>). Finally, the coding levels of cortical networks during delay period activity remain poorly characterized. Experiments in IT cortex <xref ref-type="bibr" rid="pcbi.1003727-Miyashita2">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Miyashita3">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Nakamura1">[42]</xref> are consistent with coding levels of order 1%. Our results indicate that in networks of reasonable sizes, these coding levels are not far from the optimal values.</p>
<p>The SP and MP models investigated in this paper can be thought of as minimal models for learning in hippocampus and neocortex. The SP model bears some resemblance to the function of hippocampus, which is supposed to keep a memory of recent episodes that are learned in one shot, thanks to highly plastic synapses. The MP model relates to the function of neocortex, where a longer-term memory can be stored, thanks to repeated presentations of a set of prototypes that occur repeatedly in the environment, and perhaps during sleep under the supervision of the hippocampus. The idea that hippocampal and cortical networks learn on different time scales has been exploited in several modeling studies <xref ref-type="bibr" rid="pcbi.1003727-Alvarez1">[43]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Kli1">[44]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Roxin1">[45]</xref>, in which the memories are first stored in the hippocampus and then gradually transferred to cortical networks. It would be interesting to extend the type of analysis presented here to coupled hippocampo-cortical networks with varying degrees of plasticity.</p>
</sec><sec id="s4" sec-type="methods">
<title>Methods</title>
<sec id="s4a">
<title>Capacity calculation for infinite size networks</title>
<p>We are interested at retrieving pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e446" xlink:type="simple"/></inline-formula> that has been presented during the learning phase. We set the network in this state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e447" xlink:type="simple"/></inline-formula> and ask whether the network remains in this state while the dynamics (2) is running. At the first iteration, each neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e448" xlink:type="simple"/></inline-formula> is receiving a field<disp-formula id="pcbi.1003727.e449"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e449" xlink:type="simple"/><label>(35)</label></disp-formula></p>
<p>Where M+1 is the number of selective neurons in pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e450" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e451" xlink:type="simple"/></inline-formula>. Where we use the standard ‚ÄòLandau‚Äô notations: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e452" xlink:type="simple"/></inline-formula> means that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e453" xlink:type="simple"/></inline-formula> goes to a finite limit in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e454" xlink:type="simple"/></inline-formula> limit, while <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e455" xlink:type="simple"/></inline-formula> means that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e456" xlink:type="simple"/></inline-formula> goes to zero in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e457" xlink:type="simple"/></inline-formula> limit. and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e458" xlink:type="simple"/></inline-formula>. We recall that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e459" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e460" xlink:type="simple"/></inline-formula>. Thus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e461" xlink:type="simple"/></inline-formula> is a binary random variable which is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e462" xlink:type="simple"/></inline-formula> with probability, either <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e463" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e464" xlink:type="simple"/></inline-formula> is a selective neuron (sites <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e465" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e466" xlink:type="simple"/></inline-formula>), or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e467" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e468" xlink:type="simple"/></inline-formula> is a non-selective neuron (sites <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e469" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e470" xlink:type="simple"/></inline-formula>). Neglecting correlations between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e471" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e472" xlink:type="simple"/></inline-formula> (it is legitimate in the sparse coding limit we are interested in, see <xref ref-type="bibr" rid="pcbi.1003727-Amit4">[17]</xref>), the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e473" xlink:type="simple"/></inline-formula>'s are independent and the distribution of the field on selective neurons can be written as<disp-formula id="pcbi.1003727.e474"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e474" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003727.e475"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e475" xlink:type="simple"/><label>(36)</label></disp-formula>where we used Stirling formula for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e476" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e477" xlink:type="simple"/></inline-formula> defined in (13). For non-selective neurons<disp-formula id="pcbi.1003727.e478"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e478" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003727.e479"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e479" xlink:type="simple"/><label>(37)</label></disp-formula></p>
<p>Now write<disp-formula id="pcbi.1003727.e480"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e480" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003727.e481"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e481" xlink:type="simple"/><label>(38)</label></disp-formula></p>
<p>In the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e482" xlink:type="simple"/></inline-formula> we are considering in this section, and if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e483" xlink:type="simple"/></inline-formula>, the sums corresponding to the probabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e484" xlink:type="simple"/></inline-formula> are dominated by their first term (corrections are made explicit in the following section). Keeping only higher order terms in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e485" xlink:type="simple"/></inline-formula> in Eqs. (36) and (37), we have:<disp-formula id="pcbi.1003727.e486"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e486" xlink:type="simple"/><label>(39)</label></disp-formula>and<disp-formula id="pcbi.1003727.e487"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e487" xlink:type="simple"/><label>(40)</label></disp-formula></p>
<p>yielding Eq. (15) with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e488" xlink:type="simple"/></inline-formula>. Note that with the coding levels we are considering here (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e489" xlink:type="simple"/></inline-formula>), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e490" xlink:type="simple"/></inline-formula> is of order <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e491" xlink:type="simple"/></inline-formula>. When the number of selective neurons per pattern is fixed at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e492" xlink:type="simple"/></inline-formula>, we choose <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e493" xlink:type="simple"/></inline-formula> for the activation threshold and these equations become:<disp-formula id="pcbi.1003727.e494"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e494" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003727.e495"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e495" xlink:type="simple"/><label>(41)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e496" xlink:type="simple"/></inline-formula></p>
<p>For random numbers of selective neurons we need to compute the average over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e497" xlink:type="simple"/></inline-formula>: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e498" xlink:type="simple"/></inline-formula>. Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e499" xlink:type="simple"/></inline-formula> is distributed according to a binomial of average <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e500" xlink:type="simple"/></inline-formula> and variance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e501" xlink:type="simple"/></inline-formula>, for sufficiently large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e502" xlink:type="simple"/></inline-formula>, this can be approximated as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e503" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e504" xlink:type="simple"/></inline-formula> is normally distributed:<disp-formula id="pcbi.1003727.e505"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e505" xlink:type="simple"/><label>(42)</label></disp-formula>with<disp-formula id="pcbi.1003727.e506"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e506" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003727.e507"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e507" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003727.e508"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e508" xlink:type="simple"/><label>(43)</label></disp-formula>and<disp-formula id="pcbi.1003727.e509"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e509" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003727.e510"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e510" xlink:type="simple"/><label>(44)</label></disp-formula></p>
<p>When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e511" xlink:type="simple"/></inline-formula> goes to infinity, we bring the limit into the integral in Eq. (42) and obtain<disp-formula id="pcbi.1003727.e512"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e512" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003727.e513"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e513" xlink:type="simple"/><label>(45)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e514" xlink:type="simple"/></inline-formula> is the Heaviside function. Thus in the limit of infinite size networks, the probability of no error is a step function. The first Heaviside function implies that the only requirement to avoid errors on selective neurons is to have a scaled activation threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e515" xlink:type="simple"/></inline-formula> below <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e516" xlink:type="simple"/></inline-formula>. The second Heaviside function implies that, depending on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e517" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e518" xlink:type="simple"/></inline-formula> has to be chosen far enough from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e519" xlink:type="simple"/></inline-formula>. The above equation allows to derive the inequalities (19) and (20).</p>
</sec><sec id="s4b">
<title>Capacity calculation for finite-size networks</title>
<p>We now turn to a derivation of finite-size corrections for the capacity. Here we show two different calculations. In the first calculation, we derive Eq. (32), taking into account the leading-order correction term in Eq. (43). This allows us to compute the leading-order correction to the number of patterns <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e520" xlink:type="simple"/></inline-formula> that can be stored for a given set of parameters. However, it does not predict accurately the storage capacity of the large-size but finite networks that we simulated. In the second calculation presented, we focus on computing the probability of no error in a given pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e521" xlink:type="simple"/></inline-formula>, including a next-to-leading-order correction.</p>
<p>Eq. (32) is derived for a fixed set of parameters, assuming that the set of active neurons have a fixed size, and that the activation threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e522" xlink:type="simple"/></inline-formula> has been chosen large enough such that the probability to have non-selective neurons activated is small. From the Stirling expansion, adding the first finite-size correction term in Eq. (41), we get<disp-formula id="pcbi.1003727.e523"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e523" xlink:type="simple"/><label>(46)</label></disp-formula>with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e524" xlink:type="simple"/></inline-formula>. For large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e525" xlink:type="simple"/></inline-formula>, the number of stored patterns <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e526" xlink:type="simple"/></inline-formula> can be increased until <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e527" xlink:type="simple"/></inline-formula>. Setting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e528" xlink:type="simple"/></inline-formula>, an expansion of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e529" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e530" xlink:type="simple"/></inline-formula> allows to write<disp-formula id="pcbi.1003727.e531"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e531" xlink:type="simple"/><label>(47)</label></disp-formula></p>
<p>The <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e532" xlink:type="simple"/></inline-formula> patterns are correctly stored as long as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e533" xlink:type="simple"/></inline-formula>. This condition is satisfied for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e534" xlink:type="simple"/></inline-formula>. For the SP model, we can deduce which value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e535" xlink:type="simple"/></inline-formula> yields this value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e536" xlink:type="simple"/></inline-formula> (see Eq. (26)). This allows to derive Eq. (32),<disp-formula id="pcbi.1003727.e537"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e537" xlink:type="simple"/><label>(48)</label></disp-formula></p>
<p>We now turn to a calculation of the probability of no error on a given pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e538" xlink:type="simple"/></inline-formula>, taking into account the next-to-leading order correction of order one, in addition to the term of order <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e539" xlink:type="simple"/></inline-formula> in Eq. (41). This is necessary to predict accurately the capacity of realistic size networks (for instance for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e540" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e541" xlink:type="simple"/></inline-formula>). <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e542" xlink:type="simple"/></inline-formula> is computed for a memory pattern with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e543" xlink:type="simple"/></inline-formula> selective neurons. The estimation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e544" xlink:type="simple"/></inline-formula> used in the figures is obtained by averaging over different values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e545" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e546" xlink:type="simple"/></inline-formula> drawn from a binomial distribution of mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e547" xlink:type="simple"/></inline-formula>.</p>
<p>We first provide a more detailed expansion of the sums in Eq. (38). Setting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e548" xlink:type="simple"/></inline-formula>, with the Taylor expansions:<disp-formula id="pcbi.1003727.e549"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e549" xlink:type="simple"/><label>(49)</label></disp-formula><disp-formula id="pcbi.1003727.e550"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e550" xlink:type="simple"/><label>(50)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e551" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e552" xlink:type="simple"/></inline-formula>. Using (37) we can rewrite:<disp-formula id="pcbi.1003727.e553"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e553" xlink:type="simple"/><label>(51)</label></disp-formula></p>
<p>In the cases we consider, we will always have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e554" xlink:type="simple"/></inline-formula> so that we can consider only the term of order <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e555" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e556" xlink:type="simple"/></inline-formula>. The sum is now geometric, and we obtain<disp-formula id="pcbi.1003727.e557"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e557" xlink:type="simple"/><label>(52)</label></disp-formula></p>
<p>The same kind of expansion can be applied for the selective neurons. Again if we are in a situation where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e558" xlink:type="simple"/></inline-formula><disp-formula id="pcbi.1003727.e559"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e559" xlink:type="simple"/><label>(53)</label></disp-formula></p>
<p>When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e560" xlink:type="simple"/></inline-formula> is close to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e561" xlink:type="simple"/></inline-formula> and thus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e562" xlink:type="simple"/></inline-formula>, we are then left with:<disp-formula id="pcbi.1003727.e563"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e563" xlink:type="simple"/><label>(54)</label></disp-formula><disp-formula id="pcbi.1003727.e564"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e564" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003727.e565"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e565" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003727.e566"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e566" xlink:type="simple"/><label>(55)</label></disp-formula></p>
<p>When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e567" xlink:type="simple"/></inline-formula> is too close to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e568" xlink:type="simple"/></inline-formula>, which is the case for the optimal parameters in the large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e569" xlink:type="simple"/></inline-formula> limit, we need to use (55). It only contributes a term of order <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e570" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e571" xlink:type="simple"/></inline-formula> and does not modify our results. In <xref ref-type="fig" rid="pcbi-1003727-g006">Figures 6</xref>-<xref ref-type="fig" rid="pcbi-1003727-g007">7</xref>, we use (53), which gives from (38) and (36), (37) and (53),(52):<disp-formula id="pcbi.1003727.e572"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e572" xlink:type="simple"/><label>(56)</label></disp-formula><disp-formula id="pcbi.1003727.e573"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e573" xlink:type="simple"/><label>(57)</label></disp-formula></p>
<p>The probability of no error is<disp-formula id="pcbi.1003727.e574"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e574" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003727.e575"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e575" xlink:type="simple"/><label>(58)</label></disp-formula>which leads to Eqs. (31)<disp-formula id="pcbi.1003727.e576"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e576" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003727.e577"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e577" xlink:type="simple"/></disp-formula></p>
</sec><sec id="s4c">
<title>Gaussian approximation of the fields distribution</title>
<p>For a fixed number <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e578" xlink:type="simple"/></inline-formula> of selective neurons in pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e579" xlink:type="simple"/></inline-formula>, approximating the distribution of the fields on background neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e580" xlink:type="simple"/></inline-formula> and selective neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e581" xlink:type="simple"/></inline-formula> with a gaussian distribution gives:<disp-formula id="pcbi.1003727.e582"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e582" xlink:type="simple"/><label>(59)</label></disp-formula>where<disp-formula id="pcbi.1003727.e583"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e583" xlink:type="simple"/><label>(60)</label></disp-formula>and<disp-formula id="pcbi.1003727.e584"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e584" xlink:type="simple"/><label>(61)</label></disp-formula>where<disp-formula id="pcbi.1003727.e585"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e585" xlink:type="simple"/><label>(62)</label></disp-formula></p>
<p>The probability that those fields are on the wrong side of the threshold are:<disp-formula id="pcbi.1003727.e586"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e586" xlink:type="simple"/><label>(63)</label></disp-formula>and<disp-formula id="pcbi.1003727.e587"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e587" xlink:type="simple"/><label>(64)</label></disp-formula></p>
<p>Following the same calculations presented, and keeping only terms that are relevant in the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e588" xlink:type="simple"/></inline-formula>, the probability that there is no error is given by:<disp-formula id="pcbi.1003727.e589"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e589" xlink:type="simple"/><label>(65)</label></disp-formula>where the rate function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e590" xlink:type="simple"/></inline-formula> is<disp-formula id="pcbi.1003727.e591"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e591" xlink:type="simple"/><label>(66)</label></disp-formula></p>
<p>Calculations with the binomial versus the gaussian approximation differ only in the form of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e592" xlink:type="simple"/></inline-formula>. Finite size terms can be taken into account in the same way it is done in the previous Methods section for the binomial approximation.</p>
<p>In all above calculations we assumed that fields are sums of independent random variables (35). For small <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003727.e593" xlink:type="simple"/></inline-formula> correlations are negligible <xref ref-type="bibr" rid="pcbi.1003727-Amit4">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003727-Amit5">[19]</xref>. It is possible to compute the covariances between the terms of the sum (see Eq. (3.9) in <xref ref-type="bibr" rid="pcbi.1003727-Amit5">[19]</xref>), and take them into account in the gaussian approximation. This can be done using<disp-formula id="pcbi.1003727.e594"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e594" xlink:type="simple"/><label>(67)</label></disp-formula><disp-formula id="pcbi.1003727.e595"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e595" xlink:type="simple"/><label>(68)</label></disp-formula>in Eqs. (59),(61), where<disp-formula id="pcbi.1003727.e596"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003727.e596" xlink:type="simple"/><label>(69)</label></disp-formula></p>
</sec></sec></body>
<back>
<ack>
<p>We would like to thank Stefano Fusi for his comments on a first version of the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1003727-Hopfield1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name> (<year>1982</year>) <article-title>Neural Networks and Physical Systems with Emergent Collective Computational Abilities</article-title>. <source>PNAS</source> <volume>79</volume>: <fpage>2554</fpage>‚Äì<lpage>2558</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Amit1"><label>2</label>
<mixed-citation publication-type="other" xlink:type="simple">Amit DJ (1989) Modeling Brain Function: The World of Attractor Neural Networks. Cambridge University Press.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Brunel1"><label>3</label>
<mixed-citation publication-type="other" xlink:type="simple">Brunel N (2004) Network Models of Memory, in Methods and Models in Neurophysics, CChow, BGutkin, DHansel, CMeunier and JDalibard Eds., Elsevier.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Fuster1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fuster</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Alexander</surname><given-names>G</given-names></name> (<year>1971</year>) <article-title>Neuron activity related to short-term memory</article-title>. <source>Science</source> <volume>173</volume>: <fpage>652</fpage>‚Äì<lpage>654</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Miyashita1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miyashita</surname><given-names>Y</given-names></name> (<year>1993</year>) <article-title>Inferior Temporal Cortex: where visual perception meets memory</article-title>. <source>Ann. Rev. Neurosci</source>. <volume>16</volume>: <fpage>245</fpage>‚Äì<lpage>263</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Fuster2"><label>6</label>
<mixed-citation publication-type="other" xlink:type="simple">Fuster JM (1995) Memory in the cerebral cortex. MIT press.</mixed-citation>
</ref>
<ref id="pcbi.1003727-GoldmanRakic1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goldman-Rakic</surname><given-names>PS</given-names></name> (<year>1995</year>) <article-title>Cellular basis of working memory</article-title>. <source>Neuron</source> <volume>14</volume>: <fpage>477</fpage>‚Äì<lpage>485</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Amit2"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Gutfreund</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>1987</year>) <article-title>Statistical mechanics of neural networks near saturation</article-title>. <source>Annals of Physics</source> <volume>173</volume>: <fpage>30</fpage>‚Äì<lpage>67</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Sompolinsky1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>1986</year>) <article-title>Neural networks with nonlinear synapses and a static noise</article-title>. <source>Physical Review A</source> <volume>34</volume>: <fpage>2571</fpage>‚Äì<lpage>2574</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Gardner1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gardner</surname><given-names>E</given-names></name> (<year>1988</year>) <article-title>The space of interactions in neural network models</article-title>. <source>Journal of Physics A: Mathematical and General</source> <volume>21</volume>: <fpage>257</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Tsodyks1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name>, <name name-style="western"><surname>FeigelMan</surname><given-names>M</given-names></name> (<year>1988</year>) <article-title>The enhanced storage capacity in neural networks with low activity level</article-title>. <source>EPL (Europhysics Letters)</source> <volume>6</volume>: <fpage>101</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Sejnowski1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name> (<year>1977</year>) <article-title>Storing covariance with nonlinearly interacting neurons</article-title>. <source>Journal of Mathematical Biology</source> <volume>4</volume>: <fpage>303</fpage>‚Äì<lpage>321</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Amit3"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Tsodyks</surname><given-names>MV</given-names></name> (<year>1991</year>) <article-title>Quantitative study of attractor neural networks retrieving at low spike rates: II. Low-rate retrieval in symmetric networks</article-title>. <source>Network: Computation in Neural Systems</source> <volume>2</volume>: <fpage>275</fpage>‚Äì<lpage>294</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Nadal1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nadal</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Toulouse</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Changeux</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Dehaene</surname><given-names>S</given-names></name> (<year>1986</year>) <article-title>Networks of formal neurons and memory palimpsests</article-title>. <source>Europhys. Lett</source>. <volume>1</volume>: <fpage>535</fpage>‚Äì<lpage>542</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Parisi1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parisi</surname><given-names>G</given-names></name> (<year>1986</year>) <article-title>A memory which forgets</article-title>. <source>Journal of Physics A: Mathematical and General</source> <volume>19</volume>: <fpage>L617</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Tsodyks2"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name> (<year>1990</year>) <article-title>Associative Memory in Neural Networks with Binary Synapses</article-title>. <source>Modern Physics Letters B</source> <volume>4</volume>: <fpage>713</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Amit4"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name> (<year>1994</year>) <article-title>Learning in neural networks with material synapses</article-title>. <source>Neural Computation</source> <volume>6</volume>: <fpage>957</fpage>‚Äì<lpage>982</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Willshaw1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Willshaw</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Buneman</surname><given-names>OP</given-names></name>, <name name-style="western"><surname>Longuet-Higgins</surname><given-names>HC</given-names></name> (<year>1969</year>) <article-title>Non-Holographic Associative Memory</article-title>. <source>Nature</source> <volume>222</volume>: <fpage>960</fpage>‚Äì<lpage>962</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Amit5"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Huang</surname><given-names>Y</given-names></name> (<year>2010</year>) <article-title>Precise capacity analysis in binary networks with multiple coding level inputs</article-title>. <source>Neural Computation</source> <volume>22</volume>: <fpage>660</fpage>‚Äì<lpage>688</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Huang1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huang</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Amit</surname><given-names>Y</given-names></name> (<year>2011</year>) <article-title>Capacity analysis in multi-state synaptic models: a retrieval probability perspective</article-title>. <source>Journal of Computational Neuroscience</source> <volume>30</volume>: <fpage>699</fpage>‚Äì<lpage>720</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Nadal2"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nadal</surname><given-names>JP</given-names></name> (<year>1991</year>) <article-title>Associative memory: on the (puzzling) sparse coding limit</article-title>. <source>Journal of Physics A: Mathematical and General</source> <volume>24</volume>: <fpage>1093</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Knoblauch1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knoblauch</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Palm</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Sommer</surname><given-names>FT</given-names></name> (<year>2010</year>) <article-title>Memory capacities for synaptic and structural plasticity</article-title>. <source>Neural Computation</source> <volume>22</volume>: <fpage>289</fpage>‚Äì<lpage>341</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Brunel2"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Carusi</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name> (<year>1998</year>) <article-title>Slow stochastic Hebbian learning of classes of stimuli in a recurrent neural network</article-title>. <source>Network: Computation in Neural Systems</source> <volume>9</volume>: <fpage>123</fpage>‚Äì<lpage>152</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Leibold1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leibold</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Kempter</surname><given-names>R</given-names></name> (<year>2008</year>) <article-title>Sparseness constrains the prolongation of memory lifetime via synaptic metaplasticity</article-title>. <source>Cerebral Cortex</source> <volume>18</volume>: <fpage>67</fpage>‚Äì<lpage>77</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Gutfreund1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gutfreund</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Stein</surname><given-names>Y</given-names></name> (<year>1990</year>) <article-title>Capacity of neural networks with discrete synaptic couplings</article-title>. <source>Journal of Physics A: Mathematical and General</source> <volume>23</volume>: <fpage>2613</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Brunel3"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name> (<year>1994</year>) <article-title>Storage capacity of neural networks: effect of the fluctuations of the number of active neurons per memory</article-title>. <source>Journal of Physics A: Mathematical and General</source> <volume>27</volume>: <fpage>4783</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Baldassi1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Baldassi</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Braunstein</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Zecchina</surname><given-names>R</given-names></name> (<year>2007</year>) <article-title>Efficient supervised learning in networks with binary synapses</article-title>. <source>PNAS</source> <volume>104</volume>: <fpage>11079</fpage>‚Äì<lpage>11084</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Petersen1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Petersen</surname><given-names>CC</given-names></name>, <name name-style="western"><surname>Malenka</surname><given-names>RC</given-names></name>, <name name-style="western"><surname>Nicoll</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name> (<year>1998</year>) <article-title>All-or-none potentiation at CA3-CA1 synapses</article-title>. <source>PNAS</source> <volume>95</volume>: <fpage>4732</fpage>‚Äì<lpage>4737</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Montgomery1"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Montgomery</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Madison</surname><given-names>DV</given-names></name> (<year>2004</year>) <article-title>Discrete synaptic states define a major mechanism of synapse plasticity</article-title>. <source>Trends in Neurosciences</source> <volume>27(12)</volume>: <fpage>744</fpage>‚Äì<lpage>750</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-OConnor1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>OConnor</surname><given-names>DH</given-names></name>, <name name-style="western"><surname>Wittenberg</surname><given-names>GM</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>SSH</given-names></name> (<year>2005</year>) <article-title>Graded bidirectional synaptic plasticity is composed of switch-like unitary events</article-title>. <source>PNAS</source> <volume>102</volume>: <fpage>9679</fpage>‚Äì<lpage>9684</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Enoki1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Enoki</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Hu</surname><given-names>YL</given-names></name>, <name name-style="western"><surname>Hamilton</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Fine</surname><given-names>A</given-names></name> (<year>2009</year>) <article-title>Expression of long-term plasticity at individual synapses in hippocampus is graded, bidirectional, and mainly presynaptic: optical quantal analysis</article-title>. <source>Neuron</source> <volume>62(2)</volume>: <fpage>242</fpage>‚Äì<lpage>253</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Loewenstein1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Loewenstein</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Kuras</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Rumpel</surname><given-names>S</given-names></name> (<year>2011</year>) <article-title>Multiplicative dynamics underlie the emergence of the log-normal distribution of spine sizes in the neocortex in vivo</article-title>. <source>Journal of Neuroscience</source> <volume>31(26)</volume>: <fpage>9481</fpage>‚Äì<lpage>9488</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Barrett1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barrett</surname><given-names>AB</given-names></name>, <name name-style="western"><surname>van Rossum</surname><given-names>MC</given-names></name> (<year>2008</year>) <article-title>Optimal learning rules for discrete synapses</article-title>. <source>PLoS Computational Biology</source> <volume>4(11)</volume>: <fpage>e10000230</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-VanVreeswijk1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van Vreeswijk</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>1996</year>) <article-title>Chaos in neuronal networks with balanced excitatory and inhibitory activity</article-title>. <source>Science</source> <volume>274</volume>: <fpage>1724</fpage>‚Äì<lpage>1726</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Amit6"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name> (<year>1997</year>) <article-title>Model of global spontaneous activity and local structured activity during delay periods in the cerebral cortex</article-title>. <source>Cerebral Cortex</source> <volume>7</volume>: <fpage>237</fpage>‚Äì<lpage>252</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-vanVreeswijk1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Vreeswijk</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>1998</year>) <article-title>Chaotic Balanced State in a Model of Cortical Circuits</article-title>. <source>Neural Comp</source>. <volume>10</volume>: <fpage>1321</fpage>‚Äì<lpage>1372</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Brunel4"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name> (<year>2000</year>) <article-title>Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons</article-title>. <source>Journal of Computational Neuroscience</source> <volume>8</volume>: <fpage>183‚Äì208</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Roudi1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roudi</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name> (<year>2007</year>) <article-title>A Balanced Memory Network</article-title>. <source>PLoS Computational Biology</source> <volume>3</volume>: <fpage>e141</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-vanVreeswijk2"><label>39</label>
<mixed-citation publication-type="other" xlink:type="simple">van Vreeswijk CA, Sompolinsky H (2004) Irregular activity in large networks of neurons, in Methods and Models in Neurophysics, CChow, BGutkin, DHansel, CMeunier and JDalibard Eds., Elsevier.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Miyashita2"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miyashita</surname><given-names>Y</given-names></name> (<year>1988</year>) <article-title>Neuronal correlate of visual associative long-term memory in the primate temporal cortex</article-title>. <source>Nature</source> <volume>335</volume>: <fpage>817</fpage>‚Äì<lpage>820</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Miyashita3"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miyashita</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Chang</surname><given-names>HS</given-names></name> (<year>1988</year>) <article-title>Neuronal correlate of pictorial short-term memory in the primate temporal cortex</article-title>. <source>Nature</source> <volume>331</volume>: <fpage>68</fpage>‚Äì<lpage>70</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Nakamura1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nakamura</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Kubota</surname><given-names>K</given-names></name> (<year>1995</year>) <article-title>Mnemonic firing of neurons in the monkey temporal pole during a visual recognition memory task</article-title>. <source>Journal of Neurophysiology</source> <volume>74</volume>: <fpage>162</fpage>‚Äì<lpage>178</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Alvarez1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alvarez</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Squire</surname><given-names>LR</given-names></name> (<year>1994</year>) <article-title>Memory consolidation and the medial temporal lobe: a simple network model</article-title>. <source>PNAS</source> <volume>91</volume>: <fpage>7041‚Äì7045</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Kli1"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>K√†li</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name> (<year>2004</year>) <article-title>Off-line replay maintains declarative memories in a model of hippocampal-neocortical interactions</article-title>. <source>Nature Neuroscience</source> <volume>7</volume>: <fpage>286</fpage>‚Äì<lpage>294</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003727-Roxin1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roxin</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name> (<year>2013</year>) <article-title>Efficient Partitioning of Memory Systems and Its Importance for Memory Consolidation</article-title>. <source>PLoS Computational Biology</source> <volume>9</volume>: <fpage>e1003146</fpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>