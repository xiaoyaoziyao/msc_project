<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-01546</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003512</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Circuit models</subject></subj-group></subj-group><subj-group><subject>Learning and memory</subject><subject>Neural homeostasis</subject><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Mathematics</subject><subj-group><subject>Nonlinear dynamics</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>Spatiotemporal Computations of an Excitable and Plastic Brain: Neuronal Plasticity Leads to Noise-Robust and Noise-Constructive Computations</article-title>
<alt-title alt-title-type="running-head">Computations in an Excitable and Plastic Brain</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Toutounji</surname><given-names>Hazem</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Pipa</surname><given-names>Gordon</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
</contrib-group>
<aff id="aff1"><addr-line>Institute of Cognitive Science, University of Osnabrück, Osnabrück, Lower Saxony, Germany</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Sporns</surname><given-names>Olaf</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Indiana University, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">htoutounji@uni-osnabrueck.de</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: HT GP. Performed the experiments: HT. Analyzed the data: HT. Contributed reagents/materials/analysis tools: HT. Wrote the paper: HT GP.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>3</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>20</day><month>3</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>3</issue>
<elocation-id>e1003512</elocation-id>
<history>
<date date-type="received"><day>30</day><month>8</month><year>2013</year></date>
<date date-type="accepted"><day>29</day><month>1</month><year>2014</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Toutounji, Pipa</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>It is a long-established fact that neuronal plasticity occupies the central role in generating neural function and computation. Nevertheless, no unifying account exists of how neurons in a recurrent cortical network learn to compute on temporally and spatially extended stimuli. However, these stimuli constitute the norm, rather than the exception, of the brain's input. Here, we introduce a geometric theory of learning spatiotemporal computations through neuronal plasticity. To that end, we rigorously formulate the problem of neural representations as a relation in space between stimulus-induced neural activity and the asymptotic dynamics of excitable cortical networks. Backed up by computer simulations and numerical analysis, we show that two canonical and widely spread forms of neuronal plasticity, that is, spike-timing-dependent synaptic plasticity and intrinsic plasticity, are both necessary for creating neural representations, such that these computations become realizable. Interestingly, the effects of these forms of plasticity on the emerging neural code relate to properties necessary for both combating and utilizing noise. The neural dynamics also exhibits features of the most likely stimulus in the network's spontaneous activity. These properties of the spatiotemporal neural code resulting from plasticity, having their grounding in nature, further consolidate the biological relevance of our findings.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>The world is not perceived as a chain of segmented sensory still lifes. Instead, it appears that the brain is capable of integrating the temporal dependencies of the incoming sensory stream with the spatial aspects of that input. It then transfers the resulting whole in a useful manner, in order to reach a coherent and causally sound image of our physical surroundings, and to act within it. These spatiotemporal computations are made possible through a cluster of local and coexisting adaptation mechanisms known collectively as neuronal plasticity. While this role is widely known and supported by experimental evidence, no unifying theory of how the brain, through the interaction of plasticity mechanisms, gets to represent spatiotemporal computations in its spatiotemporal activity. In this paper, we aim at such a theory. We develop a rigorous mathematical formalism of spatiotemporal representations within the input-driven dynamics of cortical networks. We demonstrate that the interaction of two of the most common plasticity mechanisms, intrinsic and synaptic plasticity, leads to representations that allow for spatiotemporal computations. We also show that these representations are structured to tolerate noise and to even benefit from it.</p>
</abstract>
<funding-group><funding-statement>The authors acknowledge the financial support of the State Lower Saxony, Germany via the University of Osnabrück, and the European project PHOCUS in the Framework ‘Information and Communication Technologies’ (FP7-ICT-2009-C/Proposal Nr. 240763). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="20"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Neuronal plasticity, both homeostatic and synaptic, is the central ingredient for the generation and adaptation of neural function and computation <xref ref-type="bibr" rid="pcbi.1003512-Cooper1">[1]</xref>. However, it remains mostly unclear how neurons in recurrent neural networks utilize neuronal plasticity to self-organize and to learn computing on temporally and spatially extended stimuli <xref ref-type="bibr" rid="pcbi.1003512-Broome1">[2]</xref>–<xref ref-type="bibr" rid="pcbi.1003512-Nikoli1">[4]</xref>.</p>
<p>A full grasp of the principles of self-organization by plasticity in recurrent neural networks is <italic>jointly</italic> hampered by the diversity of existing neuronal plasticity mechanisms <xref ref-type="bibr" rid="pcbi.1003512-Abbott1">[5]</xref>–<xref ref-type="bibr" rid="pcbi.1003512-Turrigiano1">[7]</xref> and the limited understanding of their functions and cooperations, by the <italic>emergent</italic> nature of computation in recurrent systems, in the sense that computation is a collective phenomenon of the system as a whole and cannot be fully understood from the contribution of individual neurons <xref ref-type="bibr" rid="pcbi.1003512-Rumelhart1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Izhikevich1">[9]</xref>, and by the fact that neural systems are subject to noise <xref ref-type="bibr" rid="pcbi.1003512-Shadlen1">[10]</xref>–<xref ref-type="bibr" rid="pcbi.1003512-Rolls1">[13]</xref>. In this paper, we simultaneously address these issues by studying the basic principles of self-organization in <italic>recurrent</italic> networks that arise from the interaction of synaptic and homeostatic intrinsic <italic>plasticity</italic>, and given that the network is subject to <italic>noise</italic>. To this end, we use numerical methods to explore the dynamics of nonautonomous, i.e. stimulus-driven, and plastic recurrent networks, and we provide a mathematical formalization for attaining a rigorously sound perspective (see <xref ref-type="sec" rid="s4">Methods</xref>).</p>
<p>Incorporating synaptic plasticity with homeostasis goes back to Bienenstock, Cooper, and Monro's groundbreaking work known as the <italic>BCM theory</italic> <xref ref-type="bibr" rid="pcbi.1003512-Bienenstock1">[14]</xref>. Through rigorous mathematical analysis, the BCM theory predicted the necessity of a certain form of a sliding threshold, i.e. a homeostatic adjustment of neuronal excitability, for stabilizing the plastic afferent weights of a single neuron. Empirical findings supported the hypothesis of adjustable excitability and showed that it manifests through changes of neuronal properties at the soma <xref ref-type="bibr" rid="pcbi.1003512-Zhang1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Turrigiano1">[7]</xref>. While the BCM theory suggests homeostasis as a stabilization mechanism of synaptic weights with no direct influence on the neuron's encoding properties, Triesch proposed a homeostatic <bold><italic>i</italic></bold><italic>ntrinsic </italic><bold><italic>p</italic></bold><italic>lasticity</italic> (<monospace>IP</monospace>) mechanism that increases the neuron's encoding capacity and cooperates with <bold><italic>s</italic></bold><italic>ynaptic </italic><bold><italic>p</italic></bold><italic>lasticity</italic> (SP) to discover nonlinear independent features of the neuron's inputs <xref ref-type="bibr" rid="pcbi.1003512-Triesch1">[15]</xref>.</p>
<p>These investigations, among others <xref ref-type="bibr" rid="pcbi.1003512-Toyoizumi1">[16]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Savin1">[17]</xref>, are very insightful in pinpointing how synaptic and homeostatic plasticity interact in <italic>single neurons</italic>. In addition, <italic>feedforward</italic> neural networks greatly simplify the analysis and understanding of self-organization and computation based on neuronal plasticity. For such architectures, both single plasticity rules, as well as combinations of different plasticity mechanisms, had been linked to neural computation, such as the formation of receptive fields <xref ref-type="bibr" rid="pcbi.1003512-Bienenstock1">[14]</xref>, the related identification of statistically-independent components <xref ref-type="bibr" rid="pcbi.1003512-Triesch1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Savin1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Clopath1">[18]</xref>, and predictive coding <xref ref-type="bibr" rid="pcbi.1003512-Buesing1">[19]</xref>. However, it is important to note that neurons are embedded within large and highly recurrent networks <xref ref-type="bibr" rid="pcbi.1003512-Douglas1">[20]</xref>–<xref ref-type="bibr" rid="pcbi.1003512-Weiler1">[23]</xref>, and that an efficient use of neuronal resources entails distributed encoding schemes <xref ref-type="bibr" rid="pcbi.1003512-Rumelhart1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Izhikevich1">[9]</xref>. In addition, besides the spatial features of the world, its temporal structure should also be captured by the neural code <xref ref-type="bibr" rid="pcbi.1003512-Nikoli1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-DeAngelis1">[24]</xref>–<xref ref-type="bibr" rid="pcbi.1003512-Mauk1">[26]</xref>.</p>
<p>Our understanding of neural information processing would greatly improve by extending the principles of self-organization to recurrent neural circuits, since the latter constitute the basic computational units in the cortex <xref ref-type="bibr" rid="pcbi.1003512-Douglas3">[22]</xref>. Lazar et al. were the first to study the emergence of computation from the interaction of different forms of plasticity on recurrent neural networks <xref ref-type="bibr" rid="pcbi.1003512-Lazar1">[27]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Lazar2">[28]</xref>. This study builds on their findings. However, we <italic>do not</italic> restrict the definition of computation to linear classifiers of the <bold><italic>r</italic></bold><italic>eservoir </italic><bold><italic>c</italic></bold><italic>omputing</italic> (RC) paradigm <xref ref-type="bibr" rid="pcbi.1003512-Buonomano1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Jaeger1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Maass1">[30]</xref>. In addition to training linear classifiers for measuring the <italic>computational performance</italic>, we identified the necessity of analyzing the response of the recurrent neural network itself as an <italic>input-driven dynamical system</italic> <xref ref-type="bibr" rid="pcbi.1003512-Kloeden1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Kloeden2">[32]</xref>, and of concurrently viewing the network as a <italic>communication channel</italic> by taking an <italic>information-theoretical</italic> perspective <xref ref-type="bibr" rid="pcbi.1003512-Cover1">[33]</xref>. Combining these tools enables us to understand how information is encoded in recurrent systems, how such encoding is developing from self-organization, and how noise is effecting both.</p>
<p>Analyzing the dynamics of a <italic>large</italic> and, most importantly, <italic>input-driven</italic> neural system shaped by biologically-relevant <italic>plasticity</italic> is a hard task due to several methodological constraints. First, most analysis tools from <italic>dynamical systems theory</italic> are confined to small dynamical systems with very few degrees of freedom <xref ref-type="bibr" rid="pcbi.1003512-Strogatz1">[34]</xref>. Exceptions are studies that circumvent this limitation by focusing on the low-dimensional collective dynamics of neural networks, e.g., <xref ref-type="bibr" rid="pcbi.1003512-Brunel1">[35]</xref>, or studies that probe the high-dimensional phase space of the neural network, such as the classic example of Hopfield Networks <xref ref-type="bibr" rid="pcbi.1003512-Hopfield1">[36]</xref>. Other instances of high-dimensional dynamical systems include ring networks and their coexisting periodic attractors <xref ref-type="bibr" rid="pcbi.1003512-Pasemann1">[37]</xref>, stable heteroclinic orbits <xref ref-type="bibr" rid="pcbi.1003512-Rabinovich1">[38]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Rabinovich2">[39]</xref>, unstable periodic attractors <xref ref-type="bibr" rid="pcbi.1003512-Timme1">[40]</xref>, and others <xref ref-type="bibr" rid="pcbi.1003512-Skarda1">[41]</xref>–<xref ref-type="bibr" rid="pcbi.1003512-Markovi1">[43]</xref>.</p>
<p>The second and most important methodological constraint is that the use of standard dynamical systems theory is inappropriate, since it deals with autonomous systems only, i.e. systems with no explicit dependence on time. In reality, however, neural networks are subject to a flux of ever changing stimulation that renders them nonautonomous. A theory of <italic>nonautonomous dynamical systems</italic> is only recently taking shape as a branch of applied mathematics <xref ref-type="bibr" rid="pcbi.1003512-Kloeden1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Kloeden2">[32]</xref>. The fields of neural computation and computational biology are constantly contributing to the theory with concepts such as meta-transients and attractor morphing <xref ref-type="bibr" rid="pcbi.1003512-Pasemann1">[37]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Negrello1">[44]</xref>, <italic>γ</italic>-systems <xref ref-type="bibr" rid="pcbi.1003512-Pascanu1">[45]</xref>, and the nonautonomous dynamics of echo state networks <xref ref-type="bibr" rid="pcbi.1003512-Manjunath1">[46]</xref>.</p>
<p>A simple intuition of the difference between nonautonomous and autonomous systems can be stated as follows. Attractors of an autonomous dynamical system are defined by the system alone, and are therefore fixed. In contrast, attractors of a nonautonomous system are jointly defined by the dynamical system and its input. As the input changes, so does the attractor landscape of the system. This highlights the fact that studying computations in a driven system using the methods of autonomous dynamical systems is insufficient, since the input-induced changes of the system, i.e. changes of its attractor landscape, are ignored in that case.</p>
<p>The third constraint is that the complexity of the dynamics increases due to the neural system's adaptability. The presence of plasticity imposes restrictions on the dynamics a network can exhibit, thus keeping the network dynamics in a regime that can support complex computations. To the best of our knowledge, no attempt prior to this work has been taken to combine high-dimensionality and nonautonomy with the consequences of plasticity on dynamics. We demonstrate that plasticity <italic>sculptures</italic> the stimulus-specific dynamic landscapes, and by that, serves in improving representation of the provided input. Moreover, neuronal plasticity can adapt and learn stimulus-induced sequences of such stimulus-specific landscapes. We thereby show that neuronal plasticity improves spatiotemporal computations.</p>
<p>Given the above, we highlight and explain that spatiotemporal computations require two basic ingredients: a homeostatic mechanism that regulates neuronal activity, and synaptic learning that adapts the network's recurrent connectivity to the stimulus. We show that combining both types leads to a system that: first, learns the temporal structure of the input and carries out nonlinear computations, second, is noise tolerant, and third, even benefits from the presence of noise that sets the system to an input-sensitive dynamic regime.</p>
<p>The paper is structured as follows. We first characterize the effects of self-organized adaptation that is based on synaptic and homeostatic intrinsic plasticity and their combination. For that, we use tasks where both random and temporally-structured inputs are reconstructed and predicted, as well as a task where nonlinear computations are performed. We estimate the network's self-information capacity (its <italic>entropy</italic>), and its input-information capacity (the <italic>mutual information</italic> between the input and the network). We then interlude to qualitatively analyze the resulting dynamics of plastic changes based on the theory of nonautonomous dynamical systems. We explain the superior computation of conjoining synaptic and intrinsic plasticity based on both the informational and dynamical analyses. Building upon that, we study network noise, and demonstrate how noise is combated and exploited through the interaction of synaptic and intrinsic plasticity.</p>
</sec><sec id="s2">
<title>Results</title>
<p>In this section, we guide the reader through the following topics. We start by elucidating the computational power gained through the combination of synaptic and homeostatic plasticity mechanisms on recurrent neural networks of the <italic>k-Winner-Take-All</italic> (<monospace>kWTA</monospace>) type. We investigate the role of these plasticity forms in shaping the neural code through their effects on the informational and dynamical landscapes of the network. We conclude by illustrating how synaptically and homeostatically organized recurrent networks both benefit from noise and tolerate its presence. <xref ref-type="fig" rid="pcbi-1003512-g001">Figure 1</xref> schematically illustrates the network model, the plasticity rules, and the formal probes we used to evaluate and describe the resulting computational properties. More details are available in the <xref ref-type="sec" rid="s4">Methods</xref> section.</p>
<fig id="pcbi-1003512-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003512.g001</object-id><label>Figure 1</label><caption>
<title>Overview of the recurrent network model and the methods for analyzing its computational capabilities.</title>
<p>(A) An exemplary recurrent neural network of 12 neurons. The network state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e001" xlink:type="simple"/></inline-formula> has a 4-Winner-Take-All (<monospace>4WTA</monospace>) nonlinear dynamics, where the 4 neurons with the highest membrane potential fire and the rest are silent. The membrane potential is the sum of the recurrent afferents and the external drive <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e002" xlink:type="simple"/></inline-formula>. It is also depolarized (hyperpolarized) with decreasing (increasing) excitability threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e003" xlink:type="simple"/></inline-formula>. The recurrent network can also be subject to <italic>noise</italic>, while reserving the <monospace>4WTA</monospace> dynamics: when a neuron fails to spike due to noise, another fires instead. (B) The recurrent network is adapted by two plasticity mechanisms. The excitability threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e004" xlink:type="simple"/></inline-formula> is modulated by <bold><italic>i</italic></bold><italic>ntrinsic </italic><bold><italic>p</italic></bold><italic>lasticity</italic> (<monospace>IP</monospace>), the recurrent afferents <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e005" xlink:type="simple"/></inline-formula> by <bold><italic>s</italic></bold><italic>pike-</italic><bold><italic>t</italic></bold><italic>iming-</italic><bold><italic>d</italic></bold><italic>ependent synaptic </italic><bold><italic>p</italic></bold><italic>lasticity</italic> (<monospace>STDP</monospace>). (C) The external drive <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e006" xlink:type="simple"/></inline-formula> consists of discrete symbols that follow a certain stochastic dynamics, and each projects to a corresponding <bold><italic>r</italic></bold><italic>eceptive </italic><bold><italic>f</italic></bold><italic>ield</italic> (RF). The exemplary drive is a 3-symbols Markov chain <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e007" xlink:type="simple"/></inline-formula> that allows a probability for <italic>noisy</italic> transitions, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e008" xlink:type="simple"/></inline-formula>. (D) Linear functions of the network state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e009" xlink:type="simple"/></inline-formula> parametrized by output weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e010" xlink:type="simple"/></inline-formula> fitted to (possibly nonlinear) target functions of sequences of the external drive. (E) Nonlinear information-theoretic quantities are measured: network state entropy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e011" xlink:type="simple"/></inline-formula> and the mutual information <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e012" xlink:type="simple"/></inline-formula> of the network state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e013" xlink:type="simple"/></inline-formula> and input sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e014" xlink:type="simple"/></inline-formula>. (F) Analysis of the appearance and disappearance of attractors due to the external drive within the network as an input-driven dynamical system.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003512.g001" position="float" xlink:type="simple"/></fig><sec id="s2a">
<title>Computational Power</title>
<p>The interaction of different forms of plasticity produces a rather complex emergent behavior that cannot be explained trivially by the individual operation of each. We therefore start with exploring the effects induced by the combination of <bold><italic>s</italic></bold><italic>pike-</italic><bold><italic>t</italic></bold><italic>iming-</italic><bold><italic>d</italic></bold><italic>ependent synaptic </italic><bold><italic>p</italic></bold><italic>lasticity</italic> (<monospace>STDP</monospace>) and <bold><italic>i</italic></bold><italic>ntrinsic </italic><bold><italic>p</italic></bold><italic>lasticity</italic> (<monospace>IP</monospace>). We compare the computational performance of recurrent networks trained either with both synaptic and intrinsic plasticity (<monospace>SIP-RN</monospace>s), with synaptic plasticity alone (<monospace>SP-RN</monospace>s), or with intrinsic plasticity alone (<monospace>IP-RN</monospace>s), in addition to nonplastic recurrent networks, where the synaptic efficacies and firing thresholds are random.</p>
<p>Following the <italic>plasticity phase</italic>, a network is <italic>reset</italic> to random initial conditions and the <italic>training phase</italic> starts. Output weights from the recurrent network to linear readouts are computed with linear regression so that the readouts activity is the optimal linear classifier of a target signal. The target signal depends on the computational task. That is followed by the <italic>testing phase</italic>, at which performance is computed. Performance is measured by the percentage of correctly matched readout activity to the target signal.</p>
<p>Naturally, during simulation, the recurrent network is excited by a task-dependent external drive. The battery of tasks we deployed was designed to <italic>abstract</italic> a certain aspect of the spatiotemporal computations faced by biological brains, i.e. recalling past stimuli, predicting future ones, and nonlinearly transforming them. The memory task <monospace>RAND x 4</monospace>, the prediction task <monospace>Markov-85</monospace>, and the nonlinear task <monospace>Parity-3</monospace>, as well as the plasticity models and simulation conditions, are detailed in the <xref ref-type="sec" rid="s4">Methods</xref> section.</p>
<p><xref ref-type="fig" rid="pcbi-1003512-g002">Figure 2</xref> shows that <monospace>SIP-RN</monospace>s significantly outperform both <monospace>IP-RN</monospace>s and <monospace>SP-RN</monospace>s in all tasks. Inputs from 3 time steps in the past are successfully retained far beyond chance level in the memory task <monospace>RAND x 4</monospace> (<xref ref-type="fig" rid="pcbi-1003512-g002">Figure 2A</xref>). Understandably, performance drops to chance level for future stimuli (positive time-lags), since input symbols are equiprobable and their temporal succession carries no structure. Such is the case for the nonlinear task (<xref ref-type="fig" rid="pcbi-1003512-g002">Figure 2C</xref>). It is worth noting that solving the nonlinear task <monospace>Parity-3</monospace> requires recalling three successive stimuli, which adds to the computational load. The recurrent network, through learning the temporally-structured input of the task <monospace>Markov-85</monospace>, boosts the readouts' ability to reconstruct past symbols in comparison to the structureless memory task <monospace>RAND x 4</monospace>. It also allows for the prediction of future stimuli far beyond chance (<xref ref-type="fig" rid="pcbi-1003512-g002">Figure 2B</xref>).</p>
<fig id="pcbi-1003512-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003512.g002</object-id><label>Figure 2</label><caption>
<title>Average classification performance.</title>
<p>100 networks are trained by <monospace>STDP</monospace> and IP simultaneously (orange), IP alone (blue), <monospace>STDP</monospace> alone (green), or are nonplastic (gray). Optimal linear classifiers are then trained to perform (A) the memory task <monospace>RAND x 4</monospace>, (B) the prediction task <monospace>Markov-85</monospace>, and (C) the nonlinear task <monospace>Parity-3</monospace>. Nonplastic networks have their weights trained by <monospace>STDP</monospace> and then randomly shuffled, so that they have the same weight and threshold distributions as <monospace>SP-RN</monospace>s. However, due to the shuffling, their weight matrices carry no structure. Error bars indicate standard error of the mean. The red line marks chance level. The <italic>x</italic>-axis shows the input time-lag. Negative time-lags indicate the past, and positive ones, the future.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003512.g002" position="float" xlink:type="simple"/></fig>
<fig id="pcbi-1003512-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003512.g003</object-id><label>Figure 3</label><caption>
<title>Network state entropy and the mutual information with input.</title>
<p>(A) Network state entropy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e020" xlink:type="simple"/></inline-formula> and (B) the mutual information with the three most recent <monospace>RAND x 4</monospace> inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e021" xlink:type="simple"/></inline-formula> as they develop through the plasticity phase for <monospace>SP-RN</monospace>s (green), <monospace>IP-RN</monospace>s (blue), and <monospace>SIP-RN</monospace>s (orange). Mutual information for <monospace>IP-RN</monospace>s is estimated from 500000 time steps, and is averaged over 5 networks only. Other values are averaged over 50 networks and estimated from 100000 samples for each network. Error bars indicate standard error of the mean.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003512.g003" position="float" xlink:type="simple"/></fig>
<p><monospace>STDP</monospace> alone fails to provide the recurrent network with means to encode necessary information. This leads to <monospace>SP-RN</monospace>s performing at almost chance level in all tasks. Intrinsic plasticity, on the other hand, endues recurrent networks with an intermediate ability to sustain past inputs (<xref ref-type="fig" rid="pcbi-1003512-g002">Figure 2A</xref>). <monospace>IP-RN</monospace>s also seem to learn the temporal structure of the input, as optimal linear classifiers are capable of predicting future stimuli (<xref ref-type="fig" rid="pcbi-1003512-g002">Figure 2B</xref>). Intrinsic plasticity is, however, insufficient for nonlinear computations, as <monospace>IP-RN</monospace>s barely perform above chance in the nonlinear parity task.</p>
<p>We also compare the performance of nonplastic <monospace>kWTA</monospace> networks with similar weight and threshold distributions as <monospace>SP-RN</monospace>s (shown in gray in <xref ref-type="fig" rid="pcbi-1003512-g002">Figure 2</xref>). They perform better than <monospace>IP-RN</monospace>s on the memory and nonlinear tasks, and worse on the prediction task. In all tasks, these nonplastic networks perform worse than <monospace>SIP-RN</monospace>s. We also show in <xref ref-type="supplementary-material" rid="pcbi.1003512.s005">Text S1</xref> that nonplastic networks with comparable weight and threshold distributions as <monospace>SIP-RN</monospace>s also perform significantly lower than plastic networks. These results supply the evidence that the presence of plasticity enhances the computational power of recurrent neural networks (see <xref ref-type="supplementary-material" rid="pcbi.1003512.s005">Text S1</xref> for a discussion on heuristics for finding comparable random networks). No further analysis is carried out on these nonplastic networks, since the aim of this paper is to discern the effects of synaptic and intrinsic plasticity on spatiotemporal computations.</p>
</sec><sec id="s2b">
<title>Neural Code</title>
<p>Explaining the superiority of networks modified by deploying both <monospace>STDP</monospace> and <monospace>IP</monospace> starts from isolating the individual role of each plasticity mechanism in defining the spatiotemporal neural code. In that regard, a well-informed intuition is that <monospace>STDP</monospace> learns the basic structure of the input as the connectivity resulting from <monospace>STDP</monospace> reflects the input sequence transitions. <monospace>IP</monospace>, on the other hand, increases the neural bandwidth by introducing redundancy to the code, as <monospace>IP</monospace> leads to the longest periodic cycles in the spontaneous activity of <monospace>kWTA</monospace> networks (See Figure 8 and Figure 4A in <xref ref-type="bibr" rid="pcbi.1003512-Lazar1">[27]</xref>).</p>
<fig id="pcbi-1003512-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003512.g004</object-id><label>Figure 4</label><caption>
<title>Post-plasticity perturbation.</title>
<p>100 networks are trained by <monospace>STDP</monospace> and <monospace>IP</monospace> simultaneously on (A) the memory task <monospace>RAND x 4</monospace>, (B) the prediction task <monospace>Markov-85</monospace>, and (C) the nonlinear task <monospace>Parity-3</monospace> with increasing perturbation level: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e025" xlink:type="simple"/></inline-formula> (yellow), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e026" xlink:type="simple"/></inline-formula> (orange), and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e027" xlink:type="simple"/></inline-formula> (red). Error bars indicate standard error of the mean. The red line marks chance level. The <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e028" xlink:type="simple"/></inline-formula>-axis shows the input time-lag. Negative time-lags indicate the past, and positive ones, the future. (D) Network state entropy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e029" xlink:type="simple"/></inline-formula> and (E) the mutual information with the three most recent <monospace>RAND x 4</monospace> inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e030" xlink:type="simple"/></inline-formula> at the end of the plasticity phase for different perturbation levels. Values are averaged over 50 networks and estimated from 5000 samples for each network. Error bars indicate standard error of the mean.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003512.g004" position="float" xlink:type="simple"/></fig>
<p>The spatiotemporal neural code, or the <italic>neural code</italic> for short, can be characterized by both the <italic>absolute capacity</italic> of the network activity to store information and by how network activity <italic>encodes</italic> the spatially and temporally extended network input. <italic>Entropy</italic> of the network activity measures its absolute capacity, i.e. the repertoire of network states that the network can actually visit and potentially assign to some input sequence. The assignment of a network state to an input sequence means that this particular network state <italic>encodes</italic> or <italic>represents</italic> that input sequence. <italic>Mutual information</italic> between network input sequences and network states quantifies the extent of how successful this assignment is. Not every visited network state needs be assigned an input sequence. A redundant code is reflected by input sequences being represented by multiple network states. Also, a network state might fail to encode an input, thus reflecting uninformative noise states.</p>
<p>We investigate the neural code characteristics of <monospace>kWTA</monospace> networks by estimating both the entropy of the network state and the mutual information between network input sequences and network states. We drive the network by <monospace>RAND x 4</monospace> input, and for computational tractability, we limit the estimation of mutual information to three-step inputs. An optimal encoder of this input sequence will then be a network with 6 bits of mutual information. The information-theoretical quantities are computed at intervals of the plasticity phase under the three plasticity conditions. At these intervals, the plastic variables are fixed and the driven network is reinitialized and run for a sufficient number of steps, and passed along with the input to the entropy and mutual information estimators. More details on how these measurements are carried out are found in the <xref ref-type="sec" rid="s4">Methods</xref> section.</p>
<p><xref ref-type="fig" rid="pcbi-1003512-g003">Figure 3</xref> shows how these measures develop through the plasticity phase (For a discussion on the effects of longer plasticity exposure, see <xref ref-type="supplementary-material" rid="pcbi.1003512.s006">Text S2</xref>). <monospace>SP-RN</monospace>s' entropy remains constant at 2 bits. This means that <monospace>SP-RN</monospace>s visit only 4 network states (green in <xref ref-type="fig" rid="pcbi-1003512-g003">Figure 3A</xref>). However, these network states encode no information of the input sequence, as mutual information remains practically zero (green in <xref ref-type="fig" rid="pcbi-1003512-g003">Figure 3B</xref>). We call this 2 bits input-insensitive code the <italic>minimal code</italic>, as it captures no more than a single possible succession of the 4 inputs. This effect is the result of the interaction between the machination of <monospace>STDP</monospace> and the initial firing thresholds and weights configuration. Transitions, such as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e015" xlink:type="simple"/></inline-formula> in the input space, are to be stored in some of the synapses that connect neurons in the receptive field of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e016" xlink:type="simple"/></inline-formula> with those in the receptive field of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e017" xlink:type="simple"/></inline-formula>. At each time step, one transition, such as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e018" xlink:type="simple"/></inline-formula>, could be easier to reinforce with the causal (potentiating) side of <monospace>STDP</monospace> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e019" xlink:type="simple"/></inline-formula> neurons having little higher excitability (internal drive plus their own firing threshold). Without <monospace>IP</monospace> to tune down this excitability and with further contribution from the recurrency of the network, a positive feedback loop is generated, and this transition becomes more and more potentiated at the expense of others. This transition then becomes independent of the actual drive the network is receiving: the network becomes input-insensitive.</p>
<p>On the other side of the entropy spectrum, we find <monospace>IP-RN</monospace>s. Through <monospace>IP</monospace>'s constant adjustment of the neuronal excitability, many neurons contribute to the neural code and <monospace>IP-RN</monospace>s visit a large number of states. Entropy and the network state bandwidth are the highest (blue in <xref ref-type="fig" rid="pcbi-1003512-g003">Figure 3A</xref>). One may view <monospace>IP</monospace>'s effect as an introduction of <italic>intrinsic deterministic noise</italic> to the network activity. The increase in bandwidth of the network activity raises the odds for the random weights of an <monospace>IP-RN</monospace> to store an input sequence. In fact, many network states encode the same input sequence, resulting in a redundant code. However, without a synaptic reinforcement of representations, many states are visited due to the internal dynamics of the network, and not due to the external drive. These states remain uninformative and input sequences not successfully encoded: the mutual information (blue in <xref ref-type="fig" rid="pcbi-1003512-g003">Figure 3B</xref>), and hence the classification performance, are low.</p>
<p>The development of the neural code for <monospace>SIP-RN</monospace>s follows, however, a more interesting path. At the beginning, <monospace>STDP</monospace> has the upper hand and a 2 bits minimal code is generated. Through providing intrinsic deterministic noise, <monospace>IP</monospace> enriches the neural code by increasing redundancy and entropy (orange in <xref ref-type="fig" rid="pcbi-1003512-g003">Figure 3A</xref>). At the same time, <monospace>STDP</monospace> incrementally associates different network states to different input sequences by adjusting the synaptic weights as seen from the increase of mutual information (orange in <xref ref-type="fig" rid="pcbi-1003512-g003">Figure 3B</xref>). Then together, synaptic and homeostatic plasticity cooperate to create a code that is both <italic>redundant</italic> and <italic>input-specific</italic>. These properties are crucial for noise-robustness, as will be shown later in this text.</p>
</sec><sec id="s2c">
<title>Post-Plasticity Perturbation</title>
<p>A dynamical system's behavior depends on its past activity. Therefore, testing a system requires assuming plausible initial conditions. The recurrent neural network at hand, even though it is small in comparison to a real neural circuit, has a number of possible initial conditions too large for all its initial conditions to be tested. So far, we have chosen random initial conditions for the network activity following the plasticity phase. From now on, we choose the initial conditions systematically by reinitializing the network activity depending on a <italic>perturbation</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e022" xlink:type="simple"/></inline-formula>. This perturbation is applied to the end state of the plasticity phase, such that the end state of the plasticity phase and the initial state of the training phase are at a distance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e023" xlink:type="simple"/></inline-formula> from one another. For details of how the initial conditions are selected depending on the parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e024" xlink:type="simple"/></inline-formula>, we refer the reader to the <xref ref-type="sec" rid="s4">Methods</xref> section.</p>
<p>To discern the effect of this perturbation, we compute the performance of the trained system with the three combinations of synaptic and intrinsic plasticity. We do this both for a system that is perturbed and for a system that starts from the last state that the dynamics reaches at the end of the preceding plasticity phase. We find no difference between the two cases of initial conditions for either <monospace>IP-RN</monospace>s or <monospace>SP-RN</monospace>s. However, when the neural network is trained by both synaptic and intrinsic plasticity (<monospace>SIP-RN</monospace>s), we find that the perturbed networks have better performance, as is illustrated in <xref ref-type="fig" rid="pcbi-1003512-g004">Figure 4A–C</xref>. The high performance of <monospace>SIP-RN</monospace>s that results from random initial conditions, as is shown in <xref ref-type="fig" rid="pcbi-1003512-g002">Figure 2</xref>, is easily explainable. It stems from the fact that random initialization is merely a large perturbation, since the probability of choosing a random state from such a large set of possibilities that is at a small distance from a particular region of the state space is insignificant, compared to a state that is at a large distance. Moreover, we find that regardless of the task, larger perturbations result in higher average performance. This is also reflected in the neural code, where network state entropy and the mutual information with input correlate with higher perturbation (see <xref ref-type="fig" rid="pcbi-1003512-g004">Figure 4D–E</xref>).</p>
<p>This suggests that within the phase space of <monospace>SIP-RN</monospace>s there exist at least two dynamic regimes. Post-plasticity perturbation also provides the first sign of how <monospace>SIP-RN</monospace>s can benefit from noise, as it might put the system in the regime more suitable for computation.</p>
</sec><sec id="s2d">
<title>Dynamic Regimes</title>
<p>Optimal linear classifiers show that <monospace>kWTA</monospace> networks equipped with both homeostatic and synaptic plasticity are capable of creating spatiotemporal codes and performing nonlinear computation. Measuring entropy and mutual information allows for a quantification of the emerging neural code. But what are the geometric features of the neural code that allow for such computations? How do network states <italic>represent</italic> the spatiotemporal input in a useful way? A major part of the <xref ref-type="sec" rid="s4">Methods</xref> section is devoted to developing the mathematical formalization of discrete-time nonautonomous dynamical systems. References to definitions, a proposition, and a theorem from that section are featured in the following results, as we apply these concepts to our model neural network. We view this treatment not merely as an exercise in mathematics. It allows for a rigorous description of the computational properties emerging from plasticity that are beyond the scrutiny of quantitative measures, such as linear classification performance and carried information. A consequence of these properties is also the two noise-related features we examine later.</p>
<p>For a formal treatment of spatiotemporal computations which result from plasticity, we need to extend the theory of nonautonomous dynamical systems to provide a notion for representations, to specify how these representations allow for computations, and to discern the effect of plasticity in enhancing these representations for the sake of computation. But first, we start by identifying the modes of operation, i.e. the dynamic regimes, the model plastic neural network has, since not all regimes might be suitable for computation.</p>
<p>According to Proposition 3 and Definition 6, when subject to stimulation, <monospace>kWTA</monospace> networks are <italic>input-driven discrete-time dynamical systems</italic>. For such systems, two extremes exist regarding the degree of sensitivity the system exhibits in response to its input. At one extreme, the system shows no change of response for different inputs, so that it follows its own dynamics, as if no input exists. In such a mode of operation, the system is <italic>input-insensitive</italic>. The other extreme is when the system's response is different for each input and initial condition. A single system can show, in principle, multiple modes of operation, depending on the initial conditions. The set of initial conditions that show a single mode of operation defines a dynamic regime and a basin of attraction.</p>
<p>In a first step, we visualize the high-dimensional response of the system to its input. To that end, we down-project the network activity to the first three principal components, and we study the effects of <monospace>STDP</monospace> and <monospace>IP</monospace> on the network's dynamics and input representations in this reduced 3-dimensional space (<xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5</xref>). This analysis is performed on networks with <monospace>Markov-85</monospace> input which fully demonstrate the relevant properties. It is important to note that while our analysis concerns the dynamics following the plasticity phase, we are still able to infer how it unfolds during this phase from the development of the neural code (<xref ref-type="fig" rid="pcbi-1003512-g003">Figure 3</xref>), as we make clear later.</p>
<fig id="pcbi-1003512-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003512.g005</object-id><label>Figure 5</label><caption>
<title>Plasticity effects on networks dynamics and input representations under the prediction task input.</title>
<p>The three dimensions correspond to the first three principal components (PCs) of the network activity. (A) Highly-overlapping order-1 volumes of representation of an <monospace>IP-RN</monospace>. (B) Input-insensitive global attractor of a <monospace>SP-RN</monospace> that corresponds to a minimal code. (C) With no perturbed (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e031" xlink:type="simple"/></inline-formula>), a <monospace>SIP-RN</monospace> dynamics also converges to an input-insensitive attractor and exhibits a minimal code. (D) Approximate visualization of order-1 volumes of representation of a <monospace>SIP-RN</monospace>. The approximation uses the means and the standard deviations of the corresponding coordinates of the network activity in the principal components space as the center and semi-axes lengths of ellipsoids. Arrows correspond to the transitions from one input symbol to the other. Their thickness symbolizes the probability of a transition, which reflects the <monospace>Markov-85</monospace> transition probability. The collection of volumes of representation and the arrows show the perturbation set within which the nonautonomous attractor resides. (E) Order-2 volumes of representation of a <monospace>SIP-RN</monospace> also approximated using the mean and standard deviations of coordinates. Order-2 volumes are more exact approximations to the order-1 representations according to the volumes' inclusion property. The correspondence is clarified by using similar color coding. (F) Autonomous periodic attractors of a <monospace>SIP-RN</monospace>, each belonging to one of the autonomous semi-dynamical systems associated with one <monospace>Markov-85</monospace> input. For clarity, no arrows are drawn between the vertexes of an attractor.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003512.g005" position="float" xlink:type="simple"/></fig>
<p>As suggested by the performance of <monospace>SP-RN</monospace>s (<xref ref-type="fig" rid="pcbi-1003512-g002">Figure 2</xref>) and their neural code entropy and mutual information (<xref ref-type="fig" rid="pcbi-1003512-g003">Figure 3</xref>), their state space is dominated by an <italic>input-insensitive</italic> basin of attraction and these networks behave like autonomous semi-dynamical systems (prefixing with “semi” refers to the fact that the dynamics needs not be invertible). This is confirmed by the asymptotic dynamics of <monospace>SP-RN</monospace>s, which is independent of the input (<xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5B</xref>). The dynamics within this dynamic regime follows the minimal code. The minimal code manifests itself through a period-4 periodic attractor which corresponds, in the case of <monospace>Markov-85</monospace> input, to the most probable transition in the input space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e032" xlink:type="simple"/></inline-formula>. This observation confirms the fact that <monospace>STDP</monospace> allows the system to learn the basic structure of its input.</p>
<p><monospace>SIP-RN</monospace>s exhibits similar dynamics at the end of the plasticity phase (<xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5C</xref>). However, as is evident from varying the perturbation parameter for <monospace>SIP-RN</monospace>s (<xref ref-type="fig" rid="pcbi-1003512-g004">Figure 4</xref>), the set of initial conditions that constitutes this input-insensitive basin is confined by a distance relation to the neighborhood of the periodic attractor: the probability of being in this basin diminishes the further away the initial conditions are from the input-insensitive periodic attractor.</p>
<p>The increase of performance and the neural bandwidth of <monospace>SIP-RN</monospace>s for higher <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e033" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003512-g004">Figure 4</xref>) shows that outside of the <italic>input-insensitive</italic> dynamic regime there exists a different basin of attraction. Within this basin, the network is sensitive to input, and computations are possible. The observation that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e034" xlink:type="simple"/></inline-formula> has no effect on <monospace>IP-RN</monospace>s and that they show intermediate performance and mutual information suggests that they are dominated by a dynamic regime with intermediate input-sensitivity. It also confirms that intrinsic plasticity is responsible for the emergence of the <italic>input-sensitive</italic> dynamic regime in <monospace>SIP-RN</monospace>s.</p>
</sec><sec id="s2e">
<title>Volumes of Representation</title>
<p>Now that the dynamic regimes of trained networks with the three combinations of synaptic and intrinsic plasticity are identified, we next move to formulating the notion of representations inside the input-sensitive dynamic regime. Developing such a notion allows linking the theory of nonautonomous dynamical systems to a theory of spatiotemporal computations. To this purpose, we coin the term <italic>volumes of representation</italic>, which is a concept that describes the response of a nonautonomous dynamical system in respect to its drive. The volume of representation of some input sequence within some dynamic regime is the set of network states that are <italic>accessible</italic> through exciting the network with the corresponding input sequence, starting from all network states in this dynamic regime as initial conditions (Definition 10). The <italic>order</italic> of a volume is defined by the length of the input sequence it represents. We also introduce the <italic>volumes' inclusion property</italic> which hierarchically links the system's response to spatiotemporal input sequences to their sub-sequences.</p>
<p>To visualize a network's volumes of representation, we sample the network's response. We do this because the size of the state space and the input-sensitive dynamic regime is too large, making a complete coverage impossible. Also, since volumes of representation can have complicated shapes in both the full and reduced state space, we approximate these volumes with ellipsoids.</p>
<p><xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5D</xref> provides such an approximation to the volumes of representation of order-1. The sample is a single sequence of 10000 <monospace>Markov-85</monospace> inputs to a <monospace>SIP-RN</monospace>. Each volume is replaced by an ellipsoid. The center of this ellipsoid is the coordinates' average of the visited network states in the principal components space. Each of its semi-axes has a length that is the standard deviation from the mean of the corresponding coordinate. Also, according to the <italic>volumes' inclusion property</italic>, stated formally in the <xref ref-type="sec" rid="s4">Methods</xref> section, a volume of representation of order-1 of some input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e035" xlink:type="simple"/></inline-formula> includes all volumes of order-2 for sequences whose most recent input is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e036" xlink:type="simple"/></inline-formula>. As such, <xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5E</xref>, that depicts a similar approximation to all volumes of order-2, is also a better approximation to volumes of order-1. In <xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5E</xref>, each order-1 volume consists of four order-2 volumes that are color-coded to match the rougher approximation in <xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5D</xref>. In a supporting figure, we further show that this way of presentation is sufficient, compared to using percentiles of bootstrapped network states (see <xref ref-type="supplementary-material" rid="pcbi.1003512.s001">Figure S1</xref>).</p>
<p>The volumes of representation provide a geometric view of spatiotemporal computations as the ability of the recurrent neural network to <italic>represent</italic> in its activity, in other words to <italic>encode</italic>, useful functions of the network's input sequences, and for these representations to be distinguishable and reliable. In the case of the tasks <monospace>RAND x 4</monospace> and <monospace>Markov-85</monospace>, the functions that the network activity represents are the identity, delayed or forecast. As shown in <xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5D–E</xref>, the volumes of representation of <monospace>SIP-RN</monospace>s under <monospace>Markov-85</monospace> input exhibit higher <italic>separability</italic>, which explains both their high classification performance and high mutual information. One also notices that the volumes of representation of order-2 that belong to the most probable transitions in the <monospace>Markov-85</monospace> input, e.g., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e037" xlink:type="simple"/></inline-formula>, are also the most distant from one another (<xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5E</xref>). This results in the most probable transitions to be more easily distinguishable by optimal linear classifiers.</p>
<p>In order to isolate the roles of synaptic and intrinsic plasticity in generating useful representations, we show in <xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5A</xref> the order-1 volumes of representation of an <monospace>IP-RN</monospace> in response to <monospace>Markov-85</monospace> input. Compared to the <monospace>SIP-RN</monospace>, these volumes are highly overlapping, which explains the lower classification performance. Also, the low mutual information between the network state and the input (<xref ref-type="fig" rid="pcbi-1003512-g003">Figure 3</xref>) can now be explained by various network states belonging to multiple volumes of representation, at once. Also, many network states represent the same single input which is a signature of <italic>redundancy</italic> resulting from <monospace>IP</monospace>. These observations point towards <monospace>STDP</monospace> being the source of separability of representations in <monospace>SIP-RN</monospace>s, in addition to learning the structure of the input through situating the representations of the input's most probable transitions at further distances from one another.</p>
<p>In the case of the task <monospace>Parity-3</monospace>, the function that the network activity needs to represent is the sequential <italic>exclusive or</italic> operation over three successive binary inputs. As such, within the input-sensitive dynamic regime, two volumes of representation exists, each encodes one outcome of the nonlinear task <monospace>Parity-3</monospace>. According to Definition 10, these volumes are formed from an appropriate union of order-3 volumes of representation of the binary input. We provide an illustration of these two volumes of representation in <xref ref-type="supplementary-material" rid="pcbi.1003512.s002">Figure S2</xref>. Here also, <monospace>STDP</monospace> provides the separability that allows these representations to be distinguishable, while <monospace>IP</monospace> gives the possibility of an input-sensitive and redundant regime to emerge, and, aided by <monospace>STDP</monospace>, for the volumes of representation to expand.</p>
</sec><sec id="s2f">
<title>Attractor Landscape</title>
<p>The presence of dynamic regimes entails the existence of <italic>attractors</italic>, i.e. limit sets of the dynamics, that apply a pulling force on the dynamical system's activity and dictate its course of flow. In an input-driven dynamical system, attractors are not easily defined as sets of states. Instead, <italic>nonautonomous attractors</italic> are input-dependent moving targets of the dynamics, which adds a temporal aspect to their definition (see Definition 8). As follows, for our nonautonomous dynamical systems theory of spatiotemporal computations to be complete, we link the geometry of the computational entities, i.e. the volumes of representation, to the geometry of the nonautonomous attractors. This allows us to connect the features of the volumes of representation emerging from plasticity, namely, separability and redundancy, to the effects of plasticity on the nonautonomous attractor. To that end, starting from the volumes of representations, we define the <italic>perturbation set</italic> (Definition 10) as a moving source of the neural activity towards its moving target, the nonautonomous attractor. Since the perturbation set changes with time, it is called a <italic>nonautonomous set</italic> (Definition 7). This also applies to nonautonomous attractors. The set of states constituting a nonautonomous set at a fixed time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e038" xlink:type="simple"/></inline-formula> is called the set's <italic>t-fiber</italic>. We later show how the t-fibers of these nonautonomous sets relate to each other.</p>
<p>In the input-insensitive dynamic regime, the dynamical system behaves as an autonomous dynamical system, and so does its attractor, which is the period-4 attractor in <xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5B–C</xref>. In addition, the existence of a <italic>nonautonomous basin of attraction</italic> (Definition 9), that constitutes the input-sensitive dynamic regime in <monospace>SIP-RN</monospace>s, necessitates the existence of a <italic>nonautonomous attractor</italic>.</p>
<p>It is not possible to fully identify the nonautonomous attractor by looking into the nonautonomous dynamics. This is because the attractor is not fixed in space and because the dynamics almost never converges to it. However, we prove in Theorem 11.1 that in an input-driven discrete-time dynamical system, and within a basin of attraction, the nonautonomous attractor is a subset of the basin's perturbation set, and that the t-fibers of a nonautonomous attractor are subsets of the t-fibers of the perturbation set. Given this result, the location of the nonautonomous attractor within the state space of the network can be approximated by the perturbation set. The perturbation set summarizes how the network activity passes from one volume of representation to another, at every time step, according to the input's transition statistics. We replace the time dimension in <xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5D</xref> by arrows that correspond to the transitions in <monospace>Markov-85</monospace> input. The volume of representation visited at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e039" xlink:type="simple"/></inline-formula> is the volume corresponding to the input at that time, and it forms the t-fiber of the perturbation set.</p>
<p>Instead of defining the asymptotic dynamics of the model neural network within the input-sensitive basin of attraction by a single nonautonomous attractor with different t-fibers, we can define it by multiple autonomous attractors, each belonging to a particular input. According to Theorem 11.2, within the input-sensitive basin of attraction, there exists for each input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e040" xlink:type="simple"/></inline-formula>, an autonomous attractor (Definition 4) of the autonomous semi-dynamical system defined by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e041" xlink:type="simple"/></inline-formula>. The theorem also shows that this attractor is a subset of the volume of representation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e042" xlink:type="simple"/></inline-formula>. Theorem 11.3 further shows that the basin of attraction of the autonomous attractor is also the input-sensitive basin. Accordingly, the network dynamics undergoes a <italic>bifurcation</italic> at each time step the input changes its identity. A bifurcation is a change in the topological properties of invariant sets, such as attractors. We observe bifurcations in the input-sensitive regime of <monospace>kWTA</monospace> networks. The topological property undergoing the change is the loss of stability of the periodic attractor associated with an input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e043" xlink:type="simple"/></inline-formula>, and the appearance of an attractor with a different period and location that is associated with the input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e044" xlink:type="simple"/></inline-formula>.</p>
<p><xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5F</xref> shows the autonomous periodic attractors associated with each <monospace>Markov-85</monospace> input within the input-sensitive basin of attraction of a <monospace>SIP-RN</monospace>. Each of these attractors is also a t-fiber of the nonautonomous input-sensitive attractor. While these autonomous attractors are depicted in one state space, overlaying them in a single plot serves only in illustrating the geometric relations between them. In reality, these attractors do not <italic>coexist</italic>. Each autonomous attractor appears in the phase space of the network when its associated input drives the network, and the attractor from the previous time step disappears.</p>
<p>The geometry of the nonautonomous attractor within an input-sensitive dynamic regime is very important regarding spatiotemporal computations. In fact, computations are completely defined according to the relative positions of the nonautonomous attractor's t-fibers to one another, and to the volumes of representation. An attractor consists of limit points of a basin of attraction. Thus, it exerts a pulling force on the network states that define the volumes of representation. So, if the t-fibers of a nonautonomous attractor are close to one another in the state space of the network, different volumes will be overlapping and computations will be difficult to carry through. Such is the case in IP-RNs. On the other hand, distant t-fibers of the nonautonomous attractor result in separate volumes of representation and better spatiotemporal computations, which is the case in <monospace>SIP-RN</monospace>s (<xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5D–F</xref>). Also, the number of states comprising the t-fibers of the nonautonomous attractor effects the redundancy of representations. As intrinsic plasticity increases the number of states of these t-fibers, the perturbation set becomes more redundant. Given the above, while the perturbation set contains the nonautonomous attractor, it is the attractor that defines how the perturbation set, and as a consequence the volumes of representation, extends in space.</p>
<p>For a correct characterization of spatiotemporal computations according to the geometry of the nonautonomous attractor and function representations, we borrow the concept of <italic>meta-transients</italic> <xref ref-type="bibr" rid="pcbi.1003512-Negrello1">[44]</xref>. A transient activity of an autonomous (semi-)dynamical system is the trajectory its dynamics follows as it approaches a fixed attractor. Alternatively, an attractor of an input-driven dynamical system changes constantly. This leads the trajectory pursued by the dynamics to switch its course, so as to keep track of its moving target. Such an input-dependent trajectory is termed a meta-transient. When the input changes, the meta-transient passes from one volume of representation to another, i.e. the dynamics bifurcates and the meta-transient approaches the vertexes of the current attractor, while being repelled from the others that are now unstable. It is in this geometric relation to the different attractors (or t-fibers) that computation resides. In fact, as a proof of principle, the autonomous attractors of <monospace>SIP-RN</monospace>s were allocated. This was done by clamping each input for a sufficient time until the dynamics converges to that input's periodic attractor. Then, optimal linear classifiers were fitted to perform the three computational tasks. As training data, the Hamming distances between the meta-transient and the vertexes of these periodic attractors were used. <xref ref-type="supplementary-material" rid="pcbi.1003512.s003">Figure S3</xref> shows the performance resulting from this computational procedure, which outperforms both <monospace>SP-RN</monospace>s and IP-RNs. While the performance is far from what is achieved directly from the activity of <monospace>SIP-RN</monospace>s, especially in the nonlinear task <monospace>Parity-3</monospace>, it is important to note that distance is a very rough compression of the geometric relations between the meta-transient and the autonomous attractors. For instance, distance does not allow the distinction between network states that are symmetrical in relation to the autonomous attractors.</p>
</sec><sec id="s2g">
<title>Emergence of Computation</title>
<p>We now outline how the interaction of homeostatic and synaptic plasticity gives rise to spatiotemporal computations through developing useful representations. To this end, we combine the analysis of dynamic regimes, volumes of representation, and autonomous and nonautonomous attractors (<xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5</xref>) with the informational-theoretic intuitions regarding the evolution of the neural code (<xref ref-type="fig" rid="pcbi-1003512-g003">Figure 3</xref>).</p>
<p>At the beginning of the plasticity phase, <monospace>STDP</monospace> has the upper hand and it generates a minimal code of the input. This is evident from the 2 bits network state entropy (<xref ref-type="fig" rid="pcbi-1003512-g003">Figure 3A</xref>) and the close to zero mutual information with input (<xref ref-type="fig" rid="pcbi-1003512-g003">Figure 3B</xref>) at the beginning of the plasticity phase of <monospace>SIP-RN</monospace>s. The minimal code captures, through an input-insensitive periodic attractor, the most probable transitions in the input (<xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5B</xref>). Another feature of the input-insensitive periodic attractor is the high separability of its vertexes in the state space of the <monospace>SIP-RN</monospace>.</p>
<p>At the same, <monospace>IP</monospace> time succeeds in reducing the excitability thresholds of some neurons, such that more network states become accessible at the vicinity of the vertexes of the input-insensitive attractor: entropy increases alongside the potential for redundancy. <monospace>STDP</monospace> concurrently assigns these network states to the inputs that induce them: mutual information and redundancy increase. This incremental process manifests dynamically in the appearance of the input-sensitive basin of attraction, and the associated appearance and expansion of volumes of representation (<xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5D–E</xref>). Due to the highly separate vertexes of the input-insensitive attractor and the neighborhood relations of the volumes with these vertexes, the volumes of representation are highly separate. This shows that the input-insensitive dynamics is a necessary prerequisite for the emergence of spatiotemporal computations, as it sets the stage for the appearance of separate representations that also carry the structure of the input.</p>
<p>The emerging dynamics can also be viewed through formulating the <monospace>SIP-RN</monospace> during the plasticity phase, as an input-driven dynamical system parametrized by the weights and the excitability thresholds. Through varying the parameters of the system with <monospace>STDP</monospace> and <monospace>IP</monospace>, the dynamics at some point in the parameters space bifurcates from one stable dynamics, the input-insensitive dynamics, to two stable dynamics with the appearance of the input-sensitive attractor in whose basin computations are realizable. This also applies to each member of the family of semi-dynamical systems with the appearance of new dynamics and the associated new periodic attractor (<xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5F</xref>).</p>
</sec><sec id="s2h">
<title>Noise-Robustness</title>
<p>Equipped with different vantage points to describe the information processing properties of plastic recurrent neural networks, we now turn to ask a central question: what does an information processing system like the brain require in order to be noise-robust? We state the following hypothesis. Noise-robustness is an effect of the interplay between 1) a <italic>redundant code</italic> that provides multiple possible encodings of an input, and 2) <italic>separability</italic> of representations which allows for a <italic>margin of noise</italic> without obscuring the identity of the input.</p>
<p>The analysis of the neural code (<xref ref-type="fig" rid="pcbi-1003512-g003">Figure 3</xref>) shows how <monospace>IP</monospace> increases the potential for redundancy by increasing the neuronal bandwidth. <monospace>STDP</monospace> could exploit this potential redundancy by assigning multiple neurons to the same input. Viewing the network dynamics in the principal components space, on the other hand, made clear that <monospace>STDP</monospace> ensures separability in the volumes of representation (<xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5D–E</xref>). This also suggests that the recurrent network should be more robust to noise, the more recent the decoded input is, as the margin of noise becomes smaller for older inputs. The expansion of volumes of representation in <monospace>IP</monospace>-RNs also points towards a higher potential redundancy.</p>
<p>We test the hypothesis and the role of <monospace>STDP</monospace> and <monospace>IP</monospace> interaction in noise-robustness by injecting nondeterministic noise into the recurrent network. Following the plasticity phase, we deploy a certain rate of random bit flips on the network state that reserves the <monospace>kWTA</monospace> dynamics, i.e. if some neuron is silenced due to noise, another neuron is selected at random and it fires instead. Different networks with different input statistics will amplify the same amount of noise to a varying extent. The shaded area in <xref ref-type="fig" rid="pcbi-1003512-g006">Figure 6</xref> marks the ratio-of-noisy-spikes range within the network states of 100 recurrent networks. For all tasks and networks, we measured performance of optimal linear classifiers on both the noise-free and noisy network states, and computed the relative change in performance.</p>
<fig id="pcbi-1003512-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003512.g006</object-id><label>Figure 6</label><caption>
<title>Noise-robustness is achieved through the interaction of synaptic and intrinsic plasticity.</title>
<p>Bootstrapped median relative change from the noiseless performance of 100 networks trained with both <monospace>STDP</monospace> and <monospace>IP</monospace> on (A) the memory task <monospace>RAND x 4</monospace>, (B) the prediction task <monospace>Markov-85</monospace>, and (C) the nonlinear task <monospace>Parity-3</monospace>. High perturbation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e045" xlink:type="simple"/></inline-formula> is applied at the end of the plasticity phase. Error bars correspond to the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e046" xlink:type="simple"/></inline-formula> and the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e047" xlink:type="simple"/></inline-formula> percentiles. Noise level <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e048" xlink:type="simple"/></inline-formula> is the probability of a bit flip in the network state, that is, the probability of one of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e049" xlink:type="simple"/></inline-formula> spiking neurons at time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e050" xlink:type="simple"/></inline-formula> to become silent, while a silent neuron fires instead. The shaded area indicates the ratio of noisy spikes which is measured in comparison to the noiseless <monospace>SIP-RN</monospace>s. The green line indicates the median and the orange lines the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e051" xlink:type="simple"/></inline-formula> and the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e052" xlink:type="simple"/></inline-formula> percentiles of the noisy spikes ratio.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003512.g006" position="float" xlink:type="simple"/></fig>
<p>We compare the change in performance for each time-lag with the ratio of noisy spikes. To understand how this comparison aids in characterizing noise-robustness, we rely on an example. If 10% of a network's spiking activity has been replaced by noise, spikes being the carriers of information, 10% of the information in the network would be lost. However, if the activity of other neurons within the network is a replica of half the lost spikes, only 5% of the information would be lost, and the performance of the linear classifiers would decrease just as much. Having the change of performance below noise level is evidence of noise-robustness due to redundancy and intrinsic plasticity.</p>
<p>Information carried by the network cannot deteriorate beyond the amount of noise; the ability to perform computations, on the other hand, is another story, since distinguishing between representations is a necessary condition for computation. Noise can lead to an overlap in the volumes of representation, which hinders the information processing capability of the recurrent neural network, since overlapping representations are indistinguishable and prone to over-fitting by decoders, linear or otherwise. However, when volumes of representation are well separated due to <monospace>STDP</monospace>, and redundancy is at play, performance will not exceed the amount of noise in the network: noise-robustness is still achieved.</p>
<p><xref ref-type="fig" rid="pcbi-1003512-g006">Figure 6</xref> shows that redundancy and separability are assuring noise-robustness in the three tasks. The effects are the strongest for the task <monospace>RAND x 4</monospace>. The change of performance never exceeds the range of noise for all time-lags. The change of performance on the task <monospace>Markov-85</monospace> remains below the range of noise for few time-lags in the past and it remains within the bounds of the noise range for older stimuli. The networks then are still capable of tolerating noise, while the volumes of representation are becoming more overlapping. The decrease of noise-robustness for larger time-lags in the past confirms our suggestion that volumes of representation become less separate for older inputs. The analysis of order-2 volumes of representation (<xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5E</xref>) also suggests that less probable transitions of the input are more prone to noise. This, however, was not tested. The task <monospace>Parity-3</monospace> is noise-robust for 0-time-lag only and with the change in performance being within the noise range. This is understandable, since for each time-lag, order-3 volumes of representation and the associated volumes of the <monospace>Parity-3</monospace> function should be separate and redundant.</p>
<p>These observations confirm our hypothesis that redundancy and separability are the appropriate ingredients for a noise-robust information processing system, such as our model neural network. These properties being the outcome of <monospace>STDP</monospace>'s and <monospace>IP</monospace>'s collaboration, suggest the pivotal role of the interaction between homeostatic and synaptic plasticity for combating noise.</p>
</sec><sec id="s2i">
<title>Constructive Role of Noise</title>
<p>Now that we have demonstrated the contributions of <monospace>STDP</monospace> and <monospace>IP</monospace> in combating noise, we turn to investigating noise's beneficial role. We have seen that perturbation at the end of the plasticity phase provides a solution to the network being trapped in an input-insensitive regime. Besides viewing perturbation as a form of one-shot strong noise, which is, biologically speaking, an unnatural phenomenon, what effect would a perpetual small amount of noise have on the dynamics of the recurrent neural network?</p>
<p>We again deploy a certain rate of random bit flips on the network state that reserves the <monospace>kWTA</monospace> dynamics. Unlike the previous section, we do not restrict noise to the training and testing phase, but apply it also during the plasticity phase. We also do not reset the network activity after the plasticity phase, i.e. the perturbation parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e053" xlink:type="simple"/></inline-formula> is set to 0.</p>
<p><xref ref-type="fig" rid="pcbi-1003512-g007">Figure 7A–C</xref> compares the performance of optimal linear classifiers on the three tasks for different levels of noise. For all tasks, some levels of noise resulted in a significantly higher average performance than the noiseless case. The task <monospace>Markov-85</monospace> had the highest average performance at the largest level of noise, while the tasks <monospace>RAND x 4</monospace> and <monospace>Parity-3</monospace>, where the input was uniformly random, had the highest performance at the third and fourth levels of noise, and the average performance dropped substantially at the fifth level of noise. In all tasks, performance was far off the levels it reached in the noiseless case (<xref ref-type="fig" rid="pcbi-1003512-g002">Figure 2</xref>).</p>
<fig id="pcbi-1003512-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003512.g007</object-id><label>Figure 7</label><caption>
<title>Noise at certain levels is rendered constructive when synaptic and intrinsic plasticity interact.</title>
<p>Average classification performance of 100 networks trained with both <monospace>STDP</monospace> and <monospace>IP</monospace> on (A) the memory task <monospace>RAND x 4</monospace>, (B) the prediction task <monospace>Markov-85</monospace>, and (C) the nonlinear task <monospace>Parity-3</monospace> for increasing levels of noise and no perturbation at the end of the plasticity phase (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e054" xlink:type="simple"/></inline-formula>). (D) Network state entropy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e055" xlink:type="simple"/></inline-formula> and (E) the mutual information with the three most recent <monospace>RAND x 4</monospace> inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e056" xlink:type="simple"/></inline-formula> at the end of the plasticity phase for different levels of noise. Values are averaged over 50 networks and estimated from 5000 samples for each network. (A–E) Noise levels are applied during the plasticity, training, and testing phases. They indicate the probability of a bit flip in the network state, that is, the probability of one of the <italic>k</italic> spiking neurons at time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e057" xlink:type="simple"/></inline-formula> to become silent, while silent neuron to fire instead. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e058" xlink:type="simple"/></inline-formula>. Error bars indicate standard error of the mean.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003512.g007" position="float" xlink:type="simple"/></fig>
<fig id="pcbi-1003512-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003512.g008</object-id><label>Figure 8</label><caption>
<title>Schematics of the driven dynamics of networks endowed by synaptic and homeostatic plasticity, and the emergence of noise-robust spatiotemporal computations.</title>
<p>(A) The dynamics of a recurrent network that is trained by homeostatic and synaptic plasticity and driven by a Markovian input. Each layer corresponds to one input. The layer illustrates a two-dimensional projection of the phase space of the autonomous (semi-)dynamical system associated with that input. A layer that corresponds to the spontaneous activity (SA) is added for completeness. Due to the interaction of synaptic and homeostatic plasticity, each of these (semi-)dynamical systems has two dynamic regimes: an input-insensitive dynamic regime that is shared by all the layers and that captures the temporal structure of the input, and an input-sensitive dynamic regime that contains a single periodic attractor. The input-sensitive attractor depends on the layer and is close to one of the vertexes of the input-insensitive attractor. The network is excited by the exemplary input sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e059" xlink:type="simple"/></inline-formula>. The red cross refers to the initial conditions that are chosen within the input-sensitive dynamics. Given the input sequence, the network dynamics follows the meta-transient that is illustrated by the arrows between the different layers. For instance, when the network is excited by the input <italic>B</italic>, the network activity approaches the <italic>B</italic>-attractor within the corresponding layer. When <italic>C</italic> follows, a bifurcation occurs, where the <italic>B</italic>-attractor becomes unstable and the <italic>C</italic>-attractor becomes stable. The meta-transient approaches the <italic>C</italic>-attractor from the direction of the unstable <italic>B</italic>-attractor. When <italic>C</italic> is preceded by the less common input <italic>A</italic>, the <italic>C</italic>-attractor is approached differently, such that the distance to it is bigger than in the case of the most common transition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e060" xlink:type="simple"/></inline-formula>. (B) Noise-robust computations are a result of the interaction between synaptic and homeostatic intrinsic plasticity. Synaptic plasticity leads to high separability and intrinsic plasticity to redundancy. These effects lead to a neural code that allows a higher margin of noise and alternative representations of computations, thus facilitating noise-robustness.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003512.g008" position="float" xlink:type="simple"/></fig>
<p>Information-theoretical quantities are again measured on networks with <monospace>RAND x 4</monospace> input. As expected, the network state entropy increases monotonically with noise (<xref ref-type="fig" rid="pcbi-1003512-g007">Figure 7D</xref>). Mutual information, on the other hand, starts dropping for noise larger than the third level (<xref ref-type="fig" rid="pcbi-1003512-g007">Figure 7E</xref>). This is also expected from the change of performance (<xref ref-type="fig" rid="pcbi-1003512-g007">Figure 7A</xref>). Noise then appears to provide, in some of the <monospace>SIP-RN</monospace>s, the necessary means to escape the input-insensitive dynamics. At some levels, however, the network activity becomes dominated by noise beyond the compensatory effects of redundancy and separability achieved through plasticity. In addition, more unstructured noise during the plasticity phase delays the creation and expansion of useful volumes of representation, thereby hindering computations further.</p>
</sec></sec><sec id="s3">
<title>Discussion</title>
<p>We demonstrated how the interaction of synaptic learning and homeostatic regulation boosts memory capacity of recurrent neural networks, allows them to discover regularities in the input stream, and enhances nonlinear computations. We provided a geometric interpretation of the emergence of these spatiotemporal computations through analyzing the driven dynamic response of the recurrent neural network. We view computations as a geometric relationship between <italic>representations</italic> of functions over stimuli, representations that consist of network states, and the asymptotic dynamics of the network, i.e. attractors. Accordingly, <xref ref-type="fig" rid="pcbi-1003512-g008">Figure 8A</xref> shows a possible driven-dynamics viewpoint on computation, which is the following. As the stimulus changes, a bifurcation occurs where the current attractor of the network becomes unstable, while another stabilizes according to the current stimulus. That leads the network dynamics to change its course towards the new stable region, or attractor, of the state space, and away from the previous attractors that are all unstable. As such, this path of the network activity, i.e. the meta-transient <xref ref-type="bibr" rid="pcbi.1003512-Negrello1">[44]</xref>, is defined by both the stimulus sequence and the locations of the network's attractors. Together, they lead the meta-transient to pass through particular representations which encode computations. An equivalent alternative to the <italic>chain of bifurcations</italic> between autonomous attractors is that of a single <italic>nonautonomous attractor</italic> that behaves as a stimulus-dependent moving target of the dynamics.</p>
<p>We showed that a successful implementation of these spatiotemporal computations requires the interaction of synaptic and homeostatic intrinsic plasticity which generates <italic>useful representations</italic> in the dynamics of excitable cortical networks. <xref ref-type="fig" rid="pcbi-1003512-g008">Figure 8</xref> schematically illustrates the stimulus-driven dynamical viewpoint of spatiotemporal computations and the effects of plasticity. Synaptic plasticity produces stimulus-insensitive dynamics that captures the temporal structure of the input. Intrinsic plasticity increases the neuronal bandwidth by increasing sensitivity to stimuli, which reduces the dominance of the stimulus-insensitive dynamics. This, in combination with synaptic plasticity, generates stimulus-sensitive attractors and <italic>redundant</italic> representations around them. These stimulus-sensitive components are pulled apart by the stimulus-insensitive dynamics, so that the structure of the input is preserved, and the <italic>separability</italic> of representations is higher and computations are realizable.</p>
<p>We pointed out throughout the text that computation is an <italic>emergent</italic> property of the recurrent network, and that it cannot be fully understood from the individual contribution of the parts, be it neurons or plasticity mechanisms. It might appear contradictory to that statement that the analysis was often concerned with the isolated role of each single plasticity mechanism. However, the quantitative assessments of computations point back to the emergent and collective aspect of computation. Namely, measured on <monospace>SIP-RN</monospace>s, neither performance of linear classifiers nor mutual information with input can be accounted for by a linear relationship between the respective quantities measured on <monospace>SP-RN</monospace>s and <monospace>IP-RN</monospace>s. In fact, the performance of networks where the recurrent weights and firing thresholds are adapted separately, and then combined following the plasticity phase, is far less than the performance of <monospace>SIP-RN</monospace>s, where intrinsic and synaptic plasticity are mutually active (see <xref ref-type="supplementary-material" rid="pcbi.1003512.s004">Figure S4</xref>). This further consolidates the claim that computations in <monospace>SIP-RN</monospace>s <italic>emerge</italic> from the <italic>interaction</italic> of <monospace>STDP</monospace> and <monospace>IP</monospace>, and <italic>not</italic> from their isolated contributions. It also points back to the formation of separate and redundant representations from the continuous interplay of these two mechanisms.</p>
<p>We also illustrated the combined role of synaptic and homeostatic intrinsic plasticity in creating noise-robust encoding through the generation of a redundant neural code. Many studies have investigated the redundant nature of neural information transmission in many cortical regions, and have justified this expensive allocation of neural recourses by redundancy serving as an error-correction strategy that provides neural assemblies with the capacity to average out noise <xref ref-type="bibr" rid="pcbi.1003512-Shadlen1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Panzeri1">[47]</xref>–<xref ref-type="bibr" rid="pcbi.1003512-Chechik1">[50]</xref>. Tka<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e061" xlink:type="simple"/></inline-formula>ik and colleagues have shown that in the presence of noise, a maximum entropy model of the retina increases redundancy for higher noise levels. A side effect of their model is that stimulus representations become highly separate, which increases the tolerance margin of noise and enhances information transmission <xref ref-type="bibr" rid="pcbi.1003512-Tkaik1">[51]</xref>. Our model was able, through <italic>local</italic> plasticity mechanisms, to capture both of these properties, achieved in <xref ref-type="bibr" rid="pcbi.1003512-Tkaik1">[51]</xref> through optimality principles, and to lead to a noise-robust population code (<xref ref-type="fig" rid="pcbi-1003512-g008">Figure 8B</xref>). Namely, synaptic plasticity enhances the separability of representations through the pulling force of the input-insensitive attractor, while intrinsic plasticity perturbs the network states and increases redundancy when interacting with synaptic plasticity, which allows for alternative representations of similar input sequences. Another point of similarity with the model of Tka<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e062" xlink:type="simple"/></inline-formula>ik and colleagues <xref ref-type="bibr" rid="pcbi.1003512-Tkaik1">[51]</xref> and with empirical findings <xref ref-type="bibr" rid="pcbi.1003512-Kenet1">[52]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Fiser1">[53]</xref> is the remnant fingerprint of the most common stimulus in the network's spontaneous activity, which manifests in our model neural network in the stimulus-insensitive dynamics (<xref ref-type="fig" rid="pcbi-1003512-g005">Figure 5B–C</xref>).</p>
<p>In addition to combating noise, our model explores a potential benefit from its presence. We pointed out the necessity of the stimulus-insensitive dynamics for the emergence of computation in the model neural network. The stimulus-insensitive attractor provides the baseline dynamics for the appearance of highly separate representations, and thus, the excitable dynamics necessary for computations. Getting from the input-insensitive regime to the excitable one depended, however, on the <italic>ad hoc</italic> reinitialization of the network activity at the end of the plasticity phase. Noise provides an alternative. During the plasticity phase, noise shallows the boundaries between the two basins of attraction, which reduces the dominance of the stimulus-insensitive attractor. After the plasticity phase, noise supplies the small perturbations needed to get the network activity to the sensitive dynamics where computations are possible. This solution, in comparison to reinitializing the network activity, is more inferior, specifically because noise also delays the learning of representations. We postulate that another homeostatic plasticity mechanism, <italic>synaptic scaling</italic>, might contribute to the shallowing of the attractor boundary by constraining the strength of synapse bundles between neural subpopulations (e.g., between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e063" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e064" xlink:type="simple"/></inline-formula>). For instance, synaptic scaling was necessary for implementing spatiotemporal computations in <bold><italic>s</italic></bold><italic>elf-</italic><bold><italic>o</italic></bold><italic>rganizing </italic><bold><italic>r</italic></bold><italic>ecurrent </italic><bold><italic>n</italic></bold><italic>etworks</italic> (<monospace>SORN</monospace>) <xref ref-type="bibr" rid="pcbi.1003512-Lazar2">[28]</xref>, but no analysis of the dynamics of these networks was done. Testing this hypothesis is, however, beyond the scope of this work.</p>
<p>It is also tempting to connect the topology of the attractor landscape of <monospace>SIP-RN</monospace>s to neuropathology and to a model by Pfister and Tass <xref ref-type="bibr" rid="pcbi.1003512-Pfister1">[54]</xref>. They suggest that two stable regimes of recurrent network activity, a synchronous pathological regime and an asynchronous healthy regime, coexist, and that their coexistence is a necessary condition for the functioning of a model of deep brain stimulation. In their model, the stimulation of the recurrent network destabilizes the synchronous dynamics through inducing <monospace>STDP</monospace>. The destabilization drives the network activity towards the healthy asynchronous basin of attraction. By eliminating the stimulation, the energy hill between the two dynamic regimes rises again and the network remains in the healthy dynamics. Our study has shown how these two coexisting dynamic regimes and their associated forms of activity might come into being through neuronal plasticity. We also suggested noise as a possible mechanism for avoiding the unhealthy dynamics. Further analysis is necessary to investigate how the interaction between noise and different plasticity mechanisms might contribute to our understanding of neurological disorders.</p>
<p>Our analysis of spatiotemporal computations was restricted to Markovian dependencies in the temporal structure of the stimulus or to no dependencies at all. This is often not the case in natural stimuli faced by animals and humans, where the Markov property does not always hold. Lazar et al. have shown that <monospace>SIP-RN</monospace>s are capable, to a certain degree, of performing predictions on second-order Markov chains <xref ref-type="bibr" rid="pcbi.1003512-Lazar1">[27]</xref>. However, optimal encoding of non-Markovian stimuli and performing computations over them require forms of spike-timing-dependent plasticity that are less myopic to the temporal dependencies than what we considered in this work (<xref ref-type="fig" rid="pcbi-1003512-g001">Figure 1B</xref>). For instance, Brea and colleagues have shown that storing and reproducing a non-Markovian sequence in a recurrent neural network require a nonlocal form of <monospace>STDP</monospace> with more complex temporal dependencies between pre- and post-synaptic spikes <xref ref-type="bibr" rid="pcbi.1003512-Brea1">[55]</xref>. While their model was not concerned with carrying through spatiotemporal computations of the kind we presented here, it successfully reproduced the stored non-Markovian input in the spontaneous activity of the neural network. This refers to a point of similarity to the simpler case we presented here, where Markovian input was stored and recalled in the spontaneous activity of the input-insensitive dynamics. In any case, while spatiotemporal computations over non-Markovian stimuli and the necessarily more complex plasticity mechanisms that lead to their emergence, are not considered here, we view the concepts and methodology developed above as a general framework for future studies.</p>
<p>In this article, we provided a first analysis of the combined role of synaptic and intrinsic plasticity on the emergent dynamics of recurrent neural networks subject to input. We redefined computations in relation to these emergent dynamics and related that to properties of the neural code. We also considered how the neural dynamics interact with noise, both as a nuisance to combat, and as a driving force towards healthy neural activity. The model we used is simplified, however, both in network architecture and plasticity mechanisms. While this simplification is necessary for mathematical convenience, biology never cares for formal abstractions, for the brain is a complex information processing system that is rich with a variety of neuronal morphologies and functions. The plastic changes the brain undergoes are neither confined to the two mechanisms we dealt with here, nor are they uniform across different regions. On the other hand, mathematical formalization of computation and adaptability allows the identification of unifying principles in computational biology, in general, and neural computations, in particular. We intended the current article as a step in that direction.</p>
</sec><sec id="s4" sec-type="methods">
<title>Methods</title>
<p>The setup on which we assessed spatiotemporal computations in recurrent neural networks is partially inspired by the theory of <bold><italic>r</italic></bold><italic>eservoir </italic><bold><italic>c</italic></bold><italic>omputing</italic> (RC) <xref ref-type="bibr" rid="pcbi.1003512-Buonomano1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Jaeger1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Maass1">[30]</xref>. However, as shown in the <xref ref-type="sec" rid="s2">Results</xref> section, our analysis is independent of the RC paradigm, as it is concerned with the effects of plasticity on the recurrent network, and optimal linear classifiers are only used as one possible probe to quantify these effects. We present in this section the <italic>recurrent network</italic> (RN) architecture and the plasticity mechanisms active in shaping the neural response. We follow by introducing the computational tasks and justifying their selection. We then specify the simulation conditions and the training of optimal linear classifiers, followed by demonstrating how information-theoretical quantities are estimated. We finally lay down the mathematical formalization of the autonomous, input-driven, and input-insensitive dynamics of the recurrent network: We adapt Definitions 2, 4, 6–8 from <xref ref-type="bibr" rid="pcbi.1003512-Kloeden1">[31]</xref> to the special case of discrete-time dynamics <xref ref-type="bibr" rid="pcbi.1003512-Kloeden2">[32]</xref>, which is the case that concerns the current article. We contribute the new concepts of volumes of represen\r notation and purposes.</p>
<sec id="s4a">
<title>Network Architecture</title>
<p>In this paper, the model recurrent network is of the <italic>k-Winner-Take-All</italic> (<monospace>kWTA</monospace>) type <xref ref-type="bibr" rid="pcbi.1003512-Lazar1">[27]</xref> that consists of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e065" xlink:type="simple"/></inline-formula> memoryless binary neurons from which only <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e066" xlink:type="simple"/></inline-formula> neurons are active. The discrete-time dynamics of the recurrent network at each time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e067" xlink:type="simple"/></inline-formula> is given by<disp-formula id="pcbi.1003512.e068"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e068" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e069" xlink:type="simple"/></inline-formula> is the network state. The nonlinear function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e070" xlink:type="simple"/></inline-formula> sets the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e071" xlink:type="simple"/></inline-formula> units with the highest activities to 1 (spiking), and the rest to 0 (silent). As such, the population firing rate is held constant at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e072" xlink:type="simple"/></inline-formula>, and there is no need to introduce inhibitory neurons to balance excitation and inhibition. Recurrent synaptic efficacy is defined by the weight matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e073" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e074" xlink:type="simple"/></inline-formula> being the efficacy of the synapse connecting neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e075" xlink:type="simple"/></inline-formula> to neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e076" xlink:type="simple"/></inline-formula>. Self-coupling is avoided by setting diagonal elements <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e077" xlink:type="simple"/></inline-formula> to 0. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e078" xlink:type="simple"/></inline-formula> defines neuronal firing thresholds that modulate the neurons' resistance to firing, and hence, their excitability. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e079" xlink:type="simple"/></inline-formula> is the external drive whose dynamics depends on the task performed.</p>
<p>More formally, the set of possible network states is a metric space:</p>
<p><bold>Definition 1.</bold> Given the set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e080" xlink:type="simple"/></inline-formula> of all binary vectors of size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e081" xlink:type="simple"/></inline-formula>, we define the <italic>Hamming metric</italic> by the function:<disp-formula id="pcbi.1003512.e082"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e082" xlink:type="simple"/></disp-formula></p>
<p>According to this metric, the distance between two vectors of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e083" xlink:type="simple"/></inline-formula> is the number of bits at which these two vectors differ. The Hamming metric is a proper metric on strings of fixed length which is the case for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e084" xlink:type="simple"/></inline-formula>. The pair <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e085" xlink:type="simple"/></inline-formula> then forms a <italic>metric space</italic>. It is also equivalent to the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e086" xlink:type="simple"/></inline-formula> norm on the set <italic>Y</italic>, which allows us to define the <italic>Hamming length</italic> of a vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e087" xlink:type="simple"/></inline-formula> as the Hamming distance between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e088" xlink:type="simple"/></inline-formula> and the 0-vector, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e089" xlink:type="simple"/></inline-formula>.</p>
<p>Given the <monospace>kWTA</monospace> dynamics (see <xref ref-type="disp-formula" rid="pcbi.1003512.e068">Equation 1</xref>), the network activity is restricted to the set:<disp-formula id="pcbi.1003512.e090"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e090" xlink:type="simple"/><label>(2)</label></disp-formula></p>
<p>Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e091" xlink:type="simple"/></inline-formula>, the pair <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e092" xlink:type="simple"/></inline-formula> forms a metric space as well. Distances between subsets of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e093" xlink:type="simple"/></inline-formula> can be measured using the Hausdorff metric, which we also denote <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e094" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4b">
<title>Plasticity Mechanisms</title>
<p>We are concerned with the interplay of two forms of plasticity in enhancing the computational capability of the model recurrent network.</p>
<p><bold><italic>S</italic></bold><italic>pike-</italic><bold><italic>t</italic></bold><italic>iming-</italic><bold><italic>d</italic></bold><italic>ependent synaptic </italic><bold><italic>p</italic></bold><italic>lasticity</italic> (<monospace>STDP</monospace>) is a set of Hebbian/anti-Hebbian learning rules, where synaptic efficacy is modified according to the relative firing time between pre- and post-synaptic neurons <xref ref-type="bibr" rid="pcbi.1003512-Markram1">[56]</xref>. We adapted a simple causal <monospace>STDP</monospace> learning rule by which a synapse is potentiated whenever the pre-synaptic neuron fires one time step before the post-synaptic neuron, and is depressed when a post-synaptic spike precedes a pre-synaptic spike by one time step:<disp-formula id="pcbi.1003512.e095"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e095" xlink:type="simple"/><label>(3)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e096" xlink:type="simple"/></inline-formula> is the synaptic plasticity learning rate set to 0.001. To prevent weights from switching signs or growing uncontrollably, we enforce hard bounds such that the weights remain within the interval [0, 1].</p>
<p>Competition between synapses due to <monospace>STDP</monospace> leads to neurons with synapses that won the competition to fire consistently and those who lost the competition to be constantly silent <xref ref-type="bibr" rid="pcbi.1003512-Song1">[57]</xref>. To counteract this pathological state, the time-averaged firing rate for a neuron is modulated through homeostatic modification of its excitability threshold using <bold><italic>i</italic></bold><italic>ntrinsic </italic><bold><italic>p</italic></bold><italic>lasticity</italic> (<monospace>IP</monospace>) <xref ref-type="bibr" rid="pcbi.1003512-Zhang1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Turrigiano1">[7]</xref>:<disp-formula id="pcbi.1003512.e097"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e097" xlink:type="simple"/><label>(4)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e098" xlink:type="simple"/></inline-formula> is the intrinsic plasticity learning rate set to 0.001. This rule uses subtractive normalization to pull the time-averaged firing rate of each neuron closer to the population firing rate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e099" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4c">
<title>Computational Tasks</title>
<p>Neural circuits in different brain regions adapt to best serve the region's functional purpose. To that end, we constructed three tasks, each of which resembles in spirit the demands of one such canonical function. We then, under the stimulation conditions of each task, compared the performance, information content, and dynamical response of networks optimized by combining both <monospace>STDP</monospace> and <monospace>IP</monospace> with networks that are optimized by <monospace>STDP</monospace> alone or <monospace>IP</monospace> alone.</p>
<p>In all tasks, the network is subject to perturbation by a set of inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e100" xlink:type="simple"/></inline-formula>. The receptive fields of non-overlapping subsets of neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e101" xlink:type="simple"/></inline-formula> are tuned exclusively to each input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e102" xlink:type="simple"/></inline-formula>. As such, each input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e103" xlink:type="simple"/></inline-formula> has its corresponding receptive field <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e104" xlink:type="simple"/></inline-formula> in the recurrent neural network. When an input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e105" xlink:type="simple"/></inline-formula> drives the network, all neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e106" xlink:type="simple"/></inline-formula> receive a positive drive <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e107" xlink:type="simple"/></inline-formula>, while the rest <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e108" xlink:type="simple"/></inline-formula> receive none. Readouts are trained on the current network state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e109" xlink:type="simple"/></inline-formula> to compute a function over input sequences <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e110" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e111" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e112" xlink:type="simple"/></inline-formula> being time-lags at which target inputs are applied where positive lags corresponds to future inputs and negative lags to past ones. We restrict time-lags <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e113" xlink:type="simple"/></inline-formula> to the range <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e114" xlink:type="simple"/></inline-formula>.</p>
<p>In a first task, <monospace>RAND x 4</monospace>, we assessed the capacity of the network to retain memory of past stimuli within its activity. The recurrent network is driven by four randomly drawn inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e115" xlink:type="simple"/></inline-formula>. The receptive field of each input consists of 15 neurons, and one optimal linear classifier <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e116" xlink:type="simple"/></inline-formula> is trained for each input/time-lag pair, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e117" xlink:type="simple"/></inline-formula> fires when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e118" xlink:type="simple"/></inline-formula> and is silent otherwise.</p>
<p>The second task, <monospace>Markov-85</monospace>, explores the ability of the recurrent network to discover temporal regularities in its input. The recurrent network receives one of four possible inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e119" xlink:type="simple"/></inline-formula> generated from a Markov chain with 85% probability for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e120" xlink:type="simple"/></inline-formula> to be followed by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e121" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e122" xlink:type="simple"/></inline-formula> followed by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e123" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e124" xlink:type="simple"/></inline-formula> followed by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e125" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e126" xlink:type="simple"/></inline-formula> followed by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e127" xlink:type="simple"/></inline-formula> All other transitions occur with a 5% probability. Again, the receptive field of each input consists of 15 neurons, and one optimal linear classifier <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e128" xlink:type="simple"/></inline-formula> is trained for each input/time-lag pair.</p>
<p>With the third task, <monospace>Parity-3</monospace>, we exploit the nonlinear expansion provided by the recurrent neural network. Here, the network is subject to binary input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e129" xlink:type="simple"/></inline-formula>, where each symbol has a receptive field of 40 neurons. The task is to identify the parity of a sequence of three successive inputs. This means that given an input sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e130" xlink:type="simple"/></inline-formula>, an optimal linear classifier <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e131" xlink:type="simple"/></inline-formula> fires when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e132a" xlink:type="simple"/></inline-formula><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e132b" xlink:type="simple"/></inline-formula>, and is silent otherwise. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e133" xlink:type="simple"/></inline-formula> is the nonlinear <italic>exclusive or</italic> binary operation.</p>
<p>Even though every task used here is very much simplified compared to stimuli usually processed by neural systems, we would still like to link the basic properties of every task presented here to a realistic case processed by a human or an animal. The property of the memory task <monospace>RAND x 4</monospace> that we want to emphasize is that a neural system must be able to process rapidly changing stimuli that are only shortly presented. That property is partly reminiscent of retinal input, which is rather stationary during moments of fixation, and rapidly changing due to saccadic eye movements. However, it needs to be noted that saccadic eye movements might be difficult to predict and may appear rather random, but are very likely structured and stimulus-dependent. This motivated the prediction task <monospace>Markov-85</monospace> that models temporally structured and rapidly changing sensory input that is shortly presented. Such input could either be generated by retinal input and saccadic eye movements, or by the whisking behavior and the produced neural activity in the barrel cortex of a mouse. In addition, nonlinearities are prevailing in natural stimuli, and to highlight the necessity of processing these stimuli, we used the nonlinear task <monospace>Parity-3</monospace>. Such computational demands can be easily motivated by occlusion in vision, where pixel intensities do not sum up linearly at points where one object occludes another in the visual field. Again, we stress that none of these tasks is a good model of real processing in neural systems in nature. However, each is sharing individual aspects that are motivated by real life examples.</p>
</sec><sec id="s4d">
<title>Simulation Conditions</title>
<p>In order to isolate the role of <monospace>STDP</monospace> and <monospace>IP</monospace> in shaping the computational and information processing properties of the recurrent network, we compared networks trained by both <monospace>STDP</monospace> and <monospace>IP</monospace>, with networks that are trained by <monospace>STDP</monospace> alone or <monospace>IP</monospace> alone.</p>
<p>Throughout all experiments, we trained networks of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e134" xlink:type="simple"/></inline-formula> neurons on either the <monospace>STDP</monospace>+<monospace>IP</monospace> condition, the <monospace>STDP</monospace> condition, or the <monospace>IP</monospace> condition for a <italic>plasticity phase</italic> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e135" xlink:type="simple"/></inline-formula> time steps. For convenience, we call a <italic>recurrent network</italic> trained with both <italic>synaptic</italic> and <italic>intrinsic</italic> plasticity <monospace>SIP-RN</monospace>. In contrast, we name a recurrent network that learned with a single plasticity mechanism either <monospace>SP-RN</monospace> or <monospace>IP-RN</monospace>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e136" xlink:type="simple"/></inline-formula> is set to 12, the initial weights are chosen uniformly on the interval [0, 0.1] with 10% connectivity probability, and thresholds are drown from a Gaussian distribution with 0 mean and 0.1 standard deviation. Under the <monospace>IP</monospace> condition, to assure that weights' distribution is not different from conditions where <monospace>STDP</monospace> modifies the synaptic efficacies <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e137" xlink:type="simple"/></inline-formula> a <italic>pre-plasticity phase</italic> of similar length to the plasticity phase precedes the latter, where both <monospace>STDP</monospace> and <monospace>IP</monospace> are active. Afterwards, the weights structure is destroyed by random shuffling and the plasticity phase starts where <monospace>IP</monospace> is turned on.</p>
<p>In all experiments where the performance of optimal linear classifiers is estimated, the plasticity phase was <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e138" xlink:type="simple"/></inline-formula> time steps long. Afterwards, weights and thresholds are held fixed, the network state is <italic>reset</italic> to a random initial state, and the <italic>training phase</italic> starts where linear classifiers are trained using linear regression on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e139" xlink:type="simple"/></inline-formula> time steps, followed by a <italic>testing phase</italic> of performance for another <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e140" xlink:type="simple"/></inline-formula> time steps.</p>
</sec><sec id="s4e">
<title>Post-Plasticity Perturbation</title>
<p>At the beginning of the training phase, the network state is reset to a random initial state. If the network dynamics is multistable, this resetting could bring it to a different regime than where the network was at the end of the plasticity phase. To test this possibility systematically, we perform the following post-plasticity perturbation.</p>
<p>Given some perturbation parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e141" xlink:type="simple"/></inline-formula>. We assume the network state at the end of the plasticity phase is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e142" xlink:type="simple"/></inline-formula>. Instead of randomly choosing the initial network state for the training phase, we choose a network state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e143" xlink:type="simple"/></inline-formula> such that the condition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e144" xlink:type="simple"/></inline-formula> holds. To satisfy this condition, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e145" xlink:type="simple"/></inline-formula> is chosen as follows. In the network state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e146" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e147" xlink:type="simple"/></inline-formula> firing neurons and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e148" xlink:type="simple"/></inline-formula> silent neurons are randomly selected. The <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e149" xlink:type="simple"/></inline-formula> firing neurons are then silenced and the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e150" xlink:type="simple"/></inline-formula> silent neurons are set to firing.</p>
</sec><sec id="s4f">
<title>Output Weights and Performance</title>
<p>According to the RC paradigm, an input signal undergoes a nonlinear feature expansion by projecting into a recurrent neural network of nonlinear units. The network recurrency also provides a sustained but damped trace of past inputs (echo state <xref ref-type="bibr" rid="pcbi.1003512-Jaeger1">[29]</xref> or fading memory <xref ref-type="bibr" rid="pcbi.1003512-Maass1">[30]</xref>) to propagate through the network. The network state is then read out by simple linear units through linear regression.</p>
<p>Following the plasticity phase, the network activity during the training phase<disp-formula id="pcbi.1003512.e151"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e151" xlink:type="simple"/><label>(5)</label></disp-formula>provides the training data for all optimal linear classifiers, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e152" xlink:type="simple"/></inline-formula> denotes matrix transpose. The target signal of output neurons for a particular time-lag <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e153" xlink:type="simple"/></inline-formula> is clamped in a supervised fashion to<disp-formula id="pcbi.1003512.e154"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e154" xlink:type="simple"/><label>(6)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e155" xlink:type="simple"/></inline-formula> depends on the task and is the cardinality of the set of possible values which the target signal can take. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e156" xlink:type="simple"/></inline-formula> equals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e157" xlink:type="simple"/></inline-formula> for the tasks <monospace>RAND x 4</monospace> and <monospace>Markov-85</monospace>. Output weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e158" xlink:type="simple"/></inline-formula> for each time-lag are then computed using linear regression through ordinary least squares<disp-formula id="pcbi.1003512.e159"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e159" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e160" xlink:type="simple"/></inline-formula> is the <italic>Moore-Penrose pseudoinverse</italic> of a matrix, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e161" xlink:type="simple"/></inline-formula> is the regular inverse of square matrices.</p>
<p>These optimal linear classifiers are then validated on the network activity<disp-formula id="pcbi.1003512.e162"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e162" xlink:type="simple"/><label>(8)</label></disp-formula>during the testing phase. First, a pre-estimate of the target signal is computed for each time-lag:<disp-formula id="pcbi.1003512.e163"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e163" xlink:type="simple"/><label>(9)</label></disp-formula></p>
<p>Only one output neuron fires each time step for each time-lag, and this is specified through winner-take-all on the rows of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e164" xlink:type="simple"/></inline-formula>. This leads to the final estimate <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e165" xlink:type="simple"/></inline-formula>. The <italic>classification performance</italic> for each time-lag is finally computed as the percentage of correct classifications:<disp-formula id="pcbi.1003512.e166"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e166" xlink:type="simple"/><label>(10)</label></disp-formula></p>
</sec><sec id="s4g">
<title>Computing Entropy and Mutual Information</title>
<p>On multiple occasions, both the self-information capacity of the network state and its dependence on input was measured. Entropy measures self-information capacity which is the expected value of information carried by the network activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e167" xlink:type="simple"/></inline-formula> and is given by<disp-formula id="pcbi.1003512.e168"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e168" xlink:type="simple"/><label>(11)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e169" xlink:type="simple"/></inline-formula> is the base-2 logarithm, so that entropy (and mutual information) are measured in bits. Mutual information measures the dependence of the network activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e170" xlink:type="simple"/></inline-formula> on a corresponding input sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e171" xlink:type="simple"/></inline-formula> and is given by<disp-formula id="pcbi.1003512.e172"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e172" xlink:type="simple"/><label>(12)</label></disp-formula></p>
<p>In computing entropy and mutual information, we used the algorithm and code developed in <xref ref-type="bibr" rid="pcbi.1003512-Kraskov1">[58]</xref> that computes entropy from an adaptive k-nearest-neighbor estimate of probability density functions. This allows for reliable estimates of these quantities with far fewer samples in comparison to other algorithms. Nevertheless, due to the high number of channels we have (100 neurons), and to truncate unnecessary computation time, samples from the network activity are first transfered to the principal components space, and only components that carry 95% of the information are passed to the mutual information estimator.</p>
<p>We always considered inputs from the task <monospace>RAND x 4</monospace> and we computed the mutual information between samples of the network state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e173" xlink:type="simple"/></inline-formula> and the three most recent inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e174" xlink:type="simple"/></inline-formula>. We encoded each of the four input symbols <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e175" xlink:type="simple"/></inline-formula> by a 3-bits code <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e176" xlink:type="simple"/></inline-formula> to ensure equal pairwise Hamming distances between symbols. For all cases but one, as few as 5000 samples of the network state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e177" xlink:type="simple"/></inline-formula> and input sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e178" xlink:type="simple"/></inline-formula> were enough to reliably estimate entropy and mutual information. The exception was computing mutual information between input and <monospace>IP-RN</monospace> activity, which demanded a higher number of samples (500000 time steps) and very long computation time, as covering 95% of the information required no less than 60 principal components.</p>
</sec><sec id="s4h">
<title>Autonomous Dynamics</title>
<p>For a full understanding of the emerging information processing properties of the interaction of synaptic and intrinsic plasticity, it was necessary to rely on and develop concepts from the newly emerging mathematical theory of <italic>nonautonomous dynamical systems</italic> <xref ref-type="bibr" rid="pcbi.1003512-Kloeden1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Kloeden2">[32]</xref>. Throughout what follows, the correspondence of the introduced concepts to our model is clarified. First, autonomous dynamics are defined, since they form a special instance of the nonautonomous case.</p>
<p><bold>Definition 2.</bold> Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e179" xlink:type="simple"/></inline-formula> be a metric space with a metric <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e180" xlink:type="simple"/></inline-formula> A <italic>discrete-time semi-dynamical system</italic> is a function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e181" xlink:type="simple"/></inline-formula> that satisfies</p>
<list list-type="order"><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e182" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e183" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e184" xlink:type="simple"/></inline-formula> is continuous.</p>
</list-item></list>
<p><xref ref-type="disp-formula" rid="pcbi.1003512.e068">Equation 1</xref> defines the driven or nonautonomous <monospace>kWTA</monospace> dynamics. The autonomous alternative is given by the discrete-time difference equation<disp-formula id="pcbi.1003512.e185"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e185" xlink:type="simple"/><label>(13)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e186" xlink:type="simple"/></inline-formula> is the <monospace>kWTA</monospace> nonlinearity defined as above. To relate <xref ref-type="disp-formula" rid="pcbi.1003512.e185">Equation 13</xref> to Definition 2, the function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e187" xlink:type="simple"/></inline-formula> (the <italic>solution mapping</italic>) is chosen such that<disp-formula id="pcbi.1003512.e188"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e188" xlink:type="simple"/><label>(14)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e189" xlink:type="simple"/></inline-formula> is function composition. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e190" xlink:type="simple"/></inline-formula> to be an autonomous semi-dynamical system, it has to satisfy the three conditions of Definition 2. The first two conditions are trivial, as they result from the definition of function composition. We turn to prove the third condition, namely, the continuity of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e191" xlink:type="simple"/></inline-formula>. We first observe that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e192" xlink:type="simple"/></inline-formula> is merely the t-fold composition of the function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e193" xlink:type="simple"/></inline-formula> and since the composition of continuous functions is continuous, it is sufficient to prove the continuity of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e194" xlink:type="simple"/></inline-formula></p>
<p><bold>Proposition 3.</bold> The <monospace>kWTA</monospace> function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e195" xlink:type="simple"/></inline-formula> from <xref ref-type="disp-formula" rid="pcbi.1003512.e185">Equation 13</xref> defined on the metric space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e196" xlink:type="simple"/></inline-formula> is continuous, i.e.<disp-formula id="pcbi.1003512.e197"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e197" xlink:type="simple"/></disp-formula></p>
<p><bold>Proof.</bold> For all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e198" xlink:type="simple"/></inline-formula> and all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e199" xlink:type="simple"/></inline-formula>, we choose <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e200" xlink:type="simple"/></inline-formula>. For all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e201" xlink:type="simple"/></inline-formula>, if the Hamming distance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e202" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e203" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e204" xlink:type="simple"/></inline-formula> have to be equal, since the <monospace>kWTA</monospace> dynamics restricts the distances between any two states to the set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e205" xlink:type="simple"/></inline-formula>. As such, since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e206" xlink:type="simple"/></inline-formula> is a metric, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e207" xlink:type="simple"/></inline-formula>, which is always smaller than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e208" xlink:type="simple"/></inline-formula>. Ergo, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e209" xlink:type="simple"/></inline-formula> is continuous.</p>
<p>We note that the proof to Proposition 3 becomes trivial if we consider a result from topology which states that any function from a <italic>discrete topological space</italic> to another is continuous. However, the proof is interesting in that it shows that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e210" xlink:type="simple"/></inline-formula> has a stronger form of continuity, that is, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e211" xlink:type="simple"/></inline-formula> is <italic>uniformly continuous</italic>, since the proof shows that there exists a <italic>packing radius</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e212" xlink:type="simple"/></inline-formula> such that either <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e213" xlink:type="simple"/></inline-formula>.</p>
<p>With the proof of Proposition 3, we conclude that the <monospace>kWTA</monospace> autonomous dynamics in <xref ref-type="disp-formula" rid="pcbi.1003512.e185">Equation 13</xref> generates a discrete-time semi-dynamical system. A dynamical system is a semi-dynamical system with invertible dynamics, which is not the case for <monospace>kWTA</monospace> networks. However, for all intents and purposes, being a semi-dynamical system is sufficient for formalizing the nonautonomous dynamics of the model network.</p>
</sec><sec id="s4i">
<title>Autonomous Attractors</title>
<p>Characterizing the computational properties of the model neural network requires defining invariant sets and attractors.</p>
<p><bold>Definition 4.</bold> Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e214" xlink:type="simple"/></inline-formula> be a discrete-time semi-dynamical system generated by an autonomous difference equation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e215" xlink:type="simple"/></inline-formula> on a metric space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e216" xlink:type="simple"/></inline-formula>. A subset <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e217" xlink:type="simple"/></inline-formula> is <italic>invariant</italic> under <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e218" xlink:type="simple"/></inline-formula>, and is <italic>positively invariant</italic> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e219" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e220" xlink:type="simple"/></inline-formula> is an <italic>attractor</italic> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e221" xlink:type="simple"/></inline-formula> if the following conditions hold:</p>
<list list-type="order"><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e222" xlink:type="simple"/></inline-formula> is invariant under <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e223" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e224" xlink:type="simple"/></inline-formula></p>
</list-item><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e225" xlink:type="simple"/></inline-formula> is compact.</p>
</list-item><list-item>
<p>There exists a neighborhood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e226" xlink:type="simple"/></inline-formula> of radius <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e227" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e228" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e229" xlink:type="simple"/></inline-formula></p>
</list-item></list>
<p>For the <monospace>kWTA</monospace> dynamics, the second condition is assured, since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e230" xlink:type="simple"/></inline-formula> is discrete and finite, which makes all subsets compact. The third condition assures that no subset of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e231" xlink:type="simple"/></inline-formula> satisfies the invariance and compactness conditions. Another important concept is that of a <italic>basin of attraction</italic> which associates each attractor with the region of the state space that converges to that attractor:</p>
<p><bold>Definition 5.</bold> Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e232" xlink:type="simple"/></inline-formula> be a discrete-time semi-dynamical system generated by an autonomous difference equation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e233" xlink:type="simple"/></inline-formula> on a metric space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e234" xlink:type="simple"/></inline-formula>. The <italic>basin of attraction</italic> of an attractor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e235" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e236" xlink:type="simple"/></inline-formula> is defined by<disp-formula id="pcbi.1003512.e237"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e237" xlink:type="simple"/></disp-formula></p>
</sec><sec id="s4j">
<title>Nonautonomous Dynamics</title>
<p>Unlike autonomous (semi-)dynamical systems, the elapsed time is not sufficient to find the solution for nonautonomous dynamics: both the start and end times must be specified. Accordingly, we now define a <italic>discrete-time nonautonomous dynamical system</italic> as a <italic>process</italic>. In what follows, we will make use of the set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e238" xlink:type="simple"/></inline-formula>.</p>
<p><bold>Definition 6.</bold> Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e239" xlink:type="simple"/></inline-formula> be a metric space with a metric <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e240" xlink:type="simple"/></inline-formula> A <italic>discrete-time process</italic> is a function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e241" xlink:type="simple"/></inline-formula> that satisfies</p>
<list list-type="order"><list-item>
<p>1. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e242" xlink:type="simple"/></inline-formula></p>
</list-item><list-item>
<p>2. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e243" xlink:type="simple"/></inline-formula></p>
</list-item><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e244" xlink:type="simple"/></inline-formula> is continuous.</p>
</list-item></list>
<p>We now turn to formulating the driven <monospace>kWTA</monospace> difference equation (see <xref ref-type="disp-formula" rid="pcbi.1003512.e068">Equation 1</xref>) as a discrete-time process. We first note that for a particular task, a set of possible inputs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e245" xlink:type="simple"/></inline-formula> is defined. For completeness, this set covers the autonomous case by including the 0-vector. For each member of this set, we define a separate map <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e246" xlink:type="simple"/></inline-formula>. The set of maps <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e247" xlink:type="simple"/></inline-formula> with cardinality <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e248" xlink:type="simple"/></inline-formula> defines a family of discrete-time autonomous semi-dynamical systems. These maps are chosen either randomly for the tasks <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e249" xlink:type="simple"/></inline-formula> and <monospace>Parity-3</monospace>, or in a more structured fashion for the task <monospace>Markov-85</monospace>. In either case, the <monospace>kWTA</monospace> discrete-time nonautonomous dynamics in <xref ref-type="disp-formula" rid="pcbi.1003512.e068">Equation 1</xref> can be rewritten in the form<disp-formula id="pcbi.1003512.e250"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e250" xlink:type="simple"/><label>(15)</label></disp-formula>which generates a solution mapping<disp-formula id="pcbi.1003512.e251"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e251" xlink:type="simple"/><label>(16)</label></disp-formula></p>
<p>The solution mapping <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e252" xlink:type="simple"/></inline-formula> satisfies the three properties of a process. The first two properties are a product of the definition of function composition, and the continuity condition is proven exactly as in Proposition 3. Given the above, the family of discrete-time autonomous difference equations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e253" xlink:type="simple"/></inline-formula> on the metric space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e254" xlink:type="simple"/></inline-formula> generates a process <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e255" xlink:type="simple"/></inline-formula>, and thus, it defines a particular kind of nonautonomous dynamical systems termed an <italic>input-driven dynamical system</italic>.</p>
<p>It is important to point out that an input-driven dynamical system is not defined for a particular input sequence, but for all input sequences drawn from its input set. This becomes more explicit if one considers the alternative <italic>skew product</italic> definition of a nonautonomous dynamical system, where the input is treated as a driving autonomous dynamical system <xref ref-type="bibr" rid="pcbi.1003512-Kloeden1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1003512-Kloeden2">[32]</xref>. We compare the two definitions of nonautonomous dynamical systems in <xref ref-type="supplementary-material" rid="pcbi.1003512.s007">Text S3</xref>. We now cover a few important concepts that will aid in defining the dynamic behavior of the model neural network.</p>
</sec><sec id="s4k">
<title>Nonautonomous Attractors</title>
<p>Attractors in nonautonomous dynamical systems are defined on <italic>nonautonomous sets</italic>, relating strongly to the concepts of <italic>invariance</italic> and <italic>entire solutions</italic>.</p>
<p><bold>Definition 7.</bold> Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e256" xlink:type="simple"/></inline-formula> be a discrete-time input-driven dynamical system generated by the family of autonomous difference equations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e257" xlink:type="simple"/></inline-formula> on a metric space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e258" xlink:type="simple"/></inline-formula>. A subset <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e259" xlink:type="simple"/></inline-formula> is called a <italic>nonautonomous set</italic>, and for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e260" xlink:type="simple"/></inline-formula>, the set<disp-formula id="pcbi.1003512.e261"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e261" xlink:type="simple"/></disp-formula>is called the <italic>t-fiber</italic> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e262" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e263" xlink:type="simple"/></inline-formula> is said to be <italic>invariant</italic> under <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e264" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e265" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e266" xlink:type="simple"/></inline-formula>. An <italic>entire solution</italic> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e267" xlink:type="simple"/></inline-formula> is an invariant set under <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e268" xlink:type="simple"/></inline-formula> whose t-fibers are the singleton sets <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e269" xlink:type="simple"/></inline-formula> that are the images of the function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e270" xlink:type="simple"/></inline-formula> such that<disp-formula id="pcbi.1003512.e271"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e271" xlink:type="simple"/></disp-formula></p>
<p>An important property of invariant nonautonomous sets is that they consist exclusively of entire solutions (for a proof, see Lemma 2.15 in <xref ref-type="bibr" rid="pcbi.1003512-Kloeden1">[31]</xref>). Nonautonomous attractors are nonautonomous sets. As such, they consist of entire solutions as well. There are several types of attractors of nonautonomous dynamical systems. Only of interest to our model neural network are forward attractors, so we drop the qualifier ‘forward’ and substitute it with ‘nonautonomous’.</p>
<p><bold>Definition 8.</bold> Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e272" xlink:type="simple"/></inline-formula> be a discrete-time input-driven dynamical system generated by the family of autonomous difference equations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e273" xlink:type="simple"/></inline-formula> on a metric space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e274" xlink:type="simple"/></inline-formula>. A nonautonomous set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e275" xlink:type="simple"/></inline-formula> is a <italic>nonautonomous attractor</italic> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e276" xlink:type="simple"/></inline-formula> if the following conditions hold:</p>
<list list-type="order"><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e277" xlink:type="simple"/></inline-formula> is invariant under <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e278" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e279" xlink:type="simple"/></inline-formula> is compact.</p>
</list-item><list-item>
<p>There exists a neighborhood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e280" xlink:type="simple"/></inline-formula> of radius <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e281" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e282" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e283" xlink:type="simple"/></inline-formula></p>
</list-item></list>
<p>As in the autonomous dynamics of <monospace>kWTA</monospace> networks, all subsets of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e284" xlink:type="simple"/></inline-formula> are compact. The third condition assures that no subset of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e285" xlink:type="simple"/></inline-formula> satisfies the invariance and compactness conditions. One may generalize the concept of a basin of attraction in an autonomous dynamical system to the nonautonomous case. This concept associates each nonautonomous attractor with the region of the state space that converges to that attractor:</p>
<p><bold>Definition 9.</bold> Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e286" xlink:type="simple"/></inline-formula> be a discrete-time input-driven dynamical system generated by the family of autonomous difference equations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e287" xlink:type="simple"/></inline-formula> on a metric space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e288" xlink:type="simple"/></inline-formula>. The <italic>nonautonomous basin of attraction</italic> of a nonautonomous attractor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e289" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e290" xlink:type="simple"/></inline-formula> is defined by<disp-formula id="pcbi.1003512.e291"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e291" xlink:type="simple"/></disp-formula></p>
</sec><sec id="s4l">
<title>Volumes of Representation</title>
<p>Spatiotemporal computations requires encoding different input sequences in the states of the neural network. The set of network states accessible from some initial conditions within a basin of attraction through perturbing the network with a particular input sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e292" xlink:type="simple"/></inline-formula> defines this sequence's <italic>volume of representation</italic>.</p>
<p><bold>Definition 10.</bold> Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e293" xlink:type="simple"/></inline-formula> be a discrete-time input-driven dynamical system generated by the family of autonomous difference equations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e294" xlink:type="simple"/></inline-formula> on a metric space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e295" xlink:type="simple"/></inline-formula>. Given an input sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e296" xlink:type="simple"/></inline-formula> and a basin of attraction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e297" xlink:type="simple"/></inline-formula>, a subset<disp-formula id="pcbi.1003512.e298"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e298" xlink:type="simple"/></disp-formula>is called the <italic>volume of representation</italic> of the input sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e299" xlink:type="simple"/></inline-formula> within the basin <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e300" xlink:type="simple"/></inline-formula>. The sequence length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e301" xlink:type="simple"/></inline-formula> defines the <italic>order</italic> of this volume. The nonautonomous set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e302" xlink:type="simple"/></inline-formula> whose t-fibers are order-1 volumes of representation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e303" xlink:type="simple"/></inline-formula> is called the <italic>perturbation set</italic> within <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e304" xlink:type="simple"/></inline-formula>. Also, given a function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e305" xlink:type="simple"/></inline-formula> on input sequences such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e306" xlink:type="simple"/></inline-formula>, the set<disp-formula id="pcbi.1003512.e307"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e307" xlink:type="simple"/></disp-formula>is the volume of representation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e308" xlink:type="simple"/></inline-formula> given <italic>g</italic>.</p>
<p>It is straightforward to show that, within a basin of attraction, the volume of representation of some sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e309" xlink:type="simple"/></inline-formula> is a superset of the volume of a sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e310" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e311" xlink:type="simple"/></inline-formula>, and that the volume of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e312" xlink:type="simple"/></inline-formula> is equivalent to the union of the volumes of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e313" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e314" xlink:type="simple"/></inline-formula>. We term this property the <italic>volumes' inclusion property</italic>.</p>
<p>The concept of ‘volumes of representation’ allows us to state the following theorem on the nature of attractors in discrete-time input-driven dynamical systems:</p>
<p><bold>Theorem 11.</bold> Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e315" xlink:type="simple"/></inline-formula> be a discrete-time input-driven dynamical system generated by the family of autonomous difference equations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e316" xlink:type="simple"/></inline-formula> on a metric space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e317" xlink:type="simple"/></inline-formula>, and let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e318" xlink:type="simple"/></inline-formula> be a <italic>compact</italic> nonautonomous basin of attraction. The following holds:</p>
<list list-type="order"><list-item>
<p>The perturbation set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e319" xlink:type="simple"/></inline-formula> is a superset of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e320" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p>Within <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e321" xlink:type="simple"/></inline-formula>, and for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e322" xlink:type="simple"/></inline-formula>, there exists one attractor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e323" xlink:type="simple"/></inline-formula> of the discrete-time autonomous semi-dynamical system generated by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e324" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e325" xlink:type="simple"/></inline-formula> is the basin of attraction of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e326" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e327" xlink:type="simple"/></inline-formula>.</p>
</list-item></list>
<p><bold>Proof.</bold></p>
<list list-type="order"><list-item>
<p>Since every attractor, whether autonomous or nonautonomous, is an invariant set, it is sufficient to prove that all invariant sets within a basin <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e328" xlink:type="simple"/></inline-formula> are a subset of its perturbation set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e329" xlink:type="simple"/></inline-formula>. Let's consider an entire solution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e330" xlink:type="simple"/></inline-formula>. For all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e331" xlink:type="simple"/></inline-formula>, it holds that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e332" xlink:type="simple"/></inline-formula>. It follows by induction that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e333" xlink:type="simple"/></inline-formula>. This translates to t-fibers of entire solutions being always a member of order-1 volumes of representation and that all entire solutions within a basin of attraction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e334" xlink:type="simple"/></inline-formula> are subsets of the perturbation set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e335" xlink:type="simple"/></inline-formula>. Since invariant sets consist exclusively of entire solutions, it follows that all invariant sets are subsets of the perturbation set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e336" xlink:type="simple"/></inline-formula>, including the nonautonomous attractor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e337" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p>Given some input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e338" xlink:type="simple"/></inline-formula> we consider the discrete-time semi-dynamical system generated by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e339" xlink:type="simple"/></inline-formula> on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e340" xlink:type="simple"/></inline-formula> with the solution mapping<disp-formula id="pcbi.1003512.e341"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e341" xlink:type="simple"/></disp-formula>From Definition 10 of volumes of representation, the order-<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e342" xlink:type="simple"/></inline-formula> volume generated by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e343" xlink:type="simple"/></inline-formula> is the set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e344" xlink:type="simple"/></inline-formula>. Due to the compactness of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e345" xlink:type="simple"/></inline-formula> and the continuity of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e346" xlink:type="simple"/></inline-formula> is compact for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e347" xlink:type="simple"/></inline-formula>. Moreover, due to the volumes' inclusion property, the family of compact volumes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e348" xlink:type="simple"/></inline-formula> is nested with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e349" xlink:type="simple"/></inline-formula>. As such, and according to Theorem 1.28 in <xref ref-type="bibr" rid="pcbi.1003512-Kloeden1">[31]</xref>, there exists a nonempty set<disp-formula id="pcbi.1003512.e350"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003512.e350" xlink:type="simple"/></disp-formula>that is both <italic>compact</italic> and <italic>invariant</italic> under <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e351" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e352" xlink:type="simple"/></inline-formula>. It also follows from the compactness of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e353" xlink:type="simple"/></inline-formula> that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e354" xlink:type="simple"/></inline-formula> attracts <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e355" xlink:type="simple"/></inline-formula>, i.e. there exists <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e356" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e357" xlink:type="simple"/></inline-formula>, and since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e358" xlink:type="simple"/></inline-formula> is a subset of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e359" xlink:type="simple"/></inline-formula>, the neighborhood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e360" xlink:type="simple"/></inline-formula> is also a neighborhood of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e361" xlink:type="simple"/></inline-formula>. Hence, the compact and invariant set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e362" xlink:type="simple"/></inline-formula> is an attractor of the discrete-time semi-dynamical system generated by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e363" xlink:type="simple"/></inline-formula> and is a subset of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e364" xlink:type="simple"/></inline-formula>.</p>
</list-item><list-item>
<p>Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e365" xlink:type="simple"/></inline-formula> attracts <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e366" xlink:type="simple"/></inline-formula>, it follows that the basin of attraction of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e367" xlink:type="simple"/></inline-formula> satisfies <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e368" xlink:type="simple"/></inline-formula>. Given a point <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e369" xlink:type="simple"/></inline-formula>, and since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e370" xlink:type="simple"/></inline-formula>, there exists <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e371" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e372" xlink:type="simple"/></inline-formula>, which is a contradiction, since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e373" xlink:type="simple"/></inline-formula>. Ergo, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e374" xlink:type="simple"/></inline-formula> is an empty set, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e375" xlink:type="simple"/></inline-formula> is the basin of attraction of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e376" xlink:type="simple"/></inline-formula>.</p>
</list-item></list>
<p>This theorem allows us to characterize the properties and relations between autonomous and nonautonomous attractors of <monospace>kWTA</monospace> networks, where all subsets of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e377" xlink:type="simple"/></inline-formula> are compact due to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e378" xlink:type="simple"/></inline-formula>'s finiteness and discreteness. Namely, it allows us, within some compact basin, to allocate the nonautonomous attractor's t-fibers as subsets of the t-fibers of the perturbation set, and it shows that the autonomous attractor of the input at time <italic>t</italic> is the t-fiber of the nonautonomous attractor.</p>
</sec><sec id="s4m">
<title>Input-Insensitive Dynamics</title>
<p>It is possible for a process to behave locally or globally as an autonomous (semi-)dynamical system. That is equivalent, in the case of input-driven dynamical systems, to being input-insensitive.</p>
<p><bold>Definition 12.</bold> Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e379" xlink:type="simple"/></inline-formula> be a discrete-time input-driven dynamical system generated by the family of autonomous difference equations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e380" xlink:type="simple"/></inline-formula> on a metric space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e381" xlink:type="simple"/></inline-formula>. A state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e382" xlink:type="simple"/></inline-formula> is said to be <italic>input-insensitive</italic> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e383" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e384" xlink:type="simple"/></inline-formula>. An <italic>input-insensitive basin</italic> is a basin of attraction that consists entirely of input-insensitive states.</p>
<p>This definition implies that the volumes of representation of a particular order and the t-fibers of each nonautonomous set within this basin are equivalent, including the perturbation set and the nonautonomous attractor: they reduce to autonomous sets. The <italic>input-insensitive attractor</italic> becomes the autonomous attractor of each discrete-time semi-dynamical system generated by a difference equation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e385" xlink:type="simple"/></inline-formula>.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003512.s001" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003512.s001" position="float" xlink:type="simple"><label>Figure S1</label><caption>
<p><bold>Approximating volumes of representation using percentiles.</bold> (A) Percentile approximation of the order-1 volumes of representation of a <monospace>SIP-RN</monospace>. (B) Percentile approximation of the order-2 volumes of representation of a <monospace>SIP-RN</monospace>. Order-2 volumes are more exact approximations to the order-1 volumes according to the volumes' inclusion property. The correspondence is clarified by using similar color coding. (A,B) This approximation is done as follows. After transforming the network states to the principal components space, the coordinates of the first three principal components belonging to each volume of representation are first bootstrapped to 10000 samples, and the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e386" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e387" xlink:type="simple"/></inline-formula> percentiles are computed. Each volume is then approximated by an ellipsoid whose semi-axes extend to these percentiles and is centered at their average. This alternative approximation is less liberal than the one that uses means and standard deviations in that it extends the ellipsoids to assure including more true positives, but at the expense of including more false positives. One still sees, however, that the observations from the other approximation still hold, namely, that volumes of representation are both redundant and separate from one another.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003512.s002" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003512.s002" position="float" xlink:type="simple"><label>Figure S2</label><caption>
<p><bold>Volumes of representation of a nonlinear function over input sequences.</bold> Approximation of order-3 volumes of representation of the task <monospace>Parity-3</monospace> binary input to a <monospace>SIP-RN</monospace>. By an appropriate union of these volumes, the volumes of representation of the outcome 0 (green) and 1 (orange) are identified. The approximation uses the mean and standard deviation of the coordinates. While the first three principal components are sufficient for showing distinct order-3 volumes of representation, more dimensions are necessary to illustrate separate volumes of the outcome of the nonlinear function. The separability of the function's outcomes explains the ability of optimal linear classifiers to successfully perform the nonlinear task.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003512.s003" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003512.s003" position="float" xlink:type="simple"><label>Figure S3</label><caption>
<p><bold>Average classification performance using the Hamming distance of the network states from the vertexes of autonomous attractors.</bold> 100 networks are trained by <monospace>STDP</monospace> and <monospace>IP</monospace> simultaneously on (A) the memory task <monospace>RAND x 4</monospace>, (B) the prediction task <monospace>Markov-85</monospace>, and (C) the nonlinear task <monospace>Parity-3</monospace>. Given the input set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e388" xlink:type="simple"/></inline-formula>, and the family of discrete-time autonomous semi-dynamical systems generating these networks <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e389" xlink:type="simple"/></inline-formula>, the network states comprising the autonomous attractor (the attractor's vertexes) are identified as follows. First, initial conditions are selected within the input-sensitive basin of attraction. Second, the input is clamped to one member of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e390" xlink:type="simple"/></inline-formula>. Third, the solution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003512.e391" xlink:type="simple"/></inline-formula> is generated for a sufficient number of time steps, so that the dynamics, following a transient period, converges to the attractor. Training and testing optimal linear classifiers is carried through as before. The training and testing data is, however, the Hamming distance between the network states and the vertexes of the attractors. Error bars indicate standard error of the mean. The red line marks chance level. The <italic>x</italic>-axis shows the input time-lag. Negative time-lags indicate the past, and positive ones, the future.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003512.s004" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003512.s004" position="float" xlink:type="simple"><label>Figure S4</label><caption>
<p><bold>Average classification performance of networks combining the weights of <monospace>SP-RN</monospace>s and thresholds of <monospace>IP-RN</monospace>s.</bold> 100 networks are trained by <monospace>STDP</monospace> and <monospace>IP</monospace> simultaneously (orange), <monospace>IP</monospace> alone (blue), or trained by <monospace>STDP</monospace> alone followed by injecting the thresholds resulting from <monospace>IP</monospace> at the end of the plasticity phase (green) on (A) the memory task <monospace>RAND x 4</monospace>, (B) the prediction task <monospace>Markov-85</monospace>, and (C) the nonlinear task <monospace>Parity-3</monospace>. The combined networks (green) lack the contribution of the interaction between synaptic and intrinsic plasticity during the plasticity phase. This results in their performance being inferior to the networks where synaptic and intrinsic plasticity interact. Error bars indicate standard error of the mean. The red line marks chance level. The <italic>x</italic>-axis shows the input time-lag. Negative time-lags indicate the past, and positive ones, the future.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003512.s005" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003512.s005" position="float" xlink:type="simple"><label>Text S1</label><caption>
<p><bold>Comparing nonplastic networks.</bold></p>
<p>(PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003512.s006" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003512.s006" position="float" xlink:type="simple"><label>Text S2</label><caption>
<p><bold>Long-term behavior of learning.</bold></p>
<p>(PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003512.s007" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003512.s007" position="float" xlink:type="simple"><label>Text S3</label><caption>
<p><bold>Definitions of nonautonomous dynamical systems.</bold></p>
<p>(PDF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>Discussions with Johannes Schumacher are gratefully acknowledged.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1003512-Cooper1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cooper</surname><given-names>LN</given-names></name>, <name name-style="western"><surname>Intrator</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Blais</surname><given-names>BS</given-names></name>, <name name-style="western"><surname>Shouval</surname><given-names>HZ</given-names></name> (<year>2004</year>) <article-title>Theory of cortical plasticity</article-title>. <source>World Scientific</source></mixed-citation>
</ref>
<ref id="pcbi.1003512-Broome1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Broome</surname><given-names>BM</given-names></name>, <name name-style="western"><surname>Jayaraman</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Laurent</surname><given-names>G</given-names></name> (<year>2006</year>) <article-title>Encoding and decoding of overlapping odor sequences</article-title>. <source>Neuron</source> <volume>51</volume>: <fpage>467</fpage>–<lpage>482</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Buonomano1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buonomano</surname><given-names>DV</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2009</year>) <article-title>State-dependent computations: spatiotemporal processing in cortical networks</article-title>. <source>Nat Rev Neurosci</source> <volume>10</volume>: <fpage>113</fpage>–<lpage>125</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Nikoli1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nikolić</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Häusler</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Singer</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2009</year>) <article-title>Distributed fading memory for stimulus properties in the primary visual cortex</article-title>. <source>PLoS Biol</source> <volume>7</volume>: <fpage>e1000260</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Abbott1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name>, <name name-style="western"><surname>Nelson</surname><given-names>SB</given-names></name> (<year>2000</year>) <article-title>Synaptic plasticity: taming the beast</article-title>. <source>Nat Neurosci</source> <volume>3</volume>: <fpage>1178</fpage>–<lpage>1183</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Zhang1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Linden</surname><given-names>DJ</given-names></name> (<year>2003</year>) <article-title>The other side of the engram: experience-driven changes in neuronal intrinsic excitability</article-title>. <source>Nat Rev Neurosci</source> <volume>4</volume>: <fpage>885</fpage>–<lpage>900</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Turrigiano1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name>, <name name-style="western"><surname>Nelson</surname><given-names>SB</given-names></name> (<year>2004</year>) <article-title>Homeostatic plasticity in the developing nervous system</article-title>. <source>Nat Rev Neurosci</source> <volume>5</volume>: <fpage>97</fpage>–<lpage>107</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Rumelhart1"><label>8</label>
<mixed-citation publication-type="book" xlink:type="simple">Rumelhart DE, McClelland JL (1986) Parallel distributed processing: explorations in the microstructure of cognition. 2 volumes. Cambridge, MA: MIT Press.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Izhikevich1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Izhikevich</surname><given-names>EM</given-names></name> (<year>2006</year>) <article-title>Polychronization: computation with spikes</article-title>. <source>Neural Comput</source> <volume>18</volume>: <fpage>245</fpage>–<lpage>282</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Shadlen1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shadlen</surname><given-names>MN</given-names></name>, <name name-style="western"><surname>Newsome</surname><given-names>WT</given-names></name> (<year>1994</year>) <article-title>Noise, neural codes and cortical organization</article-title>. <source>Curr Opin Neurobiol</source> <volume>4</volume>: <fpage>569</fpage>–<lpage>579</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Gerstner1"><label>11</label>
<mixed-citation publication-type="book" xlink:type="simple">Gerstner W, Kistler WM (2002) Spiking neuron models: Single neurons, populations, plasticity. Cambridge University Press.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Stein1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stein</surname><given-names>RB</given-names></name>, <name name-style="western"><surname>Gossen</surname><given-names>ER</given-names></name>, <name name-style="western"><surname>Jones</surname><given-names>KE</given-names></name> (<year>2005</year>) <article-title>Neuronal variability: noise or part of the signal?</article-title> <source>Nat Rev Neurosci</source> <volume>6</volume>: <fpage>389</fpage>–<lpage>397</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Rolls1"><label>13</label>
<mixed-citation publication-type="book" xlink:type="simple">Rolls ET, Deco G (2010) The noisy brain: stochastic dynamics as a principle of brain function. Volume 28. New York: Oxford University Press.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Bienenstock1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bienenstock</surname><given-names>EL</given-names></name>, <name name-style="western"><surname>Cooper</surname><given-names>LN</given-names></name>, <name name-style="western"><surname>Munro</surname><given-names>PW</given-names></name> (<year>1982</year>) <article-title>Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex</article-title>. <source>J Neurosci</source> <volume>2</volume>: <fpage>32</fpage>–<lpage>48</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Triesch1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Triesch</surname><given-names>J</given-names></name> (<year>2007</year>) <article-title>Synergies between intrinsic and synaptic plasticity mechanisms</article-title>. <source>Neural Comput</source> <volume>19</volume>: <fpage>885</fpage>–<lpage>909</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Toyoizumi1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Toyoizumi</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Pfister</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Aihara</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2005</year>) <article-title>Generalized bienenstock–cooper–munro rule for spiking neurons that maximizes information transmission</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>102</volume>: <fpage>5239</fpage>–<lpage>5244</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Savin1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Savin</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Joshi</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Triesch</surname><given-names>J</given-names></name> (<year>2010</year>) <article-title>Independent component analysis in spiking neurons</article-title>. <source>PLoS Comput Biol</source> <volume>6</volume>: <fpage>e1000757</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Clopath1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clopath</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Büsing</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Vasilaki</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2010</year>) <article-title>Connectivity reects coding: a model of voltage-based <monospace>STDP</monospace> with homeostasis</article-title>. <source>Nat Neurosci</source> <volume>13</volume>: <fpage>344</fpage>–<lpage>352</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Buesing1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buesing</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2010</year>) <article-title>A spiking neuron as information bottleneck</article-title>. <source>Neural Comput</source> <volume>22</volume>: <fpage>1961</fpage>–<lpage>1992</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Douglas1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Douglas</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Martin</surname><given-names>K</given-names></name> (<year>1991</year>) <article-title>A functional microcircuit for cat visual cortex</article-title>. <source>J Physiol</source> <volume>440</volume>: <fpage>735</fpage>–<lpage>769</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Douglas2"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Douglas</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Koch</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Mahowald</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Martin</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Suarez</surname><given-names>HH</given-names></name> (<year>1995</year>) <article-title>Recurrent excitation in neocortical circuits</article-title>. <source>Science</source> <volume>269</volume>: <fpage>981</fpage>–<lpage>985</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Douglas3"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Douglas</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Martin</surname><given-names>KA</given-names></name> (<year>2004</year>) <article-title>Neuronal circuits of the neocortex</article-title>. <source>Annu Rev Neurosci</source> <volume>27</volume>: <fpage>419</fpage>–<lpage>451</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Weiler1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Weiler</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Wood</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Yu</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Solla</surname><given-names>SA</given-names></name>, <name name-style="western"><surname>Shepherd</surname><given-names>GM</given-names></name> (<year>2008</year>) <article-title>Top-down laminar organization of the excitatory network in motor cortex</article-title>. <source>Nat Neurosci</source> <volume>11</volume>: <fpage>360</fpage>–<lpage>366</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-DeAngelis1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DeAngelis</surname><given-names>GC</given-names></name>, <name name-style="western"><surname>Ohzawa</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Freeman</surname><given-names>R</given-names></name> (<year>1993</year>) <article-title>Spatiotemporal organization of simple-cell receptive fields in the cat's striate cortex. ii. linearity of temporal and spatial summation</article-title>. <source>J Neurophysiol</source> <volume>69</volume>: <fpage>1118</fpage>–<lpage>1135</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-ChristopherdeCharms1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Christopher deCharms</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Blake</surname><given-names>DT</given-names></name>, <name name-style="western"><surname>Merzenich</surname><given-names>MM</given-names></name> (<year>1998</year>) <article-title>Optimizing sound features for cortical neurons</article-title>. <source>Science</source> <volume>280</volume>: <fpage>1439</fpage>–<lpage>1444</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Mauk1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mauk</surname><given-names>MD</given-names></name>, <name name-style="western"><surname>Buonomano</surname><given-names>DV</given-names></name> (<year>2004</year>) <article-title>The neural basis of temporal processing</article-title>. <source>Annu Rev Neurosci</source> <volume>27</volume>: <fpage>307</fpage>–<lpage>340</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Lazar1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lazar</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Pipa</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Triesch</surname><given-names>J</given-names></name> (<year>2007</year>) <article-title>Fading memory and time series prediction in recurrent networks with different forms of plasticity</article-title>. <source>Neural Netw</source> <volume>20</volume>: <fpage>312</fpage>–<lpage>322</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Lazar2"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lazar</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Pipa</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Triesch</surname><given-names>J</given-names></name> (<year>2009</year>) <article-title>Sorn: a self-organizing recurrent neural network</article-title>. <source>Front Comput Neurosci</source> <volume>3</volume>: <fpage>23</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Jaeger1"><label>29</label>
<mixed-citation publication-type="book" xlink:type="simple">Jaeger H (2001) The “echo state” approach to analysing and training recurrent neural networks. Techn. rep. GMD 148, Bremen: German National Research Center for Information Technology.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Maass1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Natschläger</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Markram</surname><given-names>H</given-names></name> (<year>2002</year>) <article-title>Real-time computing without stable states: A new framework for neural computation based on perturbations</article-title>. <source>Neural Comput</source> <volume>14</volume>: <fpage>2531</fpage>–<lpage>2560</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Kloeden1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kloeden</surname><given-names>PE</given-names></name>, <name name-style="western"><surname>Rasmussen</surname><given-names>M</given-names></name> (<year>2011</year>) <article-title>Nonautonomous dynamical systems</article-title>. <source>AMS Bookstore</source></mixed-citation>
</ref>
<ref id="pcbi.1003512-Kloeden2"><label>32</label>
<mixed-citation publication-type="book" xlink:type="simple">Kloeden PE, Pötzsche C, Rasmussen M (2013) Discrete-time nonautonomous dynamical systems. In: Johnson R, Pera M, editors. Stability and Bifurcation Theory for Non-Autonomous Differential Equations. Springer. pp. 35–102.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Cover1"><label>33</label>
<mixed-citation publication-type="book" xlink:type="simple">Cover TM, Thomas JA (2006) Elements of information theory. 2nd edition. John Wiley &amp; Sons,.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Strogatz1"><label>34</label>
<mixed-citation publication-type="book" xlink:type="simple">Strogatz SH (2001) Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering (studies in nonlinearity). Westview Press.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Brunel1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name> (<year>2000</year>) <article-title>Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons</article-title>. <source>J Comput Neurosci</source> <volume>8</volume>: <fpage>183</fpage>–<lpage>208</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Hopfield1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name> (<year>1982</year>) <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>79</volume>: <fpage>2554</fpage>–<lpage>2558</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Pasemann1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pasemann</surname><given-names>F</given-names></name> (<year>1995</year>) <article-title>Characterization of periodic attractors in neural ring networks</article-title>. <source>Neural Netw</source> <volume>8</volume>: <fpage>421</fpage>–<lpage>429</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Rabinovich1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rabinovich</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Volkovskii</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Lecanda</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Huerta</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Abarbanel</surname><given-names>H</given-names></name>, <etal>et al</etal>. (<year>2001</year>) <article-title>Dynamical encoding by networks of competing neuron groups: winnerless competition</article-title>. <source>Phys Rev Lett</source> <volume>87</volume>: <fpage>068102</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Rabinovich2"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rabinovich</surname><given-names>MI</given-names></name>, <name name-style="western"><surname>Huerta</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Varona</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Afraimovich</surname><given-names>VS</given-names></name> (<year>2008</year>) <article-title>Transient cognitive dynamics, metastability, and decision making</article-title>. <source>PLoS Comput Biol</source> <volume>4</volume>: <fpage>e1000072</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Timme1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Timme</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Wolf</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Geisel</surname><given-names>T</given-names></name> (<year>2002</year>) <article-title>Prevalence of unstable attractors in networks of pulse-coupled oscillators</article-title>. <source>Phys Rev Lett</source> <volume>89</volume>: <fpage>154105</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Skarda1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Skarda</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Freeman</surname><given-names>WJ</given-names></name> (<year>1987</year>) <article-title>How brains make chaos in order to make sense of the world</article-title>. <source>Behav Brain Sci</source> <volume>10</volume>: <fpage>161</fpage>–<lpage>195</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Gros1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gros</surname><given-names>C</given-names></name> (<year>2009</year>) <article-title>Cognitive computation with autonomously active neural networks: an emerging field</article-title>. <source>Cognit Comput</source> <volume>1</volume>: <fpage>77</fpage>–<lpage>90</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Markovi1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marković</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Gros</surname><given-names>C</given-names></name> (<year>2012</year>) <article-title>Intrinsic adaptation in autonomous recurrent neural networks</article-title>. <source>Neural Comput</source> <volume>24</volume>: <fpage>523</fpage>–<lpage>540</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Negrello1"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Negrello</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Pasemann</surname><given-names>F</given-names></name> (<year>2008</year>) <article-title>Attractor landscapes and active tracking: The neurodynamics of embodied action</article-title>. <source>Adapt Behav</source> <volume>16</volume>: <fpage>196</fpage>–<lpage>216</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Pascanu1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pascanu</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Jaeger</surname><given-names>H</given-names></name> (<year>2011</year>) <article-title>A neurodynamical model for working memory</article-title>. <source>Neural Netw</source> <volume>24</volume>: <fpage>199</fpage>–<lpage>207</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Manjunath1"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Manjunath</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Jaeger</surname><given-names>H</given-names></name> (<year>2013</year>) <article-title>Echo state property linked to an input: Exploring a fundamental characteristic of recurrent neural networks</article-title>. <source>Neural Comput</source> <volume>25</volume>: <fpage>671</fpage>–<lpage>696</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Panzeri1"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Panzeri</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Schultz</surname><given-names>SR</given-names></name>, <name name-style="western"><surname>Treves</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name> (<year>1999</year>) <article-title>Correlations and the encoding of information in the nervous system</article-title>. <source>Proc R Soc Lond B Biol Sci</source> <volume>266</volume>: <fpage>1001</fpage>–<lpage>1012</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Narayanan1"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Narayanan</surname><given-names>NS</given-names></name>, <name name-style="western"><surname>Kimchi</surname><given-names>EY</given-names></name>, <name name-style="western"><surname>Laubach</surname><given-names>M</given-names></name> (<year>2005</year>) <article-title>Redundancy and synergy of neuronal ensembles in motor cortex</article-title>. <source>J neuroscie</source> <volume>25</volume>: <fpage>4207</fpage>–<lpage>4216</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Puchalla1"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Puchalla</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Berry</surname><given-names>MJ</given-names></name> (<year>2005</year>) <article-title>Redundancy in the population code of the retina</article-title>. <source>Neuron</source> <volume>46</volume>: <fpage>493</fpage>–<lpage>504</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Chechik1"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chechik</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Anderson</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Bar-Yosef</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Young</surname><given-names>ED</given-names></name>, <name name-style="western"><surname>Tishby</surname><given-names>N</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>Reduction of information redundancy in the ascending auditory pathway</article-title>. <source>Neuron</source> <volume>51</volume>: <fpage>359</fpage>–<lpage>368</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Tkaik1"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tkačik</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Prentice</surname><given-names>JS</given-names></name>, <name name-style="western"><surname>Balasubramanian</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Schneidman</surname><given-names>E</given-names></name> (<year>2010</year>) <article-title>Optimal population coding by noisy spiking neurons</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>107</volume>: <fpage>14419</fpage>–<lpage>14424</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Kenet1"><label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kenet</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Bibitchkov</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Grinvald</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Arieli</surname><given-names>A</given-names></name> (<year>2003</year>) <article-title>Spontaneously emerging cortical representations of visual attributes</article-title>. <source>Nature</source> <volume>425</volume>: <fpage>954</fpage>–<lpage>956</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Fiser1"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fiser</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Chiu</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Weliky</surname><given-names>M</given-names></name> (<year>2004</year>) <article-title>Small modulation of ongoing cortical dynamics by sensory input during natural vision</article-title>. <source>Nature</source> <volume>431</volume>: <fpage>573</fpage>–<lpage>578</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Pfister1"><label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pfister</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Tass</surname><given-names>PA</given-names></name> (<year>2010</year>) <article-title><monospace>STDP</monospace> in oscillatory recurrent networks: theoretical conditions for desynchronization and applications to deep brain stimulation</article-title>. <source>Front Comput Neurosci</source> <volume>4: pii</volume>: <fpage>22</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Brea1"><label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brea</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Senn</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Pfister</surname><given-names>JP</given-names></name> (<year>2013</year>) <article-title>Matching recall and storage in sequence learning with spiking neural networks</article-title>. <source>J Neurosci</source> <volume>33</volume>: <fpage>9565</fpage>–<lpage>9575</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Markram1"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Markram</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Lübke</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Frotscher</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Sakmann</surname><given-names>B</given-names></name> (<year>1997</year>) <article-title>Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs</article-title>. <source>Science</source> <volume>275</volume>: <fpage>213</fpage>–<lpage>215</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Song1"><label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Song</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Miller</surname><given-names>KD</given-names></name>, <name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name> (<year>2000</year>) <article-title>Competitive hebbian learning through spike-timingdependent synaptic plasticity</article-title>. <source>Nat Neurosci</source> <volume>3</volume>: <fpage>919</fpage>–<lpage>926</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003512-Kraskov1"><label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kraskov</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Stögbauer</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Grassberger</surname><given-names>P</given-names></name> (<year>2004</year>) <article-title>Estimating mutual information</article-title>. <source>Phys Rev E Stat Nonlin Soft Matter Phys</source> <volume>69</volume>: <fpage>066138</fpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>