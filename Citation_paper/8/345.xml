<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group>
<journal-title>PLoS ONE</journal-title></journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-13-40860</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0095848</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Consciousness</subject></subj-group></subj-group><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group><subj-group><subject>Cognition</subject></subj-group></subj-group><subj-group><subject>Sensory systems</subject><subj-group><subject>Auditory system</subject><subject>Visual system</subject></subj-group></subj-group><subj-group><subject>Sensory perception</subject></subj-group></subj-group><subj-group><subject>Psychology</subject><subj-group><subject>Behavior</subject><subj-group><subject>Human performance</subject></subj-group></subj-group><subj-group><subject>Experimental psychology</subject><subject>Psychometrics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Social sciences</subject></subj-group></article-categories>
<title-group>
<article-title>Categorization of Natural Dynamic Audiovisual Scenes</article-title>
<alt-title alt-title-type="running-head">Natural Scene Categorization</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Rummukainen</surname><given-names>Olli</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Radun</surname><given-names>Jenni</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Virtanen</surname><given-names>Toni</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Pulkki</surname><given-names>Ville</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Department of Signal Processing and Acoustics, Aalto University, Espoo, Finland</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Institute of Behavioural Sciences, University of Helsinki, Helsinki, Finland</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Murray</surname><given-names>Micah M.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Centre Hospitalier Universitaire Vaudois and University of Lausanne, Switzerland</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">olli.rummukainen@aalto.fi</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: OR JR TV VP. Performed the experiments: OR. Analyzed the data: OR JR TV VP. Contributed reagents/materials/analysis tools: OR JR TV VP. Wrote the paper: OR JR TV VP.</p></fn>
</author-notes>
<pub-date pub-type="collection"><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>1</day><month>5</month><year>2014</year></pub-date>
<volume>9</volume>
<issue>5</issue>
<elocation-id>e95848</elocation-id>
<history>
<date date-type="received"><day>7</day><month>10</month><year>2013</year></date>
<date date-type="accepted"><day>31</day><month>3</month><year>2014</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Rummukainen et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>This work analyzed the perceptual attributes of natural dynamic audiovisual scenes. We presented thirty participants with 19 natural scenes in a similarity categorization task, followed by a semi-structured interview. The scenes were reproduced with an immersive audiovisual display. Natural scene perception has been studied mainly with unimodal settings, which have identified motion as one of the most salient attributes related to visual scenes, and sound intensity along with pitch trajectories related to auditory scenes. However, controlled laboratory experiments with natural multimodal stimuli are still scarce. Our results show that humans pay attention to similar perceptual attributes in natural scenes, and a two-dimensional perceptual map of the stimulus scenes and perceptual attributes was obtained in this work. The exploratory results show the amount of movement, perceived noisiness, and eventfulness of the scene to be the most important perceptual attributes in naturalistically reproduced real-world urban environments. We found the scene gist properties openness and expansion to remain as important factors in scenes with no salient auditory or visual events. We propose that the study of scene perception should move forward to understand better the processes behind multimodal scene processing in real-world environments. We publish our stimulus scenes as spherical video recordings and sound field recordings in a publicly available database.</p>
</abstract>
<funding-group><funding-statement>The research leading to these results has received funding from the European Research Council under the European Community's Seventh Framework Programme (FP7/2007-2013)/ERC grant agreement No. 240453 and from the Academy of Finland. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="14"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>A real-world scene can be considered as spatial and temporal context that can be found in real life, where events, including human action, can occur. The human brain is constantly selecting and combining information from different sensory channels to structure the external physical environment, but the amount of sensory stimulation is too large to be fully processed by our limited neural resources. A bottom-up process is thought to control the early stages of scene processing, where sensory information is selected for further analysis based on perceptual saliency <xref ref-type="bibr" rid="pone.0095848-Yantis1">[1]</xref>. In real-world scenes, however, top-down task-dependent information is also biasing the sensory selection process <xref ref-type="bibr" rid="pone.0095848-Sanocki1">[2]</xref>. We live in a continuous world where we know where we are, where we came from, and what we are going to do next. We are also experts in perceiving our everyday surroundings, where the sensory information from different modalities integrates seamlessly together and corresponds to our expectations of the present situation. These conditions are hard to replicate in a controlled laboratory environment, and thus investigating real-world audiovisual scene processing in an ecologically valid setting is difficult. In the study at hand we strive for high ecological validity by employing surrounding visual projections along with spatial audio reproduction, and examine audiovisual scene processing in urban environments. The aim is to study the perceptual dimensionality of natural scene processing and to achieve a starting point for more detailed modeling of multimodal scene processing in real-world environments. By natural scene we refer to a reproduced real-world scene that could be encountered in our everyday lives, instead of artificial stimulation such as simplified visual shapes and pure tones.</p>
<p>The perception of real-world scenes has been studied by analyzing saliency maps, the gist in the visual domain, and the soundscape in the auditory domain. Unimodal saliency maps have been shown to accurately predict the focus of attention especially in the visual domain <xref ref-type="bibr" rid="pone.0095848-Itti1">[3]</xref>–<xref ref-type="bibr" rid="pone.0095848-Carmi1">[5]</xref>, but also auditory saliency can be modeled <xref ref-type="bibr" rid="pone.0095848-Kayser1">[6]</xref>. A saliency map decomposes stimuli into features that are most meaningful for a human perceiver. In addition to visual saliency models, visual scene gist can be modeled computationally <xref ref-type="bibr" rid="pone.0095848-Oliva1">[7]</xref>–<xref ref-type="bibr" rid="pone.0095848-Greene1">[9]</xref>. Visual gist refers to a holistic mental representation of our surroundings, for example we are able to determine very rapidly whether we are in an open or closed space based on visual information, even before any impact from focused attention <xref ref-type="bibr" rid="pone.0095848-Potter1">[10]</xref>–<xref ref-type="bibr" rid="pone.0095848-FeiFei1">[13]</xref>. The auditory scene is analyzed in perceptual streams linked to sound events, where relevant sound information is integrated to a single stream according to its time-frequency domain properties <xref ref-type="bibr" rid="pone.0095848-Bregman1">[14]</xref>. In real-world environments, where many sound events occur simultaneously and in conjunction with events in other modalities, the auditory scene analysis based solely on auditory stimulus features becomes challenging, and the listener's goals and subjective reasoning become more influential in the stream formation process <xref ref-type="bibr" rid="pone.0095848-Botteldooren1">[15]</xref>, <xref ref-type="bibr" rid="pone.0095848-Shamma1">[16]</xref>.</p>
<p>Considering the perception of real-world environments, there is no clear knowledge about what environmental sounds people actually hear when they are not consciously listening for a particular sound <xref ref-type="bibr" rid="pone.0095848-Botteldooren1">[15]</xref>. The effect of attention and other sensory modalities, most importantly vision, on the auditory scene analysis is a little known area. Visual stimulation can bias the auditory perception <xref ref-type="bibr" rid="pone.0095848-Bonebright1">[17]</xref>, and vice versa <xref ref-type="bibr" rid="pone.0095848-Alais1">[18]</xref>, making the use of unimodal saliency modeling problematic. Recently, attention has been shown to have an effect on auditory streaming as well <xref ref-type="bibr" rid="pone.0095848-Shamma1">[16]</xref>, <xref ref-type="bibr" rid="pone.0095848-Botteldooren2">[19]</xref>, which would imply that the auditory scene can be structured differently based on the focus of attention. The study of urban soundscapes has revealed two generic cognitive categories: event sequences and amorphous sequences, referring respectively to soundscapes where individual sounds can or cannot be easily distinguished within the soundscape <xref ref-type="bibr" rid="pone.0095848-Dubois1">[20]</xref>. It appears that auditory stimuli are processed preferably as parts of a meaningful event in perceptual streams, or secondarily, in a more abstract manner along physical parameters if source identification fails. Recognizing the category of a complex soundscape (i.e. is the listener in a park or in a cafe) has been found to rely heavily on identification of the sound sources in the scene and inferring the category from the source information, while neglecting spatial information <xref ref-type="bibr" rid="pone.0095848-Peltonen1">[21]</xref>. Sound has also a strong emotional impact on humans. The most prominent emotional dimensions of a soundscape experience have been identified as the perceived pleasantness and induced arousal <xref ref-type="bibr" rid="pone.0095848-Axelsson1">[22]</xref>, <xref ref-type="bibr" rid="pone.0095848-Davies1">[23]</xref>. These are often related to the amount of natural versus mechanical sounds, natural sounds being perceived as pleasant and mechanical sounds as unpleasant and arousing.</p>
<p>Combining auditory and visual information seems to be beneficial for human perception in speech comprehension <xref ref-type="bibr" rid="pone.0095848-McGurk1">[24]</xref>, spatial orienting efficiency <xref ref-type="bibr" rid="pone.0095848-Spence1">[25]</xref>, <xref ref-type="bibr" rid="pone.0095848-Nardo1">[26]</xref>, and spatial localization accuracy <xref ref-type="bibr" rid="pone.0095848-Alais1">[18]</xref>. Both covert and overt attention are directed differently when we encounter audiovisual stimuli in contrast to purely visual stimulation <xref ref-type="bibr" rid="pone.0095848-Spence1">[25]</xref>, <xref ref-type="bibr" rid="pone.0095848-Coutrot1">[27]</xref>, <xref ref-type="bibr" rid="pone.0095848-V1">[28]</xref>. Computational modeling of audiovisual saliency is also attempted for restricted scenes, where sound has been found as a modulating factor for visual saliency <xref ref-type="bibr" rid="pone.0095848-Coutrot2">[29]</xref>, or audiovisual saliency has resulted from a linear combination of unimodal saliences <xref ref-type="bibr" rid="pone.0095848-Quigley1">[30]</xref>. From a wider viewpoint, evidence for bimodal effects on real-world scene perception have been presented in the field of environmental psychology, where tranquility has been shown to arise from a combination of naturalness in the visual world and the overall loudness and naturalness of the soundscape <xref ref-type="bibr" rid="pone.0095848-Pheasant1">[31]</xref>, <xref ref-type="bibr" rid="pone.0095848-Pheasant2">[32]</xref>.</p>
<p>Finally, the applicability of unimodal saliency maps to real-world audiovisual scenes seems questionable without taking into account the integration of sensory information and its effects on attentional control. Therefore, as already stated, the aim of this study is to further understand the perceptual dimensionality in audiovisual perception of natural scenes. This is done by evoking real-world scene experiences in a laboratory setting through ecologically valid audiovisual reproduction, and asking participants to categorize the scenes based on their perceived similarity. The empirical goal is to acquire a mapping of a diverse set of audiovisual stimuli along with perceptual attributes to study the human interpretation of real-world urban scenes. Our primary interest is to find whether a group of people is able to create a consistent low-dimensional perceptual representation for a set of natural scenes, and what are the perceptual attributes that are referred most often. Furthermore, objective environmental variables related to the loudness, dynamic visual information and indoor vs. outdoor classification are computed from the stimulus scenes, and fitted to the obtained perceptual mapping. Our hypothesis here is that the perceptual map originates from the physical world, instead of, for example, from functional attributes of the depicted spaces, and the subjective perceptual mapping should be predictable by physical properties of the stimuli.</p>
<p>A two-dimensional perceptual map of audiovisual scenes was derived in our study, confirming our hypothesis that a group of people describes natural scenes with similar perceptual attributes. The nature of our study was exploratory and thus we cannot precisely identify the relative contributions of the two sensory channels to the audiovisual scene perception. However, from the perceptual map we can draw evidence that the most important perceptual attributes in natural scenes depicting urban environments are the amount of movement, perceived noisiness, and eventfulness of the scene. Scene gist properties openness and expansion were found to remain as important attributes in scenes with no salient auditory or visual events.</p>
</sec><sec id="s2" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s2a">
<title>Ethics statement</title>
<p>The participants voluntarily registered for the study through an online scheduling program in advance after reading an outline of the study and its aim. In addition, the participants provided verbal informed consent to voluntarily participate in this study at the examination day after receiving detailed information about the contents of the study, and they were aware that they could withdraw from the study at any time. It was also stated, that the gathered data was anonymous and not traceable back to the participants. Only demographic information about the participants was collected. There is a voice recording made of every participants' interview confirming the verbal consent. Therefore, no written informed consent was considered necessary, as outlined in the Finnish Advisory Board on Research Ethics' proposal <xref ref-type="bibr" rid="pone.0095848-Finnish1">[33]</xref>. In addition, the Ethics Review Board of Aalto University was consulted and an ethics review was found unnecessary. The research was conducted in the country of residence of the authors.</p>
</sec><sec id="s2b">
<title>Participants</title>
<p>A total of 30 naive participants took the test. They were recruited through social media and Aalto University's student mailing list. Nine of the participants were female and the average age was 26.5 (SD  = 6.6). All the participants reported to have normal or corrected-to-normal vision and normal hearing. No acuity screening or audiogram measurements were considered necessary, because the participants were supposed to perceive and assess the reproduced environments as they would experience ordinary situations in their everyday lives without relative weighting of the modalities. All the participants, except for one, were familiar with most of the locations of the outdoor scenes in the study, and reported to live or having lived in the Helsinki metropolitan area. Each participant reported having Finnish as their native language. The participants received a movie ticket as a compensation for their contribution. The authors did not participate in the experiment.</p>
</sec><sec id="s2c">
<title>Catalog of environments</title>
    <p>All stimulus scenes were recorded either indoor, or during summertime in an urban environment in the greater-Helsinki area, Finland. The aim was to provide as rich stimulus set as possible with clear perceptual (both auditory and visual) gradients, even though the requirement of finding a real-world environment with desired properties was sometimes challenging. The scene duration is 15 s, and the sequences are constructed so that the scene, and the events taking place in the scene, stay as invariant as possible. For example, in the scene #<italic>Tram</italic>, the passing-by event lasts for the whole duration of the scene. The recording device, and thus the observer, stays static in all the cases. The stimuli used in the experiment are listed in <xref ref-type="table" rid="pone-0095848-t001">Table 1</xref>, along with an <italic>a priori</italic> categorization of the scenes according to their objective environmental attributes. A frame capture of each content is shown in <xref ref-type="fig" rid="pone-0095848-g001">Figure 1</xref>. In addition, the stimulus scenes are available for preview and for full-resolution download at: <ext-link ext-link-type="uri" xlink:href="http://www.acoustics.hut.fi/go/plosone14-avscenes/" xlink:type="simple">http://www.acoustics.hut.fi/go/plosone14-avscenes/</ext-link> (username: <italic>avscenes</italic>, password: <italic>isotVIDEOT</italic>). The scenes #<italic>Cafeteria</italic> and #<italic>Class room</italic> are not published due to limited rights of publication.</p>
<fig id="pone-0095848-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095848.g001</object-id><label>Figure 1</label><caption>
<title>Collage of the visual contents.</title>
<p>Dotted lines denote corners of the screens.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095848.g001" position="float" xlink:type="simple"/></fig><table-wrap id="pone-0095848-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095848.t001</object-id><label>Table 1</label><caption>
<title>Catalog of the scenes and a description of the content.</title>
</caption><alternatives><graphic id="pone-0095848-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095848.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Short name</td>
<td align="left" rowspan="1" colspan="1">Setting</td>
<td align="left" rowspan="1" colspan="1">Atmosphere</td>
<td align="left" rowspan="1" colspan="1">Soundscape</td>
<td align="left" rowspan="1" colspan="1">Description</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Beach</td>
<td align="left" rowspan="1" colspan="1">Outdoor</td>
<td align="left" rowspan="1" colspan="1">Calm</td>
<td align="left" rowspan="1" colspan="1">Quiet, 41 dB</td>
<td align="left" rowspan="1" colspan="1">Expansive view of the sea and quiet</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Bells</td>
<td align="left" rowspan="1" colspan="1">Outdoor</td>
<td align="left" rowspan="1" colspan="1">Calm</td>
<td align="left" rowspan="1" colspan="1">Quiet, 52 dB</td>
<td align="left" rowspan="1" colspan="1">Calm park with church bells ringing</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Cafeteria</td>
<td align="left" rowspan="1" colspan="1">Indoor</td>
<td align="left" rowspan="1" colspan="1">Busy</td>
<td align="left" rowspan="1" colspan="1">Noisy, 64 dB</td>
<td align="left" rowspan="1" colspan="1">Busy elementary school cafeteria</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Class room</td>
<td align="left" rowspan="1" colspan="1">Indoor</td>
<td align="left" rowspan="1" colspan="1">Busy</td>
<td align="left" rowspan="1" colspan="1">Noisy, 63 dB</td>
<td align="left" rowspan="1" colspan="1">Busy elementary school class room</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Cyclists</td>
<td align="left" rowspan="1" colspan="1">Outdoor</td>
<td align="left" rowspan="1" colspan="1">Calm</td>
<td align="left" rowspan="1" colspan="1">Quiet, 48 dB</td>
<td align="left" rowspan="1" colspan="1">Restricted view of a downtown cycle path</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Dishwasher</td>
<td align="left" rowspan="1" colspan="1">Indoor</td>
<td align="left" rowspan="1" colspan="1">Calm</td>
<td align="left" rowspan="1" colspan="1">Noisy, 63 dB</td>
<td align="left" rowspan="1" colspan="1">Close up view of a dishwasher</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Floorball</td>
<td align="left" rowspan="1" colspan="1">Indoor</td>
<td align="left" rowspan="1" colspan="1">Busy</td>
<td align="left" rowspan="1" colspan="1">Quiet, 59 dB</td>
<td align="left" rowspan="1" colspan="1">Indoor hall with a game of floorball</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Hall</td>
<td align="left" rowspan="1" colspan="1">Indoor</td>
<td align="left" rowspan="1" colspan="1">Calm</td>
<td align="left" rowspan="1" colspan="1">Quiet, 54 dB</td>
<td align="left" rowspan="1" colspan="1">Large departure hall with a few people</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Home</td>
<td align="left" rowspan="1" colspan="1">Indoor</td>
<td align="left" rowspan="1" colspan="1">Calm</td>
<td align="left" rowspan="1" colspan="1">Quiet, 41 dB</td>
<td align="left" rowspan="1" colspan="1">Small room with TV on</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Lecture hall</td>
<td align="left" rowspan="1" colspan="1">Indoor</td>
<td align="left" rowspan="1" colspan="1">Calm</td>
<td align="left" rowspan="1" colspan="1">Quiet, 38 dB</td>
<td align="left" rowspan="1" colspan="1">Large and empty lecture hall</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Market square</td>
<td align="left" rowspan="1" colspan="1">Outdoor</td>
<td align="left" rowspan="1" colspan="1">Busy</td>
<td align="left" rowspan="1" colspan="1">Quiet, 58 dB</td>
<td align="left" rowspan="1" colspan="1">Market square with a few stalls and people</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Narrow space</td>
<td align="left" rowspan="1" colspan="1">Outdoor</td>
<td align="left" rowspan="1" colspan="1">Calm</td>
<td align="left" rowspan="1" colspan="1">Quiet, 53 dB</td>
<td align="left" rowspan="1" colspan="1">Space limited by walls but expansive view</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Quiet street</td>
<td align="left" rowspan="1" colspan="1">Outdoor</td>
<td align="left" rowspan="1" colspan="1">Calm</td>
<td align="left" rowspan="1" colspan="1">Quiet, 43 dB</td>
<td align="left" rowspan="1" colspan="1">Quiet street with overhead trees</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Railway station</td>
<td align="left" rowspan="1" colspan="1">Indoor</td>
<td align="left" rowspan="1" colspan="1">Busy</td>
<td align="left" rowspan="1" colspan="1">Noisy, 62 dB</td>
<td align="left" rowspan="1" colspan="1">Large and busy departure hall</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Subway station</td>
<td align="left" rowspan="1" colspan="1">Indoor</td>
<td align="left" rowspan="1" colspan="1">Calm</td>
<td align="left" rowspan="1" colspan="1">Noisy, 58 dB</td>
<td align="left" rowspan="1" colspan="1">Large and calm space with a few people</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Talkers</td>
<td align="left" rowspan="1" colspan="1">Outdoor</td>
<td align="left" rowspan="1" colspan="1">Calm</td>
<td align="left" rowspan="1" colspan="1">Quiet, 54 dB</td>
<td align="left" rowspan="1" colspan="1">Few people talking at a street corner</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Traffic</td>
<td align="left" rowspan="1" colspan="1">Outdoor</td>
<td align="left" rowspan="1" colspan="1">Busy</td>
<td align="left" rowspan="1" colspan="1">Noisy, 71 dB</td>
<td align="left" rowspan="1" colspan="1">Busy street in front of the viewer</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Traffic behind</td>
<td align="left" rowspan="1" colspan="1">Outdoor</td>
<td align="left" rowspan="1" colspan="1">Calm</td>
<td align="left" rowspan="1" colspan="1">Noisy, 59 dB</td>
<td align="left" rowspan="1" colspan="1">Busy street behind the viewer</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Tram</td>
<td align="left" rowspan="1" colspan="1">Outdoor</td>
<td align="left" rowspan="1" colspan="1">Busy</td>
<td align="left" rowspan="1" colspan="1">Noisy, 74 dB</td>
<td align="left" rowspan="1" colspan="1">Loud tram passing close by</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap>
<p>The stimulus selection was based on previous studies of categorization of environmental scenes, where the scenes were divided according to their superordinate, basic and subordinate levels of categorization <xref ref-type="bibr" rid="pone.0095848-Tversky1">[34]</xref>. Each of the scenes in this study can be viewed as depicting a basic level scene, such as home or school, with the dynamic features of the scene determining the subordinate category. International Telecommunication Union's (ITU) recommendation defining the video content categories for subjective quality evaluation <xref ref-type="bibr" rid="pone.0095848-InternationalTelecommunication1">[35]</xref> was used as a guideline when choosing the temporal properties for the scenes.</p>
</sec><sec id="s2d">
<title>Reproduction</title>
<p>The experiment was conducted in an immersive audiovisual environment located at the Department of Signal Processing and Acoustics in Aalto University School of Electrical Engineering. The environment is built inside of an acoustically treated room and consists of three high-definition video projectors producing a horizontal field-of-view of 226° at the viewing position on three acoustically nearly transparent screens. The screens are 2.5×1.88 m each and installed to follow the shape of the base of a pentagon. The display area extends to the ground. Distance from the observation position to the center of each screen is 1.72 m. More detailed information about the technical specifications can be found in <xref ref-type="bibr" rid="pone.0095848-GmezBolaos1">[36]</xref>.</p>
<p>The videos were captured with a recording device capable of producing a spherical video (Point Grey Research: Ladybug 3). The videos were cropped to reproduce a 226° slice of the full circle on the screen, making the visual scene consistent with the auditory scene. The video was recorded and reproduced at 16 frames-per-second and the resolution of the final video was 4320×1080 pixels produced by the three projectors. With this resolution and viewing distance, the inter-line distance is 3.5 arcmin, which affects the sharpness of the image, but the objects in the scene are still easily recognizable.</p>
<p>The audio reproduction system consisted of 29 loudspeakers (Genelec 1029). The loudspeakers were located on a sphere with a 2.1 m radius centered at the observation position. The loudspeaker layout and the projection screen setup are depicted in <xref ref-type="fig" rid="pone-0095848-g002">Figure 2</xref>. The signals to the loudspeakers were derived with Directional Audio Coding (DirAC; <xref ref-type="bibr" rid="pone.0095848-Pulkki1">[37]</xref>, <xref ref-type="bibr" rid="pone.0095848-Politis1">[38]</xref>), which is a recently proposed parametric spatial audio technique. DirAC analyzes and synthesizes the sound field from A-format microphone (Soundfield SPS200) signals recorded from a live situation. The A-format microphone captures the sound field through four near-to-coincident cardioid capsules. DirAC was chosen to be used in the reproduction of spatial sound over the 29-loudspeaker layout, since in subjective testing with comparable layouts it has been found to provide a perception of sound prominently closer to the original perception than the reproduction provided with time-domain techniques that use the same input <xref ref-type="bibr" rid="pone.0095848-Vilkamo1">[39]</xref>. The A-weighted sound pressure level of each stimulus scene was set to match the one at the corresponding recording site.</p>
<fig id="pone-0095848-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095848.g002</object-id><label>Figure 2</label><caption>
<title>Loudspeaker setup and the projection screens.</title>
<p>The observer is seated in the center. Adopted from: <xref ref-type="bibr" rid="pone.0095848-GmezBolaos1">[36]</xref>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095848.g002" position="float" xlink:type="simple"/></fig></sec><sec id="s2e">
<title>Procedure</title>
<p>First, the participants were familiarized with the stimulus scenes by watching and listening the whole list of 19 scenes presented in a random order. Thereafter, the participants were instructed to freely categorize the scenes into three or more groups, and to avoid groups with only single stimulus. The motives for the categories were left for the participants to decide. The categorization had to be based on the perceived similarity of attributes of the reproduced scenes. Examples of bad attributes were said to be forming groups based on personal familiarity with the depicted space or personal significance that the space might have for the participant. In addition, an analogous case of finding discriminating attributes between three orange juices was described to the participants. In such a case, the question is how to describe the juices and maybe recommend one of them to someone who has not tasted them. As one cannot discriminate between the juices by saying that they taste like orange, they should find something essential about the taste. Similarly with the scenes, it is maybe possible to describe a given scene in a way that sets it apart from some and resembles others.</p>
<p>Participants performed the free categorization task on a tablet computer with a touch screen (Apple iPad2). The graphical user interface is depicted in <xref ref-type="fig" rid="pone-0095848-g003">Figure 3</xref>. The stimuli were illustrated on the screen by 100×100 px icons cropped from the video contents. Initially, the icons were randomly distributed on the touch screen and their purpose was to provide a mnemonic about the stimulus content but not to guide the grouping process. In addition to the icons, there was a <italic>play</italic> button that the participants could use to watch the stimuli as many times as necessary. Also, a progress bar was provided. A stimulus was selected for playback by touching the corresponding icon and clicking the play button. The stimulus would then be played once.</p>
<fig id="pone-0095848-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095848.g003</object-id><label>Figure 3</label><caption>
<title>Graphical user interface on touch screen.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095848.g003" position="float" xlink:type="simple"/></fig>
<p>It was stressed to the participants that they were not required to determine a global categorization rule. They should not think of the screen as having x- and y-axis describing some global features, but rather, the screen provided them with a plane on which to build the categories, each with their own similarity attributes. After the participant was content with the acquired categorization, a semi-structured interview took place. All interviews were conducted by the same person (first author). In the interview, the participants were asked to give short descriptive names for the categories, and thereafter, each of the categories was examined by a two-level interview process. For each group, the interviewer first asked why the participant had allocated the chosen stimuli into the given category. The reasons were supposed to be related to the perceived attributes of the scene. Next, the interviewer asked more specifically about the properties of the attributes the participant had mentioned. Basically, the interviewer aimed at finding the underlying reasons affecting the categorization by asking consecutive “why”-questions about the attributes the participant had paid attention to. The interview was conducted in Finnish and the interviewer took notes of the answers. In addition, a voice recording was made of every interview in case the interviewer needed to verify details afterwards. On average, the entire test took 45 minutes to complete (<italic>min</italic>  =  30 min, <italic>max</italic>  =  80 min, SD  =  12 min).</p>
</sec><sec id="s2f">
<title>Statistical analysis methods</title>
<p>We used three different multivariate data analysis methods to inspect different aspects of our datasets, namely hierarchical clustering, multidimensional scaling (MDS), and correspondence analysis (CA). The analysis methods make different assumptions of the underlying data, the hierarchical clustering having the least amount of assumptions, and thus it can be used to verify the following two analyses. The MDS and CA methods use different datasets, that is, the dissimilarity matrices and the interviews. The MDS was necessary to inspect the variation in the participants' categorization strategies, and the CA was important in revealing the perceptual attributes' relation to the stimulus scenes. Finally, having similar overall ordination from the two methods, originating from different datasets, adds more support to the validity of both the solutions. Overall schematic of the analysis methods and the data used in each test is depicted in <xref ref-type="fig" rid="pone-0095848-g004">Figure 4</xref>. The different analysis methods and data preprocessing are introduced in the following.</p>
<fig id="pone-0095848-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095848.g004</object-id><label>Figure 4</label><caption>
<title>Schematic of the analysis methods and the data flow.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095848.g004" position="float" xlink:type="simple"/></fig><sec id="s2f1">
<title>Dissimilarity matrix</title>
<p>The perceived similarity of the scenes was extracted from the individually formed categorizations of the scenes. A symmetric and non-negative dissimilarity matrix with zero diagonal was formed for each participant. The matrix was constructed from pair-wise dissimilarities of the scenes by denoting scenes that were in the same category as having pair-wise dissimilarity value of 0 with each other, and dissimilarity value of 1 with scenes in other categories. An example of such dissimilarity matrix is depicted in <xref ref-type="fig" rid="pone-0095848-g004">Figure 4</xref>.</p>
</sec><sec id="s2f2">
<title>Hierarchical clustering</title>
<p>The idea behind hierarchical clustering is to obtain a baseline interpretation of the high-dimensional dataset. In further analysis, the baseline clustering can be used to validate the more detailed and subjective interpretations by inspecting if the clusters remain separated. In addition, by clustering the stimulus scenes into groups, the interpretation of perceptual maps in subsequent analysis is potentially easier. Overall dissimilarities were calculated by summing the 30 dissimilarity matrices together, and by using Bray-Curtis dissimilarity <xref ref-type="bibr" rid="pone.0095848-Bray1">[40]</xref> to obtain a non-metric dissimilarity value for the scenes from the number of co-occurrences of the same scenes in the same category. Hierarchical clustering is initiated by combining the two most similar scenes (i.e. having the lowest dissimilarity score) into one cluster. Thereafter, the next fusion depends on the chosen clustering algorithm; in this study, the average linkage clustering <xref ref-type="bibr" rid="pone.0095848-Hair1">[41]</xref> was used. In average linkage clustering, the fusions occur between group center points, instead of the nearest or the furthest neighbors as in single linkage or complete linkage approaches. Average linkage approach should ideally produce compact clusters and provide a compromise between single linkage and complete linkage algorithms, which tend to produce chain-like long clusters or be sensitive to outliers, respectively. Finally, the clustering result can be inspected at any level of fusion by cutting the clustering tree (dendrogram) at the desired height.</p>
</sec><sec id="s2f3">
<title>Multidimensional scaling</title>
<p>The dissimilarity matrices were further analyzed by multidimensional scaling (MDS), which is an exploratory data analysis technique well suited for identifying unrecognized dimensions that the participants could have used when making the categorization <xref ref-type="bibr" rid="pone.0095848-Hair1">[41]</xref>. Essentially, MDS transforms subjective scene similarity judgments into distances in a multidimensional perceptual space. The dimensionality of the perceptual space has to be predefined, and typically solutions with different numbers of dimensions are tested to see which produces the smallest amount of stress between the original similarity judgments of the scene pairs and the Euclidean distances in the low-dimensional perceptual map. The MDS algorithm starts by randomly assigning the 19 scenes for example to a two-dimensional space. Next, the goodness-of-fit of the solution is evaluated by comparing the rank-order of the pair-wise Euclidean distances between scenes in the solution to the rank-order of the original similarity judgments. If the rank-orders do not agree, the 19 scene points are moved in the perceptual space to make the distance-based rank-order agree better with the similarity judgments. This process is repeated until a satisfactory fit is achieved between the distances and the similarity judgments. The algorithm is then run again with another dimensionality to see if a higher-dimensional space would produce better goodness-of-fit.</p>
<p>The individual dissimilarity matrices were analyzed using the R <xref ref-type="bibr" rid="pone.0095848-R1">[42]</xref> implementation of the individual differences scaling (INDSCAL) algorithm using iterative stress majorization <xref ref-type="bibr" rid="pone.0095848-deLeeuw1">[43]</xref>, which is a variant of MDS revealing also how similarly the participants were thinking about the dimensions present in the set of stimulus scenes. INDSCAL assumes that all participants share a common perceptual space, but have different individual weightings for the common space dimensions. The individual weights can be plotted in the formed common space, and individual differences analyzed by observing the distances from the axis. Participants positioned close to each other show similar weighting for the common perceptual dimensions. Furthermore, the participant's distance from the origin represents goodness-of-fit. Positions close to the origin have little weight on the common space and are considered to fit poorly to the proposed common solution.</p>
</sec><sec id="s2f4">
<title>Interview coding</title>
<p>The interview data was searched for expressions that could be coded under broader terms. The coding process was based on the grounded theory principle, where a phenomenon is analyzed and a theory formed beginning from the data <xref ref-type="bibr" rid="pone.0095848-Corbin1">[44]</xref>. The coding process introduces a possible error caused by the researcher's interpretation of the interview data. Therefore, the coding process was repeated by an outside researcher on five randomly selected interviews. The re-coding process was based on the interview notes taken by the interviewer. The qualitative analysis was done in Atlas.ti <xref ref-type="bibr" rid="pone.0095848-ATLASti1">[45]</xref>, where, in the first step, 355 unique codes were developed based on the data, and attached to the interview notes. The codes were related to the participants' descriptions of the perceptual attributes of the scenes (i.e., the reasons why certain categories were formed and scenes placed into them). Next, the number of codes was reduced by combining codes with similar meaning together under wider concepts. <xref ref-type="fig" rid="pone-0095848-g005">Figure 5</xref> displays a diagram of how the coding process evolved. The interviews were conducted in Finnish, and the coding was also done in Finnish. As the final step, the obtained codes were translated into English preserving the meaning of the original code.</p>
<fig id="pone-0095848-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095848.g005</object-id><label>Figure 5</label><caption>
<title>Coding process of the interview data.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095848.g005" position="float" xlink:type="simple"/></fig></sec><sec id="s2f5">
<title>Correspondence analysis</title>
<p>Correspondence analysis (CA; <xref ref-type="bibr" rid="pone.0095848-Hair1">[41]</xref>) was performed on the acquired interview data using the R-package <italic>vegan</italic> <xref ref-type="bibr" rid="pone.0095848-Oksanen1">[46]</xref>. In CA, the characterization of meaning is based on verbal ratings of the scenes. The perceptual attributes, elicited through an interview process, are presented in a low-dimensional joint space with the stimulus scenes. For the CA algorithm, a contingency matrix was formed, where the rows corresponded to the 19 scenes, and the columns corresponded to the 16 most frequent interview codes. Next, the number of occurrences of each code in each scene were counted and marked into the matrix. The CA algorithm calculates a similarity measure for the codes in relation to the scenes, quite similarly as MDS used the similarity between scenes, and uses those similarities as the basis for the perceptual map. The strength of the CA is that it is able to display both the scenes and the codes in a joint perceptual space making the interpretation of the results easier.</p>
<p>The CA is able to attenuate the effect of categories formed based on higher-level cognitive reasoning, for example personal significance of a place or knowledge of the social function of the space. This kind of categories would be attached with codes with small number of references, and thereafter they would be ruled out of the actual analysis process, since only the most frequent codes were allowed into the CA model. In effect, the CA should reveal the underlying perceptual dimensions that actually stem from the perceivable physical environment.</p>
</sec></sec></sec><sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Subjective categorization of the scenes</title>
<p>Inspecting the subjective categorizations reveals that the participants formed 5.4 categories on average, with minimum amount of three categories and maximum of eight categories. <xref ref-type="fig" rid="pone-0095848-g006">Figure 6</xref> displays the dendrogram resulting from the average-linkage clustering algorithm. The cluster tree is cut at level with five clusters, reflecting the participants' typical amount of formed categories (mode  = 5). The resulting clusters are highlighted with rectangles. In addition, holistic names describing the prevalent ambience stemming from the scenes in a cluster are presented.</p>
<fig id="pone-0095848-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095848.g006</object-id><label>Figure 6</label><caption>
<title>Dendrogram showing the hierarchical clustering result from average linkage clustering.</title>
<p>The cluster tree is cut at level with five categories, and the resulting clusters are denoted by the rectangles. The clusters are holistically named based on the prevalent ambience of the given scenes.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095848.g006" position="float" xlink:type="simple"/></fig>
<p>Cluster #1 is the largest and contains calm sonic and visual environments, both from indoor and outdoor. Therefore, the holistic name given to Cluster #1 is <italic>Calm</italic>. Cluster #2 includes visually calm outdoor scenes with some ambient sound, well described by the word <italic>Still</italic>. In contrast, Cluster #3 contains noisy outdoor and indoor scenes with modest movement, therefore it is denoted <italic>Noisy</italic>. Cluster #4 consists of large indoor spaces with lots of people, movement and sound, in other words, the cluster is <italic>Vivid</italic>. Finally, Cluster #5 contains both outdoor scenes and large indoor spaces with people, modest movement, and some sonic events. The last cluster is best described as being visually <italic>Open</italic>.</p>
</sec><sec id="s3b">
<title>Dimensionality of the categorization and participant integrity</title>
<p>Individual differences scaling (INDSCAL) models with dimensionalities ranging from two to five were fitted to the data, and the resulting nonmetric stress values were: .32, .16, .09, and .05, respectively. In this study, an INDSCAL model with three dimensions was found to explain the variance of the data with good quality (nonmetric stress  =  .16; <xref ref-type="bibr" rid="pone.0095848-Rabinowitz1">[47]</xref>), and adding a fourth dimension did not provide easily interpretable information. Therefore, a model with three dimensions was chosen as suitable representation of the present data. Given the non-metric dissimilarity data, the mapping from the dissimilarities to distances in the perceptual map is non-linear. The mapping function is shown in <xref ref-type="fig" rid="pone-0095848-g007">Figure 7</xref> which displays the Shepard diagram for the three-dimensional INDSCAL model. The isotonic regression curve in the Shepard diagram does not contain large steps indicating that the chosen model properly maps the non-metric dissimilarities to the three-dimensional common space. The grey circles in the plot show how a judged dissimilarity is mapped to the perceptual space. From the plot we can see that small dissimilarity values correspond well to small distances, and large dissimilarities correspond to large distances in the perceptual map. However, the mapping curve is clearly non-linear stating that absolute distances in the visualization are not accurate, and only relative distances should be inspected. <xref ref-type="fig" rid="pone-0095848-g008">Figure 8</xref> presents the common space perceptual map in two-dimensions, first plot shows Dimensions 1 &amp; 2 and the second plot Dimensions 1 &amp; 3. In addition, the hierarchical clustering results are displayed on top of the ordination. The corresponding maps for individual weights in the common space are displayed in <xref ref-type="fig" rid="pone-0095848-g009">Figure 9</xref> for the thirty participants.</p>
<fig id="pone-0095848-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095848.g007</object-id><label>Figure 7</label><caption>
<title>Shepard diagram displaying the observed dissimilarities against the fitted distances.</title>
<p>The diagram is drawn for a three-dimensional individual differences scaling model. The black line denotes isotonic regression.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095848.g007" position="float" xlink:type="simple"/></fig><fig id="pone-0095848-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095848.g008</object-id><label>Figure 8</label><caption>
<title>Perceptual map resulting from nonmetric multidimensional scaling with three-dimensional solution.</title>
<p>Hierarchical clusterings are drawn on top of the ordination. Dimensions 1 &amp; 2 are plotted on the left, and Dimensions 1 &amp; 3 on the right. The dimension names are inferred from the weightings of the stimuli on the common space dimensions.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095848.g008" position="float" xlink:type="simple"/></fig><fig id="pone-0095848-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095848.g009</object-id><label>Figure 9</label><caption>
<title>Participants' weights in the three-dimensional common space.</title>
<p>Dimensions 1 &amp; 2, obtained from the individual differences scaling analysis, are plotted on the left, and Dimensions 1 &amp; 3 on the right.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095848.g009" position="float" xlink:type="simple"/></fig>
<p>Multidimensional scaling was performed on the categories formed by the participants without any interpretation. Therefore, categories based on higher-level cognitive functions, with little or no correspondence to the perceptual attributes of the space, affect the obtained result. However, it is beneficial to inspect the raw categorization and the participant integrity before interpreting the results further. In <xref ref-type="fig" rid="pone-0095848-g008">Figure 8</xref>, quiet and still scenes both outdoor and indoor receive the highest weights for Dimension 1, for example #<italic>Quiet street</italic> (Dimension 1  =  0.79) and #<italic>Lecture hall</italic> (Dimension 1  =  0.68). In contrast, noisy and busy outdoor and indoor scenes, for example #<italic>Railway station</italic> (Dimension 1  =  −0.77) and #<italic>Traffic</italic> (Dimension 1  =  −0.56), are negatively weighted for the Dimension 1. It can be stated that Dimension 1 appears to be related to the perceived calmness of the scenes, where both auditory and visual content affect the perception. The result is validated by observing Cluster #1 (<italic>Calm</italic>) having the highest weights and Clusters #3 (<italic>Noisy</italic>) and #4 (<italic>Vivid</italic>) having negative weights on Dimension 1.</p>
<p>Dimension 2, on the other hand, appears to yield the highest weights for outdoor scenes, or distinctly large indoor spaces, such as #<italic>Hall</italic> (Dimension 2  =  0.32) and #<italic>Subway station</italic> (Dimension 2  =  0.47). Indoor scenes and limited outdoor scenes receive negative weights on Dimension 2, therefore, it can be assumed to describe the openness of the scene. Temporal aspects, both auditory and visual, seem to have no impact on Dimension 2, since Clusters #1 and #4 containing the calmest and busiest scenes obtain similar weights. Finally, Dimension 3 is more difficult to characterize by examining the environmental attributes. It seems, that scenes with people present in the view, such as #<italic>Hall</italic> (Dimension 3  =  0.46) and #<italic>Cafeteria</italic> (Dimension 3  =  0.43), have the highest weights on Dimension 3, whereas scenes with no people have negative weights.</p>
<p>Inspection of <xref ref-type="fig" rid="pone-0095848-g009">Figure 9</xref> for the individual weights of the participants in the common perceptual space reveals that no distinctive groups can be found. An important observation is that no participants are distinctively close to the origin that would indicate a poor fit to the common space dimensions. Instead, the participants are nicely grouped together especially in Dimensions 1 &amp; 2. Participants #12 (Dimension 1  =  1.43) and #20 (Dimension 1  =  1.52) seem to have weighted the first Dimension more than the other two dimensions, whereas participants #2 (Dimension 3  =  1.93), #15 (Dimension 3  =  1.96) and #26 (Dimension 3  =  1.99) favor the third Dimension. Participant #13 (Dimension 2  =  1.63) seems to be inclined towards the second Dimension. The few individuals who have weighted a given Dimension more than the majority indicate that they may have understood the task differently, or used one global rule in forming the categories, whereas others have used a more diverse approach. Otherwise, having one large group of participants with no division to subgroups implies that there were no varying strategies to perform the categorization. However, a division to two subgroups can be observed in Dimensions 1 &amp; 3, where some have favored the third Dimension slightly more than the others. This could imply that the presence or absence of people in the scene was more important to some.</p>
</sec><sec id="s3c">
<title>Categorization process</title>
<p>The individual motives for categorization were inspected by analyzing the interview data. Correspondence analysis was used to relate the elicited perceptual attributes to the stimulus scenes and, at this point, attributes with only a few references in the data were excluded. In the analysis, only the 16 most frequent codes, presented in <xref ref-type="table" rid="pone-0095848-t002">Table 2</xref>, were used. Codes beyond that had less references, and contained negations (i.e., they were based on the absence of some attribute), which differs from the more frequent codes. The inter-rater agreement was found acceptable with Cohen's <italic>κ</italic>  =  .62 according to <xref ref-type="bibr" rid="pone.0095848-Landis1">[48]</xref>, who have set threshold values defining .60 ≤ <italic>κ</italic> ≤ .79 as substantial and <italic>κ</italic> &gt; .80 as outstanding.</p>
<table-wrap id="pone-0095848-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095848.t002</object-id><label>Table 2</label><caption>
<title>Frequency of the codes in the interview data, and the suggested dominating modality.</title>
</caption><alternatives><graphic id="pone-0095848-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095848.t002" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Code</td>
<td align="left" rowspan="1" colspan="1">Frequency</td>
<td align="left" rowspan="1" colspan="1">Dominant modality</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Movement</td>
<td align="left" rowspan="1" colspan="1">198</td>
<td align="left" rowspan="1" colspan="1">Visual</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Noisy</td>
<td align="left" rowspan="1" colspan="1">188</td>
<td align="left" rowspan="1" colspan="1">Auditory</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Calm</td>
<td align="left" rowspan="1" colspan="1">161</td>
<td align="left" rowspan="1" colspan="1">Audiovisual</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Recognizable sound</td>
<td align="left" rowspan="1" colspan="1">138</td>
<td align="left" rowspan="1" colspan="1">Auditory</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Background sound</td>
<td align="left" rowspan="1" colspan="1">130</td>
<td align="left" rowspan="1" colspan="1">Auditory</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">People</td>
<td align="left" rowspan="1" colspan="1">117</td>
<td align="left" rowspan="1" colspan="1">Audiovisual</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">A lot to attend</td>
<td align="left" rowspan="1" colspan="1">100</td>
<td align="left" rowspan="1" colspan="1">Audiovisual</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Enclosed space</td>
<td align="left" rowspan="1" colspan="1">94</td>
<td align="left" rowspan="1" colspan="1">Visual</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Quiet</td>
<td align="left" rowspan="1" colspan="1">92</td>
<td align="left" rowspan="1" colspan="1">Auditory</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Conversation</td>
<td align="left" rowspan="1" colspan="1">78</td>
<td align="left" rowspan="1" colspan="1">Auditory</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Large indoor space</td>
<td align="left" rowspan="1" colspan="1">76</td>
<td align="left" rowspan="1" colspan="1">Visual</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Traffic</td>
<td align="left" rowspan="1" colspan="1">74</td>
<td align="left" rowspan="1" colspan="1">Audiovisual</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Open space</td>
<td align="left" rowspan="1" colspan="1">73</td>
<td align="left" rowspan="1" colspan="1">Visual</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Outdoor space</td>
<td align="left" rowspan="1" colspan="1">73</td>
<td align="left" rowspan="1" colspan="1">Visual</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Pleasant</td>
<td align="left" rowspan="1" colspan="1">73</td>
<td align="left" rowspan="1" colspan="1">Audiovisual</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Echo</td>
<td align="left" rowspan="1" colspan="1">66</td>
<td align="left" rowspan="1" colspan="1">Auditory</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap>
<p>From the 16 most frequent codes, five are related to dominantly visual content, six to auditory and five can be considered as audiovisual, or bimodal, codes. However, the relative weights of the modalities were not inspected in this study, and thus we cannot state that only visual input affected to the codes labeled here as visual, and vice versa for the auditory labels. Each of the unimodal-labeled codes have most likely been impacted by both modalities to some extent. <xref ref-type="fig" rid="pone-0095848-g010">Figure 10</xref> presents the ordination results from correspondence analysis in Dimensions 1 &amp; 2. The first two dimensions were found to explain 52% (inertia  =  .24) and 20% (inertia  =  .09) of the variance, respectively, while the third dimension accounted for 10% (inertia  =  .05) of the variance, and is not displayed graphically. In addition, the clusters obtained through hierarchical clustering are drawn on top of the ordination.</p>
<fig id="pone-0095848-g010" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095848.g010</object-id><label>Figure 10</label><caption>
<title>Correspondence analysis for the interview codings in Dimensions 1 &amp; 2.</title>
<p>Black denotes the stimulus scenes and grey denotes the codes. Hierarchical clusterings are drawn on top of the ordination.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095848.g010" position="float" xlink:type="simple"/></fig>
<p>In <xref ref-type="fig" rid="pone-0095848-g010">Figure 10</xref>, stimulus scenes perceived to be similar are displayed close together. Similarly, interview codes attached to similar scenes are displayed close to each other, and close to the corresponding scenes. The Figure reveals what were the most salient attributes that the participants paid attention to when making the categorization decisions for the respective scenes. The five clusters, obtained from hierarchical clustering, are separated in the two-dimensional perceptual map, indicating that the perceptual map is not distorting the underlying high-dimensional similarity space too much. Cluster #1 (<italic>Calm</italic>) resides close to codes reflecting perceived pleasantness and calmness mainly induced by quietness and soundscape with recognizable auditory events. In addition, codes reflecting visual movement are on the opposite side of the map, implying that the visual scene was still, although there were not enough references to codes indicating visual stillness. Cluster #2 (<italic>Still</italic>) is best described in the perceptual map by attributes related to being outdoor or in an open place. The scenes in this category did not have any specific points of interest or events and the overall atmosphere was still. This is reflected in the ordination through the absence of attributes related to temporal aspects, in favor of gist-like attributes of scene openness and background sound.</p>
<p>Cluster #3 (<italic>Noisy</italic>) contained scenes with lots of mechanical noise. The noisiness is well reflected in the perceptual map. In addition, the participants paid attention to the sound source, namely traffic. According to the ordination, Cluster #4 (<italic>Vivid</italic>) is defined to contain scenes with a lot to attend, which is consistent with the clustering result. Especially, attention has been paid to the presence of people in the scenes. Interestingly, a spatial attribute of the soundscape, echo or reverberation, has been frequently mentioned. Finally, Cluster #5 (<italic>Open</italic>) is related with multiple attributes, the most common attribute being <italic>conversation</italic>. Attention was also paid to movement, enclosure and other recognizable sounds. Although all the scenes were somehow open spaces, the attributes related to openness are not ordinated close to the cluster. Apparently, with these scenes, other perceptual properties overruled the scene gist categorization.</p>
</sec><sec id="s3d">
<title>Environmental correspondence of the ordination</title>
<p>Objective reasons for the ordination can be searched for from <xref ref-type="fig" rid="pone-0095848-g011">Figure 11</xref>, where three environmental variables are computed from the stimulus scenes and fitted to the correspondence analysis (CA) result. The sound pressure level (SPL) was measured at the observation position and it is the A-weighted average SPL over 10 seconds. Temporal information (TI) is defined as the motion difference between adjacent frames. TI is computed by observing the pixel values at the same spatial location at successive frames, as defined in <xref ref-type="bibr" rid="pone.0095848-InternationalTelecommunication1">[35]</xref>. The maximum standard deviation found in the motion difference was taken to be the TI for a given video sequence. Indoor versus outdoor classification is a binary attribute related to each scene. Value 1 denotes indoor scenes and 0 denotes outdoor. The aim is to fit two-dimensional models to predict the environmental variables from the scenes' CA scores on the two axes.</p>
<fig id="pone-0095848-g011" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095848.g011</object-id><label>Figure 11</label><caption>
<title>Correspondence analysis for the interview codings with fitted environmental variable surfaces.</title>
<p>Top-left panel shows the ordinated scenes, and the next panels display the fitted sound pressure level, indoor vs. outdoor classification, and temporal information surfaces in a clockwise manner.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095848.g011" position="float" xlink:type="simple"/></fig>
<p>Sound pressure level, temporal information and indoor vs. outdoor classification are found to have a nearly linear fit to the CA map, the models explaining 87%, 63%, and 82% of the variance in the respective variables. All three fitted surfaces were found statistically significant at <italic>p</italic>  =  .01. Dimension #1 is well described by a combination of increasing SPL and TI levels from right to left. This is in accordance with the codes in <xref ref-type="fig" rid="pone-0095848-g010">Figure 10</xref>, where the codes #<italic>Quiet</italic> and #<italic>Calm</italic> can be found on the right and codes #<italic>Movement</italic> and #<italic>Noisy</italic> on the left. Dimension #2 is characterized by the division to indoor scenes on top and outdoor scenes in bottom. This result is verified by the codes #<italic>Outdoor space</italic> and #<italic>Open space</italic> residing in bottom in contrast to #<italic>Large indoor space</italic> and #<italic>Enclosed space</italic> on top. In conclusion, the perceptual map appears to be in accordance with measurable environmental properties, validating the assumptions made in the interview coding process and lending support for the two-dimensional solution for correspondence analysis.</p>
</sec></sec><sec id="s4">
<title>Discussion</title>
<p>A perceptual map of audiovisual scenes based on a categorization task and a semi-structured interview was derived in our study, confirming our primary hypothesis that a group of people describes natural audiovisual scenes with similar perceptual attributes. The nature of our study was exploratory and thus we cannot precisely identify the relative contributions of the two sensory channels to audiovisual scene perception. From the perceptual map we can draw evidence that the most important perceptual attributes related to the scenes were movement, perceived noisiness, and eventfulness of the scene. Regarding scene gist perception, the gist properties openness and expansion, identified in previous research <xref ref-type="bibr" rid="pone.0095848-Oliva2">[49]</xref>, were found to remain as important factors only in scenes with no salient auditory or visual events. As hypothesized, three environmental variables were found to have a nearly linear fit to the perceptual mapping, adding more support to the assumption that the map is stemming from the physical environment. The perceptual map could have been also predicted by higher-order interpretations such as the social function or the aesthetic interpretation of the depicted spaces, which was not the case here. However, the origins of the attributes were not tested, and different selection of stimulus scenes could have led to different categorization criteria.</p>
<p>Based on the results obtained here, disregarding the higher level function of the space and the meaning of the events taking place at the space, and instead focusing on the physical environment is possible. However, a few scenes were more problematic than the others, for example the scenes #<italic>Class room</italic> and #<italic>Lecture hall</italic> were categorized into the same category by a few participants because they are both known to be related to education, even though the purely physical characteristics of the scenes are strikingly different. Despite the difficulties, the clustering, MDS and CA results show similar overall ordination of the stimulus scenes, which would imply that the underlying perceptual reasoning is prevalent and not resulting from random effects. The experiment assumes categories of environmental scenes to result from perceived similarity. Categorization as a method for studying similarity is justified for perceptually rich stimuli, where information is integrated from multiple sources, and categories can offer useful inferences <xref ref-type="bibr" rid="pone.0095848-Goldstone1">[50]</xref>. However, caution must be taken as the categorization task can influence the perceptual discrimination of the stimuli <xref ref-type="bibr" rid="pone.0095848-Goldstone2">[51]</xref>. Early categorization decisions may affect how people perceive new stimuli and which perceptual dimensions they pay attention to. In this case, all the stimulus scenes were viewed once before the categorization task was introduced, potentially minimizing the task influence. In addition, the participant weights on the perceptual dimensions in MDS analysis (<xref ref-type="fig" rid="pone-0095848-g009">Figure 9</xref>) do not show significantly differing weightings for the majority of participants.</p>
<p>An interesting categorization theory has been provided stating that with complex stimuli humans initially do categorization based on simple rules about their properties <xref ref-type="bibr" rid="pone.0095848-Rouder1">[52]</xref>. Gradually, as the categorization process evolves, learning about discriminating stimuli occurs and the categorization transforms to exemplar-based, and novel stimuli may be rapidly categorized according to the exemplar stimuli without the aid of rules. In this study, the categorization strategy was not under inspection, but informal discussions with the participants after the test revealed, that majority of them had initially used one or two rules, such as loudness and the indoor vs. outdoor classification, to form large categories. Thereafter, with more detailed inspection, the initial categories were divided into smaller ones according to new rules, or, some participants had found exemplar-like scenes, that they used to justify the existence of specific categories. Nevertheless, the task was so complicated that higher-order scene categorization processes cannot be ruled out. The categorization task and the interview may have forced the participants to process the stimuli in an unnatural way that would have eventually affected our results. Further studies are needed to fine-tune the perceptual map possibly by utilizing multiple simpler tasks.</p>
<p>Movement, or the lack thereof, appears to be the most useful attribute in bimodal scene perception. As identified in previous research, visual movement and on-sets quickly draw our attention and direct fixations <xref ref-type="bibr" rid="pone.0095848-Itti2">[4]</xref>, <xref ref-type="bibr" rid="pone.0095848-Brockmole1">[53]</xref>. The proximity of movement was also reported to be affecting the categorization by some participants, as moving objects nearby were regarded as threatening. The effect of auditory stimulation on the perception of movement requires further studies. The lack of movement is related to perceived calmness and pleasantness, yet, quiet or non-confusing soundscape appears to be needed for the scene to be considered as calm, as demonstrated by the CA (<xref ref-type="fig" rid="pone-0095848-g010">Figure 10</xref>). The scene #<italic>Dishwasher</italic> is revelatory in this case, because it has no movement, but very noisy soundscape, and thus it is not perceived as calm. Perceptually calm and pleasant scenes also contained visual elements present in the nature (such as trees, grass and the sea), which is in accordance with previous research <xref ref-type="bibr" rid="pone.0095848-Pheasant1">[31]</xref>, although naturalness was not frequently mentioned by the participants and thus not present in the perceptual map. The absence of nature-related attributes may result from the fact that all the stimulus scenes were recorded in dominantly man-made urban environments.</p>
<p>Even though the stimulus duration was 15 s, scene gist dimensions openness and expansion seem to be robust features also in dynamic audiovisual perception. These dimensions are perceived within a single glance of the visual world <xref ref-type="bibr" rid="pone.0095848-FeiFei1">[13]</xref>, but still they were meaningful to humans despite the long stimulus duration and the complex categorization task. There were four interview codes related to the perceived size of the space or indoor vs. outdoor classification of the scene. Especially, when the scene had no salient events, as was the case with #<italic>Narrow space</italic> and #<italic>Cyclists</italic>, the gist properties were the things that the participants noticed. Related to the size of a space, the perception of reverberation or echo was frequently mentioned with large indoor spaces. In previous research on soundscape perception, the spatial factors were seldom mentioned by test participants <xref ref-type="bibr" rid="pone.0095848-Peltonen1">[21]</xref>. Here, potentially, the accompanying visual stimulus guided also the auditory perception towards the acoustic properties of the space, because large indoor spaces are known to be reverberant. However, this phenomenon may also be related to different listening strategies: In a real environment people are prone to apply <italic>everyday listening</italic>, while in a laboratory the listening strategy is more analytic, <italic>musical listening</italic> <xref ref-type="bibr" rid="pone.0095848-Gaver1">[54]</xref>. Moreover, in a laboratory setting the participants are found to be more affected by the spatial attributes of the sound, while in a real environment the sound sources are described more precisely <xref ref-type="bibr" rid="pone.0095848-Tardieu1">[55]</xref>.</p>
<p>The auditory scene attributes appear to be divided to quiet scenes, scenes with recognizable or meaningful sound events (conversation), background sound and noise. The division is in accordance with the finding of event sequences and amorphous sequences <xref ref-type="bibr" rid="pone.0095848-Dubois1">[20]</xref>. In addition, recognized sounds vs. noise separation reflecting the perceived valence is evident in <xref ref-type="fig" rid="pone-0095848-g010">Figure 10</xref>, as found previously in soundscape studies <xref ref-type="bibr" rid="pone.0095848-Axelsson1">[22]</xref>, <xref ref-type="bibr" rid="pone.0095848-Davies1">[23]</xref> indicating that the emotional state induced by the soundscape is meaningful to humans even with complex audiovisual stimuli. Evidence for auditory gist perception <xref ref-type="bibr" rid="pone.0095848-Harding1">[56]</xref>, apart from auditory streaming, cannot be directly drawn based on this study, but the notion of amorphous sequences, or background sound, that was perceived but not analyzed further, would suggest that some kind of an overall representation of the soundscape is obtained, while attention is directed towards more salient events.</p>
<p>Based on our exploratory results, humans tend to focus on perceived movement, noisiness and eventfulness when describing their experience of real-life urban environments. However, the relative weights of the modalities cannot be quantified with our data, and future studies will be needed. Moreover, some of the elicited attributes, namely #<italic>Calm</italic> and #<italic>Pleasant</italic>, may not be readily usable for modeling, but need more elaborate investigation to relate them to perceivable scene properties. The effect of top-down information on real-world scene perception through attention allocation is also worth investigating. Arousal can have modulating effects on selective attention giving more attentional resources for the processing of the most salient stimuli while decreasing the processing of low priority stimuli <xref ref-type="bibr" rid="pone.0095848-Mather1">[57]</xref>. Previous studies on overt attention have concentrated on simple stimuli and tasks, for example by asking the participants to focus on a certain spatial location. In the real-world, however, the task might be not to get run over by a car that you can hear but not yet see, which is a lot more arousing experience, and probably affects the scene processing priorities. Therefore, we would like to draw attention towards the concept of audiovisual gist that builds on the visual gist perception and is biased by the auditory scene, to better reflect the situation in the real world. We hypothesize that audiovisual processing of the scene gist results in dissimilar perception of the reality in further processing, when compared to unimodal gist processing. For example, the perceived calmness of a scene could rapidly arise from the combination of a visual gist interpretation and a cursory overview of the soundscape. Thereafter, the acquired audiovisual gist could guide the more detailed visual and auditory exploration, and probably be related to the state of arousal experienced by the perceiver, and eventually affect how we classify scenes. However, our study did not directly provide information about gist effects due to the length of the stimulation, and further studies are needed to link the effect of scene gist to the results presented here. Similarly, we can only hypothesize that the elicited perceptual attributes are stemming from the perceptually salient aspects of the scenes, as saliency as a phenomenon was not directly studied here. Further studies are needed to test the attributes' relation to audiovisual saliency.</p>
</sec><sec id="s5">
<title>Conclusions</title>
<p>In conclusion, audiovisual perception of natural scenes was studied in this work. Perceptual similarity of 19 natural scenes reproduced with an immersive audiovisual display was evaluated through a categorization task followed by a semi-structured interview. Our exploratory results point out that a two-dimensional perceptual map of a set of natural audiovisual scenes depicting real-life urban environments is obtainable, and movement, noisiness and eventfulness of the scene are the most important perceptual attributes. Scene gist properties describing the size and openness of the space were present in the results, when there was an absence of salient events. However, we cannot state what exactly were the individual contributions of the modalities, or that the elicited perceptual attributes are stemming from perceptual saliency.</p>
<p>Overall, when considering a real-world scene, including the auditory modality has an enormous impact on our perception. We propose that the study of natural scene perception should move forward to better understand the processes behind real-world scene processing including multiple modalities. Moreover, the question of how do we arrive from the gist of the visual scene to a thorough understanding of the multimodal reality is worth investigating. The current work is an exploratory study that demonstrates the existence of frequently used perceptual scene attributes. Further studies are needed to quantify the individual contributions of auditory and visual modalities on audiovisual perception of natural scenes.</p>
</sec></body>
<back>
<ack>
<p>We wish to thank Dr. Catarina Mendonça for her insightful comments on this paper.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0095848-Yantis1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yantis</surname><given-names>S</given-names></name> (<year>1993</year>) <article-title>Stimulus-driven attentional capture</article-title>. <source>Current Directions in Psychological Science</source> <volume>2</volume>: <fpage>156</fpage>–<lpage>161</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Sanocki1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sanocki</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Sulman</surname><given-names>N</given-names></name> (<year>2013</year>) <article-title>Complex dynamic scene perception: Effects of attentional set on perceiving single and multiple event types</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source> <volume>39</volume>: <fpage>381</fpage>–<lpage>398</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Itti1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Itti</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Koch</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Niebur</surname><given-names>E</given-names></name> (<year>1998</year>) <article-title>A Model of Saliency-Based Visual Attention for Rapid Scene Analysis</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source> <volume>20</volume>: <fpage>1254</fpage>–<lpage>1259</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Itti2"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Itti</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Koch</surname><given-names>C</given-names></name> (<year>2001</year>) <article-title>Computational modelling of visual attention</article-title>. <source>Nature reviews neuroscience</source> <volume>2</volume>: <fpage>1</fpage>–<lpage>11</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Carmi1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carmi</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Itti</surname><given-names>L</given-names></name> (<year>2006</year>) <article-title>Visual causes versus correlates of attentional selection in dynamic scenes</article-title>. <source>Vision research</source> <volume>46</volume>: <fpage>4333</fpage>–<lpage>4345</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Kayser1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kayser</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Petkov</surname><given-names>CI</given-names></name>, <name name-style="western"><surname>Lippert</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Logothetis</surname><given-names>NK</given-names></name> (<year>2005</year>) <article-title>Mechanisms for allocating auditory attention: an auditory saliency map</article-title>. <source>Current Biology</source> <volume>15</volume>: <fpage>1943</fpage>–<lpage>1947</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Oliva1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oliva</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Torralba</surname><given-names>A</given-names></name> (<year>2006</year>) <article-title>Building the gist of a scene: The role of global image features in recognition</article-title>. <source>Progress in Brain Research</source> <volume>155</volume>: <fpage>23</fpage>–<lpage>39</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Castelhano1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Castelhano</surname><given-names>MS</given-names></name>, <name name-style="western"><surname>Henderson</surname><given-names>JM</given-names></name> (<year>2008</year>) <article-title>The inuence of color on the perception of scene gist</article-title>. <source>Journal of Experimental Psychology: Human perception and performance</source> <volume>34</volume>: <fpage>660</fpage>–<lpage>675</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Greene1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Greene</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Oliva</surname><given-names>A</given-names></name> (<year>2009</year>) <article-title>The briefest of glances: The time course of natural scene understanding</article-title>. <source>Psychological Science</source> <volume>20</volume>: <fpage>464</fpage>–<lpage>472</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Potter1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Potter</surname><given-names>MC</given-names></name> (<year>1975</year>) <article-title>Meaning in visual search</article-title>. <source>Science</source> <volume>187</volume>: <fpage>965</fpage>–<lpage>966</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Potter2"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Potter</surname><given-names>MC</given-names></name> (<year>1976</year>) <article-title>Short-term conceptual memory for pictures</article-title>. <source>Journal of Experimental Psychology: Human Learning and Memory</source> <volume>2</volume>: <fpage>509</fpage>–<lpage>522</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Friedman1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friedman</surname><given-names>A</given-names></name> (<year>1979</year>) <article-title>Framing pictures: the role of knowledge in automatized encoding and memory for gist</article-title>. <source>Journal of Experimental Psychology: General</source> <volume>108</volume>: <fpage>316</fpage>–<lpage>355</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-FeiFei1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fei-Fei</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Iyer</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Koch</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Perona</surname><given-names>P</given-names></name> (<year>2007</year>) <article-title>What do we perceive in a glance of a real-world scene?</article-title> <source>Journal of Vision</source> <volume>7</volume>: <fpage>1</fpage>–<lpage>29</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Bregman1"><label>14</label>
<mixed-citation publication-type="other" xlink:type="simple">Bregman A (1994) Auditory scene analysis: The perceptual organization of sound. Cambridge, MA: The MIT Press.</mixed-citation>
</ref>
<ref id="pone.0095848-Botteldooren1"><label>15</label>
<mixed-citation publication-type="other" xlink:type="simple">Botteldooren D, Lavandier C, Preis A, Dubois D, Aspuru I, <etal>et al</etal>.. (2011) Understanding urban and natural soundscapes. In: Forum Acusticum. Aalborg, Denmark, c 1–7.</mixed-citation>
</ref>
<ref id="pone.0095848-Shamma1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shamma</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Elhilali</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Micheyl</surname><given-names>C</given-names></name> (<year>2011</year>) <article-title>Temporal coherence and attention in auditory scene analysis</article-title>. <source>Trends in neurosciences</source> <volume>34</volume>: <fpage>114</fpage>–<lpage>123</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Bonebright1"><label>17</label>
<mixed-citation publication-type="other" xlink:type="simple">Bonebright T (2012) Were those coconuts or horse hoofs? Visual context effects on identification and perceived veracity of everyday sounds. In: The 18th International Conference on Auditory Display (ICAD2012) Atlanta, (GA)1–4.</mixed-citation>
</ref>
<ref id="pone.0095848-Alais1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alais</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Burr</surname><given-names>D</given-names></name> (<year>2004</year>) <article-title>The ventriloquist effect results from near-optimal bimodal integration</article-title>. <source>Current biology</source> <volume>14</volume>: <fpage>257</fpage>–<lpage>262</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Botteldooren2"><label>19</label>
<mixed-citation publication-type="other" xlink:type="simple">Botteldooren D, Boes M, Oldoni D, De Coensel B (2012) The role of paying attention to sounds in soundscape perception. In: The Acoustics 2012 Hong Kong Conference Hong Kong1–6.</mixed-citation>
</ref>
<ref id="pone.0095848-Dubois1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dubois</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Guastavino</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Raimbault</surname><given-names>M</given-names></name> (<year>2006</year>) <article-title>A cognitive approach to urban soundscapes: Using verbal data to access everyday life auditory categories</article-title>. <source>Acta Acustica united with Acustica</source> <volume>92</volume>: <fpage>865</fpage>–<lpage>874</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Peltonen1"><label>21</label>
<mixed-citation publication-type="other" xlink:type="simple">Peltonen V, Eronen A, Parviainen M, Klapuri A (2001) Recognition of everyday auditory scenes: potentials, latencies and cues. In: Audio Engineering Society 110th Convention. Amsterdam The Netherlands1–5.</mixed-citation>
</ref>
<ref id="pone.0095848-Axelsson1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Axelsson</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Nilsson</surname><given-names>ME</given-names></name>, <name name-style="western"><surname>Berglund</surname><given-names>B</given-names></name> (<year>2010</year>) <article-title>A principal components model of soundscape perception</article-title>. <source>The Journal of the Acoustical Society of America</source> <volume>128</volume>: <fpage>2836</fpage>–<lpage>2846</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Davies1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Davies</surname><given-names>WJ</given-names></name>, <name name-style="western"><surname>Adams</surname><given-names>MD</given-names></name>, <name name-style="western"><surname>Bruce</surname><given-names>NS</given-names></name>, <name name-style="western"><surname>Cain</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Carlyle</surname><given-names>A</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>Perception of soundscapes: An interdisciplinary approach</article-title>. <source>Applied Acoustics</source> <volume>7</volume>: <fpage>224</fpage>–<lpage>231</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-McGurk1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McGurk</surname><given-names>H</given-names></name>, <name name-style="western"><surname>MacDonald</surname><given-names>J</given-names></name> (<year>1976</year>) <article-title>Hearing lips and seeing voices</article-title>. <source>Nature</source> <volume>264</volume>: <fpage>746</fpage>–<lpage>748</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Spence1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Spence</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Driver</surname><given-names>J</given-names></name> (<year>1997</year>) <article-title>Audiovisual links in exogenous covert spatial orienting</article-title>. <source>Perception &amp; psychophysics</source> <volume>59</volume>: <fpage>1</fpage>–<lpage>22</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Nardo1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nardo</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Santangelo</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Macaluso</surname><given-names>E</given-names></name> (<year>2013</year>) <article-title>Spatial orienting in complex audiovisual environments</article-title>. <source>Human brain mapping</source> <volume>35(4)</volume>: <fpage>1</fpage>–<lpage>18</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Coutrot1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Coutrot</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Guyader</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Ionescu</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Caplier</surname><given-names>A</given-names></name> (<year>2012</year>) <article-title>Inuence of soundtrack on eye movements during video exploration</article-title>. <source>Journal of Eye Movement Research</source> <volume>5</volume>: <fpage>1</fpage>–<lpage>10</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-V1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Võ</surname><given-names>MLH</given-names></name>, <name name-style="western"><surname>Smith</surname><given-names>TJ</given-names></name>, <name name-style="western"><surname>Mital</surname><given-names>PK</given-names></name>, <name name-style="western"><surname>Henderson</surname><given-names>JM</given-names></name> (<year>2012</year>) <article-title>Do the eyes really have it? Dynamic allocation of attention when viewing moving faces</article-title>. <source>Journal of Vision</source> <volume>12</volume>: <fpage>1</fpage>–<lpage>14</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Coutrot2"><label>29</label>
<mixed-citation publication-type="other" xlink:type="simple">Coutrot A, Guyader N (2013) Toward the Introduction of Auditory Information in Dynamic Visual Attention Models. In: The 14th International Workshop on Image and Audio Analysis for Multimedia Interactive Services Paris, France, July1–4.</mixed-citation>
</ref>
<ref id="pone.0095848-Quigley1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Quigley</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Onat</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Harding</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Cooke</surname><given-names>M</given-names></name>, <name name-style="western"><surname>König</surname><given-names>P</given-names></name> (<year>2008</year>) <article-title>Audio-visual integration during overt visual attention</article-title>. <source>Journal of Eye Movement Research</source> <volume>1(2)</volume>: <fpage>1</fpage>–<lpage>17</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Pheasant1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pheasant</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Horoshenkov</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Watts</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Barrett</surname><given-names>B</given-names></name> (<year>2008</year>) <article-title>The acoustic and visual factors inuencing the construction of tranquil space in urban and rural environments: tranquil spaces-quiet places?</article-title> <source>The Journal of the Acoustical Society of America</source> <volume>123</volume>: <fpage>1446</fpage>–<lpage>1457</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Pheasant2"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pheasant</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Fisher</surname><given-names>MN</given-names></name>, <name name-style="western"><surname>Watts</surname><given-names>GR</given-names></name>, <name name-style="western"><surname>Whitaker</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Horoshenkov</surname><given-names>KV</given-names></name> (<year>2010</year>) <article-title>The importance of auditory-visual interaction in the construction of tranquil space</article-title>. <source>Journal of Environmental Psychology</source> <volume>30</volume>: <fpage>501</fpage>–<lpage>509</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Finnish1"><label>33</label>
<mixed-citation publication-type="other" xlink:type="simple">Finnish Advisory Board on Research Ethics (2009) Ethical principles of research in the humanities and social and behavioural sciences and proposals for ethical review. Helsinki, Finland, 1–17 pp. Available: <ext-link ext-link-type="uri" xlink:href="http://www.tenk.fi/sites/tenk.fi/files/ethicalprinciples.pdf" xlink:type="simple">http://www.tenk.fi/sites/tenk.fi/files/ethicalprinciples.pdf</ext-link>. Accessed 2014 Apr 12.</mixed-citation>
</ref>
<ref id="pone.0095848-Tversky1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tversky</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Hemenway</surname><given-names>K</given-names></name> (<year>1983</year>) <article-title>Categories of environmental scenes</article-title>. <source>Cognitive Psychology</source> <volume>15</volume>: <fpage>1</fpage>–<lpage>29</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-InternationalTelecommunication1"><label>35</label>
<mixed-citation publication-type="other" xlink:type="simple">International Telecommunication Union (1998) ITU-T Recommendation P.911: Subjective audiovisual quality assessment methods for multimedia applications. ITU Telecommunication Standardization Sector, Switzerland.</mixed-citation>
</ref>
<ref id="pone.0095848-GmezBolaos1"><label>36</label>
<mixed-citation publication-type="other" xlink:type="simple">Gómez Bolaños J, Pulkki V (2012) Immersive audiovisual environment with 3D audio playback. In: Audio Engineering Society 132nd Convention Budapest, Hungary1–9.</mixed-citation>
</ref>
<ref id="pone.0095848-Pulkki1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pulkki</surname><given-names>V</given-names></name> (<year>2007</year>) <article-title>Spatial sound reproduction with directional audio coding</article-title>. <source>Journal of the Audio Engineering Society</source> <volume>55</volume>: <fpage>503</fpage>–<lpage>516</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Politis1"><label>38</label>
<mixed-citation publication-type="other" xlink:type="simple">Politis A, Pulkki V (2011) Broadband analysis and synthesis for DirAC using A-format. In: Audio Engineering Society 131st Convention New York, NY1–11.</mixed-citation>
</ref>
<ref id="pone.0095848-Vilkamo1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vilkamo</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Lokki</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Pulkki</surname><given-names>V</given-names></name> (<year>2009</year>) <article-title>Directional audio coding: Virtual microphone-based synthesis and subjective evaluation</article-title>. <source>Journal of the Audio Engineering Society</source> <volume>57</volume>: <fpage>709</fpage>–<lpage>724</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Bray1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bray</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Curtis</surname><given-names>J</given-names></name> (<year>1957</year>) <article-title>An ordination of upland forest communities of southern Wisconsin</article-title>. <source>Ecological Monographs</source> <volume>27</volume>: <fpage>325</fpage>–<lpage>349</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Hair1"><label>41</label>
<mixed-citation publication-type="other" xlink:type="simple">Hair J, Black B, Babin B, Anderson R, Tatham R (2005) Multivariate data analysis, 6th edition. Upper Saddle River, NJ: Prentice Hall.</mixed-citation>
</ref>
<ref id="pone.0095848-R1"><label>42</label>
<mixed-citation publication-type="other" xlink:type="simple">R Development Core Team (2012). R: A language and environment for statistical computing. Available: <ext-link ext-link-type="uri" xlink:href="http://www.r-project.org/" xlink:type="simple">http://www.r-project.org/</ext-link>. Accessed 2014 Apr 12.</mixed-citation>
</ref>
<ref id="pone.0095848-deLeeuw1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>de Leeuw</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Mair</surname><given-names>P</given-names></name> (<year>2008</year>) <article-title>Multidimensional scaling using majorization: SMACOF in R. Journal of Statistical Software</article-title>. <volume>31</volume>: <fpage>1</fpage>–<lpage>30</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Corbin1"><label>44</label>
<mixed-citation publication-type="other" xlink:type="simple">Corbin J, Strauss A (2008) Basics of qualitative research: Techniques and procedures for developing grounded theory, 3rd edition. Thousand Oaks, CA: SAGE Publications.</mixed-citation>
</ref>
<ref id="pone.0095848-ATLASti1"><label>45</label>
<mixed-citation publication-type="other" xlink:type="simple">ATLASti Scientific Software Development GmbH (2011). Atlas.ti v.6. Available: <ext-link ext-link-type="uri" xlink:href="http://www.atlasti.com/" xlink:type="simple">http://www.atlasti.com/</ext-link>. Accessed 2014 Apr 12.</mixed-citation>
</ref>
<ref id="pone.0095848-Oksanen1"><label>46</label>
<mixed-citation publication-type="other" xlink:type="simple">Oksanen J, Blanchet FG, Kindt R, Legendre P, Minchin PR, <etal>et al</etal>.. (2011). vegan: Community Ecology Package. Available: <ext-link ext-link-type="uri" xlink:href="http://cran.r-project.org/package=vegan" xlink:type="simple">http://cran.r-project.org/package=vegan</ext-link>. Accessed 2014 Apr 12.</mixed-citation>
</ref>
<ref id="pone.0095848-Rabinowitz1"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rabinowitz</surname><given-names>G</given-names></name> (<year>1975</year>) <article-title>Introduction to nonmetric multidimensional scaling</article-title>. <source>American Journal of Political Science</source> <volume>19</volume>: <fpage>343</fpage>–<lpage>390</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Landis1"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Landis</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Koch</surname><given-names>GG</given-names></name> (<year>1977</year>) <article-title>The measurement of observer agreement for categorical data</article-title>. <source>Biometrics</source> <volume>33</volume>: <fpage>159</fpage>–<lpage>174</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Oliva2"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oliva</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Torralba</surname><given-names>A</given-names></name> (<year>2001</year>) <article-title>Modeling the shape of the scene: A holistic representation of the spatial envelope</article-title>. <source>International Journal of Computer Vision</source> <volume>42</volume>: <fpage>145</fpage>–<lpage>175</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Goldstone1"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goldstone</surname><given-names>RL</given-names></name> (<year>1994</year>) <article-title>The role of similarity in categorization: providing a groundwork</article-title>. <source>Cognition</source> <volume>52</volume>: <fpage>125</fpage>–<lpage>157</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Goldstone2"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goldstone</surname><given-names>RL</given-names></name> (<year>1994</year>) <article-title>Inuences of categorization on perceptual discrimination</article-title>. <source>Journal of Experimental Psychology: General</source> <volume>123</volume>: <fpage>178</fpage>–<lpage>200</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Rouder1"><label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rouder</surname><given-names>JN</given-names></name>, <name name-style="western"><surname>Ratcliff</surname><given-names>R</given-names></name> (<year>2006</year>) <article-title>Comparing exemplar- and rule-based theories of categorization</article-title>. <source>Current Directions in Psychological Science</source> <volume>15</volume>: <fpage>9</fpage>–<lpage>13</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Brockmole1"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brockmole</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Henderson</surname><given-names>JM</given-names></name> (<year>2005</year>) <article-title>Prioritization of new objects in real-world scenes: evidence from eye movements</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source> <volume>31</volume>: <fpage>857</fpage>–<lpage>868</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Gaver1"><label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gaver</surname><given-names>W</given-names></name> (<year>1993</year>) <article-title>What in the world do we hear?: An ecological approach to auditory event perception</article-title>. <source>Ecological Psychology</source> <volume>5</volume>: <fpage>1</fpage>–<lpage>29</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Tardieu1"><label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tardieu</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Susini</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Poisson</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Lazareff</surname><given-names>P</given-names></name>, <name name-style="western"><surname>McAdams</surname><given-names>S</given-names></name> (<year>2008</year>) <article-title>Perceptual study of soundscapes in train stations</article-title>. <source>Applied Acoustics</source> <volume>69</volume>: <fpage>1224</fpage>–<lpage>1239</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095848-Harding1"><label>56</label>
<mixed-citation publication-type="other" xlink:type="simple">Harding S, Cooke M, König P (2007) Auditory gist perception: an alternative to attentional selection of auditory streams? In: Paletta L, Rome E, editors. Attention in Cognitive Systems. Theories and Systems from an Interdisciplinary Viewpoint. Heidelberg: Springer. pp. 399–416.</mixed-citation>
</ref>
<ref id="pone.0095848-Mather1"><label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mather</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Sutherland</surname><given-names>M</given-names></name> (<year>2011</year>) <article-title>Arousal-biased competition in perception and memory</article-title>. <source>Perspectives on Psychological Science</source> <volume>6</volume>: <fpage>114</fpage>–<lpage>133</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>