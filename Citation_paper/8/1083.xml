<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-16-08363</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0178808</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Software engineering</subject><subj-group><subject>Preprocessing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Software engineering</subject><subj-group><subject>Preprocessing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Dogs</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Machine learning algorithms</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Machine learning algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject><subj-group><subject>Machine learning algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Neurology</subject><subj-group><subject>Epilepsy</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Forecasting</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Forecasting</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Signal filtering</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Bioassays and physiological analysis</subject><subj-group><subject>Electrophysiological techniques</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Clinical medicine</subject><subj-group><subject>Clinical neurophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Supervised filters for EEG signal in naturally occurring epilepsy forecasting</article-title>
<alt-title alt-title-type="running-head">Supervised filters for EEG signal in naturally occurring epilepsy forecasting</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2372-5712</contrib-id>
<name name-style="western">
<surname>Muñoz-Almaraz</surname> <given-names>Francisco Javier</given-names></name>
<xref ref-type="aff" rid="aff001"/>
<xref ref-type="fn" rid="currentaff001"><sup>¤</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Zamora-Martínez</surname> <given-names>Francisco</given-names></name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Botella-Rocamora</surname> <given-names>Paloma</given-names></name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Pardo</surname> <given-names>Juan</given-names></name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
</contrib-group>
<aff id="aff001">
<addr-line>ESAI - Embedded Systems and Artificial Intelligence Group Dept. of Physical Sciences, Mathematics and Computing Universidad CEU Cardenal Herrera, Valencia, Spain</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Marinazzo</surname> <given-names>Daniele</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Ghent University, BELGIUM</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p>
<list list-type="simple">
<list-item>
<p><bold>Conceptualization:</bold> FJM-A FZ-M JP.</p>
</list-item>
<list-item>
<p><bold>Data curation:</bold> FZ-M.</p>
</list-item>
<list-item>
<p><bold>Formal analysis:</bold> FJM-A.</p>
</list-item>
<list-item>
<p><bold>Funding acquisition:</bold> JP.</p>
</list-item>
<list-item>
<p><bold>Investigation:</bold> FJM-A FZ-M JP.</p>
</list-item>
<list-item>
<p><bold>Methodology:</bold> FJM-A FZ-M PB-R.</p>
</list-item>
<list-item>
<p><bold>Project administration:</bold> JP.</p>
</list-item>
<list-item>
<p><bold>Resources:</bold> FZ-M.</p>
</list-item>
<list-item>
<p><bold>Software:</bold> FZ-M.</p>
</list-item>
<list-item>
<p><bold>Supervision:</bold> JP.</p>
</list-item>
<list-item>
<p><bold>Validation:</bold> FJM-A FZ-M PB-R.</p>
</list-item>
<list-item>
<p><bold>Visualization:</bold> FJM-A FZ-M.</p>
</list-item>
<list-item>
<p><bold>Writing – original draft:</bold> FJM-A FZ-M JP.</p>
</list-item>
<list-item>
<p><bold>Writing – review &amp; editing:</bold> FJM-A JP.</p>
</list-item>
</list>
</p>
</fn>
<fn fn-type="current-aff" id="currentaff001">
<label>¤</label><p>Current address: Escuela Superior de Enseñanzas Técnicas, Universidad CEU Cardenal Herrera, C./ San Bartolomé, 55. 46115, Alfara del Patriarca, Valencia, Spain</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">malmaraz@uchceu.es</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<year>2017</year>
</pub-date>
<pub-date pub-type="epub">
<day>20</day>
<month>6</month>
<year>2017</year>
</pub-date>
<volume>12</volume>
<issue>6</issue>
<elocation-id>e0178808</elocation-id>
<history>
<date date-type="received">
<day>26</day>
<month>2</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>21</day>
<month>5</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Muñoz-Almaraz et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0178808"/>
<abstract>
<p>Nearly 1% of the global population has Epilepsy. Forecasting epileptic seizures with an acceptable confidence level, could improve the disease treatment and thus the lifestyle of the people who suffer it. To do that the electroencephalogram (EEG) signal is usually studied through spectral power band filtering, but this paper proposes an alternative novel method of preprocessing the EEG signal based on supervised filters. Such filters have been employed in a machine learning algorithm, such as the K-Nearest Neighbor (KNN), to improve the prediction of seizures. The proposed solution extends with this novel approach an algorithm that was submitted to win the third prize of an international Data Science challenge promoted by Kaggle contest platform and the American Epilepsy Society, the Epilepsy Foundation, National Institutes of Health (NIH) and Mayo Clinic. A formal description of these preprocessing methods is presented and a detailed analysis in terms of Receiver Operating Characteristics (ROC) curve and Area Under ROC curve is performed. The obtained results show statistical significant improvements when compared with the spectral power band filtering (PBF) typical baseline. A trend between performance and the dataset size is observed, suggesting that the supervised filters bring better information, compared to the conventional PBF filters, as the dataset grows in terms of monitored variables (sensors) and time length. The paper demonstrates a better accuracy in forecasting when new filters are employed and its main contribution is in the field of machine learning algorithms to develop more accurate predictive systems.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>Universidad CEU-Cardenal Herrera</institution>
</funding-source>
<award-id>Indicadores 2015-16</award-id>
</award-group>
<funding-statement>This work has been supported by the grant Consolidación de Indicadores CEU-UCH 2015-16 provided by Vicerrectorado de Investigación of the Universidad CEU Cardenal Herrera.</funding-statement>
</funding-group>
<counts>
<fig-count count="3"/>
<table-count count="7"/>
<page-count count="17"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The data set employed, as a third party, for this study is available publicly via the kaggle.com&lt;<ext-link ext-link-type="uri" xlink:href="http://kaggle.com" xlink:type="simple">http://kaggle.com</ext-link>&gt; platform (<ext-link ext-link-type="uri" xlink:href="http://kaggle.com/c/seizure-prediction" xlink:type="simple">http://kaggle.com/c/seizure-prediction</ext-link>) and they will remain accessible via International Epilepsy EEG Portal (<ext-link ext-link-type="uri" xlink:href="http://ieeg.org" xlink:type="simple">http://ieeg.org</ext-link>), an NIH-funded repository for sharing EEG data, and <ext-link ext-link-type="uri" xlink:href="http://msel.mayo.edu/data.html" xlink:type="simple">http://msel.mayo.edu/data.html</ext-link>. Algorithms and analysis tools developed for this paper are available at Github repository (<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5281/zenodo.804105" xlink:type="simple">http://dx.doi.org/10.5281/zenodo.804105</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>An estimation of the World Health Organization mention that about 50 million people around the world have Epilepsy [<xref ref-type="bibr" rid="pone.0178808.ref001">1</xref>]. Thus, epilepsy is a common neurological disorder affecting nearly 1% of the global population. An epileptic seizure starts with a storm of abnormal electrical activity in the brain. As it is stated in [<xref ref-type="bibr" rid="pone.0178808.ref002">2</xref>], this activity usually begins in one or two specific brain regions and can then expand to other parts of the brain. It can cause inconveniences in movement, sensation, mood and mental function. At worst cases, where a severe seizure is occurring, the person may have convulsions and lose consciousness. Such situation may become a terrible problem that disrupts the daily activity of the person who suffers from this disease.</p>
<p>Pharmacotherapy with anti-epileptic drugs is the keystone of epilepsy treatment, but 20–40% of patients continue having seizures despite medications [<xref ref-type="bibr" rid="pone.0178808.ref003">3</xref>–<xref ref-type="bibr" rid="pone.0178808.ref005">5</xref>]. People with less severe level of epilepsy also appeal to medicate themselves excessively due to the constant threat and fear of an unexpected seizure. Hence, the possibility of forecasting seizures with an acceptable confidence level, could substantially improve the treatment of the epilepsy and thus the lifestyle of the people suffering from this problem [<xref ref-type="bibr" rid="pone.0178808.ref003">3</xref>, <xref ref-type="bibr" rid="pone.0178808.ref006">6</xref>].</p>
<p>Other methods followed by neurosurgeons [<xref ref-type="bibr" rid="pone.0178808.ref002">2</xref>] sometimes appeal to cut away the pieces of brain tissue where the seizures originate, but in the past decade they have had another solution through the implant of neurostimulators. Those devices send pulses of electricity, through the nervous system, to prevent such electrical storms in the brain. But again, too many electrical pulses in the brain could not be the best solution, as the overmedication, and it continues being necessary to study the brain’s electrical activity to find accurate patterns of seizures in order to forecast them. A precise seizure prediction system could allow to patients to abstain risky activities, relax their level of anxiety or avoid taking unnecessary medication [<xref ref-type="bibr" rid="pone.0178808.ref006">6</xref>].</p>
<p>Some studies have proved the feasibility of forecasting human and canine epileptic seizures in naturally occurring epilepsy using long recording of electroencephalogram (EEG). As it is stated in [<xref ref-type="bibr" rid="pone.0178808.ref007">7</xref>–<xref ref-type="bibr" rid="pone.0178808.ref009">9</xref>] naturally occurring canine epilepsy is an excellent model for human epilepsy.</p>
<p>EEG is a multichannel recording of the brain’s electrical activity. EEG electrodes are located on the scalp or invasively in the brain (intracranial EEG, iEEG). As a result of greater proximity to neural activity, iEEG has a higher spatial resolution and signal-to-noise ratio than scalp EEG, thus it is more valuable for epilepsy research [<xref ref-type="bibr" rid="pone.0178808.ref010">10</xref>].</p>
<p>One of the most useful features extracted of the iEEG signal is the spectral power in different frequency bands, as for example (<italic>δ</italic> (0.1-4 Hz), <italic>θ</italic> (4-8 Hz), <italic>α</italic> (8-12 Hz), <italic>β</italic> (12-30 Hz), low-<italic>γ</italic> (30-70 Hz) and high-<italic>γ</italic> (70-180 Hz)), which were used in [<xref ref-type="bibr" rid="pone.0178808.ref003">3</xref>] for prediction of canine epileptic seizures classifying preictal (prior to seizure) and interictal (between seizures) states of the individual. From now on, this preprocessing method with the average of the frequencies within these ranges is called Spectral Power Band Filter (PBF). Several studies sustain the hypothesis of an existing preictal state, which is associated with distinctive iEEG waveforms and spectral patterns [<xref ref-type="bibr" rid="pone.0178808.ref003">3</xref>]. This preprocessing technique is unsupervised, i.e., the procedure does not use the class labels of the training set. It must be said that there exist supervised preprocessing techniques used in other frameworks, for example Common Spatio-spectral Pattern (CSP) [<xref ref-type="bibr" rid="pone.0178808.ref011">11</xref>] or Kernel Fisher Discriminant (KFD). In general, these feature extraction techniques are used to locate relevant channels for a neurological state [<xref ref-type="bibr" rid="pone.0178808.ref012">12</xref>, <xref ref-type="bibr" rid="pone.0178808.ref013">13</xref>].</p>
<p>National Institutes of Health (NINDS), the Epilepsy Foundation, the American Epilepsy Society and Mayo clinic sponsored an International data science competition known as “American Epilepsy Society Seizure Prediction Challenge” at the prestigious Kaggle platform. Its goal was to identify the best model for discriminating preictal vs interictal iEEG clips collected from dogs and persons. The ESAI research group participated, winning the third prize among more than 505 teams, coming from the most relevant universities and specialized enterprises from all over the world.</p>
<p>Our algorithm [<xref ref-type="bibr" rid="pone.0178808.ref014">14</xref>] considered as features, the spectral power in the six frequency bands and some other statistics. These were preprocessed by means of PCA (Principal Component Analysis) and ICA (Independent Component Analysis), and finally modeled with a combination of Artificial Neural Networks (ANN) and K-Nearest Neighbor (KNN) machine learning algorithms. In the present paper, the objective is to demonstrate how preprocessing the FFT (Fast Fourier Transform) signal, with a new approach, using supervised filters provides a much better outcome than conventional spectral power band filters or PBF. The machine learning technique selected to perform the comparison has been KNN, since the only parameter to tune is the number of neighbors. Nevertheless, it was compared with other machine learning algorithms to check whether similar conclusions can be drawn for such algorithms.</p>
<p>Consequently, the present study explores a new method of preprocessing the iEEG signal, based on the algorithm that was submitted to the competition. Such a new approach improves the final quality and performance of some learning machine techniques to detect preictal state, compared with other alternatives. The result of the study demonstrates that the bigger and more complex the volume of data acquired from the brain, the better performance for the methods we propose to detect these preictal states on the subjects under study, being the techniques presented in this paper useful for situations close to a big data problem. Next, it is described the dataset employed for the present study provided by Mayo Clinic and the basic formulae to preprocess the iEEG signal. Then, the machine learning algorithm and the ROC curve analysis are depicted describing the results of the comparison between several machine learning algorithms. Finally, discussion and conclusions state the reason why we consider the supervised filters proposed could improve epilepsy seizure forecasting and its reproducibility in other studies.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Materials and methods</title>
<p>This section presents a description of the dataset available at Kaggle which have been used to validate different preprocessing algorithms. The description and formalization of these algorithms are thoroughly developed in the rest of this section. An implementation of the algorithm is available at the repository <ext-link ext-link-type="uri" xlink:href="https://github.com/fjmalmaraz/supervised-filters" xlink:type="simple">https://github.com/fjmalmaraz/supervised-filters</ext-link> for reproducibility.</p>
<sec id="sec003">
<title>Dataset</title>
<p>The dataset, used for the challenge was provided by Mayo Clinic and it has been validated in several studies [<xref ref-type="bibr" rid="pone.0178808.ref003">3</xref>, <xref ref-type="bibr" rid="pone.0178808.ref006">6</xref>, <xref ref-type="bibr" rid="pone.0178808.ref009">9</xref>]. It was uploaded to the Kaggle platform and it is open to anyone who wants to employ it [<xref ref-type="bibr" rid="pone.0178808.ref015">15</xref>]. It is organized, as Intracranial EEG (iEEG) data clips, in folders containing training and testing data for each human or canine subject. There are data from five dogs and two persons. The training data is organized into ten-minute iEEG clips labeled “Preictal” for seizure data segments, or “Interictal” for non-seizure data segments. Training data segments are numbered sequentially, whereas testing data are in random order.</p>
<p>Preictal training and testing data segments were supplied covering one hour prior to seizure with a five-minute seizure temporal window (i.e. from 1:05 to 0:05 before seizure onset) This pre-seizure window ensures that: 1) seizures could be predicted with enough warning to allow administration of fast-acting medications, and 2) any seizure activity before the annotated onset that may have been missed by the epileptologist would not affect the outcome of the competition.</p>
<p>Similarly, one-hour sequences of interictal ten-minute data segments were also provided. The interictal data were chosen randomly from the full data record, with the restriction that interictal segments be as far from any seizure as can be practically achieved, to avoid contamination with preictal or postictal signals. In the long duration canine recordings it was possible to maintain a restriction of one week before or after a seizure. However, in the human recordings (which may be less than a week in total duration) interictal data was restricted to be more than four hours before or after any seizure.</p>
<p>iEEG data was recorded from five dogs with the naturally occurring epilepsy using an ambulatory monitoring system [<xref ref-type="bibr" rid="pone.0178808.ref003">3</xref>, <xref ref-type="bibr" rid="pone.0178808.ref009">9</xref>]. The iEEG was sampled from 16 intracranial electrodes at 400 Hz, and recorded voltages were referenced to the group average. These are long duration recordings, spanning multiple months up to a year and recording up to a hundred seizures in some dogs. The dogs were housed at the veterinary hospitals at the University of Minnesota and University of Pennsylvania.</p>
<p>For the human patients, the iEEG was sampled from 15 electrodes at 5000 Hz, the recorded voltages were referenced to an electrode outside the skull; monitoring period for was up to a week. The epilepsy patients, who underwent the iEEG monitoring, were reviewed at Mayo Clinic Rochester. Interictal data segments were chosen at random, within the restrictions commented above, for both canine and human subjects. <xref ref-type="table" rid="pone.0178808.t001">Table 1</xref> depicts characteristics of recorded data and clip selection:the division of testing and training data clips and subjects’ characteristics about their monitoring sampling rate, hours of recorded data, seizures and lead seizures. Lead seizures are defined as seizures occurring without a preceding seizure for a minimum of 4h. The data provided was supervised by epileptologists of the prestigious institutions aforementioned. A complete description is provided in [<xref ref-type="bibr" rid="pone.0178808.ref009">9</xref>].</p>
<table-wrap id="pone.0178808.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0178808.t001</object-id>
<label>Table 1</label>
<caption>
<title>Data characteristics for the Kaggle.com seizure forecasting contest.</title>
<p>Source: [<xref ref-type="bibr" rid="pone.0178808.ref009">9</xref>].</p>
</caption>
<alternatives>
<graphic id="pone.0178808.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0178808.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Subject</th>
<th align="left">Sampling rate<break/>(Hz)</th>
<th align="left">Recorded data<break/>(h)</th>
<th align="left">Seizures</th>
<th align="left">Lead Seizures</th>
<th align="left">Training clips<break/>(% interictal)</th>
<th align="left">Testing clips<break/>(% interictal)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Dog 1</td>
<td align="left">400</td>
<td align="left">1920</td>
<td align="left">22</td>
<td align="left">8</td>
<td align="left">504 (95.2)</td>
<td align="left">502 (95.2)</td>
</tr>
<tr>
<td align="left">Dog 2</td>
<td align="left">400</td>
<td align="left">8208</td>
<td align="left">47</td>
<td align="left">40</td>
<td align="left">542 (92.3)</td>
<td align="left">1000 (91.0)</td>
</tr>
<tr>
<td align="left">Dog 3</td>
<td align="left">400</td>
<td align="left">5112</td>
<td align="left">104</td>
<td align="left">18</td>
<td align="left">1512 (95.2)</td>
<td align="left">907 (95.4)</td>
</tr>
<tr>
<td align="left">Dog 4</td>
<td align="left">400</td>
<td align="left">7152</td>
<td align="left">29</td>
<td align="left">27</td>
<td align="left">901 (89.2)</td>
<td align="left">990 (94.2)</td>
</tr>
<tr>
<td align="left">Dog 5</td>
<td align="left">400</td>
<td align="left">5616</td>
<td align="left">19</td>
<td align="left">8</td>
<td align="left">480 (93.8)</td>
<td align="left">191 (93.7)</td>
</tr>
<tr>
<td align="left">Patient 1</td>
<td align="left">5000</td>
<td align="left">71.3</td>
<td align="left">5</td>
<td align="left">4</td>
<td align="left">68 (73.5)</td>
<td align="left">195 (93.9)</td>
</tr>
<tr>
<td align="left">Patient 2</td>
<td align="left">5000</td>
<td align="left">158.5</td>
<td align="left">41</td>
<td align="left">6</td>
<td align="left">60 (70.0)</td>
<td align="left">150 (90.7)</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The dataset, used for the competition, is still available at the Kaggle platform [<xref ref-type="bibr" rid="pone.0178808.ref015">15</xref>]. Thus, it is possible to continue exploring and improving the work done during the challenge. The Kaggle competition was based on the approach of long recording dataset available to explore different learning machine techniques in order to develop an accurate algorithm to forecast seizures.</p>
</sec>
<sec id="sec004">
<title>Overall risk of the optimal prediction for features</title>
<p>Generally, the success of any forecasting algorithm highly depends on an adequate selection of features, which is very often performed according to the technical expertise on the area. Nevertheless, signal or image processing generates such a huge volume of information that it is not the most efficient way just to rely on the experts’ opinion to tune the feature extraction, being convenient some automatic detection of high quality features.</p>
<p>The following approach is based on the concept of the overall risk of a predictor, which can be found for instance in [<xref ref-type="bibr" rid="pone.0178808.ref016">16</xref>]. Given some features, our objective is to determine the quality of the optimal predictor, assuming the theoretical distribution of the classes as known. Suppose there are two different classes, i.e. the variable <italic>G</italic> takes two possible values 0 and 1, and the value of this variable needs to be predicted by means of other variables which are represented by a random vector <bold>X</bold>. Let us suppose that the class-conditional probability density functions are known, being <italic>f</italic><sub>0</sub> and <italic>f</italic><sub>1</sub> the density functions for 0 and 1 classes respectively. If <italic>p</italic> is the prior probability of the class 1, a straightforward application of Bayes’ theorem provides the probability of belonging to the class 1:
<disp-formula id="pone.0178808.e001"><alternatives><graphic id="pone.0178808.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">X</mml:mi> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>p</mml:mi></mml:mrow> <mml:mrow><mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi mathvariant="bold">x</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where <italic>f</italic>(<bold>x</bold>) is the unconditional density function <italic>f</italic>(<bold>x</bold>) = <italic>f</italic><sub>1</sub>(<bold>x</bold>)<italic>p</italic> + <italic>f</italic><sub>0</sub>(<bold>x</bold>)(1 − <italic>p</italic>). The random variable <italic>G</italic> given that <bold>X</bold> = <bold>x</bold> is a Bernoulli random variable with the probability <italic>p</italic><sub><bold>x</bold></sub>. A rational choice to fit the classifier is consider the regressor <italic>α</italic>(<bold>x</bold>) = <italic>p</italic><sub><bold>x</bold></sub> that minimizes the expected value of (<italic>α</italic>(<bold>X</bold>) − <italic>G</italic>)<sup>2</sup>, considering as the loss function <italic>L</italic>(<bold>x</bold>, <italic>g</italic>) = (<italic>p</italic><sub><bold>x</bold></sub> − <italic>g</italic>)<sup>2</sup>. Notice that other loss functions are possible, such as cross-entropy loss function, but they are out of the scope of this paper. The risk given a vector <bold>x</bold> of features is the expected value of the loss associated:
<disp-formula id="pone.0178808.e002"><alternatives><graphic id="pone.0178808.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>R</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>L</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>G</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>g</mml:mi> <mml:mo>∈</mml:mo> <mml:mo>{</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>}</mml:mo></mml:mrow></mml:munder> <mml:mi>L</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>g</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>P</mml:mi> <mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo>=</mml:mo> <mml:mi>g</mml:mi> <mml:mo>|</mml:mo> <mml:mi>X</mml:mi> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi mathvariant="bold">x</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi mathvariant="bold">x</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
Therefore, the risk is <italic>R</italic>(<bold>x</bold>) = <italic>p</italic>(1 − <italic>p</italic>)<italic>f</italic><sub>1</sub>(<bold>x</bold>)<italic>f</italic><sub>0</sub>(<bold>x</bold>)/<italic>f</italic>(<bold>x</bold>)<sup>2</sup> and the overall risk is the integral of the risk multiplied by the unconditional density function is given by
<disp-formula id="pone.0178808.e003"><alternatives><graphic id="pone.0178808.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo> <mml:mi>R</mml:mi> <mml:mo>⟩</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mo>∫</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mi>n</mml:mi></mml:msup></mml:msub> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>p</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mfrac><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>f</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mi>f</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>d</mml:mi> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>=</mml:mo> <mml:mi>p</mml:mi> <mml:mspace width="0.166667em"/><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>p</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mo>∫</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mi>n</mml:mi></mml:msup></mml:msub> <mml:mfrac><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:msub><mml:mi>f</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mi>f</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mspace width="0.166667em"/><mml:mi>d</mml:mi> <mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
This value can be interpreted as the expected value of <italic>p</italic>(1 − <italic>p</italic>)<italic>f</italic><sub>1</sub>(<bold>Z</bold>)/<italic>f</italic>(<bold>Z</bold>) where <bold>Z</bold> is a random variable whose density function is <italic>f</italic><sub>0</sub>. The smaller the overall risk, the more useful the random vector of features <bold>X</bold> to predict the class <italic>G</italic>. If possible, the preprocessing of the features like the FFT signal for epileptic prediction should be performed in such a way that the overall risk of the new variables does not rise abruptly, being the potential prediction capability of our features lost.</p>
</sec>
<sec id="sec005">
<title>A lower bound of the overall risk</title>
<p>A set of features whose overall risk reaches the minimum of the optimal predictor would be the perfect choice for the problem of selecting the most useful variables to predict a binary classification. In most of the situations, a straightforward optimization of the overall risk would require long calculations if the number of original features is huge. Therefore, our proposal is less ambitious and, in this paper, we start with the selection of features trying to get a minimal value of a rough estimation of the lower bound of the overall risk. Our expectations are that this provides one of the best feature selection to perform a good prediction. This bound will be used as an insight to develop preprocessing algorithms for supervised classification. Let <italic>f</italic><sub><italic>max</italic></sub> be the maximum value of the unconditional density function, which can be considered a measure of the spread of the distribution. Therefore, <italic>p</italic>(1 − <italic>p</italic>)<italic>E</italic>[<italic>f</italic><sub>1</sub>(<bold>Z</bold>)/<italic>f</italic>(<bold>Z</bold>)] is less than (<italic>p</italic>(1 − <italic>p</italic>)/<italic>f</italic><sub><italic>max</italic></sub>)<italic>E</italic>[<italic>f</italic><sub>1</sub>(<bold>Z</bold>)] where <bold>Z</bold> is a random variable with probability density function <italic>f</italic><sub>0</sub>.</p>
<p>A kernel density estimator is used to approach <italic>E</italic>[<italic>f</italic><sub>1</sub>(<bold>Z</bold>)] with
<disp-formula id="pone.0178808.e004"><alternatives><graphic id="pone.0178808.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mi>n</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>h</mml:mi></mml:mrow></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="bold">Y</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow> <mml:mi>h</mml:mi></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mi>n</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>h</mml:mi></mml:mrow></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="bold">Y</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow> <mml:mi>h</mml:mi></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>h</mml:mi></mml:mfrac> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi></mml:mrow> <mml:mi>h</mml:mi></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
where <italic>K</italic> is the kernel, i.e. positive function whose integral is one and <bold>Y</bold><sub><italic>j</italic></sub> are independent and identical variables whose density function is <italic>f</italic><sub>1</sub>. For instance, <italic>K</italic>(<bold>x</bold>) = exp(−∥<bold>x</bold>∥<sup>2</sup>). Since the exponential function is convex, by Jensen’s inequality we have
<disp-formula id="pone.0178808.e005"><alternatives><graphic id="pone.0178808.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mi>E</mml:mi> <mml:mo>[</mml:mo> <mml:mo>∥</mml:mo></mml:mrow> <mml:mfrac><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi></mml:mrow> <mml:mi>h</mml:mi></mml:mfrac> <mml:msup><mml:mrow><mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>]</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≤</mml:mo> <mml:mrow><mml:mi>E</mml:mi> <mml:mo>[</mml:mo> <mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mo>∥</mml:mo></mml:mrow> <mml:mfrac><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi></mml:mrow> <mml:mi>h</mml:mi></mml:mfrac> <mml:msup><mml:mrow><mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>)</mml:mo> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi>K</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi></mml:mrow> <mml:mi>h</mml:mi></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
In order to minimize the lower bound of the approximation to the variance of the prediction, we have to maximize <inline-formula id="pone.0178808.e006"><alternatives><graphic id="pone.0178808.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:mi>E</mml:mi> <mml:mo>[</mml:mo> <mml:mo>∥</mml:mo> <mml:mfrac><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi></mml:mrow> <mml:mi>h</mml:mi></mml:mfrac> <mml:msup><mml:mo>∥</mml:mo> <mml:mn>2</mml:mn></mml:msup> <mml:mo>]</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, since the function exp(−<italic>x</italic>) is decreasing. Therefore, after fixing a value for <italic>h</italic>, the lower bound of the variance is
<disp-formula id="pone.0178808.e007"><alternatives><graphic id="pone.0178808.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e007" xlink:type="simple"/><mml:math display="block" id="M7"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mo form="prefix">exp</mml:mo> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mi>E</mml:mi> <mml:mo>[</mml:mo> <mml:mo>∥</mml:mo></mml:mrow> <mml:mfrac><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi></mml:mrow> <mml:mi>h</mml:mi></mml:mfrac> <mml:msup><mml:mrow><mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>]</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>h</mml:mi> <mml:mo>·</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>a</mml:mi> <mml:mi>x</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
Taking this expression as an approach to the overall risk, the overall risk of our features decreases with the expected value of the square of the difference and with the unconditional variability given by <italic>f</italic><sub><italic>max</italic></sub>. Following these ideas, a method to choose the variables is that the expected value of the squared difference of them must be as large as possible. This approach is developed in the next section.</p>
</sec>
<sec id="sec006">
<title>Projection with an optimal lower bound (SqD)</title>
<p>In order to perform a binary classification using a large number of features, such as FFT using long input windows, becomes necessary a data preprocessing prior to use any machine learning algorithm, summarizing all these features in a small vector. The simplest summary is a linear combination or mixture of variables. Geometrically speaking, a set of variables is going to be summarized by a number employing a linear projection <italic>π</italic><sub><bold>u</bold></sub> where <bold>u</bold> is a unit vector on the direction along the features are projected, i.e. the projection is a scalar product <italic>π</italic><sub><bold>u</bold></sub>(<bold>x</bold>) = <bold>u</bold> ⋅ <bold>x</bold>.</p>
<p>We consider <bold>Z</bold> a random vector of features under the condition of belonging to the class <italic>G</italic> = 0 and, analogously, <bold>Y</bold> a random vector of features given <italic>G</italic> = 1. Our objective is to minimize the numerator of the lower bound and this is equivalent to maximize the objective function <italic>E</italic>[(<italic>π</italic><sub><bold>u</bold></sub>(<bold>Z</bold>) − <italic>π</italic><sub><bold>u</bold></sub>(<bold>Y</bold>))<sup>2</sup>] where <bold>u</bold> is a unit vector. Applying Lagrange’s multipliers to the objective function and restriction ∥<bold>u</bold>∥<sup>2</sup> = 1, the vector <bold>u</bold> where the optimal solution is attained is a critical point of the function of <bold>u</bold> and λ,
<disp-formula id="pone.0178808.e008"><alternatives><graphic id="pone.0178808.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi mathvariant="bold">u</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mspace width="0.166667em"/><mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>]</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:mo>+</mml:mo> <mml:mo>λ</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mo>∥</mml:mo><mml:mi mathvariant="bold">u</mml:mi><mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
Vanishing the derivatives of this function, we got the equations to find the critical point
<disp-formula id="pone.0178808.e009"><alternatives><graphic id="pone.0178808.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mn>2</mml:mn> <mml:mi>E</mml:mi> <mml:mo>[</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>]</mml:mo> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:mo>-</mml:mo> <mml:mn>2</mml:mn> <mml:mo>λ</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:mo>=</mml:mo> <mml:mn mathvariant="bold">0</mml:mn> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
for unit vector <bold>u</bold>. Therefore, the critical points of <xref ref-type="disp-formula" rid="pone.0178808.e008">Eq (7)</xref> are the eigenvectors of the matrix <italic>E</italic>[(<bold>Z</bold> − <bold>Y</bold>)(<bold>Z</bold> − <bold>Y</bold>)<sup><italic>T</italic></sup>]. To get the projection <italic>π</italic><sub><bold>u</bold></sub>, we have selected the eigenvector with the largest eigenvalue by means of the power iteration algorithm. One of the main drawbacks to solve this equation is that the matrix can exceed the available computational resources. Taking into account that the data to calculate the matrix are distributed in multiple files, it is possible to apply this algorithm without storing the full matrix in memory. Let <bold>Z</bold><sub><italic>j</italic></sub> and <bold>Y</bold><sub><italic>j</italic></sub> be a sample from the training data. The initial vector <bold>u</bold><sup>(0)</sup> = <bold>v</bold>/∥<bold>v</bold>∥ where <bold>v</bold> is a vector such that all the components are 1. For the <italic>k</italic>th iteration, given the previous vector <bold>u</bold><sup>(<italic>k</italic>−1)</sup>, we multiply this vector by an estimation of the matrix <italic>E</italic>[(<bold>Z</bold> − <bold>Y</bold>)(<bold>Z</bold> − <bold>Y</bold>)<sup><italic>T</italic></sup>])]
<disp-formula id="pone.0178808.e010"><alternatives><graphic id="pone.0178808.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi mathvariant="bold">v</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:munder> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">Z</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="bold">Y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msup><mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">Z</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="bold">Y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
and <bold>u</bold><sup>(<italic>k</italic>)</sup> is the normalized vector of <bold>v</bold><sup>(<italic>k</italic>)</sup>, i.e. <bold>u</bold><sup>(<italic>k</italic>)</sup> = <bold>v</bold><sup>(<italic>k</italic>)</sup>/∥<bold>v</bold><sup>(<italic>k</italic>)</sup>∥.</p>
</sec>
<sec id="sec007">
<title>Alternative methods</title>
<p>Using a wider interpretation of the estimation of the lower bound, we are proposing several preprocessing methods based on the idea that the larger is the expected differences between two signals of distinct classes and the smaller is the unconditional variance, the better forecasting results are expected to get. Our alternatives consider the difference of the means (DM), the difference of the variance between the two clases (VAR), a linear combination which tries to trade off the variance and the mean (TVM) and a very simplified preprocessing method which consists in the difference of the square (DS). The last method winds up to be the more effective filters according to the experimental results.</p>
<sec id="sec008">
<title>Difference of the Mean(DM)</title>
<p>The most intuitive idea is to estimate a projection <italic>π</italic><sub><italic>u</italic></sub> such that the difference of projected vectors mean have the larger possible value. Hence, the objective function is <italic>E</italic>[<italic>π</italic><sub><bold>u</bold></sub>(<bold>Z</bold>) − <italic>π</italic><sub><bold>u</bold></sub>(<bold>Y</bold>)] = <italic>E</italic>[<bold>u</bold> ⋅ (<bold>Z</bold> − <bold>Y</bold>)]. This function is the scalar product with the vector difference of the mean <italic>E</italic>[(<bold>Z</bold> − <bold>Y</bold>)] ⋅ <bold>u</bold> and the maximum is attained at the point <bold>u</bold> = <italic>E</italic>[(<bold>Z</bold> − <bold>Y</bold>)]/∥<italic>E</italic>[(<bold>Z</bold> − <bold>Y</bold>)]∥. This preprocessing is straightforward and no iterative process is needed.</p>
</sec>
<sec id="sec009">
<title>Variance (VAR)</title>
<p>Most of literature preprocessing methods consist in maximizing the variance to learn new discriminative features. For binary classification, some methods, like Common Spatial Patterns (CSP), maximize the generalized Rayleigh quotient whose solutions are generalized eigenvalues [<xref ref-type="bibr" rid="pone.0178808.ref011">11</xref>, <xref ref-type="bibr" rid="pone.0178808.ref017">17</xref>, <xref ref-type="bibr" rid="pone.0178808.ref018">18</xref>]. The main drawback of this method is that the covariance matrix must be computed and full stored in the computer. For a problem with a large number of features this procedute cannot be performed. The difference between the variances can be expected to have one of the classes with a large variance and the other class with a smaller variance. In this situation, the objective function is
<disp-formula id="pone.0178808.e011"><alternatives><graphic id="pone.0178808.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi mathvariant="bold">u</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi mathvariant="bold">u</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi mathvariant="bold">u</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi mathvariant="bold">u</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
being equals to
<disp-formula id="pone.0178808.e012"><alternatives><graphic id="pone.0178808.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi mathvariant="bold">u</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi mathvariant="bold">u</mml:mi></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>E</mml:mi> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi mathvariant="bold">u</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>E</mml:mi> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi mathvariant="bold">u</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
This is a quadratic function whose associated matrix is:
<disp-formula id="pone.0178808.e013"><alternatives><graphic id="pone.0178808.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e013" xlink:type="simple"/><mml:math display="block" id="M13"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi> <mml:mo>=</mml:mo> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:msup><mml:mi mathvariant="bold">Z</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi> <mml:msup><mml:mi mathvariant="bold">Y</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>E</mml:mi> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>+</mml:mo> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>E</mml:mi> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
This matrix <italic>S</italic> is symmetric. Therefore, the maximum of the quadratic form <bold>u</bold><sup><italic>T</italic></sup> <italic>S</italic><bold>u</bold> restricted to unit vectors <bold>u</bold> is the eigenvector associated to the largest eigenvalue. The largest eigenvalue can be calculated with the power iterative method based on successive multiplications by the matrix and division of the outcome by its norm. For a large scale data set, it is possible to perform multiplications of the matrix <italic>S</italic> by a vector without storing the matrix in main memory in a similar way as done in <xref ref-type="disp-formula" rid="pone.0178808.e010">Eq (9)</xref>.</p>
</sec>
<sec id="sec010">
<title>Trade-off Variance-Mean (TVM)</title>
<p>Another proposal for objective function appears searching a trade off between the mean and the variance. Therefore, the objective function is the sum of square of the objective function for the mean and the variance difference. In this situation, the algorithm is to solve a quadratic program with the objective function
<disp-formula id="pone.0178808.e014"><alternatives><graphic id="pone.0178808.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e014" xlink:type="simple"/><mml:math display="block" id="M14"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>E</mml:mi> <mml:mo>[</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi mathvariant="bold">u</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi mathvariant="bold">u</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>V</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi mathvariant="bold">u</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>V</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi mathvariant="bold">u</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula>
whose critical points are the eigenvectors of the following matrix
<disp-formula id="pone.0178808.e015"><alternatives><graphic id="pone.0178808.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:msup><mml:mi mathvariant="bold">Z</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi> <mml:msup><mml:mi mathvariant="bold">Y</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>E</mml:mi> <mml:msup><mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>+</mml:mo> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">Y</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>E</mml:mi> <mml:mrow><mml:mo>[</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula>
The calculations are performed analogously to the previous sections.</p>
</sec>
<sec id="sec011">
<title>Squared simplification of the trade-off (DS)</title>
<p>In the previous objective function, the expression can be simplified removing the terms which are depending on the expected values of the variables. Hence, the objective function becomes <italic>E</italic>[(<italic>π</italic><sub><bold>u</bold></sub>(<bold>Z</bold>))<sup>2</sup> − (<italic>π</italic><sub><bold>u</bold></sub>(<bold>Y</bold>))<sup>2</sup>] whose interpretation is that the projection is maximizing the difference of the squares of the variables between the classes. The objective function can be rewritten as a quadratic form where the matrix of the quadratic form is the tensor product of both variables multiplied by themselves.</p>
<disp-formula id="pone.0178808.e016">
<alternatives>
<graphic id="pone.0178808.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e016" xlink:type="simple"/>
<mml:math display="block" id="M16">
<mml:mtable displaystyle="true">
<mml:mtr>
<mml:mtd columnalign="right">
<mml:mrow>
<mml:mi>E</mml:mi>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">u</mml:mi>
<mml:mi mathvariant="bold">Z</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mo>-</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">u</mml:mi>
<mml:mi mathvariant="bold">Y</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:msup>
<mml:mi mathvariant="bold">u</mml:mi>
<mml:mi>T</mml:mi>
</mml:msup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>E</mml:mi>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:mi mathvariant="bold">Z</mml:mi>
<mml:msup>
<mml:mi mathvariant="bold">Z</mml:mi>
<mml:mi>T</mml:mi>
</mml:msup>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mo>-</mml:mo>
<mml:mi>E</mml:mi>
<mml:mrow>
<mml:mo>[</mml:mo>
<mml:mi mathvariant="bold">Y</mml:mi>
<mml:msup>
<mml:mi mathvariant="bold">Y</mml:mi>
<mml:mi>T</mml:mi>
</mml:msup>
<mml:mo>]</mml:mo>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mi mathvariant="bold">u</mml:mi>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:math>
</alternatives>
<label>(15)</label>
</disp-formula>
<p>During the following sections we are going to compare all these methods with several machine learning algorithms, being the focus on KNN.</p>
</sec>
</sec>
<sec id="sec012">
<title>K-Nearest neighbors classifier</title>
<p>K-Nearest Neighbors (KNN) models were chosen because of their non-parametric nature, since the training data are the model itself. A KNN model has one sole hyper-parameter that is the number of neighbors, denoted by <italic>K</italic>. The <italic>K</italic> neighbors given by KNN were transformed into probabilities following the implementation given in APRIL-ANN toolkit [<xref ref-type="bibr" rid="pone.0178808.ref019">19</xref>], similar to [<xref ref-type="bibr" rid="pone.0178808.ref020">20</xref>]. It basically computes a posterior probability by normalizing the exponential of the negative distances, following this equation:
<disp-formula id="pone.0178808.e017"><alternatives><graphic id="pone.0178808.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi> <mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>∈</mml:mo> <mml:msub><mml:mi mathvariant="script">K</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:munder> <mml:mo form="prefix">exp</mml:mo> <mml:mfenced close=")" open="("><mml:msubsup><mml:mrow><mml:mo>-</mml:mo> <mml:mo>∥</mml:mo> <mml:mi mathvariant="bold">y</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mfenced></mml:mrow></mml:mstyle> <mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">K</mml:mi></mml:mrow></mml:munder> <mml:mo form="prefix">exp</mml:mo> <mml:mfenced close=")" open="("><mml:msubsup><mml:mrow><mml:mo>-</mml:mo> <mml:mo>∥</mml:mo> <mml:mi mathvariant="bold">y</mml:mi> <mml:mo>-</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>∥</mml:mo></mml:mrow> <mml:mn>2</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:mfenced></mml:mrow></mml:mstyle></mml:mfrac></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula>
where <bold>x</bold> is an input sample, <inline-formula id="pone.0178808.e018"><alternatives><graphic id="pone.0178808.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mi mathvariant="script">K</mml:mi></mml:math></alternatives></inline-formula> is the set of <italic>K</italic>-neighbors of <bold>x</bold>, <inline-formula id="pone.0178808.e019"><alternatives><graphic id="pone.0178808.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:msub><mml:mi mathvariant="script">K</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula> is the intersection of <inline-formula id="pone.0178808.e020"><alternatives><graphic id="pone.0178808.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mi mathvariant="script">K</mml:mi></mml:math></alternatives></inline-formula> and the class <italic>G</italic> = 1. The adequate value of <italic>K</italic> and the computation of probabilities using distances reduce the impact of over-fitting in the KNN model.</p>
<p>The hyper-parameter <italic>K</italic> was estimated during Kaggle challenge according to the performance in a cross-validation scheme and the public AUC, using for this estimation PBF preprocessing for each channel. The number of blocks has been set to the number of seizures recorded for every particular subject, leaving one seizure out for validation. A value <italic>K</italic> = 40 neighbors achieved good AUC for cross-validation and the best AUC at Kaggle public test partition. Since the hyper-parameter <italic>K</italic> = 40 has been chosen with PBF preprocessing, proposed preprocessing techniques could have a different optimal values for <italic>K</italic>. Therefore, our conclusions would be expected to be biased in favour of PBF.</p>
</sec>
<sec id="sec013">
<title>Prediction procedure</title>
<p>The goal of the system is to produce a posterior probability for preictal class given a file with 10 minutes of iEEG recording. The system is a pipeline of several methods divided in two stages: preprocessing and classification. Such system is an adaptation of the one presented in [<xref ref-type="bibr" rid="pone.0178808.ref006">6</xref>] and it is available for reproducibility issues at [<xref ref-type="bibr" rid="pone.0178808.ref014">14</xref>].</p>
<p>Starting at preprocessing stage, 1-minute Hamming windows are generated with 30 seconds of overlap from 10-minute iEEG signal, FFT transformed, log compressed and filtered according to the methods described in this paper. PBF filter is used in literature to summarize the FFT in six features per channel, according to the frequency bands <italic>δ</italic> (0.1-4 Hz), <italic>θ</italic> (4-8 Hz), <italic>α</italic> (8-12 Hz), <italic>β</italic> (12-30 Hz), low-<italic>γ</italic> (30-70 Hz) and high-<italic>γ</italic> (70-180 Hz). For each band, PBF computes an uniform average of all FFT bins corresponding to the frequencies within the band. All other supervised filtering methods proposed in this paper summarize the FFT into the same six bands. Let us remark that, instead of an arithmetic mean, these methods compute a weighted average of the FFT bins related with the frequencies of each band. These weights are estimated following the procedures described in the previous section, highlighting the most important frequencies in order to improve discrimination between classes. <xref ref-type="fig" rid="pone.0178808.g001">Fig 1</xref> shows how preprocessing stage is performed.</p>
<fig id="pone.0178808.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0178808.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Preprocessing stage diagram.</title>
<p>A sliding window is used to extract 60-second windows which are processed by FFT to generate a large vector. This vector is preprocessed with 6 filters located at the corresponding 6 bands (<italic>δ</italic>, <italic>θ</italic>, <italic>α</italic>, <italic>β</italic>, low-<italic>γ</italic>, high-<italic>γ</italic>). These filters have been previously calculated with the preprocessing methods described in this paper. Finally the output of the filters is log-compressed with the function <monospace>log1p</monospace> (Natural logarithm of 1 + <italic>x</italic>, element-wise). This procedure is repeated for each available iEEG channel.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0178808.g001" xlink:type="simple"/>
</fig>
<p>The second stage consists in the classification of an input file given its extracted features. The classifier produces a posterior probability of preictal state for each time slice of FFT sliding window. Due to there are 19 time intervals for every 10-minute input file, the model produces 19 posterior probabilities for each input file. Thus, in order to compute preictal posterior probability for one file it is required to aggregate these 19 probabilities. In order to increase sensitivity, these posterior probabilities are aggregated into one value following a geometric mean but complementing each input probability. Finally, complement operation is repeated to obtain the posterior preictal probability for the given input file. <xref ref-type="disp-formula" rid="pone.0178808.e021">Eq (17)</xref> formalizes this process:
<disp-formula id="pone.0178808.e021"><alternatives><graphic id="pone.0178808.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0178808.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi> <mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mtext>preictal</mml:mtext> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mroot><mml:mrow><mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:mfenced close=")" open="(" separators=""><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>P</mml:mi> <mml:mi>r</mml:mi> <mml:mo>(</mml:mo> <mml:mtext>preictal</mml:mtext> <mml:mo>|</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi mathvariant="bold">t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mfenced></mml:mrow> <mml:mi>n</mml:mi></mml:mroot></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula>
where <italic>n</italic> = 19 is the number of window slices, <bold>x</bold> is a matrix with <italic>n</italic> feature vectors and <italic>p</italic>(preictal|<bold>x</bold><sub><italic>t</italic></sub>) is posterior probability of KNN for feature vector <italic>t</italic> computed using <xref ref-type="disp-formula" rid="pone.0178808.e017">Eq (16)</xref>.</p>
</sec>
</sec>
<sec id="sec014">
<title>Results-ROC analysis</title>
<p>Using the Receiver Operating Characteristic (ROC) curve a quantitative assessment of the model can be obtained and therefore it is possible to represent the trade-off between sensitivity and specificity of the underlying model. Thus, an optimal point can be found in the curve in order to decide when a sample should be classified as true or false by the model. The ROC curve is created by evaluating the class probabilities for the model across a continuum of thresholds.</p>
<p>In <xref ref-type="fig" rid="pone.0178808.g002">Fig 2</xref>, the ROC curves of the KNN predictions with the preprocessing methods are compared. The greater area under the curve, the better the model is for the prediction. Accordingly, Kaggle competition states to use the Area Under Curve (AUC) as a measure of model goodness. The values calculated by the Kaggle platform for the preprocessing methods in the public and private set are shown in <xref ref-type="table" rid="pone.0178808.t002">Table 2</xref>, using KNN as machine learning algorithm. All the proposed methods improve considerably the performance of the overall system with respect to the PBF, in both public and private sets as shown in <xref ref-type="table" rid="pone.0178808.t002">Table 2</xref>.</p>
<fig id="pone.0178808.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0178808.g002</object-id>
<label>Fig 2</label>
<caption>
<title>ROC curves for the preprocessing methods.</title>
<p>Following the notation in the paper: PBF (dark green), DS (orange), VAR (blue), DM (pink), TVM (light blue), Sq.diff (yellow) for all the test set (public+private) given by the Kaggle contest.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0178808.g002" xlink:type="simple"/>
</fig>
<table-wrap id="pone.0178808.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0178808.t002</object-id>
<label>Table 2</label>
<caption>
<title>Public and private scores in the American Epilepsy Society Seizure Prediction Challenge for KNN with 40 neighbors for the preprocessing methods.</title>
</caption>
<alternatives>
<graphic id="pone.0178808.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0178808.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Method</th>
<th align="left">PBF</th>
<th align="left">DS</th>
<th align="left">VAR</th>
<th align="left">DM</th>
<th align="left">TVM</th>
<th align="left">SqD</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Public</td>
<td align="char" char=".">0.67589</td>
<td align="char" char=".">0.73958</td>
<td align="char" char=".">0.71413</td>
<td align="char" char=".">0.71106</td>
<td align="char" char=".">0.73047</td>
<td align="char" char=".">0.73967</td>
</tr>
<tr>
<td align="left">Private</td>
<td align="char" char=".">0.66939</td>
<td align="char" char=".">0.72846</td>
<td align="char" char=".">0.70507</td>
<td align="char" char=".">0.70198</td>
<td align="char" char=".">0.71593</td>
<td align="char" char=".">0.71347</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>With the purpose of answering whether or not the differences seen in <xref ref-type="fig" rid="pone.0178808.g002">Fig 2</xref> are due to chance, Delong’s method [<xref ref-type="bibr" rid="pone.0178808.ref021">21</xref>] has been used to test AUC differences between standard PBF preprocessing and our methods. The p-values for these testings are shown in <xref ref-type="table" rid="pone.0178808.t003">Table 3</xref> and it can be observed that all the proposed methods got a significant difference compared with the prediction using the conventional standard band filters (PBF). However, we have not detected any significant difference between the supervised preprocessing methods proposed in this paper, except for the DM method which is the poorest method of the proposed ones. Moreover, the AUC differences have also been tested for every individual, those whose volume of data is large enough, have undergone a much better performance with the supervised preprocessing methods as it is shown in <xref ref-type="table" rid="pone.0178808.t003">Table 3</xref>.</p>
<table-wrap id="pone.0178808.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0178808.t003</object-id>
<label>Table 3</label>
<caption>
<title>Comparison with the standard band filters (PBF) and the preprocessing techniques.</title>
</caption>
<alternatives>
<graphic id="pone.0178808.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0178808.t003" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center"/>
<th align="right">DS</th>
<th align="right">VAR</th>
<th align="right">DM</th>
<th align="right">TVM</th>
<th align="right">SqD</th>
<th align="right">Features</th>
<th align="right">Samples</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">All</td>
<td align="right">2.2e-08‡</td>
<td align="right">0.00034‡</td>
<td align="right">0.0084‡</td>
<td align="right">2.6e-06‡</td>
<td align="right">3.2e-06‡</td>
<td align="right">-</td>
<td align="right">4 057</td>
</tr>
<tr>
<td align="center">Dog 1</td>
<td align="right">0.1</td>
<td align="right">0.14</td>
<td align="right">0.35</td>
<td align="right">0.033♢</td>
<td align="right">0.45</td>
<td align="right">262 144</td>
<td align="right">504</td>
</tr>
<tr>
<td align="center">Dog 2</td>
<td align="right">0.61</td>
<td align="right">0.00029♢</td>
<td align="right">8.2e-06♢</td>
<td align="right">0.024♢</td>
<td align="right">0.7</td>
<td align="right">262 144</td>
<td align="right">542</td>
</tr>
<tr>
<td align="center">Dog 3</td>
<td align="right">3e-12‡</td>
<td align="right">4.5e-09‡</td>
<td align="right">1e-07‡</td>
<td align="right">3.2e-09‡</td>
<td align="right">9e-08‡</td>
<td align="right">262 144</td>
<td align="right">1512</td>
</tr>
<tr>
<td align="center">Dog 4</td>
<td align="right">0.0025‡</td>
<td align="right">0.033‡</td>
<td align="right">0.0085‡</td>
<td align="right">0.024‡</td>
<td align="right">0.21</td>
<td align="right">262 144</td>
<td align="right">901</td>
</tr>
<tr>
<td align="center">Dog 5</td>
<td align="right">0.13</td>
<td align="right">0.16</td>
<td align="right">0.12</td>
<td align="right">0.64</td>
<td align="right">0.12</td>
<td align="right">245 760</td>
<td align="right">480</td>
</tr>
<tr>
<td align="center">Patient 1</td>
<td align="right">0.22</td>
<td align="right">0.98</td>
<td align="right">0.68</td>
<td align="right">0.48</td>
<td align="right">0.18</td>
<td align="right">3 932 160</td>
<td align="right">68</td>
</tr>
<tr>
<td align="center">Patient 2</td>
<td align="right">0.025‡</td>
<td align="right">0.028‡</td>
<td align="right">0.6</td>
<td align="right">0.00016‡</td>
<td align="right">0.08</td>
<td align="right">6 291 456</td>
<td align="right">50</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t003fn001">
<p>For every cell the p-value is pointed out, being the null hypothesis that the preprocessing method in the column has the same AUC as PBF preprocessing method. The number of features for an individual is the product of the number of frequencies and the number of channels. The column Samples is the number of observations for every individual in the training set. If the proposed preprocessing method is significantly better that PBF, the cell is marked with ‡. The case of the difference being favourable to PBF is marked with ♢.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>A more detailed analysis can be performed comparing AUC between subjects for each of the proposed methods and the PBF method. Two random partitions of all Kaggle test data have been created to this purpose, using same Kaggle proportions which are 40% of the data for validation and 60% for test. Due to the fact that hyperparameters have been chosen with the previous partition, AUC calculations could be slightly optimistic. Nevertheless, hyperparameters have not been chosen according to performance with the proposed filters, hence AUC calculations with these filters are expected to be less optimistic than with PBF. Tables <xref ref-type="table" rid="pone.0178808.t004">4</xref> and <xref ref-type="table" rid="pone.0178808.t005">5</xref> show the results of AUC for the different individuals and for the total data (pool column), which is evidence of generalization of these techniques. The pool column demonstrates that our filters improve the AUC results in the validation and test sets. All the proposed filters got an AUC result above 0.7 whereas conventional filters (PBF) are around 0.67 in the best case. Such results are coherent with the p-value of all the dataset in <xref ref-type="table" rid="pone.0178808.t003">Table 3</xref>, which proves that there is a significant improvement with these methods.</p>
<table-wrap id="pone.0178808.t004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0178808.t004</object-id>
<label>Table 4</label>
<caption>
<title>AUC results for the validation set.</title>
</caption>
<alternatives>
<graphic id="pone.0178808.t004g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0178808.t004" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Model</th>
<th align="left">Filter</th>
<th align="center">Dog 1</th>
<th align="center">Dog 2</th>
<th align="center">Dog 3</th>
<th align="center">Dog 4</th>
<th align="center">Dog 5</th>
<th align="center">Pat. 1</th>
<th align="center">Pat. 2</th>
<th align="center">Pool</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">KNN</td>
<td align="left">PBF</td>
<td align="char" char="."><bold>0.804</bold></td>
<td align="char" char=".">0.807</td>
<td align="char" char=".">0.547</td>
<td align="char" char=".">0.737</td>
<td align="char" char=".">0.517</td>
<td align="char" char=".">0.611</td>
<td align="char" char=".">0.458</td>
<td align="char" char=".">0.663</td>
</tr>
<tr>
<td align="left"/>
<td align="left">DS</td>
<td align="char" char=".">0.750</td>
<td align="char" char=".">0.809</td>
<td align="char" char=".">0.716</td>
<td align="char" char="."><bold>0.796</bold></td>
<td align="char" char=".">0.466</td>
<td align="char" char=".">0.737</td>
<td align="char" char=".">0.616</td>
<td align="char" char="."><bold>0.732</bold></td>
</tr>
<tr>
<td align="left"/>
<td align="left">VAR</td>
<td align="char" char=".">0.738</td>
<td align="char" char=".">0.724</td>
<td align="char" char=".">0.672</td>
<td align="char" char=".">0.779</td>
<td align="char" char=".">0.479</td>
<td align="char" char=".">0.691</td>
<td align="char" char=".">0.537</td>
<td align="char" char="."><bold>0.699</bold></td>
</tr>
<tr>
<td align="left"/>
<td align="left">DM</td>
<td align="char" char="."><bold>0.804</bold></td>
<td align="char" char=".">0.710</td>
<td align="char" char="."><bold>0.765</bold></td>
<td align="char" char=".">0.774</td>
<td align="char" char="."><bold>0.524</bold></td>
<td align="char" char=".">0.672</td>
<td align="char" char=".">0.378</td>
<td align="char" char="."><bold>0.706</bold></td>
</tr>
<tr>
<td align="left"/>
<td align="left">TVM</td>
<td align="char" char=".">0.726</td>
<td align="char" char=".">0.756</td>
<td align="char" char=".">0.675</td>
<td align="char" char=".">0.769</td>
<td align="char" char=".">0.493</td>
<td align="char" char=".">0.691</td>
<td align="char" char="."><bold>0.675</bold></td>
<td align="char" char="."><bold>0.708</bold></td>
</tr>
<tr>
<td align="left"/>
<td align="left">SqD</td>
<td align="char" char=".">0.777</td>
<td align="char" char="."><bold>0.812</bold></td>
<td align="char" char=".">0.680</td>
<td align="char" char=".">0.767</td>
<td align="char" char=".">0.473</td>
<td align="char" char="."><bold>0.754</bold></td>
<td align="char" char=".">0.593</td>
<td align="char" char="."><bold>0.720</bold></td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t004fn001">
<p>For this comparison, AUCs have been calculated for the validation set which is a sample of the 40% of test data available at Kaggle. The bold-faced number is the best performance for the individual got with the different classifier systems. Average of subjects AUC is shown at column Average. Pool column shows AUC computed after union of all subject predictions, as done by Kaggle framework.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="pone.0178808.t005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0178808.t005</object-id>
<label>Table 5</label>
<caption>
<title>AUC results for test set.</title>
</caption>
<alternatives>
<graphic id="pone.0178808.t005g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0178808.t005" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Model</th>
<th align="left">Filter</th>
<th align="center">Dog 1</th>
<th align="center">Dog 2</th>
<th align="center">Dog 3</th>
<th align="center">Dog 4</th>
<th align="center">Dog 5</th>
<th align="center">Pat. 1</th>
<th align="center">Pat. 2</th>
<th align="center">Pool</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">KNN</td>
<td align="left">PBF</td>
<td align="char" char="."><bold>0.885</bold></td>
<td align="char" char="."><bold>0.756</bold></td>
<td align="char" char=".">0.673</td>
<td align="char" char=".">0.777</td>
<td align="char" char="."><bold>0.585</bold></td>
<td align="char" char=".">0.711</td>
<td align="char" char=".">0.470</td>
<td align="char" char=".">0.679</td>
</tr>
<tr>
<td align="left"/>
<td align="left">DS</td>
<td align="char" char=".">0.865</td>
<td align="char" char=".">0.741</td>
<td align="char" char=".">0.796</td>
<td align="char" char=".">0.823</td>
<td align="char" char=".">0.462</td>
<td align="char" char=".">0.720</td>
<td align="char" char=".">0.577</td>
<td align="char" char="."><bold>0.735</bold></td>
</tr>
<tr>
<td align="left"/>
<td align="left">VAR</td>
<td align="char" char=".">0.814</td>
<td align="char" char=".">0.686</td>
<td align="char" char=".">0.796</td>
<td align="char" char=".">0.804</td>
<td align="char" char=".">0.462</td>
<td align="char" char=".">0.647</td>
<td align="char" char=".">0.643</td>
<td align="char" char="."><bold>0.716</bold></td>
</tr>
<tr>
<td align="left"/>
<td align="left">DM</td>
<td align="char" char=".">0.763</td>
<td align="char" char=".">0.651</td>
<td align="char" char=".">0.786</td>
<td align="char" char="."><bold>0.835</bold></td>
<td align="char" char=".">0.382</td>
<td align="char" char=".">0.634</td>
<td align="char" char=".">0.463</td>
<td align="char" char="."><bold>0.705</bold></td>
</tr>
<tr>
<td align="left"/>
<td align="left">TVM</td>
<td align="char" char=".">0.798</td>
<td align="char" char=".">0.713</td>
<td align="char" char="."><bold>0.805</bold></td>
<td align="char" char=".">0.811</td>
<td align="char" char=".">0.530</td>
<td align="char" char=".">0.703</td>
<td align="char" char="."><bold>0.695</bold></td>
<td align="char" char="."><bold>0.733</bold></td>
</tr>
<tr>
<td align="left"/>
<td align="left">SqD</td>
<td align="char" char=".">0.866</td>
<td align="char" char=".">0.742</td>
<td align="char" char=".">0.768</td>
<td align="char" char=".">0.801</td>
<td align="char" char=".">0.453</td>
<td align="char" char="."><bold>0.722</bold></td>
<td align="char" char=".">0.571</td>
<td align="char" char="."><bold>0.728</bold></td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t005fn001">
<p>For this comparison, AUCs have been calculated for the test set which is a sample of the 60% of test data available at Kaggle.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Moreover, results from the different subjects demonstrate that for dogs 3 and 4 and the humans the proposed filters always improve the AUC results in both sets. This is due to Dogs 3 and 4 have many more samples than Dogs 1 and 2. On the other side, although humans have less samples they have been sampled at a rate of 5000 Hz, i.e. much more information for both individuals. Fewer samples has Dog 5 and thus obtains very bad results in forecasting introducing some noise when evaluating the overall dataset. To mention that using a validation test allows us to select preprocessing algorithms with good performance in the test set.</p>
<p>In conclusion, the standard filters seem to be comparable to the supervised filters when the volume of data is limited or reduced. Nevertheless, our conjecture is that there exists an important improvement when data available for training grows substantially.</p>
<p>As seen in <xref ref-type="table" rid="pone.0178808.t003">Table 3</xref>, the proposed methods show statistical significant (for <italic>α</italic> = 0.05) improvements when compared with PBF filter taken as the baseline. The positive behavior of this preprocessing motivates a deeper comparison of the methods using any other classifier algorithms, in order to check if they were able to get similar improvements to KNN. The most used algorithms among Kaggle challenges have been chosen to predict and perform a comparison with the preprocessing technique: Support Vector Classifier (SVC), Gradient Boosting Classifier (GBC), Random Forest (RF) and Logistic Regression (LR). The hyper-parameters for these algorithms have been chosen to be similar according to the best performance values used by other participants during the Kaggle competition [<xref ref-type="bibr" rid="pone.0178808.ref009">9</xref>], since AUC calculations are done using the new partition, these AUC calculations could be slightly optimistic. The AUC values of these algorithms are shown in <xref ref-type="table" rid="pone.0178808.t006">Table 6</xref>. The DS filter has been selected as it obtained in general a better performance than the others. It is observed that under no circumstances is KNN competitive with other machine learning algorithms using conventional filters as shown in <xref ref-type="table" rid="pone.0178808.t006">Table 6</xref>. Not only is KNN performance improved using DS filter, but also it becomes competitive with the rest of the algorithms.</p>
<table-wrap id="pone.0178808.t006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0178808.t006</object-id>
<label>Table 6</label>
<caption>
<title>Comparison with PBF and DS preprocessing methods using different learning machine algorithms.</title>
</caption>
<alternatives>
<graphic id="pone.0178808.t006g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0178808.t006" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Model</th>
<th align="left">Filter</th>
<th align="center">Dog 1</th>
<th align="center">Dog 2</th>
<th align="center">Dog 3</th>
<th align="center">Dog 4</th>
<th align="center">Dog 5</th>
<th align="center">Pat. 1</th>
<th align="center">Pat. 2</th>
<th align="center">Pool</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">KNN</td>
<td align="left">PBF</td>
<td align="char" char=".">0.885</td>
<td align="char" char=".">0.756</td>
<td align="char" char=".">0.673</td>
<td align="char" char=".">0.777</td>
<td align="char" char=".">0.586</td>
<td align="char" char=".">0.711</td>
<td align="char" char=".">0.470</td>
<td align="char" char=".">0.679</td>
</tr>
<tr>
<td align="left"/>
<td align="left">DS</td>
<td align="char" char=".">0.865</td>
<td align="char" char=".">0.741</td>
<td align="char" char=".">0.796</td>
<td align="char" char=".">0.823</td>
<td align="char" char=".">0.462</td>
<td align="char" char=".">0.720</td>
<td align="char" char=".">0.577</td>
<td align="char" char=".">0.735</td>
</tr>
<tr>
<td align="left">SVC</td>
<td align="left">PBF</td>
<td align="char" char=".">0.924</td>
<td align="char" char=".">0.870</td>
<td align="char" char=".">0.759</td>
<td align="char" char=".">0.845</td>
<td align="char" char=".">0.263</td>
<td align="char" char=".">0.682</td>
<td align="char" char=".">0.646</td>
<td align="char" char=".">0.734</td>
</tr>
<tr>
<td align="left"/>
<td align="left">DS</td>
<td align="char" char=".">0.888</td>
<td align="char" char=".">0.827</td>
<td align="char" char=".">0.783</td>
<td align="char" char=".">0.764</td>
<td align="char" char=".">0.292</td>
<td align="char" char=".">0.824</td>
<td align="char" char=".">0.617</td>
<td align="char" char=".">0.732</td>
</tr>
<tr>
<td align="left">GBC</td>
<td align="left">PBF</td>
<td align="char" char=".">0.882</td>
<td align="char" char=".">0.665</td>
<td align="char" char=".">0.728</td>
<td align="char" char=".">0.853</td>
<td align="char" char=".">0.258</td>
<td align="char" char=".">0.822</td>
<td align="char" char=".">0.774</td>
<td align="char" char=".">0.709</td>
</tr>
<tr>
<td align="left"/>
<td align="left">DS</td>
<td align="char" char=".">0.877</td>
<td align="char" char=".">0.642</td>
<td align="char" char=".">0.681</td>
<td align="char" char=".">0.868</td>
<td align="char" char=".">0.193</td>
<td align="char" char=".">0.832</td>
<td align="char" char=".">0.373</td>
<td align="char" char=".">0.691</td>
</tr>
<tr>
<td align="left">RF</td>
<td align="left">PBF</td>
<td align="char" char=".">0.848</td>
<td align="char" char=".">0.672</td>
<td align="char" char=".">0.623</td>
<td align="char" char=".">0.854</td>
<td align="char" char=".">0.514</td>
<td align="char" char=".">0.778</td>
<td align="char" char=".">0.812</td>
<td align="char" char=".">0.695</td>
</tr>
<tr>
<td align="left"/>
<td align="left">DS</td>
<td align="char" char=".">0.858</td>
<td align="char" char=".">0.652</td>
<td align="char" char=".">0.671</td>
<td align="char" char=".">0.863</td>
<td align="char" char=".">0.419</td>
<td align="char" char=".">0.814</td>
<td align="char" char=".">0.341</td>
<td align="char" char=".">0.694</td>
</tr>
<tr>
<td align="left">LR</td>
<td align="left">PBF</td>
<td align="char" char=".">0.816</td>
<td align="char" char=".">0.777</td>
<td align="char" char=".">0.690</td>
<td align="char" char=".">0.865</td>
<td align="char" char=".">0.439</td>
<td align="char" char=".">0.681</td>
<td align="char" char=".">0.436</td>
<td align="char" char=".">0.648</td>
</tr>
<tr>
<td align="left"/>
<td align="left">DS</td>
<td align="char" char=".">0.683</td>
<td align="char" char=".">0.717</td>
<td align="char" char=".">0.664</td>
<td align="char" char=".">0.876</td>
<td align="char" char=".">0.450</td>
<td align="char" char=".">0.598</td>
<td align="char" char=".">0.425</td>
<td align="char" char=".">0.626</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t006fn001">
<p>For this comparison, AUCs have been calculated for the test set which is a sample of the 60% of test data available at Kaggle.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>This behavior suggests a tight coupling between KNN space tessellation and the proposed preprocessing methods. It can be explained informally as a result of the proposed lower bound for variance, that follows <xref ref-type="disp-formula" rid="pone.0178808.e007">Eq (6)</xref> employing a kernel density estimator whose probability estimation resembles <xref ref-type="disp-formula" rid="pone.0178808.e017">Eq (16)</xref>.</p>
<p>
<xref ref-type="table" rid="pone.0178808.t007">Table 7</xref> shows confidence intervals of AUC for the different machine learning methods and three subsets of frequency bands. The first subset consists in the prediction using all the six bands, the second subset includes the band with the lowest frequency and the third one corresponds to the prediction of the band with only the highest frequencies. Even though our preprocessing method is able to summarize a large number of variables, the predictions with the highest band have not a better performance than the standard method (PBF).</p>
<table-wrap id="pone.0178808.t007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0178808.t007</object-id>
<label>Table 7</label>
<caption>
<title>Confidence interval for pool AUC with all the individuals.</title>
</caption>
<alternatives>
<graphic id="pone.0178808.t007g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0178808.t007" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="left"/>
<th align="center" colspan="2">All frequency bands (0.1-180 Hz)</th>
<th align="center" colspan="2"><italic>δ</italic> (0.1-4 Hz)</th>
<th align="center" colspan="2">high-<italic>γ</italic> (70-180 Hz)</th>
</tr>
<tr>
<th align="left">Model</th>
<th align="left">Filter</th>
<th align="center">AUC Validation</th>
<th align="center">AUC Test</th>
<th align="center">AUC Validation</th>
<th align="center">AUC Test</th>
<th align="center">AUC Validation</th>
<th align="center">AUC Test</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">KNN</td>
<td align="left">PBF</td>
<td align="center">0.663 ± 0.051</td>
<td align="center">0.679 ± 0.044</td>
<td align="center">0.673 ± 0.052</td>
<td align="center">0.700 ± 0.044</td>
<td align="center">0.608 ± 0.048</td>
<td align="center">0.571 ± 0.048</td>
</tr>
<tr>
<td align="left"/>
<td align="left">DS</td>
<td align="center">0.732 ± 0.049</td>
<td align="center">0.735 ± 0.043</td>
<td align="center">0.677 ± 0.049</td>
<td align="center">0.698 ± 0.043</td>
<td align="center">0.600 ± 0.053</td>
<td align="center">0.552 ± 0.048</td>
</tr>
<tr>
<td align="left">SVC</td>
<td align="left">PBF</td>
<td align="center">0.744 ± 0.059</td>
<td align="center">0.734 ± 0.050</td>
<td align="center">0.576 ± 0.057</td>
<td align="center">0.625 ± 0.044</td>
<td align="center">0.719 ± 0.054</td>
<td align="center">0.692 ± 0.049</td>
</tr>
<tr>
<td align="left"/>
<td align="left">DS</td>
<td align="center">0.740 ± 0.052</td>
<td align="center">0.732 ± 0.047</td>
<td align="center">0.610 ± 0.055</td>
<td align="center">0.645 ± 0.043</td>
<td align="center">0.566 ± 0.051</td>
<td align="center">0.619 ± 0.042</td>
</tr>
<tr>
<td align="left">GBC</td>
<td align="left">PBF</td>
<td align="center">0.709 ± 0.050</td>
<td align="center">0.709 ± 0.046</td>
<td align="center">0.679 ± 0.055</td>
<td align="center">0.718 ± 0.047</td>
<td align="center">0.693 ± 0.050</td>
<td align="center">0.645 ± 0.045</td>
</tr>
<tr>
<td align="left"/>
<td align="left">DS</td>
<td align="center">0.688 ± 0.055</td>
<td align="center">0.691 ± 0.045</td>
<td align="center">0.670 ± 0.053</td>
<td align="center">0.710 ± 0.045</td>
<td align="center">0.523 ± 0.061</td>
<td align="center">0.577 ± 0.047</td>
</tr>
<tr>
<td align="left">RF</td>
<td align="left">PBF</td>
<td align="center">0.670 ± 0.060</td>
<td align="center">0.695 ± 0.045</td>
<td align="center">0.662 ± 0.054</td>
<td align="center">0.696 ± 0.044</td>
<td align="center">0.653 ± 0.053</td>
<td align="center">0.629 ± 0.042</td>
</tr>
<tr>
<td align="left"/>
<td align="left">DS</td>
<td align="center">0.674 ± 0.057</td>
<td align="center">0.694 ± 0.047</td>
<td align="center">0.640 ± 0.055</td>
<td align="center">0.687 ± 0.043</td>
<td align="center">0.556 ± 0.057</td>
<td align="center">0.588 ± 0.045</td>
</tr>
<tr>
<td align="left">LR</td>
<td align="left">PBF</td>
<td align="center">0.667 ± 0.050</td>
<td align="center">0.648 ± 0.046</td>
<td align="center">0.639 ± 0.057</td>
<td align="center">0.658 ± 0.046</td>
<td align="center">0.577 ± 0.060</td>
<td align="center">0.566 ± 0.046</td>
</tr>
<tr>
<td align="left"/>
<td align="left">DS</td>
<td align="center">0.633 ± 0.049</td>
<td align="center">0.626 ± 0.045</td>
<td align="center">0.625 ± 0.054</td>
<td align="center">0.637 ± 0.043</td>
<td align="center">0.526 ± 0.061</td>
<td align="center">0.564 ± 0.046</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec015" sec-type="conclusions">
<title>Discussion</title>
<p>In [<xref ref-type="bibr" rid="pone.0178808.ref022">22</xref>] a Kernel Fisher Discriminant (KFD) analysis was performed determining that high gamma power with the raw signal is not found between the most discriminant features for most of the cases. Even though, their conclusion differs for time-differential signal, according to these authors raw signal is dominated by small changes in low frequencies. With the purpose of verifying whether low frequencies of the signal are more useful for predictions and the effect of preprocessing techniques, AUC values for each subject have been calculated with predictions based on KNN. Moreover, it was considered only the lowest and the highest frequencies against PBF and DS preprocessing techniques, being compared in <xref ref-type="fig" rid="pone.0178808.g003">Fig 3</xref>. Whereas there is an obvious improvement with low frequencies using DS preprocessing, there is not such a gain if only the highest frequency band is considered. Please notice that two of the individuals (Dog 3 and Dog 4) with the largest volume of information got a better performance with DS for the highest frequency band. This suggests that a dataset with subjects with a larger volume of information could give some pieces of evidence that this method could also be effective for high-<italic>γ</italic> frequencies.</p>
<fig id="pone.0178808.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0178808.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Comparison between AUC for individuals, preprocessing methods and the highest and the lowest frequency group.</title>
<p>Every panel is a comparison between all the bands, only <italic>δ</italic> (0.1-4Hz) or only high-<italic>γ</italic> (70-180Hz). The red points correspond to the AUC got by the DS preprocessing and the black points by the PBF method. Every point is labeled according to the individual D1 (Dog 1), D2 (Dog 2), D3 (Dog 3), D4 (Dog 4), D5 (Dog 5), P1 (Patient 1) and P2 (Patient 2). Most of the points are in the lower bisector of the first quadrant, pointing out that the lowest frequency band is more useful than the highest frequency band with respect to forecasting seizures.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0178808.g003" xlink:type="simple"/>
</fig>
<p>In this paper, we have provided evidence that several preprocessing methods improves KNN. These algorithms do not have the same improvement using other machine learning algorithms, finding tailored supervised preprocessing could be a promising scope for future work to improve prediction techniques.</p>
</sec>
<sec id="sec016" sec-type="conclusions">
<title>Conclusions</title>
<p>The present paper extends the work carried out in detecting preictal states using an algorithm that was submitted to win the third prize of an international research challenge proposed by the American Epilepsy Society, the Epilepsy Foundation, National Institutes of Health (NIH) and Mayo Clinic through the Kaggle platform. It has been evaluated within this study if it is possible to design and develop the fundamental equations to obtain a new method of preprocessing the iEEG signal.</p>
<p>The idea is to find if an alternative iEEG supervised signal preprocessing, different from the conventional filters bank (PBF), could improve the forecasting results of a machine learning algorithm as the KNN. Accordingly, to remark that our main contribution is in the field of machine learning and statistics and not the clinical one, as our objective is to develop new improved predictive systems with better accuracy. Results seem highly promising as the performance, robustness and quality of the predictions have been improved, mainly when the data is getting bigger and bigger. Such behavior opens the usefulness of the method if we think for the future as a big data problem. That is, if the iEEG data stream coming from the brain is continuous and has to be processed in real-time. Although this concept is not the focus of the present study, it is true that when data clips had more variables (i.e. brain sensors) and more recording duration at a higher sampling rate, the quality of the prediction improved considerably, being able to have learning machine algorithm to detect preictal states in long recording iEEG is a first step for seizure forecasting.</p>
<p>A concern and indeed a challenge is the necessary evolution from some conventional techniques to the new framework given by the Big Data due to the increasing capabilities, for example, of real-time monitoring of body health indicators. In our opinion the treatment of such massive datasets with softer techniques than the conventional ones will be required by the new juncture. This opens a new task for us in evaluating the performance and suitability of our work in other areas.</p>
</sec>
</body>
<back>
<ack>
<p>We want to thank B.H. Brinkmann from Mayo Systems Electrophysiology Laboratory of Mayo Clinic in Rochester (MN, USA) for his valuable help.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0178808.ref001">
<label>1</label>
<mixed-citation publication-type="other" xlink:type="simple">World Health Organization. Epilepsy; 2015. WHO Fact Sheet 999.</mixed-citation>
</ref>
<ref id="pone.0178808.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Denison</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Morris</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sun</surname> <given-names>F</given-names></name>. <article-title>Building a bionic nervous system</article-title>. <source>Spectrum, IEEE</source>. <year>2015</year> <month>February</month>;<volume>52</volume>(<issue>2</issue>):<fpage>32</fpage>–<lpage>39</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/MSPEC.2015.7024509" xlink:type="simple">10.1109/MSPEC.2015.7024509</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0178808.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Howbert</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Patterson</surname> <given-names>EE</given-names></name>, <name name-style="western"><surname>Stead</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Brinkmann</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Vasoli</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Crepeau</surname> <given-names>D</given-names></name>, <etal>et al</etal>. <article-title>Forecasting seizures in dogs with naturally occurring epilepsy</article-title>. <source>PloS one</source>. <year>2014</year>;<volume>9</volume>(<issue>1</issue>):<fpage>e81920</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0081920" xlink:type="simple">10.1371/journal.pone.0081920</ext-link></comment> <object-id pub-id-type="pmid">24416133</object-id></mixed-citation>
</ref>
<ref id="pone.0178808.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kwan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Brodie</surname> <given-names>MJ</given-names></name>. <article-title>Early identification of refractory epilepsy</article-title>. <source>New England Journal of Medicine</source>. <year>2000</year>;<volume>342</volume>(<issue>5</issue>):<fpage>314</fpage>–<lpage>319</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1056/NEJM200002033420503" xlink:type="simple">10.1056/NEJM200002033420503</ext-link></comment> <object-id pub-id-type="pmid">10660394</object-id></mixed-citation>
</ref>
<ref id="pone.0178808.ref005">
<label>5</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Murray</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Lopez</surname> <given-names>AD</given-names></name>, <etal>et al</etal>. <source>Global comparative assessments in the health sector: disease burden, expenditures and intervention packages</source>. <publisher-loc>Geneva</publisher-loc>: <publisher-name>World Health Organization</publisher-name>;</mixed-citation>
</ref>
<ref id="pone.0178808.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brinkmann</surname> <given-names>BH</given-names></name>, <name name-style="western"><surname>Patterson</surname> <given-names>EE</given-names></name>, <name name-style="western"><surname>Vite</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Vasoli</surname> <given-names>VM</given-names></name>, <name name-style="western"><surname>Crepeau</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Stead</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Forecasting seizures using intracranial EEG measures and SVM in naturally occurring canine epilepsy</article-title>. <source>PloS one</source>. <year>2015</year>;<volume>10</volume>(<issue>8</issue>):<fpage>e0133900</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0133900" xlink:type="simple">10.1371/journal.pone.0133900</ext-link></comment> <object-id pub-id-type="pmid">26241907</object-id></mixed-citation>
</ref>
<ref id="pone.0178808.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Potschka</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Fischer</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rüden</surname> <given-names>EL</given-names></name>, <name name-style="western"><surname>Hülsmeyer</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Baumgärtner</surname> <given-names>W</given-names></name>. <article-title>Canine epilepsy as a translational model?</article-title> <source>Epilepsia</source>. <year>2013</year>;<volume>54</volume>(<issue>4</issue>):<fpage>571</fpage>–<lpage>579</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/epi.12138" xlink:type="simple">10.1111/epi.12138</ext-link></comment> <object-id pub-id-type="pmid">23506100</object-id></mixed-citation>
</ref>
<ref id="pone.0178808.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Patterson</surname> <given-names>EE</given-names></name>. <article-title>Canine epilepsy: an underutilized model</article-title>. <source>ILAR Journal</source>. <year>2014</year>;<volume>55</volume>(<issue>1</issue>):<fpage>182</fpage>–<lpage>186</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/ilar/ilu021" xlink:type="simple">10.1093/ilar/ilu021</ext-link></comment> <object-id pub-id-type="pmid">24936038</object-id></mixed-citation>
</ref>
<ref id="pone.0178808.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brinkmann</surname> <given-names>BH</given-names></name>, <name name-style="western"><surname>Wagenaar</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Abbot</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Adkins</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Bosshard</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Crowdsourcing reproducible seizure forecasting in human and canine epilepsy</article-title>. <source>Brain</source>. <year>2016</year>;<volume>139</volume>(<issue>6</issue>):<fpage>1713</fpage>–<lpage>1722</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/brain/aww045" xlink:type="simple">10.1093/brain/aww045</ext-link></comment> <object-id pub-id-type="pmid">27034258</object-id></mixed-citation>
</ref>
<ref id="pone.0178808.ref010">
<label>10</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Korshunova</surname> <given-names>I</given-names></name>. <source>Epileptic seizure prediction using deep learning</source>. <publisher-name>Universiteit Gent</publisher-name>. <publisher-loc>Belgium</publisher-loc>; <year>2015</year>.</mixed-citation>
</ref>
<ref id="pone.0178808.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Qi</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>W</given-names></name>. <article-title>RSTFC: A Novel Algorithm for Spatio-Temporal Filtering and Classification of Single-Trial EEG</article-title>. <source>Neural Networks and Learning Systems, IEEE Transactions on</source>. <year>2015</year> <month>Dec</month>;<volume>26</volume>(<issue>12</issue>):<fpage>3070</fpage>–<lpage>3082</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TNNLS.2015.2402694" xlink:type="simple">10.1109/TNNLS.2015.2402694</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0178808.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kodipaka</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Vemuri</surname> <given-names>BC</given-names></name>, <name name-style="western"><surname>Rangarajan</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Leonard</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Schmallfuss</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Eisenschenk</surname> <given-names>S</given-names></name>. <article-title>Kernel fisher discriminant for shape-based classification in epilepsy</article-title>. <source>Medical Image Analysis</source>. <year>2007</year>;<volume>11</volume>(<issue>1</issue>):<fpage>79</fpage>–<lpage>90</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.media.2006.10.002" xlink:type="simple">10.1016/j.media.2006.10.002</ext-link></comment> <object-id pub-id-type="pmid">17157051</object-id></mixed-citation>
</ref>
<ref id="pone.0178808.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lemm</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Blankertz</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Curio</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Müller</surname> <given-names>KR</given-names></name>. <article-title>Spatio-spectral filters for improving the classification of single trial EEG</article-title>. <source>Biomedical Engineering, IEEE Transactions on</source>. <year>2005</year>;<volume>52</volume>(<issue>9</issue>):<fpage>1541</fpage>–<lpage>1548</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TBME.2005.851521" xlink:type="simple">10.1109/TBME.2005.851521</ext-link></comment> <object-id pub-id-type="pmid">16189967</object-id></mixed-citation>
</ref>
<ref id="pone.0178808.ref014">
<label>14</label>
<mixed-citation publication-type="other" xlink:type="simple">Zamora-Martínez F, Muñoz-Almaraz FJ, Botella-Rocamora P, Pardo J. Seizure prediction system of ESAI-CEU-UCH team. Universidad CEU Cardenal Herrera; 2015. <ext-link ext-link-type="uri" xlink:href="https://github.com/ESAI-CEU-UCH/kaggle-epilepsy" xlink:type="simple">https://github.com/ESAI-CEU-UCH/kaggle-epilepsy</ext-link>.</mixed-citation>
</ref>
<ref id="pone.0178808.ref015">
<label>15</label>
<mixed-citation publication-type="other" xlink:type="simple">Kaggle Inc. American Epilepsy Society Seizure Prediction Challenge; 2015. <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/c/seizure-prediction" xlink:type="simple">https://www.kaggle.com/c/seizure-prediction</ext-link>.</mixed-citation>
</ref>
<ref id="pone.0178808.ref016">
<label>16</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Duda</surname> <given-names>RO</given-names></name>, <name name-style="western"><surname>Hart</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Stork</surname> <given-names>DG</given-names></name>. <source>Pattern classification</source>. <publisher-name>John Wiley &amp; Sons</publisher-name>; <year>2012</year>.</mixed-citation>
</ref>
<ref id="pone.0178808.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Koles</surname> <given-names>ZJ</given-names></name>, <name name-style="western"><surname>Lazar</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Zhou</surname> <given-names>SZ</given-names></name>. <article-title>Spatial patterns underlying population differences in the background EEG</article-title>. <source>Brain Topography</source>. <year>1990</year>;<volume>2</volume>(<issue>4</issue>):<fpage>275</fpage>–<lpage>284</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF01129656" xlink:type="simple">10.1007/BF01129656</ext-link></comment> <object-id pub-id-type="pmid">2223384</object-id></mixed-citation>
</ref>
<ref id="pone.0178808.ref018">
<label>18</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Parlett</surname> <given-names>B</given-names></name>. <chapter-title>The Symmetric Eigenvalue Problem</chapter-title>. <source>Society for Industrial and Applied Mathematics</source>; <year>1998</year>.</mixed-citation>
</ref>
<ref id="pone.0178808.ref019">
<label>19</label>
<mixed-citation publication-type="other" xlink:type="simple">
Zamora-Martínez, F, España-Boquera, S, Gorbe-Moya, J, Pastor-Pellicer, J, Palacios-Corella, A. APRIL-ANN toolkit, A Pattern Recognizer In Lua with Artificial Neural Networks; 2014. <ext-link ext-link-type="uri" xlink:href="https://github.com/pakozm/april-ann" xlink:type="simple">https://github.com/pakozm/april-ann</ext-link>.</mixed-citation>
</ref>
<ref id="pone.0178808.ref020">
<label>20</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Goldberger</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Roweis</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Salakhutdinov</surname> <given-names>R</given-names></name>. <chapter-title>Neighbourhood Components Analysis</chapter-title>. In: <source>Advances in Neural Information Processing Systems (NIPS)</source>. <publisher-name>MIT Press</publisher-name>; <year>2005</year>. p. <fpage>513</fpage>–<lpage>520</lpage>.</mixed-citation>
</ref>
<ref id="pone.0178808.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>DeLong</surname> <given-names>ER</given-names></name>, <name name-style="western"><surname>DeLong</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Clarke-Pearson</surname> <given-names>DL</given-names></name>. <article-title>Comparing the Areas under Two or More Correlated Receiver Operating Characteristic Curves: A Nonparametric Approach</article-title>. <source>Biometrics</source>. <year>1988</year> <month>Sep</month>;<volume>44</volume>(<issue>3</issue>):<fpage>837</fpage>–<lpage>845</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2307/2531595" xlink:type="simple">10.2307/2531595</ext-link></comment> <object-id pub-id-type="pmid">3203132</object-id></mixed-citation>
</ref>
<ref id="pone.0178808.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Park</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Luo</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Parhi</surname> <given-names>KK</given-names></name>, <name name-style="western"><surname>Netoff</surname> <given-names>T</given-names></name>. <article-title>Seizure prediction with spectral power of EEG using cost-sensitive support vector machines</article-title>. <source>Epilepsia</source>. <year>2011</year>;<volume>52</volume>(<issue>10</issue>):<fpage>1761</fpage>–<lpage>1770</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1528-1167.2011.03138.x" xlink:type="simple">10.1111/j.1528-1167.2011.03138.x</ext-link></comment> <object-id pub-id-type="pmid">21692794</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>