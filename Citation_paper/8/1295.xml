<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-01075</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004895</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject><subj-group><subject>Neuronal dendrites</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject><subj-group><subject>Neuronal dendrites</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Membrane potential</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Action potentials</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Systems science</subject><subj-group><subject>Dynamical systems</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Systems science</subject><subj-group><subject>Dynamical systems</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Systems science</subject><subj-group><subject>Nonlinear dynamics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Systems science</subject><subj-group><subject>Nonlinear dynamics</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Learning Universal Computations with Spikes</article-title>
<alt-title alt-title-type="running-head">Learning Universal Computations with Spikes</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Thalmeier</surname> <given-names>Dominik</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Uhlmann</surname> <given-names>Marvin</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Kappen</surname> <given-names>Hilbert J.</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Memmesheimer</surname> <given-names>Raoul-Martin</given-names></name>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Donders Institute, Department of Biophysics, Radboud University, Nijmegen, Netherlands</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Max Planck Institute for Psycholinguistics, Department for Neurobiology of Language, Nijmegen, Netherlands</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Donders Institute, Department for Neuroinformatics, Radboud University, Nijmegen, Netherlands</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>Center for Theoretical Neuroscience, Columbia University, New York, New York, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Bethge</surname> <given-names>Matthias</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University of Tübingen and Max Planck Institute for Biologial Cybernetics, GERMANY</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Wrote the paper: DT MU HJK RMM. Developed the spiking network models and the learning methods: DT MU RMM. Performed numerical simulations: MU DT. Supervised the work: HJK RMM.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">rm3354@cumc.columbia.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>6</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="epub">
<day>16</day>
<month>6</month>
<year>2016</year>
</pub-date>
<volume>12</volume>
<issue>6</issue>
<elocation-id>e1004895</elocation-id>
<history>
<date date-type="received">
<day>2</day>
<month>7</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>1</day>
<month>4</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Thalmeier et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004895"/>
<abstract>
<p>Providing the neurobiological basis of information processing in higher animals, spiking neural networks must be able to learn a variety of complicated computations, including the generation of appropriate, possibly delayed reactions to inputs and the self-sustained generation of complex activity patterns, e.g. for locomotion. Many such computations require previous building of intrinsic world models. Here we show how spiking neural networks may solve these different tasks. Firstly, we derive constraints under which classes of spiking neural networks lend themselves to substrates of powerful general purpose computing. The networks contain dendritic or synaptic nonlinearities and have a constrained connectivity. We then combine such networks with learning rules for outputs or recurrent connections. We show that this allows to learn even difficult benchmark tasks such as the self-sustained generation of desired low-dimensional chaotic dynamics or memory-dependent computations. Furthermore, we show how spiking networks can build models of external world systems and use the acquired knowledge to control them.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Animals and humans can learn versatile computations such as the generation of complicated activity patterns to steer movements or the generation of appropriate outputs in response to inputs. Such learning must be accomplished by networks of nerve cells in the brain, which communicate with short electrical impulses, so-called spikes. Here we show how such networks may perform the learning. We track their ability back to experimentally found nonlinearities in the couplings between nerve cells and to a network connectivity that complies with constraints. We show that the spiking networks are able to learn difficult tasks such as the generation of desired chaotic activity and the prediction of the impact of actions on the environment. The latter allows to compute optimal actions by mental exploration.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>Marie Curie Initial Training Network ‘NETT’</institution>
</funding-source>
<award-id>289146</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Thalmeier</surname> <given-names>Dominik</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution>German Federal Ministry of Education and Research BMBF, Bernstein Network</institution>
</funding-source>
<award-id>Bernstein Award 2014</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Thalmeier</surname> <given-names>Dominik</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award003">
<funding-source>
<institution>Max Kade Foundation</institution>
</funding-source>
<principal-award-recipient>
<name name-style="western">
<surname>Memmesheimer</surname> <given-names>Raoul-Martin</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was supported in part by the European Commission through the Marie Curie Initial Training Network ‘NETT’, project N. 289146, by the German Federal Ministry of Education and Research BMBF through the Bernstein Network (Bernstein Award 2014) and by the Max Kade Foundation. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="5"/>
<table-count count="3"/>
<page-count count="29"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The understanding of neural network dynamics on the mesoscopic level of hundreds and thousands of neurons and their ability to learn highly complicated computations is a fundamental open challenge in neuroscience. For biological systems, such an understanding will allow to connect the microscopic level of single neurons and the macroscopic level of cognition and behavior. In artificial computing, it may allow to propose new, possibly more efficient computing schemes.</p>
<p>Randomly connected mesoscopic networks can be a suitable substrate for computations [<xref ref-type="bibr" rid="pcbi.1004895.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1004895.ref005">5</xref>], as they reflect the input in a complicated, nonlinear way and at the same time maintain, like a computational “reservoir”, fading memory of past inputs as well as of transformations and combinations of them. This includes the results of computations on current and past inputs. Simple readout neurons may then learn to extract the desired result; the computations are executed in real time, i.e. without the need to wait for convergence to an attractor (“reservoir computing”) [<xref ref-type="bibr" rid="pcbi.1004895.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref002">2</xref>]. Non-random and adaptive network connectivity can change performance [<xref ref-type="bibr" rid="pcbi.1004895.ref006">6</xref>–<xref ref-type="bibr" rid="pcbi.1004895.ref008">8</xref>].</p>
<p>Networks with higher computational power, in particular with the additional ability to learn self-sustained patterns of activity and persistent memory, require an output feedback or equivalent learning of their recurrent connections [<xref ref-type="bibr" rid="pcbi.1004895.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref003">3</xref>]. However, network modeling approaches achieving such universal (i.e. general purpose) computational capabilities so far concentrated on networks of continuous rate units [<xref ref-type="bibr" rid="pcbi.1004895.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref004">4</xref>], which do not take into account the characteristics that neurons in biological neural networks communicate via spikes. Indeed, the dynamics of spiking neural networks are discontinuous, usually highly chaotic, variable, and noisy. Readouts of such spiking networks show low signal-to-noise ratios. This hinders computations following the described principle in particular in presence of feedback or equivalent plastic recurrent connections, and has questioned it as model for computations in biological neural systems [<xref ref-type="bibr" rid="pcbi.1004895.ref009">9</xref>–<xref ref-type="bibr" rid="pcbi.1004895.ref011">11</xref>].</p>
<p>Here we first introduce a class of recurrent spiking neural networks that are suited as a substrate to learn universal computations. They are based on standard, established neuron models, take into account synaptic or dendritic nonlinearities and are required to respect some structural constraints regarding the connectivity of the network. To derive them we employ a precise spike coding scheme similar to ref. [<xref ref-type="bibr" rid="pcbi.1004895.ref012">12</xref>], which was introduced to approximate linear continuous dynamics.</p>
<p>Thereafter we endow the introduced spiking networks with learning rules for either the output or the recurrent connection weights and show that this enables them to learn equally complicated, memory dependent computations as non-spiking continuous rate networks. The spiking networks we are using have only medium sizes, between tens and a few thousands of neurons, like networks of rate neurons employed for similar tasks. We demonstrate the capabilities of our networks by applying them to challenging learning problems which are of importance in biological contexts. In particular, we show how spiking neural networks can learn the self-sustained generation of complicated dynamical patterns, and how they can build world models, which allow to compute optimal actions to appropriately influence an environment.</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>Continuous signal coding spiking neural networks (CSNs)</title>
<sec id="sec004">
<title>Network architecture</title>
<p>For our study, we use leaky integrate-and-fire neurons. These incorporate crucial features of biological neurons, such as operation in continuous time, spike generation and reset, while also maintaining some degree of analytical tractability. A network consists of <italic>N</italic> neurons. The state of a neuron <italic>n</italic> is given by its membrane potential <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>). The membrane potential performs a leaky integration of the input and a spike is generated when <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>) reaches a threshold, resulting in a spiketrain
<disp-formula id="pcbi.1004895.e001"><alternatives><graphic id="pcbi.1004895.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mrow><mml:msub><mml:mi>s</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:msub><mml:mi>t</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:munder> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>t</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula>
with spike times <italic>t</italic><sub><italic>n</italic></sub> and the Dirac delta-distribution <italic>δ</italic>. After a spike, the neuron is reset to the reset potential, which lies <italic>θ</italic> below the threshold. The spike train generates a train of exponentially decaying normalized synaptic currents
<disp-formula id="pcbi.1004895.e002"><alternatives><graphic id="pcbi.1004895.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:msub><mml:mi>t</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:munder> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>s</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>t</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup> <mml:mo>Θ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mi>t</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mo>⇔</mml:mo> <mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>s</mml:mi></mml:msub> <mml:msub><mml:mi>r</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula>
where <inline-formula id="pcbi.1004895.e003"><alternatives><graphic id="pcbi.1004895.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:msub><mml:mi>τ</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>λ</mml:mo> <mml:mrow><mml:mi>s</mml:mi></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is the time constant of the synaptic decay and Θ(.) is the Heaviside theta-function.</p>
<p>Throughout the article we consider two closely related types of neurons, neurons with saturating synapses and neurons with nonlinear dendrites (cf. <xref ref-type="fig" rid="pcbi.1004895.g001">Fig 1</xref>). In the model with saturating synapses (<xref ref-type="fig" rid="pcbi.1004895.g001">Fig 1a</xref>), the membrane potential <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>) of neuron <italic>n</italic> obeys
<disp-formula id="pcbi.1004895.e004"><alternatives><graphic id="pcbi.1004895.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>V</mml:mi></mml:msub> <mml:msub><mml:mi>V</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:munderover><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>A</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mi>tanh</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mi>γ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mtext>r</mml:mtext></mml:msub> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>s</mml:mi></mml:msub> <mml:msub><mml:mi>r</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo> <mml:mi>θ</mml:mi> <mml:msub><mml:mi>s</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mrow><mml:mtext>e,</mml:mtext> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
with membrane time constant <inline-formula id="pcbi.1004895.e005"><alternatives><graphic id="pcbi.1004895.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:msub><mml:mi>τ</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>λ</mml:mo> <mml:mrow><mml:mi>V</mml:mi></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>.</p>
<fig id="pcbi.1004895.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004895.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Coding of continuous signals in neurons with saturating synapses (a,b) and nonlinear dendrites (c,d).</title>
<p>(a,b): A neuron with saturating synapses (a) that directly codes for a continuous signal (b). Panel (a) displays the neuron with an axon (red) and dendrites (dark blue) that receive inputs from the axons of other neurons (axons at the bottom) via saturating synapses (symbolized by sigmoids at the synaptic contacts). The currents entering the soma are weighted sums of input spike trains that are synaptically filtered (generating scaled normalized synaptic currents <italic>γr</italic><sub><italic>n</italic></sub>(<italic>t</italic>), synaptic time scale <italic>τ</italic><sub><italic>s</italic></sub>) and thereafter subject to a saturating synaptic nonlinearity. External inputs (axons at the top) are received without saturation. The continuous signal <italic>x</italic>(<italic>t</italic>) (panel b left hand side, green) is the sum of the neuron’s membrane potential <italic>V</italic>(<italic>t</italic>) (red) and its scaled normalized synaptic current <italic>θr</italic>(<italic>t</italic>) (dark blue). <italic>r</italic>(<italic>t</italic>) is a low-pass filtered version of the neuron’s spike train <italic>s</italic>(<italic>t</italic>) (light blue in red box). If <italic>x</italic>(<italic>t</italic>)&gt;0, the time scale of <italic>x</italic>(<italic>t</italic>) should be large against the synaptic time scale <italic>τ</italic><sub><italic>s</italic></sub> and <italic>x</italic>(<italic>t</italic>) should predominantly be large against the neuron’s threshold, <italic>θ</italic>2 (panel b right hand side, assumptions 1,2 in the main text). <italic>x</italic>(<italic>t</italic>) is then already well approximated by <italic>θr</italic>(<italic>t</italic>), while <italic>V</italic>(<italic>t</italic>) is oscillating between ±<italic>θ</italic>2. If <italic>x</italic>(<italic>t</italic>)≤0, we have <italic>V</italic>(<italic>t</italic>)≤0, no spikes are generated and <italic>r</italic>(<italic>t</italic>) quickly decays to zero, such that we predominantly have <italic>r</italic>(<italic>t</italic>)≈0 and <italic>x</italic>(<italic>t</italic>) is well approximated by <italic>V</italic>(<italic>t</italic>) (cf. <xref ref-type="disp-formula" rid="pcbi.1004895.e032">Eq (9)</xref>). (c,d): Two neurons with nonlinear dendrites (c) from a larger network that distributedly codes for a continuous signal (d). (c): Each neuron has an axon (red) and different types of dendrites (cyan, light blue and dark blue) that receive inputs from the axons of other neurons (axons at the bottom) via fast or slow conventional synapses (highlighted by circles and squares). Linear dendrites with slow synapses (cyan with circle contacts) generate somatic currents that are weighted linear sums of low-pass filtered presynaptic spike trains (weighted sums of the <italic>r</italic><sub><italic>n</italic></sub>(<italic>t</italic>)). Linear dendrites with fast synapses (light blue with square contacts) generate somatic currents with negligible filtering (weighted sums of the spike trains <italic>s</italic><sub><italic>n</italic></sub>(<italic>t</italic>)). Spikes arriving at a nonlinear dendrite (dark blue) are also filtered (circular contact). The resulting <italic>r</italic><sub><italic>n</italic></sub>(<italic>t</italic>) are weighted, summed up linearly in the dendrite and subjected to a saturating dendritic nonlinearity (symbolized by sigmoids at dendrites), before entering the soma. We assume that the neurons have nonlinear dendrites that are located in similar tissue areas, such that they connect to the same sets of axons and receive similar inputs. (d): All neurons in the network together encode <italic>J</italic> continuous signals <bold>x</bold>(<italic>t</italic>) (one displayed in green) by a weighted sum of their membrane potentials <bold>V</bold>(<italic>t</italic>) (two traces of different neurons displayed in red) and their normalized PSCs <bold>r</bold>(<italic>t</italic>) (two traces displayed in cyan). The <bold>Γr</bold>(<italic>t</italic>) alone already approximate <bold>x</bold>(<italic>t</italic>) well. The neurons’ output spike trains <bold>s</bold>(<italic>t</italic>) (light blue in red box) generate slow and fast inputs to other neurons. (Note that spikes can be generated due to suprathreshold excitation by fast inputs. Since we plot <bold>V</bold>(<italic>t</italic>) after fast inputs and possible resets, the corresponding threshold crossings do not appear.)</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004895.g001" xlink:type="simple"/>
</fig>
<p>The saturation of synapses, e.g. due to receptor saturation or finite reversal potentials, acts as a nonlinear transfer function [<xref ref-type="bibr" rid="pcbi.1004895.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref014">14</xref>], which we model as a tanh-nonlinearity (since <italic>r</italic><sub><italic>m</italic></sub>(<italic>t</italic>)≥0 only the positive part of the tanh becomes effective). We note that this may also be interpreted as a simple implementation of synaptic depression: A spike generated by neuron <italic>m</italic> at <italic>t</italic><sub><italic>m</italic></sub> leads to an increase of <italic>r</italic><sub><italic>m</italic></sub>(<italic>t</italic><sub><italic>m</italic></sub>) by 1. As long as the synapse connecting neuron <italic>m</italic> to neuron <italic>n</italic> is far from saturation (linear part of the tanh-function) this leads to the consumption of a fraction <italic>γ</italic> of the synaptic “resources” and the effect of the spike on the neuron is approximately the effect of a current <italic>A</italic><sub><italic>nm</italic></sub> <italic>γe</italic><sup>−λ<sub><italic>s</italic></sub>(<italic>t</italic> − <italic>t</italic><sub><italic>m</italic></sub>)</sup> Θ(<italic>t</italic> − <italic>t</italic><sub><italic>m</italic></sub>). When a larger number of such spikes arrive in short time such that the consumed resources accumulate to 1 and beyond, the synapse saturates at its maximum strength <italic>A</italic><sub><italic>nm</italic></sub> and the effect of individual inputs is much smaller than before. The recovery from depression is here comparably fast, it takes place on a timescale of <inline-formula id="pcbi.1004895.e006"><alternatives><graphic id="pcbi.1004895.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:msubsup><mml:mo>λ</mml:mo> <mml:mrow><mml:mi>s</mml:mi></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> (compare, e.g., [<xref ref-type="bibr" rid="pcbi.1004895.ref015">15</xref>]).</p>
<p>The reset of the neuron is incorporated by the term −<italic>θs</italic><sub><italic>n</italic></sub>(<italic>t</italic>). The voltage lost due to this reset is partially recovered by a slow recovery current (afterdepolarization) <italic>V</italic><sub>r</sub> λ<sub><italic>s</italic></sub> <italic>r</italic><sub><italic>n</italic></sub>(<italic>t</italic>); its temporally integrated size is given by the parameter <italic>V</italic><sub>r</sub>. This is a feature of many neurons e.g. in the neocortex, in the hippocampus and in the cerebellum [<xref ref-type="bibr" rid="pcbi.1004895.ref016">16</xref>], and may be caused by different types of somatic or dendritic currents, such as persistent and resurgent sodium and calcium currents, or by excitatory autapses [<xref ref-type="bibr" rid="pcbi.1004895.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref018">18</xref>]. It provides a simple mechanism to sustain (fast) spiking and generate bursts, e.g. in response to pulses. <italic>I</italic><sub>e,<italic>n</italic></sub>(<italic>t</italic>) is an external input, its constant part may be interpreted as sampling slow inputs specifying the resting potential that the neuron asymptotically assumes for long times without any recurrent network input. We assume that the resting potential is halfway between the reset potential <italic>V</italic><sub>res</sub> and the threshold <italic>V</italic><sub>res</sub> + <italic>θ</italic>. We set it to zero such that the neuron spikes when the membrane potential reaches <italic>θ</italic>2 and resets to −<italic>θ</italic>2. To test the robustness of the dynamics we sometimes add a white noise input <italic>η</italic><sub><italic>n</italic></sub>(<italic>t</italic>) satisfying <inline-formula id="pcbi.1004895.e007"><alternatives><graphic id="pcbi.1004895.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:mo>〈</mml:mo> <mml:msub><mml:mi>η</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>η</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>〉</mml:mo> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>η</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mi>δ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:msup><mml:mi>t</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> with the Kronecker delta <italic>δ</italic><sub><italic>nm</italic></sub>.</p>
<p>For simplicity, we take the parameters λ<sub><italic>V</italic></sub>,<italic>θ</italic>,<italic>V</italic><sub>r</sub> and <italic>γ</italic>,λ<sub><italic>s</italic></sub> identical for all neurons and synapses, respectively. We take the membrane potential <italic>V</italic><sub><italic>n</italic></sub> and the parameters <italic>V</italic><sub>r</sub> and <italic>θ</italic> dimensionless, they can be fit to the voltage scale of biological neurons by rescaling with an additive and a multiplicative dimensionful constant. Time is measured in seconds.</p>
<p>We find that networks of the form <xref ref-type="disp-formula" rid="pcbi.1004895.e004">Eq (3)</xref> generate dynamics suitable for universal computation similar to continuous rate networks [<xref ref-type="bibr" rid="pcbi.1004895.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref004">4</xref>], if 0 &lt; λ<sub><italic>x</italic></sub> ≪ λ<sub><italic>s</italic></sub>, where <inline-formula id="pcbi.1004895.e008"><alternatives><graphic id="pcbi.1004895.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:msub><mml:mo>λ</mml:mo> <mml:mi>x</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>s</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:msub><mml:mi>V</mml:mi> <mml:mtext>r</mml:mtext></mml:msub> <mml:mi>θ</mml:mi></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, <italic>A</italic><sub><italic>nm</italic></sub> sufficiently large and <italic>γ</italic> small. The conditions result from requiring the network to approximate a nonlinear continuous dynamical system (see next section).</p>
<p>An alternative interpretation of the introduced nonlinearity is that the neurons have nonlinear dendrites, where each nonlinear compartment is small such that it receives at most one (conventional, nonsaturating) synapse. <italic>A</italic><sub><italic>nm</italic></sub> is then the strength of the coupling from a dendritic compartment to the soma. This interpretation suggests an extension of the neuron model allowing for several dendrites per neuron, where the inputs are linearly summed up and then subjected to a saturating dendritic nonlinearity [<xref ref-type="bibr" rid="pcbi.1004895.ref019">19</xref>–<xref ref-type="bibr" rid="pcbi.1004895.ref021">21</xref>]. Like the previous model, we find that such a model has to satisfy additional constraints to be suitable for universal computation:</p>
<p>Neurons with nonlinear dendrites need additional slow and fast synaptic contacts which arrive near the soma and are summed linearly there (<xref ref-type="fig" rid="pcbi.1004895.g001">Fig 1c</xref>). Such structuring has been found in biological neural networks [<xref ref-type="bibr" rid="pcbi.1004895.ref022">22</xref>]. We gather the different components into a dynamical equation for <italic>V</italic><sub><italic>n</italic></sub> as
<disp-formula id="pcbi.1004895.e009"><alternatives><graphic id="pcbi.1004895.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>−</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>V</mml:mi></mml:msub> <mml:msub><mml:mi>V</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:munderover><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>J</mml:mi></mml:munderover> <mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mi>tanh</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:munderover><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>j</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>r</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo> <mml:munderover><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mrow><mml:mover><mml:mi>U</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:mrow> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>r</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:munderover><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>s</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr> <mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo> <mml:munderover><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>J</mml:mi></mml:munderover> <mml:msub><mml:mo>Γ</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>I</mml:mi> <mml:mrow><mml:mtext>e</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula> <italic>D</italic><sub><italic>nj</italic></sub> is the coupling from the <italic>j</italic>th dendrite of neuron <italic>n</italic> to its soma. The total number of dendrites and neurons is referred to as <italic>J</italic> and <italic>N</italic> respectively. <italic>W</italic><sub><italic>njm</italic></sub> is the coupling strength from neuron <italic>m</italic> to the <italic>j</italic>th nonlinear dendrite of neuron <italic>n</italic>. The slow, significantly temporally filtered inputs from neuron <italic>m</italic> to the soma of neuron <italic>n</italic>, <inline-formula id="pcbi.1004895.e010"><alternatives><graphic id="pcbi.1004895.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>U</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>r</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, have connection strengths <inline-formula id="pcbi.1004895.e011"><alternatives><graphic id="pcbi.1004895.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:msub><mml:mover accent="true"><mml:mi>U</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>. The fast ones, <italic>U</italic><sub><italic>nm</italic></sub> <italic>s</italic><sub><italic>m</italic></sub>(<italic>t</italic>), have negligible synaptic filtering (i.e. negligible synaptic rise and decay times) as well as negligible conduction delays. The resets and recoveries are incorporated as diagonal elements of the matrices <italic>U</italic><sub><italic>nm</italic></sub> and <inline-formula id="pcbi.1004895.e012"><alternatives><graphic id="pcbi.1004895.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:msub><mml:mover accent="true"><mml:mi>U</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>. To test the robustness of the dynamics, also here we sometimes add a white noise input <italic>η</italic><sub><italic>n</italic></sub>(<italic>t</italic>). To increase the richness of the recurrent dynamics and the computational power of the network (cf. [<xref ref-type="bibr" rid="pcbi.1004895.ref023">23</xref>] for disconnected units without output feedback) we added inhomogeneity, e.g. through the external input current in some tasks. In the control/mental exploration task, we added a constant bias term <italic>b</italic><sub><italic>j</italic></sub> as argument of the tanh to introduce inhomogeneity.</p>
<p>We find that the network couplings <bold>D</bold>,<bold>W</bold>,<bold>U</bold> and <inline-formula id="pcbi.1004895.e013"><alternatives><graphic id="pcbi.1004895.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mover accent="true"><mml:mi mathvariant="bold">U</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> <xref ref-type="disp-formula" rid="pcbi.1004895.e009">Eq (4)</xref> (we use bold letters for vectors and matrices) should satisfy certain interrelations. As motivated in the subsequent section and derived in the supporting material, their components may be expressed in terms of the components of a <italic>J</italic>×<italic>N</italic> matrix <bold>Γ</bold>, and a <italic>J</italic>×<italic>J</italic> matrix <bold>A</bold> as <inline-formula id="pcbi.1004895.e014"><alternatives><graphic id="pcbi.1004895.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>J</mml:mi></mml:msubsup> <mml:msub><mml:mo>Γ</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>A</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, <italic>W</italic><sub><italic>njm</italic></sub> = <italic>Γ</italic><sub><italic>jm</italic></sub>, <inline-formula id="pcbi.1004895.e015"><alternatives><graphic id="pcbi.1004895.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>U</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mi>a</mml:mi> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>J</mml:mi></mml:msubsup> <mml:msub><mml:mo>Γ</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mo>Γ</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mi>μ</mml:mi> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>s</mml:mi></mml:msub> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1004895.e016"><alternatives><graphic id="pcbi.1004895.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:msub><mml:mi>U</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>J</mml:mi></mml:msubsup> <mml:msub><mml:mo>Γ</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mo>Γ</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mi>μ</mml:mi> <mml:msub><mml:mi>δ</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>a</italic> = λ<sub><italic>s</italic></sub> − λ<sub><italic>x</italic></sub> and <italic>μ</italic> ≥ 0 is small (see also <xref ref-type="table" rid="pcbi.1004895.t001">Table 1</xref> for an overview). The thresholds are chosen identical, <italic>θ</italic><sup><italic>n</italic></sup> = <italic>θ</italic>, see <xref ref-type="sec" rid="sec014">Methods</xref>.</p>
<table-wrap id="pcbi.1004895.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004895.t001</object-id>
<label>Table 1</label>
<caption>
<title>Parameters of a network of neurons with nonlinear dendrites (cf. <xref ref-type="disp-formula" rid="pcbi.1004895.e009">Eq (4)</xref>) and their optimal values.</title>
</caption>
<alternatives>
<graphic id="pcbi.1004895.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004895.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center"/>
<th align="center">explanation</th>
<th align="center">optimal value</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><italic>D</italic><sub><italic>nj</italic></sub></td>
<td align="center">coupling from the <italic>j</italic>th dendrite of neuron <italic>n</italic>to its soma</td>
<td align="center"><inline-formula id="pcbi.1004895.e017">
<alternatives>
<graphic id="pcbi.1004895.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e017" xlink:type="simple"/>
<mml:math display="inline" id="M17">
<mml:mrow>
<mml:msub>
<mml:mi>D</mml:mi>
<mml:mrow>
<mml:mi>n</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msubsup>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>J</mml:mi>
</mml:msubsup>
<mml:msub>
<mml:mo>Γ</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mi>A</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula></td>
</tr>
<tr>
<td align="center"><italic>W</italic><sub><italic>njm</italic></sub></td>
<td align="center">coupling strength from neuron <italic>m</italic> to the <italic>j</italic>th nonlinear dendrite of neuron <italic>n</italic></td>
<td align="center"><italic>W</italic><sub><italic>njm</italic></sub> = <italic>Γ</italic><sub><italic>jm</italic></sub></td>
</tr>
<tr>
<td align="center"><inline-formula id="pcbi.1004895.e018">
<alternatives>
<graphic id="pcbi.1004895.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e018" xlink:type="simple"/>
<mml:math display="inline" id="M18">
<mml:msub>
<mml:mover accent="true">
<mml:mi>U</mml:mi>
<mml:mo>˜</mml:mo>
</mml:mover>
<mml:mrow>
<mml:mi>n</mml:mi>
<mml:mi>m</mml:mi>
</mml:mrow>
</mml:msub>
</mml:math>
</alternatives>
</inline-formula></td>
<td align="center">slow coupling from neuron <italic>m</italic> to neuron <italic>n</italic>; diagonal elements incorporate a recovery current</td>
<td align="center"><inline-formula id="pcbi.1004895.e019">
<alternatives>
<graphic id="pcbi.1004895.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e019" xlink:type="simple"/>
<mml:math display="inline" id="M19">
<mml:mrow>
<mml:msub>
<mml:mover accent="true">
<mml:mi>U</mml:mi>
<mml:mo>˜</mml:mo>
</mml:mover>
<mml:mrow>
<mml:mi>n</mml:mi>
<mml:mi>m</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:mi>a</mml:mi>
<mml:msubsup>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>J</mml:mi>
</mml:msubsup>
<mml:msub>
<mml:mo>Γ</mml:mo>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mi>n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mo>Γ</mml:mo>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mi>m</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mi>μ</mml:mi>
<mml:msub>
<mml:mo>λ</mml:mo>
<mml:mi>s</mml:mi>
</mml:msub>
<mml:msub>
<mml:mi>δ</mml:mi>
<mml:mrow>
<mml:mi>n</mml:mi>
<mml:mi>m</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula><break/><italic>a</italic> = λ<sub><italic>s</italic></sub> − λ<sub><italic>x</italic></sub></td>
</tr>
<tr>
<td align="center"><italic>U</italic><sub><italic>nm</italic></sub></td>
<td align="center">fast coupling from neuron <italic>m</italic> to neuron <italic>n</italic>; diagonal elements incorporate the reset</td>
<td align="center"><inline-formula id="pcbi.1004895.e020">
<alternatives>
<graphic id="pcbi.1004895.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e020" xlink:type="simple"/>
<mml:math display="inline" id="M20">
<mml:mrow>
<mml:msub>
<mml:mi>U</mml:mi>
<mml:mrow>
<mml:mi>n</mml:mi>
<mml:mi>m</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msubsup>
<mml:mo>∑</mml:mo>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>J</mml:mi>
</mml:msubsup>
<mml:msub>
<mml:mo>Γ</mml:mo>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mi>n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mo>Γ</mml:mo>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mi>m</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:mi>μ</mml:mi>
<mml:msub>
<mml:mi>δ</mml:mi>
<mml:mrow>
<mml:mi>n</mml:mi>
<mml:mi>m</mml:mi>
</mml:mrow>
</mml:msub>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula></td>
</tr>
<tr>
<td align="center"><italic>θ</italic><sup><italic>n</italic></sup></td>
<td align="center">threshold of neuron <italic>n</italic></td>
<td align="center"><inline-formula id="pcbi.1004895.e021">
<alternatives>
<graphic id="pcbi.1004895.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e021" xlink:type="simple"/>
<mml:math display="inline" id="M21">
<mml:mrow>
<mml:msup>
<mml:mi>θ</mml:mi>
<mml:mi>n</mml:mi>
</mml:msup>
<mml:mo>=</mml:mo>
<mml:mfrac>
<mml:msub>
<mml:mi>U</mml:mi>
<mml:mrow>
<mml:mi>n</mml:mi>
<mml:mi>n</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mn>2</mml:mn>
</mml:mfrac>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula></td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Again, the conditions result from requiring the network to approximate a nonlinear continuous dynamical system. This system, <xref ref-type="disp-formula" rid="pcbi.1004895.e037">Eq (11)</xref>, is characterized by the <italic>J</italic>×<italic>J</italic> coupling matrix <bold>A</bold> and a <italic>J</italic>-dimensional input <bold>c</bold>(<italic>t</italic>) whose components are identical to the <italic>J</italic> independent components of the external input current <bold>I</bold><sub>e</sub> in <xref ref-type="disp-formula" rid="pcbi.1004895.e009">Eq (4)</xref>; the matrix <bold>Γ</bold> is a decoding matrix that fixes the relation between spiking and continuous dynamics (see next section). We note that the matrices <bold>Γ</bold> and <bold>A</bold> are largely unconstrained, such that the coupling strengths maintain a large degree of arbitrariness. Ideally, <italic>W</italic><sub><italic>njm</italic></sub> is independent of <italic>n</italic>, therefore neurons have dendrites that are similar in their input characteristics to dendrites in some other neurons (note that <bold>D</bold> may have zero entries, so dendrites can be absent). We interpret these as dendrites that are located in a similar tissue area and therefore connect to the same axons and receive similar inputs (cf. <xref ref-type="fig" rid="pcbi.1004895.g001">Fig 1c</xref> for an illustration). The interrelations between the coupling matrices might be realized by spike-timing dependent synaptic or structural plasticity. Indeed, for a simpler model and task, appropriate biologically plausible learning rules have been recently highlighted [<xref ref-type="bibr" rid="pcbi.1004895.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref025">25</xref>]. We tested robustness of our schemes against structural perturbations (see Figs C and D in <xref ref-type="supplementary-material" rid="pcbi.1004895.s001">S1 Text</xref>), in particular for deviations from the <italic>n</italic>-independence of <italic>W</italic><sub><italic>njm</italic></sub> (Fig C in <xref ref-type="supplementary-material" rid="pcbi.1004895.s001">S1 Text</xref>).</p>
<p>The networks <xref ref-type="disp-formula" rid="pcbi.1004895.e004">Eq (3)</xref> with saturating synapses have a largely unconstrained topology, in particular they can satisfy the rule that neurons usually act only excitatorily or inhibitorily. For the networks <xref ref-type="disp-formula" rid="pcbi.1004895.e009">Eq (4)</xref> with nonlinear dendrites, it is less obvious how to reconcile the rule with the constraints on the network connectivity. Solutions for this have been suggested in simpler systems and are subject to current research [<xref ref-type="bibr" rid="pcbi.1004895.ref012">12</xref>].</p>
<p>The key property of the introduced neural architecture is that the spike trains generated by the neurons encode with high signal-to-noise ratio a continuous signal that can be understood in terms of ordinary differential equations. In the following section we show how this signal is decoded from the spike trains. Thereafter, we may conclude that the spiking dynamics are sufficiently “tamed” such that standard learning rules can be applied to learn complicated computations.</p>
</sec>
<sec id="sec005">
<title>Direct encoding of continuous dynamics</title>
<p>The dynamics of a neural network with <italic>N</italic> integrate-and-fire neurons consist of two components, the sub-threshold dynamics <bold>V</bold>(<italic>t</italic>) = (<italic>V</italic><sub>1</sub>(<italic>t</italic>),…,<italic>V</italic><sub><italic>N</italic></sub>(<italic>t</italic>))<sup><italic>T</italic></sup> of the membrane potentials and the spike trains <bold>s</bold>(<italic>t</italic>) = (<italic>s</italic><sub>1</sub>(<italic>t</italic>),…,<italic>s</italic><sub><italic>N</italic></sub>(<italic>t</italic>))<sup><italic>T</italic></sup> (<xref ref-type="disp-formula" rid="pcbi.1004895.e001">Eq (1)</xref>), which are temporal sequences of <italic>δ</italic>-distributions. In the model with saturating synapses, all synaptic interactions are assumed to be significantly temporally filtered, such that the <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>) are continuous except at reset times after spiking (<xref ref-type="disp-formula" rid="pcbi.1004895.e004">Eq (3)</xref>). We posit that the <bold>V</bold>(<italic>t</italic>) and the <bold>s</bold>(<italic>t</italic>) should together form some <italic>N</italic>-dimensional continuous dynamics <bold>x</bold>(<italic>t</italic>) = (<italic>x</italic><sub>1</sub>(<italic>t</italic>),…,<italic>x</italic><sub><italic>N</italic></sub>(<italic>t</italic>))<sup><italic>T</italic></sup>. The simplest approach is to setup <bold>x</bold>(<italic>t</italic>) as a linear combination of the two components <bold>V</bold>(<italic>t</italic>) and <bold>s</bold>(<italic>t</italic>). To avoid infinities in <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>), we need to eliminate the occurring <italic>δ</italic>-distributions by employing a smoothed version of <italic>s</italic><sub><italic>n</italic></sub>(<italic>t</italic>). This should have a finite discontinuity at spike times such that the discontinuity in <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>) can be balanced. A straightforward choice is to use <italic>θr</italic><sub><italic>n</italic></sub>(<italic>t</italic>) (<xref ref-type="disp-formula" rid="pcbi.1004895.e002">Eq (2)</xref>) and to set
<disp-formula id="pcbi.1004895.e022"><alternatives><graphic id="pcbi.1004895.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mrow><mml:msub><mml:mi>V</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>θ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula>
(cf. <xref ref-type="fig" rid="pcbi.1004895.g001">Fig 1b</xref>). When the abovementioned conditions on λ<sub><italic>x</italic></sub>, λ<sub><italic>s</italic></sub>, <bold>A</bold> and <italic>γ</italic> are satisfied (cf. end of the section introducing networks with saturating synapses), the continuous signal <bold>x</bold>(<italic>t</italic>) follows a system of first order nonlinear ordinary differential equations similar to those describing standard non-spiking continuous rate networks used for computations (cf. [<xref ref-type="bibr" rid="pcbi.1004895.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref026">26</xref>] and <xref ref-type="disp-formula" rid="pcbi.1004895.e037">Eq (11)</xref> below),
<disp-formula id="pcbi.1004895.e023"><alternatives><graphic id="pcbi.1004895.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>V</mml:mi></mml:msub> <mml:msub><mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>-</mml:mo></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>x</mml:mi></mml:msub> <mml:msub><mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>+</mml:mo></mml:msub> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>A</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo form="prefix">tanh</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mi>γ</mml:mi> <mml:mi>θ</mml:mi></mml:mfrac> <mml:msub><mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>+</mml:mo></mml:msub></mml:mfenced> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mrow><mml:mtext>e</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula>
with the rectifications [<italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)]<sub>+</sub> = max(<italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>),0), [<italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)]<sub>−</sub> = min(<italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>),0). We call spiking networks where this is the case <italic>continuous signal coding spiking neural networks (CSNs)</italic>.</p>
<p>Except for the rectifications, <xref ref-type="disp-formula" rid="pcbi.1004895.e023">Eq (6)</xref> has a standard form for non-spiking continuous rate networks, used for computations [<xref ref-type="bibr" rid="pcbi.1004895.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref026">26</xref>]. A salient choice for λ<sub><italic>x</italic></sub> is λ<sub><italic>x</italic></sub> = λ<sub><italic>V</italic></sub>, i.e. <inline-formula id="pcbi.1004895.e024"><alternatives><graphic id="pcbi.1004895.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mrow><mml:msub><mml:mi>V</mml:mi> <mml:mtext>r</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:msub><mml:mo>λ</mml:mo> <mml:mi>V</mml:mi></mml:msub> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>s</mml:mi></mml:msub></mml:mfrac> <mml:mo>)</mml:mo> <mml:mi>θ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, such that the rectifications outside the tanh-nonlinearity vanish. <xref ref-type="disp-formula" rid="pcbi.1004895.e023">Eq (6)</xref> generates dynamics that are different from the standard ones in the respect that the trajectories of individual neurons are, e.g. for random Gaussian matrices <bold>A</bold>, not centered at zero. However, they can satisfy the conditions for universal computation (enslaveability/echo state property and high dimensional nonlinear dynamics) and generate longer-term fading memory for appropriate scaling of <bold>A</bold>. Also the corresponding spiking networks are then suitable for fading memory-dependent computations. Like for the standard networks [<xref ref-type="bibr" rid="pcbi.1004895.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref028">28</xref>], we can derive sufficient conditions to guarantee that the dynamics <xref ref-type="disp-formula" rid="pcbi.1004895.e023">Eq (6)</xref> are enslaveable by external signals (echo state property). ∥<bold>A</bold>∥ &lt; min(<italic>λ<sub>V</sub></italic>, <italic>λ<sub>x</sub></italic>), where ∥<bold>A</bold>∥ is the largest singular value of the matrix <bold>A</bold>, provides such a condition (see Supplementary material for the proof). The condition is rather strict, our applications indicate that the CSNs are also suited as computational reservoirs when it is violated. This is similar to the situation in standard rate network models [<xref ref-type="bibr" rid="pcbi.1004895.ref027">27</xref>]. We note that if the system is enslaved by an external signal, the time scale of <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>) is largely determined by this signal and not anymore by the intrinsic scales of the dynamical system.</p>
<p>We will now show that spiking neural networks <xref ref-type="disp-formula" rid="pcbi.1004895.e004">Eq (3)</xref> can encode continuous dynamics <xref ref-type="disp-formula" rid="pcbi.1004895.e023">Eq (6)</xref>. For this we derive the dynamical equation of the membrane potential <xref ref-type="disp-formula" rid="pcbi.1004895.e004">Eq (3)</xref> from the dynamics of <bold>x</bold>(<italic>t</italic>) using the coding rule <xref ref-type="disp-formula" rid="pcbi.1004895.e022">Eq (5)</xref>, the dynamical <xref ref-type="disp-formula" rid="pcbi.1004895.e002">Eq (2)</xref> for <italic>r</italic><sub><italic>n</italic></sub>(<italic>t</italic>) and the rule that a spike is generated whenever <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>) reaches threshold <italic>θ</italic>2: We first differentiate <xref ref-type="disp-formula" rid="pcbi.1004895.e022">Eq (5)</xref> to eliminate <inline-formula id="pcbi.1004895.e025"><alternatives><graphic id="pcbi.1004895.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> from <xref ref-type="disp-formula" rid="pcbi.1004895.e023">Eq (6)</xref> and employ <xref ref-type="disp-formula" rid="pcbi.1004895.e002">Eq (2)</xref> to eliminate <inline-formula id="pcbi.1004895.e026"><alternatives><graphic id="pcbi.1004895.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. The resulting expression for <inline-formula id="pcbi.1004895.e027"><alternatives><graphic id="pcbi.1004895.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> reads
<disp-formula id="pcbi.1004895.e028"><alternatives><graphic id="pcbi.1004895.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e028" xlink:type="simple"/><mml:math display="block" id="M28"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>V</mml:mi></mml:msub> <mml:msub><mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>-</mml:mo></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>x</mml:mi></mml:msub> <mml:msub><mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>+</mml:mo></mml:msub> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>A</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo form="prefix">tanh</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mi>γ</mml:mi> <mml:mi>θ</mml:mi></mml:mfrac> <mml:msub><mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>+</mml:mo></mml:msub></mml:mfenced> <mml:mo>-</mml:mo> <mml:mi>θ</mml:mi> <mml:msub><mml:mi>s</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>s</mml:mi></mml:msub> <mml:mi>θ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mrow><mml:mtext>e,</mml:mtext> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(7)</label></disp-formula>
It already incorporates the resets of size <italic>θ</italic> (cf. the term −<italic>θs</italic><sub><italic>n</italic></sub>(<italic>t</italic>)), they arise since <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>) = <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>) + <italic>θr</italic><sub><italic>n</italic></sub>(<italic>t</italic>) is continuous and <italic>r</italic><sub><italic>n</italic></sub>(<italic>t</italic>) increases by one at spike times (thus V must decrease by <italic>θ</italic>). We now eliminate the occurrences of [<italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)]<sub>+</sub> and [<italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)]<sub>−</sub>.</p>
<p>For this, we make two assumptions (cf. <xref ref-type="fig" rid="pcbi.1004895.g001">Fig 1b</xref>) on the <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>) if they are positive:</p>
<list list-type="order">
<list-item>
<p>The dynamics of <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>) are slow against the synaptic timescale <italic>τ</italic><sub><italic>s</italic></sub>,</p>
</list-item>
<list-item>
<p>the <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>) assume predominantly values <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)≫<italic>θ</italic>2.</p>
</list-item>
</list>
<p>First we consider the case <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)&gt;0. Since <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>) is reset when it reaches its threshold value <italic>θ</italic>2, <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>) is always smaller than <italic>θ</italic>2. Thus, given <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>)&gt;0 assumption 2 implies that we can approximate <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)≈<italic>θr</italic><sub><italic>n</italic></sub>(<italic>t</italic>), as the contribution of <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>) is negligible because <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>)≤<italic>θ</italic>2. This still holds if <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>) is negative and its absolute value is not large against <italic>θ</italic>2. Furthermore, assumption 1 implies that smaller negative <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>) cannot co-occur with positive <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>): <italic>r</italic><sub><italic>n</italic></sub>(<italic>t</italic>) is positive and in the absence of spikes it decays to zero on the synaptic time scale <italic>τ</italic><sub><italic>s</italic></sub> (<xref ref-type="disp-formula" rid="pcbi.1004895.e002">Eq (2)</xref>). When <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>)&lt;0, neuron <italic>n</italic> is not spiking anymore. Thus when <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>) is shrinking towards small negative values and <italic>r</italic><sub><italic>n</italic></sub>(<italic>t</italic>) is decaying on a timescale of <italic>τ</italic><sub><italic>s</italic></sub>, <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>) is also decaying on a time-scale <italic>τ</italic><sub><italic>s</italic></sub>. This contradicts assumption 1. Thus when <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)&gt;0, the absolute magnitude of <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>) is on the order of <italic>θ</italic>2. With assumption 2 we can thus set <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)≈<italic>θr</italic><sub><italic>n</italic></sub>(<italic>t</italic>), whenever <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)&gt;0, neglecting contributions of size <italic>θ</italic>2.</p>
<p>Now we consider <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)≤0. This implies <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>)≤0 (since always <italic>r</italic><sub><italic>n</italic></sub>(<italic>t</italic>)≥0) as well as a quick decay of <italic>r</italic><sub><italic>n</italic></sub>(<italic>t</italic>) to zero. When <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>) assumes values significantly below zero, assumption 1 implies that we have <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)≈<italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>) and <italic>r</italic><sub><italic>n</italic></sub>(<italic>t</italic>)≈0, otherwise <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>) must have changed from larger positive (assumption 2) to larger negative values on a timescale of <italic>τ</italic><sub><italic>s</italic></sub>.</p>
<p>The approximate expressions may be gathered in the replacements [<italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)]<sub>+</sub> = <italic>θr</italic><sub><italic>n</italic></sub>(<italic>t</italic>) and [<italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)]<sub>−</sub> = [<italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>)]<sub>−</sub>. Using these in <xref ref-type="disp-formula" rid="pcbi.1004895.e028">Eq (7)</xref> yields together with <inline-formula id="pcbi.1004895.e029"><alternatives><graphic id="pcbi.1004895.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mrow><mml:msub><mml:mo>λ</mml:mo> <mml:mi>x</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>s</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:msub><mml:mi>V</mml:mi> <mml:mtext>r</mml:mtext></mml:msub> <mml:mi>θ</mml:mi></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> <disp-formula id="pcbi.1004895.e030"><alternatives><graphic id="pcbi.1004895.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e030" xlink:type="simple"/><mml:math display="block" id="M30"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>V</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>V</mml:mi></mml:msub> <mml:msub><mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>V</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>-</mml:mo></mml:msub> <mml:mo>+</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>A</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mo form="prefix">tanh</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mi>γ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>+</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mtext>r</mml:mtext></mml:msub> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>s</mml:mi></mml:msub> <mml:msub><mml:mi>r</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>θ</mml:mi> <mml:msub><mml:mi>s</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mrow><mml:mtext>e,</mml:mtext> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(8)</label></disp-formula>
Note that our replacements allowed to eliminate the biologically implausible <inline-formula id="pcbi.1004895.e031"><alternatives><graphic id="pcbi.1004895.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:mi mathvariant="bold">V</mml:mi></mml:math></alternatives></inline-formula>-dependencies in the interaction term.</p>
<p>To simplify the remaining <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>)-dependence, we additionally assume that 2’ <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>) assumes predominantly values <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)≫λ<sub><italic>V</italic></sub> <italic>θ</italic>/(2λ<sub><italic>x</italic></sub>), if <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>) is positive. This can be stricter than assumption 2 depending on the values of λ<sub><italic>x</italic></sub> and λ<sub><italic>V</italic></sub>. For positive <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>), where λ<sub><italic>V</italic></sub>[<italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)]<sub>−</sub> in <xref ref-type="disp-formula" rid="pcbi.1004895.e028">Eq (7)</xref> is zero, λ<sub><italic>V</italic></sub> <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>) has an absolute magnitude on the order of λ<sub><italic>V</italic></sub> <italic>θ</italic>/2 (see the arguments above). Assumption 2’ implies that this is negligible against −λ<sub><italic>x</italic></sub>[<italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)]<sub>+</sub>. For negative <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>), we still have <italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)≈<italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>). This means that we may replace −λ<sub><italic>V</italic></sub>[<italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)]<sub>−</sub> by λ<sub><italic>V</italic></sub> <italic>V</italic><sub><italic>n</italic></sub>(<italic>t</italic>) in <xref ref-type="disp-formula" rid="pcbi.1004895.e028">Eq (7)</xref>. Taken together, under the assumptions 1,2,2’ we may use the replacements
<disp-formula id="pcbi.1004895.e032"><alternatives><graphic id="pcbi.1004895.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e032" xlink:type="simple"/><mml:math display="block" id="M32"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>+</mml:mo></mml:msub> <mml:mo>≈</mml:mo> <mml:mi>θ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mfenced close="]" open="[" separators=""><mml:msub><mml:mi>x</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>-</mml:mo></mml:msub> <mml:mo>≈</mml:mo> <mml:msub><mml:mi>V</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
in <xref ref-type="disp-formula" rid="pcbi.1004895.e028">Eq (7)</xref>, which directly yield <xref ref-type="disp-formula" rid="pcbi.1004895.e004">Eq (3)</xref>. Note that this also implies <italic>r</italic><sub><italic>n</italic></sub>(<italic>t</italic>)≫<italic>θ</italic>2 if the neuron is spiking, so during active periods inter-spike-intervals need to be considerably smaller than the synaptic time scale.</p>
<p>
<xref ref-type="disp-formula" rid="pcbi.1004895.e023">Eq (6)</xref> implies that the assumptions are justified for suitable parameters: For fixed parameters <inline-formula id="pcbi.1004895.e033"><alternatives><graphic id="pcbi.1004895.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mrow><mml:msub><mml:mi>τ</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>λ</mml:mo> <mml:mrow><mml:mi>s</mml:mi></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> and <italic>θ</italic> of the <bold>r</bold>-dynamics, we can choose sufficiently small λ<sub><italic>x</italic></sub>, large <italic>A</italic><sub><italic>nm</italic></sub> and small <italic>γ</italic> to ensure assumptions 1,2,2’ (cf. the conditions highlighted in the section “Network architecture”). On the other hand, for given dynamics <xref ref-type="disp-formula" rid="pcbi.1004895.e023">Eq (6)</xref>, we can always find a spiking system which generates the dynamics via Eqs (<xref ref-type="disp-formula" rid="pcbi.1004895.e004">3</xref>), (<xref ref-type="disp-formula" rid="pcbi.1004895.e002">2</xref>) and (<xref ref-type="disp-formula" rid="pcbi.1004895.e022">5</xref>), and satisfies the assumptions: We only need to choose <italic>τ</italic><sub><italic>s</italic></sub> sufficiently small such that assumption 1 is satisfied and the spike threshold sufficiently small such that assumptions 2,2’ are satisfied. For the latter, <italic>γ</italic> needs to be scaled like <italic>θ</italic> to maintain the dynamics of <italic>x</italic><sub><italic>n</italic></sub> and <italic>V</italic><sub><italic>r</italic></sub> needs to be computed from the expression for λ<sub><italic>x</italic></sub>. Interestingly, we find that also outside the range where the assumptions are satisfied, our approaches can still generate good results.</p>
<p>The recovery current in our model has the same time constant as the slow synaptic current. Indeed, experiments indicate that they possess the same characteristic timescales: Timescales for NMDA [<xref ref-type="bibr" rid="pcbi.1004895.ref029">29</xref>] and slow GABA<sub>A</sub>[<xref ref-type="bibr" rid="pcbi.1004895.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref031">31</xref>] receptor mediated currents are several tens of milliseconds. Afterdepolarizations have timescales of several tens of milliseconds as well [<xref ref-type="bibr" rid="pcbi.1004895.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref032">32</xref>–<xref ref-type="bibr" rid="pcbi.1004895.ref035">35</xref>]. Another prominent class of slow inhibitory currents is mediated by GABA<sub>B</sub> receptors and has time scales of one hundred to a few hundreds of milliseconds [<xref ref-type="bibr" rid="pcbi.1004895.ref036">36</xref>]. We remark that in our model the time constants of the afterdepolarization and the synaptic input currents may also be different without changing the dynamics: Assume that the synaptic time constant is different from that of the recovery current, but still satisfies the conditions that it is large against the inter-spike-intervals when the neuron is spiking and small against the timescale of [<italic>x</italic><sub><italic>n</italic></sub>(<italic>t</italic>)]<sub>+</sub>. The synaptic current generated by the spike train of neuron <italic>n</italic> will then be approximately continuous and the filtering does not seriously affect its overall shape beyond smoothing out the spikes. As a consequence, the synaptic and the recovery currents are approximately proportional up to a constant factor that results from the different integrated contribution of individual spikes to them. Rescaling <italic>γ</italic> by this factor thus yields dynamics equivalent to the one with identical time constants.</p>
</sec>
<sec id="sec006">
<title>Distributed encoding of continuous dynamics</title>
<p>In the above-described simple CSNs (CSNs with saturating synapses), each spiking neuron gives rise to one nonlinear continuous variable. The resulting condition that the inter-spike-intervals are small against the synaptic time constants if the neuron is spiking may in biological neural networks be satisfied for bursting or fast spiking neurons with slow synaptic currents. It will be invalid for different neurons and synaptic currents. The condition becomes unnecessary when the spiking neurons encode continuous variables collectively, i.e. if we partially replace the temporal averaging in <italic>r</italic><sub><italic>n</italic></sub>(<italic>t</italic>) by an ensemble averaging. This can be realized by an extension of the above model, where only a lower, say <italic>J</italic>−, dimensional combination <bold>x</bold>(<italic>t</italic>) of the <italic>N</italic> − dimensional vectors <bold>V</bold>(<italic>t</italic>) and <bold>r</bold>(<italic>t</italic>) is continuous,
<disp-formula id="pcbi.1004895.e034"><alternatives><graphic id="pcbi.1004895.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e034" xlink:type="simple"/><mml:math display="block" id="M34"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold">L</mml:mi> <mml:mi mathvariant="bold">V</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mover accent="true"><mml:mo mathvariant="bold">Γ</mml:mo> <mml:mo>˜</mml:mo></mml:mover> <mml:mi mathvariant="bold">r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(10)</label></disp-formula>
where <bold>L</bold> and <inline-formula id="pcbi.1004895.e035"><alternatives><graphic id="pcbi.1004895.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:mover accent="true"><mml:mo>Γ</mml:mo> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> are <italic>J</italic>×<italic>N</italic> matrices (note that <xref ref-type="disp-formula" rid="pcbi.1004895.e022">Eq (5)</xref> is a special case with <italic>N</italic> = <italic>J</italic> and diagonal matrices <bold>L</bold> and <inline-formula id="pcbi.1004895.e036"><alternatives><graphic id="pcbi.1004895.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:mover accent="true"><mml:mo>Γ</mml:mo> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula>). We find that spiking networks with nonlinear dendrites <xref ref-type="disp-formula" rid="pcbi.1004895.e009">Eq (4)</xref> can encode such a lower dimensional variable <bold>x</bold>(<italic>t</italic>). The <bold>x</bold>(<italic>t</italic>) satisfy <italic>J</italic>-dimensional standard equations describing non-spiking continuous rate networks used for reservoir computing [<xref ref-type="bibr" rid="pcbi.1004895.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref026">26</xref>],
<disp-formula id="pcbi.1004895.e037"><alternatives><graphic id="pcbi.1004895.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e037" xlink:type="simple"/><mml:math display="block" id="M37"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msub><mml:mo>λ</mml:mo> <mml:mi>x</mml:mi></mml:msub> <mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">A</mml:mi> <mml:mo form="prefix">tanh</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mfenced> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">c</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(11)</label></disp-formula>
We denote the resulting spiking networks as CSNs with nonlinear dendrites.</p>
<p>The derivation (see Supplementary material for details) generalizes the ideas introduced in refs. [<xref ref-type="bibr" rid="pcbi.1004895.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref037">37</xref>] to the approximation of nonlinear dynamical systems: We assume an approximate decoding equation (cf. also <xref ref-type="disp-formula" rid="pcbi.1004895.e032">Eq (9)</xref>),
<disp-formula id="pcbi.1004895.e038"><alternatives><graphic id="pcbi.1004895.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e038" xlink:type="simple"/><mml:math display="block" id="M38"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>≈</mml:mo> <mml:mo mathvariant="bold">Γ</mml:mo> <mml:mi mathvariant="bold">r</mml:mi> <mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(12)</label></disp-formula>
where <bold>Γ</bold> is a <italic>J</italic>×<italic>N</italic> decoding matrix and employ an optimization scheme that minimizes the decoding error resulting from <xref ref-type="disp-formula" rid="pcbi.1004895.e038">Eq (12)</xref> at each time point. This yields the condition that a spike should be generated when a linear combination of <bold>x</bold>(<italic>t</italic>) and <bold>r</bold>(<italic>t</italic>) exceeds some constant value. We interpret this linear combination as membrane potential <bold>V</bold>(<italic>t</italic>). Solving for x(<italic>t</italic>) gives <bold>L</bold> and <inline-formula id="pcbi.1004895.e039"><alternatives><graphic id="pcbi.1004895.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:mover accent="true"><mml:mo>Γ</mml:mo> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> in terms of <bold>Γ</bold> in <xref ref-type="disp-formula" rid="pcbi.1004895.e034">Eq (10)</xref>. Taking the temporal derivative yields <inline-formula id="pcbi.1004895.e040"><alternatives><graphic id="pcbi.1004895.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">V</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, first in terms of <inline-formula id="pcbi.1004895.e041"><alternatives><graphic id="pcbi.1004895.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004895.e042"><alternatives><graphic id="pcbi.1004895.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and after replacing them via Eqs (<xref ref-type="disp-formula" rid="pcbi.1004895.e002">2</xref>),(<xref ref-type="disp-formula" rid="pcbi.1004895.e037">11</xref>), in terms of <bold>x</bold>(<italic>t</italic>), <bold>r</bold>(<italic>t</italic>) and <bold>s</bold>(<italic>t</italic>). We then eliminate x(<italic>t</italic>) using <xref ref-type="disp-formula" rid="pcbi.1004895.e038">Eq (12)</xref> and add a membrane potential leak term for biological realism and increased stability of numerical simulations. This yields <xref ref-type="disp-formula" rid="pcbi.1004895.e009">Eq (4)</xref> together with the optimal values of the parameters given in <xref ref-type="table" rid="pcbi.1004895.t001">Table 1</xref>. We note that the difference to the derivation in ref. [<xref ref-type="bibr" rid="pcbi.1004895.ref012">12</xref>] is the use of a nonlinear equation when replacing <inline-formula id="pcbi.1004895.e043"><alternatives><graphic id="pcbi.1004895.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. We further note that the spiking approximation of the continuous dynamics becomes exact, if in the last step <bold>x</bold>(<italic>t</italic>) is eliminated using <xref ref-type="disp-formula" rid="pcbi.1004895.e034">Eq (10)</xref> and the leak term is omitted as it does not arise from the formalism in contrast to the case of CSNs with saturating synapses. Like in CSNs with saturating synapses, using the approximated decoding <xref ref-type="disp-formula" rid="pcbi.1004895.e038">Eq (12)</xref> eliminates the biologically implausible <bold>V</bold>-dependencies in the interaction terms. For an illustration of this coding see <xref ref-type="fig" rid="pcbi.1004895.g001">Fig 1d</xref>.</p>
</sec>
</sec>
<sec id="sec007">
<title>Learning universal computations</title>
<p>Recurrent continuous rate networks are a powerful means for learning of various kinds of computations, like steering of movements and processing of sequences [<xref ref-type="bibr" rid="pcbi.1004895.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref004">4</xref>]. For this, an input and/or an output feedback signal needs to be able to “enslave” the network’s high-dimensional dynamics [<xref ref-type="bibr" rid="pcbi.1004895.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref028">28</xref>]. This means that at any point in time the network’s state is a deterministic function of the recent history of input and feedback signals. The function needs to be high dimensional, nonlinear, and possess fading memory. A standard model generating suitable dynamics are continuous rate networks of the form <xref ref-type="disp-formula" rid="pcbi.1004895.e037">Eq (11)</xref>. Due to the typically assumed random recurrent connectivity, each neuron acts as a randomly chosen, nonlinear function with fading memory. Linearly combining them like basis functions by a linear readout can approximate arbitrary, nonlinear functions with fading memory (time-scales are limited by the memory of the network), and in this sense <italic>universal computations</italic> on the input and the feedback. The feedback can prolong the fading memory and allow to generate self-contained dynamical systems and output sequences [<xref ref-type="bibr" rid="pcbi.1004895.ref002">2</xref>–<xref ref-type="bibr" rid="pcbi.1004895.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref038">38</xref>]. The feedback can be incorporated into the network by directly training the recurrent synaptic weights [<xref ref-type="bibr" rid="pcbi.1004895.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref038">38</xref>].</p>
<p>Our understanding of the complex spiking dynamics of CSNs in terms of nonlinear first order differential equations enables us to apply the above theory to spiking neural networks: In the first step, we were able to conclude that our CSNs can generate enslaveable and thus computationally useful dynamics as they can be decoded to continuous dynamics that possess this property. In the second step, we have to ask which and how output signals should be learned to match a desired signal: In a biological setting, the appropriate signals are the sums of synaptic or dendritic input currents that spike trains generate, since these affect the somata of postsynaptic neurons as well as effectors such as muscles [<xref ref-type="bibr" rid="pcbi.1004895.ref039">39</xref>]. To perform, e.g., a desired continuous movement, they have to prescribe the appropriate muscle contraction strengths. For both CSNs with saturating synapses and with nonlinear dendrites, we choose the outputs to have the same form as the recurrent inputs that a soma of a neuron within the CSN receives. Accordingly, in our CSNs with saturating synapses, we interpret sums of the postsynaptic currents
<disp-formula id="pcbi.1004895.e044"><alternatives><graphic id="pcbi.1004895.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e044" xlink:type="simple"/><mml:math display="block" id="M44"><mml:mrow><mml:msub><mml:mi>z</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>m</mml:mi></mml:mrow> <mml:mi>o</mml:mi></mml:msubsup> <mml:mo form="prefix">tanh</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mi>γ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>=</mml:mo> <mml:mo>:</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>m</mml:mi></mml:mrow> <mml:mi>o</mml:mi></mml:msubsup> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>m</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(13)</label></disp-formula>
as output signals, where the index <italic>k</italic> distinguishes <italic>K</italic><sub>out</sub> different outputs, and <inline-formula id="pcbi.1004895.e045"><alternatives><graphic id="pcbi.1004895.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>m</mml:mi></mml:mrow> <mml:mi>o</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> are the learnable synaptic output weights. For networks with nonlinear dendrites the outputs are a linear combination of inputs preprocessed by nonlinear dendrites
<disp-formula id="pcbi.1004895.e046"><alternatives><graphic id="pcbi.1004895.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e046" xlink:type="simple"/><mml:math display="block" id="M46"><mml:mrow><mml:msub><mml:mi>z</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>J</mml:mi></mml:munderover> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mi>o</mml:mi></mml:msubsup> <mml:mo form="prefix">tanh</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mo>Γ</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>r</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfenced> <mml:mo>=</mml:mo> <mml:mo>:</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>J</mml:mi></mml:munderover> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mi>o</mml:mi></mml:msubsup> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(14)</label></disp-formula>
where the strengths <inline-formula id="pcbi.1004895.e047"><alternatives><graphic id="pcbi.1004895.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e047" xlink:type="simple"/><mml:math display="inline" id="M47"><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mi>o</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> of the dendro-somatic coupling are learned [<xref ref-type="bibr" rid="pcbi.1004895.ref040">40</xref>]. The networks can now learn the output weights such that <italic>z</italic><sub><italic>k</italic></sub>(<italic>t</italic>) imitates a target signal <italic>F</italic><sub><italic>k</italic></sub>(<italic>t</italic>), using standard learning rules for linear readouts (see <xref ref-type="fig" rid="pcbi.1004895.g002">Fig 2a</xref> for an illustration). We employ the recursive least squares method [<xref ref-type="bibr" rid="pcbi.1004895.ref041">41</xref>].</p>
<fig id="pcbi.1004895.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004895.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Setups used to learn versatile nonlinear computations with spiking neural networks.</title>
<p>(a) A static <italic>continuous signal coding spiking neural network</italic> (<italic>CSN</italic>, gray shaded) serves as a spiking computational reservoir with high signal-to-noise ratio. The results of computations on current and past external inputs <bold>I</bold><sub>e</sub> can be extracted by simple neuron-like readouts. These linearly combine somatic inputs generated by saturating synapses or nonlinear dendrites, <inline-formula id="pcbi.1004895.e048"><alternatives><graphic id="pcbi.1004895.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:mover accent="true"><mml:mi mathvariant="bold">r</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:math></alternatives></inline-formula> (red), to output signals <bold>z</bold> (Eqs (<xref ref-type="disp-formula" rid="pcbi.1004895.e044">13</xref>, <xref ref-type="disp-formula" rid="pcbi.1004895.e046">14</xref>)). The output weights <bold>w</bold><sup>o</sup> are learned such that <bold>z</bold> approximates the desired continuous target signals. (b) <italic>Plastic continuous signal coding spiking neural networks (PCSNs)</italic> possess a loop that feeds the outputs <bold>z</bold> back via static connections as an additional input <inline-formula id="pcbi.1004895.e049"><alternatives><graphic id="pcbi.1004895.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:msubsup><mml:mi mathvariant="bold">I</mml:mi> <mml:mrow><mml:mtext>e</mml:mtext></mml:mrow> <mml:mi>f</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> (blue, <xref ref-type="disp-formula" rid="pcbi.1004895.e050">Eq (15)</xref>). Such networks have increased computational capabilities allowing them to, e.g., generate desired self-sustained activity. (c) The feedback loop can be incorporated into the recurrent network via plastic recurrent connections (red in gray shaded area).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004895.g002" xlink:type="simple"/>
</fig>
<p>To increase the computational and learning abilities, the output signals should be fed back to the network as an (additional) input (<xref ref-type="fig" rid="pcbi.1004895.g002">Fig 2b</xref>)
<disp-formula id="pcbi.1004895.e050"><alternatives><graphic id="pcbi.1004895.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e050" xlink:type="simple"/><mml:math display="block" id="M50"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mrow><mml:mtext>e</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>β</mml:mi></mml:mrow> <mml:mi>f</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>K</mml:mi> <mml:mtext>out</mml:mtext></mml:msub></mml:munderover> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>β</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mi>f</mml:mi></mml:msubsup> <mml:msub><mml:mi>z</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>K</mml:mi> <mml:mtext>out</mml:mtext></mml:msub></mml:munderover> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>β</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mi>f</mml:mi></mml:msubsup> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>ρ</mml:mi></mml:munder> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>ρ</mml:mi></mml:mrow> <mml:mi>o</mml:mi></mml:msubsup> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>ρ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(15)</label></disp-formula>
where each neuron receives a linear combination of the output signals <italic>z</italic><sub><italic>k</italic></sub>(<italic>t</italic>) with static feedback connection strengths <inline-formula id="pcbi.1004895.e051"><alternatives><graphic id="pcbi.1004895.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e051" xlink:type="simple"/><mml:math display="inline" id="M51"><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>β</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mi>f</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>. Here and in the following Greek letter indices such as <italic>β</italic>,<italic>ρ</italic> range over all saturating synapses (<italic>β</italic>,<italic>ρ</italic> = 1,…,<italic>N</italic>; <inline-formula id="pcbi.1004895.e052"><alternatives><graphic id="pcbi.1004895.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>β</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo form="prefix">tanh</mml:mo> <mml:mo>(</mml:mo> <mml:mi>γ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mi>β</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>) in CSNs with saturating synapses, or over all nonlinear dendrites (<italic>β</italic>,<italic>ρ</italic> = 1,…,<italic>J</italic>; <inline-formula id="pcbi.1004895.e053"><alternatives><graphic id="pcbi.1004895.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e053" xlink:type="simple"/><mml:math display="inline" id="M53"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>β</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo form="prefix">tanh</mml:mo> <mml:mo>(</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mo>Γ</mml:mo> <mml:mrow><mml:mi>β</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>r</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>) in CSNs with nonlinear dendrites.</p>
<p>It often seems biologically more plausible not to assume a strong feedback loop that enslaves the recurrent network, but rather to train recurrent weights. Our CSNs allow for this (<xref ref-type="fig" rid="pcbi.1004895.g002">Fig 2c</xref>): We can transform the learning of output weights in networks with feedback into mathematically equivalent learning of recurrent connection strengths, between synapses (CSNs with saturating synapses) or dendrites (CSNs with nonlinear dendrites) and the soma [<xref ref-type="bibr" rid="pcbi.1004895.ref040">40</xref>] (we learn <italic>A</italic><sub><italic>nm</italic></sub>, see <xref ref-type="sec" rid="sec014">Methods</xref> for details of the implementation). We note that approximating different dynamical systems, e.g. ones equivalent to <xref ref-type="disp-formula" rid="pcbi.1004895.e037">Eq (11)</xref> but with the coupling matrix inside the nonlinearity [<xref ref-type="bibr" rid="pcbi.1004895.ref042">42</xref>], may also in CSNs with nonlinear dendrites allow to learn synaptic weights in similar manner. We call CSNs with learning of outputs in presence of feedback, or with learning of recurrent connections <italic>plastic continuous signal coding spiking neural networks (PCSNs).</italic></p>
<p>To learn feedback and recurrent connections, we use the FORCE imitation learning rule, which has recently been suggested for networks of continuous rate neurons [<xref ref-type="bibr" rid="pcbi.1004895.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref038">38</xref>]: We use fast online learning based on the recursive least squares rule of the output weights in order to ensure that the output of the network is similar to the desired output at all times. Since during training the output is ensured to be close to the desired one, it can be used as feedback to the network at all times. The remaining deviations from the desired output are expected to be particularly suited as training noise as they reflect the system’s inherent noise. As mentioned before, the feedback loop may be incorporated in the recurrent network connectivity. During training, the reservoir connections are then learned in a similar manner as the readout.</p>
<p>In the following, we show that our approach allows spiking neural networks to perform a broad variety of tasks. In particular, we show learning of desired self-sustained dynamics at a degree of difficulty that has, to our knowledge, previously only been accessible with continuous rate networks.</p>
</sec>
<sec id="sec008">
<title>Applications</title>
<sec id="sec009">
<title>Self-sustained pattern generation</title>
<p>Animals including humans can learn a great variety of movements, from periodic patterns like gait or swimming, to much more complex ones like producing speech, generating chaotic locomotion [<xref ref-type="bibr" rid="pcbi.1004895.ref043">43</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref044">44</xref>] or playing the piano. Moreover when an animal learns to use an object (<xref ref-type="fig" rid="pcbi.1004895.g003">Fig 3a</xref>), it has to learn the dynamical properties of the object as well as how its body behaves when interacting with it. Especially for complex, non-periodic dynamics, a dynamical system has to be learned with high precision.</p>
<p>How are spiking neural networks able to learn dynamical systems, store them and replay their activity? We find that PCSNs may solve the problem. They are able to learn periodic patterns of different degree of complexity as well as chaotic dynamical systems by imitation learning. <xref ref-type="fig" rid="pcbi.1004895.g003">Fig 3</xref> illustrates this for PCSNs with nonlinear synapses (<xref ref-type="fig" rid="pcbi.1004895.g003">Fig 3d and 3e</xref>) and with nonlinear dendrites (<xref ref-type="fig" rid="pcbi.1004895.g003">Fig 3b, 3e and 3f</xref>).</p>
<fig id="pcbi.1004895.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004895.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Learning dynamics with spiking neural networks.</title>
<p>(a): Schematic hunting scene, illustrating the need for complicated dynamical systems learning and control. The hominid has to predict the motion of its prey, and to predict and control the movements of its body and the projectile. (b-h): Learning of self-sustained dynamical patterns by spiking neural networks. (b): A sine wave generated by summed, synaptically and dendritically filtered output spike trains of a PCSN with nonlinear dendrites. (c): A sample of the network’s spike trains generating the sine in (b). (d): A saw tooth pattern generated by a PCSN with saturating synapses. (e): A more complicated smooth pattern generated by both architectures (blue: nonlinear dendrites, red: saturating synapses). (f-h): Learning of chaotic dynamics (Lorenz system), with a PCSN with nonlinear dendrites. (f): The spiking network imitates an example trajectory of the Lorenz system during training (blue); it continues generating the dynamics during testing (red). (g): Detailed view of (f) highlighting how the example trajectory (yellow) is imitated during training and continued during testing. (h): The spiking network approximates not explicitly trained quantitative dynamical features, like the tent map between subsequent maxima of the z-coordinate. The ideal tent map (yellow) is closely approximated by the tent map generated by the PCSN (red). The spiking network sporadically generates errors, cf. the larger loop in (f) and the outlier points in (h). Panel (h) shows a ten times longer time series than (f), with three errors.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004895.g003" xlink:type="simple"/>
</fig>
<p>The figure displays the recall of three periodic movements after learning: a sine wave, a more complicated non-differentiable saw tooth pattern and a “camel’s hump” superposition of sine and cosine. Also for long simulation times, we find no deviation from the displayed dynamics except for an inevitable phase shift (Fig Ga in <xref ref-type="supplementary-material" rid="pcbi.1004895.s001">S1 Text</xref>). It results from accumulation of small differences between the learned and desired periods. Apart from this, the error between the recalled and the desired signals is approximately constant over time (Fig Gb in <xref ref-type="supplementary-material" rid="pcbi.1004895.s001">S1 Text</xref>). This indicates that the network has learned a stable periodic orbit to generate the desired dynamics, the orbit is sufficiently stable to withstand the intrinsic noise of the system. <xref ref-type="fig" rid="pcbi.1004895.g003">Fig 3</xref> furthermore illustrates learning of a chaotic dynamical system. Here, the network learns to generate the time varying dynamics of all three components of the Lorenz system and produces the characteristic attractor pattern after learning (<xref ref-type="fig" rid="pcbi.1004895.g003">Fig 3f</xref>). Due to the encoding of the dynamics in spike trains, the signal maintains a small deterministic error which emerges from the encoding of a continuous signal by discrete spikes (<xref ref-type="fig" rid="pcbi.1004895.g003">Fig 3g</xref>). The individual training and recall trajectories quickly depart from each other after the end of learning since they are chaotic. However, also for long simulation times, we observe qualitatively the same dynamics, indicating that the correct dynamical system was learned (Fig Gc in <xref ref-type="supplementary-material" rid="pcbi.1004895.s001">S1 Text</xref>). Occasionally, errors occur, cf. the larger loop in <xref ref-type="fig" rid="pcbi.1004895.g003">Fig 3f</xref>. This is to be expected due to the relatively short training period, during which only a part of the phase space covered by the attractor is visited. Importantly, we observe that after errors the dynamics return to the desired ones indicating that the general stability property of the attractor is captured by the learned system. To further test these observations, we considered a not explicitly trained long-term feature of the Lorenz-dynamics, namely the tent-map which relates the height <italic>z</italic><sub><italic>n</italic> − 1</sub> of the (<italic>n</italic> − 1)th local maximum in the <italic>z</italic> − coordinate, to the height <italic>z</italic><sub><italic>n</italic></sub> of the subsequent local maximum. The spiking network indeed generates the map (<xref ref-type="fig" rid="pcbi.1004895.g003">Fig 3h</xref>), with two outlier points corresponding to each error.</p>
<p>In networks with saturating synapses, the spike trains are characterized by possibly intermittent periods of rather high-frequency spiking. In networks with nonlinear dendrites, the spike trains can have low frequencies and they are highly irregular (<xref ref-type="fig" rid="pcbi.1004895.g003">Fig 3c</xref>, Fig F in <xref ref-type="supplementary-material" rid="pcbi.1004895.s001">S1 Text</xref>). In agreement with experimental observations (e.g. [<xref ref-type="bibr" rid="pcbi.1004895.ref045">45</xref>]), the neurons can have preferred parts of the encoded signal in which they spike with increased rates.</p>
<p>The dynamics of the PCSNs and the generation of the desired signal are robust against dynamic and structural perturbations. They sustain noise inputs which would accumulate to several ten percent of the level of the threshold within the membrane time constant, for a neuron without further input (Fig B in <xref ref-type="supplementary-material" rid="pcbi.1004895.s001">S1 Text</xref>). For larger deviations of <italic>W</italic><sub><italic>njm</italic></sub> from their optimal values, PCSNs with nonlinear dendrites can keep their learning capabilities, if <italic>μ</italic> is tuned to a specific range. Outside this range, the capabilities break down at small deviations (Fig C in <xref ref-type="supplementary-material" rid="pcbi.1004895.s001">S1 Text</xref>). However, a slightly modified version of the models, where the reset is always to −<italic>θ</italic> (even if there was fast excitation that drove the neuron to spike by a suprathreshold input), has a high degree of robustness against such structural perturbations. We also checked that the fast connections are important, albeit substantial weakening can be tolerated (Fig D in <xref ref-type="supplementary-material" rid="pcbi.1004895.s001">S1 Text</xref>).</p>
<p>The deterministic spike code of our PCSNs encodes the output signal much more precisely than neurons generating a simple Poisson code, which facilitates learning. We have quantified this using a comparison between PCSNs with saturating synapses and networks of Poisson neurons of equal size, both learning the saw tooth pattern in the same manner. Since both codes become more precise with increasing spike rate of individual neurons, we compared the testing error between networks with equal spike rates. Due to their higher signal-to-noise ratio, firing rates required by the PCSNs to achieve the same pattern generation quality are more than one order of magnitude lower (Fig A in <xref ref-type="supplementary-material" rid="pcbi.1004895.s001">S1 Text</xref>).</p>
</sec>
<sec id="sec010">
<title>Delayed reaction/time interval estimation</title>
<p>For many tasks, e.g. computations focusing on recent external input and generation of self-sustained patterns, it is essential that the memory of the involved recurrent networks is fading: If past states cannot be forgotten, they lead to different states in response to similar recent inputs. A readout that learns to extract computations on recent input will then quickly reach its capacity limit. In neural networks, fading memory originates on the one hand from the dynamics of single neurons, e.g. due to their finite synaptic and membrane time constants; on the other hand it is a consequence of the neurons’ connection to a network [<xref ref-type="bibr" rid="pcbi.1004895.ref046">46</xref>–<xref ref-type="bibr" rid="pcbi.1004895.ref048">48</xref>]. In standard spiking neural network models, the overall fading memory is short, of the order of hundreds of milliseconds [<xref ref-type="bibr" rid="pcbi.1004895.ref009">9</xref>–<xref ref-type="bibr" rid="pcbi.1004895.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref049">49</xref>]. It is a matter of current debate how this can be extended by suitable single neuron properties and topology [<xref ref-type="bibr" rid="pcbi.1004895.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref050">50</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref051">51</xref>]. Many biological computations, e.g. the simple understanding of a sentence, require longer memory, on the order of seconds.</p>
<p>We find that CSNs without learning of recurrent connectivity or feedback access such time scales. We illustrate this by means of a delayed reaction/time estimation task: In the beginning of a trial, the network receives a short input pulse. By imitation learning, the network output learns to generate a desired delayed reaction. For this, it needs to specifically amplify the input’s dynamical trace in the recurrent spiking activity, at a certain time interval. The desired response is a Gaussian curve, representative for any type of delayed reaction. The reaction can be generated several seconds after the input (<xref ref-type="fig" rid="pcbi.1004895.g004">Fig 4a-4c</xref>).</p>
<fig id="pcbi.1004895.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004895.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Learning of longer-term memory dependent computations with spiking neural networks.</title>
<p>(a-c): Delayed reaction and time interval estimation: The synaptic output of a CSN learns to generate a generic reaction several seconds after a short input. Upper panels show typical examples of input, desired and actual reactions (green, yellow and red traces). In the three panels, the desired reaction delay is the same (9sec), the networks (CSNs with saturating synapses) have different levels of recurrent connection strengths ((a), (b), (c): low, intermediate, high level). The generation of the reaction is best for the network with intermediate level of connection strength. The CSNs with lower or higher levels have not maintained sufficient memory due to their extinguished or noisy and likely chaotic dynamics (gray background lines: spike rates of individual neurons). The median errors of responses measured for different delays in ensembles of networks (levels of connection strength as in the upper panels), are given in the lower panels. The shaded regions represent the area between the first and third quartile of the response errors. Dashed lines highlight delay and error size of the examples in the upper panels. (d): Persistent retaining of instructions and switching between computations: The network receives (i) two random continuous operand inputs (upper sub-panel, yellow and purple traces), and (ii) two pulsed instruction inputs (middle sub-panel, blue and green; memory of last instruction pulse: red). The network has learned to perform different computations on the operand inputs, depending on the last instruction (lower subpanel): if it was +1 (triggered by instruction channel 1), the network performs a nonlinear computation, it outputs the absolute value of the difference of the operands (red trace (network output) agrees with blue); if it was -1 (triggered by channel 2), the values of the operands are added (red trace agrees with green trace).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004895.g004" xlink:type="simple"/>
</fig>
<p>The quality of the reaction pattern depends on the connection strengths within the network, specified by the spectral radius <italic>g</italic> of the coupling matrix divided by the leak of a single corresponding continuous unit λ<sub><italic>x</italic></sub>. Memory is kept best in an intermediate regime (<xref ref-type="fig" rid="pcbi.1004895.g004">Fig 4b</xref>), where the CSN stays active over long periods of time without overwriting information. This has also been observed for continuous rate networks [<xref ref-type="bibr" rid="pcbi.1004895.ref052">52</xref>]. For too weak connections (<xref ref-type="fig" rid="pcbi.1004895.g004">Fig 4a</xref>), the CSN returns to the inactive state after short time, rendering it impossible to retrieve input information later. If the connections are too strong, (<xref ref-type="fig" rid="pcbi.1004895.g004">Fig 4c</xref>), the CSN generates self-sustained, either irregular asynchronous or oscillating activity, partly overwriting information and hindering its retrieval. We observe that already the memory in disconnected CSNs with synaptic saturation can last for times beyond hundreds of milliseconds (cf. Fig E in <xref ref-type="supplementary-material" rid="pcbi.1004895.s001">S1 Text</xref>). This is a consequence of the recovery current: If a neuron has spiked several times in succession, the accumulated recovery current leads to further spiking (and further recovery current), and thus dampens the decay of a strong activation of the neuron [<xref ref-type="bibr" rid="pcbi.1004895.ref053">53</xref>].</p>
<p>Experiments show that during time estimation tasks, neurons are particularly active at two times: When the stimulus is received and when the estimated time has passed [<xref ref-type="bibr" rid="pcbi.1004895.ref054">54</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref055">55</xref>]. Often the neuron populations that show activity at these points are disjoint. Our model reproduces this behavior for networks with good memory performance. In particular, at the time of the initial input the recurrently connected neurons become highly active (gray traces in <xref ref-type="fig" rid="pcbi.1004895.g004">Fig 4b</xref>, upper sub-panel) while at the estimated reaction time, readout neurons would show increased activity (red trace).</p>
</sec>
<sec id="sec011">
<title>Persistent memory and context dependent switching</title>
<p>Tasks often also require to store memories persistently, e.g. to remember instructions [<xref ref-type="bibr" rid="pcbi.1004895.ref056">56</xref>]. Such memories may be maintained in learned attractor states (e.g. [<xref ref-type="bibr" rid="pcbi.1004895.ref057">57</xref>–<xref ref-type="bibr" rid="pcbi.1004895.ref060">60</xref>]). In the framework of our computing scheme, this requires the presence of output feedback [<xref ref-type="bibr" rid="pcbi.1004895.ref003">3</xref>]. Here, we illustrate the ability of PCSNs to learn and maintain persistent memories as attractor states as well as the ability to change behavior according to them. For this, we use a task that requires memorizing computational instructions (<xref ref-type="fig" rid="pcbi.1004895.g004">Fig 4d</xref>) [<xref ref-type="bibr" rid="pcbi.1004895.ref003">3</xref>]. The network has two types of inputs: After pulses in the instruction channels, it needs to switch persistently between different rules for computation on the current values of operand channels. To store persistent memory, the recurrent connections are trained such that an appropriate output can indicate the instruction channel that has sent the last pulse: The network learns to largely ignore the signal when a pulse arrives from the already remembered instruction channel, and to switch states otherwise. Due to the high signal-to-noise ratio of our deterministic spike code, the PCSNs are able to keep a very accurate representation of the currently valid instruction in their recurrent dynamics. <xref ref-type="fig" rid="pcbi.1004895.g004">Fig 4d</xref>, middle sub-panel, shows this by displaying the output of the linear readout trained to extract this instruction from the network dynamics. A similarly high precision can be observed for the output of the computational task, cf. <xref ref-type="fig" rid="pcbi.1004895.g004">Fig 4d</xref>, lower sub-panel.</p>
</sec>
<sec id="sec012">
<title>Building of world models, and control</title>
<p>In order to control its environment, an animal has to learn the laws that govern the environment’s dynamics, and to develop a control strategy. Since environments are partly unpredictable and strategies are subject to evolutionary pressure, we expect that they may be described by stochastic optimal control theory. A particularly promising candidate framework is path integral control, since it computes the optimal control by simulating possible future scenarios under different random exploratory controls, and the optimal control is a simple weighted average of them [<xref ref-type="bibr" rid="pcbi.1004895.ref061">61</xref>]. For this, an animal needs an internal model of the system or tool it wants to act on. It can then mentally simulate different ways to deal with the system and compute an optimal one. Recent experiments indicate that animals indeed conduct thought experiments exploring and evaluating possible future actions and movement trajectories before performing one [<xref ref-type="bibr" rid="pcbi.1004895.ref062">62</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref063">63</xref>].</p>
<p>Here we show that by imitation learning, spiking neural networks, more precisely PCSNs with a feedback loop, can acquire an internal model of a dynamical system and that this can be used to compute optimal controls and actions. As a specific, representative task, we choose to learn and control a stochastic pendulum (<xref ref-type="fig" rid="pcbi.1004895.g005">Fig 5a and 5b</xref>). The pendulum’s dynamics are given by
<disp-formula id="pcbi.1004895.e054"><alternatives><graphic id="pcbi.1004895.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e054" xlink:type="simple"/><mml:math display="block" id="M54"><mml:mrow><mml:mover accent="true"><mml:mi>ϕ</mml:mi> <mml:mo>¨</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>c</mml:mi> <mml:msub><mml:mi>ω</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mover accent="true"><mml:mi>ϕ</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>ω</mml:mi> <mml:mrow><mml:mn>0</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo form="prefix">sin</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ϕ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>ξ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>u</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(16)</label></disp-formula>
with the angular displacement <italic>ϕ</italic> relative to the direction of gravitational acceleration, the undamped angular frequency for small amplitudes <italic>ω</italic><sub>0</sub>, the damping ratio <italic>c</italic>, a random (white noise) angular force <italic>ξ</italic>(<italic>t</italic>) and the deterministic control angular force <italic>u</italic>(<italic>t</italic>), both applied to the pivot axis. The PCSN needs to learn the pendulum’s dynamics under largely arbitrary external control forces; this goes beyond the tasks of the previous sections. It is achieved during an initial learning phase characterized by motor babbling as observed in infants [<xref ref-type="bibr" rid="pcbi.1004895.ref064">64</xref>] and similarly in bird song learning [<xref ref-type="bibr" rid="pcbi.1004895.ref065">65</xref>]: During this phase, there is no deterministic control, <italic>u</italic> = 0, and the pendulum is driven by a random exploratory force <italic>ξ</italic> only. Also the PCSN receives <italic>ξ</italic> as input and learns to imitate the resulting pendulum’s dynamics with its output.</p>
<fig id="pcbi.1004895.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004895.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Model building and mental exploration to compute optimal control.</title>
<p>(a): Learning of an internal world model with spiking neural networks. During model building, random exploratory control drives the dynamical system (here: a swinging pendulum). The spiking neural network is provided with the same control as input and learns to mimic the behavior of the pendulum as its output. (b): After learning, the spiking network can simulate the system’s response to control signals. The panel displays the height of the real pendulum in the past (solid black line) and future heights under different exploratory controls (dashed lines). For the same controls, the spiking neural network predicts very similar future positions (colored lines) as the imitated system. It can therefore be used for mental exploration and computation of optimal control to reach an aim, here: to invert the pendulum. (c): During mental exploration, the network simulates in regular time intervals a set of possible future trajectories for different controls, starting from the actual state of the pendulum. From this, the optimal control until the next exploration can be computed and applied to the pendulum. The control reaches its aim: The pendulum is swung up and held in inverted position, despite a high level of noise added during testing (uncontrolled dynamics as in panel (a)).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004895.g005" xlink:type="simple"/>
</fig>
<p>During the subsequent control phase starting at <italic>t</italic> = 0, the aim is to swing the pendulum up and hold it in the inverted position (<xref ref-type="fig" rid="pcbi.1004895.g005">Fig 5c</xref>). For this, the PCSN simulates at time <italic>t</italic> a set of <italic>M</italic> future trajectories of the pendulum, for different random exploratory forces <italic>ξ</italic><sub><italic>i</italic></sub> (“mental exploration” with <italic>u</italic> = 0, cf. <xref ref-type="fig" rid="pcbi.1004895.g005">Fig 5a and 5b</xref>), starting with the current state of the pendulum. In a biological system, the initialization may be achieved through sensory input taking advantage of the fact that an appropriately initialized output enslaves the network through the feedback. Experiments indicate that explored trajectories are evaluated, by brain regions separate from the ones storing the world model [<xref ref-type="bibr" rid="pcbi.1004895.ref066">66</xref>–<xref ref-type="bibr" rid="pcbi.1004895.ref068">68</xref>]. We thus assign to the simulated trajectories a reward <italic>R</italic><sub><italic>i</italic></sub> measuring the agreement of the predicted states with the desired ones. The optimal control <italic>u</italic>(<italic>t</italic> + <italic>s</italic>) (cf. <xref ref-type="disp-formula" rid="pcbi.1004895.e054">Eq (16)</xref>) for a subsequent, not too large time interval <italic>s</italic> ∈ [0,<italic>δ</italic>] is then approximately given by a temporal average over the initial phase of the assumed random forces, weighted by the exponentiated total expected reward,
<disp-formula id="pcbi.1004895.e055"><alternatives><graphic id="pcbi.1004895.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e055" xlink:type="simple"/><mml:math display="block" id="M55"><mml:mrow><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(17)</label></disp-formula>
where <inline-formula id="pcbi.1004895.e056"><alternatives><graphic id="pcbi.1004895.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e056" xlink:type="simple"/><mml:math display="inline" id="M56"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>ξ</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>δ</mml:mi></mml:mfrac> <mml:mrow><mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mi>δ</mml:mi></mml:mrow></mml:msubsup> <mml:msub><mml:mi>ξ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>t</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>d</mml:mi> <mml:mover accent="true"><mml:mi>t</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> and λ<sub><italic>c</italic></sub> is a weighting factor. We have chosen <inline-formula id="pcbi.1004895.e057"><alternatives><graphic id="pcbi.1004895.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e057" xlink:type="simple"/><mml:math display="inline" id="M57"><mml:mrow><mml:msub><mml:mi>R</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mo>=</mml:mo> <mml:mrow><mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>T</mml:mi> <mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:msubsup> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>t</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>d</mml:mi> <mml:mover accent="true"><mml:mi>t</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>, i.e. the expected reward increases linearly with the heights <inline-formula id="pcbi.1004895.e058"><alternatives><graphic id="pcbi.1004895.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e058" xlink:type="simple"/><mml:math display="inline" id="M58"><mml:mrow><mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>t</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mo form="prefix">cos</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>ϕ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>t</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> predicted for the pendulum for input trajectory <italic>ξ</italic><sub><italic>i</italic></sub>; it becomes maximal for a trajectory at the inversion point. <italic>T</italic><sub><italic>r</italic></sub> is the duration of a simulated trajectory. The optimal control is applied to the pendulum until <italic>t</italic> + Δ, with Δ &lt; <italic>δ</italic>. Then, at <italic>t</italic> + Δ, the PCSN simulates a new set of trajectories starting with the pendulum’s updated state and a new optimal control is computed. This is valid and applied to the pendulum between <italic>t</italic> + Δ and <italic>t</italic> + 2Δ, and so on. We find that controlling the pendulum by this principle leads to the desired upswing and stabilization in the inversion point, even though we assume that the perturbing noise force <italic>ξ</italic> (<xref ref-type="disp-formula" rid="pcbi.1004895.e054">Eq (16)</xref>) acting on the pendulum in addition to the deterministic control <italic>u</italic>, remains as strong as it was during the exploration/learning phase (cf. <xref ref-type="fig" rid="pcbi.1004895.g005">Fig 5a and 5b</xref>).</p>
<p>We find that for controlling the pendulum, the learned internal model of the system has to be very accurate. This implies that particular realizations of the PCSN can be unsuited to learn the model (we observed this for about half of the realizations), a phenomenon that has also been reported for small continuous rate networks before. However, we checked that continuous rate networks as encoded by our spiking ones reliably learn the task. Since the encoding quality increases with the number of spiking neurons, we expect that sufficiently large PCSNs reliably learn the task as well.</p>
</sec>
</sec>
</sec>
<sec id="sec013" sec-type="conclusions">
<title>Discussion</title>
<p>The characteristic means of communication between neurons in the nervous system are spikes. It is widely accepted that sequences of spikes form the basis of neural computations in higher animals. How computations are performed and learned is, however, largely unclear. Here we have derived <italic>continuous signal coding spiking neural networks (CSNs),</italic> a class of mesoscopic spiking neural networks that are a suitable substrate for computation. Together with plasticity rules for their output or recurrent connections, they are able to learn general, complicated computations by imitation learning (plastic CSNs, <italic>PCSNs</italic>). Learning can be highly reliable and accurate already for comparably small networks of hundreds of neurons. The underlying principle is that the networks reflect the input in a complicated nonlinear way, generate nonlinear transformations of it and use fading memory such that the inputs and their pasts interfere with each other. This requires an overall nonlinear relaxation dynamics suitable for computations [<xref ref-type="bibr" rid="pcbi.1004895.ref002">2</xref>]. Such dynamics are different from standard spiking neural network dynamics, which are characterized by a high level of noise and short intrinsic memory [<xref ref-type="bibr" rid="pcbi.1004895.ref009">9</xref>–<xref ref-type="bibr" rid="pcbi.1004895.ref011">11</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref069">69</xref>].</p>
<p>To find spiking networks that generate appropriate dynamics, we use a linear decoding scheme for continuous signals encoded in the network dynamics as combinations of membrane potentials and synaptic currents. A specific coding scheme like this was introduced in refs. [<xref ref-type="bibr" rid="pcbi.1004895.ref012">12</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref037">37</xref>] to derive spiking networks encoding linear dynamics in an optimal way. We introduce spiking networks where the encoded signals have dynamics desirable for computation, i.e. a nonlinear, high-dimensional, low-noise, relaxational character as well as significant fading memory. We conclude that, since we use simple linear decoding, already the dynamics of the spiking networks must possess these properties.</p>
<p>Using this approach, we study two types of CSNs: Networks with saturating synapses and networks with nonlinear dendrites. The CSNs with saturating synapses use a direct signal encoding; each neuron codes for one continuous variable. It requires spiking dynamics characterized by possibly intermittent phases of high rate spiking, or bursting, with inter-spike-intervals smaller than the synaptic time constants, which leads to a temporal averaging over spikes. Dynamics that appear externally similar to such dynamics were recently highlighted as a ‘second type of balanced state’ in networks of pulse-coupled, intrinsically oscillating model neurons [<xref ref-type="bibr" rid="pcbi.1004895.ref051">51</xref>]. Very recently [<xref ref-type="bibr" rid="pcbi.1004895.ref070">70</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref071">71</xref>] showed that networks whose spiking dynamics are temporally averaged due to slow synapses possess a phase transition from a fixed point to chaotic dynamics in the firing rates, like the corresponding rate models that they directly encode. In the analytical computations the spike coding was not specified [<xref ref-type="bibr" rid="pcbi.1004895.ref070">70</xref>] or assumed to be Poissonian [<xref ref-type="bibr" rid="pcbi.1004895.ref071">71</xref>]. Numerical simulations of leaky integrate-and-fire neurons in the chaotic rate regime can generate intermittent phases of rather regular high-rate spiking [<xref ref-type="bibr" rid="pcbi.1004895.ref070">70</xref>]. The networks might provide a suitable substrate for learning computations as well. However, since the chaotic rate dynamics have correlations on the time scale of the slow synapses its applicability is limited to learning tasks where only a short fading memory of the reservoir is needed. For example delayed reaction tasks as illustrated in <xref ref-type="fig" rid="pcbi.1004895.g004">Fig 4a-4c</xref> would not be possible. Interestingly, in our scheme a standard leaky integrate-and-fire neuron with saturating synapses appears as a special case with recovery current of amplitude zero. According to our analysis it can act as a leaky integrator with a leak of the same time constant as the synapses, λ<sub><italic>x</italic></sub> = λ<sub><italic>s</italic></sub>. In contrast, in presence of a recovery current, our networks with saturating synapses can encode slower dynamics on the order of seconds. After training the network, the time scales can be further extended.</p>
<p>In the CSNs with nonlinear dendrites the entire neural population codes for a usually smaller number of continuous variables, avoiding high firing rates in sufficiently large networks. The networks generate irregular, low frequency spiking and simultaneously a noise-reduced encoding of nonlinear dynamics, the temporal averaging over spikes in the direct coding case is partially replaced by a spatial averaging over spike trains from many neurons. The population coding scheme and our derivations of CSNs with nonlinear dendrites generalize the predictive coding proposed in ref. [<xref ref-type="bibr" rid="pcbi.1004895.ref012">12</xref>] to nonlinear dynamics. The roles of our slow and fast connections are similar to those used there: In particular, redundancies in the spiking are eliminated by fast recurrent connections without synaptic filtering. We expect that these couplings can be replaced by fast connections that have small finite synaptic time constants, as shown for the networks of ref. [<xref ref-type="bibr" rid="pcbi.1004895.ref012">12</xref>] in ref. [<xref ref-type="bibr" rid="pcbi.1004895.ref072">72</xref>]. In contrast to previous work, in the CSNs with nonlinear dendrites we have linear and nonlinear slow couplings. The former contribute to coding precision and implement linear parts of the encoded dynamics, the latter implement the nonlinearities in the encoded dynamics. Further, in contrast to previous work, the spike coding networks provide only the substrate for learning of general dynamical systems by adapting their recurrent connections. Importantly, this implies (i) that the neurons do not have to adapt their nonlinearities to each nonlinear dynamical system that is to be learned (which would not seem biologically plausible) and (ii) that the CSNs do not have to provide a faithful approximation of the nonlinear dynamics Eqs (<xref ref-type="disp-formula" rid="pcbi.1004895.e023">6</xref>),(<xref ref-type="disp-formula" rid="pcbi.1004895.e037">11</xref>), since a rough dynamical character (i.e. slow dynamics and the echo state property) is sufficient for serving as substrates. We note that refs. [<xref ref-type="bibr" rid="pcbi.1004895.ref073">73</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref074">74</xref>] suggested to use the differential equations that characterize dynamical systems to engineer spiking neural networks that encode the dynamics. The approach suggests an alternative derivation of spiking networks that may be suitable as substrate for learning computations. Their rate coding scheme, however, allows for redundancy and thus higher noise levels, and it generates high frequency spiking. In a future publication, B. DePasquale, M. Churchland, and L.F. Abbott will present an approach to train rate coding spiking neural networks, with continuous rate networks providing the target signals [<xref ref-type="bibr" rid="pcbi.1004895.ref075">75</xref>]. We will discuss the relation between our and this approach in a joint review [<xref ref-type="bibr" rid="pcbi.1004895.ref076">76</xref>].</p>
<p>A characteristic feature of our neuron models is that they take into account nonlinearities in the synapses or in the dendrites. On the one hand this is biologically plausible [<xref ref-type="bibr" rid="pcbi.1004895.ref013">13</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref019">19</xref>–<xref ref-type="bibr" rid="pcbi.1004895.ref021">21</xref>], on the other hand it is important for generating nonlinear computations. Our nonlinearities are such that the decoded continuous dynamics match those for typical networks of continuous rate neurons and provide a simple model for dendritic and synaptic saturation. However, the precise form of the neuron model and its nonlinearity is not important for our approaches: As long as the encoded dynamical system is suitable as a computational reservoir, the spiking system is a CSN and our learning schemes will work. As an example, a dendritic tree with multiple interacting compartments may be directly implemented in both the networks with saturating synapses and in the networks with nonlinear dendrites. A future task is to explore the computational capabilities of CSNs incorporating different and biologically more detailed features that lead to nonlinearities, e.g. neural refractory periods, dendritic trees with calcium and NMDA voltage dependent channels and/or standard types of short term synaptic plasticity.</p>
<p>Inspired by animals’ needs to generate and predict continuous dynamics such as their own body and external world movements, we let our networks learn to approximate desired continuous dynamics. Since effector organs such as muscles and post-synaptic neurons react to weighted, possibly dendritically processed sums of post-synaptic currents, we interpret these sums as the relevant, continuous signal-approximating outputs of the network [<xref ref-type="bibr" rid="pcbi.1004895.ref039">39</xref>]. Importantly, this is not the same as Poissonian rate coding of a continuous signal: As a simple example, consider a single spiking neuron. In our scheme it will spike with constant inter-spike-intervals to encode a constant output. In Poissonian rate coding, the inter-spike-intervals will be random, exponentially distributed and many more spikes need to be sampled to decode the constant output (cf. Fig A in <xref ref-type="supplementary-material" rid="pcbi.1004895.s001">S1 Text</xref>).</p>
<p>The outputs and recurrent connections of CSNs can be learned by standard learning rules [<xref ref-type="bibr" rid="pcbi.1004895.ref004">4</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref041">41</xref>]. The weight changes depend on the product of the error and the synaptic or dendritic currents and may be interpreted as delta-rules with synapse- and time-dependent learning rates. PCSNs, with learning of recurrent weights or output feedback, show how spiking neural networks may learn internal models of complicated, self-sustained environmental dynamics. In our applications, we demonstrate that they can learn to generate and predict the dynamics in different depths, ranging from the learning of single stable patterns over the learning of chaotic dynamics to the learning of dynamics incorporating their reactions to external influences.</p>
<p>The spiking networks we use have medium size, like networks with continuous neurons used in the literature [<xref ref-type="bibr" rid="pcbi.1004895.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref004">4</xref>]. CSNs with saturating synapses have, by construction, the same size as their non-spiking counterparts. In CSNs with nonlinear dendrites the spike load necessary to encode the continuous signals is distributed over the entire network. This leads to a trade-off between lower spiking frequency per neuron and larger network size (cf. Fig F in <xref ref-type="supplementary-material" rid="pcbi.1004895.s001">S1 Text</xref>): The faster the neurons can spike the smaller the network may be to solve a given task.</p>
<p>Previous work using spiking neurons as a reservoir to generate a high dimensional, nonlinear projection of a signal for computation, concentrated on networks without output feedback or equivalent task-specific learning of recurrent connectivity [<xref ref-type="bibr" rid="pcbi.1004895.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref050">50</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref077">77</xref>]. Such networks are commonly called “liquid state machines” [<xref ref-type="bibr" rid="pcbi.1004895.ref078">78</xref>]. By construction, they are unable to solve tasks like the generation of self-sustained activity and persistent memorizing of instructions; these require an effective output feedback, since the current output determines the desired future one: To compute the latter, the former must be made available to the network as an input. The implementation of spiking reservoir computers with feedback was hindered by the high level of noise in the relevant signals: The computations depend on the spike rate, the spike trains provide a too noisy approximation of this average signal and the noise is amplified in the feedback loop. While analytically considering feedback in networks of continuous rate neurons, ref. [<xref ref-type="bibr" rid="pcbi.1004895.ref003">3</xref>] showed examples of input-output tasks solved by spiking networks with a feedback circuit, the output signals are affected by a high level of noise. This concerns even output signals just keeping a constant value. We implemented similar tasks (<xref ref-type="fig" rid="pcbi.1004895.g004">Fig 4d</xref>), and find that our networks solve them very accurately due to their more efficient coding and the resulting comparably high signal-to-noise ratio. In contrast to previous work, our derivations systematically delineate spiking networks which are suitable for the computational principle with feedback or recurrent learning; the networks can accurately learn universal, complicated memory dependent computations as well as dynamical systems approximation, in particular the generation of self-sustained dynamics.</p>
<p>In the control task, we show how a spiking neural network can learn an internal model of a dynamical system, which subsequently allows to control the system. We use a path integral approach, which has already previously been suggested as a theory for motor control in biological systems [<xref ref-type="bibr" rid="pcbi.1004895.ref079">79</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref080">80</xref>]. We apply it to learned world models, and to neural networks. Path integral control assumes that noise and control act in a similar way on the system [<xref ref-type="bibr" rid="pcbi.1004895.ref061">61</xref>]. This assumption is comparably weak and the path integral control method has been successfully applied in many robotics applications [<xref ref-type="bibr" rid="pcbi.1004895.ref081">81</xref>–<xref ref-type="bibr" rid="pcbi.1004895.ref083">83</xref>], where it was found to be superior to reinforcement learning and adaptive control methods.</p>
<p>Continuous rate networks using recurrence, readouts, and feedback or equivalent recurrent learning, are versatile, powerful devices for nonlinear computations. This has inspired their use in manifold applications in science and engineering, such as control, forecasting and pattern recognition [<xref ref-type="bibr" rid="pcbi.1004895.ref026">26</xref>]. Our study has demonstrated that it is possible to obtain similar performance using spiking neural networks. Therewith, our study makes spiking neural networks available for similarly diverse, complex computations and supports the feasibility of the considered computational principle as a principle for information processing in the brain.</p>
</sec>
<sec id="sec014" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec015">
<title>Network simulation</title>
<p>We use a time grid based simulation scheme (step size <italic>dt</italic>). If not mentioned otherwise, between time points, we compute the membrane potentials using a Runge-Kutta integration scheme for dynamics without noise and an Euler-Maruyama integration scheme for dynamics with noise. Since CSNs with nonlinear dendrites have fast connections without conduction delays and synaptic filtering, we process spikings at a time point as follows: We test whether the neuron with the highest membrane potential is above threshold. If the outcome is positive, the neuron is reset and the impact of the spike on postsynaptic neurons is evaluated. Thereafter, we compute the neuron with the highest, possibly updated, membrane potential and repeat the procedure. If all neurons have subthreshold membrane potential, we proceed to the next time point. The described consecutive updating of neurons in a single time step increases in networks with nonlinear dendrites the robustness of the simulations against larger time steps, as the neurons maintain an order of spiking and responding like in a simulation with smaller time steps and a small but finite conduction delay and/or slight filtering of fast inputs. As an example, the scheme avoids that neurons that code for similar features and thus possess fast mutual inhibition, spike together within one step and generate an overshoot in the readout, as it would be the case in a parallel membrane potential updating scheme. The different tasks use either networks with saturating synapses or networks with nonlinear dendrites. In both cases, <bold>A</bold> is a sparse matrix with a fraction <italic>p</italic> of non-zero values. These are drawn independently from a Gaussian distribution with zero mean and variance <inline-formula id="pcbi.1004895.e059"><alternatives><graphic id="pcbi.1004895.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e059" xlink:type="simple"/><mml:math display="inline" id="M59"><mml:mfrac><mml:msup><mml:mi>g</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mi>p</mml:mi> <mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula> (CSNs with saturating synapses) or <inline-formula id="pcbi.1004895.e060"><alternatives><graphic id="pcbi.1004895.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e060" xlink:type="simple"/><mml:math display="inline" id="M60"><mml:mfrac><mml:msup><mml:mi>g</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mrow><mml:mi>p</mml:mi> <mml:mi>J</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula> (CSNs with nonlinear dendrites), which sets the spectral radius of <bold>A</bold> approximately to <italic>g</italic>. For networks with nonlinear dendrites, the elements of <bold>Γ</bold> are drawn from a standard normal distribution. To keep the approach simple, we allow for positive and negative dendro-somatic couplings. In order to achieve a uniform distribution of spiking over the neurons in the network, we normalize the columns of <bold>Γ</bold> to have the same norm, which we control with the parameter <italic>γ</italic><sub><italic>s</italic></sub>. This implies that the thresholds are identical.</p>
</sec>
<sec id="sec016">
<title>Training phase</title>
<p>The networks are trained for a period of length <italic>T</italic><sub><italic>t</italic></sub> such that the readouts <italic>z</italic><sub><italic>k</italic></sub> imitate target signals <italic>F</italic><sub><italic>k</italic></sub>(<italic>t</italic>), i.e. such that the time average of the square of the errors <italic>e</italic><sub><italic>k</italic></sub>(<italic>t</italic>) = <italic>z</italic><sub><italic>k</italic></sub>(<italic>t</italic>) − <italic>F</italic><sub><italic>k</italic></sub>(<italic>t</italic>) is minimized. At <italic>T</italic><sub><italic>t</italic></sub>, training stops and the weights are not updated anymore in the subsequent testing. If present, the external input to the neurons is a weighted sum of <italic>K</italic><sub>in</sub> continuous input signals <italic>f</italic><sub><italic>k</italic></sub>(<italic>t</italic>), <inline-formula id="pcbi.1004895.e061"><alternatives><graphic id="pcbi.1004895.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e061" xlink:type="simple"/><mml:math display="inline" id="M61"><mml:mrow><mml:msub><mml:mi>I</mml:mi> <mml:mrow><mml:mtext>e,</mml:mtext> <mml:mi>β</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>K</mml:mi> <mml:mtext>in</mml:mtext></mml:msub></mml:msubsup> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>β</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>f</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, where the index <italic>β</italic> runs from 1 to <italic>N</italic> (CSNs with saturating synapses) or from 1 to <italic>J</italic> (CSNs with nonlinear dendrites). The weights <italic>w</italic><sub><italic>βk</italic></sub> are fixed and drawn from a uniform distribution in the range <inline-formula id="pcbi.1004895.e062"><alternatives><graphic id="pcbi.1004895.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e062" xlink:type="simple"/><mml:math display="inline" id="M62"><mml:mrow><mml:mo>[</mml:mo> <mml:mo>-</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msup> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. If present, the feedback weights <inline-formula id="pcbi.1004895.e063"><alternatives><graphic id="pcbi.1004895.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e063" xlink:type="simple"/><mml:math display="inline" id="M63"><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>β</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mi>f</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> (cf. <xref ref-type="disp-formula" rid="pcbi.1004895.e050">Eq (15)</xref>) are likewise chosen randomly from a uniform distribution in the range <inline-formula id="pcbi.1004895.e064"><alternatives><graphic id="pcbi.1004895.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e064" xlink:type="simple"/><mml:math display="inline" id="M64"><mml:mrow><mml:mo>[</mml:mo> <mml:mo>-</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>f</mml:mi></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>f</mml:mi></mml:msup> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> with a global feedback parameter <inline-formula id="pcbi.1004895.e065"><alternatives><graphic id="pcbi.1004895.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e065" xlink:type="simple"/><mml:math display="inline" id="M65"><mml:msup><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>f</mml:mi></mml:msup></mml:math></alternatives></inline-formula>.</p>
<p>For the delayed reaction/time estimation task (<xref ref-type="fig" rid="pcbi.1004895.g004">Fig 4a-4c</xref>, Fig E in <xref ref-type="supplementary-material" rid="pcbi.1004895.s001">S1 Text</xref>), we applied the RLS (recursive least squares) algorithm [<xref ref-type="bibr" rid="pcbi.1004895.ref041">41</xref>] to learn the linear outputs. For the pattern generation, instruction switching and control tasks, we applied the FORCE (first-order reduced and controlled error) algorithm [<xref ref-type="bibr" rid="pcbi.1004895.ref004">4</xref>] (Figs <xref ref-type="fig" rid="pcbi.1004895.g003">3</xref>, <xref ref-type="fig" rid="pcbi.1004895.g004">4d</xref> and <xref ref-type="fig" rid="pcbi.1004895.g005">5</xref>; Figs A-D, F and G in <xref ref-type="supplementary-material" rid="pcbi.1004895.s001">S1 Text</xref>) to learn the recurrent connections and linear outputs.</p>
</sec>
<sec id="sec017">
<title>Learning rules</title>
<p>The output weights <inline-formula id="pcbi.1004895.e066"><alternatives><graphic id="pcbi.1004895.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e066" xlink:type="simple"/><mml:math display="inline" id="M66"><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>m</mml:mi></mml:mrow> <mml:mi>o</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> are trained using the standard recursive least squares method [<xref ref-type="bibr" rid="pcbi.1004895.ref041">41</xref>]. They are initialized with 0, we use weight update intervals of Δ<italic>t</italic>. The weight update uses the current training error <italic>e</italic><sub><italic>k</italic></sub>(<italic>t</italic>) = <italic>z</italic><sub><italic>k</italic></sub>(<italic>t</italic>) − <italic>F</italic><sub><italic>k</italic></sub>(<italic>t</italic>), where <italic>z</italic><sub><italic>k</italic></sub>(<italic>t</italic>) is the output that should imitate the target signal <italic>F</italic><sub><italic>k</italic></sub>(<italic>t</italic>), it further uses an estimate <italic>P</italic><sub><italic>βρ</italic></sub>(<italic>t</italic>) of the inverse correlation matrix of the unweighted neural synaptic or dendritic inputs <inline-formula id="pcbi.1004895.e067"><alternatives><graphic id="pcbi.1004895.e067g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e067" xlink:type="simple"/><mml:math display="inline" id="M67"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>β</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, as well as these inputs,
<disp-formula id="pcbi.1004895.e068"><alternatives><graphic id="pcbi.1004895.e068g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e068" xlink:type="simple"/><mml:math display="block" id="M68"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>β</mml:mi></mml:mrow> <mml:mi>o</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>β</mml:mi></mml:mrow> <mml:mi>o</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi>e</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>ρ</mml:mi></mml:munder> <mml:msub><mml:mi>P</mml:mi> <mml:mrow><mml:mi>β</mml:mi> <mml:mi>ρ</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>ρ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(18)</label></disp-formula></p>
<p>The indices <italic>β</italic>,<italic>ρ</italic> range over all saturating synapses (<italic>β</italic>,<italic>ρ</italic> = 1,…,<italic>N</italic>; <inline-formula id="pcbi.1004895.e069"><alternatives><graphic id="pcbi.1004895.e069g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e069" xlink:type="simple"/><mml:math display="inline" id="M69"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>β</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo form="prefix">tanh</mml:mo> <mml:mo>(</mml:mo> <mml:mi>γ</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mi>β</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>) or all non-linear dendrites (<italic>β</italic>,<italic>ρ</italic> = 1,…,<italic>J</italic>; <inline-formula id="pcbi.1004895.e070"><alternatives><graphic id="pcbi.1004895.e070g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e070" xlink:type="simple"/><mml:math display="inline" id="M70"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>β</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo form="prefix">tanh</mml:mo> <mml:mo>(</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mo>Γ</mml:mo> <mml:mrow><mml:mi>β</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>r</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>) of the output neuron. The square matrix <italic>P</italic> is a running filter estimate of the inverse correlation matrix of the activity of the saturated synapses (CSNs with saturating synapses) or non-linear dendrites (CSNs with nonlinear dendrites). The matrix is updated via
<disp-formula id="pcbi.1004895.e071"><alternatives><graphic id="pcbi.1004895.e071g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e071" xlink:type="simple"/><mml:math display="block" id="M71"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mrow><mml:mi>β</mml:mi> <mml:mi>γ</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>P</mml:mi> <mml:mrow><mml:mi>β</mml:mi> <mml:mi>γ</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mo>∑</mml:mo> <mml:mi>ρ</mml:mi></mml:msub> <mml:msub><mml:mo>∑</mml:mo> <mml:mi>σ</mml:mi></mml:msub> <mml:msub><mml:mi>P</mml:mi> <mml:mrow><mml:mi>β</mml:mi> <mml:mi>ρ</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>ρ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>σ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>P</mml:mi> <mml:mrow><mml:mi>σ</mml:mi> <mml:mi>γ</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msub><mml:mo>∑</mml:mo> <mml:mi>ρ</mml:mi></mml:msub> <mml:msub><mml:mo>∑</mml:mo> <mml:mi>σ</mml:mi></mml:msub> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>ρ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>P</mml:mi> <mml:mrow><mml:mi>ρ</mml:mi> <mml:mi>σ</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>σ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(19)</label></disp-formula>
where the indices <italic>β</italic>,<italic>γ</italic>,<italic>ρ</italic>,<italic>σ</italic> run from 1 to <italic>N</italic> (CSNs with saturating synapses) or from 1 to <italic>J</italic> (CSNs with nonlinear dendrites). <bold>P</bold> is initialized as <bold>P</bold>(0) = <italic>α</italic><sup>−1</sup> <bold>1</bold> with <italic>α</italic><sup>−1</sup> acting as a learning rate.</p>
<p>For the update of output weights in presence of feedback and of recurrent weights we adopt the FORCE algorithm [<xref ref-type="bibr" rid="pcbi.1004895.ref004">4</xref>]. In presence of feedback, this means that recursive least squares learning of output is fast against the temporal evolution of the network, and already during training the output is fed back into the network. Thus, each neuron gets a feedback input
<disp-formula id="pcbi.1004895.e072"><alternatives><graphic id="pcbi.1004895.e072g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e072" xlink:type="simple"/><mml:math display="block" id="M72"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi> <mml:mrow><mml:mtext>e</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>β</mml:mi></mml:mrow> <mml:mi>f</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>K</mml:mi> <mml:mtext>out</mml:mtext></mml:msub></mml:munderover> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>β</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mi>f</mml:mi></mml:msubsup> <mml:msub><mml:mi>z</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>K</mml:mi> <mml:mtext>out</mml:mtext></mml:msub></mml:munderover> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>β</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mi>f</mml:mi></mml:msubsup> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>ρ</mml:mi></mml:munder> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>ρ</mml:mi></mml:mrow> <mml:mi>o</mml:mi></mml:msubsup> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>ρ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(20)</label></disp-formula>
The feedback weights <inline-formula id="pcbi.1004895.e073"><alternatives><graphic id="pcbi.1004895.e073g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e073" xlink:type="simple"/><mml:math display="inline" id="M73"><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>β</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mi>f</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> are static, the output weights are learned according to <xref ref-type="disp-formula" rid="pcbi.1004895.e068">Eq (18)</xref>.</p>
<p>Since the outputs are linear combinations of synaptic or dendritic currents, which also the neurons within the network linearly combine, the feedback loop can be implemented by modifying the recurrent connectivity, by adding a term <inline-formula id="pcbi.1004895.e074"><alternatives><graphic id="pcbi.1004895.e074g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e074" xlink:type="simple"/><mml:math display="inline" id="M74"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>K</mml:mi> <mml:mtext>out</mml:mtext></mml:msub></mml:msubsup> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>ρ</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mi>f</mml:mi></mml:msubsup> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>β</mml:mi></mml:mrow> <mml:mi>o</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> to the matrix <italic>A</italic><sub><italic>ρβ</italic></sub>. Learning then affects the output weights as well as the recurrent connections, separate feedback connections are not present. This learning and learning of output weights with a feedback loop are just two different interpretations of the same learning rule. For networks with saturating synapses the update is
<disp-formula id="pcbi.1004895.e075"><alternatives><graphic id="pcbi.1004895.e075g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e075" xlink:type="simple"/><mml:math display="block" id="M75"><mml:mrow><mml:msub><mml:mi>A</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>A</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>K</mml:mi> <mml:mtext>out</mml:mtext></mml:msub></mml:munderover> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mi>f</mml:mi></mml:msubsup> <mml:msub><mml:mi>e</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>P</mml:mi> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(21)</label></disp-formula>
where the <inline-formula id="pcbi.1004895.e076"><alternatives><graphic id="pcbi.1004895.e076g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e076" xlink:type="simple"/><mml:math display="inline" id="M76"><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mi>f</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> are now acting as learning rates. For networks with nonlinear dendrites, the update is
<disp-formula id="pcbi.1004895.e077"><alternatives><graphic id="pcbi.1004895.e077g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e077" xlink:type="simple"/><mml:math display="block" id="M77"><mml:mrow><mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>D</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>J</mml:mi></mml:munderover> <mml:msub><mml:mo>Γ</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msub> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:msub><mml:mi>K</mml:mi> <mml:mtext>out</mml:mtext></mml:msub></mml:munderover> <mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mi>f</mml:mi></mml:msubsup> <mml:msub><mml:mi>e</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>h</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>J</mml:mi></mml:munderover> <mml:msub><mml:mi>P</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mi>h</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mover accent="true"><mml:mi>r</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>h</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(22)</label></disp-formula></p>
</sec>
<sec id="sec018">
<title>Control task</title>
<p>The task is achieved in two phases, the learning and the control phase.</p>
<p>1. Learning: The PCSN learns a world model of the noisy pendulum, i.e. it learns the dynamical system and how it reacts to input. The pendulum follows the differential <xref ref-type="disp-formula" rid="pcbi.1004895.e054">Eq (16)</xref> with <italic>cω</italic><sub>0</sub> = 0.1s<sup>−1</sup> and <inline-formula id="pcbi.1004895.e078"><alternatives><graphic id="pcbi.1004895.e078g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e078" xlink:type="simple"/><mml:math display="inline" id="M78"><mml:mrow><mml:msubsup><mml:mi>ω</mml:mi> <mml:mrow><mml:mn>0</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>=</mml:mo> <mml:mn>10</mml:mn> <mml:msup><mml:mtext>s</mml:mtext> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, <italic>ξ</italic>(<italic>t</italic>) is a white noise force with 〈<italic>ξ</italic>(<italic>t</italic>)<italic>ξ</italic>(<italic>t</italic><sup>′</sup>)〉 = s<sup>−3</sup> <italic>δ</italic>(<italic>t</italic> − <italic>t</italic><sup>′</sup>), <italic>x</italic>(<italic>t</italic>) = sin(<italic>ϕ</italic>(<italic>t</italic>)) and <italic>y</italic>(<italic>t</italic>) = −cos(<italic>ϕ</italic>(<italic>t</italic>)) are Cartesian coordinates of the point mass. The neural network has one input and three outputs which are fed back into the network; it learns to output the <italic>x</italic>- and the <italic>y</italic>-coordinate, as well as the angular velocity of the pendulum when it receives as input the strength of the angular force (noise plus control) <italic>ξ</italic>(<italic>t</italic>) + <italic>u</italic>(<italic>t</italic>) applied to the pivot axis of the pendulum. The learning is here interpreted as learning in a network with feedback, cf. <xref ref-type="disp-formula" rid="pcbi.1004895.e072">Eq (20)</xref>.</p>
<p>We created a training trajectory of length <italic>T</italic><sub><italic>t</italic></sub> = 1000s by simulating the pendulum with the given parameters and by driving it with white noise <italic>ξ</italic>(<italic>t</italic>) as an exploratory control (<italic>u</italic>(<italic>t</italic>) = 0). Through its input, the PCSN receives the same white noise realization <italic>ξ</italic>(<italic>t</italic>). During training the PCSN learns to imitate the reaction of the pendulum to this control, more precisely its outputs learn to approximate the trajectories of <italic>x</italic>, <italic>y</italic> and <italic>ω</italic>. As feedback to the reservoir during training we choose a convex combination of the reservoir output and the target (<inline-formula id="pcbi.1004895.e079"><alternatives><graphic id="pcbi.1004895.e079g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e079" xlink:type="simple"/><mml:math display="inline" id="M79"><mml:mrow><mml:mtext>feedback</mml:mtext> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>9</mml:mn> <mml:mo>·</mml:mo> <mml:mtext>output</mml:mtext> <mml:mo>+</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>1</mml:mn> <mml:mo>·</mml:mo> <mml:mtext>target</mml:mtext></mml:mrow></mml:math></alternatives></inline-formula>). We find that such a combination improves performance: If the output at the beginning of the training is very erroneous, those errors are accumulated through the feedback-loop, which prevents the algorithm from working. On the other hand, if one feeds back only the target signal, the algorithm does not learn how to correct for feedback transmitted readout errors. In our task, the convex combination alleviates both problems.</p>
<p>2. Control: In the second phase, the learned world model of the pendulum is used to compute stochastic optimal control that swings the pendulum up and keeps it in the inverted position. The PCSN does not learn its weights in this phase anymore. It receives the different realizations of exploratory (white noise) control and predicts the resulting motion (“mental exploration”). From this, the optimal control may be computed using the path integral framework [<xref ref-type="bibr" rid="pcbi.1004895.ref061">61</xref>]. In this framework a stochastic dynamical system (which is possibly multivariate)
<disp-formula id="pcbi.1004895.e080"><alternatives><graphic id="pcbi.1004895.e080g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e080" xlink:type="simple"/><mml:math display="block" id="M80"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold">f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives> <label>(23)</label></disp-formula>
with arbitrary nonlinearity <bold>f</bold>(<bold>x</bold>(<italic>t</italic>)) and white noise <bold>ξ</bold>(<italic>t</italic>), is controlled by the feedback controller <bold>u</bold>(<bold>x</bold>(<italic>t</italic>),<italic>t</italic>) to optimize an integral <italic>C</italic>(<italic>t</italic>) over a state cost <inline-formula id="pcbi.1004895.e081"><alternatives><graphic id="pcbi.1004895.e081g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e081" xlink:type="simple"/><mml:math display="inline" id="M81"><mml:mrow><mml:mi>U</mml:mi> <mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>t</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and a moving horizon quadratic control cost, <inline-formula id="pcbi.1004895.e082"><alternatives><graphic id="pcbi.1004895.e082g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e082" xlink:type="simple"/><mml:math display="inline" id="M82"><mml:mrow><mml:mi>C</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mo>=</mml:mo> <mml:mrow><mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>T</mml:mi> <mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:msubsup> <mml:mi>U</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>t</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">u</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>t</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mi>d</mml:mi> <mml:mover accent="true"><mml:mi>t</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>. The reward is related to the cost by <italic>R</italic> = −<italic>C</italic>. Path integral control theory shows that the control at time <italic>t</italic> can be computed by generating samples from the dynamical system under the uncontrolled dynamics
<disp-formula id="pcbi.1004895.e083"><alternatives><graphic id="pcbi.1004895.e083g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e083" xlink:type="simple"/><mml:math display="block" id="M83"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold">f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold-italic">ξ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(24)</label></disp-formula>
The control is then given by the success weighted average of the noise realizations <italic>ξ</italic><sub><italic>i</italic></sub>
<disp-formula id="pcbi.1004895.e084"><alternatives><graphic id="pcbi.1004895.e084g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e084" xlink:type="simple"/><mml:math display="block" id="M84"><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>lim</mml:mtext></mml:mrow><mml:mrow><mml:mi>δ</mml:mi><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mtext>lim</mml:mtext></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:munder><mml:munderover><mml:mstyle displaystyle="true" mathsize="140%"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mfrac><mml:mn>1</mml:mn><mml:mi>δ</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:mrow><mml:munderover><mml:mo>∫</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>δ</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>t</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mover><mml:mi>t</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></alternatives> <label>(25)</label></disp-formula>
where <inline-formula id="pcbi.1004895.e085"><alternatives><graphic id="pcbi.1004895.e085g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e085" xlink:type="simple"/><mml:math display="inline" id="M85"><mml:mrow><mml:msub><mml:mi>C</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mo>=</mml:mo> <mml:mrow><mml:mi>t</mml:mi></mml:mrow> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>T</mml:mi> <mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:msubsup> <mml:mi>U</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>t</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>d</mml:mi> <mml:mover accent="true"><mml:mi>t</mml:mi> <mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> is the cost observed in the <italic>i</italic>th realization of the uncontrolled dynamics, which is driven by noise realization <italic>ξ</italic><sub><italic>i</italic></sub> and <italic>u</italic> = 0. <xref ref-type="disp-formula" rid="pcbi.1004895.e055">Eq (17)</xref> is a discrete approximation to <xref ref-type="disp-formula" rid="pcbi.1004895.e084">Eq (25)</xref>. In our task, <xref ref-type="disp-formula" rid="pcbi.1004895.e083">Eq (24)</xref> becomes
<disp-formula id="pcbi.1004895.e086"><alternatives><graphic id="pcbi.1004895.e086g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e086" xlink:type="simple"/><mml:math display="block" id="M86"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mover accent="true"><mml:mi>ϕ</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>ω</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mover accent="true"><mml:mi>ω</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>ω</mml:mi> <mml:mrow><mml:mn>0</mml:mn></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo form="prefix">sin</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ϕ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>c</mml:mi> <mml:msub><mml:mi>ω</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mi>ω</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>ξ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mi>u</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
and <italic>U</italic>(<bold>x</bold>(<italic>t</italic>)) = −<italic>y</italic>(<italic>t</italic>) = cos(<italic>ϕ</italic>(<italic>t</italic>)).</p>
</sec>
<sec id="sec019">
<title>Figure details</title>
<p>The parameters of the different simulations are given in <xref ref-type="table" rid="pcbi.1004895.t002">Table 2</xref> for simulations using saturating synapses and in <xref ref-type="table" rid="pcbi.1004895.t003">Table 3</xref> for simulations using nonlinear dendrites. Further parameters and details about the figures and simulations are given in the following paragraphs.</p>
<table-wrap id="pcbi.1004895.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004895.t002</object-id>
<label>Table 2</label>
<caption>
<title>Parameters used in the different figures for simulations of networks with saturating synapses.</title>
</caption>
<alternatives>
<graphic id="pcbi.1004895.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004895.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Sat. syn.</th>
<th align="center">N</th>
<th align="center"><italic>α</italic></th>
<th align="center">dt</th>
<th align="center"><italic>T</italic><sub><italic>t</italic></sub></th>
<th align="center">
<inline-formula id="pcbi.1004895.e087">
<alternatives>
<graphic id="pcbi.1004895.e087g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e087" xlink:type="simple"/>
<mml:math display="inline" id="M87">
<mml:msubsup>
<mml:mo>λ</mml:mo>
<mml:mrow>
<mml:mi>s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>-</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msubsup>
</mml:math>
</alternatives>
</inline-formula>
</th>
<th align="center">
<inline-formula id="pcbi.1004895.e088">
<alternatives>
<graphic id="pcbi.1004895.e088g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e088" xlink:type="simple"/>
<mml:math display="inline" id="M88">
<mml:msubsup>
<mml:mo>λ</mml:mo>
<mml:mrow>
<mml:mi>V</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>-</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msubsup>
</mml:math>
</alternatives>
</inline-formula>
</th>
<th align="center">V<sub><italic>r</italic></sub></th>
<th align="center"><italic>θ</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><xref ref-type="fig" rid="pcbi.1004895.g003">Fig 3d</xref></td>
<td align="center">50</td>
<td align="char" char=".">0.1</td>
<td align="center">0.1ms</td>
<td align="center">100s</td>
<td align="center">100ms</td>
<td align="center">100ms</td>
<td align="center">0.9<italic>θ</italic></td>
<td align="char" char=".">0.03</td>
</tr>
<tr>
<td align="left"><xref ref-type="fig" rid="pcbi.1004895.g003">Fig 3e</xref></td>
<td align="center">50</td>
<td align="char" char=".">0.1</td>
<td align="center">1ms</td>
<td align="center">100s</td>
<td align="center">100ms</td>
<td align="center">100ms</td>
<td align="center">0.9<italic>θ</italic></td>
<td align="char" char=".">0.03</td>
</tr>
<tr>
<td align="left"><xref ref-type="fig" rid="pcbi.1004895.g004">Fig 4a-4c</xref></td>
<td align="center">200</td>
<td align="char" char=".">0.1</td>
<td align="center">1ms</td>
<td align="center">800s</td>
<td align="center">100ms</td>
<td align="center">50ms</td>
<td align="center">0.54<italic>θ</italic></td>
<td align="char" char=".">0.1</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="pcbi.1004895.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004895.t003</object-id>
<label>Table 3</label>
<caption>
<title>Parameters used in the different figures for simulations of networks with nonlinear dendrites.</title>
<p>The parameter <italic>a</italic> = λ<sub><italic>s</italic></sub> − λ<sub><italic>x</italic></sub> is given in terms of λ<sub><italic>s</italic></sub> and λ<sub><italic>x</italic></sub>.</p>
</caption>
<alternatives>
<graphic id="pcbi.1004895.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004895.t003" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Nonlin. dendr.</th>
<th align="center">N</th>
<th align="center">J</th>
<th align="center"><italic>α</italic></th>
<th align="center"><italic>γ</italic><sub><italic>s</italic></sub></th>
<th align="center">dt</th>
<th align="center"><italic>T</italic><sub><italic>t</italic></sub></th>
<th align="center"><italic>μ</italic></th>
<th align="center">
<inline-formula id="pcbi.1004895.e089">
<alternatives>
<graphic id="pcbi.1004895.e089g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e089" xlink:type="simple"/>
<mml:math display="inline" id="M89">
<mml:msubsup>
<mml:mo>λ</mml:mo>
<mml:mrow>
<mml:mi>s</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>-</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msubsup>
</mml:math>
</alternatives>
</inline-formula>
</th>
<th align="center">
<inline-formula id="pcbi.1004895.e090">
<alternatives>
<graphic id="pcbi.1004895.e090g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e090" xlink:type="simple"/>
<mml:math display="inline" id="M90">
<mml:msubsup>
<mml:mo>λ</mml:mo>
<mml:mrow>
<mml:mi>V</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>-</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msubsup>
</mml:math>
</alternatives>
</inline-formula>
</th>
<th align="center">a</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><xref ref-type="fig" rid="pcbi.1004895.g003">Fig 3b,3c</xref></td>
<td align="center">500</td>
<td align="center">50</td>
<td align="center">0.1</td>
<td align="char" char=".">0.03</td>
<td align="center">1ms</td>
<td align="center">100s</td>
<td align="center">0</td>
<td align="center">100ms</td>
<td align="center">100ms</td>
<td align="center">λ<sub><italic>s</italic></sub> − 1s</td>
</tr>
<tr>
<td align="left"><xref ref-type="fig" rid="pcbi.1004895.g003">Fig 3e</xref></td>
<td align="center">500</td>
<td align="center">50</td>
<td align="center">0.1</td>
<td align="char" char=".">0.03</td>
<td align="center">1ms</td>
<td align="center">100s</td>
<td align="center">0</td>
<td align="center">100ms</td>
<td align="center">100ms</td>
<td align="center">λ<sub><italic>s</italic></sub> − 1s</td>
</tr>
<tr>
<td align="left"><xref ref-type="fig" rid="pcbi.1004895.g003">Fig 3f-3h</xref></td>
<td align="center">1600</td>
<td align="center">800</td>
<td align="center">0.1</td>
<td align="char" char=".">0.03</td>
<td align="center">1ms</td>
<td align="center">200s</td>
<td align="center">0</td>
<td align="center">100ms</td>
<td align="center">100ms</td>
<td align="center">λ<sub><italic>s</italic></sub> − 1s</td>
</tr>
<tr>
<td align="left"><xref ref-type="fig" rid="pcbi.1004895.g004">Fig 4d</xref></td>
<td align="center">300</td>
<td align="center">300</td>
<td align="center">50</td>
<td align="char" char=".">0.5</td>
<td align="center">10ms</td>
<td align="center">1000s</td>
<td align="center">0</td>
<td align="center">1s</td>
<td align="center">0.5s</td>
<td align="center">λ<sub><italic>s</italic></sub> − 0.02s</td>
</tr>
<tr>
<td align="left"><xref ref-type="fig" rid="pcbi.1004895.g005">Fig 5</xref></td>
<td align="center">500</td>
<td align="center">300</td>
<td align="center">0.1</td>
<td align="char" char=".">0.03</td>
<td align="center">1ms</td>
<td align="center">1000s</td>
<td align="center">20/<italic>N</italic><sup>2</sup></td>
<td align="center">100ms</td>
<td align="center">50ms</td>
<td align="center">λ<sub><italic>s</italic></sub> − 10s</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>If not mentioned otherwise, for all simulations we use <inline-formula id="pcbi.1004895.e091"><alternatives><graphic id="pcbi.1004895.e091g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e091" xlink:type="simple"/><mml:math display="inline" id="M91"><mml:mrow><mml:mi>g</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>.</mml:mo> <mml:mn>5</mml:mn> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>, <italic>p</italic> = 0.1, <inline-formula id="pcbi.1004895.e092"><alternatives><graphic id="pcbi.1004895.e092g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e092" xlink:type="simple"/><mml:math display="inline" id="M92"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>f</mml:mi></mml:msup> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1004895.e093"><alternatives><graphic id="pcbi.1004895.e093g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e093" xlink:type="simple"/><mml:math display="inline" id="M93"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msup> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>, Δ<italic>t</italic> = 0.01s, <italic>γ</italic> = <italic>θ</italic> and <inline-formula id="pcbi.1004895.e094"><alternatives><graphic id="pcbi.1004895.e094g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e094" xlink:type="simple"/><mml:math display="inline" id="M94"><mml:mrow><mml:msub><mml:mi>σ</mml:mi> <mml:mi>η</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mfrac><mml:mn>1</mml:mn> <mml:msqrt><mml:mtext>s</mml:mtext></mml:msqrt></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>. We note that for simulations with saturating synapses, we model the slow synaptic currents to possess synaptic time constants of 100ms (cf., e.g., [<xref ref-type="bibr" rid="pcbi.1004895.ref050">50</xref>, <xref ref-type="bibr" rid="pcbi.1004895.ref060">60</xref>]). We usually use the same value for the slow synapses in networks with nonlinear dendrites. Upon rescaling time, these networks can be interpreted as networks with faster time constants, which learn faster target dynamics. Since the spike rates scale likewise, we have to consider larger networks to generate rates in the biologically plausible range (cf. Fig F in <xref ref-type="supplementary-material" rid="pcbi.1004895.s001">S1 Text</xref>).</p>
<sec id="sec020">
<title>Figure 3</title>
<p>Figure 3b, 3c: The PCSN has non-linear dendrites. The target signal is a sine with period 4<italic>π</italic>s and amplitude 2 (normalized to one in the figure). During recall, the neurons of the PCSN spike with mean rate 30.2Hz.</p>
<p>Figure 3d: The PCSN has saturating synapses. The target signal is a saw tooth pattern with period 2s and amplitude 10 (normalized to one in the figure). We used an Euler scheme here. The mean spike rate is 226Hz.</p>
<p>Figure 3e: The task is performed by a PCSN with non-linear dendrites and by a PCSN with saturating synapses. The target signal is <inline-formula id="pcbi.1004895.e095"><alternatives><graphic id="pcbi.1004895.e095g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e095" xlink:type="simple"/><mml:math display="inline" id="M95"><mml:mrow><mml:mo form="prefix">sin</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mfrac><mml:mrow><mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>5</mml:mn></mml:mrow> <mml:mtext>s</mml:mtext></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mo form="prefix">cos</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>s</mml:mtext></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. The mean spike rate is 77.8Hz for saturating synapses and 21.3Hz for non-linear dendrites.</p>
<p>Figure 3f-3h: The PCSN has nonlinear dendrites. As teacher we use the standard Lorenz system
<disp-formula id="pcbi.1004895.e096"><alternatives><graphic id="pcbi.1004895.e096g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e096" xlink:type="simple"/><mml:math display="block" id="M96"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>σ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>ρ</mml:mi> <mml:mo>-</mml:mo> <mml:mi>z</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>y</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mover accent="true"><mml:mi>z</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>y</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>β</mml:mi> <mml:mi>z</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
with parameters <italic>σ</italic> = 10, <italic>ρ</italic> = 28, <italic>β</italic> = 8/3; we set the dimensionless temporal unit to 0.2s and scale the dynamical variables by a factor of 0.1. Panels (f,g) show a recall phase of 400s, panel (h) shows points from a simulation of 4000s. Panel (f) only shows every 10th data point, panel (g) shows every data point. The mean spike rate is 432Hz.</p>
</sec>
<sec id="sec021">
<title>Figure 4</title>
<p>Figure 4a-4c: We quantified the memory capacity of a CSN with saturating synapses. The network has a sparse connectivity matrix <bold>A</bold> without autapses. We applied white noise with <inline-formula id="pcbi.1004895.e097"><alternatives><graphic id="pcbi.1004895.e097g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e097" xlink:type="simple"/><mml:math display="inline" id="M97"><mml:mrow><mml:msub><mml:mi>σ</mml:mi> <mml:mi>η</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>001</mml:mn> <mml:mfrac><mml:mn>1</mml:mn> <mml:msqrt><mml:mtext>s</mml:mtext></mml:msqrt></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>. The input is a Gaussian bell curve with <italic>σ</italic> = 0.2s and integral 10s (height normalized to one in the figure). The target is a Gaussian bell curve with <italic>σ</italic> = 1s and integral 1s (height normalized to one in the figure). The target is presented several seconds after the input. Trials consisting of inputs and subsequent desired outputs are generated at random times with exponential inter-trial-interval distribution with time constant 10s and a refractory time of 100s. Training time is <italic>T</italic><sub><italic>t</italic></sub> = 800s, i.e. the network is trained with about 6 to 8 trials. Testing has the same duration with a similar number of trials. There is no feedback introduced by initialization or by learning, so the memory effect is purely inherent to the random network. We compute the quality of the desired output generation as the root mean squared (RMS) error between the generated and the desired response, normalized by the number of test trials. As reference, we set the error of the “extinguished” network, which does not generate any reaction to the input, to 1. Lower panels of <xref ref-type="fig" rid="pcbi.1004895.g004">Fig 4a-4c</xref> display medians and quartiles taken over 50 task repetitions. The sweep was done for time-delays 2 − 20s in steps of 0.5 s.</p>
<p>Figure 4d: The PCSN has nonlinear dendrites. For this task a constant input of <inline-formula id="pcbi.1004895.e098"><alternatives><graphic id="pcbi.1004895.e098g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e098" xlink:type="simple"/><mml:math display="inline" id="M98"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">I</mml:mi> <mml:mrow><mml:mi mathvariant="bold">e</mml:mi></mml:mrow> <mml:mtext mathvariant="bold">const</mml:mtext></mml:msubsup> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold">b</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> is added to the network with the elements of the vector <bold>b</bold> chosen uniformly from <inline-formula id="pcbi.1004895.e099"><alternatives><graphic id="pcbi.1004895.e099g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e099" xlink:type="simple"/><mml:math display="inline" id="M99"><mml:mrow><mml:mo>[</mml:mo> <mml:mn>0</mml:mn> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>s</mml:mtext></mml:mfrac> <mml:mo>,</mml:mo> <mml:mn>250</mml:mn> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>s</mml:mtext></mml:mfrac> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> to introduce inhomogeneity. Four different inputs are fed into the network, two continuous <inline-formula id="pcbi.1004895.e100"><alternatives><graphic id="pcbi.1004895.e100g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e100" xlink:type="simple"/><mml:math display="inline" id="M100"><mml:msubsup><mml:mi>f</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow> <mml:mi>c</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> and two pulsed input channels <inline-formula id="pcbi.1004895.e101"><alternatives><graphic id="pcbi.1004895.e101g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e101" xlink:type="simple"/><mml:math display="inline" id="M101"><mml:msubsup><mml:mi>f</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow> <mml:mi>p</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>. The continuous inputs are created by convolving white noise twice with an exponential kernel <inline-formula id="pcbi.1004895.e102"><alternatives><graphic id="pcbi.1004895.e102g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e102" xlink:type="simple"/><mml:math display="inline" id="M102"><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>t</mml:mi> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> (equivalent to convolving once with an alpha function) during training and <inline-formula id="pcbi.1004895.e103"><alternatives><graphic id="pcbi.1004895.e103g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e103" xlink:type="simple"/><mml:math display="inline" id="M103"><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>t</mml:mi> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>10</mml:mn> <mml:mtext>s</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> during testing. The continuous input signals are normalized to have mean 0 and standard deviation 0.5. The pulsed instruction input is created by the convolution of a Poisson spike train with an exponential kernel <inline-formula id="pcbi.1004895.e104"><alternatives><graphic id="pcbi.1004895.e104g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e104" xlink:type="simple"/><mml:math display="inline" id="M104"><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>t</mml:mi> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>. The rate of the delta pulses during training is <inline-formula id="pcbi.1004895.e105"><alternatives><graphic id="pcbi.1004895.e105g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e105" xlink:type="simple"/><mml:math display="inline" id="M105"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>04</mml:mn> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>. During testing we choose a slower rate of <inline-formula id="pcbi.1004895.e106"><alternatives><graphic id="pcbi.1004895.e106g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e106" xlink:type="simple"/><mml:math display="inline" id="M106"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>01</mml:mn> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> for a clearer presentation. In the rare case when two pulses overlap such that the pulsed signal exceeds an absolute value of 1.01 times the maximal pulse height of one, we shift the pulse by the minimal required amount of time to achieve a sum of the pulses below or equal to 1.01. We use weights <inline-formula id="pcbi.1004895.e107"><alternatives><graphic id="pcbi.1004895.e107g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e107" xlink:type="simple"/><mml:math display="inline" id="M107"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>p</mml:mi></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mn>100</mml:mn> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> for the pulsed inputs, <inline-formula id="pcbi.1004895.e108"><alternatives><graphic id="pcbi.1004895.e108g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e108" xlink:type="simple"/><mml:math display="inline" id="M108"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>c</mml:mi></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mn>250</mml:mn> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> for the continuous inputs and <inline-formula id="pcbi.1004895.e109"><alternatives><graphic id="pcbi.1004895.e109g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e109" xlink:type="simple"/><mml:math display="inline" id="M109"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>f</mml:mi></mml:msup> <mml:mo>=</mml:mo> <mml:mn>250</mml:mn> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> for the feedback; <inline-formula id="pcbi.1004895.e110"><alternatives><graphic id="pcbi.1004895.e110g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e110" xlink:type="simple"/><mml:math display="inline" id="M110"><mml:mrow><mml:mi>g</mml:mi> <mml:mo>=</mml:mo> <mml:mn>75</mml:mn> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>. The recurrent weights of the network are trained with respect to the memory target <italic>F</italic><sub><italic>m</italic></sub>(<italic>t</italic>). This target is +1 if the last instruction pulse came from <inline-formula id="pcbi.1004895.e111"><alternatives><graphic id="pcbi.1004895.e111g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e111" xlink:type="simple"/><mml:math display="inline" id="M111"><mml:msubsup><mml:mi>f</mml:mi> <mml:mrow><mml:mn>1</mml:mn></mml:mrow> <mml:mi>p</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula> and it is −1 if the last pulse came from <inline-formula id="pcbi.1004895.e112"><alternatives><graphic id="pcbi.1004895.e112g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e112" xlink:type="simple"/><mml:math display="inline" id="M112"><mml:msubsup><mml:mi>f</mml:mi> <mml:mrow><mml:mn>2</mml:mn></mml:mrow> <mml:mi>p</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>. During switching the target follows the integral of the input pulse. The corresponding readout is <italic>z</italic><sub><italic>m</italic></sub>. The second readout <italic>z</italic><sub><italic>c</italic></sub> is trained to output the absolute value of the difference of the two continuous inputs, if the last instruction pulse came from <inline-formula id="pcbi.1004895.e113"><alternatives><graphic id="pcbi.1004895.e113g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e113" xlink:type="simple"/><mml:math display="inline" id="M113"><mml:msubsup><mml:mi>f</mml:mi> <mml:mrow><mml:mn>1</mml:mn></mml:mrow> <mml:mi>p</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>, and to output their sum, if the last instruction pulse came from <inline-formula id="pcbi.1004895.e114"><alternatives><graphic id="pcbi.1004895.e114g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e114" xlink:type="simple"/><mml:math display="inline" id="M114"><mml:msubsup><mml:mi>f</mml:mi> <mml:mrow><mml:mn>2</mml:mn></mml:mrow> <mml:mi>p</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>. The specific analytical form of this target is <inline-formula id="pcbi.1004895.e115"><alternatives><graphic id="pcbi.1004895.e115g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e115" xlink:type="simple"/><mml:math display="inline" id="M115"><mml:mrow><mml:msub><mml:mi>F</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>|</mml:mo> <mml:msubsup><mml:mi>f</mml:mi> <mml:mrow><mml:mn>1</mml:mn></mml:mrow> <mml:mi>c</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>f</mml:mi> <mml:mrow><mml:mn>2</mml:mn></mml:mrow> <mml:mi>c</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>F</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn> <mml:mo>-</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>f</mml:mi> <mml:mrow><mml:mn>1</mml:mn></mml:mrow> <mml:mi>c</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>f</mml:mi> <mml:mrow><mml:mn>2</mml:mn></mml:mrow> <mml:mi>c</mml:mi></mml:msubsup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>F</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. The mean spike rate is 5.53Hz.</p>
</sec>
<sec id="sec022">
<title>Figure 5</title>
<p>Since we have white noise as input we use the Euler-Maruyama scheme in all differential equations. The PCSN has nonlinear dendrites. Non-plastic coupling strengths are <inline-formula id="pcbi.1004895.e116"><alternatives><graphic id="pcbi.1004895.e116g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e116" xlink:type="simple"/><mml:math display="inline" id="M116"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>f</mml:mi> <mml:mo>,</mml:mo> <mml:mi>y</mml:mi></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mn>100</mml:mn> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> for the feedback of the y-coordinate, <inline-formula id="pcbi.1004895.e117"><alternatives><graphic id="pcbi.1004895.e117g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e117" xlink:type="simple"/><mml:math display="inline" id="M117"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>f</mml:mi> <mml:mo>,</mml:mo> <mml:mi>x</mml:mi></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mn>100</mml:mn> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> for the feedback of the x-coordinate, <inline-formula id="pcbi.1004895.e118"><alternatives><graphic id="pcbi.1004895.e118g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e118" xlink:type="simple"/><mml:math display="inline" id="M118"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>f</mml:mi> <mml:mo>,</mml:mo> <mml:mi>ω</mml:mi></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mn>20</mml:mn> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> for the feedback of the angular velocity and <inline-formula id="pcbi.1004895.e119"><alternatives><graphic id="pcbi.1004895.e119g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e119" xlink:type="simple"/><mml:math display="inline" id="M119"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msup> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>2</mml:mn> <mml:mn>7</mml:mn></mml:mfrac> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> for the input. We introduce an additional random constant bias term into the nonlinearity to increase inhomogeneity between the neurons: The nonlinearity is <inline-formula id="pcbi.1004895.e120"><alternatives><graphic id="pcbi.1004895.e120g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e120" xlink:type="simple"/><mml:math display="inline" id="M120"><mml:mrow><mml:mo form="prefix">tanh</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mi>j</mml:mi> <mml:mi>m</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>r</mml:mi> <mml:mi>m</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> where <italic>b</italic><sub><italic>j</italic></sub> is drawn from a Gaussian distribution with standard deviation 0.01. The integration time <italic>δ</italic> is 0.1s. During the control/testing phase, every Δ = 0.01s, <italic>M</italic> = 200 samples of length <italic>T</italic><sub><italic>r</italic></sub> = 1s are created, the cost function is weighted with <inline-formula id="pcbi.1004895.e121"><alternatives><graphic id="pcbi.1004895.e121g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004895.e121" xlink:type="simple"/><mml:math display="inline" id="M121"><mml:mrow><mml:msub><mml:mo>λ</mml:mo> <mml:mi>c</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>01</mml:mn> <mml:mfrac><mml:mn>1</mml:mn> <mml:mtext>s</mml:mtext></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>. The mean spike rate is 146Hz.</p>
</sec>
</sec>
</sec>
<sec id="sec023">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1004895.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004895.s001" xlink:type="simple">
<label>S1 Text</label>
<caption>
<title>Supporting text, figures and methods.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Marije ter Wal, Hans Ruiz, Sep Thijssen, Joris Bierkens, Mario Mulansky, Vicenç Gomez and Kevin Sharp for fruitful discussions and Hans Günter Memmesheimer and Verena Thalmeier for help with the graphical illustrations.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004895.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Natschläger</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Markram</surname> <given-names>H</given-names></name> (<year>2002</year>) <article-title>Real-time computing without stable states: A new framework for neural computation based on perturbations</article-title>. <source>Neural Comput</source> <volume>14</volume>: <fpage>2531</fpage>–<lpage>2560</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976602760407955" xlink:type="simple">10.1162/089976602760407955</ext-link></comment> <object-id pub-id-type="pmid">12433288</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jaeger</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Haas</surname> <given-names>H</given-names></name> (<year>2004</year>) <article-title>Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication</article-title>. <source>Science</source> <volume>304</volume>: <fpage>78</fpage>–<lpage>80</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1091277" xlink:type="simple">10.1126/science.1091277</ext-link></comment> <object-id pub-id-type="pmid">15064413</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Joshi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Sontag</surname> <given-names>ED</given-names></name> (<year>2007</year>) <article-title>Computational aspects of feedback in neural circuits</article-title>. <source>PLoS Comput Biol</source> <volume>3</volume>: <fpage>e165</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.0020165" xlink:type="simple">10.1371/journal.pcbi.0020165</ext-link></comment> <object-id pub-id-type="pmid">17238280</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name> (<year>2009</year>) <article-title>Generating coherent patterns of activity from chaotic neural networks</article-title>. <source>Neuron</source> <volume>63</volume>: <fpage>544</fpage>–<lpage>557</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2009.07.018" xlink:type="simple">10.1016/j.neuron.2009.07.018</ext-link></comment> <object-id pub-id-type="pmid">19709635</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name> (<year>2013</year>) <article-title>Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks</article-title>. <source>Neural Comput</source> <volume>25</volume>: <fpage>626</fpage>–<lpage>649</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00409" xlink:type="simple">10.1162/NECO_a_00409</ext-link></comment> <object-id pub-id-type="pmid">23272922</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jaeger</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Lukosevicius</surname> <given-names>A</given-names></name> (<year>2009</year>) <article-title>Reservoir computing approach to recurrent neural network training</article-title>. <source>Computer Sci Rev</source> <volume>3</volume>: <fpage>127</fpage>–<lpage>149</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cosrev.2009.03.005" xlink:type="simple">10.1016/j.cosrev.2009.03.005</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lazar</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pipa</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Triesch</surname> <given-names>J</given-names></name> (<year>2009</year>) <article-title>SORN: a self-organizing recurrent neural network</article-title>. <source>Front Comput Neurosci</source> <volume>3</volume>: <fpage>23</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/neuro.10.023.2009" xlink:type="simple">10.3389/neuro.10.023.2009</ext-link></comment> <object-id pub-id-type="pmid">19893759</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Klampfl</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name> (<year>2013</year>) <article-title>Emergence of dynamic memory traces in cortical microcircuit models through STDP</article-title>. <source>J Neurosci</source> <volume>33</volume>: <fpage>11515</fpage>–<lpage>11529</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.5044-12.2013" xlink:type="simple">10.1523/JNEUROSCI.5044-12.2013</ext-link></comment> <object-id pub-id-type="pmid">23843522</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Joshi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Maas</surname> <given-names>W</given-names></name> (<year>2005</year>) <article-title>Movement generation with circuits of spiking neurons</article-title>. <source>Neural Comput</source> <volume>17</volume>: <fpage>1715</fpage>–<lpage>1738</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/0899766054026684" xlink:type="simple">10.1162/0899766054026684</ext-link></comment> <object-id pub-id-type="pmid">15969915</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mayor</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name> (<year>2005</year>) <article-title>Signal buffering in random networks of spiking neurons: microscopic versus macroscopic phenomena</article-title>. <source>Phys Rev E</source> <volume>72</volume>: <fpage>051906</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevE.72.051906" xlink:type="simple">10.1103/PhysRevE.72.051906</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wallace</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Maei</surname> <given-names>HR</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name> (<year>2013</year>) <article-title>Randomly connected networks have short temporal memory</article-title>. <source>Neural Comput</source> <volume>25</volume>: <fpage>1408</fpage>–<lpage>1439</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00449" xlink:type="simple">10.1162/NECO_a_00449</ext-link></comment> <object-id pub-id-type="pmid">23517097</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Boerlin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Machens</surname> <given-names>CK</given-names></name>, <name name-style="western"><surname>Denève</surname> <given-names>S</given-names></name> (<year>2013</year>) <article-title>Predictive coding of dynamical variables in balanced spiking networks</article-title>. <source>PLoS Comput Biol</source> <volume>9</volume>: <fpage>e1003258</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1003258" xlink:type="simple">10.1371/journal.pcbi.1003258</ext-link></comment> <object-id pub-id-type="pmid">24244113</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Blitz</surname> <given-names>DM</given-names></name>, <name name-style="western"><surname>Foster</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Regehr</surname> <given-names>WG</given-names></name> (<year>2004</year>) <article-title>Short-term synaptic plasticity: a comparison of two synapses</article-title>. <source>Nat Rev Neurosci</source> <volume>5</volume>: <fpage>630</fpage>–<lpage>640</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn1475" xlink:type="simple">10.1038/nrn1475</ext-link></comment> <object-id pub-id-type="pmid">15263893</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref014">
<label>14</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>L</given-names></name> (<year>2001</year>) <source>Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004895.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Abbott</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Regehr</surname> <given-names>W</given-names></name> (<year>2004</year>) <article-title>Synaptic computation</article-title>. <source>Nature</source> <volume>431</volume>: <fpage>796</fpage>–<lpage>803</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature03010" xlink:type="simple">10.1038/nature03010</ext-link></comment> <object-id pub-id-type="pmid">15483601</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bean</surname> <given-names>BP</given-names></name> (<year>2007</year>) <article-title>The action potential in mammalian central neurons</article-title>. <source>Nat Rev Neurosci</source> <volume>8</volume>: <fpage>451</fpage>–<lpage>465</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nrn2148" xlink:type="simple">10.1038/nrn2148</ext-link></comment> <object-id pub-id-type="pmid">17514198</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lübke</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Markram</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Frotscher</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sakmann</surname> <given-names>B</given-names></name> (<year>1996</year>) <article-title>Frequency and dendritic distribution of autapses established by layer 5 pyramidal neurons in the developing rat neocortex: comparison with synaptic innervation of adjacent neurons of the same class</article-title>. <source>J Neurosci</source> <volume>16</volume>: <fpage>3209</fpage>–<lpage>3218</lpage>. <object-id pub-id-type="pmid">8627359</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bekkers</surname> <given-names>JM</given-names></name> (<year>2009</year>) <article-title>Synaptic transmission: Excitatory autapses find a function?</article-title> <source>Current Biology</source> <volume>19</volume>: <fpage>R296</fpage>–<lpage>R298</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2009.02.010" xlink:type="simple">10.1016/j.cub.2009.02.010</ext-link></comment> <object-id pub-id-type="pmid">19368875</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>London</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Häusser</surname> <given-names>M</given-names></name> (<year>2005</year>) <article-title>Dendritic computation</article-title>. <source>Annu Rev Neurosci</source> <volume>28</volume>: <fpage>503</fpage>–<lpage>532</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.28.061604.135703" xlink:type="simple">10.1146/annurev.neuro.28.061604.135703</ext-link></comment> <object-id pub-id-type="pmid">16033324</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Memmesheimer</surname> <given-names>RM</given-names></name> (<year>2010</year>) <article-title>Quantitative prediction of intermittent high-frequency oscillations in neural networks with supralinear dendritic interactions</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>107</volume>: <fpage>11092</fpage>–<lpage>11097</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0909615107" xlink:type="simple">10.1073/pnas.0909615107</ext-link></comment> <object-id pub-id-type="pmid">20511534</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cazé</surname> <given-names>RD</given-names></name>, <name name-style="western"><surname>Humphries</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Gutkin</surname> <given-names>B</given-names></name> (<year>2013</year>) <article-title>Passive dendrites enable single neurons to compute linearly non-separable functions</article-title>. <source>PLoS Comput Biol</source> <volume>9</volume>: <fpage>e1002867</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002867" xlink:type="simple">10.1371/journal.pcbi.1002867</ext-link></comment> <object-id pub-id-type="pmid">23468600</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Branco</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Häusser</surname> <given-names>M</given-names></name> (<year>2011</year>) <article-title>Synaptic integration gradients in single cortical pyramidal cell dendrites</article-title>. <source>Neuron</source> <volume>69</volume>: <fpage>885</fpage>–<lpage>892</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2011.02.006" xlink:type="simple">10.1016/j.neuron.2011.02.006</ext-link></comment> <object-id pub-id-type="pmid">21382549</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Huang</surname> <given-names>GB</given-names></name>, <name name-style="western"><surname>Zhu</surname> <given-names>QY</given-names></name>, <name name-style="western"><surname>Siew</surname> <given-names>CK</given-names></name> (<year>2006</year>) <article-title>Extreme learning machine: theory and applications</article-title>. <source>Neurocomputing</source> <volume>70</volume>: <fpage>489</fpage>–<lpage>501</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neucom.2005.12.126" xlink:type="simple">10.1016/j.neucom.2005.12.126</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bourdoukan</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Barrett</surname> <given-names>DG</given-names></name>, <name name-style="western"><surname>Machens</surname> <given-names>CK</given-names></name>, <name name-style="western"><surname>Denève</surname> <given-names>S</given-names></name> (<year>2012</year>) <article-title>Learning optimal spike-based representations</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>25</volume>: <fpage>2294</fpage>–<lpage>2302</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004895.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bourdoukan</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Denève</surname> <given-names>S</given-names></name> (<year>2015</year>) <article-title>Enforcing balance allows local supervised learning in spiking recurrent networks</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>28</volume>: <fpage>982</fpage>–<lpage>990</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004895.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lukosevicius</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Jaeger</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Schrauwen</surname> <given-names>B</given-names></name> (<year>2012</year>) <article-title>Reservoir computing trends</article-title>. <source>Künstl Intell</source> <volume>26</volume>: <fpage>365</fpage>–<lpage>371</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004895.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jaeger</surname> <given-names>H</given-names></name> (<year>2001</year>) <article-title>The “echo state” approach to analysing and training recurrent neural networks-with an erratum note</article-title>. <source>Bonn, Germany: German National Research Center for Information Technology GMD Technical Report</source> <volume>148</volume>: <fpage>34</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004895.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yildiz</surname> <given-names>IB</given-names></name>, <name name-style="western"><surname>Jaeger</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Kiebel</surname> <given-names>SJ</given-names></name> (<year>2012</year>) <article-title>Re-visiting the echo state property</article-title>. <source>Neural Netw</source> <volume>35</volume>: <fpage>1</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neunet.2012.07.005" xlink:type="simple">10.1016/j.neunet.2012.07.005</ext-link></comment> <object-id pub-id-type="pmid">22885243</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wang</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Stradtman</surname> <given-names>GG</given-names> <suffix>3rd</suffix></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>, <name name-style="western"><surname>Gao</surname> <given-names>WJ</given-names></name> (<year>2008</year>) <article-title>A specialized nmda receptor function in layer 5 recurrent microcircuitry of the adult rat prefrontal cortex</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>105</volume>: <fpage>16791</fpage>–<lpage>16796</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0804318105" xlink:type="simple">10.1073/pnas.0804318105</ext-link></comment> <object-id pub-id-type="pmid">18922773</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Petrides</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Georgopoulos</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Kostopoulos</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Papatheodoropoulos</surname> <given-names>C</given-names></name> (<year>2007</year>) <article-title>The GABAA receptor-mediated recurrent inhibition in ventral compared with dorsal CA1 hippocampal region is weaker, decays faster and lasts less</article-title>. <source>Exp Brain Res</source> <volume>177</volume>: <fpage>370</fpage>–<lpage>383</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00221-006-0681-6" xlink:type="simple">10.1007/s00221-006-0681-6</ext-link></comment> <object-id pub-id-type="pmid">16988819</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sceniak</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Maciver</surname> <given-names>MB</given-names></name> (<year>2008</year>) <article-title>Slow GABA(A) mediated synaptic transmission in rat visual cortex</article-title>. <source>BMC Neurosci</source> <volume>9</volume>: <fpage>8</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/1471-2202-9-8" xlink:type="simple">10.1186/1471-2202-9-8</ext-link></comment> <object-id pub-id-type="pmid">18199338</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Storm</surname> <given-names>JF</given-names></name> (<year>1987</year>) <article-title>Action potential repolarization and a fast after-hyperpolarization in rat hippocampal pyramidal cells</article-title>. <source>J Physiol</source> <volume>385</volume>: <fpage>733</fpage>–<lpage>759</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1113/jphysiol.1987.sp016517" xlink:type="simple">10.1113/jphysiol.1987.sp016517</ext-link></comment> <object-id pub-id-type="pmid">2443676</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Raman</surname> <given-names>IM</given-names></name>, <name name-style="western"><surname>Bean</surname> <given-names>BP</given-names></name> (<year>1997</year>) <article-title>Resurgent sodium current and action potential formation in dissociated cerebellar purkinje neurons</article-title>. <source>J Neurosci</source> <volume>17</volume>: <fpage>4517</fpage>–<lpage>4526</lpage>. <object-id pub-id-type="pmid">9169512</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chen</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Yaari</surname> <given-names>Y</given-names></name> (<year>2008</year>) <article-title>Spike CA2+ influx upmodulates the spike afterdepolarization and bursting via intracellular inhibition of Kv7/m channels</article-title>. <source>J Physiol</source> <volume>586</volume>: <fpage>1351</fpage>–<lpage>1363</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1113/jphysiol.2007.148171" xlink:type="simple">10.1113/jphysiol.2007.148171</ext-link></comment> <object-id pub-id-type="pmid">18187471</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brown</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Randall</surname> <given-names>AD</given-names></name> (<year>2009</year>) <article-title>Activity-dependent depression of the spike after-depolarization generates long-lasting intrinsic plasticity in hippocampal CA3 pyramidal neurons</article-title>. <source>The Journal of Physiology</source> <volume>587</volume>: <fpage>1265</fpage>–<lpage>1281</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1113/jphysiol.2008.167007" xlink:type="simple">10.1113/jphysiol.2008.167007</ext-link></comment> <object-id pub-id-type="pmid">19171653</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lüscher</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Jan</surname> <given-names>LY</given-names></name>, <name name-style="western"><surname>Stoffel</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Malenka</surname> <given-names>RC</given-names></name>, <name name-style="western"><surname>Nicoll</surname> <given-names>RA</given-names></name> (<year>1997</year>) <article-title>G protein-coupled inwardly rectifying K+ channels (GIRKs) mediate postsynaptic but not presynaptic transmitter actions in hippocampal neurons</article-title>. <source>Neuron</source> <volume>19</volume>: <fpage>687</fpage>–<lpage>695</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(00)80381-5" xlink:type="simple">10.1016/S0896-6273(00)80381-5</ext-link></comment> <object-id pub-id-type="pmid">9331358</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Boerlin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Denève</surname> <given-names>S</given-names></name> (<year>2011</year>) <article-title>Spike-based population coding and working memory</article-title>. <source>PLoS Comput Biol</source> <volume>7</volume>: <fpage>e1001080</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1001080" xlink:type="simple">10.1371/journal.pcbi.1001080</ext-link></comment> <object-id pub-id-type="pmid">21379319</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sussillo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name> (<year>2012</year>) <article-title>Transferring learning from external to internal weights in echo-state networks with sparse connectivity</article-title>. <source>PLoS One</source> <volume>7</volume>: <fpage>e37372</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0037372" xlink:type="simple">10.1371/journal.pone.0037372</ext-link></comment> <object-id pub-id-type="pmid">22655041</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref039">
<label>39</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Eliasmith</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Anderson</surname> <given-names>C</given-names></name> (<year>2003</year>) <source>Neural Engineering: Computation, Representation and Dynamics in Neurobiological Systems</source>. <publisher-loc>Cambridge MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004895.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Losonczy</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Makara</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Magee</surname> <given-names>J</given-names></name> (<year>2008</year>) <article-title>Compartmentalized dendritic plasticity and input feature storage in neurons</article-title>. <source>Nature</source> <volume>452</volume>: <fpage>436</fpage>–<lpage>441</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature06725" xlink:type="simple">10.1038/nature06725</ext-link></comment> <object-id pub-id-type="pmid">18368112</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref041">
<label>41</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Haykin</surname> <given-names>S</given-names></name> (<year>2002</year>) <source>Adaptive Filter Theory</source>. <publisher-loc>Upper Saddle River, NJ</publisher-loc>: <publisher-name>Prentice Hall</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004895.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hirsch</surname> <given-names>M</given-names></name> (<year>1989</year>) <article-title>Convergent activation dynamics in continuous time networks</article-title>. <source>Neural Netw</source> <volume>2</volume>: <fpage>331</fpage>–<lpage>349</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0893-6080(89)90018-X" xlink:type="simple">10.1016/0893-6080(89)90018-X</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Grillner</surname> <given-names>S</given-names></name> (<year>2006</year>) <article-title>Biological patern generation: The cellular and computational logic of networks in motion</article-title>. <source>Neuron</source> <volume>52</volume>: <fpage>751</fpage>–<lpage>766</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2006.11.008" xlink:type="simple">10.1016/j.neuron.2006.11.008</ext-link></comment> <object-id pub-id-type="pmid">17145498</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Li</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Peng</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Kurths</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Schellnhuber</surname> <given-names>HJ</given-names></name> (<year>2014</year>) <article-title>Chaos-order transition in foraging behavior of ants</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>111</volume>: <fpage>8392</fpage>–<lpage>8397</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1407083111" xlink:type="simple">10.1073/pnas.1407083111</ext-link></comment> <object-id pub-id-type="pmid">24912159</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Quiroga</surname> <given-names>RQ</given-names></name>, <name name-style="western"><surname>Reddy</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Kreiman</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Fried</surname> <given-names>I</given-names></name> (<year>2005</year>) <article-title>Invariant visual representation by single neurons in the human brain</article-title>. <source>Nature</source> <volume>435</volume>: <fpage>1102</fpage>–<lpage>1107</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature03687" xlink:type="simple">10.1038/nature03687</ext-link></comment> <object-id pub-id-type="pmid">15973409</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Timme</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wolf</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Geisel</surname> <given-names>T</given-names></name> (<year>2004</year>) <article-title>Topological speed limits to network synchronization</article-title>. <source>Phys Rev Lett</source> <volume>92</volume>: <fpage>074101</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.92.074101" xlink:type="simple">10.1103/PhysRevLett.92.074101</ext-link></comment> <object-id pub-id-type="pmid">14995853</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>White</surname> <given-names>OL</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>DD</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name> (<year>2004</year>) <article-title>Short-term memory in orthogonal neural networks</article-title>. <source>Phys Rev Lett</source> <volume>92</volume>: <fpage>148102</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.92.148102" xlink:type="simple">10.1103/PhysRevLett.92.148102</ext-link></comment> <object-id pub-id-type="pmid">15089576</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Goldman</surname> <given-names>MS</given-names></name> (<year>2009</year>) <article-title>Memory without feedback in a neural network</article-title>. <source>Neuron</source> <volume>61</volume>: <fpage>621</fpage>–<lpage>634</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2008.12.012" xlink:type="simple">10.1016/j.neuron.2008.12.012</ext-link></comment> <object-id pub-id-type="pmid">19249281</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ivry</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Spencer</surname> <given-names>R</given-names></name> (<year>2004</year>) <article-title>The neural representation of time</article-title>. <source>Current opinion in neurobiology</source> <volume>14</volume>: <fpage>225</fpage>–<lpage>232</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2004.03.013" xlink:type="simple">10.1016/j.conb.2004.03.013</ext-link></comment> <object-id pub-id-type="pmid">15082329</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref050">
<label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hennequin</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Vogels</surname> <given-names>TP</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name> (<year>2014</year>) <article-title>Optimal control of transient dynamics in balanced networks supports generation of complex movements</article-title>. <source>Neuron</source> <volume>82</volume>: <fpage>1394</fpage>–<lpage>1406</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2014.04.045" xlink:type="simple">10.1016/j.neuron.2014.04.045</ext-link></comment> <object-id pub-id-type="pmid">24945778</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ostojic</surname> <given-names>S</given-names></name> (<year>2014</year>) <article-title>Two types of asynchronous activity in networks of excitatory and inhibitory spiking neurons</article-title>. <source>Nat Neurosci</source> <volume>17</volume>: <fpage>594</fpage>–<lpage>600</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.3658" xlink:type="simple">10.1038/nn.3658</ext-link></comment> <object-id pub-id-type="pmid">24561997</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sompolinsky</surname></name>, <name name-style="western"><surname>Crisanti</surname></name>, <name name-style="western"><surname>Sommers</surname></name> (<year>1988</year>) <article-title>Chaos in random neural networks</article-title>. <source>Phys Rev Lett</source> <volume>61</volume>: <fpage>259</fpage>–<lpage>262</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.61.259" xlink:type="simple">10.1103/PhysRevLett.61.259</ext-link></comment> <object-id pub-id-type="pmid">10039285</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Seung</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>DD</given-names></name>, <name name-style="western"><surname>Reis</surname> <given-names>BY</given-names></name>, <name name-style="western"><surname>Tank</surname> <given-names>DW</given-names></name> (<year>2000</year>) <article-title>The autapse: a simple illustration of short-term analog memory storage by tuned synaptic feedback</article-title>. <source>J Comput Neurosci</source> <volume>9</volume>: <fpage>171</fpage>–<lpage>185</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/A:1008971908649" xlink:type="simple">10.1023/A:1008971908649</ext-link></comment> <object-id pub-id-type="pmid">11030520</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Deadwyler</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Hampson</surname> <given-names>RE</given-names></name> (<year>2006</year>) <article-title>Temporal coupling between subicular and hippocampal neurons underlies retention of trial-specific events</article-title>. <source>Behavioural brain research</source> <volume>174</volume>: <fpage>272</fpage>–<lpage>280</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.bbr.2006.05.038" xlink:type="simple">10.1016/j.bbr.2006.05.038</ext-link></comment> <object-id pub-id-type="pmid">16876266</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Matell</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Meck</surname> <given-names>WH</given-names></name>, <name name-style="western"><surname>Nicolelis</surname> <given-names>MA</given-names></name> (<year>2003</year>) <article-title>Interval timing and the encoding of signal duration by ensembles of cortical and striatal neurons</article-title>. <source>Behavioral neuroscience</source> <volume>117</volume>: <fpage>760</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0735-7044.117.4.760" xlink:type="simple">10.1037/0735-7044.117.4.760</ext-link></comment> <object-id pub-id-type="pmid">12931961</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref056">
<label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sakai</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Passingham</surname> <given-names>RE</given-names></name> (<year>2003</year>) <article-title>Prefrontal interactions reflect future task operations</article-title>. <source>Nat Neurosci</source> <volume>6</volume>: <fpage>75</fpage>–<lpage>81</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn987" xlink:type="simple">10.1038/nn987</ext-link></comment> <object-id pub-id-type="pmid">12469132</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Tsodyks</surname> <given-names>M</given-names></name> (<year>2014</year>) <article-title>Working models of working memory</article-title>. <source>Curr Opin Neurobiol</source> <volume>25</volume>: <fpage>20</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2013.10.008" xlink:type="simple">10.1016/j.conb.2013.10.008</ext-link></comment> <object-id pub-id-type="pmid">24709596</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hopfield</surname> <given-names>J</given-names></name> (<year>1982</year>) <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>79</volume>: <fpage>2554</fpage>–<lpage>2558</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.79.8.2554" xlink:type="simple">10.1073/pnas.79.8.2554</ext-link></comment> <object-id pub-id-type="pmid">6953413</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Litwin-Kumar</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Doiron</surname> <given-names>B</given-names></name> (<year>2014</year>) <article-title>Formation and maintenance of neuronal assemblies through synaptic plasticity</article-title>. <source>Nat Commun</source> <volume>5</volume>: <fpage>5319</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/ncomms6319" xlink:type="simple">10.1038/ncomms6319</ext-link></comment> <object-id pub-id-type="pmid">25395015</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zenke</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Agnes</surname> <given-names>EJ</given-names></name>, <name name-style="western"><surname>Gerstner</surname> <given-names>W</given-names></name> (<year>2015</year>) <article-title>Diverse synaptic plasticity mechanisms orchestrated to form and retrieve memories in spiking neural networks</article-title>. <source>Nat Commun</source> <volume>6</volume>: <fpage>6922</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/ncomms7922" xlink:type="simple">10.1038/ncomms7922</ext-link></comment> <object-id pub-id-type="pmid">25897632</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kappen</surname> <given-names>HJ</given-names></name> (<year>2005</year>) <article-title>Linear theory for control of nonlinear stochastic systems</article-title>. <source>Phys Rev Lett</source> <volume>95</volume>: <fpage>200201</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.95.200201" xlink:type="simple">10.1103/PhysRevLett.95.200201</ext-link></comment> <object-id pub-id-type="pmid">16384034</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pfeiffer</surname> <given-names>BE</given-names></name>, <name name-style="western"><surname>Foster</surname> <given-names>DJ</given-names></name> (<year>2013</year>) <article-title>Hippocampal place-cell sequences depict future paths to remembered goals</article-title>. <source>Nature</source> <volume>497</volume>: <fpage>74</fpage>–<lpage>79</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nature12112" xlink:type="simple">10.1038/nature12112</ext-link></comment> <object-id pub-id-type="pmid">23594744</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref063">
<label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Van Der Meer</surname> <given-names>MAA</given-names></name>, <name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name> (<year>2010</year>) <article-title>Expectancies in decision making, reinforcement learning, and ventral striatum</article-title>. <source>Frontiers in Neuroscience</source> <volume>4</volume>: <fpage>6</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/neuro.01.006.2010" xlink:type="simple">10.3389/neuro.01.006.2010</ext-link></comment> <object-id pub-id-type="pmid">21221409</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref064">
<label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>von Hofsten</surname> <given-names>C</given-names></name> (<year>1982</year>) <article-title>Eye hand coordination in the newborn</article-title>. <source>Developmental Psychology</source> <volume>18</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0012-1649.18.3.450" xlink:type="simple">10.1037/0012-1649.18.3.450</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref065">
<label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fiete</surname> <given-names>IR</given-names></name>, <name name-style="western"><surname>Fee</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Seung</surname> <given-names>HS</given-names></name> (<year>2007</year>) <article-title>Model of birdsong learning based on gradient estimation by dynamic perturbation of neural conductances</article-title>. <source>J Neurophysiol</source> <volume>98</volume>: <fpage>2038</fpage>–<lpage>2057</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1152/jn.01311.2006" xlink:type="simple">10.1152/jn.01311.2006</ext-link></comment> <object-id pub-id-type="pmid">17652414</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref066">
<label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lansink</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Goltstein</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Lankelma</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Joosten</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>McNaughton</surname> <given-names>B</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Preferential reactivation of motivationally relevant information in the ventral striatum</article-title>. <source>J Neurosci</source> <volume>28</volume>: <fpage>6372</fpage>–<lpage>6382</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.1054-08.2008" xlink:type="simple">10.1523/JNEUROSCI.1054-08.2008</ext-link></comment> <object-id pub-id-type="pmid">18562607</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref067">
<label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>van der Meer</surname> <given-names>MAA</given-names></name>, <name name-style="western"><surname>Redish</surname> <given-names>AD</given-names></name> (<year>2009</year>) <article-title>Covert expectation-of-reward in rat ventral striatum at decision points</article-title>. <source>Front Integr Neurosci</source> <volume>3</volume>: <fpage>1</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/neuro.07.001.2009" xlink:type="simple">10.3389/neuro.07.001.2009</ext-link></comment> <object-id pub-id-type="pmid">19225578</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref068">
<label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lansink</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Goltstein</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Lankelma</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>McNaughton</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Pennartz</surname> <given-names>C</given-names></name> (<year>2009</year>) <article-title>Hippocampus leads ventral striatum in replay of place-reward information</article-title>. <source>PLoS Biol</source> <volume>7</volume>: <fpage>e1000173</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.1000173" xlink:type="simple">10.1371/journal.pbio.1000173</ext-link></comment> <object-id pub-id-type="pmid">19688032</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref069">
<label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jahnke</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Memmesheimer</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Timme</surname> <given-names>M</given-names></name> (<year>2009</year>) <article-title>How chaotic is the balanced state?</article-title> <source>Front Comput Neurosci</source> <volume>3</volume>: <fpage>13</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/neuro.10.013.2009" xlink:type="simple">10.3389/neuro.10.013.2009</ext-link></comment> <object-id pub-id-type="pmid">19936316</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref070">
<label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Harish</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Hansel</surname> <given-names>D</given-names></name> (<year>2015</year>) <article-title>Asynchronous rate chaos in spiking neuronal circuits</article-title>. <source>PLoS Comput Biol</source> <volume>11</volume>: <fpage>e1004266</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1004266" xlink:type="simple">10.1371/journal.pcbi.1004266</ext-link></comment> <object-id pub-id-type="pmid">26230679</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref071">
<label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kadmon</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name> (<year>2015</year>) <article-title>Transition to chaos in random neuronal networks</article-title>. <source>Physical Review X</source> <volume>5</volume>: <fpage>041030</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevX.5.041030" xlink:type="simple">10.1103/PhysRevX.5.041030</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref072">
<label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schwemmer</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Fairhall</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Denéve</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Shea-Brown</surname> <given-names>ET</given-names></name> (<year>2015</year>) <article-title>Constructing precisely computing networks with biophysical spiking neurons</article-title>. <source>The Journal of Neuroscience</source> <volume>35</volume>: <fpage>10112</fpage>–<lpage>10134</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1523/JNEUROSCI.4951-14.2015" xlink:type="simple">10.1523/JNEUROSCI.4951-14.2015</ext-link></comment> <object-id pub-id-type="pmid">26180189</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref073">
<label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Eliasmith</surname> <given-names>C</given-names></name> (<year>2005</year>) <article-title>A unified approach to building and controlling spiking attractor networks</article-title>. <source>Neural Comput</source> <volume>17</volume>: <fpage>1276</fpage>–<lpage>1314</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/0899766053630332" xlink:type="simple">10.1162/0899766053630332</ext-link></comment> <object-id pub-id-type="pmid">15901399</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref074">
<label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Eliasmith</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Stewart</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Choo</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Bekolay</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>DeWolf</surname> <given-names>T</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>A large-scale model of the functioning brain</article-title>. <source>Science</source> <volume>338</volume>: <fpage>1202</fpage>–<lpage>1205</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1225266" xlink:type="simple">10.1126/science.1225266</ext-link></comment> <object-id pub-id-type="pmid">23197532</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref075">
<label>75</label>
<mixed-citation publication-type="other" xlink:type="simple">DePasquale B, Churchland MM, Abbott L (2016) Using firing-rate dynamics to train recurrent networks of spiking model neurons. ArXiv:1601.07620.</mixed-citation>
</ref>
<ref id="pcbi.1004895.ref076">
<label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Abbott</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>DePasquale</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Memmesheimer</surname> <given-names>RM</given-names></name> (<year>2016</year>) <article-title>Building functional networks of spiking model neurons</article-title>. <source>Nat. Neurosci.</source> <volume>19</volume>: <fpage>350</fpage>–<lpage>355</lpage> <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn.4241" xlink:type="simple">10.1038/nn.4241</ext-link></comment> <object-id pub-id-type="pmid">26906501</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref077">
<label>77</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Legenstein</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name> (<year>2007</year>) <article-title>Edge of chaos and prediction of computational performance for neural circuit models</article-title>. <source>Neural Netw</source> <volume>20</volume>: <fpage>323</fpage>–<lpage>334</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neunet.2007.04.017" xlink:type="simple">10.1016/j.neunet.2007.04.017</ext-link></comment> <object-id pub-id-type="pmid">17517489</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref078">
<label>78</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name> (<year>2010</year>) <source>Liquid State Machines: Motivation, Theory, and Applications</source>, <volume>volume 189</volume>. <publisher-loc>Singapore</publisher-loc>: <publisher-name>World Scientific Review</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004895.ref079">
<label>79</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name> (<year>2011</year>) <article-title>What is optimal about motor control?</article-title> <source>Neuron</source> <volume>72</volume>: <fpage>488</fpage>–<lpage>498</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2011.10.018" xlink:type="simple">10.1016/j.neuron.2011.10.018</ext-link></comment> <object-id pub-id-type="pmid">22078508</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref080">
<label>80</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Todorov</surname> <given-names>E</given-names></name> (<year>2009</year>) <article-title>Efficient computation of optimal actions</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>106</volume>: <fpage>11478</fpage>–<lpage>11483</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0710743106" xlink:type="simple">10.1073/pnas.0710743106</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004895.ref081">
<label>81</label>
<mixed-citation publication-type="other" xlink:type="simple">Theodorou E, Buchli J, Schaal S (2010) Reinforcement learning of motor skills in high dimensions: A path integral approach. In: Robotics and Automation (ICRA), 2010 IEEE International Conference on. pp. 2397–2403.</mixed-citation>
</ref>
<ref id="pcbi.1004895.ref082">
<label>82</label>
<mixed-citation publication-type="other" xlink:type="simple">Pastor P, Kalakrishnan M, Chitta S, Theodorou E, Schaal S (2011) Skill learning and task outcome prediction for manipulation. In: Robotics and Automation (ICRA), 2011 IEEE International Conference on. pp. 3828–3834.</mixed-citation>
</ref>
<ref id="pcbi.1004895.ref083">
<label>83</label>
<mixed-citation publication-type="other" xlink:type="simple">Buchli J, Theodorou E, Stulp F, Schaal S (2010) Variable impedance control—a reinforcement learning approach. In: Proceedings of Robotics: Science and Systems. Zaragoza, Spain, pp. 153–160.</mixed-citation>
</ref>
</ref-list>
</back>
</article>