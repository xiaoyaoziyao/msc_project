<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group>
<journal-title>PLoS ONE</journal-title></journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-13-18083</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0094204</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject></subj-group></subj-group><subj-group><subject>Neuroscience</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Computer and information sciences</subject><subj-group><subject>Computing methods</subject><subj-group><subject>Mathematical computing</subject></subj-group></subj-group><subj-group><subject>Computing systems</subject><subj-group><subject>Analog computing</subject><subject>Digital computing</subject><subject>Hybrid computing</subject></subj-group></subj-group><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Discrete mathematics</subject><subj-group><subject>Computational systems</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>An Attractor-Based Complexity Measurement for Boolean Recurrent Neural Networks</article-title>
<alt-title alt-title-type="running-head">Attractor-Based Complexity of Neural Networks</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Cabessa</surname><given-names>Jérémie</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Villa</surname><given-names>Alessandro E. P.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Neuroheuristic Research Group, Faculty of Business and Economics, University of Lausanne, Lausanne, Switzerland</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Laboratory of Mathematical Economics (LEMMA), University of Paris 2 – Panthéon-Assas, Paris, France</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Grenoble Institute of Neuroscience, Faculty of Medicine, University Joseph Fourier, Grenoble, France</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Gómez</surname><given-names>Sergio</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Universitat Rovira i Virgili, Spain</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">jcabessa@nhrg.org</email> (JC); <email xlink:type="simple">avilla@nhrg.org</email> (AV)</corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: JC AV. Performed the experiments: JC AV. Analyzed the data: JC AV. Contributed reagents/materials/analysis tools: JC AV. Wrote the paper: JC AV.</p></fn>
</author-notes>
<pub-date pub-type="collection"><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>11</day><month>4</month><year>2014</year></pub-date>
<volume>9</volume>
<issue>4</issue>
<elocation-id>e94204</elocation-id>
<history>
<date date-type="received"><day>3</day><month>5</month><year>2013</year></date>
<date date-type="accepted"><day>14</day><month>3</month><year>2014</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Cabessa, Villa</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>We provide a novel refined attractor-based complexity measurement for Boolean recurrent neural networks that represents an assessment of their computational power in terms of the significance of their attractor dynamics. This complexity measurement is achieved by first proving a computational equivalence between Boolean recurrent neural networks and some specific class of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e001" xlink:type="simple"/></inline-formula>-automata, and then translating the most refined classification of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e002" xlink:type="simple"/></inline-formula>-automata to the Boolean neural network context. As a result, a hierarchical classification of Boolean neural networks based on their attractive dynamics is obtained, thus providing a novel refined attractor-based complexity measurement for Boolean recurrent neural networks. These results provide new theoretical insights to the computational and dynamical capabilities of neural networks according to their attractive potentialities. An application of our findings is illustrated by the analysis of the dynamics of a simplified model of the basal ganglia-thalamocortical network simulated by a Boolean recurrent neural network. This example shows the significance of measuring network complexity, and how our results bear new founding elements for the understanding of the complexity of real brain circuits.</p>
</abstract>
<funding-group><funding-statement>This work is funded by the University of Lausanne, Switzerland. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="22"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>In neural computation, understanding the computational and dynamical properties of biological neural networks is an issue of central importance. In this context, much interest has been focused on comparing the computational power of diverse theoretical neural models with those of abstract computing devices. Nowadays, the computational capabilities of neural models is known to be tightly related to the nature of the activation function of the neurons, to the nature of their synaptic connections, to the eventual presence of noise in the model, to the possibility for the networks to evolve over time, and to the computational paradigm performed by the networks.</p>
<p>The first and seminal results in this direction were provided by McCulloch and Pitts, Kleene, and Minsky who proved that first-order Boolean recurrent neural networks were computationally equivalent to classical finite state automata <xref ref-type="bibr" rid="pone.0094204-McCulloch1">[1]</xref>–<xref ref-type="bibr" rid="pone.0094204-Minsky1">[3]</xref>. Kremer extended these results to the class of Elman-style recurrent neural nets <xref ref-type="bibr" rid="pone.0094204-Kremer1">[4]</xref>, and Sperduti discussed the computational power of different other architecturally constrained classes of networks <xref ref-type="bibr" rid="pone.0094204-Sperduti1">[5]</xref>.</p>
<p>Later, Siegelmann and Sontag proved that by considering rational synaptic weights and by extending the activation functions of the cells from Boolean to linear-sigmoid, the corresponding neural networks have their computational power drastically increased from finite state automata up to Turing machines <xref ref-type="bibr" rid="pone.0094204-Siegelmann1">[6]</xref>–<xref ref-type="bibr" rid="pone.0094204-Neto1">[8]</xref>. Kilian and Siegelmann then generalised the Turing universality of neural networks to a broader class of sigmoidal activation functions <xref ref-type="bibr" rid="pone.0094204-Kilian1">[9]</xref>. The computational equivalence between so-called “rational recurrent neural networks” and Turing machines has now become standard result in the field.</p>
<p>Following von Neumann considerations <xref ref-type="bibr" rid="pone.0094204-Neumann1">[10]</xref>, Siegelmann and Sontag further assumed that the variables appearing in the underlying chemical and physical phenomena could be modelled by continuous rather than discrete (rational) numbers, and therefore proposed a study of the computational capabilities of recurrent neural networks equipped with real instead of rational synaptic weights <xref ref-type="bibr" rid="pone.0094204-Siegelmann2">[11]</xref>. They proved that the so-called “analog recurrent neural networks” are computationally equivalent to Turing machines with advices, hence capable of super-Turing computational power from polynomial time of computation already <xref ref-type="bibr" rid="pone.0094204-Siegelmann2">[11]</xref>. In this context, a proper internal hierarchical classification of analog recurrent neural networks according to the Kolmogorov complexity of their underlying real synaptic weights was described <xref ref-type="bibr" rid="pone.0094204-Balczar1">[12]</xref>.</p>
<p>It was also shown that the presence of arbitrarily small amount of analog noise seriously reduces the computational capability of both rational- and real-weighted recurrent neural networks to those of finite automata <xref ref-type="bibr" rid="pone.0094204-Maass1">[13]</xref>. In the presence of Gaussian or other common analog noise distribution with sufficiently large support, the computational power of recurrent neural networks is reduced to even less than finite automata, namely to the recognition of definite languages <xref ref-type="bibr" rid="pone.0094204-Maass2">[14]</xref>.</p>
<p>Besides, the concept of evolvability has also turned out to be essential in the study of the computational power of circuits closer to the biological world. The research in this context has initially been focused almost exclusively on the application of genetic algorithms aimed at allowing networks with fully-connected topology and satisfying selected fitness functions (e.g., performed well on specific tasks) to reproduce and multiply <xref ref-type="bibr" rid="pone.0094204-Fogel1">[15]</xref>–<xref ref-type="bibr" rid="pone.0094204-Yao1">[18]</xref>. This approach aimed to optimise the connection weights that determine the functionality of a network with fixed-topology. However, the topology of neural networks, i.e. their structure and connectivity patterns, greatly affects their functionality. The evolution of both topologies and connection weights following bioinspired rules that may also include features derived from the study of neural development, differentiation, genetically programmed cell-death and synaptic plasticity rules has become increasingly studied in recent years <xref ref-type="bibr" rid="pone.0094204-Angeline1">[19]</xref>–<xref ref-type="bibr" rid="pone.0094204-Shaposhnyk1">[26]</xref>. Along this line, Cabessa and Siegelmann provided a theoretical study proving that both models of rational-weighted and analog evolving recurrent neural networks are capable of super-Turing computational capabilities, equivalent to those of static analog neural networks <xref ref-type="bibr" rid="pone.0094204-Cabessa1">[27]</xref>.</p>
<p>Finally, from a general perspective, the classical computational approach from Turing <xref ref-type="bibr" rid="pone.0094204-Turing1">[28]</xref> was argued to “no longer fully corresponds to the current notion of computing in modern systems” <xref ref-type="bibr" rid="pone.0094204-vanLeeuwen1">[29]</xref> – especially when it refers to bio-inspired complex information processing systems. In the brain (or in organic life in general), information is rather processed in an interactive way <xref ref-type="bibr" rid="pone.0094204-Goldin1">[30]</xref>, where previous experience must affect the perception of future inputs, and where older memories may themselves change with response to new inputs. Following this perspective, Cabessa and Villa described the super-Turing computational power of analog recurrent neural networks involved in a reactive computational framework <xref ref-type="bibr" rid="pone.0094204-Cabessa2">[31]</xref>. Cabessa and Siegelmann provided a characterisation of the Turing and super-Turing capabilities of rational and analog recurrent neural networks involved in a basic interactive computational paradigm, respectively <xref ref-type="bibr" rid="pone.0094204-Cabessa3">[32]</xref>. Moreover, Cabessa and Villa proved that neural models combining the two crucial features of evolvability and interactivity were capable of super-Turing computational capabilities <xref ref-type="bibr" rid="pone.0094204-Cabessa4">[33]</xref>.</p>
<p>In this paper, we pursue the study of the computational power of neural models and provide two novel refined attractor-based complexity measurement for Boolean recurrent neural networks. More precisely, as a first step we provide a generalisation to the precise infinite input stream context of the classical equivalence result between Boolean neural networks and finite state automata <xref ref-type="bibr" rid="pone.0094204-McCulloch1">[1]</xref>–<xref ref-type="bibr" rid="pone.0094204-Minsky1">[3]</xref>. Under some natural condition on the type specification of their attractors, we show that Boolean recurrent neural networks disclose the very same expressive power as deterministic Büchi automata <xref ref-type="bibr" rid="pone.0094204-Bchi1">[34]</xref>. This equivalence allows to establish a hierarchical classification of Boolean neural networks by translating the Wagner classification theory from the Büchi automaton to the neural network context <xref ref-type="bibr" rid="pone.0094204-Wagner1">[35]</xref>. The obtained classification consists of a pre-well ordering of width 2 and height <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e003" xlink:type="simple"/></inline-formula> (where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e004" xlink:type="simple"/></inline-formula> denotes the first infinite ordinal). As a second step, we show that by totally relaxing the restrictions on the type specification of their attractors, the Boolean neural networks significantly increase their expressive power from deterministic Büchi automata up to Muller automata. Hence, another more refined hierarchical classification of Boolean neural networks is obtained by translating the Wagner classification theory from the Muller automaton to the neural network context. This classification consists of a pre-well ordering of width 2 and height <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e005" xlink:type="simple"/></inline-formula>. The complexity measurements induced by these two hierarchical classifications refer to the possibility of networks' dynamics to maximally alternate between attractors of different types along their evolutions. They represent an assessment of the computational power of Boolean neural networks in terms of the significance of their attractor dynamics. Finally, an application of this approach to a Boolean model of the basal ganglia-thalamocortical network is provided. This practical example shows that our automata-theoretical approach might bear new founding elements for the understanding of the complexity of real brain circuits.</p>
</sec><sec id="s2" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s2a">
<title>Network Model</title>
<p>In this work, we focus on synchronous discrete-time first-order recurrent neural networks made up of classical McCulloch and Pitts cells. Such a neural network is modelled by a general labelled directed graph. The nodes and labelled edges of the graph respectively represent the cells and synaptic connections of the network. At each time step, the status of each activation cell can be of only two kinds: firing or quiet. When firing, a cell instantaneously transmits an action potential throughout all its outgoing connections, the intensity of which being equal to the label of the underlying connection. Then, a given cell is firing at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e006" xlink:type="simple"/></inline-formula> whenever the summed intensity of all the incoming action potentials transmitted at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e007" xlink:type="simple"/></inline-formula> by both its afferent cells and background activity exceeds its threshold (which we suppose without loss of generality to be equal to 1). The definition of such a network can be formalised as follows:</p>
<p><bold>Definition 1.</bold> <italic>A first-order Boolean recurrent neural network (RNN)</italic> consists of a tuple <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e008" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e009" xlink:type="simple"/></inline-formula> is a finite set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e010" xlink:type="simple"/></inline-formula> activation cells, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e011" xlink:type="simple"/></inline-formula> is a finite set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e012" xlink:type="simple"/></inline-formula> input units, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e013" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e014" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e015" xlink:type="simple"/></inline-formula> are rational matrices describing the weighted synaptic connections between cells, the weighted connections from the input units to the activation cells, and the background activity, respectively.</p>
<p>The activation value of cells <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e016" xlink:type="simple"/></inline-formula> and input units <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e017" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e018" xlink:type="simple"/></inline-formula>, respectively denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e019" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e020" xlink:type="simple"/></inline-formula>, is a Boolean value equal to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e021" xlink:type="simple"/></inline-formula> if the corresponding cell is firing at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e022" xlink:type="simple"/></inline-formula> and equal to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e023" xlink:type="simple"/></inline-formula> otherwise. Given the activation values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e024" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e025" xlink:type="simple"/></inline-formula>, the value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e026" xlink:type="simple"/></inline-formula> is then updated by the following equation<disp-formula id="pone.0094204.e027"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0094204.e027" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e028" xlink:type="simple"/></inline-formula> is the classical Heaviside step function, i.e. a hard-threshold activation function defined by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e029" xlink:type="simple"/></inline-formula> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e030" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e031" xlink:type="simple"/></inline-formula> otherwise.</p>
<p>According to <xref ref-type="disp-formula" rid="pone.0094204.e027">Equation (1)</xref>, the dynamics of the whole network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e032" xlink:type="simple"/></inline-formula> is described by the following governing equation<disp-formula id="pone.0094204.e033"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0094204.e033" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e034" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e035" xlink:type="simple"/></inline-formula> are Boolean vectors describing the spiking configuration of the activation cells and input units, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e036" xlink:type="simple"/></inline-formula> denotes the Heaviside step function applied component by component.</p>
<p>Such Boolean neural networks have already been proven to reveal same computational capabilities as finite state automata <xref ref-type="bibr" rid="pone.0094204-McCulloch1">[1]</xref>–<xref ref-type="bibr" rid="pone.0094204-Minsky1">[3]</xref>. Furthermore, it can be observed that rational- and real-weighted Boolean neural networks are actually computationally equivalent.</p>
<p><bold>Example 1.</bold> Consider the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e037" xlink:type="simple"/></inline-formula> depicted in <xref ref-type="fig" rid="pone-0094204-g001">Figure 1</xref>. The dynamics of this network is then governed by the following system of equation:<disp-formula id="pone.0094204.e038"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0094204.e038" xlink:type="simple"/></disp-formula></p>
<fig id="pone-0094204-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0094204.g001</object-id><label>Figure 1</label><caption>
<title>A simple neural network.</title>
<p>The network is formed by two input units (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e039" xlink:type="simple"/></inline-formula>) and three activation cells (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e040" xlink:type="simple"/></inline-formula>). In this example the synaptic weights are all equal to 1/2, with positive sign corresponding to an excitatory input and a negative sign corresponding to a negative input. Notice that both cells <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e041" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e042" xlink:type="simple"/></inline-formula> receive an excitatory background activity weighing 1/2.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0094204.g001" position="float" xlink:type="simple"/></fig></sec><sec id="s2b">
<title>Attractors</title>
<sec id="s2b1">
<title>Neurophysiological Meaningfulness</title>
<p>In bio-inspired complex systems, the concept of an <italic>attractor</italic> has been shown to carry strong biological and computational implications. According to Kauffman: “Because many complex systems harbour attractors to which the system settle down, the attractors literally are most of what the systems do” <xref ref-type="bibr" rid="pone.0094204-Kauffman1">[36, p. 191]</xref>. The central hypothesis for brain attractors is that, once activated by appropriate activity, network behaviour is maintained by continuous reentry of activity <xref ref-type="bibr" rid="pone.0094204-Abeles1">[37]</xref>, <xref ref-type="bibr" rid="pone.0094204-Amit1">[38]</xref>. This involves strong correlations between neuronal activities in the network and a high incidence of repeating firing patterns therein, being generated by the underlying attractors. Alternative attractors are commonly interpreted as alternative memories <xref ref-type="bibr" rid="pone.0094204-Little1">[39]</xref>–<xref ref-type="bibr" rid="pone.0094204-Knierim1">[46]</xref>.</p>
<p>Certain pathways through the network may be favoured by preferred synaptic interactions between the neurons following developmental and learning processes <xref ref-type="bibr" rid="pone.0094204-Braitenberg1">[47]</xref>–<xref ref-type="bibr" rid="pone.0094204-Iglesias4">[49]</xref>. The plasticity of these phenomena is likely to play a crucial role to shape the <italic>meaningfulness</italic> of an attractor and attractors must be stable at short time scales. Whenever the same information is presented in a network, the same pattern of activity is evoked in a circuit of functionally interconnected neurons, referred to as “cell assembly”. In cell assemblies interconnected in this way, some ordered and precise neurophysiological activity referred to as preferred firing sequences, or spatio-temporal patterns of discharges, may recur above chance levels whenever the same information is presented <xref ref-type="bibr" rid="pone.0094204-Abeles2">[50]</xref>–<xref ref-type="bibr" rid="pone.0094204-Tetko1">[52]</xref>. Recurring firing patterns may be detected without a specific association to a stimulus in large networks of spiking neural networks or during spontaneous activity in electrophysiological recordings <xref ref-type="bibr" rid="pone.0094204-Villa2">[53]</xref>–<xref ref-type="bibr" rid="pone.0094204-Iglesias5">[55]</xref>. These patterns may be viewed as <italic>spurious patterns</italic> generated by <italic>spurious attractors</italic> that are associated with the underlying topology of the network rather than with a specific signal <xref ref-type="bibr" rid="pone.0094204-Iglesias6">[56]</xref>. On the other hand, several examples exist of spatiotemporal firing patterns in behaving animals, from rats to primates <xref ref-type="bibr" rid="pone.0094204-Abeles3">[57]</xref>–<xref ref-type="bibr" rid="pone.0094204-Shmiel1">[61]</xref>, where preferred firing sequences can be associated to specific types of stimuli or behaviours. These can be viewed as <italic>meaningful patterns</italic> associated with <italic>meaningful attractors</italic>. However, meaningfulness cannot be reduced to the detection of a behavioural correlate <xref ref-type="bibr" rid="pone.0094204-Amari1">[62]</xref>–<xref ref-type="bibr" rid="pone.0094204-Freeman1">[64]</xref>. The repeating activity in a network may also be considered meaningful if it allows the activation of neural elements that can be associated to other attractors, thus allowing the build-up of higher order dynamics by means of itinerancy between attractor basins and opening the way to chaotic neural dynamics <xref ref-type="bibr" rid="pone.0094204-Villa1">[51]</xref>, <xref ref-type="bibr" rid="pone.0094204-Freeman2">[65]</xref>–<xref ref-type="bibr" rid="pone.0094204-Fujii1">[70]</xref>.</p>
<p>The dynamics of rather simple Boolean recurrent neural networks can implement an associative memory with bioinspired features <xref ref-type="bibr" rid="pone.0094204-Hopfield2">[71]</xref>, <xref ref-type="bibr" rid="pone.0094204-Watta1">[72]</xref>. In the Hopfield framework, stable equilibria of the network that do not represent any valid configuration of the optimisation problem are referred to as <italic>spurious attractors</italic>. Spurious modes can disappear by “unlearning” <xref ref-type="bibr" rid="pone.0094204-Hopfield2">[71]</xref>, but rational successive memory recall can actually be implemented by triggering spurious modes and achieving meaningful memory storage <xref ref-type="bibr" rid="pone.0094204-Tsuda1">[66]</xref>, <xref ref-type="bibr" rid="pone.0094204-Amit3">[73]</xref>–<xref ref-type="bibr" rid="pone.0094204-Knoblauch1">[77]</xref>. In this paper, the notions of attractors, meaningful attractors, and spurious attractors are reformulated in our precise Boolean network context. Networks will then be classified according to their ability to alternate between different types of attractive behaviours. For this purpose, the following definitions need to be introduced.</p>
</sec><sec id="s2b2">
<title>Formal Definitions</title>
<p>As preliminary notations, for any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e043" xlink:type="simple"/></inline-formula>, the space of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e044" xlink:type="simple"/></inline-formula>-dimensional Boolean vectors is denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e045" xlink:type="simple"/></inline-formula>. For any vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e046" xlink:type="simple"/></inline-formula> and any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e047" xlink:type="simple"/></inline-formula>, the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e048" xlink:type="simple"/></inline-formula>-th component of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e049" xlink:type="simple"/></inline-formula> is denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e050" xlink:type="simple"/></inline-formula>. Moreover, the spaces of finite and infinite sequences of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e051" xlink:type="simple"/></inline-formula>-dimensional Boolean vectors are denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e052" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e053" xlink:type="simple"/></inline-formula>, respectively. Any finite sequence of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e054" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e055" xlink:type="simple"/></inline-formula> will be denoted by an expression of the form <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e056" xlink:type="simple"/></inline-formula>, and any infinite sequence of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e057" xlink:type="simple"/></inline-formula> by an expression of the form <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e058" xlink:type="simple"/></inline-formula>, where each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e059" xlink:type="simple"/></inline-formula>. For any finite sequence of Boolean vectors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e060" xlink:type="simple"/></inline-formula>, we let the expression <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e061" xlink:type="simple"/></inline-formula> denote the infinite sequence obtained by infinitely many consecutive concatenations of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e062" xlink:type="simple"/></inline-formula>, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e063" xlink:type="simple"/></inline-formula>.</p>
<p>Now, let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e064" xlink:type="simple"/></inline-formula> be some network with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e065" xlink:type="simple"/></inline-formula> activation cells and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e066" xlink:type="simple"/></inline-formula> input units. For each time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e067" xlink:type="simple"/></inline-formula>, the Boolean vectors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e068" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e069" xlink:type="simple"/></inline-formula> describing the spiking configurations of both the activation cells and input units of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e070" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e071" xlink:type="simple"/></inline-formula> are called the <italic>state</italic> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e072" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e073" xlink:type="simple"/></inline-formula> and the <italic>input</italic> submitted to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e074" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e075" xlink:type="simple"/></inline-formula>, respectively. An infinite <italic>input stream</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e076" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e077" xlink:type="simple"/></inline-formula> is then defined as an infinite sequence of consecutive inputs, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e078" xlink:type="simple"/></inline-formula>. Now, assuming the initial state of the network to be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e079" xlink:type="simple"/></inline-formula>, any infinite input stream <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e080" xlink:type="simple"/></inline-formula> induces via <xref ref-type="disp-formula" rid="pone.0094204.e033">Equation (2)</xref> an infinite sequence of consecutive states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e081" xlink:type="simple"/></inline-formula> called the <italic>evolution</italic> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e082" xlink:type="simple"/></inline-formula> induced by the input stream <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e083" xlink:type="simple"/></inline-formula>.</p>
<p>Note that the set of all possible distinct states of a given Boolean network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e084" xlink:type="simple"/></inline-formula> is always finite; indeed, if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e085" xlink:type="simple"/></inline-formula> possesses <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e086" xlink:type="simple"/></inline-formula> activation cells, then there are at most <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e087" xlink:type="simple"/></inline-formula> distinct possible states of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e088" xlink:type="simple"/></inline-formula>. Hence, any infinite evolution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e089" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e090" xlink:type="simple"/></inline-formula> consists of an infinite sequence of only finitely many distinct states. Therefore, in any evolution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e091" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e092" xlink:type="simple"/></inline-formula>, there necessarily exists at least one state that recurs infinitely many times in the infinite sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e093" xlink:type="simple"/></inline-formula>, irrespective of the fact that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e094" xlink:type="simple"/></inline-formula> is periodic or not. The non-empty set of all such states that recurs infinitely often in the evolution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e095" xlink:type="simple"/></inline-formula> will be denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e096" xlink:type="simple"/></inline-formula>.</p>
<p>By definition, every state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e097" xlink:type="simple"/></inline-formula> that is visited only finitely often in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e098" xlink:type="simple"/></inline-formula> will no longer occur in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e099" xlink:type="simple"/></inline-formula> after some time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e100" xlink:type="simple"/></inline-formula>. By taking the maximum of these time steps <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e101" xlink:type="simple"/></inline-formula>, we obtain a global time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e102" xlink:type="simple"/></inline-formula> such that all states of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e103" xlink:type="simple"/></inline-formula> occurring after time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e104" xlink:type="simple"/></inline-formula> will necessarily repeat infinitely often in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e105" xlink:type="simple"/></inline-formula>. Formally, there necessarily exists an index <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e106" xlink:type="simple"/></inline-formula> such that, for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e107" xlink:type="simple"/></inline-formula>, one has <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e108" xlink:type="simple"/></inline-formula>. It is important to note that the reoccurrence of the states belonging to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e109" xlink:type="simple"/></inline-formula> after time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e110" xlink:type="simple"/></inline-formula> does not necessarily occur in a periodic manner during the evolution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e111" xlink:type="simple"/></inline-formula>. Therefore, any evolution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e112" xlink:type="simple"/></inline-formula> consists of a possibly empty prefix of successive states that repeat only finite many times, followed by an infinite suffix of successive states that repeat infinitely often, yet not necessarily in a periodic way. A set of states of the form <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e113" xlink:type="simple"/></inline-formula> for some evolution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e114" xlink:type="simple"/></inline-formula> is commonly called an <italic>attractor</italic> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e115" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pone.0094204-Kauffman1">[36]</xref>. A precise definition can be given as follows:</p>
<p><bold>Definition 2.</bold> Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e116" xlink:type="simple"/></inline-formula> be some Boolean neural network with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e117" xlink:type="simple"/></inline-formula> activation cells. A set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e118" xlink:type="simple"/></inline-formula> is called an attractor for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e119" xlink:type="simple"/></inline-formula> if there exists an input stream <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e120" xlink:type="simple"/></inline-formula> such that the corresponding evolution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e121" xlink:type="simple"/></inline-formula> satisfies <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e122" xlink:type="simple"/></inline-formula>.</p>
<p>In other words, an attractor of a Boolean neural network is a set of states such that the behaviour of the network could eventually become forever confined to that set of states. In this sense, the definition of an attractor requires the infinite input stream context to be properly formulated.</p>
<p>In this work, we suppose that attractors can only be of two distinct types, namely either <italic>meaningful</italic> or <italic>spurious</italic>. For instance, the type of each attractor could be determined by its topological features or by its neurophysiological significance with respect to measurable observations associated with certain behaviours or sensory discriminations (see Section “Neurophysiological Meaningfulness” above). From this point onwards, any given network is assumed to be provided with a corresponding classification of all of its attractors into meaningful and spurious types. Further discussions about the attribution of the attractors to either types will be addressed in the forthcoming sections.</p>
<p>An infinite input stream <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e123" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e124" xlink:type="simple"/></inline-formula> is called <italic>meaningful</italic> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e125" xlink:type="simple"/></inline-formula> is a meaningful attractor, and it is called <italic>spurious</italic> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e126" xlink:type="simple"/></inline-formula> is a spurious attractor. In other words, an input stream is called meaningful (respectively spurious) if the network dynamics induced by this input stream will eventually become confined into some meaningful (respectively spurious) attractor. Then, the set of all meaningful input streams of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e127" xlink:type="simple"/></inline-formula> is called the <italic>neural language</italic> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e128" xlink:type="simple"/></inline-formula> and is denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e129" xlink:type="simple"/></inline-formula>. Finally, an arbitrary set of input streams <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e130" xlink:type="simple"/></inline-formula> is said to be <italic>recognisable</italic> by some Boolean neural network if there exists a network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e131" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e132" xlink:type="simple"/></inline-formula>.</p>
<p>Besides, if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e133" xlink:type="simple"/></inline-formula> denotes some Boolean neural network provided with an additional specification of the type of each of its attractors, then the <italic>complementary</italic> network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e134" xlink:type="simple"/></inline-formula> is defined to be the same network as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e135" xlink:type="simple"/></inline-formula> yet with a completely opposite type specification of its attractors. Then, an attractor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e136" xlink:type="simple"/></inline-formula> is meaningful for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e137" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e138" xlink:type="simple"/></inline-formula> is a spurious attractor for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e139" xlink:type="simple"/></inline-formula> and one has <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e140" xlink:type="simple"/></inline-formula>. All preceding definitions are illustrated by the next Example 2.</p>
<p><bold>Example 2.</bold> Let us consider the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e141" xlink:type="simple"/></inline-formula> described in Example 1 and illustrated in <xref ref-type="fig" rid="pone-0094204-g001">Figure 1</xref>. Let us further assume that the network state where the three cells <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e142" xlink:type="simple"/></inline-formula> simultaneously fire determines the meaningfulness of the attractors of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e143" xlink:type="simple"/></inline-formula>. In other words, the meaningful attractors of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e144" xlink:type="simple"/></inline-formula> are precisely those containing the state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e145" xlink:type="simple"/></inline-formula>; all other attractors are assumed to be spurious.</p>
<p>Let us consider the periodic input stream <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e146" xlink:type="simple"/></inline-formula> and its corresponding evolution<disp-formula id="pone.0094204.e147"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0094204.e147" xlink:type="simple"/></disp-formula>From time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e148" xlink:type="simple"/></inline-formula>, the evolution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e149" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e150" xlink:type="simple"/></inline-formula> remains confined in a cyclic visit of the states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e151" xlink:type="simple"/></inline-formula>. Thence, the set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e152" xlink:type="simple"/></inline-formula> is an attractor of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e153" xlink:type="simple"/></inline-formula>. Moreover, since the state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e154" xlink:type="simple"/></inline-formula> does not belong to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e155" xlink:type="simple"/></inline-formula>, the attractor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e156" xlink:type="simple"/></inline-formula> is spurious. Therefore, the input stream <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e157" xlink:type="simple"/></inline-formula> is also spurious, and hence does not belong to the neural language of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e158" xlink:type="simple"/></inline-formula>, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e159" xlink:type="simple"/></inline-formula>.</p>
<p>Let us consider another periodic input stream <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e160" xlink:type="simple"/></inline-formula> and its corresponding evolution<disp-formula id="pone.0094204.e161"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0094204.e161" xlink:type="simple"/></disp-formula>The set of states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e162" xlink:type="simple"/></inline-formula> is an attractor, and the evolution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e163" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e164" xlink:type="simple"/></inline-formula> is confined in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e165" xlink:type="simple"/></inline-formula> already from the very first time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e166" xlink:type="simple"/></inline-formula>. Yet in this case, since the Boolean vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e167" xlink:type="simple"/></inline-formula> belongs to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e168" xlink:type="simple"/></inline-formula>, the attractor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e169" xlink:type="simple"/></inline-formula> is meaningful. It follows that the input stream <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e170" xlink:type="simple"/></inline-formula> is also meaningful, and thus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e171" xlink:type="simple"/></inline-formula>.</p>
</sec></sec><sec id="s2c">
<title><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e172" xlink:type="simple"/></inline-formula>-Automata</title>
<sec id="s2c1">
<title>Büchi Automata</title>
<p>A <italic>finite deterministic Büchi automaton</italic> <xref ref-type="bibr" rid="pone.0094204-Bchi1">[34]</xref> is a 5-tuple <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e173" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e174" xlink:type="simple"/></inline-formula> is a finite set called the set of states, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e175" xlink:type="simple"/></inline-formula> is a finite alphabet, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e176" xlink:type="simple"/></inline-formula> is an element of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e177" xlink:type="simple"/></inline-formula> called the initial state, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e178" xlink:type="simple"/></inline-formula> is a partial function from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e179" xlink:type="simple"/></inline-formula> into <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e180" xlink:type="simple"/></inline-formula> called the transition function, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e181" xlink:type="simple"/></inline-formula> is a subset of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e182" xlink:type="simple"/></inline-formula> called the set of final states. A finite deterministic Büchi automaton is generally represented as a directed labelled graph whose nodes and labelled edges correspond to the states and transitions of the automaton, respectively.</p>
<p>Given some finite deterministic Büchi automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e183" xlink:type="simple"/></inline-formula>, every triple <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e184" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e185" xlink:type="simple"/></inline-formula> is called a <italic>transition</italic> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e186" xlink:type="simple"/></inline-formula>. Then, a <italic>path</italic> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e187" xlink:type="simple"/></inline-formula> is a sequence of consecutive transitions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e188" xlink:type="simple"/></inline-formula>, also denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e189" xlink:type="simple"/></inline-formula>. The path <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e190" xlink:type="simple"/></inline-formula> is said to successively <italic>visit</italic> the states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e191" xlink:type="simple"/></inline-formula> and the word <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e192" xlink:type="simple"/></inline-formula> is the <italic>label</italic> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e193" xlink:type="simple"/></inline-formula>. The state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e194" xlink:type="simple"/></inline-formula> is called the <italic>origin</italic> of path <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e195" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e196" xlink:type="simple"/></inline-formula> is said to be <italic>initial</italic> if its starting state is initial, i.e. if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e197" xlink:type="simple"/></inline-formula>. If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e198" xlink:type="simple"/></inline-formula> is an infinite path, the set of states visited infinitely many times by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e199" xlink:type="simple"/></inline-formula> is denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e200" xlink:type="simple"/></inline-formula>.</p>
<p>An infinite initial path <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e201" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e202" xlink:type="simple"/></inline-formula> is said to be <italic>successful</italic> if it visits at least one of the final states infinitely often, i.e. if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e203" xlink:type="simple"/></inline-formula>. An infinite word is then said to be <italic>recognised</italic> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e204" xlink:type="simple"/></inline-formula> if it is the label of a successful infinite path in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e205" xlink:type="simple"/></inline-formula>. The <italic>language recognised by </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e206" xlink:type="simple"/></inline-formula>, denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e207" xlink:type="simple"/></inline-formula>, is the set of all infinite words recognised by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e208" xlink:type="simple"/></inline-formula>.</p>
<p>A <italic>cycle</italic> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e209" xlink:type="simple"/></inline-formula> consists of a finite set of states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e210" xlink:type="simple"/></inline-formula> such that there exists a finite path in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e211" xlink:type="simple"/></inline-formula> with same initial and final state and visiting precisely all states of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e212" xlink:type="simple"/></inline-formula>. A cycle <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e213" xlink:type="simple"/></inline-formula> is said to be <italic>accessible</italic> from cycle <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e214" xlink:type="simple"/></inline-formula> if there exists a path from some state of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e215" xlink:type="simple"/></inline-formula> to some state of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e216" xlink:type="simple"/></inline-formula>. Furthermore, a cycle is called <italic>successful</italic> if it contains a state belonging to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e217" xlink:type="simple"/></inline-formula>, and <italic>non-successful</italic> otherwise.</p>
<p>An <italic>alternating chain of length </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e218" xlink:type="simple"/></inline-formula> (respectively <italic>co-alternating chain of length </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e219" xlink:type="simple"/></inline-formula>) is a finite sequence of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e220" xlink:type="simple"/></inline-formula> distinct cycles <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e221" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e222" xlink:type="simple"/></inline-formula> is successful (resp. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e223" xlink:type="simple"/></inline-formula> is non-successful), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e224" xlink:type="simple"/></inline-formula> is successful iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e225" xlink:type="simple"/></inline-formula> is non-successful, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e226" xlink:type="simple"/></inline-formula> is accessible from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e227" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e228" xlink:type="simple"/></inline-formula> is not accessible from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e229" xlink:type="simple"/></inline-formula>, for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e230" xlink:type="simple"/></inline-formula>. An <italic>alternating chain of length </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e231" xlink:type="simple"/></inline-formula> is a sequence of two cycles <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e232" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e233" xlink:type="simple"/></inline-formula> is successful, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e234" xlink:type="simple"/></inline-formula> is non-successful, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e235" xlink:type="simple"/></inline-formula> is accessible from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e236" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e237" xlink:type="simple"/></inline-formula> is also accessible from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e238" xlink:type="simple"/></inline-formula> (we recall that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e239" xlink:type="simple"/></inline-formula> denotes the least infinite ordinal). In this case, cycles <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e240" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e241" xlink:type="simple"/></inline-formula> are said to communicate. For any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e242" xlink:type="simple"/></inline-formula>, an alternating chain of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e243" xlink:type="simple"/></inline-formula> is said to be <italic>maximal</italic> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e244" xlink:type="simple"/></inline-formula> if there is no alternating chain and no co-alternating chain in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e245" xlink:type="simple"/></inline-formula> with a length strictly larger than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e246" xlink:type="simple"/></inline-formula>. A co-alternating chain of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e247" xlink:type="simple"/></inline-formula> is said to be maximal in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e248" xlink:type="simple"/></inline-formula> if exactly the same condition holds.</p>
<p>The above definitions are illustrated by the Example S1 and <xref ref-type="supplementary-material" rid="pone.0094204.s001">Figure S1 in File S1</xref>.</p>
</sec><sec id="s2c2">
<title>Muller Automata</title>
<p>A <italic>finite deterministic Muller automaton</italic> is a 5-tuple <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e249" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e250" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e251" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e252" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e253" xlink:type="simple"/></inline-formula> are defined exactly like for deterministic Büchi automata, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e254" xlink:type="simple"/></inline-formula> is a set of states' sets called the <italic>table of the automaton</italic>. The notions of transition and path are defined as for deterministic Büchi automata. An infinite initial path <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e255" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e256" xlink:type="simple"/></inline-formula> is now called <italic>successful</italic> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e257" xlink:type="simple"/></inline-formula>. Given a finite deterministic Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e258" xlink:type="simple"/></inline-formula>, a cycle in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e259" xlink:type="simple"/></inline-formula> is called <italic>successful</italic> if it belongs to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e260" xlink:type="simple"/></inline-formula>, and <italic>non-succesful</italic> otherwise. An infinite word is then said to be <italic>recognised</italic> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e261" xlink:type="simple"/></inline-formula> if it is the label of a successful infinite path in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e262" xlink:type="simple"/></inline-formula>, and the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e263" xlink:type="simple"/></inline-formula><italic>-language recognised by </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e264" xlink:type="simple"/></inline-formula>, denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e265" xlink:type="simple"/></inline-formula>, is defined as the set of all infinite words recognised by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e266" xlink:type="simple"/></inline-formula>. The class of all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e267" xlink:type="simple"/></inline-formula>-languages recognisable by some deterministic Muller automata is precisely the class of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e268" xlink:type="simple"/></inline-formula><italic>-rational languages</italic> <xref ref-type="bibr" rid="pone.0094204-Perrin1">[79]</xref>.</p>
<p>It can be shown that deterministic Muller automata are strictly more powerful than deterministic Büchi automata, but have an equivalent expressive power as non-deterministic Büchi automata, Rabin automata, Street automata, parity automata, and non-deterministic Muller automata <xref ref-type="bibr" rid="pone.0094204-Piterman1">[81]</xref>.</p>
<p>For each ordinal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e269" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e270" xlink:type="simple"/></inline-formula>, we introduce the concept of an <italic>alternating tree</italic> of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e271" xlink:type="simple"/></inline-formula> in a deterministic Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e272" xlink:type="simple"/></inline-formula>, which consists of a tree-like disposition of the successful and non-successful cycles of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e273" xlink:type="simple"/></inline-formula> induced by the ordinal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e274" xlink:type="simple"/></inline-formula>, as illustrated in <xref ref-type="fig" rid="pone-0094204-g002">Figure 2</xref>. In order to describe this tree-like disposition, we first recall that any ordinal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e275" xlink:type="simple"/></inline-formula> can uniquely be written of the form <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e276" xlink:type="simple"/></inline-formula>, for some <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e277" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e278" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e279" xlink:type="simple"/></inline-formula>. Then, given some deterministic Muller automata <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e280" xlink:type="simple"/></inline-formula> and some strictly positive ordinal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e281" xlink:type="simple"/></inline-formula>, an <italic>alternating tree</italic> (respectively <italic>co-alternating tree</italic>) of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e282" xlink:type="simple"/></inline-formula> is a sequence of cycles of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e283" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e284" xlink:type="simple"/></inline-formula> such that:</p>
<fig id="pone-0094204-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0094204.g002</object-id><label>Figure 2</label><caption>
<title>An alternating tree of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e298" xlink:type="simple"/></inline-formula>, for some ordinal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e299" xlink:type="simple"/></inline-formula>.</title>
<p>Illustration of the inclusion and accessibility relations between cycles forming an alternating tree of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e300" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0094204.g002" position="float" xlink:type="simple"/></fig>
<list list-type="roman-lower"><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e285" xlink:type="simple"/></inline-formula> is successful (respectively non-successful);</p>
</list-item><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e286" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e287" xlink:type="simple"/></inline-formula> is successful iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e288" xlink:type="simple"/></inline-formula> is non-successful;</p>
</list-item><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e289" xlink:type="simple"/></inline-formula> is accessible from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e290" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e291" xlink:type="simple"/></inline-formula> is successful iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e292" xlink:type="simple"/></inline-formula> is non-successful;</p>
</list-item><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e293" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e294" xlink:type="simple"/></inline-formula> are both accessible from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e295" xlink:type="simple"/></inline-formula>, and each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e296" xlink:type="simple"/></inline-formula> is successful whereas each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e297" xlink:type="simple"/></inline-formula> is non-successful.</p>
</list-item></list>
<p>An alternating tree of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e301" xlink:type="simple"/></inline-formula> is said to be maximal in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e302" xlink:type="simple"/></inline-formula> if there is no alternating or co-altenrating tree in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e303" xlink:type="simple"/></inline-formula> of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e304" xlink:type="simple"/></inline-formula>. A co-alternating tree of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e305" xlink:type="simple"/></inline-formula> is said to be maximal in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e306" xlink:type="simple"/></inline-formula> if exactly the same condition holds. An alternating tree of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e307" xlink:type="simple"/></inline-formula> is illustrated in <xref ref-type="fig" rid="pone-0094204-g002">Figure 2</xref>.</p>
<p>The above definitions are illustrated by the Example S2 and <xref ref-type="supplementary-material" rid="pone.0094204.s002">Figure S2 in File S2</xref>.</p>
</sec></sec></sec><sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Hierarchical Classification of Neural Networks</title>
<p>Our notion of an attractor refers to a set of states such that the behaviour of the network could forever be confined into that set of states. In other words, an attractor corresponds to a cyclic behaviour of the network produced by an infinite input stream. According to these considerations, we provide a generalisation to this precise infinite input stream context of the classical equivalence result between Boolean neural networks and finite state automata <xref ref-type="bibr" rid="pone.0094204-McCulloch1">[1]</xref>–<xref ref-type="bibr" rid="pone.0094204-Minsky1">[3]</xref>. More precisely, we show that, under some natural specific conditions on the specification of the type of their attractors, Boolean recurrent neural networks express the very same expressive power as deterministic Büchi automata. This equivalence result enables us to establish a hierarchical classification of neural networks by translating the Wagner classification theory from the Büchi automaton to the neural network context <xref ref-type="bibr" rid="pone.0094204-Wagner1">[35]</xref>. The obtained classification is intimately related to the attractive properties of the neural networks, and hence provides a new refined measurement of the computational power of Boolean neural networks in terms of their attractive behaviours.</p>
<sec id="s3a1">
<title>Boolean Recurrent Neural Networks and Büchi Automata</title>
<p>We now prove that, under some natural conditions, Boolean recurrent neural networks are computationally equivalent to deterministic Büchi automata. Towards this purpose, we consider that the neural networks include selected elements belonging to an output layer. The activation of the output layer communicates the output of the system to the environment.</p>
<p>Formally, let us consider a recurrent neural network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e308" xlink:type="simple"/></inline-formula>, as described in Definition 1, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e309" xlink:type="simple"/></inline-formula> activation cells and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e310" xlink:type="simple"/></inline-formula> input units. In addition, let us assume that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e311" xlink:type="simple"/></inline-formula> cells chosen among the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e312" xlink:type="simple"/></inline-formula> activation cells form the <italic>output layer</italic> of the neural network, denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e313" xlink:type="simple"/></inline-formula>. For graphical purpose, the activation cells of the output layer are represented as double-circled nodes in the next figures. Thus, a recurrent neural network is now defined by a tuple <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e314" xlink:type="simple"/></inline-formula>. Let us assume also that the specification type of the attractors of a network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e315" xlink:type="simple"/></inline-formula> is naturally related to its output layer as follows: an attractor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e316" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e317" xlink:type="simple"/></inline-formula> is considered <italic>meaningful</italic> if it contains at least one state where some output cell is spiking, i.e. if there exist <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e318" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e319" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e320" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e321" xlink:type="simple"/></inline-formula>; the attractor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e322" xlink:type="simple"/></inline-formula> is considered <italic>spurious</italic> otherwise. According to these assumptions, meaningful attractors refer to the cyclic behaviours of the network that induce some response activity of the system via its output layer, whereas spurious attractors refer to the cyclic behaviours of the system that do not evoke any response at all of the output layer.</p>
<p>It can be stated that the expressive powers of Boolean recurrent neural networks and deterministic Büchi automaton are equivalent. As a first step towards this result, the following proposition shows that any Boolean recurrent neural network can be simulated by some deterministic Büchi automaton.</p>
<p><bold>Proposition 1.</bold> <italic>Let </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e323" xlink:type="simple"/></inline-formula><italic> be some Boolean recurrent neural network provided with an output layer. Then there exists a deterministic Büchi automaton </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e324" xlink:type="simple"/></inline-formula><italic> such that </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e325" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<p><italic>Proof.</italic> Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e326" xlink:type="simple"/></inline-formula> be some neural network given by the tuple <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e327" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e328" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e329" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e330" xlink:type="simple"/></inline-formula>. Consider the deterministic Büchi automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e331" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e332" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e333" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e334" xlink:type="simple"/></inline-formula> is the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e335" xlink:type="simple"/></inline-formula>-dimensional zero vector, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e336" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e337" xlink:type="simple"/></inline-formula> is the function defined by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e338" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e339" xlink:type="simple"/></inline-formula>. Note that the complexity of the transformation is exponential, since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e340" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e341" xlink:type="simple"/></inline-formula>.</p>
<p>According to this construction, any infinite evolution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e342" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e343" xlink:type="simple"/></inline-formula> naturally induces a corresponding infinite initial path <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e344" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e345" xlink:type="simple"/></inline-formula>. Moreover, by the definitions of meaningful and spurious attractors of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e346" xlink:type="simple"/></inline-formula>, an infinite input stream <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e347" xlink:type="simple"/></inline-formula> is meaningful for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e348" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e349" xlink:type="simple"/></inline-formula> is recognised by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e350" xlink:type="simple"/></inline-formula>. In other words, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e351" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e352" xlink:type="simple"/></inline-formula>, and therefore <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e353" xlink:type="simple"/></inline-formula>.</p>
<p>According to the construction given in the proof of Proposition 1, any infinite evolution of the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e354" xlink:type="simple"/></inline-formula> is naturally associated with a corresponding infinite initial path in the automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e355" xlink:type="simple"/></inline-formula>, and conversely, any infinite initial path in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e356" xlink:type="simple"/></inline-formula> corresponds to some possible infinite evolution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e357" xlink:type="simple"/></inline-formula>. Consequently, there is a biunivocal correspondence between the <italic>attractors</italic> of the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e358" xlink:type="simple"/></inline-formula> and the <italic>cycles</italic> in the graph of the corresponding Büchi automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e359" xlink:type="simple"/></inline-formula>. As a result, a procedure to compute all possible attractors of a given network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e360" xlink:type="simple"/></inline-formula> is obtained by firstly constructing the corresponding deterministic Büchi automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e361" xlink:type="simple"/></inline-formula> and secondly listing all cycles in the graph of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e362" xlink:type="simple"/></inline-formula>.</p>
<p>As a second step towards the equivalence result, we prove now that any deterministic Büchi automaton can be simulated by some Boolean recurrent neural network.</p>
<p><bold>Proposition 2.</bold> <italic>Let </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e363" xlink:type="simple"/></inline-formula><italic> be some deterministic Büchi automaton over the alphabet </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e364" xlink:type="simple"/></inline-formula><italic>, with </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e365" xlink:type="simple"/></inline-formula><italic>. Then there exists a Boolean recurrent neural network </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e366" xlink:type="simple"/></inline-formula><italic> provided with an output layer such that </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e367" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<p><italic>Proof.</italic> Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e368" xlink:type="simple"/></inline-formula> be some deterministic Büchi automaton over alphabet <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e369" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e370" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e371" xlink:type="simple"/></inline-formula>. Consider the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e372" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e373" xlink:type="simple"/></inline-formula> cells given as follows: firstly, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e374" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e375" xlink:type="simple"/></inline-formula> is decomposed into a set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e376" xlink:type="simple"/></inline-formula> “letter cells” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e377" xlink:type="simple"/></inline-formula>, a “delay-cell” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e378" xlink:type="simple"/></inline-formula>, and a set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e379" xlink:type="simple"/></inline-formula> “state cells” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e380" xlink:type="simple"/></inline-formula>; secondly, the set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e381" xlink:type="simple"/></inline-formula> “input units” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e382" xlink:type="simple"/></inline-formula>, and thirdly, the outptut layer <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e383" xlink:type="simple"/></inline-formula>. The idea of the simulation is that the “letter cells” and “state cells” of the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e384" xlink:type="simple"/></inline-formula> simulate the letters and states currently read and entered by the automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e385" xlink:type="simple"/></inline-formula>, respectively.</p>
<p>Towards this purpose, the weight matrices <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e386" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e387" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e388" xlink:type="simple"/></inline-formula> are described as follows. Concerning the matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e389" xlink:type="simple"/></inline-formula>: for any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e390" xlink:type="simple"/></inline-formula>, we consider the binary decomposition of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e391" xlink:type="simple"/></inline-formula>, namely <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e392" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e393" xlink:type="simple"/></inline-formula>, and for any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e394" xlink:type="simple"/></inline-formula>, we set the weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e395" xlink:type="simple"/></inline-formula>; for all other <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e396" xlink:type="simple"/></inline-formula>, we set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e397" xlink:type="simple"/></inline-formula>, for any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e398" xlink:type="simple"/></inline-formula>. Concerning the matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e399" xlink:type="simple"/></inline-formula>: for any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e400" xlink:type="simple"/></inline-formula>, we set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e401" xlink:type="simple"/></inline-formula>; we also set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e402" xlink:type="simple"/></inline-formula>; for all other <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e403" xlink:type="simple"/></inline-formula>, we set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e404" xlink:type="simple"/></inline-formula>. Concerning the matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e405" xlink:type="simple"/></inline-formula>: we set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e406" xlink:type="simple"/></inline-formula>, and for any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e407" xlink:type="simple"/></inline-formula> and any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e408" xlink:type="simple"/></inline-formula>, we set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e409" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e410" xlink:type="simple"/></inline-formula> is a transition of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e411" xlink:type="simple"/></inline-formula>; otherwise, for any pair of indices <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e412" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e413" xlink:type="simple"/></inline-formula> has not been set to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e414" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e415" xlink:type="simple"/></inline-formula>, we set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e416" xlink:type="simple"/></inline-formula>. This construction is illustrated in <xref ref-type="fig" rid="pone-0094204-g003">Figure 3</xref>.</p>
<fig id="pone-0094204-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0094204.g003</object-id><label>Figure 3</label><caption>
<title>The network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e417" xlink:type="simple"/></inline-formula> described in the proof of Proposition 2.</title>
<p>The network is characterised by a set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e418" xlink:type="simple"/></inline-formula> input cells <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e419" xlink:type="simple"/></inline-formula> reading the alphabet <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e420" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e421" xlink:type="simple"/></inline-formula> “letter cells” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e422" xlink:type="simple"/></inline-formula>, a “delay-cell” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e423" xlink:type="simple"/></inline-formula>, and a set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e424" xlink:type="simple"/></inline-formula> “state cells” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e425" xlink:type="simple"/></inline-formula>. The idea of the simulation is that the “letter cells” and “state cells” of the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e426" xlink:type="simple"/></inline-formula> simulate the letters and states currently read and entered by the automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e427" xlink:type="simple"/></inline-formula>, respectively. In this illustration, we assume that the binary decomposition of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e428" xlink:type="simple"/></inline-formula> is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e429" xlink:type="simple"/></inline-formula>, so that the “letter cell” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e430" xlink:type="simple"/></inline-formula> receives synaptic connections of intensities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e431" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e432" xlink:type="simple"/></inline-formula> from input cells <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e433" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e434" xlink:type="simple"/></inline-formula>, respectively, and it receives synaptic connections of intensities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e435" xlink:type="simple"/></inline-formula> from any other input cells. Consequently, the “letter cell” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e436" xlink:type="simple"/></inline-formula> becomes active at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e437" xlink:type="simple"/></inline-formula> iff the sole input cells <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e438" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e439" xlink:type="simple"/></inline-formula> are active at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e440" xlink:type="simple"/></inline-formula>. The synaptic connections to other “letter cells” are not illustrated. Moreover, the synaptic connections <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e441" xlink:type="simple"/></inline-formula> model the transition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e442" xlink:type="simple"/></inline-formula> of automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e443" xlink:type="simple"/></inline-formula>. The synaptic connections modelling other transitions are not illustrated.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0094204.g003" position="float" xlink:type="simple"/></fig>
<p>According to this construction, if we let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e444" xlink:type="simple"/></inline-formula> denote the boolean vector whose components are the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e445" xlink:type="simple"/></inline-formula>'s (for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e446" xlink:type="simple"/></inline-formula>), one has that the “letter cell” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e447" xlink:type="simple"/></inline-formula> will spike at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e448" xlink:type="simple"/></inline-formula> iff the input vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e449" xlink:type="simple"/></inline-formula> is received at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e450" xlink:type="simple"/></inline-formula>. Moreover, at every time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e451" xlink:type="simple"/></inline-formula>, a unique “letter cell” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e452" xlink:type="simple"/></inline-formula> and “state cell” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e453" xlink:type="simple"/></inline-formula> are spiking, and, if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e454" xlink:type="simple"/></inline-formula> performs the transition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e455" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e456" xlink:type="simple"/></inline-formula>, then network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e457" xlink:type="simple"/></inline-formula> evokes the spiking pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e458" xlink:type="simple"/></inline-formula>. The relation between the final states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e459" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e460" xlink:type="simple"/></inline-formula> and the output layer <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e461" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e462" xlink:type="simple"/></inline-formula> ensures that any infinite input stream <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e463" xlink:type="simple"/></inline-formula> is recognised by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e464" xlink:type="simple"/></inline-formula> if and only if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e465" xlink:type="simple"/></inline-formula> is meaningful for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e466" xlink:type="simple"/></inline-formula>. Therefore, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e467" xlink:type="simple"/></inline-formula>.</p>
<p>The proof of Proposition 2 can be generalised to any network dynamics driven by unate local transition functions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e468" xlink:type="simple"/></inline-formula>, for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e469" xlink:type="simple"/></inline-formula>, rather than by the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e470" xlink:type="simple"/></inline-formula> threshold local transition functions defined by <xref ref-type="disp-formula" rid="pone.0094204.e027">Equation 1</xref>. Since unate functions are a generalisation of threshold functions, this proof can be interesting in the broader context of switching theory.</p>
<p>Propositions 1 and 2 yield to the following equivalence between recurrent neural networks and deterministic Büchi automata.</p>
<p><bold>Theorem 1.</bold> <italic>Let </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e471" xlink:type="simple"/></inline-formula><italic> for some </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e472" xlink:type="simple"/></inline-formula><italic>. Then </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e473" xlink:type="simple"/></inline-formula><italic> is recognisable by some Boolean recurrent neural network provided with an output layer iff </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e474" xlink:type="simple"/></inline-formula><italic> is recognisable by some deterministic Büchi automaton.</italic></p>
<p><italic>Proof.</italic> Proposition 1 shows that every language recognisable by some Boolean recurrent neural network is also recognisable by some deterministic Büchi automaton. Conversely, Proposition 2 shows that every language recognisable by some deterministic Büchi automaton is also recognisable by some Boolean recurrent neural network.</p>
<p>The two procedures given in the proofs of propositions 1 and 2 are illustrated by the Example S3 and <xref ref-type="supplementary-material" rid="pone.0094204.s003">Figure S3 in File S3</xref>.</p>
</sec><sec id="s3a2">
<title>RNN Hierarchy</title>
<p>In the theory of infinite word reading machines, abstract devices are commonly classified according to the topological complexity of their underlying <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e475" xlink:type="simple"/></inline-formula>-language (i.e., the languages of infinite words that they recognise). Such classifications provide an interesting measurement of the expressive power of various kinds of infinite word reading machines. In this context, the most refined hierarchical classification of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e476" xlink:type="simple"/></inline-formula>-automata – or equivalently, of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e477" xlink:type="simple"/></inline-formula>-rational languages – is the so-called <italic>Wagner hierarchy</italic> <xref ref-type="bibr" rid="pone.0094204-Wagner1">[35]</xref>.</p>
<p>Here, this classification approach is translated from the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e478" xlink:type="simple"/></inline-formula>-automaton to the neural network context. More precisely, according to the equivalence given by Theorem 1, the Wagner hierarchy can naturally be translated from Büchi automata to Boolean neural networks. As a result, a hierarchical classification of first-order Boolean recurrent neural networks is obtained. Interestingly, the obtained classification is tightly related to the attractive properties of the networks, and, more precisely, refers to the ability of the networks to switch between meaningful and spurious attractive behaviours along their evolutions. Hence, the obtained hierarchical classification provides a new measurement of complexity of neural networks associated with their abilities to switch between different types of attractors along their evolutions.</p>
<p>As a first step, the following facts and definitions need to be introduced. To begin with, for any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e479" xlink:type="simple"/></inline-formula>, the space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e480" xlink:type="simple"/></inline-formula> can naturally be equipped with the product topology of the discrete topology over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e481" xlink:type="simple"/></inline-formula>. Accordingly, one can show that the basic open sets of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e482" xlink:type="simple"/></inline-formula> are the sets of infinite sequences of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e483" xlink:type="simple"/></inline-formula>-dimensional Boolean vectors which all begin with a same prefix, or formally, the sets of the form <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e484" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e485" xlink:type="simple"/></inline-formula>. An open set is then defined as a union of basic open sets. Moreover, as usual, a function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e486" xlink:type="simple"/></inline-formula> is said to be continuous iff the inverse image by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e487" xlink:type="simple"/></inline-formula> of every open set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e488" xlink:type="simple"/></inline-formula> is an open set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e489" xlink:type="simple"/></inline-formula>. Now, given two Boolean recurrent neural networks <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e490" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e491" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e492" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e493" xlink:type="simple"/></inline-formula> input units respectively, we say that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e494" xlink:type="simple"/></inline-formula> <italic>reduces</italic> (or <italic>Wadge reduces</italic> or <italic>continuously reduces</italic>) to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e495" xlink:type="simple"/></inline-formula>, denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e496" xlink:type="simple"/></inline-formula>, iff there exists a continuous function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e497" xlink:type="simple"/></inline-formula> such that, for any input stream <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e498" xlink:type="simple"/></inline-formula>, one has <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e499" xlink:type="simple"/></inline-formula>, or equivalently, such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e500" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pone.0094204-Wadge1">[78]</xref>. Intuitively, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e501" xlink:type="simple"/></inline-formula> iff the problem of determining whether some input stream <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e502" xlink:type="simple"/></inline-formula> belongs to the neural language of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e503" xlink:type="simple"/></inline-formula> (i.e. whether <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e504" xlink:type="simple"/></inline-formula> is meaningful for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e505" xlink:type="simple"/></inline-formula>) reduces via some simple function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e506" xlink:type="simple"/></inline-formula> to the problem of knowing whether <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e507" xlink:type="simple"/></inline-formula> belongs to the neural language of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e508" xlink:type="simple"/></inline-formula> (i.e. whether <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e509" xlink:type="simple"/></inline-formula> is meaningful for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e510" xlink:type="simple"/></inline-formula>). The corresponding strict reduction, equivalence relation, and incomparability relation are then naturally defined by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e511" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e512" xlink:type="simple"/></inline-formula>, as well as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e513" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e514" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e515" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e516" xlink:type="simple"/></inline-formula>. Moreover, a network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e517" xlink:type="simple"/></inline-formula> is called <italic>self-dual</italic> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e518" xlink:type="simple"/></inline-formula>; it is called <italic>non-self-dual</italic> if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e519" xlink:type="simple"/></inline-formula>, which can be proved to be equivalent to saying that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e520" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pone.0094204-Wadge1">[78]</xref>. We recall that the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e521" xlink:type="simple"/></inline-formula>, as defined in Section “Formal Definitions”, corresponds to the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e522" xlink:type="simple"/></inline-formula> whose type specification of its attractors has been inverted. Consequently, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e523" xlink:type="simple"/></inline-formula> does not correspond <italic>a priori</italic> to some neural network provided with an output layer. By extension, an <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e524" xlink:type="simple"/></inline-formula>-equivalence class of networks is called <italic>self-dual</italic> if all its elements are self-dual, and <italic>non-self-dual</italic> if all its elements are non-self-dual.</p>
<p>The continuous reduction relation over the class of Boolean recurrent neural networks naturally induces a hierarchical classification of networks formally defined as follows:</p>
<p><bold>Definition 3.</bold> The collection of all Boolean recurrent neural networks ordered by the reduction “<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e525" xlink:type="simple"/></inline-formula>” is called the RNN <italic>hierarchy</italic>.</p>
<p>We now provide a precise description of the RNN hierarchy. The result is obtained by drawing a parallel between the RNN hierarchy and the restriction of the Wagner hierarchy to Büchi automata. For this purpose, let us define the <italic>DBA hierarchy</italic> to be the collection of all deterministic Büchi automata over multidimensional Boolean alphabets <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e526" xlink:type="simple"/></inline-formula> ordered by the continuous reduction relation “<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e527" xlink:type="simple"/></inline-formula>”. More precisely, given two deterministic Büchi automata <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e528" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e529" xlink:type="simple"/></inline-formula>, we set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e530" xlink:type="simple"/></inline-formula> iff there exists a continuous function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e531" xlink:type="simple"/></inline-formula> such that, for any input stream <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e532" xlink:type="simple"/></inline-formula>, one has <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e533" xlink:type="simple"/></inline-formula>. The following result shows that the RNN hierarchy and the DBA hierarchy are actually isomorphic. Moreover, a possible isomorphism is given by the mapping described in Proposition 1 which associates to every network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e534" xlink:type="simple"/></inline-formula> a corresponding deterministic Büchi automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e535" xlink:type="simple"/></inline-formula>.</p>
<p><bold>Proposition 3.</bold> <italic>The RNN hierarchy and the DBA hierarchy are isomorphic.</italic></p>
<p><italic>Proof.</italic> Consider the mapping described in Proposition 1 which associates to every network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e536" xlink:type="simple"/></inline-formula> a corresponding deterministic automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e537" xlink:type="simple"/></inline-formula>. We prove that this mapping is an embedding from the RNN hierarchy into the DBA hierarchy. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e538" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e539" xlink:type="simple"/></inline-formula> be any two networks, and let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e540" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e541" xlink:type="simple"/></inline-formula> be their corresponding deterministic Büchi automata. Proposition 1 ensures that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e542" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e543" xlink:type="simple"/></inline-formula>. Hence, one has <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e544" xlink:type="simple"/></inline-formula> iff by definition there exists a continuous function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e545" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e546" xlink:type="simple"/></inline-formula> iff there exists a continuous function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e547" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e548" xlink:type="simple"/></inline-formula>, iff by definition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e549" xlink:type="simple"/></inline-formula>. Therefore <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e550" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e551" xlink:type="simple"/></inline-formula>. It follows that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e552" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e553" xlink:type="simple"/></inline-formula>, proving that the considered mapping is an embedding. We now show that, up to the continuous equivalence relation “<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e554" xlink:type="simple"/></inline-formula>”, this mapping is also onto. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e555" xlink:type="simple"/></inline-formula> be some deterministic Büchi automaton. By Proposition 2, there exists a network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e556" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e557" xlink:type="simple"/></inline-formula>. Moreover, by Proposition 1, the automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e558" xlink:type="simple"/></inline-formula> satisfies <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e559" xlink:type="simple"/></inline-formula>. It follows that for any infinite input stream <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e560" xlink:type="simple"/></inline-formula>, one has <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e561" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e562" xlink:type="simple"/></inline-formula>, meaning that both <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e563" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e564" xlink:type="simple"/></inline-formula> hold, and thus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e565" xlink:type="simple"/></inline-formula>. Therefore, for any deterministic Büchi automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e566" xlink:type="simple"/></inline-formula>, there exists a neural network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e567" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e568" xlink:type="simple"/></inline-formula>, meaning precisely that up to the continuous equivalence relation “<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e569" xlink:type="simple"/></inline-formula>”, the mapping <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e570" xlink:type="simple"/></inline-formula> described in Proposition 1 is onto. This concludes the proof.</p>
<p>By Proposition 3 and the usual results of the DBA hierarchy, a precise description of the RNN hierarchy can be given. First of all, the RNN hierarchy is well-founded, i.e. there is no infinite strictly descending sequence of networks <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e571" xlink:type="simple"/></inline-formula>. Moreover, the maximal strict chains in the RNN hierarchy have length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e572" xlink:type="simple"/></inline-formula>, meaning that the RNN hierarchy has a height of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e573" xlink:type="simple"/></inline-formula>. (A strict chain of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e574" xlink:type="simple"/></inline-formula> in the RNN hierarchy is a sequence of neural networks <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e575" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e576" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e577" xlink:type="simple"/></inline-formula>; a strict chain is said to be maximal if its length is at least as large as the length of every other strict chain.) Furthermore, the maximal antichains of the RNN hierarchy have length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e578" xlink:type="simple"/></inline-formula>, meaning that the RNN hierarchy has a width of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e579" xlink:type="simple"/></inline-formula>. (An antichain of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e580" xlink:type="simple"/></inline-formula> in the RNN hierarchy is a sequence of neural networks <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e581" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e582" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e583" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e584" xlink:type="simple"/></inline-formula>; an antichain is said to be maximal if its length is at least as large as the length of every other antichain.) More precisely, it can be shown that incomparable networks are equivalent (for the relation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e585" xlink:type="simple"/></inline-formula>) up to complementation, i.e., for any two networks <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e586" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e587" xlink:type="simple"/></inline-formula>, one has <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e588" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e589" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e590" xlink:type="simple"/></inline-formula> are non-self-dual and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e591" xlink:type="simple"/></inline-formula>. These properties imply that, up to equivalence and complementation, the RNN hierarchy is actually a well-ordering. In fact, the RNN hierarchy consists of an infinite alternating succession of pairs of non-self-dual and single self-dual classes, overhung by an additional single non-self-dual class at the first limit level <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e592" xlink:type="simple"/></inline-formula>, as illustrated in <xref ref-type="fig" rid="pone-0094204-g004">Figure 4</xref>.</p>
<fig id="pone-0094204-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0094204.g004</object-id><label>Figure 4</label><caption>
<title>The RNN hierarchy.</title>
<p>An infinite alternating succession of pairs of non-self-dual classes of networks followed by single self-dual classes of networks, all of them overhung by an additional single non-self-dual class at the first limit level. Circles represent the equivalence classes of networks (with respect to the relation “<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e593" xlink:type="simple"/></inline-formula>”) and arrows between circles represent the strict reduction “<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e594" xlink:type="simple"/></inline-formula>” between all elements of the corresponding classes.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0094204.g004" position="float" xlink:type="simple"/></fig>
<p>For convenience reasons, the degree of a network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e595" xlink:type="simple"/></inline-formula> in the RNN hierarchy is defined such that the same degree is shared by both non-self-dual networks at some level and self-dual networks located just one level higher, as illustrated in <xref ref-type="fig" rid="pone-0094204-g004">Figure 4</xref>:<disp-formula id="pone.0094204.e596"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0094204.e596" xlink:type="simple"/></disp-formula>Moreover, the equivalence between the DBA and RNN hierarchies ensures that the RNN hierarchy is actually decidable, in the sense that there exists an algorithmic procedure which is able to compute the degree of any network in the RNN hierarchy. All the above properties of the RNN hierarchy are summarised in the following result.</p>
<p><bold>Theorem 2.</bold> <italic>The RNN hierarchy is a decidable pre-well-ordering of width </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e597" xlink:type="simple"/></inline-formula><italic> and height </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e598" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<p><italic>Proof.</italic> The DBA hierarchy consists of a decidable pre-well-ordering of width <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e599" xlink:type="simple"/></inline-formula> and height <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e600" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pone.0094204-Perrin1">[79]</xref>. Proposition 3 ensures that the RNN hierarchy and the DBA hierarchy are isomorphic.</p>
<p>The following result provides a detailed description of the decidability procedure of the RNN hierarchy. More precisely, it is shown that the degree of a network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e601" xlink:type="simple"/></inline-formula> in the RNN hierarchy corresponds precisely to the maximal number of times that this network might switch between visits of meaningful and spurious attractors along some evolution.</p>
<p><bold>Theorem 3.</bold> <italic>Let </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e602" xlink:type="simple"/></inline-formula><italic> be some network provided with an additional specification of an output layer, </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e603" xlink:type="simple"/></inline-formula><italic> be the corresponding deterministic Büchi automaton of </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e604" xlink:type="simple"/></inline-formula><italic>, and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e605" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<list list-type="bullet"><list-item>
<p><italic>If there exists in </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e606" xlink:type="simple"/></inline-formula><italic> a maximal alternating chain of length </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e607" xlink:type="simple"/></inline-formula><italic> and no maximal co-alternating chain of length </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e608" xlink:type="simple"/></inline-formula><italic>, then </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e609" xlink:type="simple"/></inline-formula><italic> and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e610" xlink:type="simple"/></inline-formula><italic> is non-self-dual.</italic></p>
</list-item><list-item>
<p><italic>Symmetrically, if there exists in </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e611" xlink:type="simple"/></inline-formula><italic> a maximal co-alternating chain of length </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e612" xlink:type="simple"/></inline-formula><italic> but no maximal alternating chain of length </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e613" xlink:type="simple"/></inline-formula><italic>, then also </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e614" xlink:type="simple"/></inline-formula><italic> and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e615" xlink:type="simple"/></inline-formula><italic> is non-self-dual.</italic></p>
</list-item><list-item>
<p><italic>If there exist in </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e616" xlink:type="simple"/></inline-formula><italic> a maximal alternating chain of length </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e617" xlink:type="simple"/></inline-formula><italic> as well as a maximal co-alternating chain of length </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e618" xlink:type="simple"/></inline-formula><italic>, then </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e619" xlink:type="simple"/></inline-formula><italic> and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e620" xlink:type="simple"/></inline-formula><italic> is self-dual.</italic></p>
</list-item><list-item>
<p><italic>If there exist in </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e621" xlink:type="simple"/></inline-formula><italic> a maximal alternating chain of length </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e622" xlink:type="simple"/></inline-formula><italic>, then </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e623" xlink:type="simple"/></inline-formula><italic> and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e624" xlink:type="simple"/></inline-formula><italic> is non-self-dual.</italic></p>
</list-item></list>
<p><italic>Proof.</italic> By Proposition 3, the degree of a network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e625" xlink:type="simple"/></inline-formula> in the RNN hierarchy is equal to the degree of its corresponding deterministic Büchi automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e626" xlink:type="simple"/></inline-formula> in the DBA hierarchy. Moreover, the degree of a deterministic Büchi automaton in the DBA hierarchy corresponds precisely to the length of a maximal alternating or co-alternating chain contained in this automaton <xref ref-type="bibr" rid="pone.0094204-Perrin1">[79]</xref>.</p>
<p>By Theorem 3, the decidability procedure of the degree of a neural network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e627" xlink:type="simple"/></inline-formula> in the RNN hierarchy consists firstly in translating the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e628" xlink:type="simple"/></inline-formula> into its corresponding deterministic Büchi automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e629" xlink:type="simple"/></inline-formula>, as described in Proposition 1, and secondly in returning the ordinal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e630" xlink:type="simple"/></inline-formula> corresponding to the length of a maximal alternating chain or co-alternating chain contained in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e631" xlink:type="simple"/></inline-formula>. Note that this procedure can clearly be achieved by some graph analysis of the automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e632" xlink:type="simple"/></inline-formula>, since the graph of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e633" xlink:type="simple"/></inline-formula> is always finite. Furthermore, since alternating and co-alternating chains are defined in terms of cycles in the graph of the automaton, then according to the biunivocal correspondence between cycles in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e634" xlink:type="simple"/></inline-formula> and attractors of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e635" xlink:type="simple"/></inline-formula>, it can be deduced that the complexity of a network in the RNN hierarchy is in fact directly related to the attractive properties of this network.</p>
<p>More precisely, it can be observed that the complexity measurement provided by the RNN hierarchy actually corresponds to the maximal number of times that a network might alternate between visits of meaningful and spurious attractors along some evolution. Indeed, the existence of a maximal alternating or co-alternating chain <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e636" xlink:type="simple"/></inline-formula> of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e637" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e638" xlink:type="simple"/></inline-formula> means that every infinite initial path in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e639" xlink:type="simple"/></inline-formula> might alternate at most <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e640" xlink:type="simple"/></inline-formula> times between visits of successful and non-successful cycles. Yet this is precisely equivalent to saying that every evolution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e641" xlink:type="simple"/></inline-formula> can only alternate at most <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e642" xlink:type="simple"/></inline-formula> times between visits of meaningful and spurious attractors before eventually becoming trapped forever by a last attractor. In this case, Theorem 3 ensures that the degree of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e643" xlink:type="simple"/></inline-formula> is equal to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e644" xlink:type="simple"/></inline-formula>. Moreover, the existence of an alternating chain <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e645" xlink:type="simple"/></inline-formula> of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e646" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e647" xlink:type="simple"/></inline-formula> is equivalent to the existence of an infinite initial path in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e648" xlink:type="simple"/></inline-formula> that might alternate infinitely many times between visits of the cycles <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e649" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e650" xlink:type="simple"/></inline-formula>. This is equivalent to saying that there exists an evolution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e651" xlink:type="simple"/></inline-formula> that might alternate infinitely many times between visits of a meaningful and a spurious attractor. By Theorem 3, the degree of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e652" xlink:type="simple"/></inline-formula> is equal to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e653" xlink:type="simple"/></inline-formula> is this case. Therefore, the RNN hierarchy provides a new measurement of complexity of neural networks according to their ability to maximally alternate between different types of attractors along their evolutions.</p>
<p>Finally, the decidability procedure of the RNN hierarchy is illustrated by the Example S4 in <xref ref-type="supplementary-material" rid="pone.0094204.s004">File S4</xref>.</p>
</sec></sec><sec id="s3b">
<title>Refined Hierarchical Classification of Neural Networks</title>
<p>In this section, we show that by relaxing the restrictions on the specification of the type of their attractors, the networks significantly increase their expressive power from deterministic Büchi automata up to Muller automata <xref ref-type="bibr" rid="pone.0094204-McNaughton1">[80]</xref>. Hence, by translating once again the Wagner classification theory from the Muller automaton to the neural network context, another more refined hierarchical classification of Boolean neural networks can be obtained. The obtained classification is also tightly related to the attractive properties of the networks, and hence provides once again a new refined measurement of complexity of Boolean recurrent neural networks in terms of their attractive behaviours.</p>
<sec id="s3b1">
<title>Boolean Recurrent Neural Networks and Muller Automata</title>
<p>The assumption that the networks are provided with an additional description of an output layer, which would subsequently influence the type specifications (meaningful/spurious) of their attractors, is not necessary anymore from this point onwards. Instead, let us assume that, for any network, the precise classification of its attractors into meaningful and spurious types is known. How the meaningfulness of the attractors is determined is an issue that is not considered here. For instance, the specification of the type of each attractor might have been determined by its neurophysiological significance with respect to measurable observations associated to certain behaviours or sensory discriminations. Formally, in this section, a recurrent neural network consists of a tuple <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e654" xlink:type="simple"/></inline-formula>, as described in Definition 1, but also provided with an additional specification into meaningful and spurious type for each one of its attractors.</p>
<p>We now prove that, by totally relaxing the restrictions on the specification of the type of their attractors, the Boolean neural networks significantly increase their expressive powers from deterministic Büchi automata up to Muller automata. The following straightforward generalisation of Proposition 1 states that any such Boolean recurrent neural network can be simulated by some deterministic Muller automaton.</p>
<p><bold>Proposition 4.</bold> <italic>Let </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e655" xlink:type="simple"/></inline-formula><italic> be some Boolean recurrent neural network provided with a type specification of each of its attractors. Then there exists a deterministic Muller automaton </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e656" xlink:type="simple"/></inline-formula><italic> such that </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e657" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<p><italic>Proof.</italic> Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e658" xlink:type="simple"/></inline-formula> be given by the tuple <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e659" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e660" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e661" xlink:type="simple"/></inline-formula>, and let the meaningful attractors of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e662" xlink:type="simple"/></inline-formula> be given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e663" xlink:type="simple"/></inline-formula>, all others being spurious. Now, consider the deterministic Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e664" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e665" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e666" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e667" xlink:type="simple"/></inline-formula> is the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e668" xlink:type="simple"/></inline-formula>-dimensional zero vector, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e669" xlink:type="simple"/></inline-formula> is defined by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e670" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e671" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e672" xlink:type="simple"/></inline-formula>. According to this construction, any input stream <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e673" xlink:type="simple"/></inline-formula> is meaningful for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e674" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e675" xlink:type="simple"/></inline-formula> is recognised by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e676" xlink:type="simple"/></inline-formula>. In other words, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e677" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e678" xlink:type="simple"/></inline-formula>, and therefore <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e679" xlink:type="simple"/></inline-formula>.</p>
<p>Conversely, as a generalisation of Proposition 2, we can prove that any deterministic Muller automaton can be simulated by some Boolean recurrent neural network provided with a suitable type specification of its attractors.</p>
<p><bold>Proposition 5.</bold> <italic>Let </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e680" xlink:type="simple"/></inline-formula><italic> and let </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e681" xlink:type="simple"/></inline-formula><italic> be some deterministic Muller automaton over the alphabet </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e682" xlink:type="simple"/></inline-formula><italic>. Then there exists a Boolean recurrent neural network </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e683" xlink:type="simple"/></inline-formula><italic> provided with a type specification of each of its attractors such that </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e684" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<p><italic>Proof.</italic> Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e685" xlink:type="simple"/></inline-formula> be given by the tuple <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e686" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e687" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e688" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e689" xlink:type="simple"/></inline-formula>. Now, consider the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e690" xlink:type="simple"/></inline-formula> described in the proof of Proposition 2. It remains to define which are the meaningful and spurious attractors of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e691" xlink:type="simple"/></inline-formula>. As mentioned in the proof of Proposition 2, at every time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e692" xlink:type="simple"/></inline-formula>, only one among the “state cells” <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e693" xlink:type="simple"/></inline-formula> is spiking. Hence, for any state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e694" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e695" xlink:type="simple"/></inline-formula> that might occur at some time step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e696" xlink:type="simple"/></inline-formula>, let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e697" xlink:type="simple"/></inline-formula> be the index such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e698" xlink:type="simple"/></inline-formula> is the unique “state cell” which is spiking during state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e699" xlink:type="simple"/></inline-formula>. An attractor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e700" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e701" xlink:type="simple"/></inline-formula> is then said to be meaningful iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e702" xlink:type="simple"/></inline-formula>.</p>
<p>Consequently, for any infinite infinite sequence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e703" xlink:type="simple"/></inline-formula>, the infinite path <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e704" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e705" xlink:type="simple"/></inline-formula> satisfies <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e706" xlink:type="simple"/></inline-formula> iff the evolution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e707" xlink:type="simple"/></inline-formula> in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e708" xlink:type="simple"/></inline-formula> is such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e709" xlink:type="simple"/></inline-formula> is a meaningful attractor. Therefore, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e710" xlink:type="simple"/></inline-formula> is recognised by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e711" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e712" xlink:type="simple"/></inline-formula> is meaningful for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e713" xlink:type="simple"/></inline-formula>, showing that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e714" xlink:type="simple"/></inline-formula>.</p>
<p>Propositions 4 and 5 yield the following equivalence between Boolean recurrent neural networks and deterministic Muller automata.</p>
<p><bold>Theorem 4.</bold> <italic>Let </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e715" xlink:type="simple"/></inline-formula><italic> for some </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e716" xlink:type="simple"/></inline-formula><italic>. Then the following conditions are equivalent:</italic></p>
<list list-type="alpha-lower"><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e717" xlink:type="simple"/></inline-formula><italic> is recognisable by some Boolean recurrent neural network provided with a type specification of its attractors;</italic></p>
</list-item><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e718" xlink:type="simple"/></inline-formula><italic> is recognisable by some deterministic Muller automaton;</italic></p>
</list-item><list-item>
<p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e719" xlink:type="simple"/></inline-formula><italic> is </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e720" xlink:type="simple"/></inline-formula><italic>-rational.</italic></p>
</list-item></list>
<p><italic>Proof.</italic> The equivalence between conditions a and b is given by propositions 4 and 5. The equivalence between conditions b and c is a well-known result of automata theory <xref ref-type="bibr" rid="pone.0094204-Perrin1">[79]</xref>.</p>
<p>The two procedures described in the proofs of propositions 4 and 5 are illustrated by the Example S5 and <xref ref-type="supplementary-material" rid="pone.0094204.s005">Figure S4 in File S5</xref>.</p>
</sec><sec id="s3b2">
<title>Complete RNN Hierarchy</title>
<p>In this section, we prove that the collection of Boolean recurrent neural networks ordered by the continuous reduction corresponds to a refined hierarchical classification of height <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e721" xlink:type="simple"/></inline-formula>. This classification is directly related to the attractive properties of the networks, and therefore provides a new refined measurement of complexity of neural networks according to their attractive behaviours. This hierarchical classification is formally defined as follows.</p>
<p><bold>Definition 4.</bold> <italic>The collection of all Boolean recurrent neural networks provided with a type specification of their attractors ordered by the continuous reduction “</italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e722" xlink:type="simple"/></inline-formula><italic>” is called the complete RNN hierarchy.</italic></p>
<p>Like in Section “RNN Hierarchy”, a precise characterisation of the complete RNN hierarchy can be obtained by translating the Wagner classification theory from the Muller automaton to the neural network context. For this purpose, we shall consider the collection of all deterministic Muller automata over multidimensional Boolean alphabets <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e723" xlink:type="simple"/></inline-formula> ordered by the continuous reduction “<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e724" xlink:type="simple"/></inline-formula>”. This hierarchy is commonly referred to as the <italic>Wagner hierarchy</italic> <xref ref-type="bibr" rid="pone.0094204-Wagner1">[35]</xref>. A generalisation of Proposition 3 shows that the complete RNN hierarchy and the Wagner hierarchy are isomorphic, and a possible isomorphism is also given by the mapping described in Proposition 4 which associates to every network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e725" xlink:type="simple"/></inline-formula> a corresponding deterministic Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e726" xlink:type="simple"/></inline-formula>.</p>
<p><bold>Proposition 6.</bold> <italic>The complete RNN hierarchy and the Wagner hierarchy are isomorphic.</italic></p>
<p><italic>Proof.</italic> Consider the mapping described in Proposition 4 which associates to every network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e727" xlink:type="simple"/></inline-formula> a corresponding deterministic Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e728" xlink:type="simple"/></inline-formula>. A similar reasoning as the one presented in the proof of Proposition 3 shows that this mapping is an isomorphism between the complete RNN hierarchy and the Wagner hierarchy.</p>
<p>By Proposition 6 and the usual results on the Wagner hierarchy <xref ref-type="bibr" rid="pone.0094204-Wagner1">[35]</xref>, the following precise description of the complete RNN hierarchy can be given. First of all, like the RNN hierarchy, the complete RNN hierarchy also consists of a pre-well ordering of width <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e729" xlink:type="simple"/></inline-formula>, and any two networks <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e730" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e731" xlink:type="simple"/></inline-formula> satisfy the incomparability relation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e732" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e733" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e734" xlink:type="simple"/></inline-formula> are non-self-dual networks such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e735" xlink:type="simple"/></inline-formula>. However, while the RNN hierarchy has only height <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e736" xlink:type="simple"/></inline-formula>, the complete RNN hierarchy shows a height of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e737" xlink:type="simple"/></inline-formula> levels. In fact, the complete RNN hierarchy consists of an infinite alternating succession of pairs of non-self-dual and single self-dual classes, with non-self-dual classes at each limit level, as illustrated in <xref ref-type="fig" rid="pone-0094204-g005">Figure 5</xref>. For convenience reasons, the degree <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e738" xlink:type="simple"/></inline-formula> of a network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e739" xlink:type="simple"/></inline-formula> in the complete RNN hierarchy is also defined such that the same degree is shared by both non-self-dual networks at some level and self-dual networks located just one level higher, namely:<disp-formula id="pone.0094204.e740"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0094204.e740" xlink:type="simple"/></disp-formula>Besides, the isomorphism between the Wagner hierarchy and the complete RNN hierarchy ensures that the complete RNN hierarchy is actually decidable, in the sense that there exists an algorithmic procedure allowing to compute the degree of any network in the complete RNN hierarchy. The following theorem summarises the properties of the complete RNN hierarchy.</p>
<fig id="pone-0094204-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0094204.g005</object-id><label>Figure 5</label><caption>
<title>The complete RNN hierarchy.</title>
<p>A transfinite alternating succession of pairs of non-self-dual classes of networks followed by single self-dual classes of networks, with non-self-dual classes at each limit level.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0094204.g005" position="float" xlink:type="simple"/></fig>
<p><bold>Theorem 5.</bold> <italic>The complete RNN hierarchy is a decidable pre-well-ordering of width </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e741" xlink:type="simple"/></inline-formula><italic> and height </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e742" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<p><italic>Proof.</italic> The Wagner hierarchy consists of a decidable pre-well-ordering of width <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e743" xlink:type="simple"/></inline-formula> and height <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e744" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pone.0094204-Wagner1">[35]</xref>. Proposition 6 ensures that the complete RNN hierarchy and the Wagner hierarchy are isomorphic.</p>
<p>The following result provides a detailed description of the decidability procedure of the complete RNN hierarchy. More precisely, it is shown that the degree of a network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e745" xlink:type="simple"/></inline-formula> in the RNN hierarchy corresponds precisely to the largest ordinal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e746" xlink:type="simple"/></inline-formula> such that there exists an alternating tree or a co-alternating tree of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e747" xlink:type="simple"/></inline-formula> in the deterministic Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e748" xlink:type="simple"/></inline-formula>.</p>
<p><bold>Theorem 6.</bold> <italic>Let </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e749" xlink:type="simple"/></inline-formula><italic> be some Boolean recurrent network provided with a type specification of its attractors, </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e750" xlink:type="simple"/></inline-formula><italic> be the corresponding deterministic Muller automaton of </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e751" xlink:type="simple"/></inline-formula><italic>, and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e752" xlink:type="simple"/></inline-formula><italic> be an ordinal such that </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e753" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<list list-type="bullet"><list-item>
<p><italic>If there exists in </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e754" xlink:type="simple"/></inline-formula><italic> a maximal alternating tree of length </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e755" xlink:type="simple"/></inline-formula><italic> and no maximal co-alternating tree of length </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e756" xlink:type="simple"/></inline-formula><italic>, then </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e757" xlink:type="simple"/></inline-formula><italic> and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e758" xlink:type="simple"/></inline-formula><italic> is non-self-dual.</italic></p>
</list-item><list-item>
<p><italic>If there exists in </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e759" xlink:type="simple"/></inline-formula><italic> a maximal co-alternating tree of length </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e760" xlink:type="simple"/></inline-formula><italic> and no maximal alternating tree of length </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e761" xlink:type="simple"/></inline-formula><italic>, then </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e762" xlink:type="simple"/></inline-formula><italic> and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e763" xlink:type="simple"/></inline-formula><italic> is non-self-dual.</italic></p>
</list-item><list-item>
<p><italic>If there exist in </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e764" xlink:type="simple"/></inline-formula><italic> both a maximal alternating tree as well as a maximal co-alternating tree of length </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e765" xlink:type="simple"/></inline-formula><italic>, then </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e766" xlink:type="simple"/></inline-formula><italic> and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e767" xlink:type="simple"/></inline-formula><italic> is self-dual.</italic></p>
</list-item></list>
<p><italic>Proof.</italic> By Proposition 6, the degree of a network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e768" xlink:type="simple"/></inline-formula> in the complete RNN hierarchy is equal to the degree of its corresponding deterministic Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e769" xlink:type="simple"/></inline-formula> in the Wagner hierarchy. Moreover, the degree of a deterministic Muller automaton in the Wagner hierarchy corresponds precisely to the length of a maximal alternating or co-alternating tree contained in this automaton <xref ref-type="bibr" rid="pone.0094204-Wagner1">[35]</xref>, <xref ref-type="bibr" rid="pone.0094204-Selivanov1">[82]</xref>.</p>
<p>The decidability procedure of the degree of a neural network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e770" xlink:type="simple"/></inline-formula> in the complete RNN hierarchy thus consists in first translating the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e771" xlink:type="simple"/></inline-formula> into its corresponding deterministic Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e772" xlink:type="simple"/></inline-formula>, as described in Proposition 4, and then returning the ordinal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e773" xlink:type="simple"/></inline-formula> corresponding to the length of a maximal alternating tree, or co-alternating tree, contained in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e774" xlink:type="simple"/></inline-formula>. Note that this procedure can be achieved by some graph analysis of the automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e775" xlink:type="simple"/></inline-formula>, since the graph of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e776" xlink:type="simple"/></inline-formula> is always finite.</p>
<p>By Theorem 6, the degree of a neural network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e777" xlink:type="simple"/></inline-formula> in the complete RNN hierarchy corresponds precisely to the length of a maximal alternating, or co-alternating, tree contained in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e778" xlink:type="simple"/></inline-formula>. Since alternating and co-alternating trees are defined in terms of cycles in the graph of the Muller automaton, and according to the biunivocal correspondence between cycles in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e779" xlink:type="simple"/></inline-formula> and attractors of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e780" xlink:type="simple"/></inline-formula>, it can be deduced that, like for the RNN hierarchy, the complexity of a network in the complete RNN hierarchy is also directly related to the attractive properties of this network. In fact, the complexity measurement provided by the complete RNN hierarchy refers to the maximal number of times that a network might alternate between visits of meaningful and spurious attractors along some evolution.</p>
<p>More precisely, the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e781" xlink:type="simple"/></inline-formula> first levels of the complete RNN hierarchy provide a classification of the collection of networks that might switch at most <italic>finitely</italic> many times between different types of attractors along their evolutions. Indeed, by Theorem 6, for any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e782" xlink:type="simple"/></inline-formula>, a network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e783" xlink:type="simple"/></inline-formula> satisfies <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e784" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e785" xlink:type="simple"/></inline-formula> contains a maximal alternating, or co-alternating, tree of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e786" xlink:type="simple"/></inline-formula>. In other words, for any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e787" xlink:type="simple"/></inline-formula>, a network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e788" xlink:type="simple"/></inline-formula> satisfies <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e789" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e790" xlink:type="simple"/></inline-formula> is able to switch at most <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e791" xlink:type="simple"/></inline-formula> times between visits of different types of attractors during all its possible evolutions.</p>
<p>The levels of transfinite degrees provide a refined classification of the collection of networks that are able to alternate <italic>infinitely</italic> many times between different types of attractors. Indeed, according to Theorem 6, for any ordinal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e792" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e793" xlink:type="simple"/></inline-formula>, a network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e794" xlink:type="simple"/></inline-formula> satisfies <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e795" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e796" xlink:type="simple"/></inline-formula> contains a maximal alternating or co-alternating tree of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e797" xlink:type="simple"/></inline-formula>. Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e798" xlink:type="simple"/></inline-formula>, this implies that the graph of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e799" xlink:type="simple"/></inline-formula> necessarily contains at least two cycles <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e800" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e801" xlink:type="simple"/></inline-formula> such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e802" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e803" xlink:type="simple"/></inline-formula> is successful iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e804" xlink:type="simple"/></inline-formula> is non-successful. But since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e805" xlink:type="simple"/></inline-formula>, it follows that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e806" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e807" xlink:type="simple"/></inline-formula> are both accessible one from the other in the graph of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e808" xlink:type="simple"/></inline-formula>. By the biunivocal correspondence between cycles and attractors, the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e809" xlink:type="simple"/></inline-formula> contains at least the two attractors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e810" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e811" xlink:type="simple"/></inline-formula>, and the accessibility between those ensures that the network is capable of alternating infinitely often between visits of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e812" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e813" xlink:type="simple"/></inline-formula> along some evolution. In fact, the collection of levels of transfinite degrees of the complete RNN hierarchy provides a refined classification of these potentially infinitely switching networks based on the intricacy of their underlying attractive structures (tree-like representation induced by the inclusion and accessibility relations between the attractors, as illustrated in <xref ref-type="fig" rid="pone-0094204-g002">Figure 2</xref>).</p>
<p>It can be noticed, according to the definition of alternating and co-alternating trees, that if some given Muller automaton contains either an alternating or a co-alternating tree of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e814" xlink:type="simple"/></inline-formula> in its underlying graph, then this automaton also necessarily contains in its graph both an alternating and a co-alternating tree of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e815" xlink:type="simple"/></inline-formula>, for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e816" xlink:type="simple"/></inline-formula>. Therefore, any network of the complete RNN hierarchy is capable of disclosing an attractive behaviour analogous to any other network of strictly smaller degree. In this precise sense, a network of the complete RNN hierarchy potentially contains in its structure all the possible attractive behaviours of every other networks of strictly smaller degrees. In this framework, the concept of alternation between different types of attractors corresponds to the transient trajectories between attractor basins, a concept referred to as “itinerancy” elsewhere in the literature <xref ref-type="bibr" rid="pone.0094204-Villa1">[51]</xref>, <xref ref-type="bibr" rid="pone.0094204-Freeman2">[65]</xref>–<xref ref-type="bibr" rid="pone.0094204-Freeman3">[67]</xref>, <xref ref-type="bibr" rid="pone.0094204-Tsuda3">[83]</xref>, <xref ref-type="bibr" rid="pone.0094204-Kaneko1">[84]</xref>.</p>
<p>The decidability procedure of the complete RNN hierarchy is illustrated by the Example S6 in <xref ref-type="supplementary-material" rid="pone.0094204.s006">File S6</xref>.</p>
<p>It is worth noting that the complete RNN hierarchy can actually be seen as a proper extension of the RNN hierarchy. Indeed, the next result shows that the networks of RNN hierarchy and the networks of the specific initial segment of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e817" xlink:type="simple"/></inline-formula> of the complete RNN hierarchy recognise precisely the same languages. In this precise sense, the RNN hierarchy consists of an initial segment of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e818" xlink:type="simple"/></inline-formula> of the complete RNN hierarchy.</p>
<p><bold>Proposition 7.</bold> <italic>Let </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e819" xlink:type="simple"/></inline-formula><italic>. Then </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e820" xlink:type="simple"/></inline-formula><italic> is recognisable by some network </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e821" xlink:type="simple"/></inline-formula><italic> of the RNN hierarchy iff </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e822" xlink:type="simple"/></inline-formula><italic> is also recognisable by some network </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e823" xlink:type="simple"/></inline-formula><italic> of the complete RNN hierarchy such that either </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e824" xlink:type="simple"/></inline-formula><italic> or </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e825" xlink:type="simple"/></inline-formula><italic> and </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e826" xlink:type="simple"/></inline-formula><italic> contains a maximal co-alternating tree of length </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e827" xlink:type="simple"/></inline-formula><italic> but no maximal alternating tree of length </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e828" xlink:type="simple"/></inline-formula><italic>.</italic></p>
<p><italic>Proof.</italic> Given any deterministic Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e829" xlink:type="simple"/></inline-formula>, let the degree of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e830" xlink:type="simple"/></inline-formula> in the Wagner hierarchy be denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e831" xlink:type="simple"/></inline-formula>. Then, the relationship between the DBA and the Wagner hierarchies ensures that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e832" xlink:type="simple"/></inline-formula> is recognisable by some deterministic Büchi automaton iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e833" xlink:type="simple"/></inline-formula> is also recognisable by some deterministic Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e834" xlink:type="simple"/></inline-formula> such that either <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e835" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e836" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e837" xlink:type="simple"/></inline-formula> contains a maximal co-alternating tree of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e838" xlink:type="simple"/></inline-formula> but no maximal alternating tree of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e839" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pone.0094204-Perrin1">[79]</xref>. Theorems 1 and 4 together with Proposition 6 allow to translate these results to the neural network context, and therefore lead to the conclusion.</p>
<p>We recall that the RNN hierarchy consists of the classification of networks whose attractors' type specifications are induced by the existence of an output layer, whereas the complete RNN hierarchy consists of the classification of networks whose attractors' type specifications are <italic>a priori</italic> given without any restriction at all. For any ordinal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e840" xlink:type="simple"/></inline-formula>, the two notions of alternating chain and alternating tree of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e841" xlink:type="simple"/></inline-formula> coincide. Hence, by Theorem 3 and Theorem 6, the two decidability procedures of the RNN hierarchy and the complete RNN hierarchy reduce to the very same, and the decidability procedures simply consist in computing the length of a maximal alternating or co-alternating tree contained in the underlying automata.</p>
<p>However, it is important to clarify the difference between the RNN hierarchy and the complete RNN hierarchy, illustrated in <xref ref-type="fig" rid="pone-0094204-g006">Figure 6</xref>. The restriction on the type specification of the attractors imposed by the existence of an output layer ensures that the networks of the RNN hierarchy will never be able to contain a maximal alternating or co-alternating tree of length strictly larger than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e842" xlink:type="simple"/></inline-formula> in their underlying Büchi automata. Indeed, if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e843" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e844" xlink:type="simple"/></inline-formula> are two cycles in a deterministic Büchi automaton such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e845" xlink:type="simple"/></inline-formula> is successful and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e846" xlink:type="simple"/></inline-formula>, then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e847" xlink:type="simple"/></inline-formula> is necessarily also successful (since it visits the same final states as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e848" xlink:type="simple"/></inline-formula> plus potentially some other ones), meaning that no meaningful cycle could ever be included in some spurious cycle in a deterministic Büchi automaton; consequently, the maximal number alternations between different type of cycles that can be found in a deterministic Büchi automaton is bounded by one – a spurious cycle included in a meaningful cycle – and therefore no alternating or co-alternating trees of length strictly larger than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e849" xlink:type="simple"/></inline-formula> will never exist in a deterministic Büchi automaton. From this observation, it follows that the degree of any network of the RNN hierarchy is bounded by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e850" xlink:type="simple"/></inline-formula>, meaning that the length of the RNN hierarchy cannot exceed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e851" xlink:type="simple"/></inline-formula>, whereas the length of the complete RNN hierarchy climbs up to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e852" xlink:type="simple"/></inline-formula>, as illustrated by <xref ref-type="fig" rid="pone-0094204-g006">Figure 6</xref>.</p>
<fig id="pone-0094204-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0094204.g006</object-id><label>Figure 6</label><caption>
<title>Comparison between the RNN and the complete RNN hierarchies.</title>
<p>The RNN hierarchy, depicted by the sequence of blacks classes, consists of an initial segment of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e853" xlink:type="simple"/></inline-formula> of the complete RNN hierarchy, which has height <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e854" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0094204.g006" position="float" xlink:type="simple"/></fig></sec></sec><sec id="s3c">
<title>The “basal ganglia-thalamocortical network”</title>
<sec id="s3c1">
<title>Neurobiological description</title>
<p>In order to illustrate the application of our method to a case study, we have considered one of the main systems of the brain involved in information processing, the basal ganglia-thalamocortical network. This network is formed by several parallel and segregated circuits involving different areas of the cerebral cortex, striatum, pallidum, thalamus, subthalamic nucleus and midbrain <xref ref-type="bibr" rid="pone.0094204-Alexander1">[85]</xref>–<xref ref-type="bibr" rid="pone.0094204-Krauzlis1">[94]</xref>. This network has been investigated for many years in particular in relation to disorders of the motor system and of the sleep-waking cycle. The simulations were generally performed by considering the basal ganglia-thalamocortical network as a circuit composed of several interconnected areas, each area being modeled by a network of spiking neurons, and were analysed using statistical approaches based on mean-field theory <xref ref-type="bibr" rid="pone.0094204-Terman1">[95]</xref>–<xref ref-type="bibr" rid="pone.0094204-Guthrie1">[106]</xref>.</p>
<p>In the basal ganglia-thalamocortical network are several types of connections and transmitters. Based on the observation that almost all neurons of the central nervous system can be subdivided into projection neurons and interneurons, we consider the connections mediated by projection neurons, both glutamatergic excitatory projections and GABAergic inhibitory projections, as part of an information transmitting system. The local connections established by the interneurons, i.e. the connections remaining confined within a small distance from the projection neurons, are considered forming part of a regulatory system. The other connections, mainly produced by different types of projection neurons, i.e. the dopaminergic (including those from the substantia nigra pars compacta like the nigrostriatal and those from the ventromedial tegmental area), cholinergic (including those from the basal forebrain), the noradrenergic (including those from locus coeruleus), serotoninergic (including those from the dorsal raphe), histaminergic (from the tuberomamillary nucleus) and orexinergic projections (from the lateral and posterior hypothalamus) are considered forming part of a modulatory system. The three systems, information transmitting, regulatory and modulatory have an extensive pattern of reciprocal interconnectivity at various levels that is not addressed in this paper.</p>
<p>A characteristic of all the circuits of the basal ganglia-thalamocortical network is a combination of “open” and “closed” loops with ascending sensory afferences reaching the thalamus and the midbrain, and with descending motor efferences from the midbrain (the tectospinal tract) and the cortex (the corticospinal tract). We assume that the encoding of a large amount of the information treated by the brain is performed by recurrent patterns of activity circulating in the information transmitting system. For this reason, we focus our attention on the complexity of the dynamics that may emerge from that system. To this purpose, we present a Boolean recurrent neural network model of the information transmitting system of the basal ganglia-thalamocortical network, illustrated by <xref ref-type="fig" rid="pone-0094204-g007">Figure 7</xref>.</p>
<fig id="pone-0094204-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0094204.g007</object-id><label>Figure 7</label><caption>
<title>Model of the basal ganglia-thalamocortical network.</title>
<p>The network is constituted of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e855" xlink:type="simple"/></inline-formula> different interconnected brain areas, each one represented by a single node in the Boolean neural network model: superior colliculus (SC), Thalamus, thalamic reticular nucleus (NRT), Cerebral Cortex, the striatopallidal and the striatonigral components of the striatum (Str), the subthalamic nucleus (STN), the external part of the pallidum (GPe), and the output nuclei of the basal ganglia formed by the GABAergic projection neurons of the intermediate part of the pallidum and of the substantia nigra pars reticulata (GPi/SNR). We consider also the inputs (IN) from the ascending sensory pathway and the motor outputs (OUT). The excitatory pathways are labeled in blue and the inhibitory ones in orange.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0094204.g007" position="float" xlink:type="simple"/></fig>
<p>The pattern of connectivity corresponds to the wealth of data reported in the literature <xref ref-type="bibr" rid="pone.0094204-Alexander1">[85]</xref>–<xref ref-type="bibr" rid="pone.0094204-Krauzlis1">[94]</xref>. We assume that each brain area is formed by a neural network and that the network of brain areas corresponding to the basal ganglia-thalamocortical network can be modeled by a Boolean neural network formed by 9 nodes: superior colliculus (SC), Thalamus, thalamic reticular nucleus (NRT), Cerebral Cortex, the two functional parts (striatopallidal and the striatonigral components) of the striatum (Str), the subthalamic nucleus (STN), the external part of the pallidum (GPe), and the output nuclei of the basal ganglia formed by the GABAergic projection neurons of the intermediate part of the pallidum and of the substantia nigra pars reticulata (GPi/SNR).</p>
<p>We consider the ascending sensory pathway (IN), that reaches SC and the Thalamus. SC does not send other projections to the system and sends a projection outside of the system (OUT), to the motor system. The thalamus sends excitatory connections within the system via the thalamo-pallidal, thalamo-striatal and thalamo-cortical projections. Notice that STN receives also an excitatory projection from the Thalamus. NRT receives excitatory collateral projections from both the thalamo-cortical and cortico-thalamic projections. In turn, NRT sends an inhibitory projection to the Thalamus. The Cerebral Cortex receives also an excitatory input from STN and sends corticofugal projections to the basal ganglia (striatum and STN), to the thalamus, to the midbrain and to the motor system (OUT). The only excitatory nucleus of the basal ganglia is STN, that sends projections to the Cerebral Cortex, to GPe and to GPi/SNR. In the striatum (Str) the striatopallidal neurons send inhibitory projections to GPe and the striatonigral neurons send inhibitory projections to GPi/SNR, via the so-called “direct” pathway. The pallidum (GPe) plays a paramount role because it is an inhibitory nucleus, with reciprocal connections back to the striatum and to STN and a downstream inhibitory projection to GPi/SNR via the so-called “indirect” pathway. It is interesting to notice the presence of inhibitory projections that inhibit the inhibitory nuclei within the basal ganglia, thus leading to a kind of “facilitation”, but also inhibitory projections that inhibit RTN, that is a major nucleus in regulating the activity of the thalamus. The connectivity of the Boolean model of the basal ganglia-thalamocortical network is described by the adjacency matrix of the network in <xref ref-type="table" rid="pone-0094204-t001">Table 1</xref>.</p>
<table-wrap id="pone-0094204-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0094204.t001</object-id><label>Table 1</label><caption>
<title>The adjancency matrix of the Boolean model of the basal ganglia-thalamocortical network.</title>
</caption><alternatives><graphic id="pone-0094204-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0094204.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td colspan="2" align="left" rowspan="1">Source</td>
<td colspan="9" align="left" rowspan="1">Target</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Node</td>
<td align="left" rowspan="1" colspan="1">Name</td>
<td align="left" rowspan="1" colspan="1">SC</td>
<td align="left" rowspan="1" colspan="1">Thalamus</td>
<td align="left" rowspan="1" colspan="1">RTN</td>
<td align="left" rowspan="1" colspan="1">GPi/SNr</td>
<td align="left" rowspan="1" colspan="1">STN</td>
<td align="left" rowspan="1" colspan="1">GPe</td>
<td align="left" rowspan="1" colspan="1">Str-D2</td>
<td align="left" rowspan="1" colspan="1">Str-D1</td>
<td align="left" rowspan="1" colspan="1">CCortex</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">SC</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e856" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e857" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e858" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e859" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e860" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e861" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e862" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e863" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">Thalamus</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e864" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e865" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e866" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
<td align="left" rowspan="1" colspan="1">1</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">RTN</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e867" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">−1</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e868" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e869" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e870" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e871" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e872" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e873" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e874" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">GPi/SNr</td>
<td align="left" rowspan="1" colspan="1">−1</td>
<td align="left" rowspan="1" colspan="1">−1</td>
<td align="left" rowspan="1" colspan="1">−1</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e875" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e876" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e877" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e878" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e879" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e880" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">5</td>
<td align="left" rowspan="1" colspan="1">STN</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e881" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e882" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e883" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e884" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e885" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e886" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">2</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">6</td>
<td align="left" rowspan="1" colspan="1">GPe</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e887" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e888" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">−1/2</td>
<td align="left" rowspan="1" colspan="1">−1/2</td>
<td align="left" rowspan="1" colspan="1">−1/2</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e889" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">−1/2</td>
<td align="left" rowspan="1" colspan="1">−1/2</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e890" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">7</td>
<td align="left" rowspan="1" colspan="1">Str-D2</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e891" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e892" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e893" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e894" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e895" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">−1</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e896" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e897" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e898" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">8</td>
<td align="left" rowspan="1" colspan="1">Str-D1</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e899" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e900" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e901" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">−1/2</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e902" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">−1/2</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e903" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e904" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e905" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">9</td>
<td align="left" rowspan="1" colspan="1">Cer. Cortex</td>
<td align="left" rowspan="1" colspan="1">1/2</td>
<td align="left" rowspan="1" colspan="1">1/2</td>
<td align="left" rowspan="1" colspan="1">1/2</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e906" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">1/2</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e907" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">1/2</td>
<td align="left" rowspan="1" colspan="1">1/2</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e908" xlink:type="simple"/></inline-formula></td>
</tr>
</tbody>
</table>
</alternatives></table-wrap></sec><sec id="s3c2">
<title>Computation of attractor-based complexity</title>
<p>For sake of simplicity, we consider that the two inputs to the basal ganglia-thalamocortical network (<xref ref-type="fig" rid="pone-0094204-g007">Figure 7</xref>) are reduced to 1 input node sending projections to Thalamus and SC with synaptic weight equal to 1. We reduce our neurobiological model to a Boolean recurrent neural network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e909" xlink:type="simple"/></inline-formula> that contains 9 activation nodes and 1 input node. Every activation node can be either active or quiet, which means <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e910" xlink:type="simple"/></inline-formula> possible states for the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e911" xlink:type="simple"/></inline-formula>. Every state of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e912" xlink:type="simple"/></inline-formula> is represented by a 9-dimensional Boolean vector describing the sequence of active and quiet activation nodes. For example, the network state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e913" xlink:type="simple"/></inline-formula> means that the nodes #1 (SC), #3 (RTN) and #4 (GPi/SNR) are quiet, whereas every other activation node is active.</p>
<p>In this section, we provide a practical illustration of our new attractor-based complexity measurement applied to the simplified model of the basal ganglia-thalamocortical network. Since the behaviour of network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e914" xlink:type="simple"/></inline-formula> is not determined by any designated output layer, the attractor-based complexity of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e915" xlink:type="simple"/></inline-formula> will be measured with respect to the complete RNN hierarchy rather than with respect to the RNN hierarchy, as described in Section “Complete RNN Hierarchy”. According to these considerations, as mentioned in Theorem 6, the attractor-based complexity of network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e916" xlink:type="simple"/></inline-formula> relies on the graphical structure of its corresponding deterministic Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e917" xlink:type="simple"/></inline-formula>. Hence, we shall now describe the structure of the deterministic Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e918" xlink:type="simple"/></inline-formula> associated to network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e919" xlink:type="simple"/></inline-formula>.</p>
<p>Firstly, as mentioned in the proof of Proposition 4, the states of the Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e920" xlink:type="simple"/></inline-formula> correspond precisely to the states of network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e921" xlink:type="simple"/></inline-formula>. Hence, the deterministic Muller automaton associated to the basal ganglia-thalamocortical network contains <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e922" xlink:type="simple"/></inline-formula> states, numbered from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e923" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e924" xlink:type="simple"/></inline-formula>. The numbering of the states is chosen such that state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e925" xlink:type="simple"/></inline-formula> is numbered by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e926" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e927" xlink:type="simple"/></inline-formula> is the decimal representation of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e928" xlink:type="simple"/></inline-formula>-digit binary number <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e929" xlink:type="simple"/></inline-formula>. For instance, state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e930" xlink:type="simple"/></inline-formula> is referred to as number <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e931" xlink:type="simple"/></inline-formula>, since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e932" xlink:type="simple"/></inline-formula> is the decimal representation of the binary number <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e933" xlink:type="simple"/></inline-formula>. Secondly, also as mentioned in the proof of Proposition 4, the transitions of the Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e934" xlink:type="simple"/></inline-formula> are constructed as follows: there is a transition labelled by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e935" xlink:type="simple"/></inline-formula> (resp. by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e936" xlink:type="simple"/></inline-formula>) from state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e937" xlink:type="simple"/></inline-formula> to state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e938" xlink:type="simple"/></inline-formula> if and only if network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e939" xlink:type="simple"/></inline-formula> transits from state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e940" xlink:type="simple"/></inline-formula> to state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e941" xlink:type="simple"/></inline-formula> when it receives input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e942" xlink:type="simple"/></inline-formula> (resp. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e943" xlink:type="simple"/></inline-formula>). Hence, the deterministic Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e944" xlink:type="simple"/></inline-formula> contains <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e945" xlink:type="simple"/></inline-formula> transitions (one 0-labelled and one 1-labelled outgoing transition from each of the 512 state), among which <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e946" xlink:type="simple"/></inline-formula> are labelled by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e947" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e948" xlink:type="simple"/></inline-formula> are labelled by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e949" xlink:type="simple"/></inline-formula>. For instance, in the Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e950" xlink:type="simple"/></inline-formula> there is a transition labelled by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e951" xlink:type="simple"/></inline-formula> (drawn in red in <xref ref-type="fig" rid="pone-0094204-g008">Figure 8</xref>) from state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e952" xlink:type="simple"/></inline-formula> to state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e953" xlink:type="simple"/></inline-formula> because network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e954" xlink:type="simple"/></inline-formula> transits from state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e955" xlink:type="simple"/></inline-formula> to state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e956" xlink:type="simple"/></inline-formula> when it receives input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e957" xlink:type="simple"/></inline-formula>. <xref ref-type="fig" rid="pone-0094204-g008">Figure 8a</xref> illustrates the graph of the deterministic Muller automaton associated to the basal ganglia-thalamocortical network.</p>
<fig id="pone-0094204-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0094204.g008</object-id><label>Figure 8</label><caption>
<title>Deterministic Muller automaton based on the “basal ganglia-thalamocortical” network of <xref ref-type="fig" rid="pone-0094204-g007">Figure 7</xref>.</title>
<p><bold>a.</bold> The graph of the automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e958" xlink:type="simple"/></inline-formula> associated to network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e959" xlink:type="simple"/></inline-formula> contains <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e960" xlink:type="simple"/></inline-formula> states and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e961" xlink:type="simple"/></inline-formula> directed transitions. The colours of the transitions represent their labels: green for label <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e962" xlink:type="simple"/></inline-formula> and red for label <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e963" xlink:type="simple"/></inline-formula>. For sake of readability, the directions of the transitions have been removed. The states and transitions of the strongly connected component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e964" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e965" xlink:type="simple"/></inline-formula> have been pulled out of the central graph and drawn in larger font. <bold>b.</bold> The graph of the strongly connected component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e966" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e967" xlink:type="simple"/></inline-formula>. Every state and transition of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e968" xlink:type="simple"/></inline-formula> that does not belong to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e969" xlink:type="simple"/></inline-formula> has been erased. The directions of the transitions are indicated by the arrowheads.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0094204.g008" position="float" xlink:type="simple"/></fig>
<p>An analysis of the graph of the automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e970" xlink:type="simple"/></inline-formula> reveals that it contains only one strongly connected component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e971" xlink:type="simple"/></inline-formula> given by the states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e972" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e973" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e974" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e975" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e976" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e977" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e978" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e979" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e980" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e981" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e982" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e983" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e984" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e985" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e986" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e987" xlink:type="simple"/></inline-formula> and the transitions between those states, as illustrated in <xref ref-type="fig" rid="pone-0094204-g008">Figure 8b</xref> (we recall that a directed graph is called strongly connected if there is a path from every vertex of the graph to every other vertex). This strongly connected component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e988" xlink:type="simple"/></inline-formula> corresponds to the subgraph of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e989" xlink:type="simple"/></inline-formula> constituted by all states reachable from the initial state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e990" xlink:type="simple"/></inline-formula>. In other words, any state of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e991" xlink:type="simple"/></inline-formula> outside the strongly connected component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e992" xlink:type="simple"/></inline-formula> cannot be reached from the initial state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e993" xlink:type="simple"/></inline-formula>, meaning that it can never occur in the dynamics of network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e994" xlink:type="simple"/></inline-formula> starting from initial state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e995" xlink:type="simple"/></inline-formula>, and hence plays no role in the attractor-based complexity of network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e996" xlink:type="simple"/></inline-formula>. In fact, the attractor-based complexity of network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e997" xlink:type="simple"/></inline-formula> will be precisely determined by the cyclic structure of the strongly connected component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e998" xlink:type="simple"/></inline-formula> of automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e999" xlink:type="simple"/></inline-formula>.</p>
<p>In order to complete the description of the Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1000" xlink:type="simple"/></inline-formula>, it is necessary to specify its table, or in other words, to determine among all possible cycles of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1001" xlink:type="simple"/></inline-formula> which ones are successful and which ones are non-successful. Since every cycle of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1002" xlink:type="simple"/></inline-formula> is by definition contained in a strongly connected component of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1003" xlink:type="simple"/></inline-formula> and since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1004" xlink:type="simple"/></inline-formula> is the only strongly connected component of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1005" xlink:type="simple"/></inline-formula>, it follows that all cycles of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1006" xlink:type="simple"/></inline-formula> are necessarily contained in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1007" xlink:type="simple"/></inline-formula>. Therefore, the specification of the table of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1008" xlink:type="simple"/></inline-formula> amounts to the assignment of a type specification to every cycle of the strongly connected component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1009" xlink:type="simple"/></inline-formula>. According to the biunivocal correspondence between cycles of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1010" xlink:type="simple"/></inline-formula> and attractors of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1011" xlink:type="simple"/></inline-formula>, this assignment procedure consists in determining the type specification (meaningful or spurious) of all possible attractors of the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1012" xlink:type="simple"/></inline-formula>.</p>
<p>In order to assign a type specification to every cycle of the strongly connected component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1013" xlink:type="simple"/></inline-formula>, we have computed the list of all cycles starting from every state of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1014" xlink:type="simple"/></inline-formula>, and for each cycle, we have further computed its decomposition into constitutive cycles (cycles which do not visit the same vertex two times). The results are summarised in <xref ref-type="table" rid="pone-0094204-t002">Table 2</xref>.</p>
<table-wrap id="pone-0094204-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0094204.t002</object-id><label>Table 2</label><caption>
<title>Number of cycles and constitutive cycles found for each starting state of the strongly connected component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1015" xlink:type="simple"/></inline-formula>.</title>
</caption><alternatives><graphic id="pone-0094204-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0094204.t002" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">State</td>
<td align="left" rowspan="1" colspan="1"># cycles</td>
<td align="left" rowspan="1" colspan="1"># constitutive cycles</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">68</td>
<td align="left" rowspan="1" colspan="1">24</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">31</td>
<td align="left" rowspan="1" colspan="1">47</td>
<td align="left" rowspan="1" colspan="1">20</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">33</td>
<td align="left" rowspan="1" colspan="1">87</td>
<td align="left" rowspan="1" colspan="1">24</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">63</td>
<td align="left" rowspan="1" colspan="1">93</td>
<td align="left" rowspan="1" colspan="1">21</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">95</td>
<td align="left" rowspan="1" colspan="1">39</td>
<td align="left" rowspan="1" colspan="1">21</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">127</td>
<td align="left" rowspan="1" colspan="1">21</td>
<td align="left" rowspan="1" colspan="1">17</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">128</td>
<td align="left" rowspan="1" colspan="1">63</td>
<td align="left" rowspan="1" colspan="1">24</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">159</td>
<td align="left" rowspan="1" colspan="1">77</td>
<td align="left" rowspan="1" colspan="1">22</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">161</td>
<td align="left" rowspan="1" colspan="1">72</td>
<td align="left" rowspan="1" colspan="1">20</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">191</td>
<td align="left" rowspan="1" colspan="1">52</td>
<td align="left" rowspan="1" colspan="1">19</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">223</td>
<td align="left" rowspan="1" colspan="1">43</td>
<td align="left" rowspan="1" colspan="1">21</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">255</td>
<td align="left" rowspan="1" colspan="1">53</td>
<td align="left" rowspan="1" colspan="1">17</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">384</td>
<td align="left" rowspan="1" colspan="1">67</td>
<td align="left" rowspan="1" colspan="1">24</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">417</td>
<td align="left" rowspan="1" colspan="1">35</td>
<td align="left" rowspan="1" colspan="1">20</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">479</td>
<td align="left" rowspan="1" colspan="1">48</td>
<td align="left" rowspan="1" colspan="1">16</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">511</td>
<td align="left" rowspan="1" colspan="1">84</td>
<td align="left" rowspan="1" colspan="1">21</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap>
<p>Then, we have assigned a type specification to each cycle of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1016" xlink:type="simple"/></inline-formula> according to the following neurobiological criteria. First, a constitutive cycle is considered to be spurious if it is characterised either by active SC and quiet Thalamus at the same time step or by a quiet GPi/SNR during the majority of the duration of the constitutive cycle. A constitutive cycle is meaningful otherwise. Second, a non-constitutive cycle is considered to be meaningful if it contains a majority of meaningful constitutive cycles, and spurious if it contains a majority of spurious constitutive cycles – and in case of it containing as much meaningful as spurious constitutive cycles, its type specification was chosen to be meaningful. In order to illustrate this procedure, let us consider for example the cycles starting from state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1017" xlink:type="simple"/></inline-formula>. <xref ref-type="table" rid="pone-0094204-t002">Table 2</xref> shows that there are overall <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1018" xlink:type="simple"/></inline-formula> cycles and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1019" xlink:type="simple"/></inline-formula> constitutive cycles starting from state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1020" xlink:type="simple"/></inline-formula>. We consider here the example of one out of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1021" xlink:type="simple"/></inline-formula> cycles, e.g. cycle <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1022" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pone-0094204-g009">Figure 9a</xref>). This cycle can be decomposed into three constitutive cycles (<xref ref-type="fig" rid="pone-0094204-g009">Figure 9b</xref>), namely <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1023" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1024" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1025" xlink:type="simple"/></inline-formula>. When state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1026" xlink:type="simple"/></inline-formula> receives input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1027" xlink:type="simple"/></inline-formula> the network dynamics evolves into the constitutive cycle <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1028" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pone-0094204-g009">Figure 9c</xref>), whereas if state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1029" xlink:type="simple"/></inline-formula> receives input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1030" xlink:type="simple"/></inline-formula> the dynamics evolves into the constitutive cycle <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1031" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pone-0094204-g009">Figure 9d</xref>). According to the aforementioned neurobiological criteria, the constitutive cycles <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1032" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1033" xlink:type="simple"/></inline-formula> are spurious, whereas <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1034" xlink:type="simple"/></inline-formula> is meaningful, and therefore cycle <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1035" xlink:type="simple"/></inline-formula> is spurious.</p>
<fig id="pone-0094204-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0094204.g009</object-id><label>Figure 9</label><caption>
<title>A cycle and its constitutive cycles.</title>
<p><bold>a.</bold> Among all cycles that can be observed starting from state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1036" xlink:type="simple"/></inline-formula> (indicated by the short arrow showing the entry point), we consider here an example, i.e. the cycle (0, 0, 384, 223, 511, 191, 63, 33, 128, 95, 33, 0). <bold>b.</bold> This cycle contains three constitutive cycles <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1037" xlink:type="simple"/></inline-formula>, (0, 384, 223, 511, 191, 63, 33, 0) and (33, 128, 95, 33) that were assigned with type specification spurious (dotted line), meaningful (solid line), and spurious (dotted line), respectively. <bold>c.</bold> Sequence of states with graphical representation of the corresponding activated nodes of the basal ganglia-thalamocortical network for the spurious constitutive cycle <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1038" xlink:type="simple"/></inline-formula>. <bold>d.</bold> Sequence of states and activated network areas for the meaningful constitutive cycle (0, 384, 223, 511, 191, 63, 33, 0). <bold>e.</bold> Sequence of states and activated network areas for the spurious constitutive cycle (33, 128, 95, 33).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0094204.g009" position="float" xlink:type="simple"/></fig>
<p>After the assignation of the type specification to every cycle, the attractor-based complexity of the network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1039" xlink:type="simple"/></inline-formula> can be explicitly computed. More precisely, according to Theorem 6, the attractor-based degree of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1040" xlink:type="simple"/></inline-formula> is given by the length of a maximal (co-)alternating tree contained in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1041" xlink:type="simple"/></inline-formula>. Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1042" xlink:type="simple"/></inline-formula> contains only one strongly connected component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1043" xlink:type="simple"/></inline-formula>, the maximal (co-)alternating tree of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1044" xlink:type="simple"/></inline-formula> is necessarily contained in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1045" xlink:type="simple"/></inline-formula>. Indeed, every cycle of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1046" xlink:type="simple"/></inline-formula> is, being a cycle, necessarily contained in a strongly connected component of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1047" xlink:type="simple"/></inline-formula>; hence in particular, every cycle of the maximal (co-)alternating tree is also contained in a strongly connected component of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1048" xlink:type="simple"/></inline-formula>; yet since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1049" xlink:type="simple"/></inline-formula> is the only strongly connected component of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1050" xlink:type="simple"/></inline-formula>, every cycle of the maximal (co-)alternating tree is contained in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1051" xlink:type="simple"/></inline-formula>, meaning that the maximal (co-)alternating tree itself is contained in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1052" xlink:type="simple"/></inline-formula>.</p>
<p>After an exhaustive analysis of the strongly connected component <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1053" xlink:type="simple"/></inline-formula> and of all its cycles (<xref ref-type="table" rid="pone-0094204-t002">Table 2</xref>) we observed no maximal alternating trees with length above <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1054" xlink:type="simple"/></inline-formula>. Conversely, we found <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1055" xlink:type="simple"/></inline-formula> maximal co-alternating trees of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1056" xlink:type="simple"/></inline-formula> with length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1057" xlink:type="simple"/></inline-formula>. For sake of clarity, we describe one such maximal co-alternating tree: it consists of an alternating sequence of seven cycles included one into the other, summarised in <xref ref-type="table" rid="pone-0094204-t003">Table 3</xref> below and illustrated in <xref ref-type="fig" rid="pone-0094204-g010">Figure 10</xref>. Notice that there is no alternation between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1058" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1059" xlink:type="simple"/></inline-formula> because both cycles <italic>C</italic><sub>0</sub> = (0, 0) and <italic>C</italic><sub>1</sub> = (0, 384, 223, 511, 63, 33, 0) are spurious. According to these results, it follows from Theorem 6 that the attractor-based complexity of network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1060" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1061" xlink:type="simple"/></inline-formula> and that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1062" xlink:type="simple"/></inline-formula> is non-self-dual.</p>
<fig id="pone-0094204-g010" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0094204.g010</object-id><label>Figure 10</label><caption>
<title>A maximal co-alternating tree of the deterministic Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1063" xlink:type="simple"/></inline-formula>.</title>
<p>Panels 0 to 7 illustrate the sequence of eight cycles <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1064" xlink:type="simple"/></inline-formula> one included into the next. Cycles <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1065" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1066" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1067" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1068" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1069" xlink:type="simple"/></inline-formula> are spurious whereas cycles <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1070" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1071" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1072" xlink:type="simple"/></inline-formula> are meaningful. The sequence of cycles <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1073" xlink:type="simple"/></inline-formula> compose a maximal co-alternating tree of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1074" xlink:type="simple"/></inline-formula>. This maximal co-alternating tree contains <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1075" xlink:type="simple"/></inline-formula> alternations between spurious and meaningful cycles, and thus has a length of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1076" xlink:type="simple"/></inline-formula>. Therefore, the attractor-based degree of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1077" xlink:type="simple"/></inline-formula> equals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1078" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0094204.g010" position="float" xlink:type="simple"/></fig><table-wrap id="pone-0094204-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0094204.t003</object-id><label>Table 3</label><caption>
<title>A maximal co-alternating tree of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1079" xlink:type="simple"/></inline-formula> of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1080" xlink:type="simple"/></inline-formula> referred to <xref ref-type="fig" rid="pone-0094204-g010">Figure 10</xref>.</title>
</caption><alternatives><graphic id="pone-0094204-t003-3" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0094204.t003" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Name</td>
<td align="left" rowspan="1" colspan="1">State sequence</td>
<td align="left" rowspan="1" colspan="1">Specification type</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1081" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1082" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">spurious</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1083" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1084" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">meaningful</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1085" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1086" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">spurious</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1087" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1088" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">meaningful</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1089" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1090" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">spurious</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1091" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1092" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">meaningful</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1093" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1094" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">spurious</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap></sec></sec></sec><sec id="s4">
<title>Discussion</title>
<p>The present work revisits and extends in light of modern automata theory the seminal studies by McCulloch and Pitts, Minsky and Kleene concerning the computational power of recurrent neural networks <xref ref-type="bibr" rid="pone.0094204-McCulloch1">[1]</xref>–<xref ref-type="bibr" rid="pone.0094204-Minsky1">[3]</xref>. We present two novel attractor-based complexity measures for Boolean neural networks, and finally illustrate the application of our results to a model of the basal ganglia-thalamocortical network.</p>
<p>More precisely, we prove two computational equivalence between Boolean neural networks and Büchi and Muller automata, and deduce from these results two hierarchical classifications of Boolean recurrent neural networks based on their attractive behaviours. The hierarchical classifications are obtained by translating the Wagner classification theory from the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1095" xlink:type="simple"/></inline-formula>-automaton to the neural network context. The first classification concerns the neural networks characterised by the specification of an output layer and the properties of the attractor dynamics associated with the activation of that output layer. In this case, the obtained hierarchical classification corresponds to a decidable pre-well ordering of width <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1096" xlink:type="simple"/></inline-formula> and height of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1097" xlink:type="simple"/></inline-formula>. The second classification concerns the neural networks whose conditions on the type specifications of their attractors have been totally relaxed. In this case, the resulting hierarchy is significantly richer and consists of a decidable pre-well ordering of width <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1098" xlink:type="simple"/></inline-formula> and height of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1099" xlink:type="simple"/></inline-formula>. We prove that both hierarchical classifications are decidable and provide the decidability procedures aimed at computing the degrees of the networks in the respective hierarchies. We also show that the shorter hierarchy corresponds to an initial segment of the longer one in a precise sense. The notable result is the proof that the two hierarchical classifications are directly related to the attractive properties of the neural networks. More precisely, the degrees of the Boolean neural networks in the hierarchies correspond to the ability of the networks to maximally alternate between visits of meaningful and spurious attractors along their evolutions. The two hierarchies therefore provide two novel complexity measurments of Boolean recurrent neural networks according to their attractive potentialities. These complexity measurements represents an assessment of the computational power of Boolean neural networks in terms of the significance of their attractor dynamics.</p>
<sec id="s4a">
<title>Attractor-Based Complexity Measurement</title>
<p>The degree of a neural network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1100" xlink:type="simple"/></inline-formula> in the RNN hierarchy or in the complete RNN hierarchy corresponds precisely to the length of a maximal alternating chain or alternating tree contained in the graph of its corresponding automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1101" xlink:type="simple"/></inline-formula>, respectively. Since alternating chains and trees are described in terms of accessibility and inclusion relations between cycles of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1102" xlink:type="simple"/></inline-formula>, and according to the biunivocal correspondence between cycles of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1103" xlink:type="simple"/></inline-formula> and attractors of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1104" xlink:type="simple"/></inline-formula>, it follows that the degree of a neural network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1105" xlink:type="simple"/></inline-formula> corresponds precisely to some intricacy relation – accessibility and inclusion – between the set of its meaningful and spurious attractors.</p>
<p>In order to better explain the attractor-based complexity measurement, suppose that some network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1106" xlink:type="simple"/></inline-formula> follows the periodic infinite evolution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1107" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1108" xlink:type="simple"/></inline-formula> are states of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1109" xlink:type="simple"/></inline-formula>. It follows that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1110" xlink:type="simple"/></inline-formula> alternates infinitely often between the two cycles of states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1111" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1112" xlink:type="simple"/></inline-formula>, or equivalently, between the two attractors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1113" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1114" xlink:type="simple"/></inline-formula>. If we suppose that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1115" xlink:type="simple"/></inline-formula> is meaningful and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1116" xlink:type="simple"/></inline-formula> is spurious, then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1117" xlink:type="simple"/></inline-formula> alternates infinitely often between a meaningful and a spurious attractor along the evolution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1118" xlink:type="simple"/></inline-formula>. However, note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1119" xlink:type="simple"/></inline-formula> also visits infinitely often the composed attractor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1120" xlink:type="simple"/></inline-formula>. Hence, if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1121" xlink:type="simple"/></inline-formula> is meaningful (resp. spurious), then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1122" xlink:type="simple"/></inline-formula> not only alternates infinitely often between a meaningful and a spurious attractor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1123" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1124" xlink:type="simple"/></inline-formula> respectively, but also visits infinitely often the third composed meaningful (resp. spurious) attractor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1125" xlink:type="simple"/></inline-formula>.</p>
<p>In fact, for any infinite evolution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1126" xlink:type="simple"/></inline-formula>, there always exists a unique such maximal attractor (maximal for the inclusion relation) that is visited infinitely often. Let us call this attractor the <italic>global attractor</italic> associated to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1127" xlink:type="simple"/></inline-formula>. The attractor-based complexity measurement can now be understood as follows. A network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1128" xlink:type="simple"/></inline-formula> is more complex than a network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1129" xlink:type="simple"/></inline-formula> iff for any infinite evolution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1130" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1131" xlink:type="simple"/></inline-formula>, there exists a corresponding infinite evolution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1132" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1133" xlink:type="simple"/></inline-formula> that can be build “simultaneously” to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1134" xlink:type="simple"/></inline-formula> (in a precise sense described below) and such that, after infinitely many time steps, the types of global attractors visited by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1135" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1136" xlink:type="simple"/></inline-formula> are the very same. In other words, a network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1137" xlink:type="simple"/></inline-formula> is more complex than a network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1138" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1139" xlink:type="simple"/></inline-formula> is able to mimic step by step every possible infinite evolution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1140" xlink:type="simple"/></inline-formula> in order to finally obtain a global attractor of the same type.</p>
<p>This property can actually be precisely expressed in terms of game-theoretic considerations. Consider the game <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1141" xlink:type="simple"/></inline-formula> between networks <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1142" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1143" xlink:type="simple"/></inline-formula> wholes rules are the following. Both networks begin in the rest state. Network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1144" xlink:type="simple"/></inline-formula> begins the game and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1145" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1146" xlink:type="simple"/></inline-formula> play in turn during infinitely many rounds. At every step, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1147" xlink:type="simple"/></inline-formula> chooses a possible next state (accessible from its previous one), and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1148" xlink:type="simple"/></inline-formula> answers by either also choosing a possible next state (accessible from its previous one), or by skipping its turn. However, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1149" xlink:type="simple"/></inline-formula> is obliged to chose infinitely many next states during the game. After infinitely many time steps, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1150" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1151" xlink:type="simple"/></inline-formula> will have produced two infinite evolutions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1152" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1153" xlink:type="simple"/></inline-formula>, respectively. If the types of the global attractors of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1154" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1155" xlink:type="simple"/></inline-formula> are the same, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1156" xlink:type="simple"/></inline-formula> wins the game. Otherwise, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1157" xlink:type="simple"/></inline-formula> wins the game. One can prove that the attractor based complexity measures of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1158" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1159" xlink:type="simple"/></inline-formula> can then be expressed as follows: the degree of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1160" xlink:type="simple"/></inline-formula> is higher than that of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1161" xlink:type="simple"/></inline-formula> iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1162" xlink:type="simple"/></inline-formula> has a winning strategy in the game <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1163" xlink:type="simple"/></inline-formula>.</p>
<p>In other words, a network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1164" xlink:type="simple"/></inline-formula> is more complex than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1165" xlink:type="simple"/></inline-formula> according to our attractor-based complexity iff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1166" xlink:type="simple"/></inline-formula> is capable of mimicking <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1167" xlink:type="simple"/></inline-formula> in every of its possible attractive behaviours. Two networks <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1168" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1169" xlink:type="simple"/></inline-formula> are equivalent if both are capable of mimicking each other in every one of its possible attractive behaviours. Assuming that the set of all possible attractive behaviours of a network is related to its computational power, our attractor-based complexity degree therefore represents a measurement of the computational power of Boolean neural networks in terms of the significance of their attractor dynamics.</p>
<p>Finally, note that the degree of a neural network in the RNN hierarchy or in the complete RNN hierarchy is intimately related to the structure of this network, more precisely to its connectivity. Indeed, for any neural network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1170" xlink:type="simple"/></inline-formula> that would be given without any output layer or type specification of its attractors, it is possible to compute, by some graph analysis, the maximal alternating chains or alternating trees that could be contained in the graph of its corresponding automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1171" xlink:type="simple"/></inline-formula>, and therefore, by theorems 3 and 6, to know the maximal degree that this network could be able to achieve in the RNN or in the complete RNN hierarchy, if the type specification of its attractors were optimally distributed. In other words, any neural network, according to its connectivity structure, contains a potential maximal degree, which is achieved only if the set of its attractors are optimally discriminated into meaningful and spurious types. Hence, based on its connectivity, a certain network could be characterised by a high potential maximal degree, but in practice, due to a very limited discrimination – i.e. non-alternation – between its spurious and meaningful attractors, it will exhibit a low degree of network complexity.</p>
</sec><sec id="s4b">
<title>Significance of measuring network complexity</title>
<p>In an application of our network complexity measurement to a model of a real brain circuit, we demonstrated that, under specific assumptions of connectivity and dynamics, the basal ganglia-thalamocortical network can be modeled by a network of degree <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1172" xlink:type="simple"/></inline-formula>. Why is it so interesting to know this degree? What kind of increased understanding of that network do we gain from that? The degree of network complexity for a given network is important to be determined if we want to assess the computational power that can be achieved by that network. In other words, the degree of network complexity is a functional characteristic of a given network.</p>
<p>For example, a model of the basal ganglia-thalamocortical network with a complexity of degree <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1173" xlink:type="simple"/></inline-formula> is able to perform all possible computations made by a model of the same network with a complexity of degree <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1174" xlink:type="simple"/></inline-formula> and many more computations in addition. If we were able to associate certain functional states of cognitive relevance (or certain pathological conditions of clinical relevance, respectively) to an increase (or to a decrease, respectively) in network complexity, we would certainly gain a better insight into the role and the factors that modulate the operations executed by certain brain circuits.</p>
<p>Then, how and why the network complexity of a model of the basal ganglia-thalamocortical network could vary? The degree of complexity of a network is upper bounded by its potential maximal degree. In the next section, we discuss how control parameters can affect network dynamics and eventually its complexity degree.</p>
</sec><sec id="s4c">
<title>Control parameters of network dynamics</title>
<p>The central hypothesis for brain attractors is that, once activated by appropriate activity, the network behaviour is maintained by continuous reentry of activity, thus generating a high incidence of repeating firing patterns associated with underlying attractors <xref ref-type="bibr" rid="pone.0094204-Abeles1">[37]</xref>, <xref ref-type="bibr" rid="pone.0094204-Amit1">[38]</xref>. The question whether the attractors revealed by certain patterns of activity are spurious or meaningful cannot be answered easily. Certain patterns may repeat above chance and occur transiently during the evolution of a network <xref ref-type="bibr" rid="pone.0094204-Iglesias2">[23]</xref>, <xref ref-type="bibr" rid="pone.0094204-Iglesias5">[55]</xref> and during the transient inactivation of part of the newtwork, as shown experimentally with thalamic firing patterns during reversible inactivation of the cerebral cortex <xref ref-type="bibr" rid="pone.0094204-Tetko3">[60]</xref>, <xref ref-type="bibr" rid="pone.0094204-Villa4">[107]</xref>. On the other hand, patterns and attractors <italic>per se</italic> may reveal an epiphenomenon or a byproduct of the network dynamics, thus being classified as spurious. However, changing conditions and association of attractors into higher-order attractors may turn a spurious into a meaningful type, and vice versa. For this reason, in the present paper, we have emphasised the importance of the specification types of the constitutive cycles and how these affect the specification type of a cycle.</p>
<p>The measurements of networks complexities refer to the possibility of networks' dynamics to maximally alternate between attractors of different types along their evolutions. This is interesting for an overall assessment of the properties of a network because it associates the computational power of that network with the significance of their attractor dynamics.</p>
<p>The excitatory/inhibitory balance in a neural network is the major factor affecting the dynamics of its activity <xref ref-type="bibr" rid="pone.0094204-Amit1">[38]</xref>, <xref ref-type="bibr" rid="pone.0094204-Douglas1">[109]</xref>–<xref ref-type="bibr" rid="pone.0094204-Hill1">[111]</xref>. The activity of the basal ganglia-thalamocortical network is modulated by a complex set of brain structures, including the dopaminergic (including those from the substantia nigra pars compacta like the nigrostriatal and those from the ventromedial tegmental area), cholinergic (including those from the basal forebrain), the noradrenergic (including those from locus coeruleus), serotoninergic (including those from the dorsal raphe), histaminergic (from the tuberomamillary nucleus) and orexinergic nuclei (from the lateral and posterior hypothalamus) <xref ref-type="bibr" rid="pone.0094204-ReinosoSurez1">[103]</xref>, <xref ref-type="bibr" rid="pone.0094204-Phillis1">[112]</xref>–<xref ref-type="bibr" rid="pone.0094204-Parent1">[115]</xref>. These neuromodulators affect, among other parameters, the synaptic kinetics (i.e., the decay time of the synaptic interaction) and the cellular excitability, thus producing stable or unstable spatiotemporally organised modes of activity and rapid state switches <xref ref-type="bibr" rid="pone.0094204-Segundo1">[69]</xref>, <xref ref-type="bibr" rid="pone.0094204-Hill1">[111]</xref>, <xref ref-type="bibr" rid="pone.0094204-Fukai1">[116]</xref>–<xref ref-type="bibr" rid="pone.0094204-Kanamaru1">[119]</xref>. The effect of cholinergic modulation exerted by the basal forebrain is particularly noticeable to this aspect <xref ref-type="bibr" rid="pone.0094204-Villa5">[120]</xref>–<xref ref-type="bibr" rid="pone.0094204-Kanamaru2">[122]</xref>.</p>
<p>The possible different dynamics of a given network can be represented by an equilibrium surface where each point is determined by a network complexity associated with two (in the simplest abstraction) independent variables. Such a situation is illustrated in <xref ref-type="fig" rid="pone-0094204-g011">Figure 11</xref> by the cusp catastrophe of the Catastrophe theory <xref ref-type="bibr" rid="pone.0094204-Thom1">[108]</xref>. In our example, the two control parameters are the excitability and the synaptic kinetics. Depending on the ranges of the parameters that control the network dynamics, the network complexity may remain identical or only slightly modified, in which case we refer to a “smooth” path on the network dynamics surface. In other cases, small changes in the parameter values may provoke rapid state switches corresponding to “sudden” changes in network complexity (e.g., see <xref ref-type="bibr" rid="pone.0094204-Hill1">[111]</xref>).</p>
<fig id="pone-0094204-g011" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0094204.g011</object-id><label>Figure 11</label><caption>
<title>Cusp catastrophe model.</title>
<p>We consider an example of network dynamics controlled by two independent parameters, the synaptic kinetics and the cell excitability. Divergent behaviour is accounted for since as the dynamics moves out from the edge (point A) toward the fold, which is the starting point of separation between an upper and lower limbs, the network dynamics is forced to move towards one of the two opposing behaviours: either point B for network dynamics dominated by deterministic chaos and chaotic itinerancy, or point C for network dynamics dominated by stochastic activity.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0094204.g011" position="float" xlink:type="simple"/></fig>
<p>The network dynamics surface has a singularity represented by a fold (or Riemann-Hugoniot cusp) in it. A bifurcation set is defined by the thresholds where sudden changes can occur, depending on the initial conditions, by projecting the cusp onto the control surface. The network complexity, as defined in this study, depends on the maximal (co-)alternation between spurious and meaningful attractors. In the network dynamics surface, the edge toward the fold (point A, in <xref ref-type="fig" rid="pone-0094204-g011">Figure 11</xref>) is the starting point of separation between two surfaces. One surface is the top sheet representing network dynamics with a high degree of complexity because of the presence of deterministic chaos enabling the possibility to increase the (co-)alternation by mean of chaotic itinerancy (point B, in <xref ref-type="fig" rid="pone-0094204-g011">Figure 11</xref>) <xref ref-type="bibr" rid="pone.0094204-Tsuda1">[66]</xref>, <xref ref-type="bibr" rid="pone.0094204-Freeman3">[67]</xref>, <xref ref-type="bibr" rid="pone.0094204-Segundo1">[69]</xref>. The other surface is the bottom sheet reflecting the dominance of stochastic dynamics, hence absence of alternation (point C, in <xref ref-type="fig" rid="pone-0094204-g011">Figure 11</xref>). Hence, as the network dynamics moves out from the edge near the fold the dynamics is diverging and forced to move toward one of the two opposing behaviours. The path that will be followed by the dynamics depends on the values of the control parameters defining the state of the neural network just prior to reaching the fold. Sudden transitions are accounted for at the edges of the fold, for example as the stochastic dynamics moves along the surface toward the pleat, at some point a small change in control parameters may cause a sudden shift such that, after a long interval without cyclic activity, quasi-random activity develops into quasi-attractors and long cycles may suddenly appear containing many constitutive cycles and many alternations between spurious and meaningful attractors, e.g. tuning thalamic activity by corticofugal activity <xref ref-type="bibr" rid="pone.0094204-Lien1">[123]</xref>, <xref ref-type="bibr" rid="pone.0094204-Lintas1">[124]</xref>.</p>
</sec><sec id="s4d">
<title>Conclusion</title>
<p>The present work can be extended in at least three directions. First, it is expected to study the computational and dynamical complexity of neural networks induced by other mathematical bio-inspired criteria. Indeed, the approach followed in this paper provides a hierarchical classification of neural networks according to the topological complexity of their underlying neural languages, and subsequently, according to the complexity of their attractive behaviours. However, it remains to be clarified how this natural mathematical criterion could be translated into the real biological complexity of the networks. Other complexity measures might bring further insights to the global understanding of brain information processing.</p>
<p>Secondly, it is envisioned to describe the computational power of more biologically oriented neuronal models. For instance, first-order recurrent neural networks provided with some simple spike-timing dependent plasticity (STDP) rule could be of interest <xref ref-type="bibr" rid="pone.0094204-Iglesias3">[48]</xref>, <xref ref-type="bibr" rid="pone.0094204-Turova1">[125]</xref>–<xref ref-type="bibr" rid="pone.0094204-Kerr2">[128]</xref>. Also, neural networks equipped with more complex activation function or dynamical equations governing the membrane dynamics could be relevant <xref ref-type="bibr" rid="pone.0094204-Asai1">[129]</xref>. Important preliminary steps in this direction were made by providing a description of the computational capabilities of static/evolving rational-weighted/analog recurrent neural networks involved in a classical as well as in a memory active and interactive paradigm of computation <xref ref-type="bibr" rid="pone.0094204-Siegelmann1">[6]</xref>, <xref ref-type="bibr" rid="pone.0094204-Siegelmann2">[11]</xref>,<xref ref-type="bibr" rid="pone.0094204-Cabessa1">[27]</xref>,<xref ref-type="bibr" rid="pone.0094204-Cabessa2">[31]</xref>–<xref ref-type="bibr" rid="pone.0094204-Cabessa4">[33]</xref>.</p>
<p>The third and maybe most important extension of our study is oriented towards the application of our new attractor-based complexity measurement to other examples of real neural networks, and to studying the effect of modulatory projections in controlling the network complexity. Indeed, the parameters that control neural dynamics (e.g., excitability and synaptic kinetics) are driven by so-called modulatory projections, such as the cholinergic and serotoninergic projections.</p>
<p>Finally, we believe that the theoretical approach to the computational power of neural models might ultimately bring further insight to the understanding of the intrinsic natures of both biological as well as artificial intelligences. On the one hand, the study of the computational and dynamical capabilities of brain-like models might improve the understanding of the biological features that are most relevant to brain information processing. On the other hand, foundational approaches to alternative models of computation might lead in the long term not only to relevant theoretical considerations <xref ref-type="bibr" rid="pone.0094204-Copeland1">[130]</xref>, <xref ref-type="bibr" rid="pone.0094204-Copeland2">[131]</xref>, but also to practical applications.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pone.0094204.s001" mimetype="application/zip" xlink:href="info:doi/10.1371/journal.pone.0094204.s001" position="float" xlink:type="simple"><label>File S1</label><caption>
<p><bold>Example S1,</bold> Description of a deterministic Büchi automaton, and illustration of the concept of an alternating chain. <bold>Figure S1, A deterministic Büchi automaton </bold><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1175" xlink:type="simple"/></inline-formula><bold>.</bold> The nodes and edges correspond to the states and transitions of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1176" xlink:type="simple"/></inline-formula>, respectively. The node <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1177" xlink:type="simple"/></inline-formula> corresponds to the initial state, as indicated by the short input arrow. The double-circled red nodes correspond to the final states of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1178" xlink:type="simple"/></inline-formula>. The Büchi automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1179" xlink:type="simple"/></inline-formula> contains a maximal alternating chain of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1180" xlink:type="simple"/></inline-formula>, and a maximal co-alternating chain of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1181" xlink:type="simple"/></inline-formula> also.</p>
<p>(ZIP)</p>
</caption></supplementary-material><supplementary-material id="pone.0094204.s002" mimetype="application/zip" xlink:href="info:doi/10.1371/journal.pone.0094204.s002" position="float" xlink:type="simple"><label>File S2</label><caption>
<p><bold>Example S2,</bold> Description of a deterministic Muller automaton, and illustration of the concept of an alternating tree. <bold>Figure S2, A Muller automaton </bold><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1182" xlink:type="simple"/></inline-formula><bold>.</bold> The underlying alphabet of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1183" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1184" xlink:type="simple"/></inline-formula>. The table <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1185" xlink:type="simple"/></inline-formula> represents the set of cycles of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1186" xlink:type="simple"/></inline-formula> that are successful. All other cycles of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1187" xlink:type="simple"/></inline-formula> are by definition non-successful. The successful and non-successful cycles are denoted in green and red, respectively. This Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1188" xlink:type="simple"/></inline-formula> contains a maximal alternating tree of length <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1189" xlink:type="simple"/></inline-formula>.</p>
<p>(ZIP)</p>
</caption></supplementary-material><supplementary-material id="pone.0094204.s003" mimetype="application/zip" xlink:href="info:doi/10.1371/journal.pone.0094204.s003" position="float" xlink:type="simple"><label>File S3</label><caption>
<p><bold>Example S3,</bold> Illustration of the translation procedures described in Propositions 1 and 2. <bold>Figure S3, Panels a, b.</bold> Translation from a neural network to its corresponding deterministic Büchi automaton. <bold>a.</bold> The neural network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1190" xlink:type="simple"/></inline-formula> of <xref ref-type="fig" rid="pone-0094204-g001">Figure 1</xref> provided with an additional specification of an output layer <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1191" xlink:type="simple"/></inline-formula> denoted in red and double-circled. <bold>b.</bold> The deterministic Büchi automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1192" xlink:type="simple"/></inline-formula> corresponding to the neural network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1193" xlink:type="simple"/></inline-formula> of panel <italic>a</italic>. The final states are denoted in red and double-circled, and the active status of the output layer, namely cell <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1194" xlink:type="simple"/></inline-formula>, is emphasised by a bold red <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1195" xlink:type="simple"/></inline-formula>. <bold>Panels c, d.</bold> Translation from a deterministic Büchi automaton to its corresponding neural network. <bold>c.</bold> A deterministic Büchi automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1196" xlink:type="simple"/></inline-formula> with three states. The initial state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1197" xlink:type="simple"/></inline-formula> is denoted with an incoming edge. The final state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1198" xlink:type="simple"/></inline-formula> is emphasised in red and double-circled. <bold>d.</bold> The network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1199" xlink:type="simple"/></inline-formula> corresponding to the Büchi automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1200" xlink:type="simple"/></inline-formula>. The output layer is represented by the cell <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1201" xlink:type="simple"/></inline-formula>, denoted in red and double-circled. The background activities are labeled in blue.</p>
<p>(ZIP)</p>
</caption></supplementary-material><supplementary-material id="pone.0094204.s004" mimetype="application/zip" xlink:href="info:doi/10.1371/journal.pone.0094204.s004" position="float" xlink:type="simple"><label>File S4</label><caption>
<p><bold>Example S4,</bold> Illustration of the decidability procedure of the RNN hierarchy.</p>
<p>(ZIP)</p>
</caption></supplementary-material><supplementary-material id="pone.0094204.s005" mimetype="application/zip" xlink:href="info:doi/10.1371/journal.pone.0094204.s005" position="float" xlink:type="simple"><label>File S5</label><caption>
<p><bold>Example S5,</bold> Illustration of the translation procedures described in Propositions 4 and 5. <bold>Figure S4, Panels a, b.</bold> Translation from a Boolean neural network provided with a type specification of its attractors to its corresponding deterministic Muller automaton. <bold>a.</bold> A neural network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1202" xlink:type="simple"/></inline-formula> provided with an additional type specification of each of its attractors. In this case, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1203" xlink:type="simple"/></inline-formula> contains only one meaningful attractor determined by the following set of states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1204" xlink:type="simple"/></inline-formula>; all other ones are considered as spurious. <bold>b.</bold> The deterministic Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1205" xlink:type="simple"/></inline-formula> corresponding to the neural network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1206" xlink:type="simple"/></inline-formula> of panel <italic>a</italic>. Automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1207" xlink:type="simple"/></inline-formula> works over alphabet <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1208" xlink:type="simple"/></inline-formula>, contains six states, and possesses in its table <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1209" xlink:type="simple"/></inline-formula> the sole cycle <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1210" xlink:type="simple"/></inline-formula>, which corresponds to the sole meaningful attractor of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1211" xlink:type="simple"/></inline-formula>. <bold>Panels c, d.</bold> Translation from a deterministic Muller automaton to its corresponding Boolean neural network provided with a type specification of its attractors. <bold>c.</bold> A deterministic Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1212" xlink:type="simple"/></inline-formula>. The automaton works over alphabet <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1213" xlink:type="simple"/></inline-formula>, has three states, and possesses the two successful cycles <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1214" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1215" xlink:type="simple"/></inline-formula>, as mentioned by its table <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1216" xlink:type="simple"/></inline-formula>. <bold>d.</bold> The neural network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1217" xlink:type="simple"/></inline-formula> corresponding to the Muller automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1218" xlink:type="simple"/></inline-formula> of panel <italic>c</italic>. The network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1219" xlink:type="simple"/></inline-formula> contains two letter cells, one delay cell, and three state cells to simulate the two possible inputs and three states of automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1220" xlink:type="simple"/></inline-formula>. It has only two meaningful attractors corresponding to the two successful cycles of automaton <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0094204.e1221" xlink:type="simple"/></inline-formula>.</p>
<p>(ZIP)</p>
</caption></supplementary-material><supplementary-material id="pone.0094204.s006" mimetype="application/zip" xlink:href="info:doi/10.1371/journal.pone.0094204.s006" position="float" xlink:type="simple"><label>File S6</label><caption>
<p><bold>Example S6,</bold> Illustration of the decidability procedure of the complete RNN hierarchy.</p>
<p>(ZIP)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>The authors thank all the reviewers for their comments, and in particular the last reviewer for his suggestions concerning the proof of Proposition 2.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0094204-McCulloch1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McCulloch</surname><given-names>WS</given-names></name>, <name name-style="western"><surname>Pitts</surname><given-names>W</given-names></name> (<year>1943</year>) <article-title>A logical calculus of the ideas immanent in nervous activity</article-title>. <source>Bulletin of Mathematical Biophysic</source> <volume>5</volume>: <fpage>115</fpage>–<lpage>133</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Kleene1"><label>2</label>
<mixed-citation publication-type="book" xlink:type="simple">Kleene SC (1956) Representation of events in nerve nets and finite automata. In: Automata Studies, Princeton, N. J.: Princeton University Press, volume 34 of <italic>Annals of Mathematics Studies</italic>. pp. 3–42.</mixed-citation>
</ref>
<ref id="pone.0094204-Minsky1"><label>3</label>
<mixed-citation publication-type="book" xlink:type="simple">Minsky ML (1967) Computation: finite and infinite machines. Upper Saddle River, NJ, USA: Prentice-Hall, Inc.</mixed-citation>
</ref>
<ref id="pone.0094204-Kremer1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kremer</surname><given-names>SC</given-names></name> (<year>1995</year>) <article-title>On the computational power of elman-style recurrent networks</article-title>. <source>Neural Networks, IEEE Transactions on</source> <volume>6</volume>: <fpage>1000</fpage>–<lpage>1004</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Sperduti1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sperduti</surname><given-names>A</given-names></name> (<year>1997</year>) <article-title>On the computational power of recurrent neural networks for structures</article-title>. <source>Neural Netw</source> <volume>10</volume>: <fpage>395</fpage>–<lpage>400</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Siegelmann1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Siegelmann</surname><given-names>HT</given-names></name>, <name name-style="western"><surname>Sontag</surname><given-names>ED</given-names></name> (<year>1995</year>) <article-title>On the computational power of neural nets</article-title>. <source>J Comput Syst Sci</source> <volume>50</volume>: <fpage>132</fpage>–<lpage>150</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Hytyniemi1"><label>7</label>
<mixed-citation publication-type="book" xlink:type="simple">Hyötyniemi H (1996) Turing machines are recurrent neural networks. In: Proceedings of STeP'96. Finnish Artificial Intelligence Society, pp. 13–24.</mixed-citation>
</ref>
<ref id="pone.0094204-Neto1"><label>8</label>
<mixed-citation publication-type="book" xlink:type="simple">Neto JaPG, Siegelmann HT, Costa JF, Araujo CPS (1997) Turing universality of neural nets (revisited). In: EUROCAST '97: Proceedings of the A Selection of Papers from the 6th International Workshop on Computer Aided Systems Theory. London, UK: Springer-Verlag, pp. 361–366.</mixed-citation>
</ref>
<ref id="pone.0094204-Kilian1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kilian</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Siegelmann</surname><given-names>HT</given-names></name> (<year>1996</year>) <article-title>The dynamic universality of sigmoidal neural networks</article-title>. <source>Inf Comput</source> <volume>128</volume>: <fpage>48</fpage>–<lpage>56</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Neumann1"><label>10</label>
<mixed-citation publication-type="book" xlink:type="simple">Neumann Jv (1958) The computer and the brain. New Haven, CT, USA: Yale University Press.</mixed-citation>
</ref>
<ref id="pone.0094204-Siegelmann2"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Siegelmann</surname><given-names>HT</given-names></name>, <name name-style="western"><surname>Sontag</surname><given-names>ED</given-names></name> (<year>1994</year>) <article-title>Analog computation via neural networks</article-title>. <source>Theor Comput Sci</source> <volume>131</volume>: <fpage>331</fpage>–<lpage>360</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Balczar1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balcázar</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Gavaldà</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Siegelmann</surname><given-names>HT</given-names></name> (<year>1997</year>) <article-title>Computational power of neural networks: a characterization in terms of kolmogorov complexity</article-title>. <source>IEEE Transactions on Information Theory</source> <volume>43</volume>: <fpage>1175</fpage>–<lpage>1183</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Maass1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Orponen</surname><given-names>P</given-names></name> (<year>1998</year>) <article-title>On the effect of analog noise in discrete-time analog computations</article-title>. <source>Neural Comput</source> <volume>10</volume>: <fpage>1071</fpage>–<lpage>1095</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Maass2"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Sontag</surname><given-names>ED</given-names></name> (<year>1999</year>) <article-title>Analog neural nets with gaussian or other common noise distributions cannot recognize arbitary regular languages</article-title>. <source>Neural Comput</source> <volume>11</volume>: <fpage>771</fpage>–<lpage>782</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Fogel1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fogel</surname><given-names>DB</given-names></name>, <name name-style="western"><surname>Fogel</surname><given-names>LJ</given-names></name>, <name name-style="western"><surname>Porto</surname><given-names>V</given-names></name> (<year>1990</year>) <article-title>Evolving neural networks</article-title>. <source>Biological Cybernetics</source> <volume>63</volume>: <fpage>487</fpage>–<lpage>493</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Whitley1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Whitley</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Dominic</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Das</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Anderson</surname><given-names>CW</given-names></name> (<year>1993</year>) <article-title>Genetic reinforcement learning for neurocontrol problems</article-title>. <source>Machine Learning</source> <volume>13</volume>: <fpage>259</fpage>–<lpage>284</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Moriarty1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moriarty</surname><given-names>DE</given-names></name>, <name name-style="western"><surname>Miikkulainen</surname><given-names>R</given-names></name> (<year>1997</year>) <article-title>Forming neural networks through efficient and adaptive coevolution</article-title>. <source>Evolutionary Computation</source> <volume>5</volume>: <fpage>373</fpage>–<lpage>399</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Yao1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yao</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>Y</given-names></name> (<year>1997</year>) <article-title>A new evolutionary system for evolving artificial neural networks</article-title>. <source>Trans Neur Netw</source> <volume>8</volume>: <fpage>694</fpage>–<lpage>713</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Angeline1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Angeline</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Saunders</surname><given-names>GM</given-names></name>, <name name-style="western"><surname>Pollack</surname><given-names>JB</given-names></name> (<year>1994</year>) <article-title>An evolutionary algorithm that constructs recurrent neural networks</article-title>. <source>Neural Networks, IEEE Transactions on 5</source> <fpage>54</fpage>–<lpage>65</lpage> Angeline1994pp54.</mixed-citation>
</ref>
<ref id="pone.0094204-Chechik1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chechik</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Meilijson</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Ruppin</surname><given-names>E</given-names></name> (<year>1999</year>) <article-title>Neuronal regulation: A mechanism for synaptic pruning during brain maturation</article-title>. <source>Neural Comput</source> <volume>11</volume>: <fpage>2061</fpage>–<lpage>2080</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Iglesias1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Iglesias</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Eriksson</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Pardo</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Tomassini</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Villa</surname><given-names>A</given-names></name> (<year>2005</year>) <article-title>Emergence of oriented cell assemblies associated with spike-timing-dependent plasticity</article-title>. <source>Lecture Notes in Computer Science</source> <volume>3696</volume>: <fpage>127</fpage>–<lpage>132</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Chao1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chao</surname><given-names>TC</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>CM</given-names></name> (<year>2005</year>) <article-title>Learning-induced synchronization and plasticity of a developing neural network</article-title>. <source>Journal of Computational Neuroscience</source> <volume>19</volume>: <fpage>311</fpage>–<lpage>324</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Iglesias2"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Iglesias</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Villa</surname><given-names>AEP</given-names></name> (<year>2010</year>) <article-title>Recurrent spatiotemporal firing patterns in large spiking neural networks with ontogenetic and epigenetic processes</article-title>. <source>J Physiol Paris</source> <volume>104</volume>: <fpage>137</fpage>–<lpage>146</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Perrig1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perrig</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Iglesias</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Shaposhnyk</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Chibirova</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Dutoit</surname><given-names>P</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Functional interactions in hierarchically organized neural networks studied with spatiotemporal firing patterns and phase-coupling frequencies</article-title>. <source>Chin J Physiol</source> <volume>53</volume>: <fpage>382</fpage>–<lpage>395</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Tetzlaff1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tetzlaff</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Okujeni</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Egert</surname><given-names>U</given-names></name>, <name name-style="western"><surname>Wörgötter</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Butz</surname><given-names>M</given-names></name> (<year>2010</year>) <article-title>Self-organized criticality in developing neuronal networks</article-title>. <source>PLoS computational biology</source> <volume>6</volume>: <fpage>e1001013</fpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Shaposhnyk1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shaposhnyk</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Villa</surname><given-names>AEP</given-names></name> (<year>2012</year>) <article-title>Reciprocal projections in hierarchically organized evolvable neural circuits affect EEG-like signals</article-title>. <source>Brain Res</source> <volume>1434</volume>: <fpage>266</fpage>–<lpage>276</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Cabessa1"><label>27</label>
<mixed-citation publication-type="book" xlink:type="simple">Cabessa J, Siegelmann HT (2011) Evolving recurrent neural networks are super-turing. In: IJCNN. IEEE, pp. 3200–3206.</mixed-citation>
</ref>
<ref id="pone.0094204-Turing1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Turing</surname><given-names>AM</given-names></name> (<year>1936</year>) <article-title>On computable numbers, with an application to the Entscheidungsproblem</article-title>. <source>Proc London Math Soc</source> <volume>2</volume>: <fpage>230</fpage>–<lpage>265</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-vanLeeuwen1"><label>29</label>
<mixed-citation publication-type="book" xlink:type="simple">van Leeuwen J, Wiedermann J (2008) How we think of computing today. In: Beckmann A, Dimitracopoulos C, Löwe B, editors, Logic and Theory of Algorithms, Springer Berlin/Heidelberg, volume 5028 of <italic>LNCS</italic>. pp. 579–593.</mixed-citation>
</ref>
<ref id="pone.0094204-Goldin1"><label>30</label>
<mixed-citation publication-type="book" xlink:type="simple">Goldin D, Smolka SA, Wegner P (2006) Interactive Computation: The New Paradigm. Secaucus, NJ, USA: Springer-Verlag New York, Inc.</mixed-citation>
</ref>
<ref id="pone.0094204-Cabessa2"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cabessa</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Villa</surname><given-names>AEP</given-names></name> (<year>2012</year>) <article-title>The expressive power of analog recurrent neural networks on infinite input streams</article-title>. <source>Theor Comput Sci</source> <volume>436</volume>: <fpage>23</fpage>–<lpage>34</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Cabessa3"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cabessa</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Siegelmann</surname><given-names>HT</given-names></name> (<year>2012</year>) <article-title>The computational power of interactive recurrent neural networks</article-title>. <source>Neural Computation</source> <volume>24</volume>: <fpage>996</fpage>–<lpage>1019</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Cabessa4"><label>33</label>
<mixed-citation publication-type="book" xlink:type="simple">Cabessa J, Villa AEP (2013) The super-turing computational power of interactive evolving recurrent neural networks. In: Mladenov V, Koprinkova-Hristova PD, Palm G, Villa AEP, Appollini B, et al., editors, ICANN. Springer, volume 8131 of <italic>Lecture Notes in Computer Science</italic>, pp. 58–65.</mixed-citation>
</ref>
<ref id="pone.0094204-Bchi1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Büchi</surname><given-names>JR</given-names></name> (<year>1966</year>) <article-title>Symposium on decision problems: On a decision method in restricted second order arithmetic</article-title>. <source>Studies in Logic and the Foundations of Mathematics</source> <volume>44</volume>: <fpage>1</fpage>–<lpage>11</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Wagner1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wagner</surname><given-names>K</given-names></name> (<year>1979</year>) <article-title>On <italic>ω</italic>-regular sets</article-title>. <source>Inform and Control</source> <volume>43</volume>: <fpage>123</fpage>–<lpage>177</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Kauffman1"><label>36</label>
<mixed-citation publication-type="book" xlink:type="simple">Kauffman SA (1993) The origins of order: Self-organization and selection in evolution. New York: Oxford University Press.</mixed-citation>
</ref>
<ref id="pone.0094204-Abeles1"><label>37</label>
<mixed-citation publication-type="book" xlink:type="simple">Abeles M (1991) Corticonics: Neural Circuits of the Cerebral Cortex. Cambridge University Press, first edition.</mixed-citation>
</ref>
<ref id="pone.0094204-Amit1"><label>38</label>
<mixed-citation publication-type="book" xlink:type="simple">Amit DJ (1992) Modeling brain function: The world of attractor neural networks. Cambridge University Press.</mixed-citation>
</ref>
<ref id="pone.0094204-Little1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Little</surname><given-names>WA</given-names></name> (<year>1974</year>) <article-title>The existence of persistent states in the brain</article-title>. <source>Mathematical biosciences</source> <volume>19</volume>: <fpage>101</fpage>–<lpage>120</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Little2"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Little</surname><given-names>WA</given-names></name>, <name name-style="western"><surname>Shaw</surname><given-names>GL</given-names></name> (<year>1978</year>) <article-title>Analytical study of the memory storage capacity of a neural network</article-title>. <source>Mathematical biosciences</source> <volume>39</volume>: <fpage>281</fpage>–<lpage>290</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Seung1"><label>41</label>
<mixed-citation publication-type="book" xlink:type="simple">Seung HS (1998) Learning continuous attractors in recurrent networks. In: Advances in Neural Information Processing Systems. MIT Press, pp. 654–660.</mixed-citation>
</ref>
<ref id="pone.0094204-Hopfield1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name> (<year>1982</year>) <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>79</volume>: <fpage>2554</fpage>–<lpage>2558</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Amit2"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Tsodyks</surname><given-names>MV</given-names></name> (<year>1991</year>) <article-title>Quantitative study of attractor neural network retrieving at low spike rates: I. substrate–spikes, rates and neuronal gain</article-title>. <source>Network: Computation in Neural Systems</source> <volume>2</volume>: <fpage>259</fpage>–<lpage>273</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Coolen1"><label>44</label>
<mixed-citation publication-type="book" xlink:type="simple">Coolen T, Sherrington D (1993) Dynamics of Attractor Neural Networks. In: Taylor J, editor, Mathematical Approaches to Neural Networks, Elsevier, volume 51 of <italic>North-Holland Mathematical Library</italic>. pp. 293–306. doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0924-6509(08)70041-2" xlink:type="simple">http://dx.doi.org/10.1016/S0924-6509(08)70041-2</ext-link>. Available: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S0924650908700412" xlink:type="simple">http://www.sciencedirect.com/science/article/pii/S0924650908700412</ext-link>.</mixed-citation>
</ref>
<ref id="pone.0094204-Eliasmith1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eliasmith</surname><given-names>C</given-names></name> (<year>2005</year>) <article-title>A unified approach to building and controlling spiking attractor networks</article-title>. <source>Neural Comput</source> <volume>17</volume>: <fpage>1276</fpage>–<lpage>1314</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Knierim1"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knierim</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>K</given-names></name> (<year>2012</year>) <article-title>Attractor dynamics of spatially correlated neural activity in the limbic system</article-title>. <source>Annu Rev Neurosci</source> <volume>35</volume>: <fpage>267</fpage>–<lpage>285</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Braitenberg1"><label>47</label>
<mixed-citation publication-type="book" xlink:type="simple">Braitenberg V, Schüz A (1998) Cortex: Statistics and Geometry of Neuronal Connectivity. Berlin, Germany: Springer, 249 pp. ISBN: 3-540-63816-4.</mixed-citation>
</ref>
<ref id="pone.0094204-Iglesias3"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Iglesias</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Eriksson</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Grize</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Tomassini</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Villa</surname><given-names>AE</given-names></name> (<year>2005</year>) <article-title>Dynamics of pruning in simulated large-scale spiking neural networks</article-title>. <source>BioSystems</source> <volume>79</volume>: <fpage>11</fpage>–<lpage>20</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Iglesias4"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Iglesias</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Villa</surname><given-names>AEP</given-names></name> (<year>2008</year>) <article-title>Emergence of preferred firing sequences in large spiking neural networks during simulated neuronal development</article-title>. <source>Int J Neural Syst</source> <volume>18</volume>: <fpage>267</fpage>–<lpage>277</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Abeles2"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abeles</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Gerstein</surname><given-names>GL</given-names></name> (<year>1988</year>) <article-title>Detecting spatiotemporal firing patterns among simultaneously recorded single neurons</article-title>. <source>J Neurophysiol</source> <volume>60</volume>: <fpage>909</fpage>–<lpage>924</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Villa1"><label>51</label>
<mixed-citation publication-type="book" xlink:type="simple">Villa AEP (2000) Empirical Evidence about Temporal Structure in Multi-unit Recordings. In: Miller R, editor, Time and the brain, Amsterdam, The Netherlands: Harwood Academic, volume 3 of <italic>Conceptual Advances in Brain Research</italic>, chapter 1. pp. 1–51.</mixed-citation>
</ref>
<ref id="pone.0094204-Tetko1"><label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tetko</surname><given-names>IV</given-names></name>, <name name-style="western"><surname>Villa</surname><given-names>AEP</given-names></name> (<year>2001</year>) <article-title>A pattern grouping algorithm for analysis of spatiotemporal patterns in neuronal spike trains. 1. Detection of repeated patterns</article-title>. <source>J Neurosci Meth</source> <volume>105</volume>: <fpage>1</fpage>–<lpage>14</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Villa2"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Villa</surname><given-names>AEP</given-names></name>, <name name-style="western"><surname>Abeles</surname><given-names>M</given-names></name> (<year>1990</year>) <article-title>Evidence for spatiotemporal firing patterns within the auditory thalamus of the cat</article-title>. <source>Brain Res</source> <volume>509</volume>: <fpage>325</fpage>–<lpage>327</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Tetko2"><label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tetko</surname><given-names>IV</given-names></name>, <name name-style="western"><surname>Villa</surname><given-names>AEP</given-names></name> (<year>1997</year>) <article-title>Fast combinatorial methods to estimate the probability of complex temporal patterns of spikes</article-title>. <source>Biol Cybern</source> <volume>76</volume>: <fpage>397</fpage>–<lpage>408</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Iglesias5"><label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Iglesias</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Villa</surname><given-names>AEP</given-names></name> (<year>2007</year>) <article-title>Effect of stimulus-driven pruning on the detection of spatiotemporal patterns of activity in large neural networks</article-title>. <source>BioSystems</source> <volume>89</volume>: <fpage>287</fpage>–<lpage>293</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Iglesias6"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Iglesias</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Chibirova</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Villa</surname><given-names>A</given-names></name> (<year>2007</year>) <article-title>Nonlinear dynamics emerging in large scale neural networks with ontogenetic and epigenetic processes</article-title>. <source>Lecture Notes in Computer Science</source> <volume>4668</volume>: <fpage>579</fpage>–<lpage>588</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Abeles3"><label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abeles</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Bergman</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Margalit</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Vaadia</surname><given-names>E</given-names></name> (<year>1993</year>) <article-title>Spatiotemporal firing patterns in the frontal cortex of behaving monkeys</article-title>. <source>J Neurophysiol</source> <volume>70</volume>: <fpage>1629</fpage>–<lpage>1638</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Prut1"><label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Prut</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Vaadia</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Bergman</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Slovin</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Abeles</surname><given-names>M</given-names></name> (<year>1998</year>) <article-title>Spatiotemporal structure of cortical activity: Properties and behavioral relevance</article-title>. <source>J Neurophysiol</source> <volume>79</volume>: <fpage>2857</fpage>–<lpage>2874</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Villa3"><label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Villa</surname><given-names>AEP</given-names></name>, <name name-style="western"><surname>Tetko</surname><given-names>IV</given-names></name>, <name name-style="western"><surname>Hyland</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Najem</surname><given-names>A</given-names></name> (<year>1999</year>) <article-title>Spatiotemporal activity patterns of rat cortical neurons predict responses in a conditioned task</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>96</volume>: <fpage>1106</fpage>–<lpage>1111</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Tetko3"><label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tetko</surname><given-names>IV</given-names></name>, <name name-style="western"><surname>Villa</surname><given-names>AE</given-names></name> (<year>2001</year>) <article-title>A pattern grouping algorithm for analysis of spatiotemporal patterns in neuronal spike trains. 2. application to simultaneous single unit recordings</article-title>. <source>J Neurosci Meth</source> <volume>105</volume>: <fpage>15</fpage>–<lpage>24</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Shmiel1"><label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shmiel</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Drori</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Shmiel</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Ben-Shaul</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Nadasdy</surname><given-names>Z</given-names></name>, <etal>et al</etal>. (<year>2005</year>) <article-title>Neurons of the cerebral cortex exhibit precise interspike timing in correspondence to behavior</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>102</volume>: <fpage>18655</fpage>–<lpage>18657</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Amari1"><label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amari</surname><given-names>SI</given-names></name> (<year>1975</year>) <article-title>Homogeneous nets of neuron-like elements</article-title>. <source>Biol Cybern</source> <volume>17</volume>: <fpage>211</fpage>–<lpage>220</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Skarda1"><label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Skarda</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Freeman</surname><given-names>WJ</given-names></name> (<year>1987</year>) <article-title>How brains make chaos in order to make sense of the world</article-title>. <source>Behavioral and brain sciences</source> <volume>10</volume>: <fpage>161</fpage>–<lpage>195</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Freeman1"><label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Freeman</surname><given-names>WJ</given-names></name> (<year>2003</year>) <article-title>A neurobiological theory of meaning in perception Part I: Information and meaning in nonconvergent and nonlocal brain dynamics</article-title>. <source>International Journal of Bifurcation and Chaos</source> <volume>13</volume>: <fpage>2493</fpage>–<lpage>2511</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Freeman2"><label>65</label>
<mixed-citation publication-type="book" xlink:type="simple">Freeman W (1975) Mass action in the nervous system. Academic Press.</mixed-citation>
</ref>
<ref id="pone.0094204-Tsuda1"><label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsuda</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Koerner</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Shimizu</surname><given-names>H</given-names></name> (<year>1987</year>) <article-title>Memory dynamics in asynchronous neural networks</article-title>. <source>Prog Th Phys</source> <volume>78</volume>: <fpage>51</fpage>–<lpage>71</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Freeman3"><label>67</label>
<mixed-citation publication-type="book" xlink:type="simple">Freeman W (1990) On the problem of anomalous dispersion in chaoto-chaotic phase transitions of neural masses, and its significance for the management of perceptual information in brains. In: Haken H, Stadler M, editors, Synergetics of Cognition, Springer Berlin Heidelberg, volume 45 of <italic>Springer Series in Synergetics</italic>. pp. 126–143.</mixed-citation>
</ref>
<ref id="pone.0094204-Tsuda2"><label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsuda</surname><given-names>I</given-names></name> (<year>2001</year>) <article-title>Toward an interpretation of dynamic neural activity in terms of chaotic dynamical systems</article-title>. <source>Behav Brain Sci</source> <volume>24</volume>: <fpage>793</fpage>–<lpage>810</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Segundo1"><label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Segundo</surname><given-names>JP</given-names></name> (<year>2003</year>) <article-title>Nonlinear dynamics of point process systems and data</article-title>. <source>International Journal of Bifurcation and Chaos</source> <volume>13</volume>: <fpage>2035</fpage>–<lpage>2116</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Fujii1"><label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fujii</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Tsuda</surname><given-names>I</given-names></name> (<year>2004</year>) <article-title>Neocortical gap junction-coupled interneuron systems may induce chaotic behavior itinerant among quasi-attractors exhibiting transient synchrony</article-title>. <source>Neurocomputing</source> <volume>58</volume>: <fpage>151</fpage>–<lpage>157</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Hopfield2"><label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Feinstein</surname><given-names>DI</given-names></name>, <name name-style="western"><surname>Palmer</surname><given-names>RG</given-names></name> (<year>1983</year>) <article-title>‘Unlearning’ has a stabilizing effect in collective memories</article-title>. <source>Nature</source> <volume>304</volume>: <fpage>158</fpage>–<lpage>159</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Watta1"><label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Watta</surname><given-names>PB</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Hassoun</surname><given-names>MH</given-names></name> (<year>1997</year>) <article-title>Recurrent neural nets as dynamical boolean systems with application to associative memory</article-title>. <source>IEEE Trans Neural Netw</source> <volume>8</volume>: <fpage>1268</fpage>–<lpage>1280</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Amit3"><label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Treves</surname><given-names>A</given-names></name> (<year>1989</year>) <article-title>Associative memory neural network with low temporal spiking rates</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>86</volume>: <fpage>7871</fpage>–<lpage>7875</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Griniasty1"><label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Griniasty</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Tsodyks</surname><given-names>MV</given-names></name>, <name name-style="western"><surname>Amit</surname><given-names>DJ</given-names></name> (<year>1993</year>) <article-title>Conversion of temporal correlations between stimuli to spatial correlations between attractors</article-title>. <source>Neural Computation</source> <volume>5</volume>: <fpage>1</fpage>–<lpage>17</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Nara1"><label>75</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nara</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Davis</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Totsuji</surname><given-names>H</given-names></name> (<year>1993</year>) <article-title>Memory search using complex dynamics in a recurrent neural network model</article-title>. <source>Neural Networks</source> <volume>6</volume>: <fpage>963</fpage>–<lpage>973</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Sandberg1"><label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sandberg</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Lansner</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Petersson</surname><given-names>KM</given-names></name>, <name name-style="western"><surname>Ekeberg</surname><given-names>O</given-names></name> (<year>2002</year>) <article-title>A bayesian attractor network with incremental learning</article-title>. <source>Network</source> <volume>13</volume>: <fpage>179</fpage>–<lpage>194</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Knoblauch1"><label>77</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Knoblauch</surname><given-names>A</given-names></name> (<year>2011</year>) <article-title>Neural associative memory with optimal bayesian learning</article-title>. <source>Neural Computation</source> <volume>23</volume>: <fpage>1393</fpage>–<lpage>1451</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Wadge1"><label>78</label>
<mixed-citation publication-type="book" xlink:type="simple">Wadge WW (1983) Reducibility and determinateness on the Baire space. Ph.D. thesis, University of California, Berkeley.</mixed-citation>
</ref>
<ref id="pone.0094204-Perrin1"><label>79</label>
<mixed-citation publication-type="book" xlink:type="simple">Perrin D, Pin JE (2004) Infinite Words, volume 141 of <italic>Pure and Applied Mathematics</italic>. Elsevier. ISBN 0-12-532111-2.</mixed-citation>
</ref>
<ref id="pone.0094204-McNaughton1"><label>80</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McNaughton</surname><given-names>R</given-names></name> (<year>1966</year>) <article-title>Testing and generating infinite sequences by a finite automaton</article-title>. <source>Information and control</source> <volume>9</volume>: <fpage>521</fpage>–<lpage>530</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Piterman1"><label>81</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Piterman</surname><given-names>N</given-names></name> (<year>2007</year>) <article-title>From nondeterministic büchi and streett automata to deterministic parity automata</article-title>. <source>Logical Methods in Computer Science</source> <volume>3</volume>: <fpage>1</fpage>–<lpage>21</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Selivanov1"><label>82</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Selivanov</surname><given-names>VL</given-names></name> (<year>1998</year>) <article-title>Fine hierarchy of regular omega-languages</article-title>. <source>Theor Comput Sci</source> <volume>191</volume>: <fpage>37</fpage>–<lpage>59</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Tsuda3"><label>83</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsuda</surname><given-names>I</given-names></name> (<year>1991</year>) <article-title>Chaotic itinerancy as a dynamical basis of hermeneutics of brain and mind</article-title>. <source>World Futures</source> <volume>32</volume>: <fpage>167</fpage>–<lpage>185</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Kaneko1"><label>84</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kaneko</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Tsuda</surname><given-names>I</given-names></name> (<year>2003</year>) <article-title>Chaotic itinerancy</article-title>. <source>Chaos</source> <volume>13</volume>: <fpage>926</fpage>–<lpage>936</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Alexander1"><label>85</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alexander</surname><given-names>GE</given-names></name>, <name name-style="western"><surname>Crutcher</surname><given-names>MD</given-names></name> (<year>1990</year>) <article-title>Functional architecture of basal ganglia circuits: neural substrates of parallel processing</article-title>. <source>Trends Neurosci</source> <volume>13</volume>: <fpage>266</fpage>–<lpage>271</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Hoover1"><label>86</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hoover</surname><given-names>JE</given-names></name>, <name name-style="western"><surname>Strick</surname><given-names>PL</given-names></name> (<year>1993</year>) <article-title>Multiple output channels in the basal ganglia</article-title>. <source>Science</source> <volume>259</volume>: <fpage>819</fpage>–<lpage>821</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Asanuma1"><label>87</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Asanuma</surname><given-names>C</given-names></name> (<year>1994</year>) <article-title>GABAergic and pallidal terminals in the thalamic reticular nucleus of squirrel monkeys</article-title>. <source>Exp Brain Res</source> <volume>101</volume>: <fpage>439</fpage>–<lpage>451</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Groenewegen1"><label>88</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Groenewegen</surname><given-names>HJ</given-names></name>, <name name-style="western"><surname>Galis-de Graaf</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Smeets</surname><given-names>WJ</given-names></name> (<year>1999</year>) <article-title>Integration and segregation of limbic cortico-striatal loops at the thalamic level: an experimental tracing study in rats</article-title>. <source>J Chem Neuroanat</source> <volume>16</volume>: <fpage>167</fpage>–<lpage>185</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Yasukawa1"><label>89</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yasukawa</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Kita</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Xue</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Kita</surname><given-names>H</given-names></name> (<year>2004</year>) <article-title>Rat intralaminar thalamic nuclei projections to the globus pallidus: a biotinylated dextran amine anterograde tracing study</article-title>. <source>J Comp Neurol</source> <volume>471</volume>: <fpage>153</fpage>–<lpage>167</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Cebrin1"><label>90</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cebrián</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Parent</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Prensa</surname><given-names>L</given-names></name> (<year>2005</year>) <article-title>Patterns of axonal branching of neurons of the substantia nigra pars reticulata and pars lateralis in the rat</article-title>. <source>J Comp Neurol</source> <volume>492</volume>: <fpage>349</fpage>–<lpage>369</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Degos1"><label>91</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Degos</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Deniau</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Le Cam</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Mailly</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Maurice</surname><given-names>N</given-names></name> (<year>2008</year>) <article-title>Evidence for a direct subthalamo-cortical loop circuit in the rat</article-title>. <source>Eur J Neurosci</source> <volume>27</volume>: <fpage>2599</fpage>–<lpage>2610</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Smith1"><label>92</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Raju</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Nanda</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Pare</surname><given-names>JF</given-names></name>, <name name-style="western"><surname>Galvan</surname><given-names>A</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>The thalamostriatal systems: anatomical and functional organization in normal and parkinsonian states</article-title>. <source>Brain Res Bull</source> <volume>78</volume>: <fpage>60</fpage>–<lpage>68</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Gandhi1"><label>93</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gandhi</surname><given-names>NJ</given-names></name>, <name name-style="western"><surname>Katnani</surname><given-names>HA</given-names></name> (<year>2011</year>) <article-title>Motor functions of the superior colliculus</article-title>. <source>Annu Rev Neurosci</source> <volume>34</volume>: <fpage>205</fpage>–<lpage>231</lpage> Gandhi2011pp205.</mixed-citation>
</ref>
<ref id="pone.0094204-Krauzlis1"><label>94</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Krauzlis</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Lovejoy</surname><given-names>LP</given-names></name>, <name name-style="western"><surname>Zénon</surname><given-names>A</given-names></name> (<year>2013</year>) <article-title>Superior colliculus and visual spatial attention</article-title>. <source>Annu Rev Neurosci</source> <volume>36</volume>: <fpage>165</fpage>–<lpage>182</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Terman1"><label>95</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Terman</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Rubin</surname><given-names>JE</given-names></name>, <name name-style="western"><surname>Yew</surname><given-names>AC</given-names></name>, <name name-style="western"><surname>Wilson</surname><given-names>CJ</given-names></name> (<year>2002</year>) <article-title>Activity patterns in a model for the subthalamopallidal network of the basal ganglia</article-title>. <source>J Neurosci</source> <volume>22</volume>: <fpage>2963</fpage>–<lpage>2976</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Nakahara1"><label>96</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nakahara</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Amari Si</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Hikosaka</surname><given-names>O</given-names></name> (<year>2002</year>) <article-title>Self-organization in the basal ganglia with modulation of reinforcement signals</article-title>. <source>Neural Comput</source> <volume>14</volume>: <fpage>819</fpage>–<lpage>844</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Rubin1"><label>97</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rubin</surname><given-names>JE</given-names></name>, <name name-style="western"><surname>Terman</surname><given-names>D</given-names></name> (<year>2004</year>) <article-title>High frequency stimulation of the subthalamic nucleus eliminates pathological thalamic rhythmicity in a computational model</article-title>. <source>J Comput Neurosci</source> <volume>16</volume>: <fpage>211</fpage>–<lpage>235</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Jones1"><label>98</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jones</surname><given-names>BE</given-names></name> (<year>2005</year>) <article-title>From waking to sleeping: neuronal and chemical substrates</article-title>. <source>Trends Pharmacol Sci</source> <volume>26</volume>: <fpage>578</fpage>–<lpage>586</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Leblois1"><label>99</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leblois</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Boraud</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Meissner</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Bergman</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Hansel</surname><given-names>D</given-names></name> (<year>2006</year>) <article-title>Competition between feedback loops underlies normal and pathological dynamics in the basal ganglia</article-title>. <source>J Neurosci</source> <volume>26</volume>: <fpage>3567</fpage>–<lpage>3583</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Silkis1"><label>100</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Silkis</surname><given-names>I</given-names></name> (<year>2007</year>) <article-title>A hypothetical role of cortico-basal ganglia-thalamocortical loops in visual processing</article-title>. <source>Biosystems</source> <volume>89</volume>: <fpage>227</fpage>–<lpage>235</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Tsujino1"><label>101</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsujino</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Sakurai</surname><given-names>T</given-names></name> (<year>2009</year>) <article-title>Orexin/hypocretin: a neuropeptide at the interface of sleep, energy homeostasis, and reward system</article-title>. <source>Pharmacol Rev</source> <volume>61</volume>: <fpage>162</fpage>–<lpage>176</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-vanAlbada1"><label>102</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Albada</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Robinson</surname><given-names>PA</given-names></name> (<year>2009</year>) <article-title>Mean-field modeling of the basal ganglia-thalamocortical system. I Firing rates in healthy and parkinsonian states</article-title>. <source>J Theor Biol</source> <volume>257</volume>: <fpage>642</fpage>–<lpage>663</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-ReinosoSurez1"><label>103</label>
<mixed-citation publication-type="book" xlink:type="simple">Reinoso-Suárez F, De Andrés I, Garzón M (2011) The Sleep–Wakefulness Cycle, volume 208 of <italic>Advances in Anatomy, Embryology and Cell Biology</italic>. Berlin Heidelberg: Springer, 1–128 pp.</mixed-citation>
</ref>
<ref id="pone.0094204-Meijer1"><label>104</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meijer</surname><given-names>HG</given-names></name>, <name name-style="western"><surname>Krupa</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Cagnan</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Lourens</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Heida</surname><given-names>T</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>From Parkinsonian thalamic activity to restoring thalamic relay using deep brain stimulation: new insights from computational modeling</article-title>. <source>J Neural Eng</source> <volume>8</volume>: <fpage>066005</fpage>–<lpage>066005</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Kerr1"><label>105</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kerr</surname><given-names>CC</given-names></name>, <name name-style="western"><surname>Neymotin</surname><given-names>SA</given-names></name>, <name name-style="western"><surname>Chadderdon</surname><given-names>GL</given-names></name>, <name name-style="western"><surname>Fietkiewicz</surname><given-names>CT</given-names></name>, <name name-style="western"><surname>Francis</surname><given-names>JT</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Electrostimulation as a prosthesis for repair of information flow in a computer model of neocortex</article-title>. <source>Neural Systems and Rehabilitation Engineering, IEEE Transactions on</source> <volume>20</volume>: <fpage>153</fpage>–<lpage>160</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Guthrie1"><label>106</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Guthrie</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Leblois</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Garenne</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Boraud</surname><given-names>T</given-names></name> (<year>2013</year>) <article-title>Interaction between cognitive and motor cortico-basal ganglia loops during decision making: a computational study</article-title>. <source>J Neurophysiol</source> <volume>109</volume>: <fpage>3025</fpage>–<lpage>3040</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Villa4"><label>107</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Villa</surname><given-names>AEP</given-names></name>, <name name-style="western"><surname>Tetko</surname><given-names>IV</given-names></name>, <name name-style="western"><surname>Dutoit</surname><given-names>P</given-names></name>, <name name-style="western"><surname>De Ribaupierre</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>De Ribaupierre</surname><given-names>F</given-names></name> (<year>1999</year>) <article-title>Corticofugal modulation of functional connectivity within the auditory thalamus of rat, guinea pig and cat revealed by cooling deactivation</article-title>. <source>J Neurosci Methods</source> <volume>86</volume>: <fpage>161</fpage>–<lpage>178</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Thom1"><label>108</label>
<mixed-citation publication-type="book" xlink:type="simple">Thom R (1972) Stabilité structurelle et morphogenèse. Essai d'une théorie générale des modèles. oaris: InterÉditions.</mixed-citation>
</ref>
<ref id="pone.0094204-Douglas1"><label>109</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Douglas</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Martin</surname><given-names>KA</given-names></name>, <name name-style="western"><surname>Whitteridge</surname><given-names>D</given-names></name> (<year>1989</year>) <article-title>A canonical microcircuit for neocortex</article-title>. <source>Neural computation</source> <volume>1</volume>: <fpage>480</fpage>–<lpage>488</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-vanVreeswijk1"><label>110</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Vreeswijk</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>1996</year>) <article-title>Chaos in neuronal networks with balanced excitatory and inhibitory activity</article-title>. <source>Science</source> <volume>274</volume>: <fpage>1724</fpage>–<lpage>1726</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Hill1"><label>111</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hill</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Villa</surname><given-names>AEP</given-names></name> (<year>1997</year>) <article-title>Dynamic transitions in global network activity influenced by the balance of excitation and inhibtion</article-title>. <source>Network: computational neural networks</source> <volume>8</volume>: <fpage>165</fpage>–<lpage>184</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Phillis1"><label>112</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Phillis</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Kirkpatrick</surname><given-names>JR</given-names></name> (<year>1980</year>) <article-title>The actions of motilin, luteinizing hormone releasing hormone, cholecystokinin, somatostatin, vasoactive intestinal peptide, and other peptides on rat cerebral cortical neurons</article-title>. <source>Can J Physiol Pharmacol</source> <volume>58</volume>: <fpage>612</fpage>–<lpage>623</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Steriade1"><label>113</label>
<mixed-citation publication-type="book" xlink:type="simple">Steriade M, Jones EG, Llinás R (1990) Thalamic oscillations and signalling. New York: Wiley.</mixed-citation>
</ref>
<ref id="pone.0094204-Wright1"><label>114</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wright</surname><given-names>JJ</given-names></name> (<year>1990</year>) <article-title>Reticular activation and the dynamics of neuronal networks</article-title>. <source>Biol Cybern</source> <volume>62</volume>: <fpage>289</fpage>–<lpage>298</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Parent1"><label>115</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parent</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Hazrati</surname><given-names>LN</given-names></name> (<year>1995</year>) <article-title>Functional anatomy of the basal ganglia. i. the cortico-basal ganglia-thalamo-cortical loop</article-title>. <source>Brain Res Brain Res Rev</source> <volume>20</volume>: <fpage>91</fpage>–<lpage>127</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Fukai1"><label>116</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fukai</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Shiino</surname><given-names>M</given-names></name> (<year>1990</year>) <article-title>Asymmetric neural networks incorporating the Dale hypothesis and noise-driven chaos</article-title>. <source>Phys Rev Lett</source> <volume>64</volume>: <fpage>1465</fpage>–<lpage>1468</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Tsodyks1"><label>117</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsodyks</surname><given-names>MV</given-names></name>, <name name-style="western"><surname>Sejnowski</surname><given-names>T</given-names></name> (<year>1995</year>) <article-title>Rapid state switching in balanced cortical network models</article-title>. <source>Network: Computation in Neural Systems</source> <volume>6</volume>: <fpage>111</fpage>–<lpage>124</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Taylor1"><label>118</label>
<mixed-citation publication-type="book" xlink:type="simple">Taylor JG, Villa AEP (2001) The “Conscious I”: A Neuroheuristic Approach to the Mind. In: Baltimore D, Dulbecco R, Jacob F, Levi Montalcini R, editors, Frontiers of Life, Academic Press, volume III. pp. 349–270. ISBN: 0-12-077340-6.</mixed-citation>
</ref>
<ref id="pone.0094204-Kanamaru1"><label>119</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kanamaru</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Sekine</surname><given-names>M</given-names></name> (<year>2005</year>) <article-title>Synchronized firings in the networks of class 1 excitable neurons with excitatory and inhibitory connections and their dependences on the forms of interactions</article-title>. <source>Neural Computation</source> <volume>17</volume>: <fpage>1315</fpage>–<lpage>1338</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Villa5"><label>120</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Villa</surname><given-names>AEP</given-names></name>, <name name-style="western"><surname>Bajo Lorenzana</surname><given-names>VM</given-names></name>, <name name-style="western"><surname>Vantini</surname><given-names>G</given-names></name> (<year>1996</year>) <article-title>Nerve growth factor modulates information processing in the auditory thalamus</article-title>. <source>Brain Res Bull</source> <volume>39</volume>: <fpage>139</fpage>–<lpage>147</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Villa6"><label>121</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Villa</surname><given-names>AEP</given-names></name>, <name name-style="western"><surname>Tetko</surname><given-names>IV</given-names></name>, <name name-style="western"><surname>Dutoit</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Vantini</surname><given-names>G</given-names></name> (<year>2000</year>) <article-title>Non-linear cortico-cortical interactions modulated by cholinergic afferences from the rat basal forebrain</article-title>. <source>Biosystems</source> <volume>58</volume>: <fpage>219</fpage>–<lpage>228</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Kanamaru2"><label>122</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kanamaru</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Fujii</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Aihara</surname><given-names>K</given-names></name> (<year>2013</year>) <article-title>Deformation of attractor landscape via cholinergic presynaptic modulations: a computational study using a phase neuron model</article-title>. <source>PLoS One</source> <volume>8</volume>.</mixed-citation>
</ref>
<ref id="pone.0094204-Lien1"><label>123</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lien</surname><given-names>AD</given-names></name>, <name name-style="western"><surname>Scanziani</surname><given-names>M</given-names></name> (<year>2013</year>) <article-title>Tuned thalamic excitation is amplified by visual cortical circuits</article-title>. <source>Nat Neurosci</source> <volume>16</volume>: <fpage>1315</fpage>–<lpage>1323</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Lintas1"><label>124</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lintas</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Schwaller</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Villa</surname><given-names>AE</given-names></name> (<year>2013</year>) <article-title>Visual thalamocortical circuits in parvalbumin-deficient mice</article-title>. <source>Brain Res</source></mixed-citation>
</ref>
<ref id="pone.0094204-Turova1"><label>125</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Turova</surname><given-names>TS</given-names></name>, <name name-style="western"><surname>Villa</surname><given-names>AEP</given-names></name> (<year>2007</year>) <article-title>On a phase diagram for random neural networks with embedded spike timing dependent plasticity</article-title>. <source>Biosystems</source> <volume>89</volume>: <fpage>280</fpage>–<lpage>286</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Kozloski1"><label>126</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kozloski</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Cecchi</surname><given-names>GA</given-names></name> (<year>2010</year>) <article-title>A theory of loop formation and elimination by spike timing-dependent plasticity</article-title>. <source>Front Neural Circuits</source> <volume>4</volume>: <fpage>e7</fpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Waddington1"><label>127</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Waddington</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Appleby</surname><given-names>PA</given-names></name>, <name name-style="western"><surname>De Kamps</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Cohen</surname><given-names>N</given-names></name> (<year>2012</year>) <article-title>Triphasic spike-timing-dependent plasticity organizes networks to produce robust sequences of neural activity</article-title>. <source>Front Comput Neurosci</source> <volume>6</volume>: <fpage>e88</fpage> Waddington2012e88.</mixed-citation>
</ref>
<ref id="pone.0094204-Kerr2"><label>128</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kerr</surname><given-names>RR</given-names></name>, <name name-style="western"><surname>Burkitt</surname><given-names>AN</given-names></name>, <name name-style="western"><surname>Thomas</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Gilson</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Grayden</surname><given-names>DB</given-names></name> (<year>2013</year>) <article-title>Delay selection by spike-timing-dependent plasticity in recurrent networks of spiking neurons receiving oscillatory inputs</article-title>. <source>PLoS Comput Biol</source> <volume>9</volume>.</mixed-citation>
</ref>
<ref id="pone.0094204-Asai1"><label>129</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Asai</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Villa</surname><given-names>AEP</given-names></name> (<year>2012</year>) <article-title>Integration and transmission of distributed deterministic neural activity in feed-forward networks</article-title>. <source>Brain Res</source> <volume>1434</volume>: <fpage>17</fpage>–<lpage>33</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Copeland1"><label>130</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Copeland</surname><given-names>BJ</given-names></name> (<year>2002</year>) <article-title>Hypercomputation</article-title>. <source>Minds Mach</source> <volume>12</volume>: <fpage>461</fpage>–<lpage>502</lpage>.</mixed-citation>
</ref>
<ref id="pone.0094204-Copeland2"><label>131</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Copeland</surname><given-names>BJ</given-names></name> (<year>2004</year>) <article-title>Hypercomputation: philosophical issues</article-title>. <source>Theor Comput Sci</source> <volume>317</volume>: <fpage>251</fpage>–<lpage>267</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>