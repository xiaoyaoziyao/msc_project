<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
<front>
<journal-meta><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id><journal-id journal-id-type="pmc">ploscomp</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS Computational Biology</journal-title></journal-title-group><issn pub-type="ppub">1553-734X</issn><issn pub-type="epub">1553-7358</issn><publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta>
<article-meta><article-id pub-id-type="publisher-id">10-PLCB-RA-2308R3</article-id><article-id pub-id-type="doi">10.1371/journal.pcbi.1001133</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Neuroscience</subject><subject>Neuroscience/Behavioral Neuroscience</subject><subject>Neuroscience/Theoretical Neuroscience</subject></subj-group></article-categories><title-group><article-title>An Imperfect Dopaminergic Error Signal Can Drive Temporal-Difference Learning</article-title><alt-title alt-title-type="running-head">Learning from Dopamine Signals</alt-title></title-group><contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Potjans</surname><given-names>Wiebke</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Diesmann</surname><given-names>Markus</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="aff" rid="aff5"><sup>5</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Morrison</surname><given-names>Abigail</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib>
</contrib-group><aff id="aff1"><label>1</label><addr-line>Institute of Neuroscience and Medicine (INM-6), Computational and Systems Neuroscience, Research Center Jülich, Jülich, Germany</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>RIKEN Brain Science Institute, Wako-shi, Saitama, Japan</addr-line>       </aff><aff id="aff3"><label>3</label><addr-line>Functional Neural Circuits Group, Faculty of Biology, Albert-Ludwig University of Freiburg, Freiburg, Germany</addr-line>       </aff><aff id="aff4"><label>4</label><addr-line>Bernstein Center Freiburg, Albert-Ludwig University of Freiburg, Freiburg, Germany</addr-line>       </aff><aff id="aff5"><label>5</label><addr-line>Brain and Neural Systems Team, RIKEN Computational Science Research Program, Wako-shi, Saitama, Japan</addr-line>       </aff><contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Behrens</surname><given-names>Tim</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group><aff id="edit1">John Radcliffe Hospital, United Kingdom</aff><author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">w.potjans@fz-juelich.de</email></corresp>
<fn fn-type="con"><p>Conceived and designed the experiments: WP AM. Performed the experiments: WP. Analyzed the data: WP. Contributed reagents/materials/analysis tools: WP AM. Wrote the paper: WP MD AM.</p></fn>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><month>5</month><year>2011</year></pub-date><pub-date pub-type="epub"><day>12</day><month>5</month><year>2011</year></pub-date><volume>7</volume><issue>5</issue><elocation-id>e1001133</elocation-id><history>
<date date-type="received"><day>28</day><month>5</month><year>2010</year></date>
<date date-type="accepted"><day>6</day><month>4</month><year>2011</year></date>
</history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2011</copyright-year><copyright-holder>Potjans et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
<p>An open problem in the field of computational neuroscience is how to link synaptic plasticity to system-level learning. A promising framework in this context is temporal-difference (TD) learning. Experimental evidence that supports the hypothesis that the mammalian brain performs temporal-difference learning includes the resemblance of the phasic activity of the midbrain dopaminergic neurons to the TD error and the discovery that cortico-striatal synaptic plasticity is modulated by dopamine. However, as the phasic dopaminergic signal does not reproduce all the properties of the theoretical TD error, it is unclear whether it is capable of driving behavior adaptation in complex tasks. Here, we present a spiking temporal-difference learning model based on the actor-critic architecture. The model dynamically generates a dopaminergic signal with realistic firing rates and exploits this signal to modulate the plasticity of synapses as a third factor. The predictions of our proposed plasticity dynamics are in good agreement with experimental results with respect to dopamine, pre- and post-synaptic activity. An analytical mapping from the parameters of our proposed plasticity dynamics to those of the classical discrete-time TD algorithm reveals that the biological constraints of the dopaminergic signal entail a modified TD algorithm with self-adapting learning parameters and an adapting offset. We show that the neuronal network is able to learn a task with sparse positive rewards as fast as the corresponding classical discrete-time TD algorithm. However, the performance of the neuronal network is impaired with respect to the traditional algorithm on a task with both positive and negative rewards and breaks down entirely on a task with purely negative rewards. Our model demonstrates that the asymmetry of a realistic dopaminergic signal enables TD learning when learning is driven by positive rewards but not when driven by negative rewards.</p>
</abstract><abstract abstract-type="summary"><title>Author Summary</title>
<p>What are the physiological changes that take place in the brain when we solve a problem or learn a new skill? It is commonly assumed that behavior adaptations are realized on the microscopic level by changes in synaptic efficacies. However, this is hard to verify experimentally due to the difficulties of identifying the relevant synapses and monitoring them over long periods during a behavioral task. To address this question computationally, we develop a spiking neuronal network model of actor-critic temporal-difference learning, a variant of reinforcement learning for which neural correlates have already been partially established. The network learns a complex task by means of an internally generated reward signal constrained by recent findings on the dopaminergic system. Our model combines top-down and bottom-up modelling approaches to bridge the gap between synaptic plasticity and system-level learning. It paves the way for further investigations of the dopaminergic system in reward learning in the healthy brain and in pathological conditions such as Parkinson's disease, and can be used as a module in functional models based on brain-scale circuitry.</p>
</abstract><funding-group><funding-statement>This work was supported by BMBF Grant 01GQ0420 to BCCN Freiburg, EU Grant 15879 (FACETS), EU Grant 269921 (BrainScaleS), DIP F1.2, Helmholtz Alliance on Systems Biology (Germany), Next-Generation Supercomputer Project of MEXT (Japan) and the Junior Professor Advancement Program of Baden-Wuerttemberg. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="20"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Every higher organism needs to be able to make predictions about future rewards and adapt its behavior accordingly. One computational approach for modifying behavior to maximize reward on the basis of interactions with the environment is reinforcement learning <xref ref-type="bibr" rid="pcbi.1001133-Sutton1">[1]</xref>. Within that class of algorithms, temporal-difference (TD) learning, so called because it is based on comparing reward estimations at successive time steps, is particularly interesting to neuroscientists as it can solve tasks in which rewards or punishments are rare. Learning is driven by the TD error signal, which is positive when actions result in a condition that is better than expected, and negative if worse than expected.</p>
<p>Experimental findings, particularly on the dopaminergic system, support the hypothesis that the mammalian brain uses a TD learning strategy. During conditioning tasks, monkey midbrain dopamine neurons show phasic bursting activity following the presentation of an unpredicted reward. If, however, the reward is repeatedly paired with a reward predicting stimulus, the dopaminergic response shifts from the time of the reward delivery to the time of the stimulus onset. Furthermore, the dopaminergic activity decreases at the time of an expected reward if the reward is omitted <xref ref-type="bibr" rid="pcbi.1001133-Schultz1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Schultz2">[3]</xref>. This phasic activity has strikingly similar characteristics to the TD error signal <xref ref-type="bibr" rid="pcbi.1001133-Schultz1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Montague1">[4]</xref>, although other interpretations also exist <xref ref-type="bibr" rid="pcbi.1001133-Redgrave1">[5]</xref>. Recently, dopamine-dependent prediction errors have also been observed in humans <xref ref-type="bibr" rid="pcbi.1001133-Pessiglione1">[6]</xref>. The main target for dopamine innervation is the striatum, the input area of the basal ganglia, where the released dopamine modulates the plasticity of synapses between the cortex and the striatum <xref ref-type="bibr" rid="pcbi.1001133-Reynolds1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Pawlak1">[8]</xref>; see <xref ref-type="bibr" rid="pcbi.1001133-Reynolds2">[9]</xref> for a review.</p>
<p>These results suggest that the basal ganglia play an important role in any implementation of TD learning in the brain. There is some evidence that the cortico-striatal circuit realizes a variant of TD learning known as the actor-critic architecture <xref ref-type="bibr" rid="pcbi.1001133-ODoherty1">[10]</xref>. In this formulation of TD learning, explained in greater detail below, the agent learns an estimate for the amount of reward that can be gained starting from a given state <xref ref-type="bibr" rid="pcbi.1001133-Witten1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Barto1">[12]</xref>. An alternative interpretation is that the agent learns the amount of reward that can be expected for a given choice of action <xref ref-type="bibr" rid="pcbi.1001133-Morris1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Attalah1">[14]</xref>. Regardless of the exact formulation of TD learning assumed, it is still unclear what the mechanisms are that would enable it to be implemented in the mammalian brain. Dopaminergic activity is typically recorded in classical conditioning <xref ref-type="bibr" rid="pcbi.1001133-Fiorillo1">[15]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Tobler1">[16]</xref>, instructed-choice instrumental conditioning <xref ref-type="bibr" rid="pcbi.1001133-Morris2">[17]</xref> or simple decision trials with only a few number of possible actions <xref ref-type="bibr" rid="pcbi.1001133-Morris1">[13]</xref>. In these tasks, a reward is delivered (sometimes delayed) after every (correct) action. Such experiments cannot tell us whether the phasic dopaminergic signal is able to guide learning in complex tasks with sparse reward.</p>
<p>This is a crucial point, as the phasic dopaminergic firing rate only resembles the error signal of TD learning to a limited extent. The most obvious difference between the two signals is that the low baseline firing rate of the dopamine neurons implies a lower bound for the representation of negative errors in the dopaminergic error signal, whereas the TD error is unbounded. To address the question of whether dopamine-dependent plasticity can implement TD learning on the basis of a dopaminergic signal, despite its deviations from a standard TD error, we use a computational model. In this way, we can study the dopaminergic error signal, the evolution of synapses subject to dopamine-dependent plasticity and the adaptation of behavior over a long time period in complex tasks. Previous models implementing TD learning by utilizing a dopaminergic signal have only been formulated for nonspiking neurons <xref ref-type="bibr" rid="pcbi.1001133-Montague1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Houk1">[18]</xref>–<xref ref-type="bibr" rid="pcbi.1001133-Suri2">[21]</xref> (for reviews see <xref ref-type="bibr" rid="pcbi.1001133-Joel1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Wrgtter1">[23]</xref>). Conversely, most existing spiking reinforcement learning models have focused on non-TD learning strategies <xref ref-type="bibr" rid="pcbi.1001133-Seung1">[24]</xref>–<xref ref-type="bibr" rid="pcbi.1001133-Frmaux1">[30]</xref>. Some of these non-TD models have been shown to solve quite complex tasks, e.g. <xref ref-type="bibr" rid="pcbi.1001133-Legenstein1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Frmaux1">[30]</xref>.</p>
<p>Aspects of TD learning in the context of spiking activity have been studied in <xref ref-type="bibr" rid="pcbi.1001133-Rao1">[31]</xref>–<xref ref-type="bibr" rid="pcbi.1001133-Izhikevich1">[33]</xref>. However, the models developed in these studies do not perform the complete TD algorithm, which involves both prediction and control. Rao and Sejnowski demonstrate that in a two-neuron network, one neuron can learn to predict the firing times of the other <xref ref-type="bibr" rid="pcbi.1001133-Rao1">[31]</xref>, but the control aspect of TD learning is not addressed. The model presented by Farries and Fairhall includes an actor <xref ref-type="bibr" rid="pcbi.1001133-Farries1">[32]</xref>, but its decisions do not influence the state transitions. This is essentially a prediction task with a simplified TD error equal to the difference of the current reward and the average previous reward. The model proposed by Izhikevich uses a reward signal that is not equivalent to the TD error to solve a prediction task or to associate the presentation of a specific stimulus with one of two possible actions <xref ref-type="bibr" rid="pcbi.1001133-Izhikevich1">[33]</xref>. The fact that in each case the TD algorithm has been substantially simplified or reduced to just the prediction aspect is reflected in the simplicity of the tasks the models have been shown to solve. In these tasks either no reward is given at all <xref ref-type="bibr" rid="pcbi.1001133-Rao1">[31]</xref> or a reward is given or withheld at the end of every episode <xref ref-type="bibr" rid="pcbi.1001133-Farries1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Izhikevich1">[33]</xref>. Such tasks are more akin to supervised learning paradigms, as the output of the network can be clearly identified as ‘right’ or ‘wrong’ for each decision.</p>
<p>Recently, we proposed the first spiking neuronal network model to implement a complete TD(0) implementation with both prediction and control, and demonstrated that it is able to solve a non-trivial task with sparse rewards <xref ref-type="bibr" rid="pcbi.1001133-Potjans1">[34]</xref>. However, in that model each synapse performs its own approximation of the TD error rather than receiving it in the form of a neuromodulatory signal as suggested by experimental evidence <xref ref-type="bibr" rid="pcbi.1001133-Schultz1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Schultz2">[3]</xref>. We now present the first spiking neuronal model of an actor-critic TD learning agent that adapts its behavior on the basis of a dopaminergic signal dynamically generated by the network itself. We develop the model following a combination of top-down and bottom-up approaches. These terms can be interpreted in several different ways; see <xref ref-type="bibr" rid="pcbi.1001133-Dennett1">[35]</xref> for an analysis. Our interpretation is as follows: a top-down approach constructs a system to fulfill a desired function. In our case, we design synaptic plasticity rules that map to the update rules of temporal-difference learning whilst obeying reasonable biological constraints on the information available to the synapse. Conversely, a bottom-up approach to neuronal modeling integrates information from experimental analyses to generate a more complex system. Here, we integrate the known dynamical features of the dopaminergic activity with the sensitivity of cortico-striatal synapses to the presence of dopamine.</p>
<p>We show that dopamine-dependent plasticity relying on a dopaminergic signal with realistic firing rates can indeed realize TD learning. Our plasticity models depend on the global dopaminergic signal and the timing of pre- and post-synaptic spikes. Although the dynamics of the synaptic plasticity are constructed using a top-down approach to reproduce the key characteristics of the behavior-modifying updates of TD learning, we find a good agreement between the predictions of our plasticity models and experimental findings on cortico-striatal synapses. The discrepancies between the dopaminergic signal with realistic firing rates and the TD error result in a slightly modified TD learning algorithm with self-adapting learning parameters and an adapting offset. The parameters of our proposed synaptic plasticity models can be analytically mapped piecewise to the parameters of a classical discrete-time implementation of the TD algorithm for positive and small negative values of the TD error. We show that despite these modifications, the neuronal network is able to solve a non-trivial grid-world task with sparse positive rewards as quickly and as stably as the corresponding algorithmic implementation. The synaptic weights develop during the learning process to reflect the values of states with respect to their reward proximity as well as an optimal policy in order to maximize the reward. We demonstrate the consequences of the modifications to the learning algorithm on a cliff-walk task. The neuronal network cannot learn the task when the external rewards are purely negative. If the task includes both positive and negative rewards, the neuronal network can still learn it, but more slowly than the corresponding classical discrete-time algorithm and with a worse equilibrium performance. Our results support the hypothesis that negative rewards are mediated by different anatomical structures and neuromodulatory systems.</p>
<sec id="s1a">
<title>Temporal-difference learning in the actor-critic architecture</title>
<p>In this article we focus on a specific variant of TD learning: the TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e001" xlink:type="simple"/></inline-formula> algorithm as implemented by the actor-critic architecture <xref ref-type="bibr" rid="pcbi.1001133-Barto2">[36]</xref>. Here, we summarize the basic principles; a thorough introduction can be found in <xref ref-type="bibr" rid="pcbi.1001133-Sutton1">[1]</xref>.</p>
<p>The goal of a TD learning agent, as for every reinforcement learning agent, is to maximize the accumulated reward it receives over time. The actor-critic architecture (see <xref ref-type="fig" rid="pcbi-1001133-g001">Fig. 1</xref>) achieves this goal by making use of two modules, the actor and the critic. The actor module learns a policy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e002" xlink:type="simple"/></inline-formula>, which gives the probability of selecting an action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e003" xlink:type="simple"/></inline-formula> in a state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e004" xlink:type="simple"/></inline-formula>. A common method of defining a policy is given by the Gibbs softmax distribution:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e005" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e006" xlink:type="simple"/></inline-formula> is known as the preference of action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e007" xlink:type="simple"/></inline-formula> in state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e008" xlink:type="simple"/></inline-formula> and the index <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e009" xlink:type="simple"/></inline-formula> runs over all possible actions in state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e010" xlink:type="simple"/></inline-formula>.</p>
<fig id="pcbi-1001133-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001133.g001</object-id><label>Figure 1</label><caption>
<title>Actor-critic architecture.</title>
<p>The environment (E) informs the critic and the actor about the current state (s). In addition, it transmits the current reward information (r) to the critic. The critic calculates based on the value function of the current and previous state and the reward information the TD error signal, which is used to update the policy and the value function of the previous state. The actor selects based on the policy of the current state an action (a), which is read out by the environment. (Figure adapted from <xref ref-type="bibr" rid="pcbi.1001133-Sutton1">[1]</xref>).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.g001" xlink:type="simple"/></fig>
<p>The critic evaluates the consequences of the actor module's chosen actions with respect to a value function. Once learning has reached equilibrium, the value function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e011" xlink:type="simple"/></inline-formula> is the expected summed discounted future reward when starting from state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e012" xlink:type="simple"/></inline-formula> and following policy <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e013" xlink:type="simple"/></inline-formula>. During the learning process only estimates <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e014" xlink:type="simple"/></inline-formula> of the actual value function are available. The performance of the agent on a task is improved by making successive updates to the policy and the value function. These updates are usually formulated assuming a discretization of time and space: an error term <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e015" xlink:type="simple"/></inline-formula> is calculated based on the difference in estimations of the value function when moving from one discrete state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e016" xlink:type="simple"/></inline-formula> to the next, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e017" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e018" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e019" xlink:type="simple"/></inline-formula> is the reward the agent receives when moving into state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e020" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e021" xlink:type="simple"/></inline-formula> is a discount factor. This error signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e022" xlink:type="simple"/></inline-formula>, known as the TD error, is positive if the reward is greater than the expected discounted difference between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e023" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e024" xlink:type="simple"/></inline-formula>, indicating that the estimate of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e025" xlink:type="simple"/></inline-formula> needs to be increased. Conversely, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e026" xlink:type="simple"/></inline-formula> is negative if the reward is less than the expected discounted difference, indicating that the estimate of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e027" xlink:type="simple"/></inline-formula> needs to be decreased. In the simplest version of TD learning, known as the TD(<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e028" xlink:type="simple"/></inline-formula>) algorithm, the critic improves its estimate of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e029" xlink:type="simple"/></inline-formula> as follows:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e030" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e031" xlink:type="simple"/></inline-formula> is a small positive step-size parameter. For a given policy and a sufficiently small <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e032" xlink:type="simple"/></inline-formula>, the TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e033" xlink:type="simple"/></inline-formula> learning algorithm converges with probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e034" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1001133-Dayan1">[37]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Dayan2">[38]</xref>. Additionally, the preference of the chosen action <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e035" xlink:type="simple"/></inline-formula> in state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e036" xlink:type="simple"/></inline-formula> is adjusted to make the selection of this action correspondingly more or less likely the next time the agent visits that state. One possibility to update the preference in the actor-critic architecture is given by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e037" xlink:type="simple"/><label>(3)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e038" xlink:type="simple"/></inline-formula> is another small step-size parameter. For the purposes of this manuscript, we shall refer to the calculation of the error signal and the update of value function and policy described above as the classical discrete-time TD(<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e039" xlink:type="simple"/></inline-formula>) algorithm.</p>
</sec></sec><sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Spiking actor-critic architecture</title>
<p><xref ref-type="fig" rid="pcbi-1001133-g002">Fig. 2</xref> illustrates the architecture of our actor-critic spiking network model implementing temporal-difference learning (see <xref ref-type="sec" rid="s1">Introduction</xref>). All neurons in the network are represented by current-based integrate-and-fire neurons with alpha shaped post-synaptic currents. A tabular description of our model and its neuronal, synaptic and external stimulation parameters are given in <xref ref-type="sec" rid="s4">Methods</xref>. The network interacts with an environment, which is implemented purely algorithmically for the purpose of this work. The input layer of the neural network represents the cortex; it encodes information about <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e040" xlink:type="simple"/></inline-formula> states, each represented by a population of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e041" xlink:type="simple"/></inline-formula> neurons. The environment stimulates the population associated with the current state of the agent with a constant DC input, causing the neurons to fire with a mean rate of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e042" xlink:type="simple"/></inline-formula>; in the inactivated state the neurons fire on average with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e043" xlink:type="simple"/></inline-formula>. The low background rate in the inactivated state is chosen for the sake of simplicity in developing the synaptic plasticity dynamics, but is not a critical assumption of the model (see section “Synaptic-plasticity”). Each population in the cortex projects to the actor and critic modules.</p>
<fig id="pcbi-1001133-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001133.g002</object-id><label>Figure 2</label><caption>
<title>Neuronal actor-critic architecture generating and exploiting a dopaminergic TD error signal.</title>
<p>The input layer of the neuronal network consists of pools of cortical neurons (C) representing state information. The critic module is composed of neurons in the striatum (STR), neurons in the ventral pallidum (VP) and dopaminergic neurons (DA). The direct pathway from the striatum to the dopamine neurons is delayed with respect to the indirect pathway via the neuron population in the ventral pallidum. The actor module consists of one neuron for each possible action. The neural network interacts with an environment (E). The environment stimulates the cortical neurons representing the current state with a DC input. Whichever action neuron fires first is interpreted by the environment as the chosen action for the current state. After an action has been chosen the environment inhibits the actor neurons for a short time period by a negative DC input. If the current state is associated with a reward, the environment delivers a reward signal (R) in form of a DC input to the dopaminergic neurons. The dopaminergic signal modulates as a global third factor the plasticity of cortico-striatal synapses and the synapses between cortex and actor neurons. Red lines; inhibitory connections, blue lines; excitatory connections, purple lines; dopaminergic signal. All neurons receive additional Poissonian background noise (not shown).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.g002" xlink:type="simple"/></fig>
<p>As the focus of our study is the consequences of a realistic dopaminergic signal for temporal-difference learning rather than action selection, we keep the actor model as simple as possible. As in previous models <xref ref-type="bibr" rid="pcbi.1001133-Suri1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Potjans1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Foster1">[39]</xref>, the actor module consists of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e044" xlink:type="simple"/></inline-formula> actor neurons, each corresponding to one action. The synaptic weights between the cortical and the actor neurons represent the policy in our model. Whichever action neuron fires first in response to the activation of the state neurons is interpreted by the environment as the chosen action (for a review of first-spike coding, see <xref ref-type="bibr" rid="pcbi.1001133-VanRullen1">[40]</xref>). Immediately after an action has been chosen, i.e. after an actor neuron has spiked, the environment deactivates the previous state neurons and activates the neurons representing the new state resulting from the chosen action. At the same time the environment inhibits the actor neurons for a short time period <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e045" xlink:type="simple"/></inline-formula>, during which no further action can be chosen, allowing the cortical signal from a newly entered state to build up. For more sophisticated approaches to the action selection problem, see <xref ref-type="bibr" rid="pcbi.1001133-Gurney1">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Humphries1">[42]</xref>.</p>
<p>Two key experimentally observed features of the activity of the dopaminergic neurons are a constant low background rate with phasic activity with asymmetric amplitude depending on whether a reward is given or withheld <xref ref-type="bibr" rid="pcbi.1001133-Schultz1">[2]</xref>. As the basal ganglia dynamics generating this signal is unknown, we select the simplest possible network that generates these features; in general, multiple network configurations can produce the same dynamics <xref ref-type="bibr" rid="pcbi.1001133-Prinz1">[43]</xref>. We adapt the circuit model proposed in <xref ref-type="bibr" rid="pcbi.1001133-Houk1">[18]</xref> to perform the role of the critic module, which is responsible for generating a temporal-difference error. The major model assumption here is that the weights of the synapses between the neurons representing a given state and the critic module encode the value of that state. The circuit connects a population of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e046" xlink:type="simple"/></inline-formula> neurons representing the striatum, the input layer of the basal ganglia, to a population of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e047" xlink:type="simple"/></inline-formula> dopaminergic neurons directly and also indirectly via a population of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e048" xlink:type="simple"/></inline-formula> neurons representing the ventral pallidum. The direct and indirect pathways are both inhibitory. Consequently, the synaptic input from the striatum via the indirect pathway has a net excitatory effect, whereas the delayed striatal synaptic input via the direct pathway has an inhibitory effect on the dopamine neurons. This results in a phasic increase if the agent moves from a state with low cortico-striatal synaptic weights to a state with high weights (see <xref ref-type="fig" rid="pcbi-1001133-g003">Fig. 3</xref>) and a phasic decrease if the agent moves from a state with high cortico-striatal synaptic weights to a state with low weights. The length of the phasic activation is determined by the difference in the delays of the direct pathway <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e049" xlink:type="simple"/></inline-formula> and the indirect one <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e050" xlink:type="simple"/></inline-formula>. We have chosen <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e051" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e052" xlink:type="simple"/></inline-formula> which results in a duration of the phasic activation similar to that observed experimentally (see Fig. 1 in <xref ref-type="bibr" rid="pcbi.1001133-Schultz1">[2]</xref>). If the agent enters a rewarded state, the dopamine neurons receive an additional DC stimulation from the environment starting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e053" xlink:type="simple"/></inline-formula> after the agent moves and lasting for the duration of the phasic activity, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e054" xlink:type="simple"/></inline-formula>. Assuming the cortico-striatal synaptic weights represent the value function, after each state transition the dopamine neurons integrate information about the current value function with a positive sign, information about the previous value function with a negative sign, and a reward signal. Thus all the information necessary to calculate a form of temporal-difference error is present (see Eq. (1)).</p>
<fig id="pcbi-1001133-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001133.g003</object-id><label>Figure 3</label><caption>
<title>Spiking activity of one dopamine neuron in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e055" xlink:type="simple"/></inline-formula> trials.</title>
<p>(A) The agent moves from a state with cortico-striatal synaptic weights of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e056" xlink:type="simple"/></inline-formula> to a state with cortico-striatal synaptic weights of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e057" xlink:type="simple"/></inline-formula> at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e058" xlink:type="simple"/></inline-formula>, leading to a phasic increase in the dopaminergic activity. Each horizontal line in the lower panel shows the spike times of the dopamine neuron in one trial; the upper panel shows the histogram of the spiking activity over the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e059" xlink:type="simple"/></inline-formula> trials with a bin width of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e060" xlink:type="simple"/></inline-formula>. (B) As in (A), but here the agent moves from the higher valued state (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e061" xlink:type="simple"/></inline-formula>) to the lower value state (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e062" xlink:type="simple"/></inline-formula>) at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e063" xlink:type="simple"/></inline-formula> leading to a phasic decrease in the dopaminergic activity.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.g003" xlink:type="simple"/></fig>
<p>The <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e064" xlink:type="simple"/></inline-formula> dopaminergic neurons project back and release dopamine into the extracellular space (<xref ref-type="fig" rid="pcbi-1001133-g002">Fig. 2</xref> purple arrows) which modulates as a third factor the plasticity of the synapses between the cortex and the striatum and between the cortex and the actor neurons. Later in this section we develop synaptic plasticity models using a top-down approach to implement TD learning.</p>
</sec><sec id="s2b">
<title>Dopaminergic error signal</title>
<p>In this section we show that our network is able to generate dopaminergic activity with realistic firing rates and discuss its similarities to, and differences from, the classical discrete-time algorithmic definition of the TD error signal given in Eq. (1). It has been found that dopamine neurons fire with a low constant baseline activity (approx. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e065" xlink:type="simple"/></inline-formula> in rats <xref ref-type="bibr" rid="pcbi.1001133-Dai1">[44]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Hyland1">[45]</xref> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e066" xlink:type="simple"/></inline-formula> in monkeys <xref ref-type="bibr" rid="pcbi.1001133-Bayer1">[46]</xref>) as long as nothing unpredicted happens. This is known as the tonic activity of the dopaminergic neurons. For our model, this implies that the baseline firing rate should be independent of the strength of the cortical-striatal synapses associated with each state. This condition can be fulfilled in our architecture for an infinite number of configurations assuming linear relationships between the firing rates of the neurons in the striatum and the ventral pallidum; for a derivation of these relationships, see Supplementary <xref ref-type="supplementary-material" rid="pcbi.1001133.s001">Text S1</xref>. We select the simplest rate relationship with a linear coefficient of one. This relationship generates a constant baseline activity when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e067" xlink:type="simple"/></inline-formula> and the synaptic weights connecting the striatum to the dopamine neurons are equal in strength to the synaptic weights between the ventral pallidum and the dopamine neurons. For the parameters given in <xref ref-type="sec" rid="s4">Methods</xref> the mean dopaminergic baseline firing rate in our network is approx. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e068" xlink:type="simple"/></inline-formula>, which is close to the experimentally observed stationary dopaminergic firing rate.</p>
<p>When the agent transits from one state to another, the dopamine neurons exhibit phasic activity lasting for around <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e069" xlink:type="simple"/></inline-formula> in accordance with durations found experimentally <xref ref-type="bibr" rid="pcbi.1001133-Ljungberg1">[47]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Schultz3">[48]</xref>, see <xref ref-type="fig" rid="pcbi-1001133-g003">Fig. 3</xref>. <xref ref-type="fig" rid="pcbi-1001133-g004">Fig. 4</xref> shows the amplitude of phasic activity of the dopaminergic neurons after the agent transits from state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e070" xlink:type="simple"/></inline-formula> to state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e071" xlink:type="simple"/></inline-formula> in dependence of the difference in the corresponding cortico-striatal synaptic weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e072" xlink:type="simple"/></inline-formula>. In accordance with experimental observation <xref ref-type="bibr" rid="pcbi.1001133-Bayer1">[46]</xref> the dopamine neurons show a continuum of firing rates lower than the baseline for outcomes that are worse than predicted (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e073" xlink:type="simple"/></inline-formula>) and higher than the baseline for outcomes better than expected (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e074" xlink:type="simple"/></inline-formula>). Likewise, entering a state with an unpredicted reward induces a phasic increase of activity. The amplitude of the phasic activity of the dopaminergic neurons therefore has similar properties to the algorithmic TD error signal given in Eq.(1). However, the properties of the dopaminergic signal deviate from the TD error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e075" xlink:type="simple"/></inline-formula> in the following points:</p>
<list list-type="order"><list-item>
<p>Due to the low baseline firing rate of the dopamine neurons, the dopaminergic signal does not have as large a dynamic range to represent negative errors as it has to represent positive errors</p>
</list-item><list-item>
<p>The phasic dopaminergic activity is a nonlinear function of the difference in cortico-striatal synaptic weights of successive states whereas the classical algorithmic TD error signal depends linearly on the difference in the value function for successive states</p>
</list-item><list-item>
<p>The slope of the phasic dopaminergic signal as a function of the difference in the cortico-striatal synaptic weights of successive states is greater when an additional reward signal is present</p>
</list-item><list-item>
<p>As the baseline firing rate is independent of the current striatal firing rate, i.e. the value of the current state, the amplitude of the phasic activity depends on the absolute difference between the value of two successive states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e076" xlink:type="simple"/></inline-formula> rather than the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e077" xlink:type="simple"/></inline-formula>-discounted difference <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e078" xlink:type="simple"/></inline-formula></p>
</list-item></list>
<fig id="pcbi-1001133-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001133.g004</object-id><label>Figure 4</label><caption>
<title>Amplitude of the phasic dopaminergic activity averaged over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e079" xlink:type="simple"/></inline-formula> following a transition from state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e080" xlink:type="simple"/></inline-formula> with cortico-striatal synaptic weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e081" xlink:type="simple"/></inline-formula> to state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e082" xlink:type="simple"/></inline-formula> with cortico-striatal synaptic weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e083" xlink:type="simple"/></inline-formula> as a function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e084" xlink:type="simple"/></inline-formula>.</title>
<p>No external reward signal: black curve, external reward signal of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e085" xlink:type="simple"/></inline-formula>: gray curve. The values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e086" xlink:type="simple"/></inline-formula> are chosen as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e087" xlink:type="simple"/></inline-formula>; the data point for a specific weight difference is calculated as the amplitude of the dopaminergic rate excursion averaged over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e088" xlink:type="simple"/></inline-formula> trials for each combination of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e089" xlink:type="simple"/></inline-formula> that results in that weight difference. Error bars indicate the standard deviation. The dashed black line indicates the dopaminergic base firing rate. Inset: discrete-time algorithmic TD error signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e090" xlink:type="simple"/></inline-formula> Eq. (1) as a function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e091" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e092" xlink:type="simple"/></inline-formula>. Reward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e093" xlink:type="simple"/></inline-formula>: black curve, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e094" xlink:type="simple"/></inline-formula>: gray curve.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.g004" xlink:type="simple"/></fig>
<p>Point 2 arises due to the nonlinearities inherent in spiking neuronal networks, particularly at low rates (for a recent account see <xref ref-type="bibr" rid="pcbi.1001133-Helias1">[49]</xref>). If a linear rate-based model was assumed, the amplitude of the phasic response would also vary linearly until an amplitude of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e095" xlink:type="simple"/></inline-formula> was reached for some negative value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e096" xlink:type="simple"/></inline-formula>. Similarly, the addition of the reward signal could only affect the offset of the curve in a linear rate-based model (point 3). A nonlinear rate-based model may well be able to capture these features, but in order to make the correct non-linear assumptions, the behavior of the system to be abstracted needs to be known first. A nonlinear dependence of the dopaminergic firing rate on the reward prediction error has recently also been observed experimentally <xref ref-type="bibr" rid="pcbi.1001133-Bayer1">[46]</xref>. As we show in the next subsection, point 4 can be compensated by introducing a discount factor in the synaptic plasticity dynamics. A <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e097" xlink:type="simple"/></inline-formula>-discounted difference can also be obtained if the dopaminergic rate is assumed to depend on the striatal firing rate. As this is not in accordance with experimental findings we do not make this assumption, however, a derivation of the relationship between the firing rates and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e098" xlink:type="simple"/></inline-formula> is derived in Supplementary <xref ref-type="supplementary-material" rid="pcbi.1001133.s001">Text S1</xref>.</p>
</sec><sec id="s2c">
<title>Synaptic plasticity</title>
<p>In order for the network model to realize TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e099" xlink:type="simple"/></inline-formula> learning, the right synapses have to undergo the right changes in strength at the right time; this is also known as the credit assignment problem <xref ref-type="bibr" rid="pcbi.1001133-Sutton1">[1]</xref>. Here, we derive synaptic plasticity dynamics in a top-down fashion for the cortico-striatal synapses and the synapses between the cortical populations and the actor module representing the value function and the policy respectively. In the classical TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e100" xlink:type="simple"/></inline-formula> algorithm, when the agent transits from state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e101" xlink:type="simple"/></inline-formula> into state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e102" xlink:type="simple"/></inline-formula>, only the value <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e103" xlink:type="simple"/></inline-formula> and preference <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e104" xlink:type="simple"/></inline-formula> of the most recently exited state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e105" xlink:type="simple"/></inline-formula> are updated (see Eq. (2) and Eq. (3)).</p>
<p>For a synapse to implement this feature it requires a mechanism that enables plasticity for a short time period after the agent has left the state associated with the pre-synaptic neuron. This situation is characterized by the pre-synaptic rate being initially high and then dropping, as the population of cortical neurons associated with a state is strongly stimulated when the agent is in that state and weakly stimulated otherwise. An appropriate dynamics can be constructed if the synapse maintains two dynamic variables driven by the spikes of the pre-synaptic neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e106" xlink:type="simple"/></inline-formula> as originally proposed in <xref ref-type="bibr" rid="pcbi.1001133-Potjans1">[34]</xref>: a pre-synaptic activity trace <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e107" xlink:type="simple"/></inline-formula> and a pre-synaptic efficacy trace <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e108" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e109" xlink:type="simple"/><label>(4)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e110" xlink:type="simple"/><label>(5)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e111" xlink:type="simple"/></inline-formula> denotes the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e112" xlink:type="simple"/></inline-formula>th spike of the pre-synaptic neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e113" xlink:type="simple"/></inline-formula>. The pre-synaptic activity trace is an approximation of the pre-synaptic firing rate; it is incremented at every pre-synaptic spike and decays to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e114" xlink:type="simple"/></inline-formula> with a time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e115" xlink:type="simple"/></inline-formula> (see top panel of <xref ref-type="fig" rid="pcbi-1001133-g005">Fig. 5</xref>). To restrict the plasticity to the period immediately following a state transition, we assume a value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e116" xlink:type="simple"/></inline-formula> such that the activity trace decays to zero before the agent performs a further state transition. Efficacy traces as defined in Eq.(5) have previously been postulated as part of a spike-timing dependent plasticity model that accounts for data obtained from triplet and quadruplet spike protocols <xref ref-type="bibr" rid="pcbi.1001133-Froemke1">[50]</xref>. The efficacy trace is set to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e117" xlink:type="simple"/></inline-formula> at every pre-synaptic spike and relaxes exponentially to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e118" xlink:type="simple"/></inline-formula> with a time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e119" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1001133-g005">Fig. 5</xref>, middle panel). This time constant is assumed to be large such that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e120" xlink:type="simple"/></inline-formula> is small in the presence of pre-synaptic activity. When the agent is in the state associated with neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e121" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e122" xlink:type="simple"/></inline-formula> is high and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e123" xlink:type="simple"/></inline-formula> is close to zero. When the agent leaves the state, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e124" xlink:type="simple"/></inline-formula> relaxes to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e125" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e126" xlink:type="simple"/></inline-formula> relaxes to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e127" xlink:type="simple"/></inline-formula>. A product of the two traces is therefore close to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e128" xlink:type="simple"/></inline-formula> at all times except for the period shortly after the agent leaves the state associated with neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e129" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1001133-g005">Fig. 5</xref>, bottom panel). Therefore, a synaptic plasticity dynamics proportional to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e130" xlink:type="simple"/></inline-formula> ensures that the right synapses are sensitive to modifications at the right time to implement TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e131" xlink:type="simple"/></inline-formula> learning.</p>
<fig id="pcbi-1001133-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001133.g005</object-id><label>Figure 5</label><caption>
<title>Pre-synaptic activity trace <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e132" xlink:type="simple"/></inline-formula> (top), pre-synaptic efficacy trace <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e133" xlink:type="simple"/></inline-formula> (middle) and their product <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e134" xlink:type="simple"/></inline-formula> (bottom) as functions of time.</title>
<p>The agent enters the state represented by the pre-synaptic neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e135" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e136" xlink:type="simple"/></inline-formula> and leaves the state at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e137" xlink:type="simple"/></inline-formula>.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.g005" xlink:type="simple"/></fig>
<p>This simple relationship only holds for a very low rate in the inactive state. If the firing rate of cortical neurons in the inactive state were higher, then the product <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e138" xlink:type="simple"/></inline-formula> would be non-negligible at all times, resulting in permanent sensitivity of the synapse to irrelevant fluctations in the dopamine signal. Of course, this could be compensated for without altering the functionality by requiring <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e139" xlink:type="simple"/></inline-formula> to exceed a threshold, or by adopting a triphasic approach based on successive pre-synaptic activity thresholds as in our earlier work <xref ref-type="bibr" rid="pcbi.1001133-Potjans1">[34]</xref>. The low rate therefore does not constitute a requirement for our model. However, to avoid additional factors in the plasticity dynamics, we prefer to keep the rate relationships as simple as possible.</p>
<p>In TD learning the value function and the policy are both updated proportionally to the TD error (see Eq. (2) and Eq. (3)) which in our network model is signalled by the deviation of the dopaminergic firing rate from its baseline. For the sake of simplicity we model the dopamine concentration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e140" xlink:type="simple"/></inline-formula> as the superposition of the activity traces of all dopaminergic neurons:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e141" xlink:type="simple"/><label>(6)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e142" xlink:type="simple"/></inline-formula> is the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e143" xlink:type="simple"/></inline-formula>th dopamine spike and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e144" xlink:type="simple"/></inline-formula> is a time constant. This simplified model captures the experimentally observed feature that the concentration of dopamine is dependent on the firing times of the dopaminergic neurons <xref ref-type="bibr" rid="pcbi.1001133-Garris1">[51]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Montague3">[52]</xref>. Moreover, we set <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e145" xlink:type="simple"/></inline-formula> in agreement with experimental findings on the dopamine uptake time in the striatum <xref ref-type="bibr" rid="pcbi.1001133-Garris1">[51]</xref>. A more sophisticated approach to modelling the extracellular dopamine concentration can be found in <xref ref-type="bibr" rid="pcbi.1001133-Montague3">[52]</xref>. A suitable synaptic plasticity dynamics between a cortical neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e146" xlink:type="simple"/></inline-formula> and a striatal neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e147" xlink:type="simple"/></inline-formula> to implement value function updates is therefore given by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e148" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e149" xlink:type="simple"/></inline-formula> is the baseline concentration of dopamine and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e150" xlink:type="simple"/></inline-formula> is a learning rate parameter.</p>
<p>As discussed in the previous section, one difference between the dopaminergic signal as generated by our network model and the TD error is that the dopaminergic firing rate depends on the total value of the current state, rather than the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e151" xlink:type="simple"/></inline-formula>-discounted value (compare Eq.(2)). However, it is possible to compensate for this discrepancy in the following way. The firing rate of the striatum population expresses the value of the current state, as the value function is encoded by the cortico-striatal synaptic weights. For a given cortico-striatal synapse, the current state value can therefore be approximated by a post-synaptic activity trace as defined in Eq. (4) with a time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e152" xlink:type="simple"/></inline-formula>, which can be chosen quite arbitrarily. We therefore include a term in Eq. (7) proportional to the post-synaptic activity trace <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e153" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e154" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e155" xlink:type="simple"/></inline-formula>. In our numerical simulations we assume a plasticity dynamics at the cortico-striatal synapses as given by Eq. (8).</p>
<p>During the short period after a transition from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e156" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e157" xlink:type="simple"/></inline-formula>, the cortico-striatal synapses associated with state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e158" xlink:type="simple"/></inline-formula> are sensitive to modification. As discussed in the previous section, the dopaminergic signal depends nonlinearly on successive reward predictions encoded in the cortico-striatal synaptic weights, whereas the TD error is a linear function on the value function of successive states. Furthermore the slope of the non-linear function depends on the magnitude of any external reward. This means that it is not possible to define a single mapping from the units of synaptic weights to the units of the value function that holds for all values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e159" xlink:type="simple"/></inline-formula> and all rewards, as in our previous study <xref ref-type="bibr" rid="pcbi.1001133-Potjans1">[34]</xref>. However, it is possible to generate a piecewise mapping by approximating the nonlinear function for a given reward signal in a given range of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e160" xlink:type="simple"/></inline-formula> by a linear function.</p>
<p>The mapping (Eq. (11)) is derived in detail in the Supplementary <xref ref-type="supplementary-material" rid="pcbi.1001133.s002">Text S2</xref> and consists of two steps. First, the synaptic plasticity dynamics is integrated to calculate the net change in the mean outgoing synaptic weight of the neurons associated with a state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e161" xlink:type="simple"/></inline-formula> when the agent moves from <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e162" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e163" xlink:type="simple"/></inline-formula>. Second, the net weight change is converted from units of synaptic weight to units of the value function according to the linear relationships:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e164" xlink:type="simple"/><label>(9)</label></disp-formula><disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e165" xlink:type="simple"/><label>(10)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e166" xlink:type="simple"/></inline-formula> is a proportionality parameter mapping the mean striatal firing rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e167" xlink:type="simple"/></inline-formula> to the units of the value function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e168" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e169" xlink:type="simple"/></inline-formula> is a proportionality factor mapping the mean cortico-striatal weights of a state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e170" xlink:type="simple"/></inline-formula> to the mean striatal firing rate. For our choice of parameters (see <xref ref-type="sec" rid="s4">Methods</xref>) Eq. (10) is fulfilled in the allowed range for the cortico-striatal weights with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e171" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e172" xlink:type="simple"/></inline-formula>.</p>
<p>Within a given range of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e173" xlink:type="simple"/></inline-formula>, the mean net weight change of the synapses immediately after transition out of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e174" xlink:type="simple"/></inline-formula> corresponds to a slightly modified version of the classical discrete-time value function update with an additional offset <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e175" xlink:type="simple"/></inline-formula>:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e176" xlink:type="simple"/><label>(11)</label></disp-formula></p>
<p>The learning parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e177" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e178" xlink:type="simple"/></inline-formula> of the equivalent TD(<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e179" xlink:type="simple"/></inline-formula>) algorithm and the offset <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e180" xlink:type="simple"/></inline-formula> depend on the synaptic parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e181" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e182" xlink:type="simple"/></inline-formula> as defined above. They additionally depend on the slope <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e183" xlink:type="simple"/></inline-formula> and intercept <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e184" xlink:type="simple"/></inline-formula> of the linear approximation of the dopaminergic signal:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e185" xlink:type="simple"/><label>(12)</label></disp-formula>The constants <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e186" xlink:type="simple"/></inline-formula> depend on the synaptic time constants; see Supplementary <xref ref-type="supplementary-material" rid="pcbi.1001133.s002">Text S2</xref> for the definitions.</p>
<p>Because <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e187" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e188" xlink:type="simple"/></inline-formula> are dependent on the range of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e189" xlink:type="simple"/></inline-formula> and the direct current applied to the dopamine neurons, the weight update <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e190" xlink:type="simple"/></inline-formula> can be interpreted as a TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e191" xlink:type="simple"/></inline-formula> learning value function update with self-adapting learning parameters and a self-adapting offset that depend on the current weight change and reward. The greater the difference between the mean synaptic weights of successive states <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e192" xlink:type="simple"/></inline-formula>, the higher the learning rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e193" xlink:type="simple"/></inline-formula> and discount factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e194" xlink:type="simple"/></inline-formula>. For the parameters used in our simulations, a range of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e195" xlink:type="simple"/></inline-formula> can be realized by a range of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e196" xlink:type="simple"/></inline-formula>. A choice of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e197" xlink:type="simple"/></inline-formula> results in a discount factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e198" xlink:type="simple"/></inline-formula>. For a specific choice of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e199" xlink:type="simple"/></inline-formula>, the learning rate <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e200" xlink:type="simple"/></inline-formula> can be determined by the synaptic parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e201" xlink:type="simple"/></inline-formula>. For <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e202" xlink:type="simple"/></inline-formula>, the range <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e203" xlink:type="simple"/></inline-formula> can be realized by the range <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e204" xlink:type="simple"/></inline-formula>. As <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e205" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e206" xlink:type="simple"/></inline-formula> can be chosen independently, all possible combinations of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e207" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e208" xlink:type="simple"/></inline-formula> can be realized.</p>
<p>If the current state is rewarded, the offset <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e209" xlink:type="simple"/></inline-formula> is a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e210" xlink:type="simple"/></inline-formula>-dependent analog to the reward in the TD error Eq. (1). Otherwise, for an appropriate choice of parameters (see <xref ref-type="sec" rid="s4">Methods</xref>) <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e211" xlink:type="simple"/></inline-formula> is always smaller than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e212" xlink:type="simple"/></inline-formula> and has no analog in classical TD learning.</p>
<p>Self-adjusting parameters have also been implemented in other three-factor learning rules such as the one in <xref ref-type="bibr" rid="pcbi.1001133-Soltani1">[53]</xref> based on the meta-learning algorithm proposed in <xref ref-type="bibr" rid="pcbi.1001133-Schweighofer1">[54]</xref>. In contrast to meta-learning, in our model the values of the parameters do not adjust themselves to optimal parameters for a given task but vary according to the difference between the estimated values of successive states, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e213" xlink:type="simple"/></inline-formula>, and the current reward value. The range of possible learning parameters for a given <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e214" xlink:type="simple"/></inline-formula> and reward value depends on the current choice of synaptic parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e215" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e216" xlink:type="simple"/></inline-formula>, which can be set arbitrarily. However, meta-learning could be an additional mechanism that adjusts the parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e217" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e218" xlink:type="simple"/></inline-formula> to optimal values for a given task.</p>
<p>The variable parameters suggest a similarity with value learning, another learning algorithm similar to TD but with a variable discount rate <xref ref-type="bibr" rid="pcbi.1001133-Friston1">[55]</xref>. However, in value learning the discount rate changes over time: it is lowest immediately after an unconditioned stimulus and increases in between them, making the algorithm more sensitive to long term rewards. In our model the learning parameters do not depend on time but on the current reward and the difference in successive reward predictions encoded by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e219" xlink:type="simple"/></inline-formula>.</p>
<p>Similarly to the update of the value function, in the classical discrete-time TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e220" xlink:type="simple"/></inline-formula> algorithm only the policy for the recently vacated state is updated. As described earlier in this section, in the neuronal architecture an action is chosen by the first spike of an action neuron. Therefore an appropriate plasticity dynamics for the synapse between a cortex neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e221" xlink:type="simple"/></inline-formula> and an actor neuron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e222" xlink:type="simple"/></inline-formula> is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e223" xlink:type="simple"/><label>(13)</label></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e224" xlink:type="simple"/></inline-formula> determines the learning speed, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e225" xlink:type="simple"/></inline-formula> is a post-synaptic activity trace as defined in Eq. (4) with time constant <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e226" xlink:type="simple"/></inline-formula>. The choice of post-synaptic time constant is not critical, but the activity trace should decay within the typical time an agent spends in a state in order to be selective for the most recently chosen action. Unlike the cortico-striatal synapses described above, the lack of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e227" xlink:type="simple"/></inline-formula>-discounting in the dopamine signal cannot be compensated for by the addition of an additional local term in the synaptic plasticity dynamics. This is due to the fact that the post-synaptic activity here represents whether the encoded action was selected rather than the value function of the next state as in the previous case. Information about the value of the new state could only arrive at the synapse through an additional non-local mechanism.</p>
<p>In order to ensure the agent continues to occasionally explore alternative directions to its preferred direction in any given state, we restrict the synaptic weights of the synapses between the cortex and the actor neurons to the range <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e228" xlink:type="simple"/></inline-formula>. This results in a maximal probability of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e229" xlink:type="simple"/></inline-formula> and a minimal probability of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e230" xlink:type="simple"/></inline-formula> for any movement direction in any state (see Supplementary <xref ref-type="supplementary-material" rid="pcbi.1001133.s002">Text S2</xref> for a mapping of synaptic weights to probabilities).</p>
<p>The parameters for synaptic plasticity models used in our study are summarized in <xref ref-type="sec" rid="s4">Methods</xref>.</p>
</sec><sec id="s2d">
<title>Comparison of predictions of the synaptic plasticity models with experimental results</title>
<p>The proposed cortico-striatal synaptic plasticity dynamics Eq. (8) depends on three factors: the pre-synaptic firing rate, the post-synaptic firing rate and the dopamine concentration. For cortico-striatal synapses the effect on the plasticity of each of these factors has experimentally been studied in vivo and in vitro (see <xref ref-type="bibr" rid="pcbi.1001133-Reynolds2">[9]</xref> for a review). The long-term effects found on average across studies are summarized in column six of <xref ref-type="table" rid="pcbi-1001133-t001">Table 1</xref>. These results show that in order to induce any long lasting changes in synaptic plasticity, a conjunction of pre- and post-synaptic activity is required. Early studies on the effect of conjoined pre-synaptic and post-synaptic activity on the cortico-striatal plasticity reported exclusively long term depression (LTD). More recent studies have shown that long term potentiation (LTP) can also be obtained under some circumstances. The expression of LTP or LTD seems to depend on methodological factors such as the age of the animal, the location of the neuron and the stimulating electrode and the stimulus parameters <xref ref-type="bibr" rid="pcbi.1001133-Reynolds2">[9]</xref>. Although in these studies it is assumed that dopamine is not involved, it cannot be ruled out as cortico-striatal high frequency stimulation causes dopamine release <xref ref-type="bibr" rid="pcbi.1001133-Calabresi1">[56]</xref>. The main findings resulting from studies involving all three factors can be summarized in the following three-factor rule <xref ref-type="bibr" rid="pcbi.1001133-Wickens1">[57]</xref>: under normal and low dopamine concentrations, the conjunction of pre- and post-synaptic activity leads to LTD, whereas a large phasic increase in dopamine concentration during pre- and post-synaptic activity results in LTP.</p>
<table-wrap id="pcbi-1001133-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001133.t001</object-id><label>Table 1</label><caption>
<title>Theoretical predictions of cortico-striatal synaptic plasticity dynamics as functions of pre-synaptic activity, post-synaptic activity, and dopamine concentration in comparison with the average experimental findings across studies on long-term effects in synaptic plasticity.</title>
</caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pcbi-1001133-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.t001" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" colspan="1" rowspan="1">pre</td>
<td align="left" colspan="1" rowspan="1">post</td>
<td align="left" colspan="1" rowspan="1">dopa</td>
<td align="left" colspan="1" rowspan="1">theoretical predictions <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e231" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">theoretical prediction <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e232" xlink:type="simple"/></inline-formula></td>
<td align="left" colspan="1" rowspan="1">experimental results</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="1" rowspan="1">0</td>
<td align="left" colspan="1" rowspan="1">0</td>
<td align="left" colspan="1" rowspan="1">0</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">-</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="left" colspan="1" rowspan="1">0</td>
<td align="left" colspan="1" rowspan="1">0</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">-</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">0</td>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="left" colspan="1" rowspan="1">0</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">-</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">0</td>
<td align="left" colspan="1" rowspan="1">0</td>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">-</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="left" colspan="1" rowspan="1">0</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">LTD</td>
<td align="left" colspan="1" rowspan="1">LTD (LTP)</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="left" colspan="1" rowspan="1">0</td>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="left" colspan="1" rowspan="1">LTD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e233" xlink:type="simple"/></inline-formula>LTP</td>
<td align="left" colspan="1" rowspan="1">LTD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e234" xlink:type="simple"/></inline-formula>LTP</td>
<td align="left" colspan="1" rowspan="1">-</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">0</td>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">-</td>
<td align="left" colspan="1" rowspan="1">-</td>
</tr>
<tr>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="left" colspan="1" rowspan="1">1</td>
<td align="left" colspan="1" rowspan="1">LTD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e235" xlink:type="simple"/></inline-formula>LTP</td>
<td align="left" colspan="1" rowspan="1">LTD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e236" xlink:type="simple"/></inline-formula>LTP</td>
<td align="left" colspan="1" rowspan="1">LTD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e237" xlink:type="simple"/></inline-formula>LTP</td>
</tr>
</tbody>
</table></alternatives><table-wrap-foot><fn id="nt101"><label/><p>The predictions are based on eq:value function weight update for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e238" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e239" xlink:type="simple"/></inline-formula>, corresponding to discount factors <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e240" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e241" xlink:type="simple"/></inline-formula>, respectively; the experimental findings on <xref ref-type="bibr" rid="pcbi.1001133-Reynolds2">[9]</xref>. A 1 in the first three columns denotes an active influence, whereas a 0 indicates that the corresponding activity is not involved in the synaptic changes. The symbol <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e242" xlink:type="simple"/></inline-formula> indicates that either LTD or LTP occurs depending on the concentration of dopamine; the symbol - denotes an absence of long-term changes in the synaptic weights.</p></fn></table-wrap-foot></table-wrap>
<p>The predictions of the cortico-striatal synaptic dynamics given by Eq. (8) for the various permutations of pre- and post-synaptic activity and dopamine concentration are summarized in column <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e243" xlink:type="simple"/></inline-formula> (for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e244" xlink:type="simple"/></inline-formula>, corresponding to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e245" xlink:type="simple"/></inline-formula>) and column <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e246" xlink:type="simple"/></inline-formula> (for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e247" xlink:type="simple"/></inline-formula>, corresponding to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e248" xlink:type="simple"/></inline-formula>) of <xref ref-type="table" rid="pcbi-1001133-t001">Table 1</xref>. We assume that a value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e249" xlink:type="simple"/></inline-formula> in the first three columns denotes recent activity; due to the time constants of the activity traces this activation is still perceptible from the point of view of the synapse and can thus be assumed to have an active influence on plasticity. Assuming the baseline dopamine concentration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e250" xlink:type="simple"/></inline-formula> only changes on a long time scale, experiments involving no particular manipulations of the dopamine concentration (denoted by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e251" xlink:type="simple"/></inline-formula> in <xref ref-type="table" rid="pcbi-1001133-t001">Table 1</xref>) will be characterized by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e252" xlink:type="simple"/></inline-formula>. The plasticity dynamics Eq. (8) predicts LTD for an active influence of pre- and post-synaptic activity, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e253" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e254" xlink:type="simple"/></inline-formula> in accordance with the majority of the experimental findings; for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e255" xlink:type="simple"/></inline-formula> no change in synaptic strength is predicted.</p>
<p>Furthermore, Eq. (8) predicts that for simultaneous influence of pre- and post-synaptic activity, the direction of the synaptic change depends on the concentration of dopamine. For <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e256" xlink:type="simple"/></inline-formula> normal (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e257" xlink:type="simple"/></inline-formula>) as well as low dopamine concentration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e258" xlink:type="simple"/></inline-formula> results in LTD (see <xref ref-type="fig" rid="pcbi-1001133-g006">Fig. 6</xref>), while a large phasic increase in the dopamine concentration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e259" xlink:type="simple"/></inline-formula> results in LTP. For <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e260" xlink:type="simple"/></inline-formula> the change from LTD to LTP occurs at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e261" xlink:type="simple"/></inline-formula>, resulting in no change in synaptic strength under normal dopamine concentration in contrast to the experimental findings. The theoretical model makes additional predictions in this case that go beyond the presence or absence of activity and the direction of change. Due to the timing sensitivity of the plasticity dynamics given in Eq. (8), a weak synaptic weight change is predicted if the activity of the pre-synaptic neuron overlaps with the activity of the post-synaptic neuron in the presence of dopamine and a strong change if the pre-synaptic activity precedes the post-synaptic activity. Such a dependency on timing involving extended periods of activation have so far not been tested experimentally. However, protocols involving individual spike pairs have revealed comparable effects; for a review, see <xref ref-type="bibr" rid="pcbi.1001133-Pawlak2">[58]</xref>.</p>
<fig id="pcbi-1001133-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001133.g006</object-id><label>Figure 6</label><caption>
<title>Change in strength of cortico-striatal synapses predicted by Eq. (8) as a function of the dopaminergic concentration <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e262" xlink:type="simple"/></inline-formula> assuming a conjunction of pre- and post-synaptic activity for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e263" xlink:type="simple"/></inline-formula> (dashed line) and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e264" xlink:type="simple"/></inline-formula> (solid line).</title>
<p>For <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e265" xlink:type="simple"/></inline-formula>, the change from LTD to LTP occurs at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e266" xlink:type="simple"/></inline-formula>, whereas for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e267" xlink:type="simple"/></inline-formula> the switch occurs at a higher concentration of dopamine.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.g006" xlink:type="simple"/></fig>
<p>The greatest difference between our predictions and the experimental findings is that a simultaneously active influence of pre-synaptic activity and dopamine is sufficient to induce LTD or LTP in the absence of post-synaptic activity. However, this is quite an artificial case as pre-synaptic activity always generates post-synaptic activity in our network model dynamics. The behavior of the model could be brought into better alignment with the experimental data by adding additional complexity. For example, a multiplicative Heaviside function that evaluates to one when the post-synaptic activity exceeds a certain threshold would eliminate the generation of LTP/LTD in the absence of post-synaptic activity without altering the functionality of our model. As the plasticity dynamics was derived to fulfil a particular computational function rather than to provide a phenomenological fit to the experimental data, we prefer to avoid this additional complexity. Apart from this case, our predictions on the direction of cortico-striatal plasticity under the active conjunction of pre- and post-synaptic activity for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e268" xlink:type="simple"/></inline-formula> are in good agreement with experimental findings.</p>
</sec><sec id="s2e">
<title>Grid-world task</title>
<p>As in our previous study <xref ref-type="bibr" rid="pcbi.1001133-Potjans1">[34]</xref>, we tested the learning capability of our neuronal network model on a grid-world task, a standard task for TD learning algorithms. In our variant of this task, the grid consists of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e269" xlink:type="simple"/></inline-formula> states arranged in a five by five grid (see inset of <xref ref-type="fig" rid="pcbi-1001133-g007">Fig. 7</xref>). The agent can choose between four different actions (south, north, east, west) represented by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e270" xlink:type="simple"/></inline-formula> actor neurons. If the agent chooses an action which would lead outside the grid world, the action does not lead to a change in its position. Only a single state is rewarded; when the agent enters it a direct current with amplitude <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e271" xlink:type="simple"/></inline-formula> is applied to the dopaminergic neurons corresponding to the real-valued reward sent to the critic module in a classical discrete-time TD algorithm (see <xref ref-type="sec" rid="s1">Introduction</xref>). After the agent has found the reward and selected a new action, it is moved to a new starting position that is chosen randomly and independently of the selected action. This is therefore a continuing task rather than an episodic task, as there are no terminal states. To maximize its reward, the agent must find the reward from random starting positions in as few steps as possible. The difficulty of the task is that the agent has to learn a series of several actions starting from each state in which only the last one results in a reward. The grid world task is useful to visualize the behavior of a learning algorithm but is not intended to represent physical navigation task, as spatial information is not taken into consideration (e.g. exploiting the knowledge of which states are neighbors).</p>
<fig id="pcbi-1001133-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001133.g007</object-id><label>Figure 7</label><caption>
<title>The grid-world task.</title>
<p>Average latency in reaching the reward state and standard deviations over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e272" xlink:type="simple"/></inline-formula> runs for the neuronal network model with optimized parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e273" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e274" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e275" xlink:type="simple"/></inline-formula> and reward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e276" xlink:type="simple"/></inline-formula> (red curve) and the corresponding classical discrete-time algorithmic TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e277" xlink:type="simple"/></inline-formula> implementation with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e278" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e279" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e280" xlink:type="simple"/></inline-formula> and reward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e281" xlink:type="simple"/></inline-formula> (blue curve). Each data point shows the average latency over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e282" xlink:type="simple"/></inline-formula> successive trials. Inset: grid-world environment consisting of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e283" xlink:type="simple"/></inline-formula> states. Only the state marked with an asterisk is rewarded. In each state the agent (A) can choose between <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e284" xlink:type="simple"/></inline-formula> directions (indicated by the arrows). Once the rewarded state has been found, the agent is moved randomly to a new starting position.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.g007" xlink:type="simple"/></fig>
<p>To evaluate the performance of our model on the grid-world task, we separate the ongoing sequence of states and actions into trials, where a trial is defined as the period between the agent being placed in a starting position and the agent reaching the reward state. We measure the latency for each trial, i.e. the difference between the number of steps the agent takes to reach the reward state and the minimum number of steps required to reach the reward state for the given starting position. To provide a comparison, we also measure the performance of a classical discrete-time TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e285" xlink:type="simple"/></inline-formula> learning algorithmic implementation with corresponding parameters. The specification of the discrete-time implementation is obtained by mapping the synaptic parameters to the discrete-time parameters for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e286" xlink:type="simple"/></inline-formula> and determining the corresponding reward via a search algorithm (see Supplementary <xref ref-type="supplementary-material" rid="pcbi.1001133.s002">Text S2</xref>).</p>
<p><xref ref-type="fig" rid="pcbi-1001133-g007">Fig. 7</xref> shows the evolution of latency on the grid-world task for the neuronal network model with optimized parameters and the discrete-time algorithmic implementation with corresponding parameters. Within the first <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e287" xlink:type="simple"/></inline-formula> trials the latency of the neuronal network model drops from around <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e288" xlink:type="simple"/></inline-formula> steps to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e289" xlink:type="simple"/></inline-formula> steps. After <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e290" xlink:type="simple"/></inline-formula> trials the agent has learnt the task; the latency is always below <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e291" xlink:type="simple"/></inline-formula> steps. The learning speed and the equilibrium performance of the neuronal network model are as good as those of the corresponding discrete-time algorithmic implementation. The performance of the discrete-time algorithmic implementation does not deteriorate if a discount factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e292" xlink:type="simple"/></inline-formula> is assumed for the updates to the policy in correspondence with the synaptic plasticity dynamics given by Eq. (13) (data not shown).</p>
<p>As discussed in section “Synaptic-plasticity”, we impose hard bounds on the weights of the synapses between the cortex and the actor to ensure that for a given state, no action becomes either impossible or certain. For this task, it turns out that the lower bound is not necessary; restricting the weights to the range <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e293" xlink:type="simple"/></inline-formula> results in a similar learning performance (data not shown). However, the upper bound is necessary for the stability of the system. In the absence of an upper bound, synaptic weights between the cortex and all action neurons other than south increase to unbiological levels. This runaway behavior is detrimental to the learning process; in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e294" xlink:type="simple"/></inline-formula> the agent only locates the rewarded state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e295" xlink:type="simple"/></inline-formula> times, a factor of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e296" xlink:type="simple"/></inline-formula> fewer than for the bounded learning agent.</p>
<p>In our model, all cortico-striatal synaptic weights as well as all synaptic weights between the cortex and the actor neurons are initialized with the same value. This corresponds to all states being estimated at the same value and all possible directions of movement from each state being equally preferred. <xref ref-type="fig" rid="pcbi-1001133-g008">Fig. 8A</xref> shows the value function encoded in the mean synaptic cortico-striatal weights associated with each state after the task has been learnt. A gradient towards the rewarded state can be seen, showing that the agent has learnt to correctly evaluate the states with respect to their reward proximity. In order to represent the policy, we mapped the synaptic weights between cortex and actor neurons to the probabilities of choosing each action (see Supplementary <xref ref-type="supplementary-material" rid="pcbi.1001133.s002">Text S2</xref>). <xref ref-type="fig" rid="pcbi-1001133-g008">Fig. 8B</xref> shows the preferred direction in a given state after the task has been learnt indicated by the arrows. The x-component of an arrow <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e297" xlink:type="simple"/></inline-formula> in a state <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e298" xlink:type="simple"/></inline-formula> gives the difference between the probabilities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e299" xlink:type="simple"/></inline-formula> of choosing east and west, the y-component the difference between the probabilities of choosing north and south:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e300" xlink:type="simple"/></disp-formula>After the task has been learnt the agent tends to choose actions that move it closer to the rewarded state. These results show that not only can our model perform the TD(<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e301" xlink:type="simple"/></inline-formula>) algorithm, but that its parameters can be successfully mapped to an equivalent classical discrete-time implementation. Despite the inherent noisiness of the neuronal network implementation, it learns as quickly and as well as a traditional algorithmic implementation.</p>
<fig id="pcbi-1001133-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001133.g008</object-id><label>Figure 8</label><caption>
<title>Average value function and policy over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e302" xlink:type="simple"/></inline-formula> runs for the neuronal network model after <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e303" xlink:type="simple"/></inline-formula> simulation of biological time corresponding to around <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e304" xlink:type="simple"/></inline-formula> trials.</title>
<p>(A) Value function. Each square represents the mean synaptic weight between the cortical neurons representing the associated state and the striatal neurons of the critic module (see <xref ref-type="fig" rid="pcbi-1001133-g002">Fig. 2</xref>). (B) Policy. The arrows indicate the preferred direction for each state given by the mean synaptic weights between the cortical neurons representing the associated state and the actor neurons.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.g008" xlink:type="simple"/></fig></sec><sec id="s2f">
<title>Cliff-walk task</title>
<p>In the previous section we demonstrated the ability of the spiking neuronal network model to solve a reinforcement learning problem with sparse positive reward. However, due to the asymmetry of the dopaminergic signal, it is to be expected that differences between the neuronal network model and a standard TD learning algorithm become more apparent in tasks where learning is driven by negative rewards. In this section we study the learning performance of the spiking neuronal network model in tasks with negative rewards and investigate the consequences of the modified TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e305" xlink:type="simple"/></inline-formula> learning algorithm implemented by the neuronal network.</p>
<p>An appropriate task to discriminate between the standard and the modified TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e306" xlink:type="simple"/></inline-formula> algorithms is the cliff-walk task <xref ref-type="bibr" rid="pcbi.1001133-Sutton1">[1]</xref>. In our version of this task, the cliff-walk environment consists of 25 states with five special states: a start state in the lower left, a goal state in the lower right and three cliff states in between the start and the goal state (see <xref ref-type="fig" rid="pcbi-1001133-g009">Fig. 9A</xref>). When the agent moves into a cliff state (i.e. falls off the cliff) a negative direct current with amplitude <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e307" xlink:type="simple"/></inline-formula> is applied to the dopaminergic neurons, corresponding to a negative reward value in a traditional TD learning algorithm. In the cliff states and the goal state, the agent is sent back to the start state regardless of the next action selected. As before, we treat the task as a continuous one, i.e. the synaptic weights representing the value function and the policy are continuously updated, even when the agent is sent back to the start state.</p>
<fig id="pcbi-1001133-g009" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001133.g009</object-id><label>Figure 9</label><caption>
<title>The cliff-walk task.</title>
<p>(A) The environment consists of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e308" xlink:type="simple"/></inline-formula> states. The agent starts each trial in the start state, marked with S and ends at the goal state, marked with an asterisk. The three states between the start state and the goal state represent the cliff. When the agent either moves into the cliff state or the goal state it is sent back to the start state. In a first variant of this task the agent never receives positive rewards. It receives a large negative reward for moving into the cliff and a smaller negative reward in all other states except the start and goal states, which have a reward of zero. In a second variant of this task the agent receives a positive reward for moving into the goal state and a negative reward when for moving into the cliff; in all other states the reward is zero. (B) Performance on the first variant of the cliff-walk task. Total reward in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e309" xlink:type="simple"/></inline-formula> bins averaged over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e310" xlink:type="simple"/></inline-formula> runs for the neuronal network model (red curve) and the discrete-time algorithmic TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e311" xlink:type="simple"/></inline-formula> learning implementation (blue curve).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.g009" xlink:type="simple"/></fig>
<p>In a first variant of this task, a smaller negative direct current <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e312" xlink:type="simple"/></inline-formula> is applied to the dopamine neurons in all non-cliff states except the start and goal states, where the reward is zero. Thus, the agent only receives negative rewards from the environment. Setting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e313" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e314" xlink:type="simple"/></inline-formula> corresponds to setting a negative reward of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e315" xlink:type="simple"/></inline-formula> in the cliff states and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e316" xlink:type="simple"/></inline-formula> in all other states except the start and goal states for the discrete-time algorithmic TD(<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e317" xlink:type="simple"/></inline-formula>) agent.</p>
<p><xref ref-type="fig" rid="pcbi-1001133-g009">Fig. 9B</xref> shows the total reward received by the neuronal agent and the traditional algorithmic agent, summed in bins of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e318" xlink:type="simple"/></inline-formula> and averaged over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e319" xlink:type="simple"/></inline-formula> runs. All parameters are set as for the grid-world task. The traditional TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e320" xlink:type="simple"/></inline-formula> learning agent improves its performance rapidly. After approx. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e321" xlink:type="simple"/></inline-formula> the average reward over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e322" xlink:type="simple"/></inline-formula> is always above <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e323" xlink:type="simple"/></inline-formula>. The performance continues to improve up to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e324" xlink:type="simple"/></inline-formula>, after which the average reward saturates at around <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e325" xlink:type="simple"/></inline-formula>. Unlike the grid-world task, the neuronal agent does not improve its performance even after <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e326" xlink:type="simple"/></inline-formula>. During this time the neuronal agent reaches the goal state on average only 30 times. In the same period the traditional agent reaches the goal state on average more than 700 times. Similarly, the average number of times the neuronal agent falls off the cliff is around 660, whereas the traditional agent makes this mistake on average less than <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e327" xlink:type="simple"/></inline-formula> times. These results demonstrate that although the neuronal agent performs as well as the traditional discrete-time agent on the grid-world task, the traditional agent can learn the cliff-walk task with purely negative rewards and the neuronal agent cannot. This is due to the fact that the true underlying optimal value function is negative for this variant of the task, as the expected future rewards are negative. Thus, the synaptic weights representing the value function all reach their minimal allowed values and do not allow the agent to distinguish between states with respect to their reward proximity.</p>
<p>In a second variant of this task the agent receives a positive reward in the form of a direct current with amplitude <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e328" xlink:type="simple"/></inline-formula> applied to the dopaminergic neurons when it reaches the goal state. The reward in all other states except the cliff and goal states is zero. For the purposes of analysis, the end of a trial is defined by the agent reaching the goal state, regardless of the number of times it falls off the cliff and is sent back to the start state.</p>
<p><xref ref-type="fig" rid="pcbi-1001133-g010">Fig. 10A</xref> shows the development of the latency on the cliff-walk task for the neuronal network model and the discrete-time algorithmic implementation, both with the same parameters used in the grid-world task. The cliff-walk task can be learnt much faster than the grid-world task, as the start state is not randomized, so the agent only needs to learn a good policy for the states around the cliff and the goal. The neuronal network model learns the task more slowly than the discrete-time algorithmic implementation, requiring around <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e329" xlink:type="simple"/></inline-formula> trials and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e330" xlink:type="simple"/></inline-formula> trials, respectively. The average latency after learning is slightly higher for the traditional agent (approx. 3) than for the neuronal agent (approx. 2.3). However, this does not mean that the neuronal agent has learned a better strategy for the task, as can be seen in the average total reward per trial shown in <xref ref-type="fig" rid="pcbi-1001133-g010">Fig. 10B</xref>. For the traditional algorithm, the summed reward after learning is equal to the reward of the goal state in almost every trial, demonstrating that the agent has learnt to completely avoid the cliff. The average reward received by the neuronal agent deviates much more frequently from the maximum, which shows that the neuronal agent still selects actions that cause it to fall off the cliff.</p>
<fig id="pcbi-1001133-g010" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001133.g010</object-id><label>Figure 10</label><caption>
<title>Performance on the second variant of the cliff-walk task.</title>
<p>(A) Average latency in reaching the goal state and standard deviations over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e331" xlink:type="simple"/></inline-formula> runs for the neuronal network model with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e332" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e333" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e334" xlink:type="simple"/></inline-formula>, positive reward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e335" xlink:type="simple"/></inline-formula> and negative reward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e336" xlink:type="simple"/></inline-formula> (red curve) and the corresponding classical discrete-time algorithmic TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e337" xlink:type="simple"/></inline-formula> implementation with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e338" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e339" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e340" xlink:type="simple"/></inline-formula>, positive reward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e341" xlink:type="simple"/></inline-formula> and negative reward <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e342" xlink:type="simple"/></inline-formula> (blue curve). Each data point shows the average latency over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e343" xlink:type="simple"/></inline-formula> successive trials. (B) Total reward in each trial averaged over <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e344" xlink:type="simple"/></inline-formula> runs for the neuronal network model (red curve) and the discrete-time algorithmic TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e345" xlink:type="simple"/></inline-formula> learning implementation (blue curve).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.g010" xlink:type="simple"/></fig>
<p>As for the grid-world task, it turns out that the upper bound on the weights of the synapses between the cortex and the actor neurons is necessary for the stability of the system but the lower bound is not. In the absence of an upper bound, the agent still initially learns the task within about <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e346" xlink:type="simple"/></inline-formula> trials. However, the synaptic weights increase to unbiologically high values after approximately <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e347" xlink:type="simple"/></inline-formula> trials, which causes the task to be unlearned again. In contrast, the absence of a lower bound on the synaptic weights does not affect the learning performance (data not shown).</p>
<p>The differences in the behavior learned by the traditional and neuronal agents are also evident in <xref ref-type="fig" rid="pcbi-1001133-g011">Fig. 11</xref>, which shows for one run the relative frequencies with which each state is visited after the performance has reached equilibrium. For this purpose, we assume an agent to have reached equilibrium performance once it has visited <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e348" xlink:type="simple"/></inline-formula> states. While the traditional agent (<xref ref-type="fig" rid="pcbi-1001133-g011">Fig. 11B</xref>) has learnt to avoid the cliff altogether and chooses a safe path one row away from the cliff, the neuronal agent (<xref ref-type="fig" rid="pcbi-1001133-g011">Fig. 11A</xref>) typically moves directly along the edge of the cliff and in some trials falls off it. The differences in the strategies learned by the traditional and the neuronal agents account for the finding that the neuronal agent exhibits a shorter average latency but a lower average reward per trial than the traditional discrete-time TD(<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e349" xlink:type="simple"/></inline-formula>) agent.</p>
<fig id="pcbi-1001133-g011" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001133.g011</object-id><label>Figure 11</label><caption>
<title>Learned strategies in the first variant of the cliff-walk task.</title>
<p>Color indicates the number of visits the agent makes to that state as a percentage of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e350" xlink:type="simple"/></inline-formula> visited states in one run after learning is complete. (A) Neuronal agent. (B) Traditional TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e351" xlink:type="simple"/></inline-formula> learning agent. (C) Modified discrete-time TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e352" xlink:type="simple"/></inline-formula> learning agent with a minimal TD error <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e353" xlink:type="simple"/></inline-formula>. (D) Modified TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e354" xlink:type="simple"/></inline-formula> learning agent with a lower and an upper bound in the value function. (E) Modified TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e355" xlink:type="simple"/></inline-formula> learning agent with a discount factor present only in the value function. (F) Modified TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e356" xlink:type="simple"/></inline-formula> learning agent with self-adapting parameters and an additional offset. (G) Modified TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e357" xlink:type="simple"/></inline-formula> learning agent with adapting parameters and offset in addition to a bounded value function. (H) Modified TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e358" xlink:type="simple"/></inline-formula> learning agent implementing all limitations studied individually in (C–F).</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.g011" xlink:type="simple"/></fig>
<p>As discussed in section “Synaptic-plasticity” and derived in detail in the Supplementary <xref ref-type="supplementary-material" rid="pcbi.1001133.s002">Text S2</xref>, the neuronal network implements a modified TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e359" xlink:type="simple"/></inline-formula> learning algorithm with self-adapting learning parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e360" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e361" xlink:type="simple"/></inline-formula>, and a self-adapting additional offset (see Eq. (11) and Eq. (12)). Furthermore, a discount factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e362" xlink:type="simple"/></inline-formula> is only present in the value function update and not in the policy update. Another constraint of the neuronal network is that there is a natural lower bound in the dopaminergic firing rate, so there is a limited representation of negative temporal-difference errors. Similarly, the synaptic weights encoding the value function and the policy have lower bounds and are thus limited in their ability to encode negative values for states.</p>
<p>To analyze the consequences of these modifications from the traditional learning method, we implement modified versions of the traditional discrete-time TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e363" xlink:type="simple"/></inline-formula> learning algorithm incorporating the various modifications present in the neuronal network model. The learned strategies are visualized in <xref ref-type="fig" rid="pcbi-1001133-g011">Fig. 11C–H</xref>. In all variants as well as in the original discrete-time TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e364" xlink:type="simple"/></inline-formula> learning algorithm, we restrict the maximal and the minimal values for the action preferences <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e365" xlink:type="simple"/></inline-formula> to the range <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e366" xlink:type="simple"/></inline-formula>. This results in the same maximum probability of choosing an action as given in the neuronal network by the bounds on the synaptic weights representing the policy. In all versions the parameters are set according to our derived mapping; the units of the synaptic weights are mapped into the units of the value function according to Eq. (9) for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e367" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e368" xlink:type="simple"/></inline-formula>.</p>
<p>In the first version, a lower bound <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e369" xlink:type="simple"/></inline-formula> is applied to the TD error, thus limiting the system's ability to express that an action led to a much worse state than expected (<xref ref-type="fig" rid="pcbi-1001133-g011">Fig. 11C</xref>). In the second version the values of the value function are bounded to a minimal value function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e370" xlink:type="simple"/></inline-formula> and a maximal value function of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e371" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1001133-g011">Fig. 11D</xref>). Neither version results in a different strategy on the cliff-walk task from that learned by the traditional algorithm without modifications (<xref ref-type="fig" rid="pcbi-1001133-g011">Fig. 11B</xref>). A minor difference can be seen for the third version (<xref ref-type="fig" rid="pcbi-1001133-g011">Fig. 11E</xref>), which applies a discount factor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e372" xlink:type="simple"/></inline-formula> to the updates of the value function but not to those of the policy. We can therefore conclude that none of these modifications in isolation substantially alters the strategy learned for the cliff-walk task by the traditional TD(<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e373" xlink:type="simple"/></inline-formula>) algorithm. The fourth version incorporates self-adapting learning parameters and an additional self-adapting offset in the TD error as given by Eq. (11) and Eq. (12). The mapping results in the following parameter sets for different external reward values: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e374" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e375" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e376" xlink:type="simple"/></inline-formula> for the goal state, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e377" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e378" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e379" xlink:type="simple"/></inline-formula> for the cliff states and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e380" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e381" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e382" xlink:type="simple"/></inline-formula> for all other states. This modification results in a strategy that is much more similar to that developed by the neuronal system, in that the agent typically walks directly along the edge of the cliff (<xref ref-type="fig" rid="pcbi-1001133-g011">Fig. 11F</xref>). Unlike the neuronal system, the modified TD(<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e383" xlink:type="simple"/></inline-formula>) algorithm does not select actions that cause it to fall off the cliff. This can be clearly seen as the cliff states are not visited at all and all the states on the path are equally bright, indicating that the agent is only returned to the start state at the successful end of a trial. The key component of the modification is likely to be the additional offset: a similar strategy is learned by the traditional TD learning agent in an altered version of the cliff-walk task, in which each state other than the goal and the cliff states is associated with a negative reward equivalent to the offset (data not shown).</p>
<p>By combining the modifications, the strategy of the neuronal agent is recovered. <xref ref-type="fig" rid="pcbi-1001133-g011">Fig. 11G</xref> shows the strategy learned by a TD learning algorithm with self-adapting learning parameters and offset and with the value function restricted to the range <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e384" xlink:type="simple"/></inline-formula>. In this case, the agent mostly chooses the path closest to the edge of the cliff, but occasionally selects actions that cause it to fall off. Additionally enforcing a lower bound on the TD error and applying the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e385" xlink:type="simple"/></inline-formula>-discount to the value function updates only do not cause any further alterations to the learned strategy (<xref ref-type="fig" rid="pcbi-1001133-g011">Fig. 11H</xref>).</p>
<p>These results show that whereas the neuronal agent cannot learn a task with purely negative rewards, it can learn a task where external negative rewards are applied when the underlying optimal value function is positive. However, even in this case the neuronal agent learns more slowly than a traditional agent and attains an inferior equilibrium performance. For the cliff-walk task, it is the self-adapting parameters and additional offset which contribute the most to the difference in the strategies learned by the neuronal and traditional agents. The bounds imposed on the value function in the modified TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e386" xlink:type="simple"/></inline-formula> algorithm contribute second most, whereas the lower bound on the TD error and the absence of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e387" xlink:type="simple"/></inline-formula>-discounting on the policy updates do not play major roles.</p>
</sec></sec><sec id="s3">
<title>Discussion</title>
<p>We have presented the first spiking neuronal network model of an actor-critic temporal-difference learning agent that simultaneously accounts for multiple experimental results: the generation of a dopaminergic TD error signal with realistic firing rates, and plasticity dynamics in accordance with experimental findings with respect to pre-synaptic activity, post-synaptic activity and dopamine. The predictions of our plasticity dynamics are furthermore compatible with those of a recently proposed kinetic model of cortico-striatal synaptic plasticity <xref ref-type="bibr" rid="pcbi.1001133-Nakano1">[59]</xref>. The good agreement of the predictions of the proposed plasticity dynamics with experimental findings is particularly surprising, as we constructed the dynamics of the synaptic plasticity to result in TD learning using a top-down approach. The agreement between the synaptic dynamics derived from computational principles and the experimentally observed synaptic dynamics can be interpreted as supporting evidence for the theory that the mammalian brain implements TD learning. In the model there is a strong interaction between changes on the behavioral and on the synaptic level; modifications of synaptic strengths have an impact on the agent's choice, whereas the agent's choice determines the change in synaptic efficacy. This work can therefore be seen as a step towards a better understanding between synaptic plasticity and system-level learning taking place on completely different temporal and spatial scales. For other examples of modeling studies which similarly aim to bridge the considerable distance between these two levels of description, see <xref ref-type="bibr" rid="pcbi.1001133-Seung1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Izhikevich1">[33]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Potjans1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Loewenstein1">[60]</xref>–<xref ref-type="bibr" rid="pcbi.1001133-Soltani2">[62]</xref>.</p>
<p>We developed our model by combining a top-down with a bottom-up approach, which we think is the best approach to try and understand multi-scale dynamics. A purely top-down approach is under-constrained. Developing a model solely to provide a specific function can in principle result in many different architectures with no guarantee of biological plausibility. Conversely, a purely bottom-up approach starting from experimentally observed properties of neurons and synapses tends to generate models that are too complex to be understood. Moreover, it is very unlikely that a model developed in this way will spontaneously exhibit a complex functionality on the behavioral level. By combining the two approaches we can develop models that are biologically plausible, account for multiple experimental findings and yet are still simple enough to yield insights into the mechanisms of information processing in the brain. In the following, we will discuss the significance of our results and the limits, predictions and future directions of this study.</p>
<sec id="s3a">
<title>Learning performance on the grid-world task</title>
<p>The learning speed and performance of the neuronal network on the grid-world task with sparse positive reward are comparable to that of a discrete-time actor-critic TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e388" xlink:type="simple"/></inline-formula> learning implementation. In some respects this result is not surprising, as the plasticity dynamics were designed to fulfill the main properties of TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e389" xlink:type="simple"/></inline-formula> learning: value function and policy updates are proportional to the TD error and modifications are applied only with respect to the most recently exited state and the most recently chosen action. However, the dopaminergic signal does not perfectly reproduce the characteristics of the algorithmic TD error signal. The amplitude of the phasic activity is a nonlinear function of the difference in value between two states, and the dynamic range for negative errors is small. Moreover, synapses are not only updated due the presence of an error signal, but also due to small fluctuations of the dopaminergic firing rate around the baseline firing rate. Finally, the timing condition given by the product of the pre-synaptic efficacy and the pre-synaptic activity trace is not as strict as that defined by the discrete-time updates. Consequently, synapses undergo minor changes outside of the desired period of sensitivity.</p>
<p>The learning speed of our model is better than that exhibited by an earlier proposed TD learning model on the same task <xref ref-type="bibr" rid="pcbi.1001133-Potjans1">[34]</xref>. The major difference between the two models is that in the previously proposed model, each synapse calculates its own approximation of the TD error based on a comparison of two post-synaptic activity traces with different time constants, whereas in the model presented here the TD error is represented as a population signal available to all synapses. This suggests that a population signal is a more reliable method for the brain to represent reward information.</p>
<p>Although the grid-world task resembles a navigational task, it has more in common with an abstract association task such as learning associations between pairs of words, as the neuronal agent has no ability to exploit information about the underlying grid-world structure. This is also the reason why the agent requires many more trials to converge to a good performance than a rat requires to reliably find a hidden platform in a watermaze experiment <xref ref-type="bibr" rid="pcbi.1001133-Steele1">[63]</xref>. Considerably faster convergence times have been demonstrated by reinforcement learning methods if the underlying structure of the environment is incorporated into the algorithm, for example by assuming overlapping state representations <xref ref-type="bibr" rid="pcbi.1001133-Vasilaki1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Foster1">[39]</xref>.</p>
<p>In our model, all states are initialized to the same value, reflecting the assumption that the agent knows nothing about the proximity of the states to the reward position at the outset. After the task has been learnt, a gradient is developed with higher values around the reward state. Clearly, it will take the agent longer to re-learn a new reward position far away from the previous one than it took to learn the original position, as the gradient has to be unlearnt. In contrast, rats re-learn a modified task much faster than they learnt the original task <xref ref-type="bibr" rid="pcbi.1001133-Steele1">[63]</xref>. Faster re-learning has been demonstrated in a non-spiking actor-critic model when the agent learns an abstract internal state representation in addition to the value function and policy <xref ref-type="bibr" rid="pcbi.1001133-Foster1">[39]</xref>. Interestingly, it has been shown that mice with suppressed adult neurogenesis also show highly specific learning deficits, especially in re-learning, which demonstrates the importance of newly generated neurons <xref ref-type="bibr" rid="pcbi.1001133-Garthe1">[64]</xref>. In future work we will extend our model to investigate the relationship between neurogenesis, internal state representation and the speed of re-learning a modified task.</p>
<p>We have chosen the grid-world task to study the learning behavior of the proposed network model, as the complexity of the task makes it an adequate test case for TD learning algorithms. However, in experimental set-ups the role of dopamine in reward learning is typically studied in conditioning tasks, where a single stimulus is followed by a delayed reward. In order to test our network in such tasks requires an input representation different from the discrete state representation chosen in our model. Typically, in TD learning models such a stimulus is represented as a complete serial compound <xref ref-type="bibr" rid="pcbi.1001133-Schultz1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Montague1">[4]</xref>. Here, the stimulus is represented by a vector, where the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e390" xlink:type="simple"/></inline-formula>th entry represents the stimulus <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e391" xlink:type="simple"/></inline-formula> time steps into the future. Such a representation requires the system to know the number of time steps between the stimulus presentation and the reward delivery. A biologically more plausible representation of stimuli has recently been presented in <xref ref-type="bibr" rid="pcbi.1001133-Ludvig1">[65]</xref>. Here the complete serial compound is replaced by a series of internal overlapping microstimuli. It has been demonstrated that such a representation results in a TD error in good agreement with most experimental findings on the dopaminergic activity during conditioning experiments <xref ref-type="bibr" rid="pcbi.1001133-Ludvig1">[65]</xref>. It remains to be investigated in how far such a state representation can be adapted to spiking neuronal networks.</p>
</sec><sec id="s3b">
<title>Learning performance on the cliff-walk task</title>
<p>Due to its low baseline level, the dopaminergic firing rate has a much smaller dynamic range available for the representation of negative errors than for positive errors. In the literature two main possibilities to represent negative TD errors have been discussed. One possibility is that negative errors are represented by a different neuromodulator such as serotonin <xref ref-type="bibr" rid="pcbi.1001133-Daw1">[66]</xref>. Another possibility is that negative errors are encoded in the duration of the phasic pauses in the dopamine neurons <xref ref-type="bibr" rid="pcbi.1001133-Bayer1">[46]</xref>, suggesting that one neurotransmitter is enough to encode negative as well as positive errors. The latter hypothesis is supported in a modeling study demonstrating that dopamine is able to encode the full range of TD errors when the external stimuli are represented by a series of internal microstimuli <xref ref-type="bibr" rid="pcbi.1001133-Ludvig1">[65]</xref>. Our study on the cliff-walk task with purely negative rewards reveals an additional problem to that of representing negative TD errors: due to their inherent lower bound the cortico-striatal synapses are limited in their ability to store estimates of future negative rewards.</p>
<p>A possible hypothesis that would also allow learning to be driven by purely negative rewards is that the absolute values of the estimates of future negative rewards are stored in different synaptic structures from those storing estimates of future positive rewards. This hypothesis is in line with experimental studies in rats and humans showing a functional segregation within the striatum, with anterior regions responding more strongly to positive rewards and posterior regions to negative rewards <xref ref-type="bibr" rid="pcbi.1001133-Reynolds3">[67]</xref>–<xref ref-type="bibr" rid="pcbi.1001133-Seymour1">[69]</xref>. An analogous segregation has also been reported between the amygdala and the ventral striatum, with the former responding only to losses and the latter to gains <xref ref-type="bibr" rid="pcbi.1001133-Yacubian1">[70]</xref>. Our results support the hypothesis that prediction errors with respect to negative rewards are represented by a different neuromodulator and possibly a different anatomical system, rather than the duration of the phasic pauses in the dopamine neurons. On the other hand, they are compatible with a hybrid strategy in which the brain uses both mechanisms: a neuromodulator other than dopamine to encode negative errors due to punishment, and the phasic pauses in the dopaminergic firing rate to represent disappointment about an omitted reward. These hypotheses could be differentiated by tests on patients with Parkinson's disease or on animal Parkinson's models. In either case, we predict that learning is less impaired when driven by external negative rewards than by positive ones. The extent of the learning impairment in tasks where reward omission plays an important role will further discriminate whether the brain relies on dopamine or some other system to signal such events.</p>
</sec><sec id="s3c">
<title>Model architecture</title>
<p>We investigated to what extent a top-down derived plasticity model dependent on the dynamics of a dopaminergic signal with realistic firing rates is able to implement the TD(<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e392" xlink:type="simple"/></inline-formula>) algorithm. For this purpose we assumed a very simplified model of the basal ganglia adapted from <xref ref-type="bibr" rid="pcbi.1001133-Houk1">[18]</xref>. The key feature for our model is that the critic module dynamically generates a realistic error signal in response to the development of the value function encoded in the cortico-striatal synapses and the chosen action, rather than artificially generating a perfect error signal outside of the network. The mechanism by which the dopaminergic error signal is generated by the basal ganglia is as yet unknown, and answering this question is outside the scope of this manuscript. The architecture of the critic module assumed in our model uses an indirect and a delayed direct pathway from the striatum to the dopamine neurons to produce an error signal with activity and temporal features similar to those experimentally. We implement the slowness of the direct pathway by a long synaptic delay; a more biologically realistic realization could be <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e393" xlink:type="simple"/></inline-formula> receptors, which are known to mediate slow inhibitory processes. Indeed, high densities of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e394" xlink:type="simple"/></inline-formula> receptors have been found in the substantia nigra <xref ref-type="bibr" rid="pcbi.1001133-Bowery1">[71]</xref>. However, there are contradictory findings on whether the inhibitory response of the dopamine neurons is mediated by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e395" xlink:type="simple"/></inline-formula>. Whereas in vitro inhibitory responses in midbrain dopamine neurons can be mediated by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e396" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e397" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1001133-Husser1">[72]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Sugita1">[73]</xref>, in vivo studies in rats have reported that the synaptic connections between the neurons in the striatum and dopamine neurons in the substantia nigra act predominantly or exclusively via the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e398" xlink:type="simple"/></inline-formula> receptors <xref ref-type="bibr" rid="pcbi.1001133-Tepper1">[74]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Paladini1">[75]</xref>. However, a recent in vivo study in mice found that after stimulation of the striatum, dopamine neurons in the substantia nigra show a long lasting inhibition mediated by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e399" xlink:type="simple"/></inline-formula> receptors absent in rats <xref ref-type="bibr" rid="pcbi.1001133-Brazhnik1">[76]</xref>.</p>
<p>Future experimental studies may reveal whether the dopaminergic signal is indeed generated by a fast indirect path and a slow direct pathway, or by some other mechanism <xref ref-type="bibr" rid="pcbi.1001133-Joel1">[22]</xref>. Some alternative actor-critic models of the basal ganglia are discussed in <xref ref-type="bibr" rid="pcbi.1001133-Wrgtter1">[23]</xref>. Most of the alternative models make assumptions that are experimentally not well supported. For example, several models assume a direct excitatory pathway and an indirect inhibitory pathway between the striatum and the dopamine neurons <xref ref-type="bibr" rid="pcbi.1001133-Montague1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Montague2">[19]</xref>–<xref ref-type="bibr" rid="pcbi.1001133-Suri2">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Suri3">[77]</xref>, whereas in reality the situation is reversed <xref ref-type="bibr" rid="pcbi.1001133-Wrgtter1">[23]</xref>. A model that basically resembles that proposed by Houk et al. <xref ref-type="bibr" rid="pcbi.1001133-Houk1">[18]</xref> but implements several known anatomical structures more accurately than any other model was presented in <xref ref-type="bibr" rid="pcbi.1001133-Berns1">[78]</xref>. However, this model relies on three-factor synaptic plasticity rules for striato-nigral connections, for which there is no experimental evidence. This assumption is also made in <xref ref-type="bibr" rid="pcbi.1001133-ContrerasVidal1">[79]</xref>. Some of the alternative models also posit a divergent architecture, in which the input arises from two different sources <xref ref-type="bibr" rid="pcbi.1001133-ContrerasVidal1">[79]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Brown1">[80]</xref>. Due to the different timing properties along the two divergent pathways, the model proposed in <xref ref-type="bibr" rid="pcbi.1001133-Brown1">[80]</xref> is able to reproduce most of the known experimental data. However, where parallel reciprocal architectures such as those proposed in <xref ref-type="bibr" rid="pcbi.1001133-Montague1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Houk1">[18]</xref>–<xref ref-type="bibr" rid="pcbi.1001133-Suri2">[21]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Suri3">[77]</xref> can be directly related to TD learning, the same is not true for divergent or non-reciprocal architecture <xref ref-type="bibr" rid="pcbi.1001133-Wrgtter1">[23]</xref>. The generation mechanism may also depend on pathways within the basal ganglia that have so far been neglected in modeling studies. For example, input from the lateral habenula to the dopamine neurons has recently been shown to be an important source of negative inputs to the dopamine neurons <xref ref-type="bibr" rid="pcbi.1001133-Matsumoto1">[81]</xref>.</p>
<p>The focus of our work is action learning rather than action selection. Consequently, we have kept the actor module as simple as possible. One disadvantage of this choice is its vulnerability: if one actor neuron dies, the action that is represented by that neuron can never be chosen again. Furthermore, the inhibition of the actor neurons after an action has been chosen is applied externally rather than arising naturally through the network dynamics. Candidate action selection mechanisms that would overcome these limitations include attractor networks <xref ref-type="bibr" rid="pcbi.1001133-Hopfield1">[82]</xref> and competing synfire chains <xref ref-type="bibr" rid="pcbi.1001133-Jin1">[83]</xref>–<xref ref-type="bibr" rid="pcbi.1001133-Schrader1">[85]</xref>. Moreover, we have not related the action module to any specific brain region. Imaging experiments have found that the activity in the ventral striatum is correlated with the TD error during a prediction and action selection task, whereas the activity in the dorsal striatum is correlated with the TD error only during the action selection task <xref ref-type="bibr" rid="pcbi.1001133-ODoherty1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Seymour2">[86]</xref>. In the context of the actor-critic architecture, this finding implies that the ventral striatum performs the role of the critic and the dorsal striatum performs the role of the actor. Detailed models have been developed that relate the problem of action selection to loops through the basal ganglia <xref ref-type="bibr" rid="pcbi.1001133-Gurney1">[41]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Humphries1">[42]</xref> and also loops through the cerebellum and the cerebral cortex <xref ref-type="bibr" rid="pcbi.1001133-Houk2">[87]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Houk3">[88]</xref>. An overview of different basal ganglia models that especially focuses on the action selection problem can be found in <xref ref-type="bibr" rid="pcbi.1001133-Houk4">[89]</xref>.</p>
</sec><sec id="s3d">
<title>Dependence on model size</title>
<p>The error signal in our model is encoded in the difference between the dopaminergic population firing rate from its baseline level. The learning behavior of the model therefore depends on the number of dopamine neurons generating the population signal and the noise of this signal. As learning is driven by fluctuations in the dopaminergic firing rate from the baseline level, a noisier signal will drive the learning process less efficiently. A thorough investigation of the effects of model size and noise is outside the scope of this article, however, it is possible to extrapolate some of these effects from the dynamics of our model.</p>
<p>We have shown that even as few as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e400" xlink:type="simple"/></inline-formula> dopamine neurons generate a signal that is sufficiently reliable to learn the tasks investigated here. Increasing the number of neurons, assuming the synaptic baseline reference is correspondingly increased, would have the effect of reducing the noise in the dopamine signal. However, as the neuronal network model already performs as well as the discrete-time algorithm, no performance improvement can be expected. Conversely, decreasing the number of dopaminergic neurons reduces both the amplitude of the phasic signal and the baseline activity and makes the remaining signal noisier and less reliable.</p>
<p>Even assuming a perfectly reliable signal, the dynamics developed in our model are such that if the synaptic baseline reference is not reduced accordingly, the lower baseline activity appears in the synaptic plasticity dynamics as a permanent negative error signal. This depresses the synaptic weights that encode the value function and policy until they reach their minimum values. At this point the agent can no longer distinguish between states with respect to their reward proximity and has no preference for any action over any other action. Moreover, decreasing the synaptic weights that encode the policy slows the responses of the actor neurons and therefore leads to slower decision processes. Analogous behavior has been observed in patients with Parkinson's disease, which is characterized by a gradual loss in the number of dopamine neurons, who show movement as well as cognitive deficits <xref ref-type="bibr" rid="pcbi.1001133-Sethi1">[90]</xref>.</p>
<p>The dynamics of our model predicts that increasing background dopamine concentration after a gradual loss in dopamine neurons maintains any existing memory of state values, as it will restore the amount of available dopamine to the baseline level used as a reference by the synapse. However, learning in new tasks is still impaired, as this is driven by fluctuations in the dopaminergic signal rather than its baseline level. The reduced remaining population of dopaminergic neurons necessarily produces smaller and noisier fluctuations than those generated by an intact population; consequently, they provide a less effective learning signal. This is an equivalent situation to reducing the size of the dopamine population and reducing the baseline reference value in the synapse accordingly. This prediction is consistent with the finding that even fully medicated Parkinson's patients exhibit deficits in a probabilistic classification task <xref ref-type="bibr" rid="pcbi.1001133-Knowlton1">[91]</xref>. The dynamics of the critic module also predicts that the size of the striatal population should also be critical for the learning behavior, as it determines the amplitude of the phasic dopaminergic signal. This is in agreement with studies showing that a lesion of the dorsal striatum impairs the learning behavior of rats in stimulus-response learning <xref ref-type="bibr" rid="pcbi.1001133-McDonald1">[92]</xref>.</p>
</sec><sec id="s3e">
<title>Synaptic plasticity dynamics realizing TD learning</title>
<p>The plasticity dynamics presented in Eq. (8) is in some degree similar to the plasticity dynamics derived in our previous investigation of a spiking neuronal network model capable of implementing actor-critic TD learning <xref ref-type="bibr" rid="pcbi.1001133-Potjans1">[34]</xref>. The two plasticity dynamics have in common that the dynamics is triggered by biologically plausible measures of the pre-synaptic activity and is dependent on a TD error signal. However, in our earlier model there is no dopaminergic error signal available; each synapse performs its own approximation of an TD error based on the difference in a rapid and a laggard post-synaptic activity trace. The aim was to develop a continuous-time plasticity mechanism that mapped the properties of the discrete-time TD learning algorithm as accurately as possible. Thus, the study can be seen as a proof of principle that a spiking neuronal network model can implement actor-critic TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e401" xlink:type="simple"/></inline-formula> learning. On the basis of this, in our current study we focus on applying biological constraints to the range of possible plasticity dynamics by combining the previous top-down approach with a bottom-up approach.</p>
<p>The biological constraints entailed by our use of a dopaminergic error signal with realistic firing rates to represent the TD error lead to two major differences from the original plasticity mechanism developed in <xref ref-type="bibr" rid="pcbi.1001133-Potjans1">[34]</xref>. First, whereas the plasticity dynamics presented in the previous model belongs to the class of differential Hebbian learning rules modulated with a non-local constant reward signal, in the model presented here, the plasticity dynamics belongs to the class of neuromodulated, heterosynaptic plasticity. Second, whereas the earlier synaptic plasticity dynamics can be mapped exactly to the value function update of TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e402" xlink:type="simple"/></inline-formula> learning, the plasticity dynamics presented here corresponds to a slightly modified TD learning algorithm with self-adapting learning parameters.</p>
<p>Our finding that the learning parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e403" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e404" xlink:type="simple"/></inline-formula> increase with the difference in successive cortico-striatal synaptic weights <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e405" xlink:type="simple"/></inline-formula> could be tested experimentally by fitting TD learning algorithms to behavioral data gathered from animals learning two versions of a task: one with large rewards and one with small rewards. As long as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e406" xlink:type="simple"/></inline-formula>, the task with larger rewards will develop greater differences in the estimation of future rewards of successive states than the task with smaller rewards. We therefore predict that the values of the learning parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e407" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e408" xlink:type="simple"/></inline-formula> fitted to the former set of behavioral data will be greater those fitted to the latter set. Additionally, the values calculated by fitting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e409" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e410" xlink:type="simple"/></inline-formula> to different epochs in behavioral data gathered from an animal learning a given task should vary in a systematic fashion. At the very beginning, the animal presumably has no expectations about future rewards and thus estimates all states similarly. During the middle of the learning process, when the animal's performance is improving rapidly, large differences between the estimation of states can be expected. Finally, as the animal approaches its equilibrium performance, differences between the estimations of states should vary smoothly. We therefore predict that fitting <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e411" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e412" xlink:type="simple"/></inline-formula> to data gathered from the beginning and end of the learning process will result in lower values than fitting the learning parameters to data gathered whilst the performance on a given learning task is improving rapidly.</p>
</sec><sec id="s3f">
<title>TD learning and the brain</title>
<p>Is actor-critic TD learning the correct model? This is outside the scope of the current manuscript, and perhaps out of our remit altogether - this kind of question can only be answered by analyzing behavioral, electrophysiological and anatomical data from carefully designed experiments. There is evidence on the behavioral american <xref ref-type="bibr" rid="pcbi.1001133-Sutton2">[93]</xref> as well as on the cellular level american <xref ref-type="bibr" rid="pcbi.1001133-Schultz1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Reynolds2">[9]</xref> that mammals implement TD learning strategies. TD learning has been successfully applied to model bee foraging in uncertain environments american <xref ref-type="bibr" rid="pcbi.1001133-Montague2">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Niv1">[94]</xref>, human decision making american <xref ref-type="bibr" rid="pcbi.1001133-Montague1">[4]</xref> and rat navigation american <xref ref-type="bibr" rid="pcbi.1001133-Foster1">[39]</xref>, but it is unlikely to be the only learning strategy used by the brain <xref ref-type="bibr" rid="pcbi.1001133-Doya1">[95]</xref>. In line with previous studies <xref ref-type="bibr" rid="pcbi.1001133-ODoherty1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Houk1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Suri1">[20]</xref>, we have focused on TD learning with the actor-critic architecture instead of other TD learning methods, such as SARSA or Q-learning <xref ref-type="bibr" rid="pcbi.1001133-Sutton1">[1]</xref>. However, recent experimental findings also support the interpretation that mammals implement TD learning methods based on action values <xref ref-type="bibr" rid="pcbi.1001133-Morris2">[17]</xref> or an actor-director model <xref ref-type="bibr" rid="pcbi.1001133-Attalah1">[14]</xref>. Further research is needed, especially on the theoretical side, in order to understand if these models are compatible with spiking neuronal networks.</p>
<p>We have focused on the simplest TD learning algorithm: TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e413" xlink:type="simple"/></inline-formula>. However, it is likely that the mammalian brain uses more advanced TD learning strategies. TD<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e414" xlink:type="simple"/></inline-formula> learning is efficient as long as the number of possible states and actions are restricted to a small to moderate number. To address problems with a large number of states and possible actions, TD learning methods that generalize from a small number of observed states and chosen actions are needed (see <xref ref-type="bibr" rid="pcbi.1001133-Sutton1">[1]</xref>). Furthermore, it has been demonstrated that classical TD learning schemes cannot account for behavioral data involving motivation. Modified TD algorithms can explain these data, either by explicitly including a motivational term <xref ref-type="bibr" rid="pcbi.1001133-LaCamera1">[96]</xref> or by ‘average-reward TD-learning’, where an average reward acts as a baseline <xref ref-type="bibr" rid="pcbi.1001133-Dayan3">[97]</xref>.</p>
<p>Here, we have interpreted the phasic dopaminergic signal in the light of TD learning. However, the literature presents a much broader picture of the functional role of the dopaminergic activity. It has been found that only a small subgroup of dopamine neurons show a response consistent with the TD error hypothesis; a much broader group responds with an increase in activity to positive as well as negative reward related signals inconsistent with the hypothesis <xref ref-type="bibr" rid="pcbi.1001133-Matsumoto2">[98]</xref>. There is also evidence that dopamine is involved with signalling ‘desire’ for a reward rather than the reward itself <xref ref-type="bibr" rid="pcbi.1001133-AriasCarrion1">[99]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Pecina1">[100]</xref>. Furthermore, the phasic dopaminergic signal responds to a much larger category of events than just to reward related events, including aversive, high intensity or novel stimuli <xref ref-type="bibr" rid="pcbi.1001133-Horvitz1">[101]</xref>. Alternative interpretations of the phasic signal include the theory that it acts more like a switch than a reward signal, triggering learning at the right point in time <xref ref-type="bibr" rid="pcbi.1001133-Redgrave2">[102]</xref>, <xref ref-type="bibr" rid="pcbi.1001133-Porr1">[103]</xref>, or that it promotes the discovery of new actions and learning of new action-outcome associations, independent of the economic value of the action <xref ref-type="bibr" rid="pcbi.1001133-Redgrave1">[5]</xref>. Given the diversity of dopaminergic responses and considering the fact that midbrain dopamine neurons project to many different brain areas, such as the striatum, the orbifrontal cortex and the amygdala <xref ref-type="bibr" rid="pcbi.1001133-Schultz2">[3]</xref>, it is also likely that different interpretations are simultaneously valid; the information encoded in the phasic signal being combined with local information in specific areas of the brain to realize a variety of functions.</p>
</sec></sec><sec id="s4" sec-type="methods">
<title>Methods</title>
<sec id="s4a">
<title>Neuronal network simulations</title>
<p>We investigated our model using numerical simulations. We implemented the model in the simulator NEST <xref ref-type="bibr" rid="pcbi.1001133-Gewaltig1">[104]</xref> and performed the simulations in parallel on two nodes of a cluster of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e415" xlink:type="simple"/></inline-formula> SUN X<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e416" xlink:type="simple"/></inline-formula> machines, each with two <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e417" xlink:type="simple"/></inline-formula> AMD Opteron <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e418" xlink:type="simple"/></inline-formula> quad core processors running Ubuntu Linux. The dopamine modulated plasticity dynamics Eq. (8) and Eq. (13) are implemented employing the distributed simulation framework presented in <xref ref-type="bibr" rid="pcbi.1001133-Potjans2">[105]</xref>.</p>
<p>All neurons in the network are modeled as current-based integrate-and-fire neurons. The dynamics of the membrane potential for each neuron is given by:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e419" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e420" xlink:type="simple"/></inline-formula> is the time constant, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e421" xlink:type="simple"/></inline-formula> the capacity of the membrane and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e422" xlink:type="simple"/></inline-formula> the input current to the neurons <xref ref-type="bibr" rid="pcbi.1001133-Tuckwell1">[106]</xref>. When <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e423" xlink:type="simple"/></inline-formula> reaches a threshold <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e424" xlink:type="simple"/></inline-formula>, a spike is emitted. The membrane potential is subsequently clamped to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e425" xlink:type="simple"/></inline-formula> for the duration of an absolute refractory period <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e426" xlink:type="simple"/></inline-formula>. The synaptic current due to an incoming spike is represented as an <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e427" xlink:type="simple"/></inline-formula>-function<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e428" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e429" xlink:type="simple"/></inline-formula> is the peak amplitude and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1001133.e430" xlink:type="simple"/></inline-formula> the rise time. The neuronal parameters are specified in the following section.</p>
</sec><sec id="s4b">
<title>Model description and parameter specification</title>
<p>The details of the model are summarized in <xref ref-type="fig" rid="pcbi-1001133-g012">Fig. 12</xref> using the scheme developed by <xref ref-type="bibr" rid="pcbi.1001133-Nordlie1">[107]</xref>. The parameters used in the numerical simulations are specified in <xref ref-type="fig" rid="pcbi-1001133-g013">Fig. 13</xref>.</p>
<fig id="pcbi-1001133-g012" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001133.g012</object-id><label>Figure 12</label><caption>
<title>Model description after <xref ref-type="bibr" rid="pcbi.1001133-Nordlie1">[<bold>107</bold>]</xref>.</title>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.g012" xlink:type="simple"/></fig><fig id="pcbi-1001133-g013" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1001133.g013</object-id><label>Figure 13</label><caption>
<title>Parameter specification.</title>
<p>The categories refer to the model description in <xref ref-type="fig" rid="pcbi-1001133-g012">Fig. 12</xref>.</p>
</caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.g013" xlink:type="simple"/></fig></sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1001133.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.s001" xlink:type="simple"><label>Text S1</label><caption>
<p>Conditions for a constant dopaminergic baseline firing rate.</p>
<p>(0.14 MB PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1001133.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1001133.s002" xlink:type="simple"><label>Text S2</label><caption>
<p>Mapping parameters.</p>
<p>(0.19 MB PDF)</p>
</caption></supplementary-material></sec></body>
<back>
<ack>
<p>The authors thank H. E. Plesser for consultation about the implementation of neuromodulated plasticity in distributed simulations. We further acknowledge T. Potjans for fruitful discussions.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1001133-Sutton1"><label>1</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sutton</surname><given-names>RS</given-names></name>
<name name-style="western"><surname>Barto</surname><given-names>AG</given-names></name>
</person-group>             <year>1998</year>             <source>Reinforcement Learning: An Introduction. Adaptive Computation and Machine Learning</source>             <publisher-name>The MIT Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1001133-Schultz1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Montague</surname><given-names>PR</given-names></name>
</person-group>             <year>1997</year>             <article-title>A neural substrate of prediction and reward.</article-title>             <source>Science</source>             <volume>275</volume>             <fpage>1593</fpage>             <lpage>1599</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Schultz2"><label>3</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>
</person-group>             <year>2002</year>             <article-title>Getting formal with dopamine and reward.</article-title>             <source>Neuron</source>             <volume>36</volume>             <fpage>241</fpage>             <lpage>263</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Montague1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Montague</surname><given-names>PR</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Sejowski</surname><given-names>TJ</given-names></name>
</person-group>             <year>1996</year>             <article-title>A framework for mesencephalic dopamine systems based on predictive Hebbian learning.</article-title>             <source>J Neurosci</source>             <volume>16</volume>             <fpage>1936</fpage>             <lpage>1947</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Redgrave1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Redgrave</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Gurney</surname><given-names>K</given-names></name>
</person-group>             <year>2006</year>             <article-title>The short-latency dopamine signal: a role in discovering novel actions?</article-title>             <source>Nat Rev Neurosci</source>             <volume>7</volume>             <fpage>967</fpage>             <lpage>975</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Pessiglione1"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pessiglione</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Seymour</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Flandin</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Dolan</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Frith</surname><given-names>C</given-names></name>
</person-group>             <year>2006</year>             <article-title>Dopamine-dependent prediction errors underpin reward-seeking behaviour in humans.</article-title>             <source>Nature</source>             <volume>442</volume>             <fpage>1042</fpage>             <lpage>1045</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Reynolds1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Reynolds</surname><given-names>JNJ</given-names></name>
<name name-style="western"><surname>Hyland</surname><given-names>BI</given-names></name>
<name name-style="western"><surname>Wickens</surname><given-names>JR</given-names></name>
</person-group>             <year>2001</year>             <article-title>A cellular mechanism of reward-related learning.</article-title>             <source>Nature</source>             <volume>413</volume>             <fpage>67</fpage>             <lpage>70</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Pawlak1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pawlak</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Kerr</surname><given-names>JN</given-names></name>
</person-group>             <year>2008</year>             <article-title>Dopamine receptor activation is required for corticostriatal spike-timing-dependent plasticity.</article-title>             <source>J Neurosci</source>             <volume>28</volume>             <fpage>2435</fpage>             <lpage>2446</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Reynolds2"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Reynolds</surname><given-names>JN</given-names></name>
<name name-style="western"><surname>Wickens</surname><given-names>JR</given-names></name>
</person-group>             <year>2002</year>             <article-title>Dopamine-dependent plasticity of corticostriatal synapses.</article-title>             <source>Neural Netw</source>             <volume>15</volume>             <fpage>507</fpage>             <lpage>521</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-ODoherty1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>O'Doherty</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Schultz</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Deichmann</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>
<etal/></person-group>             <year>2004</year>             <article-title>Dissociable roles of ventral and dorsal striatum in instrumental conditioning.</article-title>             <source>Science</source>             <volume>304</volume>             <fpage>452</fpage>             <lpage>454</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Witten1"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Witten</surname><given-names>IH</given-names></name>
</person-group>             <year>1977</year>             <article-title>An adaptive optimal controller for discrete-time markov environments.</article-title>             <source>Information and Control</source>             <volume>34</volume>             <fpage>286</fpage>             <lpage>295</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Barto1"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Barto</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Sutton</surname><given-names>RS</given-names></name>
<name name-style="western"><surname>Anderson</surname><given-names>CW</given-names></name>
</person-group>             <year>1983</year>             <article-title>Neuronlike adaptive elements that can solve difficult learning control problems.</article-title>             <source>IEEE Trans Syst Man Cybern</source>             <volume>13</volume>             <fpage>834</fpage>             <lpage>846</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Morris1"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Morris</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Nevet</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Arkadir</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Vaadia</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Bergman</surname><given-names>H</given-names></name>
</person-group>             <year>2006</year>             <article-title>Midbrain dopamine neurons encode decisions for future action.</article-title>             <source>Nat Neurosci</source>             <volume>9</volume>             <fpage>1057</fpage>             <lpage>1063</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Attalah1"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Attalah</surname><given-names>HE</given-names></name>
<name name-style="western"><surname>Lopez-Paniagua</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Rudy</surname><given-names>JW</given-names></name>
<name name-style="western"><surname>O'Reilly</surname><given-names>RC</given-names></name>
</person-group>             <year>2007</year>             <article-title>Separate neural substrates for skill-learning and performance in the ventral and dorsal striatum.</article-title>             <source>Nat Neurosci</source>             <volume>10</volume>             <fpage>126</fpage>             <lpage>131</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Fiorillo1"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fiorillo</surname><given-names>CD</given-names></name>
<name name-style="western"><surname>Tobler</surname><given-names>PN</given-names></name>
<name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>
</person-group>             <year>2003</year>             <article-title>Discrete coding of reward probability and uncertainty by dopamine neurons.</article-title>             <source>Science</source>             <volume>299</volume>             <fpage>1898</fpage>             <lpage>1902</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Tobler1"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tobler</surname><given-names>PN</given-names></name>
<name name-style="western"><surname>Fiorillo</surname><given-names>CD</given-names></name>
<name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>
</person-group>             <year>2005</year>             <article-title>Adaptive coding of reward value by dopamine neurons.</article-title>             <source>Science</source>             <volume>307</volume>             <fpage>1642</fpage>             <lpage>1645</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Morris2"><label>17</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Morris</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Arkadir</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Nevet</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Vaadia</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Bergman</surname><given-names>H</given-names></name>
</person-group>             <year>2004</year>             <article-title>Coincident but distinct messages of midbrain dopamine and striatal tonically active neurons.</article-title>             <source>Neuron</source>             <volume>1</volume>             <fpage>133</fpage>             <lpage>143</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Houk1"><label>18</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Houk</surname><given-names>JC</given-names></name>
<name name-style="western"><surname>Adams</surname><given-names>JL</given-names></name>
<name name-style="western"><surname>Barto</surname><given-names>AG</given-names></name>
</person-group>             <year>1995</year>             <source>A model of how the basal ganglia generate and use neural signals that predict reinforcement</source>             <publisher-name>MIT Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1001133-Montague2"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Montague</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Person</surname><given-names>C</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>T</given-names></name>
</person-group>             <year>1995</year>             <article-title>Bee foraging in uncertain environments using predictive Hebbian learning.</article-title>             <source>Nature</source>             <volume>377</volume>             <fpage>725</fpage>             <lpage>728</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Suri1"><label>20</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Suri</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>
</person-group>             <year>1999</year>             <article-title>A neural network model with dopamine-like reinforcement signal that learns a spatial delayed reponse task.</article-title>             <source>Neuroscience</source>             <volume>91</volume>             <fpage>871</fpage>             <lpage>890</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Suri2"><label>21</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Suri</surname><given-names>RE</given-names></name>
<name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>
</person-group>             <year>2001</year>             <article-title>Temporal difference model reproduces anticipatory neural activity.</article-title>             <source>Neural Comput</source>             <volume>13</volume>             <fpage>841</fpage>             <lpage>862</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Joel1"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Joel</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Niv</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Ruppin</surname><given-names>E</given-names></name>
</person-group>             <year>2002</year>             <article-title>Actor-critic models of the basal ganglia: new anatomical and computational perspectives.</article-title>             <source>Neural Netw</source>             <volume>15</volume>             <fpage>535</fpage>             <lpage>547</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Wrgtter1"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wörgötter</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Porr</surname><given-names>B</given-names></name>
</person-group>             <year>2005</year>             <article-title>Temporal sequence learning, prediction, and control: A review of different models and their relation to biological mechanisms.</article-title>             <source>Neural Comput</source>             <volume>17</volume>             <fpage>245</fpage>             <lpage>319</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Seung1"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name>
</person-group>             <year>2003</year>             <article-title>Learning spiking neural networks by reinforcement of stochastic synaptic transmission.</article-title>             <source>Neuron</source>             <volume>40</volume>             <fpage>1063</fpage>             <lpage>1073</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Xie1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Xie</surname><given-names>X</given-names></name>
<name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name>
</person-group>             <year>2004</year>             <article-title>Learning in neural networks by reinforcement of irregular spiking.</article-title>             <source>Phys Rev E</source>             <volume>69</volume>             <fpage>41909</fpage>          </element-citation></ref>
<ref id="pcbi.1001133-Baras1"><label>26</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Baras</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Meir</surname><given-names>R</given-names></name>
</person-group>             <year>2007</year>             <article-title>Reinforcement learning, spike-time-dependent plasticity, and the BCM rule.</article-title>             <source>Neural Comput</source>             <volume>19</volume>             <fpage>2245</fpage>             <lpage>2279</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Florian1"><label>27</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Florian</surname><given-names>RV</given-names></name>
</person-group>             <year>2007</year>             <article-title>Reinforcement learning through modulation of spike-timing-dependent synaptic plasticity.</article-title>             <source>Neural Comput</source>             <volume>19</volume>             <fpage>1468</fpage>             <lpage>1502</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Legenstein1"><label>28</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Legenstein</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Pecevski</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>
</person-group>             <year>2008</year>             <article-title>A learning theory for reward-modulated spike-timing-dependent plasticity with application to biofeedback.</article-title>             <source>PLoS Comput Biol</source>             <volume>4</volume>             <fpage>e1000180</fpage>          </element-citation></ref>
<ref id="pcbi.1001133-Vasilaki1"><label>29</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Vasilaki</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Frémaux</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Urbanczik</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Senn</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name>
</person-group>             <year>2009</year>             <article-title>Spike-based reinforcement learning in continuous state and action space: When policy gradient methods fail.</article-title>             <source>PLoS Comput Biol</source>             <volume>5</volume>             <fpage>e1000586</fpage>             <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000586" xlink:type="simple">10.1371/journal.pcbi.1000586</ext-link></comment>          </element-citation></ref>
<ref id="pcbi.1001133-Frmaux1"><label>30</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Frémaux</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Sprekeler</surname><given-names>H</given-names></name>
<name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name>
</person-group>             <year>2010</year>             <article-title>Functional requirements for reward-modulated spike-timing-dependent plasticity.</article-title>             <source>J Neurosci</source>             <volume>30</volume>             <fpage>13326</fpage>             <lpage>13337</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Rao1"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Rao</surname><given-names>RPN</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name>
</person-group>             <year>2001</year>             <article-title>Spike-timing-dependent Hebbian plasticity as temporal difference learning.</article-title>             <source>Neural Comput</source>             <volume>13</volume>             <fpage>2221</fpage>             <lpage>2237</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Farries1"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Farries</surname><given-names>MA</given-names></name>
<name name-style="western"><surname>Fairhall</surname><given-names>AL</given-names></name>
</person-group>             <year>2007</year>             <article-title>Reinforcement learning with modulated spike timing-dependent synaptic plasticity.</article-title>             <source>J Neurophysiol</source>             <volume>98</volume>             <fpage>3648</fpage>             <lpage>3665</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Izhikevich1"><label>33</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Izhikevich</surname><given-names>EM</given-names></name>
</person-group>             <year>2007</year>             <article-title>Solving the distal reward problem through linkage of STDP and dopamine signaling.</article-title>             <source>Cereb Cortex</source>             <volume>17</volume>             <fpage>2443</fpage>             <lpage>2452</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Potjans1"><label>34</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Potjans</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Morrison</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Diesmann</surname><given-names>M</given-names></name>
</person-group>             <year>2009</year>             <article-title>A spiking neural network model of an actor-critic learning agent.</article-title>             <source>Neural Comput</source>             <volume>21</volume>             <fpage>301</fpage>             <lpage>339</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Dennett1"><label>35</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Dennett</surname><given-names>DC</given-names></name>
</person-group>             <year>1998</year>             <source>Brainchildren: Essays on Designing Minds</source>             <publisher-name>The MIT Press, 1 edition</publisher-name>          </element-citation></ref>
<ref id="pcbi.1001133-Barto2"><label>36</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Barto</surname><given-names>AG</given-names></name>
</person-group>             <year>1995</year>             <article-title>Adaptive critic and the basal ganglia.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Houk</surname><given-names>JC</given-names></name>
<name name-style="western"><surname>Davis</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Beiser</surname><given-names>D</given-names></name>
</person-group>             <source>Models of Information Processing in the Basal Ganglia</source>             <publisher-loc>Cambridge (Massachusetts)</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>215</fpage>             <lpage>232</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Dayan1"><label>37</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
</person-group>             <year>1992</year>             <article-title>The convergence of td(λ) for general λ.</article-title>             <source>Mach Learn</source>             <volume>8</volume>             <fpage>341</fpage>             <lpage>362</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Dayan2"><label>38</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>T</given-names></name>
</person-group>             <year>1994</year>             <article-title>Td(λ) converges with probability 1.</article-title>             <source>Mach Learn</source>             <volume>14</volume>             <fpage>295</fpage>             <lpage>301</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Foster1"><label>39</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Foster</surname><given-names>DJ</given-names></name>
<name name-style="western"><surname>Morris</surname><given-names>RGM</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
</person-group>             <year>2000</year>             <article-title>A model of hippocampally dependent navigation, using the temporal difference learning rule.</article-title>             <source>Hippocampus</source>             <volume>10</volume>             <fpage>1</fpage>             <lpage>16</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-VanRullen1"><label>40</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>VanRullen</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Guyonneau</surname><given-names>R</given-names></name>
<name name-style="western"><surname>Thorpe</surname><given-names>SJ</given-names></name>
</person-group>             <year>2005</year>             <article-title>Spike times make sense.</article-title>             <source>Trends Neurosci</source>             <volume>28</volume>             <fpage>1</fpage>             <lpage>4</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Gurney1"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gurney</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Prescott</surname><given-names>TJ</given-names></name>
<name name-style="western"><surname>Redgrave</surname><given-names>P</given-names></name>
</person-group>             <year>2001</year>             <article-title>A computational model of action selection in the basal ganglia. i. a new functional anatomy.</article-title>             <source>Biol Cybern</source>             <volume>84</volume>             <fpage>401</fpage>             <lpage>410</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Humphries1"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Humphries</surname><given-names>MD</given-names></name>
<name name-style="western"><surname>Stewart</surname><given-names>RD</given-names></name>
<name name-style="western"><surname>Gurney</surname><given-names>KN</given-names></name>
</person-group>             <year>2006</year>             <article-title>A physiologically plausible model of action selection and oscillatory activity in the basal ganglia.</article-title>             <source>J Neurosci</source>             <volume>26</volume>             <fpage>12921</fpage>             <lpage>12942</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Prinz1"><label>43</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Prinz</surname><given-names>AA</given-names></name>
<name name-style="western"><surname>Bucher</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Marder</surname><given-names>E</given-names></name>
</person-group>             <year>2004</year>             <article-title>Similar network activity from disparate circuit parameters.</article-title>             <source>Nat Neurosci</source>             <volume>7</volume>             <fpage>1345</fpage>             <lpage>1352</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Dai1"><label>44</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Dai</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Tepper</surname><given-names>JM</given-names></name>
</person-group>             <year>1998</year>             <article-title>Do silent dopaminergic neurons exist in rat substantia nigra in vivo?</article-title>             <source>Neuroscience</source>             <volume>85</volume>             <fpage>1089</fpage>             <lpage>1099</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Hyland1"><label>45</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hyland</surname><given-names>BI</given-names></name>
<name name-style="western"><surname>Reynolds</surname><given-names>JNJ</given-names></name>
<name name-style="western"><surname>Hay</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Perk</surname><given-names>CG</given-names></name>
<name name-style="western"><surname>Miller</surname><given-names>R</given-names></name>
</person-group>             <year>2002</year>             <article-title>Firing modes of midbrain dopamine cells in the freely moving rat.</article-title>             <source>Neuroscience</source>             <volume>114</volume>             <fpage>475</fpage>             <lpage>492</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Bayer1"><label>46</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bayer</surname><given-names>HM</given-names></name>
<name name-style="western"><surname>Lau</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Glimcher</surname><given-names>PW</given-names></name>
</person-group>             <year>2007</year>             <article-title>Statistics of midbrain dopamine neuron spike trains in the awake primate.</article-title>             <source>J Neurophysiol</source>             <volume>98</volume>             <fpage>1428</fpage>             <lpage>1439</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Ljungberg1"><label>47</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ljungberg</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Apicella</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>
</person-group>             <year>1992</year>             <article-title>Responses of monkey dopamine neurons during learning of behavioral reactions.</article-title>             <source>J Neurophysiol</source>             <volume>67</volume>             <fpage>145</fpage>             <lpage>163</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Schultz3"><label>48</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>
</person-group>             <year>1998</year>             <article-title>Predictive reward signal of dopamine neurons.</article-title>             <source>J Neurophysiol</source>             <volume>80</volume>             <fpage>1</fpage>             <lpage>27</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Helias1"><label>49</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Helias</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Deger</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Rotter</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Diesmann</surname><given-names>M</given-names></name>
</person-group>             <year>2010</year>             <article-title>Instantaneous non-linear processing by pulse-coupled threshold units.</article-title>             <source>PLoS Comput Biol</source>             <volume>6</volume>             <fpage>e1000929</fpage>          </element-citation></ref>
<ref id="pcbi.1001133-Froemke1"><label>50</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Froemke</surname><given-names>RC</given-names></name>
<name name-style="western"><surname>Dan</surname><given-names>Y</given-names></name>
</person-group>             <year>2002</year>             <article-title>Spike-timing-dependent synaptic modification induced by natural spike trains.</article-title>             <source>Nature</source>             <volume>416</volume>             <fpage>433</fpage>             <lpage>438</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Garris1"><label>51</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Garris</surname><given-names>PA</given-names></name>
<name name-style="western"><surname>Ciolkowski</surname><given-names>EL</given-names></name>
<name name-style="western"><surname>Pastore</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Wightman</surname><given-names>RM</given-names></name>
</person-group>             <year>1994</year>             <article-title>Efflux of dopamine from the synaptic cleft in the nucleus accumbens of the rat brain.</article-title>             <source>J Neurosci</source>             <volume>14</volume>             <fpage>6084</fpage>             <lpage>6093</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Montague3"><label>52</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Montague</surname><given-names>PR</given-names></name>
<name name-style="western"><surname>McClure</surname><given-names>SM</given-names></name>
<name name-style="western"><surname>Baldwin</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Phillips</surname><given-names>PE</given-names></name>
<name name-style="western"><surname>Budygin</surname><given-names>EA</given-names></name>
<etal/></person-group>             <year>2004</year>             <article-title>Dynamic gain control of dopamine delivery in freely moving animals.</article-title>             <source>J Neurosci</source>             <volume>24</volume>             <fpage>1754</fpage>             <lpage>1759</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Soltani1"><label>53</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Soltani</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Lee</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Wang</surname><given-names>XJ</given-names></name>
</person-group>             <year>2006</year>             <article-title>Neural mechanism for stochastic behavior during a competitive game.</article-title>             <source>Neural Netw</source>             <volume>19</volume>             <fpage>1075</fpage>             <lpage>1090</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Schweighofer1"><label>54</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schweighofer</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Doya</surname><given-names>K</given-names></name>
</person-group>             <year>2003</year>             <article-title>Meta-learning in reinforcement learning.</article-title>             <source>Neural Comput</source>             <volume>16</volume>             <fpage>5</fpage>             <lpage>9</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Friston1"><label>55</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>
<name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Reeke</surname><given-names>GN</given-names><suffix>Jr</suffix></name>
<name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name>
<name name-style="western"><surname>Edelman</surname><given-names>GM</given-names></name>
</person-group>             <year>1994</year>             <article-title>Value-dependent selection in the brain: Simulation in a synthetic neural model.</article-title>             <source>Neuroscience</source>             <volume>59</volume>             <fpage>229</fpage>             <lpage>243</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Calabresi1"><label>56</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Calabresi</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Fedele</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Pisani</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Fontana</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Mercuri</surname><given-names>N</given-names></name>
<etal/></person-group>             <year>1995</year>             <article-title>Transmitter release associated with long-term synaptic depression in rat corticostriatal slices.</article-title>             <source>Eur J Neurosci</source>             <volume>7</volume>             <fpage>1889</fpage>             <lpage>1894</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Wickens1"><label>57</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Wickens</surname><given-names>J</given-names></name>
</person-group>             <year>1993</year>             <article-title>A Theory of the Striatum.</article-title>             <comment>Pergamon Studies in Neuroscience. Pergamon</comment>          </element-citation></ref>
<ref id="pcbi.1001133-Pawlak2"><label>58</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pawlak</surname><given-names>V</given-names></name>
<name name-style="western"><surname>Wickens</surname><given-names>JR</given-names></name>
<name name-style="western"><surname>Kirkwood</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Kerr</surname><given-names>JND</given-names></name>
</person-group>             <year>2010</year>             <article-title>Timing is not everything: neuromodulation opens the STDP gate.</article-title>             <source>Front Syn Neurosci</source>             <volume>2</volume>             <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fnsyn.2010.00146" xlink:type="simple">10.3389/fnsyn.2010.00146</ext-link></comment>          </element-citation></ref>
<ref id="pcbi.1001133-Nakano1"><label>59</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Nakano</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Doi</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Yoshimoto</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Doya</surname><given-names>K</given-names></name>
</person-group>             <year>2010</year>             <article-title>A kinetic model of dopamine- and calcium-dependent striatal synaptic plasticity.</article-title>             <source>PLoS Comput Biol</source>             <volume>6</volume>             <fpage>e1000670</fpage>          </element-citation></ref>
<ref id="pcbi.1001133-Loewenstein1"><label>60</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Loewenstein</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name>
</person-group>             <year>2006</year>             <article-title>Operant matching is a generic outcome of synaptic plasticity based on the covariance between reward and neural activity.</article-title>             <source>Proc Natl Acad Sci USA</source>             <volume>103</volume>             <fpage>15224</fpage>             <lpage>9</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Fusi1"><label>61</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Asaad</surname><given-names>WF</given-names></name>
<name name-style="western"><surname>Miller</surname><given-names>EK</given-names></name>
<name name-style="western"><surname>Wang</surname><given-names>XJ</given-names></name>
</person-group>             <year>2007</year>             <article-title>A neural circuit model of flexible sensorimotor mapping: learning and forgetting on multiple timescales.</article-title>             <source>Neuron</source>             <volume>54</volume>             <fpage>319</fpage>             <lpage>33</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Soltani2"><label>62</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Soltani</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Wang</surname><given-names>XJ</given-names></name>
</person-group>             <year>2010</year>             <article-title>Synaptic computation underlying probabilistic inference.</article-title>             <source>Nat Neurosci</source>             <volume>13</volume>             <fpage>112</fpage>             <lpage>9</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Steele1"><label>63</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Steele</surname><given-names>RJ</given-names></name>
<name name-style="western"><surname>Morris</surname><given-names>RGMM</given-names></name>
</person-group>             <year>1999</year>             <article-title>Delay-dependent impairment of a matching-to-place task with chronic and intrahippocampal infusion of the nmda-antagonist d-ap5.</article-title>             <source>Hippocampus</source>             <volume>9</volume>             <fpage>118</fpage>             <lpage>136</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Garthe1"><label>64</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Garthe</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Behr</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Kempermann</surname><given-names>G</given-names></name>
</person-group>             <year>2009</year>             <article-title>Adult-generated hippocampal neurons allow the flexible use of spatially precise learning strategies.</article-title>             <source>PLoS ONE</source>             <volume>4</volume>             <fpage>e5464</fpage>          </element-citation></ref>
<ref id="pcbi.1001133-Ludvig1"><label>65</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Ludvig</surname><given-names>EA</given-names></name>
<name name-style="western"><surname>Sutton</surname><given-names>RS</given-names></name>
<name name-style="western"><surname>Kehoe</surname><given-names>EJ</given-names></name>
</person-group>             <year>2008</year>             <article-title>Stimulus representation and the timing of reward-prediction errors in models of the dopamine system.</article-title>             <source>Neural Comput</source>             <volume>20</volume>             <fpage>3034</fpage>             <lpage>3054</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Daw1"><label>66</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Daw</surname><given-names>ND</given-names></name>
<name name-style="western"><surname>Kakade</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
</person-group>             <year>2002</year>             <article-title>Opponent interactions between serotonin and dopamine.</article-title>             <source>Neural Networks</source>             <volume>15</volume>             <fpage>603</fpage>             <lpage>616</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Reynolds3"><label>67</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Reynolds</surname><given-names>SM</given-names></name>
<name name-style="western"><surname>Berridge</surname><given-names>KC</given-names></name>
</person-group>             <year>2001</year>             <article-title>Fear and feeding in the nucleus accumbens shell: Rostrocaudal segregation of gaba-elicited defensive behavior versus eating behavior.</article-title>             <source>J Neurosci</source>             <volume>21</volume>             <fpage>3261</fpage>             <lpage>3270</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Reynolds4"><label>68</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Reynolds</surname><given-names>SM</given-names></name>
<name name-style="western"><surname>Berridge</surname><given-names>KC</given-names></name>
</person-group>             <year>2002</year>             <article-title>Positive and negative motivation in nucleus accumbens shell: Bivalent rostrocaudal gradients for gaba-elicited eating, taste “liking”/“disliking” reactions, place preference/avoidance, and fear.</article-title>             <source>J Neurosci</source>             <volume>22</volume>             <fpage>7308</fpage>             <lpage>7320</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Seymour1"><label>69</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Seymour</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Daw</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Singer</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Dolan</surname><given-names>R</given-names></name>
</person-group>             <year>2007</year>             <article-title>Differential encoding of losses and gains in the human striatum.</article-title>             <source>J Neurosci</source>             <volume>27</volume>             <fpage>4826</fpage>             <lpage>4831</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Yacubian1"><label>70</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Yacubian</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Gläscher</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Schroeder</surname><given-names>K</given-names></name>
<name name-style="western"><surname>Sommer</surname><given-names>T</given-names></name>
<name name-style="western"><surname>Braus</surname><given-names>DF</given-names></name>
<etal/></person-group>             <year>2006</year>             <article-title>Dissociable systems for gain- and loss-related value predictions and errors of prediction in the human brain.</article-title>             <source>J Neurosci</source>             <volume>26</volume>             <fpage>9530</fpage>             <lpage>9537</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Bowery1"><label>71</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Bowery</surname><given-names>N</given-names></name>
<name name-style="western"><surname>Hudson</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Price</surname><given-names>G</given-names></name>
</person-group>             <year>1987</year>             <article-title>Gabaa andgabab receptor site distribution in the rat central nervous system.</article-title>             <source>Neuroscience</source>             <volume>20</volume>             <fpage>365</fpage>             <lpage>383</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Husser1"><label>72</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Häusser</surname><given-names>MA</given-names></name>
<name name-style="western"><surname>Yung</surname><given-names>WH</given-names></name>
</person-group>             <year>1994</year>             <article-title>Inhibitory synaptic potentials in guinea-pig substantia nigra dopamine neurones in vitro.</article-title>             <source>J Physiol</source>             <volume>479</volume>             <fpage>401</fpage>             <lpage>422</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Sugita1"><label>73</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sugita</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Johnson</surname><given-names>SW</given-names></name>
<name name-style="western"><surname>North</surname><given-names>RA</given-names></name>
</person-group>             <year>1992</year>             <article-title>Synaptic inputs to gabaa and gabab receptors originate from discrete afferent neurons.</article-title>             <source>Neurosci Lett</source>             <volume>134</volume>             <fpage>207</fpage>             <lpage>211</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Tepper1"><label>74</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tepper</surname><given-names>JM</given-names></name>
<name name-style="western"><surname>Martin</surname><given-names>L</given-names></name>
<name name-style="western"><surname>Anderson</surname><given-names>DR</given-names></name>
</person-group>             <year>1995</year>             <article-title>Gabaa receptor-mediated inhibition of rat substantia nigra dopaminergic neurons by pars reticulata projection neurons.</article-title>             <source>J Neurosci</source>             <volume>15</volume>             <fpage>3092</fpage>             <lpage>3103</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Paladini1"><label>75</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Paladini</surname><given-names>CA</given-names></name>
<name name-style="western"><surname>Celada</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Tepper</surname><given-names>JM</given-names></name>
</person-group>             <year>1999</year>             <article-title>Striatal, pallidal, and pars reticulata evoked inhibition of nigrostriatal dopaminergic neurons is mediated by gabaa receptors in vivo.</article-title>             <source>Neuroscience</source>             <volume>89</volume>             <fpage>799</fpage>             <lpage>812</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Brazhnik1"><label>76</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Brazhnik</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Shah</surname><given-names>F</given-names></name>
<name name-style="western"><surname>Tepper</surname><given-names>JM</given-names></name>
</person-group>             <year>2008</year>             <article-title>Gabaergic afferents activate both gabaa and gabab receptors in mouse substantia nigra dopaminergic neurons in vivo.</article-title>             <source>J Neurosci</source>             <volume>28</volume>             <fpage>10386</fpage>             <lpage>10398</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Suri3"><label>77</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Suri</surname><given-names>RE</given-names></name>
<name name-style="western"><surname>Bargas</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Arbib</surname><given-names>MA</given-names></name>
</person-group>             <year>2001</year>             <article-title>Modeling functions of striatal dopamine modulation in learning and planning.</article-title>             <source>Neuroscience</source>             <volume>103</volume>             <fpage>65</fpage>             <lpage>85</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Berns1"><label>78</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Berns</surname><given-names>GS</given-names></name>
<name name-style="western"><surname>Sejnowski</surname><given-names>TJ</given-names></name>
</person-group>             <year>1998</year>             <article-title>A computational model of how the basal ganglia produce sequences.</article-title>             <source>J Cogn Neurosci</source>             <volume>10</volume>             <fpage>108</fpage>             <lpage>121</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-ContrerasVidal1"><label>79</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Contreras-Vidal</surname><given-names>JL</given-names></name>
<name name-style="western"><surname>Schultz</surname><given-names>W</given-names></name>
</person-group>             <year>1999</year>             <article-title>A predictive reinforcement model of dopamine neurons for learning approach behavior.</article-title>             <source>J Comput Neurosci</source>             <volume>6</volume>             <fpage>191</fpage>             <lpage>214</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Brown1"><label>80</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Brown</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Bullock</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Grossberg</surname><given-names>S</given-names></name>
</person-group>             <year>1999</year>             <article-title>How the basal ganglia use parallel excitatory and inhibitory learning pathways to selectively respond to unexpected rewarding cues.</article-title>             <source>J Neurosci</source>             <volume>19</volume>             <fpage>10502</fpage>             <lpage>10511</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Matsumoto1"><label>81</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Matsumoto</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Hikosaka</surname><given-names>O</given-names></name>
</person-group>             <year>2007</year>             <article-title>Lateral habenula as a source of negative reward signals in dopamine neurons.</article-title>             <source>Nature</source>             <volume>447</volume>             <fpage>1111</fpage>             <lpage>1117</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Hopfield1"><label>82</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name>
</person-group>             <year>1982</year>             <article-title>Neural networks and physical systems with emergent collective computational abilities.</article-title>             <source>Proc Natl Acad Sci USA</source>             <volume>79</volume>             <fpage>2554</fpage>             <lpage>2558</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Jin1"><label>83</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Jin</surname><given-names>DZ</given-names></name>
</person-group>             <year>2009</year>             <article-title>Generating variable birdsong syllable sequences with branching chain networks in avian premotor nucleus HVC.</article-title>             <source>Phys Rev E</source>             <volume>80</volume>             <fpage>051902</fpage>          </element-citation></ref>
<ref id="pcbi.1001133-Hanuschkin1"><label>84</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Hanuschkin</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Herrmann</surname><given-names>JM</given-names></name>
<name name-style="western"><surname>Morrison</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Diesmann</surname><given-names>M</given-names></name>
</person-group>             <year>2010</year>             <article-title>Compositionality of arm movements can be realized by propagating synchrony.</article-title>             <source>J Comput Neurosci</source>             <comment>E-pub ahead of print. doi:10.1007/s10827-010-0285-9</comment>          </element-citation></ref>
<ref id="pcbi.1001133-Schrader1"><label>85</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Schrader</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Diesmann</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Morrison</surname><given-names>A</given-names></name>
</person-group>             <year>2010</year>             <article-title>A compositionality machine realized by a hierarchic architecture of synfire chains.</article-title>             <source>Front Comput Neurosci</source>             <volume>4</volume>             <fpage>154</fpage>          </element-citation></ref>
<ref id="pcbi.1001133-Seymour2"><label>86</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Seymour</surname><given-names>B</given-names></name>
<name name-style="western"><surname>O'Doherty</surname><given-names>J</given-names></name>
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Koltzenburg</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Jones</surname><given-names>A</given-names></name>
<etal/></person-group>             <year>2004</year>             <article-title>Temporal difference models describe higher-order learning in humans.</article-title>             <source>Nature</source>             <volume>429</volume>             <fpage>664</fpage>             <lpage>667</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Houk2"><label>87</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Houk</surname><given-names>JC</given-names></name>
<name name-style="western"><surname>Wise</surname><given-names>SP</given-names></name>
</person-group>             <year>1995</year>             <article-title>Distributed modular architectures linking basal ganglia, cerebellum, and cerebral cortex: Their role in planning and controlling action.</article-title>             <source>Cereb Cortex</source>             <volume>5</volume>             <fpage>95</fpage>             <lpage>110</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Houk3"><label>88</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Houk</surname><given-names>JC</given-names></name>
</person-group>             <year>2005</year>             <article-title>Agents of the mind.</article-title>             <source>Biol Cybern</source>             <volume>92</volume>             <fpage>427</fpage>             <lpage>437</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Houk4"><label>89</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Houk</surname><given-names>JC</given-names></name>
</person-group>             <year>2007</year>             <article-title>Models of basal ganglia.</article-title>             <source>Scholarpedia</source>             <volume>2</volume>             <fpage>1633</fpage>          </element-citation></ref>
<ref id="pcbi.1001133-Sethi1"><label>90</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sethi</surname><given-names>KD</given-names></name>
</person-group>             <year>2002</year>             <article-title>Clinical aspects of parkinson disease.</article-title>             <source>Curr Opin Neurol</source>             <volume>15</volume>             <fpage>457</fpage>             <lpage>460</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Knowlton1"><label>91</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Knowlton</surname><given-names>BJ</given-names></name>
<name name-style="western"><surname>Mangels</surname><given-names>JA</given-names></name>
<name name-style="western"><surname>Squire</surname><given-names>LR</given-names></name>
</person-group>             <year>1996</year>             <article-title>A neostriatal habit learning system in humans.</article-title>             <source>Science</source>             <volume>273</volume>             <fpage>1399</fpage>             <lpage>1420</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-McDonald1"><label>92</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>McDonald</surname><given-names>RJ</given-names></name>
<name name-style="western"><surname>White</surname><given-names>NM</given-names></name>
</person-group>             <year>1993</year>             <article-title>A triple dissociation of memory systems: hippocampus, amygdala, and dorsal striatum.</article-title>             <source>Behav Neurosci</source>             <volume>107</volume>             <fpage>3</fpage>             <lpage>22</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Sutton2"><label>93</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Sutton</surname><given-names>RS</given-names></name>
<name name-style="western"><surname>Barto</surname><given-names>AG</given-names></name>
</person-group>             <year>1990</year>             <article-title>Time-derivative models of pavlovian reinforcement.</article-title>             <person-group person-group-type="editor">
<name name-style="western"><surname>Gabriel</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Moore</surname><given-names>J</given-names></name>
</person-group>             <source>Learning and Computational Neuroscience</source>             <publisher-loc>Cambridge (Massachusetts)</publisher-loc>             <publisher-name>MIT Press</publisher-name>             <fpage>497</fpage>             <lpage>537</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Niv1"><label>94</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Niv</surname><given-names>Y</given-names></name>
<name name-style="western"><surname>Joel</surname><given-names>D</given-names></name>
<name name-style="western"><surname>Meilijson</surname><given-names>I</given-names></name>
<name name-style="western"><surname>Ruppin</surname><given-names>E</given-names></name>
</person-group>             <year>2002</year>             <article-title>Evolution of reinforcement learning in uncertain environments: A simple explanation for complex foraging behaviors.</article-title>             <source>Adapt Behav</source>             <volume>10</volume>             <fpage>5</fpage>             <lpage>24</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Doya1"><label>95</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Doya</surname><given-names>K</given-names></name>
</person-group>             <year>2000</year>             <article-title>Complementary roles of basal ganglia and cerebellum in learning and motor control.</article-title>             <source>Curr Opin Neurol</source>             <volume>10</volume>             <fpage>732</fpage>             <lpage>739</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-LaCamera1"><label>96</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>La Camera</surname><given-names>G</given-names></name>
<name name-style="western"><surname>Richmond</surname><given-names>BJ</given-names></name>
</person-group>             <year>2008</year>             <article-title>Modeling the violation of reward maximization and invariance in reinforcement schedules.</article-title>             <source>PLoS Comput Biol</source>             <volume>4</volume>             <fpage>e1000131</fpage>          </element-citation></ref>
<ref id="pcbi.1001133-Dayan3"><label>97</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>
</person-group>             <year>2009</year>             <article-title>Prospective and retrospective temporal difference learning.</article-title>             <source>Network Comput Neural Syst</source>             <volume>20</volume>             <fpage>32</fpage>             <lpage>46</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Matsumoto2"><label>98</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Matsumoto</surname><given-names>M</given-names></name>
<name name-style="western"><surname>Hikosaka</surname><given-names>O</given-names></name>
</person-group>             <year>2009</year>             <article-title>Two types of dopamine neuron distinctly convey positive and negative motivational signals.</article-title>             <source>Nature</source>             <volume>459</volume>             <fpage>837</fpage>             <lpage>842</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-AriasCarrion1"><label>99</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Arias-Carrion</surname><given-names>O</given-names></name>
<name name-style="western"><surname>Pöppel</surname><given-names>E</given-names></name>
</person-group>             <year>2007</year>             <article-title>Dopamine, learning, and reward-seeking behavior.</article-title>             <source>Acta Neurobiol Exp (Wars)</source>             <volume>67</volume>             <fpage>481</fpage>             <lpage>488</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Pecina1"><label>100</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Pecina</surname><given-names>S</given-names></name>
<name name-style="western"><surname>Cagniard</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Berridge</surname><given-names>KC</given-names></name>
<name name-style="western"><surname>Aldridge</surname><given-names>JW</given-names></name>
<name name-style="western"><surname>Zhuang</surname><given-names>X</given-names></name>
</person-group>             <year>2003</year>             <article-title>Hyperdopaminergic mutant mice have higher “wanting” but not “liking” for sweet rewards.</article-title>             <source>J Neurosci</source>             <volume>23</volume>             <fpage>9395</fpage>             <lpage>402</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Horvitz1"><label>101</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Horvitz</surname><given-names>JC</given-names></name>
</person-group>             <year>2000</year>             <article-title>Mesolimbocortical and nigrostriatal dopamine responses to salient non-reward events.</article-title>             <source>Neuroscience</source>             <volume>96</volume>             <fpage>651</fpage>             <lpage>656</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Redgrave2"><label>102</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Redgrave</surname><given-names>P</given-names></name>
<name name-style="western"><surname>Prescott</surname><given-names>TJ</given-names></name>
<name name-style="western"><surname>Gurney</surname><given-names>K</given-names></name>
</person-group>             <year>1999</year>             <article-title>Is the short-latency dopamine response too short to signal reward error?</article-title>             <source>Trends Neurosci</source>             <volume>22</volume>             <fpage>146</fpage>             <lpage>151</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Porr1"><label>103</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Porr</surname><given-names>B</given-names></name>
<name name-style="western"><surname>Wörgötter</surname><given-names>F</given-names></name>
</person-group>             <year>2007</year>             <article-title>Learning with relevance: Using a third factor to stabilise hebbian learning.</article-title>             <source>Neural Comput</source>             <volume>19</volume>             <fpage>2694</fpage>             <lpage>2719</lpage>          </element-citation></ref>
<ref id="pcbi.1001133-Gewaltig1"><label>104</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Gewaltig</surname><given-names>MO</given-names></name>
<name name-style="western"><surname>Diesmann</surname><given-names>M</given-names></name>
</person-group>             <year>2007</year>             <article-title>NEST (NEural Simulation Tool).</article-title>             <source>Scholarpedia</source>             <volume>2</volume>             <fpage>1430</fpage>          </element-citation></ref>
<ref id="pcbi.1001133-Potjans2"><label>105</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Potjans</surname><given-names>W</given-names></name>
<name name-style="western"><surname>Morrison</surname><given-names>A</given-names></name>
<name name-style="western"><surname>Diesmann</surname><given-names>M</given-names></name>
</person-group>             <year>2010</year>             <article-title>Enabling functional neural circuit simulations with distributed computing of neuromodulated plasticity.</article-title>             <source>Front Comput Neurosci</source>             <volume>4</volume>             <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2010.00141" xlink:type="simple">10.3389/fncom.2010.00141</ext-link></comment>          </element-citation></ref>
<ref id="pcbi.1001133-Tuckwell1"><label>106</label><element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Tuckwell</surname><given-names>HC</given-names></name>
</person-group>             <year>1988</year>             <source>Introduction to Theoretical Neurobiology, volume 1</source>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>Cambridge University Press</publisher-name>          </element-citation></ref>
<ref id="pcbi.1001133-Nordlie1"><label>107</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author">
<name name-style="western"><surname>Nordlie</surname><given-names>E</given-names></name>
<name name-style="western"><surname>Gewaltig</surname><given-names>MO</given-names></name>
<name name-style="western"><surname>Plesser</surname><given-names>HE</given-names></name>
</person-group>             <year>2009</year>             <article-title>Towards reproducible descriptions of neuronal network models.</article-title>             <source>PLoS Comput Biol</source>             <volume>5</volume>             <fpage>e1000456</fpage>          </element-citation></ref>
</ref-list>

</back>
</article>