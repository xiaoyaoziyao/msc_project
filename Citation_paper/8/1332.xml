<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-00902</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003219</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories>
<title-group>
<article-title>From Birdsong to Human Speech Recognition: Bayesian Inference on a Hierarchy of Nonlinear Dynamical Systems</article-title>
<alt-title alt-title-type="running-head">From Birdsong to Human Speech Recognition</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Yildiz</surname><given-names>Izzet B.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>von Kriegstein</surname><given-names>Katharina</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Kiebel</surname><given-names>Stefan J.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Group for Neural Theory, Institute of Cognitive Studies, École Normale Supérieure, Paris, France</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Humboldt University of Berlin, Department of Psychology, Berlin, Germany</addr-line></aff>
<aff id="aff4"><label>4</label><addr-line>Biomagnetic Center, Hans Berger Clinic for Neurology, University Hospital Jena, Jena, Germany</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Jirsa</surname><given-names>Viktor K.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Institut de Neurosciences des Systèmes, France</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">yildiz-izzet.burak@ens.fr</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: IBY SJK. Performed the experiments: IBY. Analyzed the data: IBY KvK SJK. Contributed reagents/materials/analysis tools: IBY SJK. Wrote the paper: IBY KvK SJK.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>9</month><year>2013</year></pub-date>
<pub-date pub-type="epub"><day>12</day><month>9</month><year>2013</year></pub-date>
<volume>9</volume>
<issue>9</issue>
<elocation-id>e1003219</elocation-id>
<history>
<date date-type="received"><day>23</day><month>5</month><year>2013</year></date>
<date date-type="accepted"><day>27</day><month>7</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2013</copyright-year>
<copyright-holder>Yildiz et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Our knowledge about the computational mechanisms underlying human learning and recognition of sound sequences, especially speech, is still very limited. One difficulty in deciphering the exact means by which humans recognize speech is that there are scarce experimental findings at a neuronal, microscopic level. Here, we show that our neuronal-computational understanding of speech learning and recognition may be vastly improved by looking at an animal model, i.e., the songbird, which faces the same challenge as humans: to learn and decode complex auditory input, in an online fashion. Motivated by striking similarities between the human and songbird neural recognition systems at the macroscopic level, we assumed that the human brain uses the same computational principles at a microscopic level and translated a birdsong model into a novel human sound learning and recognition model with an emphasis on speech. We show that the resulting Bayesian model with a hierarchy of nonlinear dynamical systems can learn speech samples such as words rapidly and recognize them robustly, even in adverse conditions. In addition, we show that recognition can be performed even when words are spoken by different speakers and with different accents—an everyday situation in which current state-of-the-art speech recognition models often fail. The model can also be used to qualitatively explain behavioral data on human speech learning and derive predictions for future experiments.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>Neuroscience still lacks a concrete explanation of how humans recognize speech. Even though neuroimaging techniques are helpful in determining the brain areas involved in speech recognition, there are rarely mechanistic explanations at a neuronal level. Here, we assume that songbirds and humans solve a very similar task: extracting information from sound wave modulations produced by a singing bird or a speaking human. Given strong evidence that both humans and songbirds, although genetically very distant, converged to a similar solution, we combined the vast amount of neurobiological findings for songbirds with nonlinear dynamical systems theory to develop a hierarchical, Bayesian model which explains fundamental functions in recognition of sound sequences. We found that the resulting model is good at learning and recognizing human speech. We suggest that this translated model can be used to qualitatively explain or predict experimental data, and the underlying mechanism can be used to construct improved automatic speech recognition algorithms.</p>
</abstract>
<funding-group><funding-statement>Funding was provided by Max Planck Institute for Human Cognitive and Brain Sciences (<ext-link ext-link-type="uri" xlink:href="http://www.cbs.mpg.de" xlink:type="simple">www.cbs.mpg.de</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="16"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Can we learn something about how humans recognize speech from how birds recognize song? The last common ancestor of humans and birds lived about 300 million years ago, nevertheless human and songbird communication share several striking features at the cognitive, neuronal and molecular level <xref ref-type="bibr" rid="pcbi.1003219-Bolhuis1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Doupe1">[2]</xref>. When we recognize speech, our brains map fast speech sound wave modulations to spectrotemporal auditory representations <xref ref-type="bibr" rid="pcbi.1003219-Creutzfeldt1">[3]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Pasley1">[4]</xref>. Similarly, songbirds map song sound wave modulations to specific internal representations <xref ref-type="bibr" rid="pcbi.1003219-Berwick1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Prather1">[6]</xref>. In addition, similar to humans, songbirds gain their vocal abilities early in life by listening to adults, and memorizing and practicing their songs <xref ref-type="bibr" rid="pcbi.1003219-Doupe1">[2]</xref>. The similarities include anatomical and functional features that characterize the pathways for vocal production, auditory processing and learning <xref ref-type="bibr" rid="pcbi.1003219-Bolhuis1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Doupe1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Jarvis1">[7]</xref>. For example, the auditory system in both humans and songbirds is organized hierarchically <xref ref-type="bibr" rid="pcbi.1003219-Okada1">[8]</xref>–<xref ref-type="bibr" rid="pcbi.1003219-Theunissen1">[10]</xref> where fast time scales are represented by lower levels and slow time scales by levels higher up in the hierarchy <xref ref-type="bibr" rid="pcbi.1003219-DeWitt1">[11]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Lewicki1">[12]</xref>. Much more is known experimentally about the exact neuronal mechanisms in songbirds than in humans, due to detailed electrophysiological studies which have shown that songbirds use a sequence of auditory dynamics to generate and recognize song in a highly effective manner <xref ref-type="bibr" rid="pcbi.1003219-Prather1">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Fee1">[13]</xref>. These detailed findings in songbirds enabled us to derive a neurobiologically plausible, computational model of how songbirds recognize the songs of their conspecifics <xref ref-type="bibr" rid="pcbi.1003219-Yildiz1">[14]</xref>. Our aim in the present paper is to attempt to translate this birdsong model to human speech by assuming that humans and birds use similar internal models for recognizing sounds. Such a translation would provide a unique opportunity to derive a mechanistic understanding and make predictions at both the microscopic and macroscopic neuronal level for the human speech learning and recognition system.</p>
<p>The birdsong model described in <xref ref-type="bibr" rid="pcbi.1003219-Yildiz1">[14]</xref> performs a Bayesian version of dynamical, predictive coding based on an internal generative model of how birdsong is produced <xref ref-type="bibr" rid="pcbi.1003219-Friston1">[15]</xref>. The core of this generative model consists of a two-level hierarchy of nonlinear dynamical systems and is the proposed mechanistic basis of how songbirds extract online information from an ongoing song. We translated this birdsong model to human sound recognition by replacing songbird related parts with human-specific parts (<xref ref-type="fig" rid="pcbi-1003219-g001">Figure 1</xref>). This included processing the input with a human cochlea model, which maps sound waves to neuronal activity. The resulting model is able to learn and recognize any sequence of sounds such as speech or music. Here, we focus on the application of the model on speech learning and recognition. The contribution of this article is threefold: First, inspired by songbird circuitry, it proposes a mechanistic hypothesis about how humans recognize speech using nonlinear dynamical systems. Secondly, if the resulting speech recognition system shows good performance, even under adverse conditions, it may be used to optimize automatic speech recognition. Thirdly, the neurobiological plausibility of the model would allow it to be used to derive predictions for neurobiological experiments.</p>
<fig id="pcbi-1003219-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003219.g001</object-id><label>Figure 1</label><caption>
<title>Summary of the hierarchical model of speech learning and recognition.</title>
<p>The core of the model is equivalent to the core of the birdsong model <xref ref-type="bibr" rid="pcbi.1003219-Yildiz1">[14]</xref>. The <xref ref-type="disp-formula" rid="pcbi.1003219.e009">Equations 1</xref> and <xref ref-type="disp-formula" rid="pcbi.1003219.e043">2</xref> on the right side generate the dynamics shown on the left side, and are described in the <xref ref-type="sec" rid="s2">Model</xref> section (see also <xref ref-type="table" rid="pcbi-1003219-t001">Table 1</xref> for the meaning of parameters). Speech sounds, i.e., sound waves, enter the model through the cochlear level. The output is a cochleagram (shown for the speech stimulus “zero”), which is a type of frequency-time diagram. There are 86 channels, which represent the firing rate (warm colors for high firing rate and cold colors for low firing rate) of the neuronal ensembles that encode lower frequencies as the channel number increases. We decrease the dimension of this input to six dimensions by averaging every 14 channels (see the color coding to the right of the cochleagram and also see <xref ref-type="sec" rid="s2">Model</xref>). After this cochlear processing, activity is fed forward into the two-level hierarchical model. This input is encoded by the activity of the first level network (shown with the same color coding on the right), which is in turn encoded by activity at the second level (no color coding at this level, different colors represent different neuronal ensembles). From the generative model shown here (core model), we derived a recognition model (for mathematical details see <xref ref-type="sec" rid="s2">Model</xref>).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003219.g001" position="float" xlink:type="simple"/></fig></sec><sec id="s2" sec-type="methods">
<title>Model</title>
<p>Here, we first describe the model conceptually, followed by mathematical details of the generative model, cochlear model, online Bayesian recognition and further details of the simulations described in <xref ref-type="sec" rid="s3">Results</xref>.</p>
<sec id="s2a">
<title>Conceptual overview: A generative model of human speech</title>
<p>As a model, we employ a novel Bayesian recognition method of dynamical sensory input such as birdsong and speech. The Bayesian approach first requires building of a so-called generative (internal) model, which is then converted to a learning and recognition model. The key advantage of this approach, as opposed to standard models in both human speech recognition and automatic speech recognition, is that the generative model is formulated as hierarchically structured, nonlinear dynamical systems. This means that one can employ generative models specifically tailored to birdsong or speech recognition. As we show in the following, this feature is crucial for translating experimental birdsong results to a concrete recognition model. This translation would not be possible with generic models such as are standard and widely used in automatic speech recognition, e.g. the hidden Markov model and, very recently, deep belief networks and liquid state machines <xref ref-type="bibr" rid="pcbi.1003219-Bilmes1">[16]</xref>–<xref ref-type="bibr" rid="pcbi.1003219-Verstraeten1">[18]</xref>. Our model has also several differences from the influential models such as TRACE <xref ref-type="bibr" rid="pcbi.1003219-Mcclelland1">[19]</xref> and Shortlist <xref ref-type="bibr" rid="pcbi.1003219-Norris1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Norris2">[21]</xref> and we provide a more detailed comparison in the <xref ref-type="sec" rid="s4">Discussion</xref>.</p>
<p>In the birdsong model, we used experimental insights about the firing patterns of the premotor area HVC (formerly known as the high vocal center) and the nucleus RA (robust nucleus of the arcopallium) to derive a hierarchical song generation model <xref ref-type="bibr" rid="pcbi.1003219-Yildiz1">[14]</xref>. In the high level structure HVC, specific neurons called HVC<sub>(RA)</sub>, fire sequentially at temporally precise moments <xref ref-type="bibr" rid="pcbi.1003219-Fee1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Hahnloser1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Yu1">[23]</xref> where each neuron of this sequence fires only once during the song to provide input to a group of RA neurons.</p>
<p>We translated these two levels to the human speech model in the present study (<xref ref-type="fig" rid="pcbi-1003219-g001">Figure 1</xref>). The second, higher level encodes a recurrent neural network producing a sequential activation of neurons in a winner-less competition setting (stable heteroclinic channels <xref ref-type="bibr" rid="pcbi.1003219-Rabinovich1">[24]</xref>, see below). These dynamic sequences control dynamics at a first, lower level (Hopfield attractor, see below), where we model amplitude variations in specific frequency bands. In comparison to the birdsong model, the generative model here does not explicitly model the vocal tract dynamics but rather the dynamics at the cochlea which would be elicited by the stimulus. Therefore, the second level dynamics act as a timing mechanism providing the temporal information and the first level dynamics represent the spectral content at different frequency bands. Such a separation of temporal and spectral processing is also suggested for the human auditory system <xref ref-type="bibr" rid="pcbi.1003219-Barton1">[25]</xref>. We do not restrict the functionality of the second level ensembles to specific phonemes or syllables but rather use them as time markers for the represented spectrotemporal stimulus (mostly words in this paper). By using this generative model (<xref ref-type="fig" rid="pcbi-1003219-g001">Figure 1</xref>), we can apply Bayesian inference to derive a mechanism, which can learn and recognize a single word. We call this mechanism for the remainder of this paper a <italic>module</italic>. Here, a module is essentially a sophisticated template matcher where the template is learned and stored in a hierarchically structured recurrent neural network and compared against a stimulus in an online fashion. Individual modules can be combined into an <italic>agent</italic> to achieve classification tasks as shown in the “Word Recognition Task” below, see <xref ref-type="fig" rid="pcbi-1003219-g002">Figure 2A</xref> for an overview. A crucial parameter in the model is called precision which is the inverse of the variance of an internal state. This is used in the model as a way to balance the (top-down) prior information and (bottom-up) sensory evidence. In the simulations, we show that the precision settings are crucial to learn new stimuli or to recognize sounds in noisy environments. We further discuss the biological plausibility of the resulting recognition model in the <xref ref-type="sec" rid="s4">Discussion</xref>.</p>
<fig id="pcbi-1003219-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003219.g002</object-id><label>Figure 2</label><caption>
<title>Schematic structure of an agent and a module.</title>
<p><bold>A</bold>) An agent consists of several modules, where each module contains an instance of the model shown in <xref ref-type="fig" rid="pcbi-1003219-g001">Figure 1</xref> and has learned to recognize a single word. Sensory input is recognized by all modules concurrently and each module experiences prediction error during recognition. A module can be considered as a sophisticated dynamic, Bayes-optimal template matcher which produces less prediction error if the stimulus matches better to the module's learned word. A minimum operator performs classification by selecting the module with the least amount of prediction error during recognition. <bold>B</bold>) At each level in a module, causal and hidden states (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e002" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e003" xlink:type="simple"/></inline-formula>, respectively) try to minimize the precision-weighted prediction errors (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e004" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e005" xlink:type="simple"/></inline-formula>) by exchanging messages. Predictions are transferred from second level to the first and prediction error is propagated back from the first to the second level (see section <xref ref-type="sec" rid="s2">Model</xref>: Learning and Recognition for more details). Adapted from <xref ref-type="bibr" rid="pcbi.1003219-Friston6">[110]</xref>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003219.g002" position="float" xlink:type="simple"/></fig></sec><sec id="s2b">
<title>Mathematical details: A generative model of human speech</title>
<sec id="s2b1">
<title>Second level: Sequential dynamics</title>
<p>One of the well-established ways for modeling the sequential activation of neuronal ensembles is the Lotka-Volterra type dynamics <xref ref-type="bibr" rid="pcbi.1003219-Afraimovich1">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Rabinovich2">[27]</xref>, which is well known in population biology. Rabinovich et al. applied this idea to neuronal dynamics under the name of winnerless competition <xref ref-type="bibr" rid="pcbi.1003219-Rabinovich1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Rabinovich2">[27]</xref>–<xref ref-type="bibr" rid="pcbi.1003219-Rabinovich3">[30]</xref>. In the winner-less competition setting, there are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e006" xlink:type="simple"/></inline-formula> equilibrium points, i.e., neuronal ensembles, which are saddles of a nonlinear dynamical system. Each of these equilibrium points has a single unstable direction that connects them to the next equilibrium point while remaining directions are stable forming a so-called stable heteroclinic channel. In the phase space, this looks like beads on a string, which attracts nearby orbits. Therefore, a typical solution of such system with a heteroclinic contour travels through all saddle points, i.e., neuronal ensembles, in a circular fashion thereby activating each ensemble for a brief period until it is deactivated as the next ensemble becomes active.</p>
<p>These dynamics can be obtained from a neural mass model of mean membrane potential and action firing potential <xref ref-type="bibr" rid="pcbi.1003219-Fukai1">[31]</xref>, reviewed in <xref ref-type="bibr" rid="pcbi.1003219-Rabinovich1">[24]</xref>. We use the following equations (see <xref ref-type="table" rid="pcbi-1003219-t001">Table 1</xref> for the constants used):<disp-formula id="pcbi.1003219.e007"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003219.e007" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003219.e008"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003219.e008" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003219.e009"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003219.e009" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e010" xlink:type="simple"/></inline-formula> are the <italic>hidden-state</italic> vectors (e.g., mean membrane potentials) at the second level, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e011" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e012" xlink:type="simple"/></inline-formula> are scalars, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e013" xlink:type="simple"/></inline-formula> is the sigmoid function applied component-wise and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e014" xlink:type="simple"/></inline-formula> is the connectivity matrix with entries <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e015" xlink:type="simple"/></inline-formula> giving the strength of inhibition from state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e016" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e017" xlink:type="simple"/></inline-formula>. While the first set of hidden states, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e018" xlink:type="simple"/></inline-formula>, describes the heteroclinic channel, the second set of hidden states, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e019" xlink:type="simple"/></inline-formula>, acts as smooth normalizing dynamics for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e020" xlink:type="simple"/></inline-formula> by limiting their dynamics to the interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e021" xlink:type="simple"/></inline-formula>. The states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e022" xlink:type="simple"/></inline-formula> are called <italic>causal states</italic> and are used to transmit the output of the second level to the first level where this transformation is taken as identity here. We also add normally distributed noise vectors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e023" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e024" xlink:type="simple"/></inline-formula> to render the model stochastic. Note that we use exponential functions in the dynamics of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e025" xlink:type="simple"/></inline-formula> to decrease the overlaps between the dynamics of two sequentially activated neurons. A simpler normalization function such as the logistic function would give mostly overlapping activations which would be problematic during recognition. Therefore, each neuron can be considered to be highly sensitive to other neurons' firing rates since even a slight activation of one neuron quickly suppresses (due to exponential function) the activation of all other neurons in the network. With an appropriately chosen connectivity matrix, one can obtain a system with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e026" xlink:type="simple"/></inline-formula> saddle points, each representing a neuronal ensemble, forming a stable heteroclinic channel <xref ref-type="bibr" rid="pcbi.1003219-Afraimovich1">[26]</xref>. For the entries of the connectivity matrix, one chooses high inhibition from the previously active neuron to the currently active neuron and low inhibition from the currently active neuron to the next neuron that will become active:<disp-formula id="pcbi.1003219.e027"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003219.e027" xlink:type="simple"/></disp-formula>(Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e028" xlink:type="simple"/></inline-formula> when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e029" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e030" xlink:type="simple"/></inline-formula> when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e031" xlink:type="simple"/></inline-formula>).</p>
<table-wrap id="pcbi-1003219-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003219.t001</object-id><label>Table 1</label><caption>
<title>Variables used in the generative model.</title>
</caption><alternatives><graphic id="pcbi-1003219-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003219.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Symbol</td>
<td align="left" rowspan="1" colspan="1">Meaning</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>x</italic><sup>(<italic>i</italic>)</sup>, <italic>y</italic><sup>(<italic>i</italic>)</sup>, <italic>v</italic><sup>(<italic>i</italic>)</sup></td>
<td align="left" rowspan="1" colspan="1">Hidden states, <italic>x</italic><sup>(<italic>i</italic>)</sup>, <italic>y</italic><sup>(<italic>i</italic>)</sup> and causal states, <italic>v</italic><sup>(<italic>i</italic>)</sup></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e001" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Normally distributed noise at the <italic>i</italic>th level</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>κ</italic><sub>1</sub>, <italic>κ</italic><sub>2</sub></td>
<td align="left" rowspan="1" colspan="1">Rate constants: <italic>κ</italic><sub>1</sub> = 2, <italic>κ</italic><sub>2</sub> = 1</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>λ</italic></td>
<td align="left" rowspan="1" colspan="1">Decay rate: 1/8</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>ρ</italic></td>
<td align="left" rowspan="1" colspan="1">Connectivity matrix of the second level</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>A</italic></td>
<td align="left" rowspan="1" colspan="1">Diagonal matrix with diagonal <italic>a</italic> = 0.2</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>W</italic></td>
<td align="left" rowspan="1" colspan="1">Connectivity matrix of the first level</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>I</italic></td>
<td align="left" rowspan="1" colspan="1">Direct input from the second level to the first level</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>N</italic>, <italic>n</italic></td>
<td align="left" rowspan="1" colspan="1">Number of ensembles (<italic>N</italic> = 8) and (<italic>n</italic> = 6)</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt101"><label/><p>Note: This table lists the variables used in the generative, hierarchical model (see <xref ref-type="disp-formula" rid="pcbi.1003219.e009">Equations 1</xref> and <xref ref-type="disp-formula" rid="pcbi.1003219.e043">2</xref> in <xref ref-type="fig" rid="pcbi-1003219-g001">Figure 1</xref> and <xref ref-type="sec" rid="s2">Model</xref>).</p></fn></table-wrap-foot></table-wrap>
<p>In the majority of the simulations below, we used <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e032" xlink:type="simple"/></inline-formula> neuronal ensembles at the second level; longer sequences can be used as well, e.g., see the Recognition in a Noisy Environment simulation below and also <xref ref-type="bibr" rid="pcbi.1003219-Yildiz1">[14]</xref>. Each second level ensemble, during its activation, sends a signal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e033" xlink:type="simple"/></inline-formula> to the first level (see next section) designed to control the activation of the neuronal ensembles. The total signal sent to the first level by all second level ensembles at any time is a linear combination of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e034" xlink:type="simple"/></inline-formula>'s: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e035" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e036" xlink:type="simple"/></inline-formula> is the output vector in <xref ref-type="disp-formula" rid="pcbi.1003219.e009">Eqn. 1</xref>. Note that, except during the transitions, only one entry of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e037" xlink:type="simple"/></inline-formula> is close to one, all others are close to zero, which specifies the currently active population and therefore the dominating <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e038" xlink:type="simple"/></inline-formula>. These <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e039" xlink:type="simple"/></inline-formula>'s are crucial for the model and the learning phase throughout the simulations above consists of reconstruction of these vectors.</p>
</sec><sec id="s2b2">
<title>First level: Spectro-temporal dynamics</title>
<p>We represent a collapsed form of lower level human auditory processing at the first level of our model. Each neuronal ensemble of the first level network represents spectral features of the cochleagram (see next section). The cochleagram consists of the firing rates of simulated auditory nerves, which are sensitive to specific frequency ranges. We encode these firing rates by the activity at the first level. When the neural network at the first level receives specific input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e040" xlink:type="simple"/></inline-formula> from the second level, the activity of the network is attracted to a global attractor encoding a specific spectral pattern in the cochleagram. As the input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e041" xlink:type="simple"/></inline-formula> from the second level changes in a continuous, sequential fashion, this global attractor also changes continuously and neural activity of each ensemble encodes the cochleagram over time.</p>
<p>Here, we use a Hopfield network <xref ref-type="bibr" rid="pcbi.1003219-Hopfield1">[32]</xref> to implement such dynamics. Hopfield network dynamics consist of stable equilibrium points that attract nearby orbits. Therefore, the itinerary of an arbitrary initial point evolves to one of these equilibrium points. Hopfield networks have been proposed to model associative memory, where each stable equilibrium point represents a memory item and an orbit attracted to such an equilibrium point represents a retrieved memory. In our model, at any given time, there is only one equilibrium point and this point changes depending on the sequential second level dynamics. We use the following equations for the first level of the generative model:<disp-formula id="pcbi.1003219.e042"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003219.e042" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003219.e043"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003219.e043" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e044" xlink:type="simple"/></inline-formula> are hidden and causal states, respectively; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e045" xlink:type="simple"/></inline-formula> with scalar <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e046" xlink:type="simple"/></inline-formula>, is a self-connectivity matrix, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e047" xlink:type="simple"/></inline-formula> is an asymmetric synaptic connectivity matrix with entries <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e048" xlink:type="simple"/></inline-formula> denoting the direction-specific connection strength from ensemble <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e049" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e050" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e051" xlink:type="simple"/></inline-formula> is a sigmoid function which we take as the <italic>tanh</italic> function applied component-wise, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e052" xlink:type="simple"/></inline-formula> is the direct input from the second level, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e053" xlink:type="simple"/></inline-formula> is a scalar and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e054" xlink:type="simple"/></inline-formula> are normally distributed noise vectors. As previously described in <xref ref-type="bibr" rid="pcbi.1003219-Yildiz1">[14]</xref>, under mild assumptions on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e055" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003219-Zheng1">[33]</xref>, one can choose the input vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e056" xlink:type="simple"/></inline-formula> appropriately to create a global attractor with desired firing rate values. In the simulations, we show that it is also possible to learn the proper <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e057" xlink:type="simple"/></inline-formula> vectors from the speech stimulus using Bayesian techniques. Here, we use <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e058" xlink:type="simple"/></inline-formula> neuronal ensembles (see “Extensions and Limitations of the <xref ref-type="sec" rid="s2">Model</xref>” in <xref ref-type="sec" rid="s4">Discussion</xref>), which represent the reduced spectral output of the cochlear model. As a result, we obtain the necessary spectrotemporal dynamics where the sequential dynamics are provided by the second level and the mapping to the spectrum is encoded by the first level. A detailed explanation for the cochlear model is provided next.</p>
</sec><sec id="s2b3">
<title>Cochlear model: From sound wave to firing rates</title>
<p>The cochlea is a spiral-shaped peripheral organ of hearing in the inner ear which is a key component of the auditory system for translating acoustic waves into neural signals (see <xref ref-type="bibr" rid="pcbi.1003219-Robles1">[34]</xref> for a review). Hearing starts with the travelling of sound waves through the ear canal and transmission of the resulting vibrations to the cochlea. The frequency specific representation of sounds comes partially from the differential stiffness of the basilar membrane, the elastic structure that extends through the cochlea. The base of the basilar membrane responds to higher frequencies and the other end, the apex, responds to lower frequencies.</p>
<p>Extensive research has been carried out to model the mechanism of the cochlea which is based on the fluid and the basilar membrane dynamics. Here, we use a classical model by R.F. Lyon <xref ref-type="bibr" rid="pcbi.1003219-Lyon1">[35]</xref> because it is simple and sufficient for our purposes, however note that more involved models exist in the literature (e.g. <xref ref-type="bibr" rid="pcbi.1003219-Vanimmerseel1">[36]</xref>–<xref ref-type="bibr" rid="pcbi.1003219-Sumner1">[40]</xref>). The output of the model, the cochleagram, is a time-frequency representation with values between zero and one which represent the firing rate of the corresponding auditory nerves (channels) at each time point.</p>
<p>The number of channels in the model depends on the sampling rate of the original signal and the frequency overlap between filters. For the results in this paper, we used the LyonPassiveEar function of the Auditory Toolbox <xref ref-type="bibr" rid="pcbi.1003219-Slaney1">[41]</xref> with default parameters that gives us 86 channels where these channels are ordered from higher to lower frequencies, i.e. the 1<sup>st</sup> channel represents the highest frequency (∼8 kHz) and the 86<sup>th</sup> channel represents the lowest frequency (∼0 kHz). Bayesian inference of 86 channels is computationally too expensive and therefore, we decrease the number of channels to six by averaging 14 channels at every time point (from channel 1 up to 84 = 14×6) and remove the last two channels which usually do not carry any significant signal. This gives us six neuronal ensembles' firing rate dynamics. As shown in <xref ref-type="sec" rid="s3">Results</xref>, these six channels are sufficient to give good discrimination results between several speech stimuli. The time duration of these channels depends on the length of the stimulus and the decimation factor. Except stated otherwise, we scaled the duration of these six signals to 100 time units which allowed us to use the same number of second level ensembles for each stimulus. However note that in all figures, we used the original length of the corresponding stimuli in milliseconds along the <italic>x</italic>-axes for clarity.</p>
</sec></sec><sec id="s2c">
<title>Learning and recognition</title>
<p>For a given speech stimulus <italic>z</italic> (preprocessed by the cochlear model) and a model <italic>m</italic>, the <italic>model evidence</italic> or <italic>marginal likelihood of z</italic> is defined by the conditional probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e059" xlink:type="simple"/></inline-formula> where the model <italic>m</italic> consists of all differential equations described before and priors for model parameters. The task for the module is to infer the corresponding causal states <italic>v</italic> and hidden states <italic>x</italic> at all levels as well as the parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e060" xlink:type="simple"/></inline-formula>, i.e. the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e061" xlink:type="simple"/></inline-formula>'s that connect the levels, which we all together denote by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e062" xlink:type="simple"/></inline-formula>. Therefore the goal is to estimate the <italic>posterior density</italic>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e063" xlink:type="simple"/></inline-formula>, which describes the mean distribution of the variables as well as the uncertainty about them. We approximate the posterior in an indirect way:</p>
<p>The marginal likelihood of <italic>z</italic> is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e064" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e065" xlink:type="simple"/></inline-formula> is defined in terms of the likelihood <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e066" xlink:type="simple"/></inline-formula> and the prior <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e067" xlink:type="simple"/></inline-formula>. We approximate this intractable integral by introducing a <italic>free-energy</italic> term which is a lower bound for the marginal likelihood. It is straightforward to show that:<disp-formula id="pcbi.1003219.e068"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003219.e068" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e069" xlink:type="simple"/></inline-formula> is the free-energy, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e070" xlink:type="simple"/></inline-formula> is the Kullback-Leibler divergence and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e071" xlink:type="simple"/></inline-formula> is the <italic>recognition density</italic>. Note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e072" xlink:type="simple"/></inline-formula> is an auxiliary function that we will use to approximate the posterior density. The divergence term <italic>D</italic> is nonnegative, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e073" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e074" xlink:type="simple"/></inline-formula> if and only if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e075" xlink:type="simple"/></inline-formula>. This means <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e076" xlink:type="simple"/></inline-formula> is a lower bound for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e077" xlink:type="simple"/></inline-formula>, and if we can maximize <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e078" xlink:type="simple"/></inline-formula>, this will minimize <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e079" xlink:type="simple"/></inline-formula> providing an approximation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e080" xlink:type="simple"/></inline-formula> for the posterior density.</p>
<p>To find <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e081" xlink:type="simple"/></inline-formula> that maximizes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e082" xlink:type="simple"/></inline-formula>, we make a Gaussian assumption about the form of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e083" xlink:type="simple"/></inline-formula>, the so called Laplace approximation. Therefore we take <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e084" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e085" xlink:type="simple"/></inline-formula> consists of the mode <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e086" xlink:type="simple"/></inline-formula> and the variance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e087" xlink:type="simple"/></inline-formula>. Now, the question turns into a maximization problem of the free energy with respect to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e088" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003219.e089"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003219.e089" xlink:type="simple"/></disp-formula>which gives the approximation for the posterior density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e090" xlink:type="simple"/></inline-formula>. Note that the above maximization process is a simplified description and is only suitable for the time-independent <italic>u</italic> parameters (static case). When time-dependent states are involved, i.e. causal and hidden states, one needs to replace the free energy with <italic>free action</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e091" xlink:type="simple"/></inline-formula> which is the anti-derivative of free energy in time, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e092" xlink:type="simple"/></inline-formula>. In this case, one aims at minimizing free action under the Laplace assumption. We note that time-dependent and independent variables can be handled concurrently and we refer the reader to <xref ref-type="bibr" rid="pcbi.1003219-Friston2">[42]</xref> for details.</p>
<p>For all simulations in this paper, we used fixed prior variances for all states and parameters. The variances for the corresponding simulations are usually described in terms of the <italic>precision</italic>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e093" xlink:type="simple"/></inline-formula>, which is defined as the inverse of the variance, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e094" xlink:type="simple"/></inline-formula>. Therefore, a high prior precision for an internal state means that the dynamics are not allowed to deviate much from expectations provided by the generative model (top-down influence) whereas a low prior precision means the dynamics is relatively susceptible to (bottom-up) influences (wider standard deviation). Throughout the <xref ref-type="sec" rid="s3">Results</xref> section, we report the log-precision values; the corresponding standard deviations can be computed by the formula: standard deviation = exp(−log precision/2).</p>
<p>The above maximization process can also be formulated in a hierarchical setting. Let us denote <italic>all</italic> hidden and causal states at level <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e095" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e096" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e097" xlink:type="simple"/></inline-formula>, respectively. We also write <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e098" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e099" xlink:type="simple"/></inline-formula> to describe the dynamics of the hidden and causal states at the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e100" xlink:type="simple"/></inline-formula> th level (see <xref ref-type="disp-formula" rid="pcbi.1003219.e009">Eqns. 1</xref> and <xref ref-type="disp-formula" rid="pcbi.1003219.e043">2</xref>): <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e101" xlink:type="simple"/></inline-formula><disp-formula id="pcbi.1003219.e102"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003219.e102" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003219.e103"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003219.e103" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003219.e104"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003219.e104" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e105" xlink:type="simple"/></inline-formula> denotes the normally distributed fluctuations at the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e106" xlink:type="simple"/></inline-formula> th level. Note that the second level causal states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e107" xlink:type="simple"/></inline-formula> provide input to the first level while the hidden states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e108" xlink:type="simple"/></inline-formula> are intrinsic to each level. The preprocessed speech stimulus enters the system through the first level: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e109" xlink:type="simple"/></inline-formula>. The optimization process described above, i.e. finding the optimum mode and variance for states and parameters, can be implemented in a message passing scheme <xref ref-type="bibr" rid="pcbi.1003219-Friston2">[42]</xref> where the optimization problem turns into a gradient descent on precision-weighted prediction errors (see also <xref ref-type="fig" rid="pcbi-1003219-g002">Figure 2B</xref>):<disp-formula id="pcbi.1003219.e110"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003219.e110" xlink:type="simple"/></disp-formula><disp-formula id="pcbi.1003219.e111"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003219.e111" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e112" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e113" xlink:type="simple"/></inline-formula> are causal and hidden prediction errors at the <italic>i</italic> th level, weighted by the causal and hidden precisions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e114" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e115" xlink:type="simple"/></inline-formula> respectively; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e116" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e117" xlink:type="simple"/></inline-formula> denote the internal predictions of the corresponding level for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e118" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e119" xlink:type="simple"/></inline-formula>, respectively. Internal predictions set the states to the right trajectory for future input. Therefore, it can be seen that as prediction error is minimized, internal predictions fit better to the external input. Intuitively, high precision for a variable means the prediction error is amplified and therefore only small errors are tolerated whereas low precision means large errors are tolerated and therefore the approximation to the states is <italic>less precise</italic>.</p>
<sec id="s2c1">
<title>Neuronal network implementation</title>
<p>Finally, the Bayesian inference described above can be implemented in a neurobiologically plausible fashion using two types of neuronal ensembles. The modes of the expected causal and hidden states, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e120" xlink:type="simple"/></inline-formula>, can be represented by the neural activity of <italic>state</italic> ensembles, while prediction error is encoded by the activity of <italic>error</italic> ensembles, with one matching error ensemble for each state ensemble. State and error ensembles interact within and between levels. The messages sent from second to first level state ensembles encode the expectations of the second level on the dynamics of the first level whereas error units at each level compare these expectations to the ongoing activity of state ensembles and compute prediction errors, which are passed on via forward and lateral connections. These error units can be identified with superficial pyramidal cells as they originate forward connections in the brain which correspond to the bottom-up error messages in our setting <xref ref-type="bibr" rid="pcbi.1003219-Friston3">[43]</xref>. The sources of backward connections can be identified with deep pyramidal cells which encode top-down expectations of the state units. This message passing scheme efficiently minimizes prediction errors and optimizes predictions at all levels (for more details, see <xref ref-type="bibr" rid="pcbi.1003219-Friston3">[43]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Mumford1">[44]</xref>).</p>
<p><italic>Software note:</italic> The routines (including commented Matlab source code) implementing this dynamic inference scheme, which were also used for the simulations in this paper, are available as academic freeware (Statistical Parametric Mapping package (SPM8) from <ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm/" xlink:type="simple">http://www.fil.ion.ucl.ac.uk/spm/</ext-link>; Dynamic Expectation Maximization (DEM) Toolbox).</p>
</sec></sec></sec><sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>A Bayesian model for learning and online recognition of human speech</title>
<p>In each module (see <xref ref-type="sec" rid="s2">Model</xref>), learning and recognition of speech are simultaneous processes of adapting internal connections and inferring the speech message dynamics of the speaker. As in the brain, learning changes <italic>parameters</italic>, such as the synaptic connectivity, of the modules relatively slowly, whereas recognition is based on rapidly changing <italic>states</italic> of the system, such as the membrane potentials and firing rate <xref ref-type="bibr" rid="pcbi.1003219-Fiser1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Friston4">[46]</xref>. In all simulations below, there are two main tasks: (i) a learning task where the feedback parameters from second level to first are allowed to change and (ii) a recognition task where parameters are fixed and the model only reconstructs the hidden dynamics. In both cases, the model is given the appropriate precision settings from the beginning of the experiment and it either performs a learning task or a recognition task. A single learning step consists of learning one word by one module.</p>
<p>Both recognition and learning in a module starts with sensation; a speech sound wave (a single word for all but one simulations below), and after passing through the cochlea model this serves as a dynamic input to the module. The speech signal is preprocessed by the cochlear model and the dynamic output of the cochlear model, which we denote by a vector <italic>z(t)</italic>, reaches the first level of the module (<xref ref-type="fig" rid="pcbi-1003219-g001">Figure 1</xref>; for mathematical details see <xref ref-type="sec" rid="s2">Model</xref>). Given this time-dependent vector <italic>z(t)</italic> and the two-level generative model (<xref ref-type="disp-formula" rid="pcbi.1003219.e009">Equations 1</xref> and <xref ref-type="disp-formula" rid="pcbi.1003219.e043">2</xref> in <xref ref-type="fig" rid="pcbi-1003219-g001">Figure 1</xref>, see <xref ref-type="sec" rid="s2">Model</xref>), each module infers the states of the first and second levels (recognition) and learns the connection weights from the second to the first level (<italic>I</italic>'s), see first line of <xref ref-type="disp-formula" rid="pcbi.1003219.e043">Equation 2</xref>. To implement this, we used the Bayesian inference technique “Dynamic Expectation Maximization” <xref ref-type="bibr" rid="pcbi.1003219-Friston2">[42]</xref>.</p>
<p>Both levels of a module consist of neuronal populations that interact within and between levels. These populations encode expectations about the cochlea model dynamics, i.e. the sensory input, using the internal generative model described in the previous section. These expectations predict the neuronal activity (i.e., firing rates) at the next lower level, i.e., either at the cochleagram or the first level. The hierarchical inference uses top-down and bottom-up messages, which aim to minimize an error signal, the so-called prediction error. At any given time <italic>t</italic>, the input from the cochlear model, <italic>z(t)</italic>, is compared to the predictions at the first level which are produced by the generative model. During recognition, the prediction error is propagated to the second level where, again, prediction errors are computed using the generative model. Both levels adjust their internal predictions to minimize the prediction errors <xref ref-type="bibr" rid="pcbi.1003219-Friston2">[42]</xref>. The module's expectation of how much an internal state will vary is a key parameter of the model: It is called “precision”. The precision determines how much error is tolerated at a specific level and we illustrate its relevance to speech learning and recognition in the next section.</p>
<p>During recognition, the second level forms predictions that are transmitted to the first level. This is only possible if the parameters for the backward connections between these two levels are appropriate; each module has to learn these parameters. In contrast to recognition, learning is not accomplished online because the information about parameters is obtained at a slower time scale, i.e., over the course of a complete stimulus (word) or repetitions of a stimulus. For learning, prediction errors are summed up for the whole stimulus duration and used after stimulus presentation to update the parameters. Therefore, as each module is exposed to repeated stimuli, the parameters are updated to minimize the prediction error accumulated over time, while states are updated in an online fashion to minimize temporally <italic>local</italic> prediction errors.</p>
<p>In summary, learning and recognition are realized as parts of the same inference scheme and work together to minimize overall prediction error. The necessary computations can be described as the dynamics of a hierarchically structured recurrent neural network operating online on the continuous speech input <xref ref-type="bibr" rid="pcbi.1003219-Friston3">[43]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Bitzer1">[47]</xref>. For further details, see <xref ref-type="sec" rid="s2">Model</xref>.</p>
</sec><sec id="s3b">
<title>Testing the human speech learning and recognition model</title>
<sec id="s3b1">
<title>Learning speech</title>
<p>Before speech can be recognized, it has to be learned <xref ref-type="bibr" rid="pcbi.1003219-Beauchemin1">[48]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Kuhl1">[49]</xref>. We, therefore, first tested whether the model could learn to recognize words. For this, we used the sound waves of the words for digits zero to nine spoken by one speaker. We took the stimuli from a speech database (TI-46, <ext-link ext-link-type="uri" xlink:href="http://www.ldc.upenn.edu" xlink:type="simple">www.ldc.upenn.edu</ext-link>), which is a standard benchmark test for speech recognition algorithms <xref ref-type="bibr" rid="pcbi.1003219-Hopfield2">[50]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Verstraeten2">[51]</xref>. We first put each module into learning mode, which is characterized by very high precision at the second level states and relatively lower precisions at the first level states (<xref ref-type="fig" rid="pcbi-1003219-g003">Figure 3A</xref>). This makes each module expect sequential dynamics at the second level and adapt states at the first level accordingly, using prediction error. In addition, at the first level, we set each module's precision for the sensory states, i.e., causal states at the first level (see <xref ref-type="supplementary-material" rid="pcbi.1003219.s001">Text S1</xref>) relatively high, while the internal dynamics at the first level have lower precision (<xref ref-type="fig" rid="pcbi-1003219-g003">Figure 3A</xref>). This precision ratio at the first level is crucial for learning: The relatively high precision forces each module to closely match the external stimulus, i.e., minimize the prediction error about the sensory input, and allow for more prediction error on the internal dynamics. To reduce these prediction errors, each module is forced to adapt the backward connections from the second level to the first level, which are free parameters in the model (the <italic>I</italic>'s in <xref ref-type="disp-formula" rid="pcbi.1003219.e043">Equation 2</xref>). This automatic optimization process iterates until the prediction error can be no further reduced and is typically completed after five to six repetitions of a word. With this learning mode, we found that learning is typically completed after five to six repetitions of a word. In general, we found that precisions deviating from these settings will lead to either slower learning rates or no learning at all. To illustrate the quality of learning, we read out the internal model of each module by using the learned parameters to generate cochleagram dynamics and compared it with the actual stimulus that was learned. In <xref ref-type="fig" rid="pcbi-1003219-g004">Figure 4</xref>, we show a typical sample where the dynamics generated using the learned parameters (dashed lines) follow the cochleagram dynamics (solid lines) closely. Qualitatively, all words have been learned similarly well.</p>
<fig id="pcbi-1003219-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003219.g003</object-id><label>Figure 3</label><caption>
<title>Schema of ideal precision settings, at the first and second levels of a module, for learning and recognition under noise.</title>
<p>The precision of a population at each level is indicated by the line thickness around the symbols, and the influence of a population over another is indicated by arrow strength. <bold>A</bold>) During learning, the precision ratio at the first level (precision of the sensory states, i.e., causal states, over precision of the internal (hidden) dynamics) should be high. Consequently, the internal dynamics at the first level are dominated by the dynamics of the sensory input. At the second level, a very high precision makes sure that the module is forced to explain the sensory input as sequential dynamics by updating (learning) the connections between first and second levels (the <italic>I</italic>'s in the first line of <xref ref-type="disp-formula" rid="pcbi.1003219.e043">Equation 2</xref>). <bold>B</bold>) Under noisy conditions, the sensory input is not reliable and recognition performance is best if the precision at the sensory level is low compared to the precision of the internal dynamics at both levels (low sensory/internal precision ratio). This allows the module to rely on its (previously learned) internal dynamics, but less-so on the noisy sensory input. For the exact values of the precision settings in each scenario, see <xref ref-type="supplementary-material" rid="pcbi.1003219.s001">Text S1</xref>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003219.g003" position="float" xlink:type="simple"/></fig><fig id="pcbi-1003219-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003219.g004</object-id><label>Figure 4</label><caption>
<title>Generated neuronal network activity at the first level after learning.</title>
<p>The solid lines represent the cochleagram dynamics obtained from the stimulus (the word “zero”, the same stimulus as shown in <xref ref-type="fig" rid="pcbi-1003219-g001">Figure 1</xref>) that the module had to learn. Neuronal activity was normalized to one. The dashed lines represent the neuronal activity generated by the module after learning and shows that the module has successfully learned the proper <italic>I</italic> vectors between two levels.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003219.g004" position="float" xlink:type="simple"/></fig></sec><sec id="s3b2">
<title>Word recognition task</title>
<p>After learning has concluded for each module separately and backward connections are fixed, we tested whether the agent showed high performance in a word classification task. We tested classification performance on a subset of the TI-46 speech database, which contained ten samples of ten words for digits (zero to nine) spoken by five female speakers, adding up to a total of 500 speech samples. To measure recognition performance, we used a cross-validation procedure, as is standard in speech recognition benchmark testing <xref ref-type="bibr" rid="pcbi.1003219-Verstraeten2">[51]</xref>: We randomly divided the 500 words into a training set (400 samples; 8 samples per digit and speaker) and a test set (100 samples; 2 samples per digit and speaker). In the training set, each module, one module for each digit sample, learned the backward connections between the second and first levels which gives us 400 parameter sets. To obtain ten speaker-independent and word-specific modules (one for each digit), we averaged these connections within digit. During the test phase, each of the 100 test samples, which had not been used during learning, were recognized by each of these ten modules while learning was turned off. For classification, we used a winner-take-all process (see <xref ref-type="supplementary-material" rid="pcbi.1003219.s001">Text S1</xref>) where the winner was the module with the <italic>lowest</italic> prediction error, i.e. the module which can best explain the sensory input using its internal model. The average Word Error Rate (WER; ratio of incorrectly classified test samples and the total number of test samples) was 1.6%. This is at roughly the same level as state-of-the art automatic speech recognition systems (<xref ref-type="table" rid="pcbi-1003219-t002">Table 2</xref>).</p>
<table-wrap id="pcbi-1003219-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003219.t002</object-id><label>Table 2</label><caption>
<title>Word Error Rates (WER) for isolated digit recognition task reported in the literature for different recognition methods.</title>
</caption><alternatives><graphic id="pcbi-1003219-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003219.t002" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">DEM</td>
<td align="left" rowspan="1" colspan="1">LSTM</td>
<td align="left" rowspan="1" colspan="1">LSM</td>
<td align="left" rowspan="1" colspan="1">LSM 2</td>
<td align="left" rowspan="1" colspan="1">HMM</td>
<td align="left" rowspan="1" colspan="1">OT</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">WER</td>
<td align="left" rowspan="1" colspan="1"><bold>1.6%</bold></td>
<td align="left" rowspan="1" colspan="1">2.0%</td>
<td align="left" rowspan="1" colspan="1">4.3%</td>
<td align="left" rowspan="1" colspan="1">0.2%</td>
<td align="left" rowspan="1" colspan="1">0.6%</td>
<td align="left" rowspan="1" colspan="1">2.4%</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt102"><label/><p>Note: DEM (Dynamic Expectation Maximization) is the recognition system used in this paper; LSTM (Long Short-Term Memory) network was introduced in <xref ref-type="bibr" rid="pcbi.1003219-Graves1">[111]</xref>, LSM (Liquid State Machine) with 1232 neurons was reported in <xref ref-type="bibr" rid="pcbi.1003219-Verstraeten2">[51]</xref> and was improved (LSM 2) in <xref ref-type="bibr" rid="pcbi.1003219-Verstraeten1">[18]</xref>. The results for the state-of-the-art speech recognition system using HMM (Hidden Markov Model) were reported in <xref ref-type="bibr" rid="pcbi.1003219-Verstraeten1">[18]</xref>. OT (Occurrence Time) features were used in <xref ref-type="bibr" rid="pcbi.1003219-Zavaglia1">[103]</xref>.</p></fn></table-wrap-foot></table-wrap>
<p>Next, we tested whether the model is robust against noise. Following a noise reduction step at the cochlear level (see <xref ref-type="supplementary-material" rid="pcbi.1003219.s001">Text S1</xref>), the classification results in WER for different signal-to-noise ratios of 30 dB, 20 dB and 10 dB were 3.6%, 5% and 11.2%, respectively. The results compare well with the state-of-the art speech recognition system that has been tested on the same noisy input, i.e., using the liquid state machine (8.5%, 10.5% and 11.5% WER), respectively <xref ref-type="bibr" rid="pcbi.1003219-Verstraeten2">[51]</xref>.</p>
<p>We next exposed the modules to situations that are quite typical for conditions under which humans perceive speech well but which pose severe challenges to automatic speech recognition schemes. These are variations in speech rate and accent, and cocktail party situations.</p>
</sec><sec id="s3b3">
<title>Variations in speech rate</title>
<p>The human auditory system shows remarkable flexibility for variations in speech rate <xref ref-type="bibr" rid="pcbi.1003219-Adank1">[52]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Miller1">[53]</xref>, whereas such variations pose a serious problem for automatic speech recognition models <xref ref-type="bibr" rid="pcbi.1003219-Hopfield2">[50]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Gutig1">[54]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Hemmen1">[55]</xref>. We, therefore, tested whether our recognition model is capable of dealing with time-compressed speech.</p>
<p>We compressed the cochleagrams in time to induce variability in speech rate. We exposed a module, which was trained on a normal length of spoken digit “eight” (M8), to a sample compressed by 25%, without changing pitch. The results show that the module can recognize the time-compressed word (<xref ref-type="fig" rid="pcbi-1003219-g005">Figure 5A</xref>). Importantly, this recognition does not require any parameter learning. The module is inherently robust against time compression because it explains away the compression, using prediction error, by speeding up the sequential dynamics at the second level (see compressed dynamics at the second level in <xref ref-type="fig" rid="pcbi-1003219-g005">Figure 5A</xref>, middle panel). This works well because the module is informed, by its second level, about the sequence of dynamics expected for a specific word and temporal variation do not change this sequence. Importantly, even under compression, the recognition performance is still high. For example, a module that was originally trained on a normal-length “three” stimulus experiences a lot of prediction error when confronted with a compressed “eight” stimulus. This can be seen qualitatively by the confused sequential dynamics at the second level (<xref ref-type="fig" rid="pcbi-1003219-g005">Figure 5A</xref>, bottom). The module trained on normal length “eight” stimulus recognizes the correct sequence (<xref ref-type="fig" rid="pcbi-1003219-g005">Figure 5A</xref>, middle) and produces the lowest prediction errors for the time compressed “eight” stimulus, among all ten modules each trained on a different normal length digit (<xref ref-type="fig" rid="pcbi-1003219-g005">Figure 5B</xref>).</p>
<fig id="pcbi-1003219-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003219.g005</object-id><label>Figure 5</label><caption>
<title>Invariance of the recognition model to variation in speech rate.</title>
<p><bold>A</bold>) The normal length stimulus “eight” (400 ms, top panel) has been learned and recognized successfully by the module “eight” (M8). For clarity, we only show the second level causal states (see <xref ref-type="sec" rid="s2">Model</xref>). The same module (without any parameter adaptation) successfully recognizes a time-compressed version of the same stimulus (300 ms, middle panel). For comparison, the module trained on a digit “three” (M3) fails to reconstruct its expected dynamics when exposed to “eight” (bottom panel). <bold>B</bold>) The total prediction errors produced at the second level hidden states by ten different modules (M0 to M9), which were previously trained on the corresponding digits with normal length, are shown. All modules were exposed to the same 25% time compressed “eight” stimulus. Module M8 (red arrow) produces the lowest prediction error and shows that prediction error can be used for classification, even though the stimulus is time compressed.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003219.g005" position="float" xlink:type="simple"/></fig></sec><sec id="s3b4">
<title>Recognition in a noisy environment</title>
<p>Humans are able to concentrate on a specific speaker's voice when there are other competing speakers, as typically experienced at a cocktail party <xref ref-type="bibr" rid="pcbi.1003219-Bronkhorst1">[56]</xref>–<xref ref-type="bibr" rid="pcbi.1003219-Mesgarani1">[59]</xref>. This is often tested with sentence-long stimuli with an increasing number of speakers. Here we used the target sentence “She argues with her sister.” (stimulus taken from <xref ref-type="bibr" rid="pcbi.1003219-McDermott1">[58]</xref>) and presented it to a module without background speaker, with one background speaker, and with three background speakers. Therefore, a module represents the dynamics of a whole sentence instead of a single word as in the previous simulations. The background speakers speak different sentences, and have a loudness level that corresponds to a location in space that is twice as far away from the listener as the target speaker. The module, as expected, is able to reconstruct the second level dynamics perfectly when it is exposed to the clear stimulus without background speakers (<xref ref-type="fig" rid="pcbi-1003219-g006">Figure 6</xref>, left column). It also reconstructs the target sentence dynamics when there is one additional speaker in the background (<xref ref-type="fig" rid="pcbi-1003219-g006">Figure 6</xref>, middle column). The second level always shows the correct order of activation even though some of the elements of the sequence are slightly misplaced in time when the background speaker masks the target (<xref ref-type="fig" rid="pcbi-1003219-g006">Figure 6</xref>). This is immediately corrected once the target sentence is again discernible in the cochleagram, i.e., when interference with the target sentence becomes small enough. In humans, such periods of recognition may be useful to help recognize the target sentence <xref ref-type="bibr" rid="pcbi.1003219-McDermott1">[58]</xref>. The module can very roughly reconstruct the second level dynamics and the correct order of activations when there are three background speakers (<xref ref-type="fig" rid="pcbi-1003219-g006">Figure 6</xref>, right column); the dynamics can be recovered at the beginning and towards the end of the sentence. These simulations suggest that the module uses expectations about sequential dynamics, i.e., dynamic predictions, at the second level to recover a target sentence from corrupted sensory dynamics.</p>
<fig id="pcbi-1003219-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003219.g006</object-id><label>Figure 6</label><caption>
<title>Performance of the recognition model in “cocktail party” situations.</title>
<p>A module is trained on an auditory sentence (“She argues with her sister”) without competing speakers and tested for recognition of this sentence in three conditions: <bold>Left column)</bold> No competing speaker, <bold>Middle column)</bold> one competing speaker, and <bold>Right column)</bold> three competing speakers. Each column shows the second level dynamics, first level dynamics and cochleagram with arbitrary units in neuronal activation. Second level dynamics were successfully reconstructed for the single speaker and also, to an extent, for the speech sample with one competing speaker. In the case of three competing speakers, the module was not able to reconstruct the second level dynamics completely, but showed some signs of recovery at the beginning and at the end of the sentence. Note that the increasing difficulty in reconstruction of the speech message from one to three speakers is not reflected in the prediction errors at the first level (dashed lines), but becomes obvious at the second level.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003219.g006" position="float" xlink:type="simple"/></fig></sec></sec><sec id="s3c">
<title>Adaptation and learning of speech</title>
<p>In the following two sections, we describe how we tested the hypothesis that the prior precision setting of a module is fundamental for understanding the learning of speech. This hypothesis follows from the construction of the module where only two different interpretations of suboptimal speech recognition exist: (i) the sensed speech is noisy, or (ii) the module's internal model is not appropriate and needs to be adapted. This is why the precision ratio at the first level, i.e., a module's expectation about how noisy speech dynamics are relative to its internal dynamics, is fundamental for learning. A precision setting as shown in <xref ref-type="fig" rid="pcbi-1003219-g003">Figure 3A</xref> will effectively exclude the module's assumption that speech is noisy; rather it will rely on the assumption that speech is sequential based on a high precision of the dynamics at the second level. This will prompt the module to adapt its internal speech model.</p>
<sec id="s3c1">
<title>Accent adaptation</title>
<p>Foreign accents are often a cause of severe variations in spoken language. Behaviorally, recognition of foreign-accented speech can affect the comprehension of words <xref ref-type="bibr" rid="pcbi.1003219-Munro1">[60]</xref> and increase the processing time of listeners who are used to unaccented speech <xref ref-type="bibr" rid="pcbi.1003219-Munro2">[61]</xref>. However, relatively brief exposure (between 2 and 64 sentences) to foreign-accented speech improves listeners' recognition accuracy <xref ref-type="bibr" rid="pcbi.1003219-Bradlow1">[62]</xref> and efficiency, measured in terms of error rates and reaction times <xref ref-type="bibr" rid="pcbi.1003219-Clarke1">[63]</xref>.</p>
<p>Here, we show how this rapid accent adaptation can be implemented by the present model and how behavioral differences in adaptation can be explained. By “adaptation” we mean that the learning of the parameters in a module proceeds from a previously learned parameter set (base accent) as opposed to learning from scratch in the “Learning speech” simulation. Therefore, adaptation can be understood as slight changes of the backward connections instead of learning a completely new word.</p>
<p>We trained a module to recognize the speech stimulus “eight” spoken with a North England accent (<xref ref-type="fig" rid="pcbi-1003219-g007">Figure 7A</xref>, top) and tested recognition for an “eight” spoken by a different speaker with a New Zealand accent (<xref ref-type="fig" rid="pcbi-1003219-g007">Figure 7A</xref>, bottom; stimuli taken from <ext-link ext-link-type="uri" xlink:href="http://www.soundcomparisons.com" xlink:type="simple">www.soundcomparisons.com</ext-link>). On first presentation of the word, the module experiences increased prediction error during recognition of the accented word, i.e., it would perform worse in a word recognition test. We hypothesized that a crucial criterion for whether a module can, or cannot, adapt to an accent, is its prior precision of the sensory states, i.e., how noisy the module expects the sensory input to be. If this precision is low compared to the precision of the internal dynamics (“recognition mode”, as shown in <xref ref-type="fig" rid="pcbi-1003219-g003">Figure 3B</xref>), no adaptation is induced, because the module accepts the slight variations due to the accent as noise on its sensory input. If, however, the module expects input to be sensed with high precision, an accented word causes the module to adapt its internal model, i.e., its backward connections from the second to the first level. This is, from the module's view, the only way to explain the unexpected variations in the input (“learning mode”, as shown in <xref ref-type="fig" rid="pcbi-1003219-g003">Figure 3A</xref>). We tested this explicitly by controlling the ratio of the module's prior precision of the sensory states and internal dynamics (sensory/internal precision ratio) at the first level of the model. As expected, we found that only a module that has a high precision ratio at the first level (learning mode, <xref ref-type="fig" rid="pcbi-1003219-g003">Figure 3A</xref>) rapidly adapts to accented speech (<xref ref-type="fig" rid="pcbi-1003219-g007">Figure 7B</xref>). With the three highest precision settings, this was achieved after only two to three iterations. For lower precision ratios, practically no adaptation occurred. This suggests a potential mechanism for the inter-individual variability to accent adaptation: agents, and potentially also humans, attending to sensory detail can adapt to accents while agents/humans who literally explain deviations as background noise cannot adapt to accents. However, it should be noted that this is a rather simplistic explanation which has emerged as a consequence of our simulations and does not explain all aspects of accent adaptation. It should be considered as an interpretation of the optimum precision settings obtained through simulations.</p>
<fig id="pcbi-1003219-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003219.g007</object-id><label>Figure 7</label><caption>
<title>Accent adaptation of the recognition model.</title>
<p><bold>A</bold>) The cochleagrams represent two utterances of “eight”. A module originally learned the word “eight” spoken with a British (North England) accent (top) and then recognized an “eight” spoken with a New Zealand accent (bottom). <bold>B</bold>) The module trained on the British accent was allowed to adapt to the New Zealand accent with the corresponding precision values for the first level sensory (causal) and internal (hidden) states (sensory log-precision: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e121" xlink:type="simple"/></inline-formula> and internal log-precision: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e122" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e123" xlink:type="simple"/></inline-formula> from left to right). For each precision ratio, we plotted the reduction in prediction error (of the causal states, see <xref ref-type="sec" rid="s2">Model</xref>) after five repetitions of the word “eight” spoken with a New Zealand accent. As expected, accent adaptation was accomplished only with high sensory/internal precision ratios (resulting in greatly reduced prediction errors) whereas no adaptation occurred (prediction errors remained high) when this ratio was low.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003219.g007" position="float" xlink:type="simple"/></fig></sec><sec id="s3c2">
<title>Speech learning: Qualitative modeling</title>
<p>Can the precision setting also explain a re-learning of speech, as for example in second language learning? People start learning second languages at different ages. The age of second language acquisition is an important factor for being fluent in the new language <xref ref-type="bibr" rid="pcbi.1003219-Birdsong1">[64]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Singleton1">[65]</xref>. This age factor is behaviorally relevant when second language learners are asked to recognize words or sentences embedded in background noise <xref ref-type="bibr" rid="pcbi.1003219-Mayo1">[66]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Meador1">[67]</xref>. Here, we tested whether the present model could be used to qualitatively model behavioral results <xref ref-type="bibr" rid="pcbi.1003219-Meador1">[67]</xref>. If this were possible, it would imply that the model represents a potential computational mechanism for explaining the importance of age in language learning.</p>
<p>A previous study examined the recognition of English words by three groups of native Italian speakers with different mean age of arrival (mAOA) when immigrating to Canada <xref ref-type="bibr" rid="pcbi.1003219-Meador1">[67]</xref>: An early group (mAOA of 7 years), a mid group (mAOA of 14 years) and a late group (mAOA of 19 years). In addition, there was a control group of native English speakers. The stimuli consisted of ten English sentences presented at four different signal-to-noise ratios (−6, 0, 6 and 12 dB). The participants repeated as many words as possible after each presentation of a sentence. Significantly higher recognition accuracies were obtained for early, as compared to the mid and late groups, and the native group performed significantly better than all immigrant groups (<xref ref-type="fig" rid="pcbi-1003219-g008">Figure 8A</xref>).</p>
<fig id="pcbi-1003219-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003219.g008</object-id><label>Figure 8</label><caption>
<title>Qualitative modeling of experimental results in second language learning.</title>
<p><bold>A</bold>) The behavioral results of an experiment <xref ref-type="bibr" rid="pcbi.1003219-Meador1">[67]</xref> for the recognition of English words by three groups of native speakers of Italian who differed in their age of arrival in Canada: Early, Mid and Late arrival groups, also compared to a native English speaker (NE) group. Participants were asked to repeat as many words as possible after they heard an English sentence. Sentences were presented at different signal-to-noise ratios given in decibels (dB). Adapted from <xref ref-type="bibr" rid="pcbi.1003219-Meador1">[67]</xref>. <bold>B</bold>) The results of the learning and recognition simulations where we used the same speech samples as in the Word Recognition Task. The different age of arrival was modeled with different precision ratios at the first level. Recognition accuracy is measured in terms of normalized, total causal prediction errors during recognition relative to a baseline condition of −30 dB noise, i.e., recognition accuracy = 100*[(baseline prediction error-test prediction error)/baseline prediction error]. Note that we used different signal-to-noise ratios than the original experiment because best recognition results with our model were obtained at 30 dB, which corresponds to almost ideal recognition results in humans around 12 dB, and we scaled the remaining ratios accordingly. Each symbol represents the average recognition accuracy obtained from 10 digits where the stimulus was masked with noise at given signal-to-noise ratios.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003219.g008" position="float" xlink:type="simple"/></fig>
<p>We simulated second language learning using the present model to explain these behavioral results. As second language, we used digit words and simulated different ages of arrival by using different precision settings (from high first level sensory/internal precision ratio for native English speakers to progressively lower ratios for early, mid and late groups, see details in <xref ref-type="supplementary-material" rid="pcbi.1003219.s001">Text S1</xref>). The recognition results compared well with the experimental results (<xref ref-type="fig" rid="pcbi-1003219-g008">Figure 8B</xref>). The recognition accuracy improved with increasing signal-to-noise ratios in all groups and the native speakers recognized more accurately at all noise levels followed by the early, mid and late groups. These results suggest the computational mechanism for the behavioral results found in the four groups: The longer someone is exposed to his/her native language, the more precise the expectations could be about the brain's internal dynamics when recognizing speech. This high precision would be counter-productive when learning a second language because internal dynamics are not learned optimally: the agent, i.e., the brain, would rather explain away prediction error by assuming that speech of the second language is relatively noisy as compared to speech of the first language. However, it should be noted that there is no experimental evidence for such a claim yet, i.e. that the words in a second language are considered to be noisy in late learners, and this point should be taken as an interpretation of our computational results. In fact, as pointed out by one of the reviewers, many studies have concluded that the amount and variability of second language input <xref ref-type="bibr" rid="pcbi.1003219-Jia1">[68]</xref>–<xref ref-type="bibr" rid="pcbi.1003219-Flege1">[70]</xref> as well as the frequency of using the native language during learning <xref ref-type="bibr" rid="pcbi.1003219-Flege2">[71]</xref> have considerable influence in the age of acquisition effects.</p>
</sec></sec></sec><sec id="s4">
<title>Discussion</title>
<p>We have developed a novel model of speech learning and recognition that is implemented as a hierarchically structured recurrent neural network. The core structure of the network was taken from a birdsong model that was based on key experimental findings in songbirds <xref ref-type="bibr" rid="pcbi.1003219-Yildiz1">[14]</xref>. We found that the resulting computational model achieves very high recognition performance when recognizing words directly from speech sound waves, both under ideal noise-free and noisy conditions. In addition, the model deals well with situations in which automatic speech recognition usually fails, but humans still perform well: adaptation to varying speech rate and competition by multiple speakers. The model is also able to explain inter-individual differences in accent adaptation, as well as age of acquisition effects in second language learning.</p>
<sec id="s4a">
<title>Sequential dynamics in song and speech recognition</title>
<p>In songbird studies, temporally precise sequential activation of neurons in a high level structure, HVC, has been observed during singing <xref ref-type="bibr" rid="pcbi.1003219-Hahnloser1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Yu1">[23]</xref> and the same area has also been shown to be involved during recognition of songs with similar precise activations <xref ref-type="bibr" rid="pcbi.1003219-Prather1">[6]</xref>. It has been suggested that Broca's area in the inferior frontal gyrus (pars opercularis) in humans corresponds functionally to HVC in songbirds <xref ref-type="bibr" rid="pcbi.1003219-Doupe1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Okanoya1">[72]</xref>. Similar to HVC, this area in the human inferior frontal gyrus is involved in recognition and production of speech. It has been implied in sequence perception and in providing top-down predictions to auditory speech processing areas (for a review see <xref ref-type="bibr" rid="pcbi.1003219-Price1">[73]</xref>). We suggest that this is a candidate area for including precise sequential activation of neurons, as modeled by dynamic sequences at the second level of the present model. The existence of sequentially activated, temporally precise, neuronal ensembles in the cortex has been proposed previously <xref ref-type="bibr" rid="pcbi.1003219-Abeles1">[74]</xref> and provides an explanation for findings of a precise spike timing which have been observed in experiments in different species, e.g. <xref ref-type="bibr" rid="pcbi.1003219-Long1">[75]</xref>–<xref ref-type="bibr" rid="pcbi.1003219-Prut1">[77]</xref>. There do not seem to be equivalent neuronal studies in humans; however, speech processing activity, as observed with magnetoencephalography, has been explained as large-scale sequential activity <xref ref-type="bibr" rid="pcbi.1003219-Pulvermuller1">[78]</xref>. Based on the results of the current paper, we predict that such sequential activations in the human brain, expressed at a microscopic level, e.g., in spike timing, are crucial in organizing the auditory information coming from the lower areas to form the dynamic percept of phonemes, syllables and words.</p>
<p>Even though the second level ensembles in the proposed model are encoded as temporally regularly spaced sequences in the generative model, we showed that during recognition (see Variations in Speech Rate simulation) they have the flexibility to activate earlier or later according to the spectrotemporal features they are tuned to. This fits well with a recent study <xref ref-type="bibr" rid="pcbi.1003219-Amador1">[79]</xref> where the authors presented evidence that HVC activity is timed to particular time points of motor gestures during song production. The current generative model does not include a vocal tract mechanism <xref ref-type="bibr" rid="pcbi.1003219-Laje1">[80]</xref>; however such a mechanism could be readily incorporated with an extra level at the bottom of the hierarchy (see <xref ref-type="bibr" rid="pcbi.1003219-Yildiz1">[14]</xref> for an example).</p>
<p>To model neurobiological findings in songbirds, we used an advanced Bayesian inference scheme using recurrent neuronal networks. To our knowledge, this type of model has not been used before, neither in human speech recognition nor automatic speech recognition. One advantage of this approach is that recognition is performed in a brain-like fashion on continuous sensory dynamics, in contrast to a standard hidden Markov model operating on discretized input <xref ref-type="bibr" rid="pcbi.1003219-Bilmes1">[16]</xref>. In addition, the present model can be used, as we have demonstrated, to incorporate experimental birdsong findings by specifying a hierarchically structured, generative model based on nonlinear dynamical systems and translate the resulting model to human speech.</p>
</sec><sec id="s4b">
<title>Comparison to other speech processing models in neuroscience</title>
<p>Our approach is unique in the sense that we use a hierarchy of nonlinear dynamical systems as a generative model to provide an online Bayesian inversion mechanism of human speech. Many other computational speech and word recognition models have been proposed that are both neurobiologically plausible and can explain experimental results <xref ref-type="bibr" rid="pcbi.1003219-Mcclelland1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Norris1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Gaskell1">[81]</xref>–<xref ref-type="bibr" rid="pcbi.1003219-Seidenberg1">[87]</xref>. These models typically focus on the word selection process rather than on how relevant spectrotemporal features are extracted from the sound wave. For example, most of these models assume that relevant phonemic features have already been extracted from the sound wave and arrive in regular intervals. This is in distinction to the present approach which models the extraction of relevant speech features from a noisy, continuous sound wave with varying speech rate. An example for these word selection models is the hierarchical TRACE model <xref ref-type="bibr" rid="pcbi.1003219-Mcclelland1">[19]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-McClelland1">[88]</xref>. There are three key differences between the TRACE and the present model. First, there is no learning in TRACE: the model parameters have to be manually set to enable recognition. Second, TRACE does not represent precision, which, as illustrated above, may be important to explain phenomena in both perception and learning. Third, TRACE is based on the competition of relatively simple processing units, and, therefore, is unable to identify local mistakes or mispronunciations; it returns the most probable word. In contrast, the present model can monitor such mismatches in an online fashion using the prediction error. This enables the processing of slight differences in pronunciations, as, for example, when the proposed model was used to adapt to speech with an unusual accent.</p>
<p>Another widely known model is the Shortlist model and its Bayesian version Shortlist B <xref ref-type="bibr" rid="pcbi.1003219-Norris1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Norris2">[21]</xref>. Both models have most of the functionalities of the TRACE where information is processed in a feed-forward fashion. The Bayesian approach introduced in Shortlist B <xref ref-type="bibr" rid="pcbi.1003219-Norris2">[21]</xref> illustrates a useful way to combine prior information such as word frequency with the likelihood function of the speech input. This demonstrates the interplay between the priors and the precision of the agent (called reliability in <xref ref-type="bibr" rid="pcbi.1003219-Norris2">[21]</xref>). This is similar to the present model, where a differential setting of the precision parameters causes either recognition or learning mode of the sensory input (see <xref ref-type="fig" rid="pcbi-1003219-g003">Figure 3</xref>). The main differences between the present model and Shortlist B are that (i) Shortlist B does not allow for speech learning, (ii) Shortlist B assumes that phonemic features have already been extracted by some preprocessing stage while we explicitly model this stage using the cochleagram, and (iii) Shortlist B has been formulated as a feed-forward model only while the present model explicitly uses top-down influence to improve recognition of noisy input.</p>
<p>A different category of models has focused, like the present model, on the processing of auditory stimuli by single neurons or network of neurons <xref ref-type="bibr" rid="pcbi.1003219-Verstraeten1">[18]</xref>, . For recognition, these models typically have to wait until the end of the stimulus to obtain all required neuronal responses. This is different from human performance where recognition can be performed online while the stimulus is received. This online recognition using predictions is also a hallmark of the recognition model proposed here, where the accumulated prediction error can be used for recognition anytime during stimulus presentation.</p>
<p>Recently, so called reservoir computing techniques using recurrent neural networks have been used for speech recognition <xref ref-type="bibr" rid="pcbi.1003219-Verstraeten1">[18]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Verstraeten2">[51]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Schrauwen1">[89]</xref>–<xref ref-type="bibr" rid="pcbi.1003219-Maass1">[92]</xref> and provide excellent recognition results. Typically, these results are achieved with large networks of hundreds of neurons. This is different from the present study where we used few neurons for word recognition, i.e. just eight neurons at the second level and six neurons at the first, for each module. It would be worthwhile to consider recurrent networks as used in reservoir computing as a generative model in a Bayesian approach to better understand the mechanism underlying high recognition performances in reservoir computing.</p>
</sec><sec id="s4c">
<title>Precision: Link to neurotransmitter</title>
<p>Using simulations, we have shown that the precisions of the states (i.e., how certain the agent is about its internal states and dynamics) at different levels of the hierarchy are fundamental to learning and recognition of speech. Here, we fixed the prior precisions at each level to use appropriate precision settings during learning and recognition. The actual mechanisms in the brain for achieving such context-dependent optimum precision values are not known. Neurobiologically, cholinergic neurons (whose main neurotransmitter is acetylcholine, ACh) are known to be involved in the modulation of perceptual processes <xref ref-type="bibr" rid="pcbi.1003219-Herrero1">[93]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Hirayama1">[94]</xref>. It has been proposed that ACh may have the role of reporting on uncertainties of internal estimates and that high levels of ACh should correspond to faster learning about the environment and enhancement of bottom-up processing <xref ref-type="bibr" rid="pcbi.1003219-Yu2">[95]</xref>. Such claims fit well with the present study since we found that increased precision about sensory states is ideal for learning speech as it enhances the influence of sensory information; whereas, learning deteriorates with decreasing precision ratios (<xref ref-type="fig" rid="pcbi-1003219-g003">Figure 3</xref> and <xref ref-type="fig" rid="pcbi-1003219-g007">7</xref>). We predict that increased levels of ACh may enhance the learning of novel auditory stimuli by suppressing top-down effects caused by a relatively low precision of internal dynamics; however, this should, in parallel, also disrupt perception of noisy stimuli since top-down information is crucial in cocktail-party like situations, (right column of <xref ref-type="fig" rid="pcbi-1003219-g006">Figure 6</xref>). Such claims could be tested with a behavioral study while manipulating the neurotransmitter levels pharmacologically <xref ref-type="bibr" rid="pcbi.1003219-Hasselmo1">[96]</xref>.</p>
</sec><sec id="s4d">
<title>A novel analysis tool for neuroimaging experiments</title>
<p>The proposed model makes a computational link between sensory input (i.e., the speech sound wave) received by subjects and the dynamics of their hypothesized internal representation <xref ref-type="bibr" rid="pcbi.1003219-Poeppel1">[97]</xref>. In particular, we found that the prediction error is a key quantity that can be used to achieve high performance in speech recognition. This quantity can be used in novel computational analysis techniques for speech recognition neuroimaging experiments: The idea is to use the dynamics of the module's internal prediction error when receiving speech input as a predictor for neuronal activity in human subjects receiving exactly the same stimuli (see <xref ref-type="bibr" rid="pcbi.1003219-Gagnepain1">[98]</xref> for a similar study). This modeling approach would enable one to identify the exact computational role of specific areas in the well-established speech recognition system. In addition, this approach can be applied to speech learning studies (accent adaptation and second language learning), where one would use the module's prediction error experienced during learning to predict subject's changing brain activity during learning and estimate the precision parameters which subjects use. This may be done using either a voxel-wise regressor-based approach, or a network analysis (Dynamic Causal Modeling <xref ref-type="bibr" rid="pcbi.1003219-Friston5">[99]</xref>,<xref ref-type="bibr" rid="pcbi.1003219-Kiebel1">[100]</xref>). For example, one may estimate the changes in effective connection strength in a network including the inferior frontal gyrus and primary auditory areas during accent adaptation or for speech recognition under different levels of noise. It would also be revealing to include a variety of precision settings as an experimental condition in studies that specifically test the hierarchical predictive coding hypothesis in the auditory cortex <xref ref-type="bibr" rid="pcbi.1003219-Wacongne1">[101]</xref>.</p>
</sec><sec id="s4e">
<title>Extensions and limitations of the model</title>
<p>Here, we only used six neuronal ensembles to represent the cochleagram in six frequency channels. This resolution is comparable to the low number of spectral channels used in cochlear implants <xref ref-type="bibr" rid="pcbi.1003219-Friesen1">[102]</xref>. Nevertheless, the model provided competitive recognition results (<xref ref-type="table" rid="pcbi-1003219-t002">Table 2</xref>). We found that this performance drops if only four channels are used, but we did not explore this using more channels because the required computational power quickly increases with the current implementation (with complexity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e124" xlink:type="simple"/></inline-formula>). This computational issue could be resolved by parallel ensemble-specific computations, which would be another step towards biological reality and probably improving recognition rates further. It would also be worthwhile extending the cochlear features in the present model with other biologically plausible preprocessing steps, such as occurrence times, which encode the onsets and offsets of specific features <xref ref-type="bibr" rid="pcbi.1003219-Hopfield2">[50]</xref>, <xref ref-type="bibr" rid="pcbi.1003219-Zavaglia1">[103]</xref>.</p>
<p>It is important to notice that the current model is not entirely specific to speech but can also be used to recognize other sound sequences such as music. In a future project, we will therefore make the model more speech-specific and extend the current model by including a vocal tract model in addition to the cochlear processing. This would make the inference more sensitive to relevant features in human speech and thereby improve recognition. Moreover, such a vocal tract mechanism would be beneficial for recognizing speech from different speakers since speaker-specific parameters can be included in the vocal tract model and constrain the recognition dynamics. This would allow the model to identify the similarities between words even if they are spoken by differently sounding speakers and therefore have little acoustic overlap. Such a model can also be used to qualitatively model specific findings at a phonemic level <xref ref-type="bibr" rid="pcbi.1003219-Best1">[69]</xref>.</p>
<p>It is also worth mentioning that we assumed a fixed second-level connectivity matrix in the model (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003219.e125" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003219.e009">Eqn. 1</xref>) which produces expectations about sequential dynamics by winnerless competition. We assumed here that such a structure already existed at the higher levels. It may also be possible to learn these specific connections from scratch; however, we expect that one would need relatively informative priors about these parameters to limit the search space.</p>
<p>Moreover, the generative model could be extended by adding extra levels to the hierarchy of nonlinear dynamical systems. This would allow the modeling of sequences of phonemes and syllables <xref ref-type="bibr" rid="pcbi.1003219-Hanuschkin1">[104]</xref>, or even sentences as sequence of words <xref ref-type="bibr" rid="pcbi.1003219-Kiebel2">[105]</xref>. This can be done either using the technique proposed in the present paper or by using carefully designed nonlinear dynamical systems, as exemplified in <xref ref-type="bibr" rid="pcbi.1003219-Perdikis1">[106]</xref>. Such detailed sentence level representations could be used to model syntactic experiments as shown in <xref ref-type="bibr" rid="pcbi.1003219-beimGraben1">[107]</xref>. Using hierarchies, it would be useful to model the competition between possible alternative descriptions that emerge from partial stimuli where predictions provide constraints for the appropriate dynamics and therefore stable perception <xref ref-type="bibr" rid="pcbi.1003219-Winkler1">[108]</xref>. Such a hierarchical extension would be ideal to model the word selection process as exemplified in Shortlist B <xref ref-type="bibr" rid="pcbi.1003219-Norris2">[21]</xref> while using real speech (sound waves) as input. Finally, the proposed learning and recognition technique could be extended to also estimate dynamically the precision values based on techniques as employed by <xref ref-type="bibr" rid="pcbi.1003219-Feldman1">[109]</xref>. This would allow the model to fine-tune the precision settings as a part of the optimization process. Currently, one still needs to provide the prior precision settings to inform the model about the context of the experiment, i.e. whether it is a learning task or recognition task.</p>
</sec><sec id="s4f">
<title>Conclusion</title>
<p>We proposed a computational model using a hierarchy of nonlinear dynamical systems and Bayesian online filtering for learning and recognizing sound sequences such as speech. This model was derived from a neuronal model for recognition of birdsong. It achieves high speech recognition performance and explains several auditory recognition phenomena, as well as behavioral data. This work has three implications. First, it shows that human speech and birdsong recognition systems may share similar computational components. Secondly, the competitive performance, even under adverse conditions, suggests that it may be used to optimize automatic speech recognition. Thirdly, the neurobiological plausibility of the model enables the generation of predictions for neurobiological, e.g., neuroimaging, experiments.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003219.s001" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003219.s001" position="float" xlink:type="simple"><label>Text S1</label><caption>
<p>Here we describe further details about the simulations presented in the <xref ref-type="sec" rid="s3">Results</xref> section.</p>
<p>(PDF)</p>
</caption></supplementary-material></sec></body>
<back><ref-list>
<title>References</title>
<ref id="pcbi.1003219-Bolhuis1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bolhuis</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Okanoya</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Scharff</surname><given-names>C</given-names></name> (<year>2010</year>) <article-title>Twitter evolution: converging mechanisms in birdsong and human speech</article-title>. <source>Nature Reviews Neuroscience</source> <volume>11</volume>: <fpage>747</fpage>–<lpage>759</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Doupe1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Doupe</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Kuhl</surname><given-names>PK</given-names></name> (<year>1999</year>) <article-title>Birdsong and human speech: Common themes and mechanisms</article-title>. <source>Annual Review of Neuroscience</source> <volume>22</volume>: <fpage>567</fpage>–<lpage>631</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Creutzfeldt1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Creutzfeldt</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Ojemann</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Lettich</surname><given-names>E</given-names></name> (<year>1989</year>) <article-title>Neuronal-Activity in the Human Lateral Temporal-Lobe .1. Responses to Speech</article-title>. <source>Experimental Brain Research</source> <volume>77</volume>: <fpage>451</fpage>–<lpage>475</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Pasley1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pasley</surname><given-names>BN</given-names></name>, <name name-style="western"><surname>David</surname><given-names>SV</given-names></name>, <name name-style="western"><surname>Mesgarani</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Flinker</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Reconstructing Speech from Human Auditory Cortex</article-title>. <source>Plos Biology</source> <volume>10</volume> (<issue>1</issue>) <fpage>e1001251</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Berwick1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berwick</surname><given-names>RC</given-names></name>, <name name-style="western"><surname>Okanoya</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Beckers</surname><given-names>GJL</given-names></name>, <name name-style="western"><surname>Bolhuis</surname><given-names>JJ</given-names></name> (<year>2011</year>) <article-title>Songs to syntax: the linguistics of birdsong</article-title>. <source>Trends in Cognitive Sciences</source> <volume>15</volume>: <fpage>113</fpage>–<lpage>121</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Prather1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Prather</surname><given-names>JF</given-names></name>, <name name-style="western"><surname>Peters</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Nowicki</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Mooney</surname><given-names>R</given-names></name> (<year>2008</year>) <article-title>Precise auditory-vocal mirroring in neurons for learned vocal communication</article-title>. <source>Nature</source> <volume>451</volume>: <fpage>305</fpage>–<lpage>U302</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Jarvis1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jarvis</surname><given-names>ED</given-names></name> (<year>2004</year>) <article-title>Learned birdsong and the neurobiology of human language</article-title>. <source>Behavioral Neurobiology of Birdsong</source> <volume>1016</volume>: <fpage>749</fpage>–<lpage>777</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Okada1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Okada</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Rong</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Venezia</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Matchin</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Hsieh</surname><given-names>IH</given-names></name>, <etal>et al</etal>. (<year>2010</year>) <article-title>Hierarchical Organization of Human Auditory Cortex: Evidence from Acoustic Invariance in the Response to Intelligible Speech</article-title>. <source>Cerebral Cortex</source> <volume>20</volume>: <fpage>2486</fpage>–<lpage>2495</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Scott1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scott</surname><given-names>SK</given-names></name>, <name name-style="western"><surname>Johnsrude</surname><given-names>IS</given-names></name> (<year>2003</year>) <article-title>The neuroanatomical and functional organization of speech perception</article-title>. <source>Trends in Neurosciences</source> <volume>26</volume>: <fpage>100</fpage>–<lpage>107</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Theunissen1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Theunissen</surname><given-names>FE</given-names></name>, <name name-style="western"><surname>Amin</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Shaevitz</surname><given-names>SS</given-names></name>, <name name-style="western"><surname>Woolley</surname><given-names>SMN</given-names></name>, <name name-style="western"><surname>Fremouw</surname><given-names>T</given-names></name>, <etal>et al</etal>. (<year>2004</year>) <article-title>Song selectivity in the song system and in the auditory forebrain</article-title>. <source>Behavioral Neurobiology of Birdsong</source> <volume>1016</volume>: <fpage>222</fpage>–<lpage>245</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-DeWitt1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DeWitt</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Rauschecker</surname><given-names>JP</given-names></name> (<year>2012</year>) <article-title>Phoneme and word recognition in the auditory ventral stream</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>109</volume>: <fpage>E505</fpage>–<lpage>E514</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Lewicki1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lewicki</surname><given-names>MS</given-names></name>, <name name-style="western"><surname>Arthur</surname><given-names>BJ</given-names></name> (<year>1996</year>) <article-title>Hierarchical organization of auditory temporal context sensitivity</article-title>. <source>Journal of Neuroscience</source> <volume>16</volume>: <fpage>6987</fpage>–<lpage>6998</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Fee1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fee</surname><given-names>MS</given-names></name>, <name name-style="western"><surname>Kozhevnikov</surname><given-names>AA</given-names></name>, <name name-style="western"><surname>Hahnloser</surname><given-names>RHR</given-names></name> (<year>2004</year>) <article-title>Neural mechanisms of vocal sequence generation in the songbird</article-title>. <source>Ann N Y Acad Sci</source> <volume>1016</volume>: <fpage>153</fpage>–<lpage>170</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Yildiz1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yildiz</surname><given-names>IB</given-names></name>, <name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name> (<year>2011</year>) <article-title>A Hierarchical Neuronal Model for Generation and Online Recognition of Birdsongs</article-title>. <source>Plos Computational Biology</source> <volume>7</volume> (<issue>12</issue>) <fpage>e1002303</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Friston1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Kiebel</surname><given-names>S</given-names></name> (<year>2009</year>) <article-title>Predictive coding under the free-energy principle</article-title>. <source>Philosophical Transactions of the Royal Society B-Biological Sciences</source> <volume>364</volume>: <fpage>1211</fpage>–<lpage>1221</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Bilmes1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bilmes</surname><given-names>JA</given-names></name> (<year>2006</year>) <article-title>What HMMs can do</article-title>. <source>Ieice Transactions on Information and Systems</source> <volume>E89d</volume>: <fpage>869</fpage>–<lpage>891</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Mohamed1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mohamed</surname><given-names>AR</given-names></name>, <name name-style="western"><surname>Dahl</surname><given-names>GE</given-names></name>, <name name-style="western"><surname>Hinton</surname><given-names>G</given-names></name> (<year>2012</year>) <article-title>Acoustic Modeling Using Deep Belief Networks</article-title>. <source>IEEE Transactions on Audio Speech and Language Processing</source> <volume>20</volume>: <fpage>14</fpage>–<lpage>22</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Verstraeten1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Verstraeten</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Schrauwen</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Stroobandt</surname><given-names>D</given-names></name> (<year>2006</year>) <article-title>Reservoir-based techniques for speech recognition</article-title>. <source>2006 Ieee International Joint Conference on Neural Network Proceedings</source> <volume>Vols 1–10</volume>: <fpage>1050</fpage>–<lpage>1053</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Mcclelland1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mcclelland</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Elman</surname><given-names>JL</given-names></name> (<year>1986</year>) <article-title>The Trace Model of Speech-Perception</article-title>. <source>Cognitive Psychology</source> <volume>18</volume>: <fpage>1</fpage>–<lpage>86</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Norris1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Norris</surname><given-names>D</given-names></name> (<year>1994</year>) <article-title>Shortlist - A Connectionist Model of Continuous Speech Recognition</article-title>. <source>Cognition</source> <volume>52</volume>: <fpage>189</fpage>–<lpage>234</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Norris2"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Norris</surname><given-names>D</given-names></name>, <name name-style="western"><surname>McQueen</surname><given-names>JM</given-names></name> (<year>2008</year>) <article-title>Shortlist B: A Bayesian model of continuous speech recognition</article-title>. <source>Psychological Review</source> <volume>115</volume>: <fpage>357</fpage>–<lpage>395</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Hahnloser1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hahnloser</surname><given-names>RHR</given-names></name>, <name name-style="western"><surname>Kozhevnikov</surname><given-names>AA</given-names></name>, <name name-style="western"><surname>Fee</surname><given-names>MS</given-names></name> (<year>2002</year>) <article-title>An ultra-sparse code underlies the generation of neural sequences in a songbird</article-title>. <source>Nature</source> <volume>419</volume>: <fpage>65</fpage>–<lpage>70</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Yu1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yu</surname><given-names>AC</given-names></name>, <name name-style="western"><surname>Margoliash</surname><given-names>D</given-names></name> (<year>1996</year>) <article-title>Temporal hierarchical control of singing in birds</article-title>. <source>Science</source> <volume>273</volume>: <fpage>1871</fpage>–<lpage>1875</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Rabinovich1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rabinovich</surname><given-names>MI</given-names></name>, <name name-style="western"><surname>Varona</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Selverston</surname><given-names>AI</given-names></name>, <name name-style="western"><surname>Abarbanel</surname><given-names>HDI</given-names></name> (<year>2006</year>) <article-title>Dynamical principles in neuroscience</article-title>. <source>Reviews of Modern Physics</source> <volume>78</volume>: <fpage>1213</fpage>–<lpage>1265</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Barton1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barton</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Venezia</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>Saberi</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Hickok</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Brewer</surname><given-names>AA</given-names></name> (<year>2012</year>) <article-title>Orthogonal acoustic dimensions define auditory field maps in human cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>109</volume> (<issue>50</issue>) <fpage>20738</fpage>–<lpage>20743</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Afraimovich1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Afraimovich</surname><given-names>VS</given-names></name>, <name name-style="western"><surname>Rabinovich</surname><given-names>MI</given-names></name>, <name name-style="western"><surname>Varona</surname><given-names>P</given-names></name> (<year>2004</year>) <article-title>Heteroclinic contours in neural ensembles and the winnerless competition principle</article-title>. <source>International Journal of Bifurcation and Chaos</source> <volume>14</volume>: <fpage>1195</fpage>–<lpage>1208</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Rabinovich2"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rabinovich</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Volkovskii</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Lecanda</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Huerta</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Abarbanel</surname><given-names>HD</given-names></name>, <etal>et al</etal>. (<year>2001</year>) <article-title>Dynamical encoding by networks of competing neuron groups: winnerless competition</article-title>. <source>Physical Review Letters</source> <volume>87</volume>: <fpage>068102</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Varona1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Varona</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Rabinovich</surname><given-names>MI</given-names></name>, <name name-style="western"><surname>Selverston</surname><given-names>AI</given-names></name>, <name name-style="western"><surname>Arshavsky</surname><given-names>YI</given-names></name> (<year>2002</year>) <article-title>Winnerless competition between sensory neurons generates chaos: A possible mechanism for molluscan hunting behavior</article-title>. <source>Chaos</source> <volume>12</volume>: <fpage>672</fpage>–<lpage>677</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Afraimovich2"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Afraimovich</surname><given-names>VS</given-names></name>, <name name-style="western"><surname>Zhigulin</surname><given-names>VP</given-names></name>, <name name-style="western"><surname>Rabinovich</surname><given-names>MI</given-names></name> (<year>2004</year>) <article-title>On the origin of reproducible sequential activity in neural circuits</article-title>. <source>Chaos</source> <volume>14</volume>: <fpage>1123</fpage>–<lpage>1129</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Rabinovich3"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rabinovich</surname><given-names>MI</given-names></name>, <name name-style="western"><surname>Huerta</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Varona</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Afraimovich</surname><given-names>VS</given-names></name> (<year>2008</year>) <article-title>Transient cognitive dynamics, metastability, and decision making</article-title>. <source>Plos Computational Biology</source> <volume>4</volume> (<issue>5</issue>) <fpage>e1000072</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Fukai1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fukai</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Tanaka</surname><given-names>S</given-names></name> (<year>1997</year>) <article-title>A simple neural network exhibiting selective activation of neuronal ensembles: From winner-take-all to winners-share-all</article-title>. <source>Neural Computation</source> <volume>9</volume>: <fpage>77</fpage>–<lpage>97</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Hopfield1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name> (<year>1982</year>) <article-title>Neural Networks and Physical Systems with Emergent Collective Computational Abilities</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>79</volume>: <fpage>2554</fpage>–<lpage>2558</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Zheng1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zheng</surname><given-names>PS</given-names></name>, <name name-style="western"><surname>Tang</surname><given-names>WS</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>JX</given-names></name> (<year>2010</year>) <article-title>Efficient Continuous-Time Asymmetric Hopfield Networks for Memory Retrieval</article-title>. <source>Neural Computation</source> <volume>22</volume>: <fpage>1597</fpage>–<lpage>1614</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Robles1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Robles</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Ruggero</surname><given-names>MA</given-names></name> (<year>2001</year>) <article-title>Mechanics of the mammalian cochlea</article-title>. <source>Physiological Reviews</source> <volume>81</volume>: <fpage>1305</fpage>–<lpage>1352</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Lyon1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lyon</surname><given-names>RF</given-names></name> (<year>1982</year>) <article-title>A Computational Model of Filtering, Detection, and Compression in the Cochlea</article-title>. <source>Proceedings IEEE International Conference on Acoustics, Speech, and Signal Processing</source> <volume>7</volume>: <fpage>1282</fpage>–<lpage>1285</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Vanimmerseel1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vanimmerseel</surname><given-names>LM</given-names></name>, <name name-style="western"><surname>Martens</surname><given-names>JP</given-names></name> (<year>1992</year>) <article-title>Pitch and Voiced Unvoiced Determination with an Auditory Model</article-title>. <source>Journal of the Acoustical Society of America</source> <volume>91</volume>: <fpage>3511</fpage>–<lpage>3526</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Givelberg1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Givelberg</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Bunn</surname><given-names>J</given-names></name> (<year>2003</year>) <article-title>A comprehensive three-dimensional model of the cochlea</article-title>. <source>Journal of Computational Physics</source> <volume>191</volume>: <fpage>377</fpage>–<lpage>391</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Beyer1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beyer</surname><given-names>RP</given-names></name> (<year>1992</year>) <article-title>A Computational Model of the Cochlea Using the Immersed Boundary Method</article-title>. <source>Journal of Computational Physics</source> <volume>98</volume>: <fpage>145</fpage>–<lpage>162</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Patterson1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Patterson</surname><given-names>RD</given-names></name>, <name name-style="western"><surname>Allerhand</surname><given-names>MH</given-names></name>, <name name-style="western"><surname>Giguere</surname><given-names>C</given-names></name> (<year>1995</year>) <article-title>Time-Domain Modeling of Peripheral Auditory Processing - a Modular Architecture and a Software Platform</article-title>. <source>Journal of the Acoustical Society of America</source> <volume>98</volume>: <fpage>1890</fpage>–<lpage>1894</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Sumner1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sumner</surname><given-names>CJ</given-names></name>, <name name-style="western"><surname>Lopez-Poveda</surname><given-names>EA</given-names></name>, <name name-style="western"><surname>O'Mard</surname><given-names>LP</given-names></name>, <name name-style="western"><surname>Meddis</surname><given-names>R</given-names></name> (<year>2002</year>) <article-title>A revised model of the inner-hair cell and auditory-nerve complex</article-title>. <source>Journal of the Acoustical Society of America</source> <volume>111</volume>: <fpage>2178</fpage>–<lpage>2188</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Slaney1"><label>41</label>
<mixed-citation publication-type="other" xlink:type="simple">Slaney M (1998) Auditory Toolbox. Interval Research Corporation, Palo Alto, CA.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Friston2"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Trujillo-Barreto</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name> (<year>2008</year>) <article-title>DEM: A variational treatment of dynamic systems</article-title>. <source>Neuroimage</source> <volume>41</volume>: <fpage>849</fpage>–<lpage>885</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Friston3"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name> (<year>2008</year>) <article-title>Hierarchical Models in the Brain</article-title>. <source>Plos Computational Biology</source> <volume>4</volume>: <fpage>e1000211</fpage> <comment>doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/1000210.1001371/journal.pcbi.1000211" xlink:type="simple">1000210.1001371/journal.pcbi.1000211</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1003219-Mumford1"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mumford</surname><given-names>D</given-names></name> (<year>1992</year>) <article-title>ON THE COMPUTATIONAL ARCHITECTURE OF THE NEOCORTEX .2. THE ROLE OF CORTICOCORTICAL LOOPS</article-title>. <source>Biological Cybernetics</source> <volume>66</volume>: <fpage>241</fpage>–<lpage>251</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Fiser1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fiser</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Berkes</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Orban</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Lengyel</surname><given-names>M</given-names></name> (<year>2010</year>) <article-title>Statistically optimal perception and learning: from behavior to neural representations</article-title>. <source>Trends in Cognitive Sciences</source> <volume>14</volume>: <fpage>119</fpage>–<lpage>130</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Friston4"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name> (<year>2005</year>) <article-title>A theory of cortical responses</article-title>. <source>Philosophical Transactions of the Royal Society B-Biological Sciences</source> <volume>360</volume>: <fpage>815</fpage>–<lpage>836</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Bitzer1"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bitzer</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name> (<year>2012</year>) <article-title>Recognizing recurrent neural networks (rRNN): Bayesian inference for recurrent neural networks</article-title>. <source>Biological Cybernetics</source> <volume>106</volume>: <fpage>201</fpage>–<lpage>217</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Beauchemin1"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beauchemin</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Gonzalez-Frankenberger</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Tremblay</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Vannasing</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Martinez-Montes</surname><given-names>E</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Mother and Stranger: An Electrophysiological Study of Voice Processing in Newborns</article-title>. <source>Cerebral cortex</source> <volume>21</volume>: <fpage>1705</fpage>–<lpage>1711</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Kuhl1"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kuhl</surname><given-names>PK</given-names></name> (<year>2000</year>) <article-title>A new view of language acquisition</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>97</volume>: <fpage>11850</fpage>–<lpage>11857</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Hopfield2"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>IJ</given-names></name>, <name name-style="western"><surname>Brody</surname><given-names>CD</given-names></name> (<year>2000</year>) <article-title>What is a moment? “Cortical” sensory integration over a brief interval</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>97</volume>: <fpage>13919</fpage>–<lpage>13924</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Verstraeten2"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Verstraeten</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Schrauwen</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Stroobandt</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Van Campenhout</surname><given-names>J</given-names></name> (<year>2005</year>) <article-title>Isolated word recognition with the Liquid State Machine: a case study</article-title>. <source>Information Processing Letters</source> <volume>95</volume>: <fpage>521</fpage>–<lpage>528</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Adank1"><label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Adank</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Devlin</surname><given-names>JT</given-names></name> (<year>2010</year>) <article-title>On-line plasticity in spoken sentence comprehension: Adapting to time-compressed speech</article-title>. <source>Neuroimage</source> <volume>49</volume>: <fpage>1124</fpage>–<lpage>1132</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Miller1"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miller</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Grosjean</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Lomanto</surname><given-names>C</given-names></name> (<year>1984</year>) <article-title>Articulation Rate and Its Variability in Spontaneous Speech - a Reanalysis and Some Implications</article-title>. <source>Phonetica</source> <volume>41</volume>: <fpage>215</fpage>–<lpage>225</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Gutig1"><label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gutig</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>2009</year>) <article-title>Time-warp-invariant neuronal processing</article-title>. <source>Plos Biology</source> <volume>7</volume>: <fpage>e1000141</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Hemmen1"><label>55</label>
<mixed-citation publication-type="other" xlink:type="simple">Hemmen JLv, Sejnowski TJ (2006) How is time represented in the brain? Oxford ; New York: Oxford University Press. xvi, 514 p. p.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Bronkhorst1"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bronkhorst</surname><given-names>AW</given-names></name> (<year>2000</year>) <article-title>The cocktail party phenomenon: A review of research on speech intelligibility in multiple-talker conditions</article-title>. <source>Acustica</source> <volume>86</volume>: <fpage>117</fpage>–<lpage>128</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Cherry1"><label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cherry</surname><given-names>EC</given-names></name> (<year>1953</year>) <article-title>Some Experiments on the Recognition of Speech, with One and with 2 Ears</article-title>. <source>Journal of the Acoustical Society of America</source> <volume>25</volume>: <fpage>975</fpage>–<lpage>979</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-McDermott1"><label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McDermott</surname><given-names>JH</given-names></name> (<year>2009</year>) <article-title>The cocktail party problem</article-title>. <source>Current Biology</source> <volume>19</volume>: <fpage>R1024</fpage>–<lpage>R1027</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Mesgarani1"><label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mesgarani</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Chang</surname><given-names>EF</given-names></name> (<year>2012</year>) <article-title>Selective cortical representation of attended speaker in multi-talker speech perception</article-title>. <source>Nature</source> <volume>485</volume>: <fpage>233</fpage>–<lpage>U118</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Munro1"><label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Munro</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Derwing</surname><given-names>TM</given-names></name> (<year>1995</year>) <article-title>Foreign Accent, Comprehensibility, and Intelligibility in the Speech of 2nd-Language Learners</article-title>. <source>Language Learning</source> <volume>45</volume>: <fpage>73</fpage>–<lpage>97</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Munro2"><label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Munro</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Derwing</surname><given-names>TM</given-names></name> (<year>1995</year>) <article-title>Processing time, accent, and comprehensibility in the perception of native and foreign-accented speech</article-title>. <source>Language and Speech</source> <volume>38</volume>: <fpage>289</fpage>–<lpage>306</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Bradlow1"><label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bradlow</surname><given-names>AR</given-names></name>, <name name-style="western"><surname>Bent</surname><given-names>T</given-names></name> (<year>2008</year>) <article-title>Perceptual adaptation to non-native speech</article-title>. <source>Cognition</source> <volume>106</volume>: <fpage>707</fpage>–<lpage>729</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Clarke1"><label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clarke</surname><given-names>CM</given-names></name>, <name name-style="western"><surname>Garrett</surname><given-names>MF</given-names></name> (<year>2004</year>) <article-title>Rapid adaptation to foreign-accented English</article-title>. <source>Journal of the Acoustical Society of America</source> <volume>116</volume>: <fpage>3647</fpage>–<lpage>3658</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Birdsong1"><label>64</label>
<mixed-citation publication-type="other" xlink:type="simple">Birdsong D (1999) Second language acquisition and the critical period hypothesis. Mahwah, N.J.: Erlbaum. ix, 191 p. p.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Singleton1"><label>65</label>
<mixed-citation publication-type="other" xlink:type="simple">Singleton DM, Ryan L (2004) Language acquisition : the age factor. Clevedon ; Buffalo: Multilingual Matters. viii, 289 p. p.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Mayo1"><label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mayo</surname><given-names>LH</given-names></name>, <name name-style="western"><surname>Florentine</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Buus</surname><given-names>S</given-names></name> (<year>1997</year>) <article-title>Age of second-language acquisition and perception of speech in noise</article-title>. <source>Journal of Speech Language and Hearing Research</source> <volume>40</volume>: <fpage>686</fpage>–<lpage>693</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Meador1"><label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meador</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Flege</surname><given-names>JE</given-names></name>, <name name-style="western"><surname>Mackay</surname><given-names>IRA</given-names></name> (<year>2000</year>) <article-title>Factors affecting the recognition of words in a second language</article-title>. <source>Bilingualism: Language and Cognition</source> <volume>3</volume>: <fpage>55</fpage>–<lpage>67</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Jia1"><label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jia</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Strange</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Collado</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Guan</surname><given-names>Q</given-names></name> (<year>2006</year>) <article-title>Perception and production of English vowels by Mandarin speakers: Age-related differences vary with amount of L2 exposure</article-title>. <source>The Journal of the Acoustical Society of America</source> <volume>119</volume>: <fpage>1118</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Best1"><label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Best</surname><given-names>CT</given-names></name>, <name name-style="western"><surname>Tyler</surname><given-names>MD</given-names></name> (<year>2007</year>) <article-title>Nonnative and second-language speech perception: Commonalities and complementarities</article-title>. <source>Language experience in second language speech learning: In honor of James Emil Flege</source> <fpage>13</fpage>–<lpage>34</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Flege1"><label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Flege</surname><given-names>J</given-names></name>, <name name-style="western"><surname>MacKay</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Imai</surname><given-names>S</given-names></name> (<year>2010</year>) <article-title>What accounts for “age” effects on overall degree foreign accent?</article-title> <source>Wrembel, M Kul and K DziubalskaKolaczyk (eds) Achievements and Perspectives in SLA of Speech: New Sounds</source> <volume>2</volume>: <fpage>65</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Flege2"><label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Flege</surname><given-names>JE</given-names></name>, <name name-style="western"><surname>MacKay</surname><given-names>IR</given-names></name> (<year>2004</year>) <article-title>Perceiving vowels in a second language</article-title>. <source>Studies in second language acquisition</source> <volume>26</volume>: <fpage>1</fpage>–<lpage>34</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Okanoya1"><label>72</label>
<mixed-citation publication-type="other" xlink:type="simple">Okanoya K, Merker B (2006) Neural substrates for string-context mutual segmentation: a path to human language. In: Nehaniv CL, Cangelosi A, Lyon C, editors. Emergence of Communication and Language: Springer-Verlag. pp. 421–434.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Price1"><label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Price</surname><given-names>CJ</given-names></name> (<year>2010</year>) <article-title>The anatomy of language: a review of 100 fMRI studies published in 2009</article-title>. <source>Year in Cognitive Neuroscience 2010</source> <volume>1191</volume>: <fpage>62</fpage>–<lpage>88</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Abeles1"><label>74</label>
<mixed-citation publication-type="other" xlink:type="simple">Abeles M (1982) Local Cortical Circuits: An Electrophysiological study: Springer, Berlin.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Long1"><label>75</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Long</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Jin</surname><given-names>DZZ</given-names></name>, <name name-style="western"><surname>Fee</surname><given-names>MS</given-names></name> (<year>2010</year>) <article-title>Support for a synaptic chain model of neuronal sequence generation</article-title>. <source>Nature</source> <volume>468</volume>: <fpage>394</fpage>–<lpage>399</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-MacDonald1"><label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>MacDonald</surname><given-names>CJ</given-names></name>, <name name-style="western"><surname>Lepage</surname><given-names>KQ</given-names></name>, <name name-style="western"><surname>Eden</surname><given-names>UT</given-names></name>, <name name-style="western"><surname>Eichenbaum</surname><given-names>H</given-names></name> (<year>2011</year>) <article-title>Hippocampal “Time Cells” Bridge the Gap in Memory for Discontiguous Events</article-title>. <source>Neuron</source> <volume>71</volume>: <fpage>737</fpage>–<lpage>749</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Prut1"><label>77</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Prut</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Vaadia</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Bergman</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Haalman</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Slovin</surname><given-names>H</given-names></name>, <etal>et al</etal>. (<year>1998</year>) <article-title>Spatiotemporal structure of cortical activity: Properties and behavioral relevance</article-title>. <source>Journal of Neurophysiology</source> <volume>79</volume>: <fpage>2857</fpage>–<lpage>2874</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Pulvermuller1"><label>78</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pulvermuller</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Shtyrov</surname><given-names>Y</given-names></name> (<year>2009</year>) <article-title>Spatiotemporal Signatures of Large-Scale Synfire Chains for Speech Processing as Revealed by MEG</article-title>. <source>Cerebral cortex</source> <volume>19</volume>: <fpage>79</fpage>–<lpage>88</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Amador1"><label>79</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amador</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Sanz Perl</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Mindlin</surname><given-names>GB</given-names></name>, <name name-style="western"><surname>Margoliash</surname><given-names>D</given-names></name> (<year>2013</year>) <article-title>Elemental gesture dynamics are encoded by song premotor cortical neurons</article-title>. <source>Nature</source> <volume>495</volume>: <fpage>59</fpage>–<lpage>64</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Laje1"><label>80</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laje</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Gardner</surname><given-names>TJ</given-names></name>, <name name-style="western"><surname>Mindlin</surname><given-names>GB</given-names></name> (<year>2002</year>) <article-title>Neuromuscular control of vocalizations in birdsong: A model</article-title>. <source>Physical Review E</source> <volume>65</volume> (<issue>5; PART 1</issue>) <fpage>051921</fpage>–<lpage>051921</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Gaskell1"><label>81</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gaskell</surname><given-names>MG</given-names></name>, <name name-style="western"><surname>Marslen-Wilson</surname><given-names>WD</given-names></name> (<year>1997</year>) <article-title>Integrating form and meaning: A distributed model of speech perception</article-title>. <source>Language and Cognitive Processes</source> <volume>12</volume>: <fpage>613</fpage>–<lpage>656</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Kroger1"><label>82</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kroger</surname><given-names>BJ</given-names></name>, <name name-style="western"><surname>Kannampuzha</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Neuschaefer-Rube</surname><given-names>C</given-names></name> (<year>2009</year>) <article-title>Towards a neurocomputational model of speech production and perception</article-title>. <source>Speech Communication</source> <volume>51</volume>: <fpage>793</fpage>–<lpage>809</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Luce1"><label>83</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luce</surname><given-names>PA</given-names></name>, <name name-style="western"><surname>Goldinger</surname><given-names>SD</given-names></name>, <name name-style="western"><surname>Auer</surname><given-names>ET</given-names></name>, <name name-style="western"><surname>Vitevitch</surname><given-names>MS</given-names></name> (<year>2000</year>) <article-title>Phonetic priming, neighborhood activation, and PARSYN</article-title>. <source>Perception &amp; Psychophysics</source> <volume>62</volume>: <fpage>615</fpage>–<lpage>625</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Grossberg1"><label>84</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grossberg</surname><given-names>S</given-names></name> (<year>2003</year>) <article-title>Resonant neural dynamics of speech perception</article-title>. <source>Journal of Phonetics</source> <volume>31</volume>: <fpage>423</fpage>–<lpage>445</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Elhilali1"><label>85</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Elhilali</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Shamma</surname><given-names>SA</given-names></name> (<year>2008</year>) <article-title>A cocktail party with a cortical twist: How cortical mechanisms contribute to sound segregation</article-title>. <source>Journal of the Acoustical Society of America</source> <volume>124</volume>: <fpage>3751</fpage>–<lpage>3771</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Kawamoto1"><label>86</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kawamoto</surname><given-names>AH</given-names></name> (<year>1993</year>) <article-title>NONLINEAR DYNAMICS IN THE RESOLUTION OF LEXICAL AMBIGUITY - A PARALLEL DISTRIBUTED-PROCESSING ACCOUNT</article-title>. <source>Journal of Memory and Language</source> <volume>32</volume>: <fpage>474</fpage>–<lpage>516</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Seidenberg1"><label>87</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seidenberg</surname><given-names>MS</given-names></name>, <name name-style="western"><surname>McClelland</surname><given-names>JL</given-names></name> (<year>1989</year>) <article-title>A DISTRIBUTED, DEVELOPMENTAL MODEL OF WORD RECOGNITION AND NAMING</article-title>. <source>Psychological Review</source> <volume>96</volume>: <fpage>523</fpage>–<lpage>568</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-McClelland1"><label>88</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McClelland</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Mirman</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Holt</surname><given-names>LL</given-names></name> (<year>2006</year>) <article-title>Are there interactive processes in speech perception?</article-title> <source>Trends in Cognitive Sciences</source> <volume>10</volume>: <fpage>363</fpage>–<lpage>369</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Schrauwen1"><label>89</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schrauwen</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Buesing</surname><given-names>L</given-names></name> (<year>2009</year>) <article-title>A hierarchy of recurrent networks for speech recognition</article-title>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Jaegera1"><label>90</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jaegera</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Lukosevicius</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Popovici</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Siewert</surname><given-names>U</given-names></name> (<year>2007</year>) <article-title>Optimization and applications of echo state networks with leaky-integrator neurons</article-title>. <source>Neural Networks</source> <volume>20</volume>: <fpage>335</fpage>–<lpage>352</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Buonomano1"><label>91</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buonomano</surname><given-names>DV</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2009</year>) <article-title>State-dependent computations: spatiotemporal processing in cortical networks</article-title>. <source>Nature Reviews Neuroscience</source> <volume>10</volume>: <fpage>113</fpage>–<lpage>125</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Maass1"><label>92</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Natschlager</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Markram</surname><given-names>H</given-names></name> (<year>2002</year>) <article-title>Real-time computing without stable states: A new framework for neural computation based on perturbations</article-title>. <source>Neural Computation</source> <volume>14</volume>: <fpage>2531</fpage>–<lpage>2560</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Herrero1"><label>93</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Herrero</surname><given-names>JL</given-names></name>, <name name-style="western"><surname>Roberts</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Delicato</surname><given-names>LS</given-names></name>, <name name-style="western"><surname>Gieselmann</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>, <etal>et al</etal>. (<year>2008</year>) <article-title>Acetylcholine contributes through muscarinic receptors to attentional modulation in V1</article-title>. <source>Nature</source> <volume>454</volume>: <fpage>1110</fpage>–<lpage>1114</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Hirayama1"><label>94</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hirayama</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Yoshimoto</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Ishii</surname><given-names>S</given-names></name> (<year>2004</year>) <article-title>Bayesian representation learning in the cortex regulated by acetylcholine</article-title>. <source>Neural Networks</source> <volume>17</volume>: <fpage>1391</fpage>–<lpage>1400</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Yu2"><label>95</label>
<mixed-citation publication-type="other" xlink:type="simple">Yu AJ, Dayan P (2003) Expected and unexpected uncertainty: ACh and NE in the neocortex. Advances in Neural Information Processing Systems <volume>15</volume> : MIT Press, Cambridge, MA.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Hasselmo1"><label>96</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hasselmo</surname><given-names>ME</given-names></name>, <name name-style="western"><surname>McGaughy</surname><given-names>J</given-names></name> (<year>2004</year>) <article-title>High acetylcholine levels set circuit dynamics for attention and encoding and low acetylcholine levels set dynamics for consolidation</article-title>. <source>Acetylcholine in the Cerebral Cortex</source> <volume>145</volume>: <fpage>207</fpage>–<lpage>231</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Poeppel1"><label>97</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poeppel</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Emmorey</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Hickok</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Pylkkanen</surname><given-names>L</given-names></name> (<year>2012</year>) <article-title>Towards a New Neurobiology of Language</article-title>. <source>Journal of Neuroscience</source> <volume>32</volume>: <fpage>14125</fpage>–<lpage>14131</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Gagnepain1"><label>98</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gagnepain</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Henson</surname><given-names>RN</given-names></name>, <name name-style="western"><surname>Davis</surname><given-names>MH</given-names></name> (<year>2012</year>) <article-title>Temporal Predictive Codes for Spoken Words in Auditory Cortex</article-title>. <source>Current Biology</source> <volume>22</volume>: <fpage>615</fpage>–<lpage>621</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Friston5"><label>99</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Harrison</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Penny</surname><given-names>W</given-names></name> (<year>2003</year>) <article-title>Dynamic causal modelling</article-title>. <source>Neuroimage</source> <volume>19</volume>: <fpage>1273</fpage>–<lpage>1302</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Kiebel1"><label>100</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Garrido</surname><given-names>MI</given-names></name>, <name name-style="western"><surname>Moran</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>CC</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name> (<year>2009</year>) <article-title>Dynamic Causal Modeling for EEG and MEG</article-title>. <source>Human Brain Mapping</source> <volume>30</volume>: <fpage>1866</fpage>–<lpage>1876</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Wacongne1"><label>101</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wacongne</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Labyt</surname><given-names>E</given-names></name>, <name name-style="western"><surname>van Wassenhove</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Bekinschtein</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Naccache</surname><given-names>L</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Evidence for a hierarchy of predictions and prediction errors in human cortex</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>108</volume>: <fpage>20754</fpage>–<lpage>20759</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Friesen1"><label>102</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friesen</surname><given-names>LM</given-names></name>, <name name-style="western"><surname>Shannon</surname><given-names>RV</given-names></name>, <name name-style="western"><surname>Baskent</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>X</given-names></name> (<year>2001</year>) <article-title>Speech recognition in noise as a function of the number of spectral channels: Comparison of acoustic hearing and cochlear implants</article-title>. <source>Journal of the Acoustical Society of America</source> <volume>110</volume>: <fpage>1150</fpage>–<lpage>1163</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Zavaglia1"><label>103</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zavaglia</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Canolty</surname><given-names>RT</given-names></name>, <name name-style="western"><surname>Schofield</surname><given-names>TM</given-names></name>, <name name-style="western"><surname>Leff</surname><given-names>AP</given-names></name>, <name name-style="western"><surname>Ursino</surname><given-names>M</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>A dynamical pattern recognition model of gamma activity in auditory cortex</article-title>. <source>Neural Networks</source> <volume>28</volume>: <fpage>1</fpage>–<lpage>14</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Hanuschkin1"><label>104</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hanuschkin</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Diesmann</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Morrison</surname><given-names>A</given-names></name> (<year>2011</year>) <article-title>A reafferent and feed-forward model of song syntax generation in the Bengalese finch</article-title>. <source>Journal of Computational Neuroscience</source> <volume>31</volume>: <fpage>509</fpage>–<lpage>532</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Kiebel2"><label>105</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>von Kriegstein</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name> (<year>2009</year>) <article-title>Recognizing Sequences of Sequences</article-title>. <source>Plos Computational Biology</source> <volume>5</volume> (<issue>8</issue>) <fpage>e1000464</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Perdikis1"><label>106</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perdikis</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Huys</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Jirsa</surname><given-names>VK</given-names></name> (<year>2011</year>) <article-title>Time Scale Hierarchies in the Functional Organization of Complex Behaviors</article-title>. <source>Plos Computational Biology</source> <volume>7</volume> (<issue>9</issue>) <fpage>e1002198</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-beimGraben1"><label>107</label>
<mixed-citation publication-type="other" xlink:type="simple">beim Graben PP, R. (2012) A dynamic field account to language-related brain potentials. In: Rabinovich MI, Friston KJ, Varona P, editors. Principles of Brain Dynamics: Global State Interactions, MIT Press, Cambridge (MA).</mixed-citation>
</ref>
<ref id="pcbi.1003219-Winkler1"><label>108</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Winkler</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Denham</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Mill</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Bohm</surname><given-names>TM</given-names></name>, <name name-style="western"><surname>Bendixen</surname><given-names>A</given-names></name> (<year>2012</year>) <article-title>Multistability in auditory stream segregation: a predictive coding view</article-title>. <source>Philosophical Transactions of the Royal Society B-Biological Sciences</source> <volume>367</volume>: <fpage>1001</fpage>–<lpage>1012</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Feldman1"><label>109</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Feldman</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name> (<year>2010</year>) <article-title>Attention, uncertainty, and free-energy</article-title>. <source>Frontiers in Human Neuroscience</source> <volume>4</volume>: <fpage>215</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Friston6"><label>110</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>K</given-names></name> (<year>2008</year>) <article-title>Hierarchical models in the brain</article-title>. <source>PLoS Comput Biol</source> <volume>4</volume>: <fpage>e1000211</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003219-Graves1"><label>111</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Graves</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Eck</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Beringer</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Schmidhuber</surname><given-names>J</given-names></name> (<year>2004</year>) <article-title>Biologically plausible speech recognition with LSTM neural nets</article-title>. <source>Biologically Inspired Approaches to Advanced Information Technology</source> <volume>3141</volume>: <fpage>127</fpage>–<lpage>136</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>