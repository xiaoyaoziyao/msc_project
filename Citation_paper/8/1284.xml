<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-18-34683</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0214541</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cellular neuroscience</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject><subj-group><subject>Recurrent neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject><subj-group><subject>Recurrent neural networks</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Data visualization</subject><subj-group><subject>Phase diagrams</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Systems science</subject><subj-group><subject>Dynamical systems</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Systems science</subject><subj-group><subject>Dynamical systems</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuronal tuning</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Information technology</subject><subj-group><subject>Information processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Coding mechanisms</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Weight statistics controls dynamics in recurrent neural networks</article-title>
<alt-title alt-title-type="running-head">Structure and dynamics in neural networks</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6611-7733</contrib-id>
<name name-style="western">
<surname>Krauss</surname> <given-names>Patrick</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Schuster</surname> <given-names>Marc</given-names></name>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Dietrich</surname> <given-names>Verena</given-names></name>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Schilling</surname> <given-names>Achim</given-names></name>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Schulze</surname> <given-names>Holger</given-names></name>
<role content-type="http://credit.casrai.org/">Supervision</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-5709-4306</contrib-id>
<name name-style="western">
<surname>Metzner</surname> <given-names>Claus</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Cognitive Computational Neuroscience Group at the Chair of English Philology and Linguistics, Department of English and American Studies, Friedrich-Alexander University Erlangen-Nürnberg (FAU), Erlangen, Germany</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Experimental Otolaryngology, Neuroscience Group, University Hospital Erlangen, Friedrich-Alexander University Erlangen-Nürnberg (FAU), Erlangen, Germany</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Biophysics Group, Department of Physics, Friedrich-Alexander University Erlangen-Nürnberg (FAU), Erlangen, Germany</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Wennekers</surname> <given-names>Thomas</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Plymouth University, UNITED KINGDOM</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">patrick.krauss@uk-erlangen.de</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<year>2019</year>
</pub-date>
<pub-date pub-type="epub">
<day>9</day>
<month>4</month>
<year>2019</year>
</pub-date>
<volume>14</volume>
<issue>4</issue>
<elocation-id>e0214541</elocation-id>
<history>
<date date-type="received">
<day>3</day>
<month>12</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>14</day>
<month>3</month>
<year>2019</year>
</date>
</history>
<permissions>
<copyright-year>2019</copyright-year>
<copyright-holder>Krauss et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0214541"/>
<abstract>
<p>Recurrent neural networks are complex non-linear systems, capable of ongoing activity in the absence of driving inputs. The dynamical properties of these systems, in particular their long-time attractor states, are determined on the microscopic level by the connection strengths <italic>w</italic><sub><italic>ij</italic></sub> between the individual neurons. However, little is known to which extent network dynamics is tunable on a more coarse-grained level by the <italic>statistical</italic> features of the weight matrix. In this work, we investigate the dynamics of recurrent networks of Boltzmann neurons. In particular we study the impact of three statistical parameters: <italic>density</italic> (the fraction of non-zero connections), <italic>balance</italic> (the ratio of excitatory to inhibitory connections), and <italic>symmetry</italic> (the fraction of neuron pairs with <italic>w</italic><sub><italic>ij</italic></sub> = <italic>w</italic><sub><italic>ji</italic></sub>). By computing a ‘phase diagram’ of network dynamics, we find that balance is the essential control parameter: Its gradual increase from negative to positive values drives the system from oscillatory behavior into a chaotic regime, and eventually into stationary fixed points. Only directly at the border of the chaotic regime do the neural networks display rich but regular dynamics, thus enabling actual information processing. These results suggest that the brain, too, is fine-tuned to the ‘edge of chaos’ by assuring a proper balance between excitatory and inhibitory neural connections.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001659</institution-id>
<institution>Deutsche Forschungsgemeinschaft</institution>
</institution-wrap>
</funding-source>
<award-id>SCHU1272/12-1</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Schulze</surname> <given-names>Holger</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>PK received one Titan Xp GPU by the NVIDIA GPU Grant Program. AS received one Titan Xp GPU by the NVIDIA GPU Grant Program. HS was supported by the Deutsche Forschungsgemeinschaft (DFG, grant SCHU1272/12-1). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="4"/>
<table-count count="0"/>
<page-count count="13"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>In contrast to the artificial neural networks used in deep learning, which typically have a strict feed-forward structure, the networks of the brain contain many loops and are therefore recurrent in nature. This feature allows the cortex to maintain dynamical activity even without incoming external stimuli [<xref ref-type="bibr" rid="pone.0214541.ref001">1</xref>] and may therefore underlie such diverse operations as short-term memory [<xref ref-type="bibr" rid="pone.0214541.ref002">2</xref>–<xref ref-type="bibr" rid="pone.0214541.ref004">4</xref>], the modulation of neuronal excitability with attention [<xref ref-type="bibr" rid="pone.0214541.ref002">2</xref>, <xref ref-type="bibr" rid="pone.0214541.ref005">5</xref>, <xref ref-type="bibr" rid="pone.0214541.ref006">6</xref>], or the generation of spontaneous activity during sleep [<xref ref-type="bibr" rid="pone.0214541.ref007">7</xref>–<xref ref-type="bibr" rid="pone.0214541.ref009">9</xref>].</p>
<p>The relation between structure and function in recurrent neural networks is a topic of considerable interest in the neurosciences and has already been addressed in several important publications. In 1988, a transition from a stationary phase to a chaotic phase was predicted [<xref ref-type="bibr" rid="pone.0214541.ref010">10</xref>], occurring at a critical value of a ‘gain’ parameter that controls the width of the neuron’s sigmoidal function. In 2007, it was shown that Network structure of the cerebral cortex shapes functional connectivity on multiple time scales [<xref ref-type="bibr" rid="pone.0214541.ref011">11</xref>]. A paper from 2011 studied the effect of broad degree distributions on network dynamics by interpolating between a binomial and a truncated power-law distribution for the in-degree and out-degree of the neurons independently [<xref ref-type="bibr" rid="pone.0214541.ref012">12</xref>]. In 2014, it was demonstrated that (1) for homogeneous external input, the structure of pairwise correlations between neuron states is mainly determined by the local recurrent connectivity, that (2) homogeneous external inputs provide an additive, unspecific contribution to the correlations, that (3) inhibitory feedback effectively decorrelates neuronal activity, even if neurons receive identical external inputs, and, finally, that (4) identical synaptic input statistics to excitatory and to inhibitory cells increases intrinsically generated fluctuations and pairwise correlations [<xref ref-type="bibr" rid="pone.0214541.ref013">13</xref>]. In 2016, a paper analyzed the anatomical origins of oscillations in the cortical microcircuit. Based on a theoretical reduction of network dynamics, a sensitivity measure was derived, resulting in a frequency-dependent connectivity map that revealed connections crucial for the peak amplitude and frequency of the observed oscillations and identifies the minimal circuit generating a given frequency [<xref ref-type="bibr" rid="pone.0214541.ref014">14</xref>].</p>
<p>Recently, more sophisticated statistical properties of weight matrices were explored. For instance, a model introduced a structured component of connectivity, in addition to random connections, which effectively embeds a feed-forward structure via unidirectional coupling between a pair of orthogonal modes [<xref ref-type="bibr" rid="pone.0214541.ref015">15</xref>]. Another approach studied a class of recurrent network models in which the connectivity is a sum of a random part and a minimal, low-dimensional structure. It was shown that, in such networks, the dynamics is low dimensional and can be directly inferred from connectivity using a geometrical approach [<xref ref-type="bibr" rid="pone.0214541.ref016">16</xref>]. Still another approach focused on the eigenvalue structure of the weight matrix and identified structural properties of networks that are associated with non-normality [<xref ref-type="bibr" rid="pone.0214541.ref017">17</xref>]. A more coarse-grained approach showed that coupling among cortical modules is central. The highest dynamical richness of the network emerges at a critical connectivity at the verge of physical disconnection. Stronger coupling leads to a persistently coherent activity among the modules, while weaker coupling precipitates the activity to be localized solely within the modules [<xref ref-type="bibr" rid="pone.0214541.ref018">18</xref>]. Finally, the effect of external inputs on network dynamics was explored. A paper identified a general criterion that distinguishes two classes of networks depending on properties of the connectivity matrix: networks in which all inputs lead to weak, decaying transients, and networks in which specific inputs elicit strongly amplified transient responses and are mapped onto orthogonal output states during the dynamics [<xref ref-type="bibr" rid="pone.0214541.ref019">19</xref>].</p>
<p>Recent micro-anatomical studies of the brain revealed that neural connectivity in the mammalian cortex has unique statistical properties. In particular, it was found that connections are sparse (low density), so that only a small fraction of possible connections are realized. The distribution of connection strengths is close to log-normal, and thus highly skewed, with a fat tail towards large magnitudes [<xref ref-type="bibr" rid="pone.0214541.ref020">20</xref>, <xref ref-type="bibr" rid="pone.0214541.ref021">21</xref>]. Although the total number of non-zero connections can vary strongly between neurons, the ratio of excitatory to inhibitory connections is relatively constant [<xref ref-type="bibr" rid="pone.0214541.ref022">22</xref>]. Moreover, cortical networks contain a ‘skeleton’ of strongly connected neurons, linked pairwise in a bidirectional, symmetric way. This skeleton is embedded in a ‘sea’ of more weakly, non-symmetrically connected neurons [<xref ref-type="bibr" rid="pone.0214541.ref020">20</xref>].</p>
<p>Whereas the role of this peculiar connection structure is still poorly understood, certain features seem to affect whether the brain can properly act as an information processor. For example, it has been shown that recurrent neural networks can show chaotic behavior for certain ratios between excitatory and inhibitory connections [<xref ref-type="bibr" rid="pone.0214541.ref001">1</xref>, <xref ref-type="bibr" rid="pone.0214541.ref023">23</xref>]. It has even been speculated that certain social dysfunctions, such as autism and schizophrenia, are related to an elevated cortical excitation/inhibition balance [<xref ref-type="bibr" rid="pone.0214541.ref024">24</xref>]. Moreover, the discovered skeleton of neurons with strong bi-directional links may help to optimize information storage [<xref ref-type="bibr" rid="pone.0214541.ref025">25</xref>].</p>
<p>In a recent paper [<xref ref-type="bibr" rid="pone.0214541.ref026">26</xref>], we have investigated the relation between connectivity and system dynamics in small motifs of probabilistic neurons with binary outputs, assuming discrete, ternary connection strengths. We found that the balance between excitatory and inhibitory connections has a strong effect on the transition probabilities between successive motif states, whereas the total density of non-zero connections is less important.</p>
<p>Here, we extent our study to larger recurrent networks that consist of deterministic neurons with continuous outputs. Connection strength follow a random, log-normal weight distribution, but have prescribed values of the three control parameters density, balance, and symmetry. We analyze how these parameters affect the dynamical properties of the networks, in particular the Lyapunov exponent of the system trajectory in state space, the period length of cyclic attractors, and the cross correlation between individual neuron states.</p>
<p>As has been previously shown by Hopfield [<xref ref-type="bibr" rid="pone.0214541.ref027">27</xref>], networks with a very large fraction of symmetric bidirectional connections (symmetry parameter close to one) tend to end up in stationary fixed points. We therefore focus on moderate and small symmetry parameters, and explore the two-dimensional phase diagram of system dynamics as a function of balance and density.</p>
<p>We find that this two-dimensional phase plane consists of three basic regions, corresponding to the possible attractors in deterministic and autonomous dynamical systems: periodic state cycles, chaos, and stationary fixed point behavior. Strikingly, it is almost exclusively the balance parameter that controls in which of these three regimes a neural network is located, while the overall density of connections has a much weaker influence. In particular, the networks behave in a way that is suitable for information processing purposes only in a narrow range of balance parameters, located at the edge of the chaotic phase. This theoretical result is in line with the experimental finding that neural networks in the mammalian cortex have moderate degrees of symmetry and are tuned to rather specific values of balance, whereas connection density can vary widely between neurons and over time.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec003">
<title>Neural network model</title>
<p>Our neural networks are based on simple deterministic neurons with zero bias (zero threshold). The total input <italic>z</italic><sub><italic>i</italic></sub>(<italic>t</italic>) of neuron <italic>i</italic> at time <italic>t</italic> is calculated as:
<disp-formula id="pone.0214541.e001"><alternatives><graphic id="pone.0214541.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="normal">z</mml:mi> <mml:mi mathvariant="normal">i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="italic">t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mspace width="0.277778em"/><mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where <italic>y</italic><sub><italic>j</italic></sub>(<italic>t</italic> − 1) is the state of neuron <italic>j</italic> at time <italic>t</italic> − 1 and <italic>w</italic><sub><italic>ij</italic></sub> is the connection weight from neuron <italic>j</italic> to neuron <italic>i</italic>. The new state <italic>y</italic><sub><italic>i</italic></sub>(<italic>t</italic>) of neuron <italic>i</italic> is computed as
<disp-formula id="pone.0214541.e002"><alternatives><graphic id="pone.0214541.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mspace width="-0.166667em"/><mml:mo>+</mml:mo> <mml:mspace width="-0.166667em"/><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>−</mml:mo> <mml:msub><mml:mi>z</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
When simulating the dynamics of the networks, all neurons are updated simultaneously. The total state of a neural network at time step <italic>t</italic> can be summarized by the <italic>n</italic>-dimensional vector <inline-formula id="pone.0214541.e003"><alternatives><graphic id="pone.0214541.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>y</italic><sub><italic>i</italic></sub>(<italic>t</italic>) is the output state of neuron <italic>i</italic> at this time. In each particular case, we simulated <italic>T</italic><sub><italic>max</italic></sub> = 10<sup>6</sup> time steps.</p>
</sec>
<sec id="sec004">
<title>Random weight matrix</title>
<p>The structure of a given neural network is defined by its weight matrix <italic>W</italic> = {<italic>w</italic><sub><italic>ij</italic></sub>}. Here, we consider networks in which self-connections are forbidden, so that <italic>w</italic><sub><italic>ii</italic></sub> = 0. For all non-zero matrix elements, the magnitudes of the weights are distributed according to a log-normal distribution,
<disp-formula id="pone.0214541.e004"><alternatives><graphic id="pone.0214541.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∝</mml:mo> <mml:mtext>lognormal</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:mi>σ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
with location <italic>μ</italic> and scale <italic>σ</italic>.</p>
</sec>
<sec id="sec005">
<title>Statistical control parameters <inline-formula id="pone.0214541.e005"><alternatives><graphic id="pone.0214541.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula></title>
<p>For a network with <italic>n</italic> neurons, the weight matrix has dimensions <italic>n</italic> × <italic>n</italic>. Due to the excluded self-connections, the diagonal elements of this matrix are zero, leaving a maximum possible number <italic>n</italic>(<italic>n</italic> − 1) of non-zero matrix elements. We denote the actual number of non-zero weights by <italic>m</italic> = <italic>m</italic><sub>+</sub> + <italic>m</italic><sub>−</sub>, where <italic>m</italic><sub>+</sub> and <italic>m</italic><sub>−</sub> are the numbers of positive and negative weights, respectively. Furthermore, we denote the number of non-zero matrix elements <italic>w</italic><sub><italic>ij</italic></sub> for which a symmetric reverse connection <italic>w</italic><sub><italic>ji</italic></sub> = <italic>w</italic><sub><italic>ij</italic></sub> exists by <italic>m</italic><sub><italic>s</italic></sub>. Based on these numbers, we define the <italic>density</italic> parameter <inline-formula id="pone.0214541.e006"><alternatives><graphic id="pone.0214541.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mover accent="true"><mml:mi>d</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, which varies between 0 for an unconnected and 1 for a fully connected network, by
<disp-formula id="pone.0214541.e007"><alternatives><graphic id="pone.0214541.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e007" xlink:type="simple"/><mml:math display="block" id="M7"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi>m</mml:mi> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
The <italic>balance</italic> parameter <inline-formula id="pone.0214541.e008"><alternatives><graphic id="pone.0214541.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, which varies between −1 for a purely inhibitory and +1 for a purely excitatory connection matrix, is defined by
<disp-formula id="pone.0214541.e009"><alternatives><graphic id="pone.0214541.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>m</mml:mi> <mml:mo>+</mml:mo></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>m</mml:mi> <mml:mo>−</mml:mo></mml:msub></mml:mrow> <mml:mi>m</mml:mi></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
The <italic>symmetry</italic> parameter <inline-formula id="pone.0214541.e010"><alternatives><graphic id="pone.0214541.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, which varies between 0 for a completely non-symmetric and +1 for a completely symmetric (Hopfield-like [<xref ref-type="bibr" rid="pone.0214541.ref027">27</xref>]) network, is defined by
<disp-formula id="pone.0214541.e011"><alternatives><graphic id="pone.0214541.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>m</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mi>m</mml:mi></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
The meaning of these three control parameters is visualized in <xref ref-type="fig" rid="pone.0214541.g001">Fig 1</xref>.</p>
<fig id="pone.0214541.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0214541.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Visualization of the control parameters density <inline-formula id="pone.0214541.e012"><alternatives><graphic id="pone.0214541.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mover accent="true"><mml:mi>d</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, balance <inline-formula id="pone.0214541.e013"><alternatives><graphic id="pone.0214541.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, and symmetry <inline-formula id="pone.0214541.e014"><alternatives><graphic id="pone.0214541.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> for eight example networks (A-H).</title>
<p>Neurons are represented by gray circles, non-zero connections between neurons by arrows. One-headed arrows stand for uni-directional, two-headed arrows for bi-directional connections. Blue/magenta connections are excitatory (<italic>w</italic><sub><italic>ij</italic></sub> &gt; 0), red/orange connections inhibitory (<italic>w</italic><sub><italic>ij</italic></sub> &lt; 0).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0214541.g001" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec006">
<title>Generation of weight matrices</title>
<p>Random weight matrices with prescribed values of the parameters <inline-formula id="pone.0214541.e015"><alternatives><graphic id="pone.0214541.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mover accent="true"><mml:mi>d</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0214541.e016"><alternatives><graphic id="pone.0214541.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, and <inline-formula id="pone.0214541.e017"><alternatives><graphic id="pone.0214541.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> were generated in a series of steps. First, a fraction <inline-formula id="pone.0214541.e018"><alternatives><graphic id="pone.0214541.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mover accent="true"><mml:mi>d</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> of the weights were drawn independently from a log-normal distribution with location <italic>μ</italic> = 0 and scale <italic>σ</italic> = 1, whereas all remaining weights were set to zero. Second, in order to introduce inhibitory connections to the network, a fraction <inline-formula id="pone.0214541.e019"><alternatives><graphic id="pone.0214541.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>−</mml:mo> <mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> of the non-zero-weights were randomly selected and multiplied by −1. For all cases with symmetry <inline-formula id="pone.0214541.e020"><alternatives><graphic id="pone.0214541.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, this produced already the final weight matrix.</p>
<p>For the case <inline-formula id="pone.0214541.e021"><alternatives><graphic id="pone.0214541.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>&gt;</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, further steps were required: the weights above the diagonal of the weight matrix were copied to below the diagonal, thereby creating a perfectly symmetric matrix. Finally, pairs of matrix elements below the diagonal were randomly selected and swapped iteratively, until the desired degree of symmetry <inline-formula id="pone.0214541.e022"><alternatives><graphic id="pone.0214541.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> was achieved.</p>
</sec>
<sec id="sec007">
<title>Fraction of positive Lyapunov exponents <italic>f</italic><sub>λ&gt;0</sub></title>
<p>Computing the new network state <inline-formula id="pone.0214541.e023"><alternatives><graphic id="pone.0214541.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> from the previous state <inline-formula id="pone.0214541.e024"><alternatives><graphic id="pone.0214541.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mspace width="-0.166667em"/><mml:mo>−</mml:mo> <mml:mspace width="-0.166667em"/><mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> can be formally described by a vectorial update function
<disp-formula id="pone.0214541.e025"><alternatives><graphic id="pone.0214541.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mi>F</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
Starting from an infinitesimally close initial state <inline-formula id="pone.0214541.e026"><alternatives><graphic id="pone.0214541.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>*</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mspace width="-0.166667em"/><mml:mo>−</mml:mo> <mml:mspace width="-0.166667em"/><mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mspace width="-0.166667em"/><mml:mo>−</mml:mo> <mml:mspace width="-0.166667em"/><mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mover accent="true"><mml:mi>ϵ</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> will lead to a different final state <inline-formula id="pone.0214541.e027"><alternatives><graphic id="pone.0214541.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>*</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mi>F</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>*</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mspace width="-0.166667em"/><mml:mo>−</mml:mo> <mml:mspace width="-0.166667em"/><mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The sensitivity of the update function to this infinitesimal perturbation can be measured by the differential quotient
<disp-formula id="pone.0214541.e028"><alternatives><graphic id="pone.0214541.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e028" xlink:type="simple"/><mml:math display="block" id="M28"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>F</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mspace width="0.166667em"/><mml:mo>′</mml:mo></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>≈</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>*</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:mover accent="true"><mml:mi>ϵ</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
The <italic>maximum Lyapunov coefficient</italic> λ of the update function is defined as
<disp-formula id="pone.0214541.e029"><alternatives><graphic id="pone.0214541.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e029" xlink:type="simple"/><mml:math display="block" id="M29"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>λ</mml:mo> <mml:mo>=</mml:mo><mml:mo>⟨</mml:mo> <mml:mo form="prefix">ln</mml:mo> <mml:mo>|</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>F</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mspace width="0.166667em"/><mml:mo>′</mml:mo></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>|</mml:mo> <mml:msub><mml:mo>⟩</mml:mo> <mml:mi>t</mml:mi></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
where 〈〉<sub><italic>t</italic></sub> denotes the time average over all successive states of the system. It can be computed using well-established algorithms [<xref ref-type="bibr" rid="pone.0214541.ref028">28</xref>, <xref ref-type="bibr" rid="pone.0214541.ref029">29</xref>]. A positive Lyapunov coefficient λ &gt; 0 indicates that two nearby points in state space diverge exponentially, thus leading to irregular (chaotic) behavior. A zero or negative λ ≤ 0 indicates regular behavior. In general, within an ensemble of networks that are all characterized by the same set of control parameters <inline-formula id="pone.0214541.e030"><alternatives><graphic id="pone.0214541.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>d</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, one finds λ-values of both signs. We therefore counted the fraction of networks (within the statistical ensemble of the considered parameter combination) that had a positive Lyapunov exponent. This fraction <italic>f</italic><sub>λ&gt;0</sub> is represented as a color code and shown for all paramater combinations in the ‘phase diagrams’ below. Note that, the so-defined quantity <italic>f</italic><sub>λ&gt;0</sub> reveals a transition at around 50%.</p>
</sec>
<sec id="sec008">
<title>Average period length <italic>T</italic><sub>av</sub></title>
<p>Our recurrent networks are deterministic and autonomous dynamical systems. Thus, their trajectory <inline-formula id="pone.0214541.e031"><alternatives><graphic id="pone.0214541.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> through <italic>n</italic>-dimensional state space is eventually governed by one of three possible attractors: a stationary fixed point, a cycle of period <italic>T</italic>, or chaotic behavior. For each investigated network, we characterize the type of attractor by the measured period length <italic>T</italic>, that is, the number of time steps before the system state repeats itself for the first time (<inline-formula id="pone.0214541.e032"><alternatives><graphic id="pone.0214541.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mi>y</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mi>T</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>). This includes the special cases <italic>T</italic> = 1, corresponding to a stationary fixed point, and <italic>T</italic> = ∞, corresponding to a chaotic attractor. To identify repeating system states, we make use of a hash table. Since period lengths fluctuate for different networks from the same ensemble <inline-formula id="pone.0214541.e033"><alternatives><graphic id="pone.0214541.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>d</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, we compute the ensemble average <italic>T</italic><sub>av</sub> and use this average for color coding the phase diagrams.</p>
</sec>
<sec id="sec009">
<title>Root mean square of cross correlations <italic>ρ</italic><sub>rms</sub></title>
<p>The Lyapunov coefficient λ and the period length <italic>T</italic> characterize the long-time behavior of the neural networks. Another property that is relevant for a network’s information processing ability is the degree of correlation between individual neuron states <italic>y</italic><sub><italic>i</italic></sub>(<italic>t</italic>) at the same time step <italic>t</italic>. For each pair <italic>i</italic>, <italic>j</italic> of neurons, it can be quantified by the Pearson cross correlation coefficient, defined as
<disp-formula id="pone.0214541.e034"><alternatives><graphic id="pone.0214541.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e034" xlink:type="simple"/><mml:math display="block" id="M34"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:msub><mml:mover><mml:mi>y</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>−</mml:mo> <mml:msub><mml:mover><mml:mi>y</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo> <mml:mo>⟩</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msub> <mml:mrow><mml:msub><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
where <inline-formula id="pone.0214541.e035"><alternatives><graphic id="pone.0214541.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:msub><mml:mover><mml:mi>y</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>k</mml:mi></mml:msub></mml:math></alternatives></inline-formula> is the temporal mean of the time series <italic>y</italic><sub><italic>k</italic></sub>(<italic>t</italic>) and <italic>σ</italic><sub><italic>k</italic></sub> its standard deviation. In cases where <italic>σ</italic><sub><italic>i</italic></sub> or <italic>σ</italic><sub><italic>j</italic></sub> were zero, <italic>ρ</italic><sub><italic>i</italic>,<italic>j</italic></sub> was set to 1. To characterize the global degree of correlation in a given neural network (without caring about the sign of the individual <italic>ρ</italic><sub><italic>ij</italic></sub>), we computed the root mean square (RMS) over all neuron pairs
<disp-formula id="pone.0214541.e036"><alternatives><graphic id="pone.0214541.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e036" xlink:type="simple"/><mml:math display="block" id="M36"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi> <mml:mtext>rms</mml:mtext></mml:msub> <mml:mo>=</mml:mo> <mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:msup><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mfrac> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:munder> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>ρ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
This quantity was additionally averaged over all members of a given <inline-formula id="pone.0214541.e037"><alternatives><graphic id="pone.0214541.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>d</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> ensemble and then used for color coding the phase diagrams.</p>
</sec>
</sec>
<sec id="sec010" sec-type="results">
<title>Results</title>
<p>We first consider non-symmetric networks (<inline-formula id="pone.0214541.e038"><alternatives><graphic id="pone.0214541.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>), that is, networks without any bidirectional links of exactly the same strength. For each combination of balance <inline-formula id="pone.0214541.e039"><alternatives><graphic id="pone.0214541.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> and density <inline-formula id="pone.0214541.e040"><alternatives><graphic id="pone.0214541.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:mover accent="true"><mml:mi>d</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> on a 11 × 10 regular grid, we generate an ensemble of 100 random networks. We then simulate the temporal dynamics of these networks, starting from random initial states. For each ensemble, we compute the fraction of positive Lyapunov coefficients <italic>f</italic><sub>λ&gt;0</sub>, the average period length <italic>T</italic><sub>av</sub>, and the RMS of cross correlations <italic>ρ</italic><sub>rms</sub>. The dependence of these dynamical quantities on the statistical control parameters is presented in the form of heat maps, which can be interpreted as dynamical ‘phase diagrams’ of these recurrent neural networks.</p>
<p>We initially focus on small networks of 100 neurons. When keeping the density close to one and gradually increasing the balance from negative to positive values, we find that the fraction of positive Lyapunov coefficients <italic>f</italic><sub>λ&gt;0</sub>, indicating chaotic behavior, is close to zero, except for a narrow interval of balance values around <inline-formula id="pone.0214541.e041"><alternatives><graphic id="pone.0214541.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e041" xlink:type="simple"/><mml:math display="inline" id="M41"><mml:mrow><mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>≈</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. This chaotic interval broadens as the density parameter is reduced (<xref ref-type="fig" rid="pone.0214541.g002">Fig 2A</xref>). In the 2D phase diagram, the chaotic regime therefore has an approximately triangular shape.</p>
<fig id="pone.0214541.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0214541.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Dynamical phases in recurrent neural networks and characteristic output signals of individual neurons.</title>
<p>(A) Two-dimensional phase diagram, showing the fraction of positive Lyapunov exponents <inline-formula id="pone.0214541.e042"><alternatives><graphic id="pone.0214541.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e042" xlink:type="simple"/><mml:math display="inline" id="M42"><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mrow><mml:mo>λ</mml:mo> <mml:mo>&gt;</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:msub> <mml:mspace width="0.277778em"/><mml:mrow><mml:mo>(</mml:mo> <mml:mspace width="0.277778em"/><mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>d</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mspace width="0.277778em"/><mml:mo>|</mml:mo> <mml:mspace width="0.277778em"/><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mspace width="-0.166667em"/><mml:mo>=</mml:mo> <mml:mspace width="-0.166667em"/><mml:mn>0</mml:mn> <mml:mspace width="0.277778em"/><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> as a function of the control parameters balance and density, for a constant symmetry parameter <inline-formula id="pone.0214541.e043"><alternatives><graphic id="pone.0214541.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mspace width="-0.166667em"/><mml:mo>=</mml:mo> <mml:mspace width="-0.166667em"/><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> (Note that in part G, we show the average Lyapunov exponent along a a one-dimensional cut through the 2D phase space of balance and density, for constant density 0.2). In the heat map, dark blue colors indicate <italic>f</italic><sub>λ&gt;0</sub> ≈ 0, dark red colors <italic>f</italic><sub>λ&gt;0</sub> ≈ 1. The red region in the center of the phase diagram is the chaotic regime, consistent with the irregular outputs of selected neurons (F). The ‘left’ blue region at negative balance values is the regime of cyclic attractors, often with small period lengths <italic>T</italic> ≈ 2, as demonstrated with the neuron output (B). The ‘right’ blue region at positive balance values is the regime of fixed points, as exemplified with the constant neuron output (D). Note that, in both cases (B, D) the fraction of positive Lyapunov exponents is zero since the dynamics is non-chaotic, and hence the color coding is identical in both cases. The most interesting dynamics is found at the edges of the chaotic regime (C, E), where one finds cases of periodic behavior with large period length <italic>T</italic> &gt; 2, periodic behavior with intermittent bursts, decaying oscillatory behavior, and ‘beating’ oscillatory behavior. Note that, the sampled time traces depicted in the figure are from selected neurons, not necessarily from within the same network.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0214541.g002" xlink:type="simple"/>
</fig>
<p>Inspecting the temporal output signals of selected neurons in the investigated networks (<xref ref-type="fig" rid="pone.0214541.g002">Fig 2B–2G</xref>), it turns out that the two regimes with <italic>f</italic><sub>λ&gt;0</sub> ≈ 0 at the ‘left’ and ‘right’ side of the chaotic regime correspond to periodic attractors (<xref ref-type="fig" rid="pone.0214541.g002">Fig 2B</xref>) and fixed point attractors (<xref ref-type="fig" rid="pone.0214541.g002">Fig 2D</xref>), respectively. The most interesting dynamics is found at the edge of the chaotic regime (<xref ref-type="fig" rid="pone.0214541.g002">Fig 2C and 2E</xref>), where one finds cases of periodic behavior with large period length <italic>T</italic> &gt; 2, periodic behavior with intermittent bursts, decaying oscillatory behavior, and ‘beating’ oscillatory behavior.</p>
<p>In a next step, we compare the phase distribution of <italic>f</italic><sub>λ&gt;0</sub> with that of the other two dynamical quantities (middle and right column in <xref ref-type="fig" rid="pone.0214541.g003">Fig 3</xref>). At the same time, we investigate the effect of system size (rows in <xref ref-type="fig" rid="pone.0214541.g003">Fig 3</xref>, with different numbers of neurons <italic>N</italic>).</p>
<fig id="pone.0214541.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0214541.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Comparing different dynamical measures, and the effect of system size.</title>
<p>The columns correspond to the quantities <italic>f</italic><sub>λ&gt;0</sub> (left), <italic>T</italic><sub>av</sub> (middle) and <italic>ρ</italic><sub>rms</sub> (right), as defined in the methods section. The rows from top to bottom correspond to increasing system sizes, characterized by the number of neurons <italic>N</italic> in the neural networks. For each of the 12 cases, a two-dimensional phase diagram is shown as a function of balance and density, keeping a constant symmetry parameter of <inline-formula id="pone.0214541.e044"><alternatives><graphic id="pone.0214541.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e044" xlink:type="simple"/><mml:math display="inline" id="M44"><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. The three dynamic phases become apparent only for systems with a minimum size of <italic>N</italic> ≥ 100. The three different dynamical measures are mutually consistent. In particular, the chaotic regime is characterized by a <italic>f</italic><sub>λ&gt;0</sub> close to one, by a diverging <italic>T</italic><sub>av</sub>, and by a vanishing <italic>ρ</italic><sub>rms</sub>. For large systems with <italic>N</italic> ≥ 10000, the density parameter has no more effect on the system dynamics, which is then controlled by the balance only.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0214541.g003" xlink:type="simple"/>
</fig>
<p>We find that the three different dynamical quantities are mutually consistent. In particular, the chaotic regime is characterized by <italic>f</italic><sub>λ&gt;0</sub> ≈ 1, by a diverging <italic>T</italic><sub>av</sub>, and by a vanishing <italic>ρ</italic><sub>RMS</sub>. The periodic regime is characterized by <italic>T</italic><sub>av</sub> ≈ 2 and by a relatively large <italic>ρ</italic><sub>rms</sub>. The fixed point regime is characterized by <italic>T</italic><sub>av</sub> = 1 and, again, by a relatively large <italic>ρ</italic><sub>rms</sub>. Approaching the chaotic regime from either side by changing the balance parameter, <italic>T</italic><sub>av</sub> is rapidly increasing in the border region.</p>
<p>With increasing system size, the influence of the density parameter on the dynamical phase of the networks is diminishing. For large networks with <italic>N</italic> ≥ 1000 neurons, the network dynamics is exclusively controlled by the balance parameter.</p>
<p>Finally, we investigate the effect of the symmetry parameter on the network dynamics (<xref ref-type="fig" rid="pone.0214541.g004">Fig 4</xref>). By computing a complete 3D phase diagram of <italic>f</italic><sub>λ&gt;0</sub> as a function of all three statistical control parameters, we find that balance and density have only an effect on the system dynamics when the symmetry is smaller than one, that is, when there are sufficiently many non-symmetric connections between the neurons. For a too large symmetry <inline-formula id="pone.0214541.e045"><alternatives><graphic id="pone.0214541.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>≈</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, the system ends up in fixed point attractors, irrespective of balance and density.</p>
<fig id="pone.0214541.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0214541.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Effect of symmetry <inline-formula id="pone.0214541.e046"><alternatives><graphic id="pone.0214541.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> on system dynamics.</title>
<p>(A): Standard plot of <italic>f</italic><sub>λ&gt;0</sub> as a function of balance and density, for constant symmetry <inline-formula id="pone.0214541.e047"><alternatives><graphic id="pone.0214541.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e047" xlink:type="simple"/><mml:math display="inline" id="M47"><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. (B): Plot of <italic>f</italic><sub>λ&gt;0</sub> as a function of balance and symmetry, for constant density <inline-formula id="pone.0214541.e048"><alternatives><graphic id="pone.0214541.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:mrow><mml:mover accent="true"><mml:mi>d</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>5</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> (see orange box in (A)). (C): Plot of <italic>f</italic><sub>λ&gt;0</sub> as a function of symmetry and density, for constant balance <inline-formula id="pone.0214541.e049"><alternatives><graphic id="pone.0214541.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:mrow><mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> (see green box in (A)). For too large symmetry <inline-formula id="pone.0214541.e050"><alternatives><graphic id="pone.0214541.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e050" xlink:type="simple"/><mml:math display="inline" id="M50"><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>≈</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>, the system ends up in fixed point attractors, irrespective of balance and density. Note that the phase diagram shown in (A) is the same as shown in <xref ref-type="fig" rid="pone.0214541.g003">Fig 3D</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0214541.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec011" sec-type="conclusions">
<title>Conclusion</title>
<p>One of the earliest studies [<xref ref-type="bibr" rid="pone.0214541.ref010">10</xref>] dealing with dynamical regimes in recurrent neural networks found that a transition from regular to chaotic behavior can be induced by increasing the nonlinearity of the neuron’s sigmoidal function. In this work, we have demonstrated that the dynamical behaviour of recurrent neural networks can be effectively tuned by certain statistical properties of the network’s connection weight matrix.</p>
<p>In particular, a large fraction of symmetric, bi-directional neural connections (<inline-formula id="pone.0214541.e051"><alternatives><graphic id="pone.0214541.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e051" xlink:type="simple"/><mml:math display="inline" id="M51"><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>≈</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>) favors fixed point attractors, and may therefore be useful for pattern completion tasks, as in the Hopfield model [<xref ref-type="bibr" rid="pone.0214541.ref027">27</xref>]. However, rich dynamical behavior is only possible for moderate or small degrees of symmetry. We point out, that besides our definition of the symmetry parameter, there are alternative definitions which are more fine grained. For instance, the symmetry definition by Esposito et al. [<xref ref-type="bibr" rid="pone.0214541.ref030">30</xref>] takes into account the magnitudes of the forward and backward connections, and vanishes in the case when only uni-directional connections exist. In addition, future extensions of our work could improve our present method to generate weight matrices with pre-defined symmetry, as the present method may create spurious correlations of weight magnitudes between the upper and lower triangle.</p>
<p>For non-symmetric networks, the statistical parameter with the largest impact on system dynamics is the balance <inline-formula id="pone.0214541.e052"><alternatives><graphic id="pone.0214541.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> [<xref ref-type="bibr" rid="pone.0214541.ref026">26</xref>]. This ratio between excitatory and inhibitory connections controls, with high fidelity, whether a free-running neural network will behave stationary, oscillatory, or irregularly. Moreover, fine tuning of the balance parameter can bring the system to the edge of the chaotic regime, where the outputs of the neurons produce complex wave forms, and where the system may depend sensibly, but still regularly, on external inputs. We speculate that this regime is most suitable for purposes of neural information processing [<xref ref-type="bibr" rid="pone.0214541.ref031">31</xref>–<xref ref-type="bibr" rid="pone.0214541.ref035">35</xref>], and that biological brains may therefore control the parameter <inline-formula id="pone.0214541.e053"><alternatives><graphic id="pone.0214541.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e053" xlink:type="simple"/><mml:math display="inline" id="M53"><mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> in a homeostatic way [<xref ref-type="bibr" rid="pone.0214541.ref001">1</xref>, <xref ref-type="bibr" rid="pone.0214541.ref036">36</xref>, <xref ref-type="bibr" rid="pone.0214541.ref037">37</xref>].</p>
<p>By contrast, the impact of the overall connection density <inline-formula id="pone.0214541.e054"><alternatives><graphic id="pone.0214541.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e054" xlink:type="simple"/><mml:math display="inline" id="M54"><mml:mover accent="true"><mml:mi>d</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> on network dynamics, at least in realistically large systems with many neurons, is much smaller than that of the balance <inline-formula id="pone.0214541.e055"><alternatives><graphic id="pone.0214541.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e055" xlink:type="simple"/><mml:math display="inline" id="M55"><mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>. In principle, a recurrent neural network can gain or loose a large random fraction of neural connections without changing its dynamical attractor state, as long as the balance <inline-formula id="pone.0214541.e056"><alternatives><graphic id="pone.0214541.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0214541.e056" xlink:type="simple"/><mml:math display="inline" id="M56"><mml:mover accent="true"><mml:mi>b</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> remains unchanged. This surprising robustness, for which the term graceful degradation has been coined [<xref ref-type="bibr" rid="pone.0214541.ref038">38</xref>], may help to keep the cortex functional during periods of growth and decay.</p>
<p>In this work we abstracted from biological detail in that each given neuron could have both, positive and negative output weights. By contrast, in the human brain the vast majority of neurons is either purely excitatory or purely inhibitory (Dale’s principle), although there are prominent exceptions to this rule, such as the dopaminergic transmission within the basal ganglia [<xref ref-type="bibr" rid="pone.0214541.ref039">39</xref>]. Nevertheless, it might be worthwhile to explore the impact of Dale’s principle on networks dynamics.</p>
<p>Future work will also need to clarify how recurrent neural networks, statistically tuned into specific attractor states, react to external inputs. A particularly interesting question will be whether the edge of chaos is also marked by a large mutual information between input signals and the internal sequence of states within the recurrent neural network. Furthermore, Wernecke et al. [<xref ref-type="bibr" rid="pone.0214541.ref040">40</xref>] proposed a method to test for partially predictable chaos, which might be applied to derive a more fine-grained description of the chaotic regime, including the edges of chaos.</p>
</sec>
</body>
<back>
<ack>
<p>This work was supported by the Deutsche Forschungsgemeinschaft (DFG, grant SCHU1272/12-1). The authors are grateful for the donation of two Titan Xp GPUs by the NVIDIA Corporation.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0214541.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Shu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Hasenstaub</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>McCormick</surname> <given-names>DA</given-names></name>. <article-title>Turning on and off recurrent balanced cortical activity</article-title>. <source>Nature</source>. <year>2003</year>;<volume>423</volume>(<issue>6937</issue>):<fpage>288</fpage>–<lpage>293</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature01616" xlink:type="simple">10.1038/nature01616</ext-link></comment> <object-id pub-id-type="pmid">12748642</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Grossberg</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Stone</surname> <given-names>G</given-names></name>. <article-title>Neural dynamics of attention switching and temporal-order information in short-term memory</article-title>. <source>Memory &amp; Cognition</source>. <year>1986</year>;<volume>14</volume>(<issue>6</issue>):<fpage>451</fpage>–<lpage>468</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BF03202517" xlink:type="simple">10.3758/BF03202517</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0214541.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Stopfer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Laurent</surname> <given-names>G</given-names></name>. <article-title>Short-term memory in olfactory network dynamics</article-title>. <source>Nature</source>. <year>1999</year>;<volume>402</volume>(<issue>6762</issue>):<fpage>664</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/45244" xlink:type="simple">10.1038/45244</ext-link></comment> <object-id pub-id-type="pmid">10604472</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kopell</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Whittington</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Kramer</surname> <given-names>M</given-names></name>. <article-title>Neuronal assembly dynamics in the beta1 frequency range permits short-term memory</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2011</year>; p. 201019676. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1019676108" xlink:type="simple">10.1073/pnas.1019676108</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0214541.ref005">
<label>5</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Carpenter</surname> <given-names>GA</given-names></name>, <name name-style="western"><surname>Grossberg</surname> <given-names>S</given-names></name>. <chapter-title>Neural dynamics of category learning and recognition: Attention, memory consolidation, and amnesia</chapter-title>. In: <source>Advances in psychology</source>. <volume>vol. 42</volume>. <publisher-name>Elsevier</publisher-name>; <year>1987</year>. p. <fpage>239</fpage>–<lpage>286</lpage>.</mixed-citation>
</ref>
<ref id="pone.0214541.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Buschman</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Kastner</surname> <given-names>S</given-names></name>. <article-title>From behavior to neural dynamics: an integrated theory of attention</article-title>. <source>Neuron</source>. <year>2015</year>;<volume>88</volume>(<issue>1</issue>):<fpage>127</fpage>–<lpage>144</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2015.09.017" xlink:type="simple">10.1016/j.neuron.2015.09.017</ext-link></comment> <object-id pub-id-type="pmid">26447577</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Babloyantz</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Salazar</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Nicolis</surname> <given-names>C</given-names></name>. <article-title>Evidence of chaotic dynamics of brain activity during the sleep cycle</article-title>. <source>Physics letters A</source>. <year>1985</year>;<volume>111</volume>(<issue>3</issue>):<fpage>152</fpage>–<lpage>156</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0375-9601(85)90444-X" xlink:type="simple">10.1016/0375-9601(85)90444-X</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0214541.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kavanau</surname> <given-names>JL</given-names></name>. <article-title>Sleep and dynamic stabilization of neural circuitry: a review and synthesis</article-title>. <source>Behavioural brain research</source>. <year>1994</year>;<volume>63</volume>(<issue>2</issue>):<fpage>111</fpage>–<lpage>126</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0166-4328(94)90082-5" xlink:type="simple">10.1016/0166-4328(94)90082-5</ext-link></comment> <object-id pub-id-type="pmid">7999294</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Watson</surname> <given-names>BO</given-names></name>, <name name-style="western"><surname>Levenstein</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Greene</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Gelinas</surname> <given-names>JN</given-names></name>, <name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>. <article-title>Network homeostasis and state dynamics of neocortical sleep</article-title>. <source>Neuron</source>. <year>2016</year>;<volume>90</volume>(<issue>4</issue>):<fpage>839</fpage>–<lpage>852</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2016.03.036" xlink:type="simple">10.1016/j.neuron.2016.03.036</ext-link></comment> <object-id pub-id-type="pmid">27133462</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Crisanti</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sommers</surname> <given-names>HJ</given-names></name>. <article-title>Chaos in random neural networks</article-title>. <source>Physical review letters</source>. <year>1988</year>;<volume>61</volume>(<issue>3</issue>):<fpage>259</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevLett.61.259" xlink:type="simple">10.1103/PhysRevLett.61.259</ext-link></comment> <object-id pub-id-type="pmid">10039285</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Honey</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Kötter</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Breakspear</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sporns</surname> <given-names>O</given-names></name>. <article-title>Network structure of cerebral cortex shapes functional connectivity on multiple time scales</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2007</year>;<volume>104</volume>(<issue>24</issue>):<fpage>10240</fpage>–<lpage>10245</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0701519104" xlink:type="simple">10.1073/pnas.0701519104</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0214541.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Roxin</surname> <given-names>A</given-names></name>. <article-title>The role of degree distribution in shaping the dynamics in networks of sparsely connected spiking neurons</article-title>. <source>Frontiers in computational neuroscience</source>. <year>2011</year>;<volume>5</volume>:<fpage>8</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fncom.2011.00008" xlink:type="simple">10.3389/fncom.2011.00008</ext-link></comment> <object-id pub-id-type="pmid">21556129</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Helias</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Tetzlaff</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Diesmann</surname> <given-names>M</given-names></name>. <article-title>The correlation structure of local neuronal networks intrinsically results from recurrent dynamics</article-title>. <source>PLoS computational biology</source>. <year>2014</year>;<volume>10</volume>(<issue>1</issue>):<fpage>e1003428</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1003428" xlink:type="simple">10.1371/journal.pcbi.1003428</ext-link></comment> <object-id pub-id-type="pmid">24453955</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bos</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Diesmann</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Helias</surname> <given-names>M</given-names></name>. <article-title>Identifying anatomical origins of coexisting oscillations in the cortical microcircuit</article-title>. <source>PLoS computational biology</source>. <year>2016</year>;<volume>12</volume>(<issue>10</issue>):<fpage>e1005132</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1005132" xlink:type="simple">10.1371/journal.pcbi.1005132</ext-link></comment> <object-id pub-id-type="pmid">27736873</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref015">
<label>15</label>
<mixed-citation publication-type="other" xlink:type="simple">Landau ID, Sompolinsky H. Coherent chaos in a recurrent neural network with structured connectivity. bioRxiv. 2018; p. 350801.</mixed-citation>
</ref>
<ref id="pone.0214541.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mastrogiuseppe</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Ostojic</surname> <given-names>S</given-names></name>. <article-title>Linking connectivity, dynamics, and computations in low-rank recurrent neural networks</article-title>. <source>Neuron</source>. <year>2018</year>;<volume>99</volume>(<issue>3</issue>):<fpage>609</fpage>–<lpage>623</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2018.07.003" xlink:type="simple">10.1016/j.neuron.2018.07.003</ext-link></comment> <object-id pub-id-type="pmid">30057201</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Asllani</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lambiotte</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Carletti</surname> <given-names>T</given-names></name>. <article-title>Structure and dynamical behavior of non-normal networks</article-title>. <source>Science advances</source>. <year>2018</year>;<volume>4</volume>(<issue>12</issue>):<fpage>eaau9403</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/sciadv.aau9403" xlink:type="simple">10.1126/sciadv.aau9403</ext-link></comment> <object-id pub-id-type="pmid">30547090</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yamamoto</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Moriya</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ide</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Hayakawa</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Akima</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Sato</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Impact of modular organization on dynamical richness in cortical networks</article-title>. <source>Science advances</source>. <year>2018</year>;<volume>4</volume>(<issue>11</issue>):<fpage>eaau4914</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/sciadv.aau4914" xlink:type="simple">10.1126/sciadv.aau4914</ext-link></comment> <object-id pub-id-type="pmid">30443598</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref019">
<label>19</label>
<mixed-citation publication-type="other" xlink:type="simple">Bondanelli G, Ostojic S. Coding with transient trajectories in recurrent neural networks. arXiv preprint arXiv:181107592. 2018.</mixed-citation>
</ref>
<ref id="pone.0214541.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Song</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sjöström</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Reigl</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name>. <article-title>Highly nonrandom features of synaptic connectivity in local cortical circuits</article-title>. <source>PLoS biology</source>. <year>2005</year>;<volume>3</volume>(<issue>3</issue>):<fpage>0507</fpage>–<lpage>0519</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.0030068" xlink:type="simple">10.1371/journal.pbio.0030068</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0214541.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Buzsáki</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Mizuseki</surname> <given-names>K</given-names></name>. <article-title>The log-dynamic brain: how skewed distributions affect network operations</article-title>. <source>Nature reviews Neuroscience</source>. <year>2014</year>;<volume>15</volume>(<issue>4</issue>):<fpage>264</fpage>–<lpage>78</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn3687" xlink:type="simple">10.1038/nrn3687</ext-link></comment> <object-id pub-id-type="pmid">24569488</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gal</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>London</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Globerson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ramaswamy</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Reimann</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Muller</surname> <given-names>E</given-names></name>, <etal>et al</etal>. <source>Rich cell-type-specific network topology in neocortical microcircuitry</source>. <year>2017</year>;(<issue>June</issue>).</mixed-citation>
</ref>
<ref id="pone.0214541.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>van Vreeswijk</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>. <article-title>Chaos in neuronal networks with balanced excitatory and inhibitory activity</article-title>. <source>Science (New York, NY)</source>. <year>1996</year>;<volume>274</volume>(<issue>5293</issue>):<fpage>1724</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.274.5293.1724" xlink:type="simple">10.1126/science.274.5293.1724</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0214541.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yizhar</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Fenno</surname> <given-names>LE</given-names></name>, <name name-style="western"><surname>Prigge</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schneider</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Davidson</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>O’shea</surname> <given-names>DJ</given-names></name>, <etal>et al</etal>. <article-title>Neocortical excitation/inhibition balance in information processing and social dysfunction</article-title>. <source>Nature</source>. <year>2011</year>;<volume>477</volume>(<issue>7363</issue>):<fpage>171</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nature10360" xlink:type="simple">10.1038/nature10360</ext-link></comment> <object-id pub-id-type="pmid">21796121</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>. <article-title>Is cortical connectivity optimized for storing information?</article-title> <source>Nature Neuroscience</source>. <year>2016</year>;<volume>19</volume>(<issue>5</issue>):<fpage>749</fpage>–<lpage>755</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4286" xlink:type="simple">10.1038/nn.4286</ext-link></comment> <object-id pub-id-type="pmid">27065365</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Krauss</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Zankl</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schilling</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schulze</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Metzner</surname> <given-names>C</given-names></name>. <article-title>Analysis of structure and dynamics in three-neuron motifs</article-title>. <source>Frontiers in Computational Neuroscience</source>. <year>2019</year>;<volume>13</volume>:<fpage>5</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fncom.2019.00005" xlink:type="simple">10.3389/fncom.2019.00005</ext-link></comment> <object-id pub-id-type="pmid">30792635</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hopfield</surname> <given-names>JJ</given-names></name>. <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proceedings of the national academy of sciences</source>. <year>1982</year>;<volume>79</volume>(<issue>8</issue>):<fpage>2554</fpage>–<lpage>2558</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.79.8.2554" xlink:type="simple">10.1073/pnas.79.8.2554</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0214541.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wolf</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Swift</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Swinney</surname> <given-names>HL</given-names></name>, <name name-style="western"><surname>Vastano</surname> <given-names>JA</given-names></name>. <article-title>Determining Lyapunov exponents from a time series</article-title>. <source>Physica D: Nonlinear Phenomena</source>. <year>1985</year>;<volume>16</volume>(<issue>3</issue>):<fpage>285</fpage>–<lpage>317</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0167-2789(85)90011-9" xlink:type="simple">10.1016/0167-2789(85)90011-9</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0214541.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rosenstein</surname> <given-names>MT</given-names></name>, <name name-style="western"><surname>Collins</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>De Luca</surname> <given-names>CJ</given-names></name>. <article-title>A practical method for calculating largest Lyapunov exponents from small data sets</article-title>. <source>Physica D: Nonlinear Phenomena</source>. <year>1993</year>;<volume>65</volume>(<issue>1-2</issue>):<fpage>117</fpage>–<lpage>134</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0167-2789(93)90009-P" xlink:type="simple">10.1016/0167-2789(93)90009-P</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0214541.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Esposito</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Giugliano</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Van Rossum</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Vasilaki</surname> <given-names>E</given-names></name>. <article-title>Measuring symmetry, asymmetry and randomness in neural network connectivity</article-title>. <source>PloS one</source>. <year>2014</year>;<volume>9</volume>(<issue>7</issue>):<fpage>e100805</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0100805" xlink:type="simple">10.1371/journal.pone.0100805</ext-link></comment> <object-id pub-id-type="pmid">25006663</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Skarda</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Freeman</surname> <given-names>WJ</given-names></name>. <article-title>How brains make chaos in order to make sense of the world</article-title>. <source>Behavioral and brain sciences</source>. <year>1987</year>;<volume>10</volume>(<issue>2</issue>):<fpage>161</fpage>–<lpage>173</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0140525X00047336" xlink:type="simple">10.1017/S0140525X00047336</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0214541.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schiff</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Jerger</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Duong</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Spano</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Ditto</surname> <given-names>WL</given-names></name>. <article-title>Controlling chaos in the brain</article-title>. <source>Nature</source>. <year>1994</year>;<volume>370</volume>(<issue>6491</issue>):<fpage>615</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/370615a0" xlink:type="simple">10.1038/370615a0</ext-link></comment> <object-id pub-id-type="pmid">8065447</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Birbaumer</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Flor</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Lutzenberger</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Elbert</surname> <given-names>T</given-names></name>. <article-title>Chaos and order in the human brain</article-title>. <source>Electroencephalography and Clinical Neurophysiology/Supplement</source>. <year>1995</year>;<volume>44</volume>:<fpage>450</fpage>–<lpage>459</lpage>.</mixed-citation>
</ref>
<ref id="pone.0214541.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chialvo</surname> <given-names>DR</given-names></name>. <article-title>Critical brain networks</article-title>. <source>Physica A: Statistical Mechanics and its Applications</source>. <year>2004</year>;<volume>340</volume>(<issue>4</issue>):<fpage>756</fpage>–<lpage>765</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.physa.2004.05.064" xlink:type="simple">10.1016/j.physa.2004.05.064</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0214541.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chialvo</surname> <given-names>DR</given-names></name>. <article-title>Emergent complex neural dynamics</article-title>. <source>Nature physics</source>. <year>2010</year>;<volume>6</volume>(<issue>10</issue>):<fpage>744</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nphys1803" xlink:type="simple">10.1038/nphys1803</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0214541.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Effenberger</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Jost</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Levina</surname> <given-names>A</given-names></name>. <article-title>Self-organization in balanced state networks by STDP and homeostatic plasticity</article-title>. <source>PLoS computational biology</source>. <year>2015</year>;<volume>11</volume>(<issue>9</issue>):<fpage>e1004420</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1004420" xlink:type="simple">10.1371/journal.pcbi.1004420</ext-link></comment> <object-id pub-id-type="pmid">26335425</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wilting</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Dehning</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Pinheiro Neto</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Rudelt</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Wibral</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Zierenberg</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Operating in a reverberating regime enables rapid tuning of network states to task requirements</article-title>. <source>Frontiers in Systems Neuroscience</source>. <year>2018</year>;<volume>12</volume>:<fpage>55</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnsys.2018.00055" xlink:type="simple">10.3389/fnsys.2018.00055</ext-link></comment> <object-id pub-id-type="pmid">30459567</object-id></mixed-citation>
</ref>
<ref id="pone.0214541.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rolls</surname> <given-names>ET</given-names></name>, <name name-style="western"><surname>Treves</surname> <given-names>A</given-names></name>. <article-title>The relative advantages of sparse versus distributed encoding for associative neuronal networks in the brain</article-title>. <source>Network: computation in neural systems</source>. <year>1990</year>;<volume>1</volume>(<issue>4</issue>):<fpage>407</fpage>–<lpage>421</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1088/0954-898X_1_4_002" xlink:type="simple">10.1088/0954-898X_1_4_002</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0214541.ref039">
<label>39</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Kandel</surname> <given-names>ER</given-names></name>, <name name-style="western"><surname>Schwartz</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Jessell</surname> <given-names>TM</given-names></name>, of Biochemistry D, <name name-style="western"><surname>Jessell</surname> <given-names>MBT</given-names></name>, <name name-style="western"><surname>Siegelbaum</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <source>Principles of neural science</source>. <volume>vol. 4</volume>. <publisher-name>McGraw-hill</publisher-name> <publisher-loc>New York</publisher-loc>; <year>2000</year>.</mixed-citation>
</ref>
<ref id="pone.0214541.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wernecke</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Sándor</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Gros</surname> <given-names>C</given-names></name>. <article-title>How to test for partially predictable chaos</article-title>. <source>Scientific reports</source>. <year>2017</year>;<volume>7</volume>(<issue>1</issue>):<fpage>1087</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41598-017-01083-x" xlink:type="simple">10.1038/s41598-017-01083-x</ext-link></comment> <object-id pub-id-type="pmid">28439074</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>