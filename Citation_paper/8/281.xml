<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pone.0120838</article-id>
<article-id pub-id-type="publisher-id">PONE-D-14-37020</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Menage a Quoi? Optimal Number of Peer Reviewers</article-title>
<alt-title alt-title-type="running-head">Optimal Number of Peer Reviewers</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Snell</surname>
<given-names>Richard R.</given-names>
</name>
<xref rid="cor001" ref-type="corresp">*</xref>
<xref rid="aff001" ref-type="aff"/>
</contrib>
</contrib-group>
<aff id="aff001"><addr-line>Research Knowledge Translation and Ethics Portfolio, Canadian Institutes of Health Research, Ottawa, Ontario, Canada</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Gray</surname>
<given-names>Clive M.</given-names>
</name>
<role>Academic Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Cape Town, SOUTH AFRICA</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The author has declared that no competing interests exist. The views presented in this article are those of the author and do not necessarily represent those of the Canadian Institutes of Health Research.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: RRS. Performed the experiments: RRS. Analyzed the data: RRS. Contributed reagents/materials/analysis tools: RRS. Wrote the paper: RRS.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">richard.snell@cihr-irsc.gc.ca</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>1</day>
<month>4</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="collection">
<year>2015</year>
</pub-date>
<volume>10</volume>
<issue>4</issue>
<elocation-id>e0120838</elocation-id>
<history>
<date date-type="received">
<day>18</day>
<month>8</month>
<year>2014</year>
</date>
<date date-type="accepted">
<day>7</day>
<month>2</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Crown Copyright</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open-access article distributed under the terms of the free Open Government Licence, which permits unrestricted use, distribution and reproduction in any medium, provided the original author and source are credited. See: <ext-link ext-link-type="uri" xlink:href="http://www.nationalarchives.gov.uk/doc/open-government-licence/open-government-licence.htm" xlink:type="simple">http://www.nationalarchives.gov.uk/doc/open-government-licence/open-government-licence.htm</ext-link></license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0120838" xlink:type="simple"/>
<abstract>
<p>Peer review represents the primary mechanism used by funding agencies to allocate financial support and by journals to select manuscripts for publication, yet recent Cochrane reviews determined literature on peer review best practice is sparse. Key to improving the process are reduction of inherent vulnerability to high degree of randomness and, from an economic perspective, limiting both the substantial indirect costs related to reviewer time invested and direct administrative costs to funding agencies, publishers and research institutions. Use of additional reviewers per application may increase reliability and decision consistency, but adds to overall cost and burden. The optimal number of reviewers per application, while not known, is thought to vary with accuracy of judges or evaluation methods. Here I use bootstrapping of replicated peer review data from a Post-doctoral Fellowships competition to show that five reviewers per application represents a practical optimum which avoids large random effects evident when fewer reviewers are used, a point where additional reviewers at increasing cost provides only diminishing incremental gains in chance-corrected consistency of decision outcomes. Random effects were most evident in the relative mid-range of competitiveness. Results support aggressive high- and low-end stratification or triaging of applications for subsequent stages of review, with the proportion and set of mid-range submissions to be retained for further consideration being dependent on overall success rate.</p>
</abstract>
<funding-group>
<funding-statement>The author has no support or funding to report.</funding-statement>
</funding-group>
<counts>
<fig-count count="5"/>
<table-count count="1"/>
<page-count count="14"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Peer review of proposals submitted to agencies for funding support and manuscripts submitted to journals for publication has been termed “a cornerstone of science” [<xref rid="pone.0120838.ref001" ref-type="bibr">1</xref>], “the cornerstone of quality assurance” [<xref rid="pone.0120838.ref002" ref-type="bibr">2</xref>], “the most commonly used method for evaluating scientific research” [<xref rid="pone.0120838.ref003" ref-type="bibr">3</xref>] and “the gold standard for evaluating scientific merit” [<xref rid="pone.0120838.ref004" ref-type="bibr">4</xref>]. In contrast, recent Cochrane reviews determined literature on evidence-based best practice in peer review is sparse [<xref rid="pone.0120838.ref005" ref-type="bibr">5</xref>,<xref rid="pone.0120838.ref006" ref-type="bibr">6</xref>]. Typically, most research funding agency peer review involves a contextual framework [<xref rid="pone.0120838.ref007" ref-type="bibr">7</xref>] of committee-based evaluation processes and, increasingly, fewer or no face-to-face meetings [<xref rid="pone.0120838.ref008" ref-type="bibr">8</xref>] to assess proposals and inform ‘yes-no’ decisions for research support [<xref rid="pone.0120838.ref009" ref-type="bibr">9</xref>]. Reported limitations to peer review include low inter-rater reliability [<xref rid="pone.0120838.ref003" ref-type="bibr">3</xref>,<xref rid="pone.0120838.ref010" ref-type="bibr">10</xref>,<xref rid="pone.0120838.ref011" ref-type="bibr">11</xref>], low reproducibility [<xref rid="pone.0120838.ref012" ref-type="bibr">12</xref>–<xref rid="pone.0120838.ref014" ref-type="bibr">14</xref>], low predictive validity [<xref rid="pone.0120838.ref015" ref-type="bibr">15</xref>–<xref rid="pone.0120838.ref017" ref-type="bibr">17</xref>], potentially limited impact from discussion on scores or outcomes [<xref rid="pone.0120838.ref004" ref-type="bibr">4</xref>,<xref rid="pone.0120838.ref008" ref-type="bibr">8</xref>], potential bias [<xref rid="pone.0120838.ref018" ref-type="bibr">18</xref>–<xref rid="pone.0120838.ref020" ref-type="bibr">20</xref>], potential conservatism [<xref rid="pone.0120838.ref021" ref-type="bibr">21</xref>,<xref rid="pone.0120838.ref022" ref-type="bibr">22</xref>] and risk aversion [<xref rid="pone.0120838.ref016" ref-type="bibr">16</xref>,<xref rid="pone.0120838.ref018" ref-type="bibr">18</xref>,<xref rid="pone.0120838.ref022" ref-type="bibr">22</xref>,<xref rid="pone.0120838.ref023" ref-type="bibr">23</xref>]. However, proposed alternatives to peer review such as sandpit methods or workshop review [<xref rid="pone.0120838.ref024" ref-type="bibr">24</xref>], community-based evaluation [<xref rid="pone.0120838.ref025" ref-type="bibr">25</xref>], or collective reallocation [<xref rid="pone.0120838.ref026" ref-type="bibr">26</xref>], have only been rarely attempted, reflecting (at least in part) a general lack of evidence of the effectiveness or efficiency of alternatives and also the difficulty of shifting the prevailing decision paradigm for peer review [<xref rid="pone.0120838.ref015" ref-type="bibr">15</xref>].</p>
<p>The overall number of proposals submitted for potential support is increasing for most research funding agencies [<xref rid="pone.0120838.ref009" ref-type="bibr">9</xref>,<xref rid="pone.0120838.ref027" ref-type="bibr">27</xref>], and funders are increasingly overburdened by workload and complexity of the review process. With more applications, there is concomitant growing pressure for the recruitment of more peer reviewers, wherein reviewer participation appears largely driven by motives related to reciprocity, social norms within the scientific community [<xref rid="pone.0120838.ref028" ref-type="bibr">28</xref>], scientific quality control, communal obligation and self—interest [<xref rid="pone.0120838.ref029" ref-type="bibr">29</xref>]. However, despite these strong motives, peer reviewers are becoming a scarce resource: the work is generally unpaid or recompensed with only modest honoraria, and many potential recruits also cite the dual burden of conflict with other work plus lack of time [<xref rid="pone.0120838.ref030" ref-type="bibr">30</xref>] as legitimate reasons to not participate in peer review. Total cost and burden of peer review is directly proportional to the number of reviewers assigned per application. Thus, determining the minimum number of reviewers per application that will inform reliable and consistent decisions, especially given the practical difficulty of increasing the reviewer count per application, is a key operational consideration for research funders, reviewers and applicants alike.</p>
<p>Little is known about variation in decision outcome with incremental increase in numbers of reviewers across a range of applications submitted to actual competition. Bootstrapped estimates of outcomes, using retrospective data from an Australian grants competition [<xref rid="pone.0120838.ref023" ref-type="bibr">23</xref>], provided evidence that the proportion of proposals “sometimes” funded was lowest with larger simulated committees, leading to the conclusion that “larger panels are better than smaller ones.” Estimates of up to 38,416 reviews per grant application to distinguish peer review scores at an acceptable level of precision [<xref rid="pone.0120838.ref031" ref-type="bibr">31</xref>], suggested unrealistic numbers of reviewers would be required in an ideal peer review system.</p>
<p>Here I use bootstrapping of replicate reviewer scores, from a Canadian Institutes of Health Research (CIHR) spring 2013 post-doctoral Fellowships competition for biomedical applicants. Consistency was estimated in decision outcomes with incremental increases in number of reviewers per applications. Guidelines are proposed for the minimum number of reviewers required per application and for triaging depending on competition success rate.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec003">
<title>Data source, rating methods and peer review</title>
<p>Prior to assignment of reviewers to applications, a subset of 100 applications (of 406 in the competition) was randomly selected for replicate review. CIHR’s standard peer review methodology for Fellowships competition, as of early 2013, included assignment of each application to 3 reviewers (referred to herein as a reviewer triad), randomly selected from all available committee members subject to the following constraints: exclusion of conflicts of interest, balancing reviewer workload and limiting the number of assignments per reviewer, and language matching (i.e., applications to CIHR may be in French or English and unilingual reviewers received applications only in their own language). For this competition, an additional constraint was used: exclusion of reviewers from more than one triad for any single application (no reviewer reviewed any application more than once).</p>
<p>Each of the 100 applications was reviewed by three randomly assigned reviewer triads (subject to the constraints outlined above), producing nine independent “at—home” pre—scores per application (which comprised the source data for this study). Reviewers scored multiple criteria on applications, on a scale of 0 [least competitive] to 4.9 [most competitive] (i.e., 50 possible states). Reviewers provided written evaluations and pre—scores through “ResearchNet” (CIHR’s Internet—enabled peer reviewer interface). Following completion of peer review, pre—score data (averaged by reviewer, for each application) were extracted from CIHR’s electronic database (anonymized data provided, <xref rid="pone.0120838.s001" ref-type="supplementary-material">S1 Data</xref>).</p>
<p>Although referred to as a “committee,” the pool of reviewers in this Fellowships competition participated entirely through ResearchNet with no face-to-face meeting of the committee as a whole. This review process entailed a series of discrete reviews by non—associated reviewer triads, some with overlapping memberships, that otherwise did not interact. After each triad member submitted their pre-scores and reviews through ResearchNet, the input of their other triad members became accessible. Reviewers were offered the option of eDiscussion through an electronic asynchronous discussion tool, for the exchange of comments, sharing of reviews and display of each other’s scores. Not all triads engaged in eDiscussion. Not all reviewers read the reviews of other reviewers. Final scores were submitted following the eDiscussion stage, adjusted as each reviewer deemed appropriate on the full range of the scale.</p>
<p>This study was based on independent pre-scores rather than final scores informed either by eDiscussion within reviewer triads or the opportunity to read the reviews of other reviewers in the triad. Previous study of CIHR Fellowships competition [<xref rid="pone.0120838.ref008" ref-type="bibr">8</xref>] provided evidence that “[…] committee discussion and rating of proposals offered no improvement to fairness and effectiveness over and above that attainable from the pre-meeting evaluations." Similar to previous results [<xref rid="pone.0120838.ref008" ref-type="bibr">8</xref>], ca. 70% of final scores provided by reviewers were unchanged from their initial pre-scores. Outcomes based on pre-scores are strong predictors of final outcomes.</p>
</sec>
<sec id="sec004">
<title>Data Manipulation</title>
<p>For each bootstrap iteration, pre-score data were resampled with replacement, for each application, to simulate 20 step-wise increments of N<sub>reviewers</sub> (i.e., N from 1 to 21). At each iteration, for each application and N<sub>reviewer</sub> combination, each set of resampled scores (from 1 to N) was averaged and converted to a percentile rank within the 100 applications. Applications with tied average scores, as determined in each iteration from resampled data, shared their maximum percentile rank. Within each increment of N<sub>reviewers</sub>, percentile rank of each application was used to determine the modeled binary competition outcome (i.e., success or not) by comparison with thresholds that represented each of five success rate scenarios (i.e., the top 5%, 15%, 25%, 35% and 50% of applications were assigned ‘1’ rather than zero). These five scenarios spanned the range of success rates commonly encountered at research funding agencies (historically, for higher success rates). The pre-score resampling process was repeated over 10,000 iterations, generating a cumulative distribution of success probability for each application across increments of N<sub>reviewers</sub>, for each success rate scenario. Success probability distributions of the 100 applications, for each success scenario, were collectively represented as 3D histograms.</p>
<p>To assess the impact on overall chance-corrected decision consistency, as a result of incremental increases to N<sub>reviewers</sub>, Cohen’s kappa [<xref rid="pone.0120838.ref032" ref-type="bibr">32</xref>] and significance levels were calculated across binary competition outcomes of the 100 applications, for each pairing of N and N+1 resampled reviewers, within each success rate scenario. To reduce sampling bias in bootstrapped estimates of kappa and significance, and to enable calculation of Monte Carlo error level [<xref rid="pone.0120838.ref033" ref-type="bibr">33</xref>], kappa statistics were recalculated (10,000 iterations). R version 3.1.0 and 3.1.1 [<xref rid="pone.0120838.ref034" ref-type="bibr">34</xref>] was used for calculations, data resampling, bootstrapping and graphs (R code for analyses and graphics provided, <xref rid="pone.0120838.s002" ref-type="supplementary-material">S1 File</xref>).</p>
</sec>
<sec id="sec005">
<title>Ethics Statement</title>
<p>Research and analytical studies at CIHR fall under the Canadian Tri—council Policy Statement 2: Ethical Conduct for Research Involving Humans [<xref rid="pone.0120838.ref035" ref-type="bibr">35</xref>]. This study had the specific objective of quality assurance and quality improvement related to design elements and design assumptions within CIHR’s ongoing reform of its Open programs [<xref rid="pone.0120838.ref036" ref-type="bibr">36</xref>], and thus fell under Clause 2.5 of TCPS-2 and not within the scope of Research Ethics Board review in Canada. Nevertheless, reviewers were informed through ResearchNet, in advance of peer review, that CIHR would be evaluating its own processes, review would be replicated for a randomized sample of applications, and the majority of ratings and reviews would be used for decision purposes. All reviewers provided their electronic consent; no potential reviewer refused to provide consent.</p>
<p>For each of the 100 application with replicate reviews, one triad was randomly selected (within ResearchNet), prior to peer review, as the ‘active’ triad that would be used to determine the actual competition outcome (i.e., the decision to fund or not). Thus, prior to peer review, a specific set of 100 triads, among all possible triad combinations (i.e., 1 of 3<sup>100</sup> possibilities), along with 306 additional triads (representing review of non-replicated applications) was identified for decision purposes. Throughout the competition process, neither CIHR staff nor reviewers were aware of which triads were ‘active’. Reviewers were unaware of which applications had replicate review. Applicants were unaware of this background study on number of reviewers which did not affect their application outcome.</p>
</sec>
</sec>
<sec id="sec006" sec-type="results">
<title>Results</title>
<p>Simulated competition outcomes for each application (<xref rid="pone.0120838.g001" ref-type="fig">Fig. 1</xref>), within a single bootstrapped iteration, are provided for N<sub>reviewers</sub> (1 to 21) in each success rate scenario [(a) 5%, (b) 15%, (c) 25%, (d) 35% and (e) 50%]. Applications were sequenced by estimated rank (a gradient of relative competitiveness), based on the bootstrapped mean (10,000 iterations) of each set of 9 independent pre—scores per application. Within single iterations, outcomes for many applications were invariant, regardless of N<sub>reviewers</sub> [i.e., continuous horizontal bands, of grey or white, represented continuous success (or not) respectively for individual applications with increased N<sub>reviewers</sub>]. For other applications, one or more additional reviewers per application resulted in one or more decision changes. Applications with higher variability among reviewer scores and especially those closer to the ‘payline’ of a particular scenario (i.e., applications with a rank closer to the cutoff of a specific success rate scenario) were more likely to exhibit decision reversals shown by horizontal pattern discontinuities.</p>
<fig id="pone.0120838.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0120838.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Simulated competition outcomes from a single bootstrap iteration (1 to 21 reviewers per application).</title>
<p>Horizontal sequences represent simulated outcomes for each of 100 Fellowships applications (‘grey’ representing success) with incremental addition of reviewers, within different overall success rate scenarios. For each N to N+1<sub>reviewers</sub>, an additional score was sampled (with replacement) from 9 independent assessments. Discontinuities in horizontal grey/white—coding reflect changes in decision outcome with addition of a single reviewer. Within each iteration (representing one simulated competition), the outcome for many applications was invariant where N &gt; 2, regardless of the success rate scenario.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0120838.g001" position="float" xlink:type="simple"/>
</fig>
<p>The 3D distribution of probability of success for the 100 applications, across estimated rank (<xref rid="pone.0120838.g002" ref-type="fig">Fig. 2</xref>), with varying numbers of reviewers per application, varied among the five overall success rate scenarios. Applications were categorized into three outcome groups, based on cumulative probability of success over 10,000 simulated competition outcomes. Within each overall success scenario, ‘Category A’ represented highly competitive applications with elevated probability of success (i.e., ≥ ~95%). The relative proportion of Category A applications increased with overall competition success rate. ‘Category B’ represented less competitive applications, where the probability of success ranged from &gt; ~5% to &lt; ~95%. The relative proportion of Category B applications increased with overall competition success rate. Conversely, ‘Category C’ represented applications with low probability of success (i.e., ≤ ~5%). The relative proportion of Category C decreased with increased overall competition success rate. Category A applications were not identified in the 5% and 15% overall competition success rate scenarios (most applications were Category C).</p>
<fig id="pone.0120838.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0120838.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Funding success probability profiles in five overall competition scenarios (1 to 21 reviewers per application).</title>
<p>Cumulative sum of 10,000 bootstrapped simulations of competition outcomes, for 100 applications within five overall success scenarios [5% (a), 15% (b), 25% (c), 35% (d), and 50% (e)]. Within some scenarios, the most competitive applications had ≥ ~95% probability of success (Category A). Applications of intermediate competitiveness (Category B) had a probability of success which varied from ~5% to ~95%. The least competitive applications (Category C) were rarely (≤ ~5% probability) or never successful.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0120838.g002" position="float" xlink:type="simple"/>
</fig>
<p>Proportions of applications in Categories A, B and C, in five overall success scenarios, were further compared for N<sub>reviewers</sub> = 5. Using locally weighted polynomial regression (LOESS smoothing in R [<xref rid="pone.0120838.ref034" ref-type="bibr">34</xref>]), probability of success for the 100 applications was calculated in relation to estimated rank. Within each success rate scenario, the proportion of applications (to the nearest 5%, based on the smoothed regression) which fell in each category was identified (<xref rid="pone.0120838.g003" ref-type="fig">Fig. 3</xref>).</p>
<fig id="pone.0120838.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0120838.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Relative proportion of applications in three outcome categories, in five overall competition scenarios (5 reviewers per application).</title>
<p>For 100 applications within five overall competition success scenarios [5% (a), 15% (b), 25% (c), 35% (d), and 50% (e)], different proportions of applications had very high (A), medium (B) or very low (C) probability of success (categories defined in text) over 10,000 iterations of simulated competition results. Horizontal green lines indicated a probability of success of 5% and 95%. Vertical blue lines indicated where locally weighted polynomial regression (LOESS smoothing) curves intersected the 5% or 95% probability of success, or reached the highest ranked application (the limit of the graph).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0120838.g003" position="float" xlink:type="simple"/>
</fig>
<p>Bootstrapped estimates of kappa and significance (alpha) levels, including Monte Carlo error estimates (i.e., as error bars) are shown (<xref rid="pone.0120838.g004" ref-type="fig">Fig. 4</xref>). Using benchmarks for kappa [<xref rid="pone.0120838.ref032" ref-type="bibr">32</xref>], there was “moderate” (0.41–0.60) to “substantial” (0.61–0.80) decision consistency, even at the 1–2 reviewer increment, although negative error bars extended to only “slight” agreement with the fewest reviewers. The 4–5 and 5–6 reviewer increment negative error bars extended to “moderate” agreement, rising to “substantial” at the 10–11 reviewer increment. Within increasing kappa, negative error extended to “almost perfect” (0.81–1.0) decision consistency at the 20–21 reviewer increment. Bootstrapped estimated of kappa were significant (α &lt; 0.05) except for the 1–2 reviewer increment in the 5% success scenario. Positive Monte Carlo error bars for alpha levels extended above α = 0.05 (i.e., NS), for the 1–2 and 2–3 reviewer increment in the 5% success scenario but otherwise were within the range of significance (α &lt; 0.05). Across all success rate scenarios, additional reviewers per application provided increased (though increasingly modest) improvement in overall decision consistency.</p>
<fig id="pone.0120838.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0120838.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Bootstrapped kappa statistics of peer review decision consistency with incremental N to N+1 reviewers.</title>
<p>In simulation of CIHR Fellowships competition outcomes, overall decision consistency improved with incremental addition of reviewers regardless of overall success rate scenario. Monte Carlo error analysis (standard deviation of bootstrapped estimates of kappa coefficients) [<xref rid="pone.0120838.ref033" ref-type="bibr">33</xref>] indicated broad overlap among incremental kappa values with increased reviewers. Kappa levels &gt; 0.8 represented “almost perfect” consistency [<xref rid="pone.0120838.ref032" ref-type="bibr">32</xref>]. Kappa values were significant (α &lt; 0.05) except in the 5% success scenario at the 1–2 and 2–3 reviewer increment.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0120838.g004" position="float" xlink:type="simple"/>
</fig>
<p>To identify a meaningful (and practical) upper limit to N<sub>reviewers</sub>, several criterion—setting stopping—approaches [<xref rid="pone.0120838.ref037" ref-type="bibr">37</xref>] were tried (e.g., Scree plot methods used to identify a meaningful number of Principal Components). The most pragmatic (or otherwise useful) stopping—approach was based on a method to select sampling duration using second derivatives [<xref rid="pone.0120838.ref038" ref-type="bibr">38</xref>]. The first derivative local slope (S1) calculated from point—to—point local changes in kappa and second derivative local change in slope (S2) similarly calculated from point—to—point changes in S1, are shown (<xref rid="pone.0120838.g005" ref-type="fig">Fig. 5</xref>). Buffin—Bélanger &amp; Roy [<xref rid="pone.0120838.ref038" ref-type="bibr">38</xref>] used natural logarithm transformation of the x—axis (time, covering many orders of magnitude), contributing to their distinct S2 inflection point. S2 of non-transformed kappa data in this study attained an asymptote at, or slightly, above the 4–5 reviewer increment in all success rate scenarios. Substantial levels of decision consistency (kappa ≥ 0.61) were achieved with 4–5 reviewers per application, for all success rate scenarios.</p>
<fig id="pone.0120838.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0120838.g005</object-id>
<label>Fig 5</label>
<caption>
<title>First derivative (S1) and second derivative (S2) of kappa, within incremental N to N+1 reviewers, across five overall competition success rate scenarios.</title>
<p>Relative improvement of kappa reached stability at 4–5 reviewers per application or shortly thereafter, across all overall success rate scenarios [5% (a), 15% (<bold>b</bold>), 25% (c), 35% (d), and 50% (e)]. Vertical dashed lines represent the approximate S2 asymptotes.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0120838.g005" position="float" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec007" sec-type="conclusions">
<title>Discussion and Conclusion</title>
<p>Variability among scores for an individual application, or among scores for applications close to it in terms of rank, yielded alternate score combinations (in different iterations) that frequently changed relative ranking. Depending on the success rate scenario and the proximity of an application to the threshold cutoff, changes in an application’s rank were often sufficient to reverse the simulated decision outcome. Even within single iterations (<xref rid="pone.0120838.g001" ref-type="fig">Fig. 1</xref>), an ‘invariant’ competition outcome for all applications was not achieved under any of the five funding scenarios as a result of adding additional reviewers per application, and ‘invariant competition outcomes’ do not appear to represent a realistic target or expectation for research funding agencies, reviewers or applicants. As has been identified in previous competition environments [<xref rid="pone.0120838.ref004" ref-type="bibr">4</xref>,<xref rid="pone.0120838.ref010" ref-type="bibr">10</xref>–<xref rid="pone.0120838.ref014" ref-type="bibr">14</xref>,<xref rid="pone.0120838.ref023" ref-type="bibr">23</xref>], some degree of alternative decision outcomes would be anticipated with re—running or replication of many or most competitions.</p>
<p>Classification of most applications to category A, B or C depended on success rate scenario. Variable outcomes were notable in Category B (Figs. <xref rid="pone.0120838.g002" ref-type="fig">2</xref>, <xref rid="pone.0120838.g003" ref-type="fig">3</xref>). Particularly where the overall competition success rate was low (i.e., 5% or 15%), the peer review process appeared highly effective in identifying applications with low probability of success (Category C). The process appeared far less effective in identifying noncompetitive Category C applications where the success rate was high (i.e., 35% or 50%). The previous observation, “[…] that reviewers have much less difficulty in agreeing on rejection than on acceptance” [<xref rid="pone.0120838.ref010" ref-type="bibr">10</xref>], may thus be linked to overall competition success rate.</p>
<p>Variable outcomes for many applications, regardless of category, were exacerbated where numbers of reviewers per application was low (especially N<sub>reviewers</sub> ≤ 3, <xref rid="pone.0120838.g002" ref-type="fig">Fig. 2</xref>). With few reviewers, there was increased probability of negative decisions even for the most highly ranked Category A applications, and an increased probability of positive decisions for many of the least competitive applications (Category C). In other words, especially with few reviewers, “luck” [<xref rid="pone.0120838.ref012" ref-type="bibr">12</xref>] played an increased role in determining outcome for many applicants, particularly in the mid—range of competitiveness. Given larger error bars for kappa statistics, with low numbers of reviewers per application, and given that success rates are dropping for funding agencies [<xref rid="pone.0120838.ref027" ref-type="bibr">27</xref>], these results are supportive of the need to increase reviewer numbers to beyond 2 or 3 reviews per application.</p>
<p>Implications for stratifying applications (i.e., triaging) are considered in the context of the diversity of profiles for probability of success over different success rate scenarios with 5 reviewers per application (<xref rid="pone.0120838.g003" ref-type="fig">Fig. 3</xref>). In a multi-stage peer review process, such as would typically involve “at-home” review [<xref rid="pone.0120838.ref008" ref-type="bibr">8</xref>], input from reviewers can be acquired that is sufficient to place applications into rank order sequence for the purposes of stratification. Varying proportions of applications could be stratified into A, B and C categories, as in <xref rid="pone.0120838.t001" ref-type="table">Table 1</xref>, depending on overall competition success rate. High and low rank applications (<xref rid="pone.0120838.g003" ref-type="fig">Fig. 3</xref>) could be excluded from further review with a decision to fund (Category A) and to reject (Category C), respectively, at this mid-point in the process. The Category B subset could be assigned to a subsequent phase of consideration (such as asynchronous online discussion, or potentially face-to-face discussion in some instances).</p>
<table-wrap id="pone.0120838.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0120838.t001</object-id>
<label>Table 1</label> <caption><title>Guidelines for triaging.</title></caption>
<alternatives>
<graphic id="pone.0120838.t001g" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0120838.t001" xlink:type="simple"/>
<table>
<colgroup span="1">
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
<col align="left" valign="middle" span="1"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1"/>
<th colspan="3" align="center" rowspan="1">Stratification of Applications (% by Category) within Gradient of Competitiveness<xref rid="t001fn001" ref-type="table-fn"><sup>a</sup></xref></th>
</tr>
<tr>
<th align="left" rowspan="1" colspan="1"><bold>Overall Competition Success Rate Scenario (%)</bold></th>
<th align="left" rowspan="1" colspan="1">C (Reject without further review)</th>
<th align="left" rowspan="1" colspan="1">B (Further review required)</th>
<th align="left" rowspan="1" colspan="1">A (Success without further review)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>5</bold></td>
<td align="left" rowspan="1" colspan="1">75</td>
<td align="left" rowspan="1" colspan="1">25</td>
<td align="left" rowspan="1" colspan="1">0</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>15</bold></td>
<td align="left" rowspan="1" colspan="1">60</td>
<td align="left" rowspan="1" colspan="1">40</td>
<td align="left" rowspan="1" colspan="1">0</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>25</bold></td>
<td align="left" rowspan="1" colspan="1">45</td>
<td align="left" rowspan="1" colspan="1">50</td>
<td align="left" rowspan="1" colspan="1">5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>35</bold></td>
<td align="left" rowspan="1" colspan="1">30</td>
<td align="left" rowspan="1" colspan="1">60</td>
<td align="left" rowspan="1" colspan="1">10</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><bold>50</bold></td>
<td align="left" rowspan="1" colspan="1">15</td>
<td align="left" rowspan="1" colspan="1">70</td>
<td align="left" rowspan="1" colspan="1">15</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t001fn001"><p><sup>a</sup>Gradient of Competitiveness established by ranking applications on the basis of average (or bootstrapped average) raw scores or score percentiles (as appropriate for any given competition).</p></fn>
</table-wrap-foot>
</table-wrap>
<p>Stratification (percent by category) could be determined on a competition—by—competition basis, at the time applications are received, or, when application volume and competition success rate is otherwise determined (such as through a quota system with a predetermined number of eligible applications), thereby limiting expenditure of additional resources and reducing reviewer burden. Other peer review contexts of <italic>ex ante</italic> review in advance of performance (e.g., of research project proposals) and <italic>ex post</italic> retrospective review (e.g., of manuscripts for publication) [<xref rid="pone.0120838.ref039" ref-type="bibr">39</xref>] may similarly benefit from higher levels of decision consistency with increase to N<sub>reviewers</sub> = 5.</p>
<p>Unlike competitions for fellowships, normally restricted to one application per applicant, many grant competitions function without intake limits. In such competitions, applicants are already incentivized to submit as many proposals as possible as a rational strategy to individually maximize the probability of funding, in an equivalent of game theory’s Prisoner’s Dilemma [<xref rid="pone.0120838.ref027" ref-type="bibr">27</xref>]. An optimal strategy, which could minimize both individual applicant and the collective total workload would be for applicants to cooperate by submitting one or a small number of applications per competition [<xref rid="pone.0120838.ref027" ref-type="bibr">27</xref>]. However, the element of chance in the decision process, evident in the 3D success probability profiles (<xref rid="pone.0120838.g002" ref-type="fig">Fig. 2</xref>) and in Category B applications (<xref rid="pone.0120838.g003" ref-type="fig">Fig. 3</xref>), may simply further encourage some applicants to pursue the strategy of submitting many proposals. Intake limits, if imposed by funding agencies for individual applicants, or quota systems, could serve as a counter measure, thereby helping to contain system cost and reduce burden to reviewers.</p>
<p>In the context of small competitions, such as the comparison of review systems for 32 applications [<xref rid="pone.0120838.ref014" ref-type="bibr">14</xref>], the suggestion that all reviewers review all applications may often be feasible. This approach would not be operationally realistic in competitions with either large committees or large competitions where there is no meeting of a committee-as-a-whole (as in this Fellowship competition). For such large competitions, especially with low overall success rates, use of 5 reviewers per application is recommended (which would also facilitate tie-breaking during discussion).</p>
<p>A true estimate of the probability distribution of the reviews for each application could be obtained in the theoretical case of a very large (or infinite) number of reviewers. However, sampling this <italic>a priori</italic> unknown distribution by a finite number of reviews per application (e.g., N from 1 to 21, as in the current study, or all actual competitions), will lead to false results some of the time. While the quality of the estimation would improve with increased reviewers, ad infinitum (e.g., as in <xref rid="pone.0120838.g004" ref-type="fig">Fig. 4</xref>, where kappas progressively increase with N<sub>reviewers</sub>), practical and resource constraints impose severe limits to reviewer numbers.</p>
<p>These findings and conclusions, based on a single set of applications from a single program, may have limited applicability to other funding or selection scenarios. It would be especially useful to have validation from research grant competitions involving multiple applicants per proposal (e.g., research grants for large teams) and from competitions involving non-uniform financial requests, where considerations of potential return on investment may be important considerations for reviewers. Nevertheless, this study does provide a basis for recommending that research funding agencies and journals avoid extremes in numbers of reviewers per application. Having too few reviewers (i.e., N ≤ 3) results in excessive decision inconsistency. On the other hand, while theoretically preferable, having larger numbers of reviewers (N &gt; 5) yields only modest improvement in consistency but does entail substantial increase to cost, burden and difficulty in recruiting additional reviewers. Five reviewers per application represents a practical trade—off, in terms of balancing increased decision consistency against incremental cost, as well as minimizing large random effects in decision outcomes and improving efficiency of the decision making process.</p>
</sec>
<sec id="sec008">
<title>Supporting Information</title>
<supplementary-material id="pone.0120838.s001" xlink:href="info:doi/10.1371/journal.pone.0120838.s001" mimetype="text/csv" position="float" xlink:type="simple">
<label>S1 Data</label>
<caption>
<title>Reviewer scores for 100 Fellowship applications (9 independent scores per application).</title>
<p>Anonymised data from Canadian Institutes of Health Research spring 2013 post-doctoral Fellowships competition for biomedical applicants.</p>
<p>(CSV)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0120838.s002" xlink:href="info:doi/10.1371/journal.pone.0120838.s002" mimetype="text/x-R" position="float" xlink:type="simple">
<label>S1 File</label>
<caption>
<title>R code used for analyses and graphics.</title>
<p>Some annotations and cropping in final images (e.g., <xref rid="pone.0120838.g002" ref-type="fig">Fig. 2</xref> category boundaries and category labels, <xref rid="pone.0120838.g003" ref-type="fig">Fig. 3</xref> arrows) done with graphic image editing software.</p>
<p>(R)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>U. Beaudry, C. Hardy and S. Shimizu-Herr played key roles in developing ResearchNet’s constrained random reviewer assignment algorithm and asynchronous discussion functionality for this competition. R. Venne, C. Sincennes and A. Poirier managed the peer review. Comments and suggestions from J.E. Aubin, D. Dempsey, D. Grote, N.H. Gendron, A—L. Kates, P. Lasko, J. O’Donoughue, D. Ryan, J. Waring, and F. Zegers substantially improved the analyses and final text.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0120838.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bornmann</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Wallon</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Ledin</surname> <given-names>A</given-names></name>. <article-title>Does the committee peer review select the best applicants for funding? An investigation of the selection process for two European molecular biology organization programmes</article-title>. <source>PLoS ONE</source>. <year>2008</year>; <volume>3</volume>(<issue>10</issue>): <fpage>e3480</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0003480" xlink:type="simple">10.1371/journal.pone.0003480</ext-link></comment> <object-id pub-id-type="pmid">18941530</object-id></mixed-citation></ref>
<ref id="pone.0120838.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kravitz</surname> <given-names>RL</given-names></name>, <name name-style="western"><surname>Franks</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Feldman</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Gerrity</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Byrne</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Tierney</surname> <given-names>WM</given-names></name>. <article-title>Editorial peer reviewers’ recommendations at a general medical journal: are they reliable and do editors care?</article-title> <source>PLoS ONE</source>. <year>2010</year>; <volume>5</volume>(<issue>4</issue>): <fpage>e10072</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0010072" xlink:type="simple">10.1371/journal.pone.0010072</ext-link></comment> <object-id pub-id-type="pmid">20386704</object-id></mixed-citation></ref>
<ref id="pone.0120838.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abdoul</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Perrey</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Amiel</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Tubach</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Gottot</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Durand-Zaleski</surname> <given-names>I</given-names></name>, <etal>et al</etal>. <article-title>Peer review of grant applications: criteria used and qualitative study of reviewer practices</article-title>. <source>PLoS ONE</source>. <year>2012</year>; <volume>7</volume>: <fpage>e46054</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0046054" xlink:type="simple">10.1371/journal.pone.0046054</ext-link></comment> <object-id pub-id-type="pmid">23029386</object-id></mixed-citation></ref>
<ref id="pone.0120838.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fogelholm</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Leppinen</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Auvinen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Raitanen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Nuutinen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Väänänen</surname> <given-names>K</given-names></name>. <article-title>Panel discussion does not improve reliability of peer review for medical research grant proposals</article-title>. <source>J Clin Epidemiol</source>. <year>2012</year>; <volume>65</volume>: <fpage>47</fpage>–<lpage>52</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.jclinepi.2011.05.001" xlink:type="simple">10.1016/j.jclinepi.2011.05.001</ext-link></comment> <object-id pub-id-type="pmid">21831594</object-id></mixed-citation></ref>
<ref id="pone.0120838.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Demicheli</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Di Pietrantonj</surname> <given-names>C</given-names></name>. <article-title>Peer review for improving the quality of grant applications</article-title>. <source>Cochrane Database of Systematic Reviews</source>. <year>2007</year>; <volume>2</volume>: MR000003. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/14651858.mr000003.pub2" xlink:type="simple">10.1002/14651858.mr000003.pub2</ext-link></comment></mixed-citation></ref>
<ref id="pone.0120838.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jefferson</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Rudin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Brodney</surname> <given-names>Folse S</given-names></name>, <name name-style="western"><surname>Davidoff</surname> <given-names>F</given-names></name>. <article-title>Editorial peer review for improving the quality of reports of biomedical studies</article-title>. <source>Cochrane Database of Systematic Reviews</source>. <year>2007</year>; <volume>2</volume>: MR000016. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/14651858.mr000016.pub3" xlink:type="simple">10.1002/14651858.mr000016.pub3</ext-link></comment></mixed-citation></ref>
<ref id="pone.0120838.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Van Arensbergen</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>van den Besselaar</surname> <given-names>P</given-names></name>. <article-title>The selection of scientific talent in the allocation of research grants</article-title>. <source>High Educ Policy</source>. <year>2012</year>; <volume>25</volume>: <fpage>381</fpage>–<lpage>405</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1057/hep.2012.15" xlink:type="simple">10.1057/hep.2012.15</ext-link></comment></mixed-citation></ref>
<ref id="pone.0120838.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Obrecht</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Tibelius</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>D’Aloisio</surname> <given-names>G</given-names></name>. <article-title>Examining the value added by committee discussion in the review of applications for research awards</article-title>. <source>Res Eval</source>. <year>2007</year>; <volume>16</volume>: <fpage>70</fpage>–<lpage>91</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3152/095820207X223785" xlink:type="simple">10.3152/095820207X223785</ext-link></comment></mixed-citation></ref>
<ref id="pone.0120838.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schroter</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Groves</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Højgaard</surname> <given-names>L</given-names></name>. <article-title>Surveys of current status in biomedical science grant review: funding organisations’ and grant reviewers' perspectives</article-title>. <source>BMC Med</source>. <year>2010</year>; <volume>8</volume>: <fpage>62</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/1741-7015-8-62" xlink:type="simple">10.1186/1741-7015-8-62</ext-link></comment> <object-id pub-id-type="pmid">20961441</object-id></mixed-citation></ref>
<ref id="pone.0120838.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cicchetti</surname> <given-names>D V</given-names></name>. <article-title>The reliability of peer review for manuscript and grant submission: A cross-disciplinary investigation</article-title>. <source>Behav Brain Sci</source>. <year>1991</year>; <volume>14</volume>: <fpage>119</fpage>–<lpage>186</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1017/S0140525X00065675" xlink:type="simple">10.1017/S0140525X00065675</ext-link></comment></mixed-citation></ref>
<ref id="pone.0120838.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marsh</surname> <given-names>HW</given-names></name>, <name name-style="western"><surname>Jayasinghe</surname> <given-names>UW</given-names></name>, <name name-style="western"><surname>Bond</surname> <given-names>NW</given-names></name>. <article-title>Improving the peer-review process for grant applications: reliability, validity, bias, and generalizability</article-title>. <source>Am Psychol</source>. <year>2008</year>; <volume>63</volume>: <fpage>160</fpage>–<lpage>168</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0003-066X.63.3.160" xlink:type="simple">10.1037/0003-066X.63.3.160</ext-link></comment> <object-id pub-id-type="pmid">18377106</object-id></mixed-citation></ref>
<ref id="pone.0120838.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cole</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Cole</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Simon</surname> <given-names>GA</given-names></name>. <article-title>Chance and consensus in peer review</article-title>. <source>Science</source>. <year>1981</year>; <volume>214</volume>: <fpage>881</fpage>–<lpage>886</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.7302566" xlink:type="simple">10.1126/science.7302566</ext-link></comment> <object-id pub-id-type="pmid">7302566</object-id></mixed-citation></ref>
<ref id="pone.0120838.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hodgson</surname> <given-names>C</given-names></name>. <article-title>How reliable is peer review? An examination of operating grant proposals simultaneously submitted to two similar peer review systems</article-title>. <source>J Clin Epidemiol</source>. <year>1997</year>; <volume>50</volume>: <fpage>1189</fpage>–<lpage>1195</lpage>. <object-id pub-id-type="pmid">9393374</object-id></mixed-citation></ref>
<ref id="pone.0120838.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mayo</surname> <given-names>NE</given-names></name>, <name name-style="western"><surname>Brophy</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Goldberg</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Klein</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Platt</surname> <given-names>RW</given-names></name>, <etal>et al</etal>. <article-title>Peering at peer review revealed high degree of chance associated with funding of grant applications</article-title>. <source>J Clin Epidemiol</source>. <year>2006</year>; <volume>59</volume>: <fpage>842</fpage>–<lpage>848</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.jclinepi.2005.12.007" xlink:type="simple">10.1016/j.jclinepi.2005.12.007</ext-link></comment> <object-id pub-id-type="pmid">16828678</object-id></mixed-citation></ref>
<ref id="pone.0120838.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roy</surname> <given-names>R</given-names></name>. <article-title>Funding science: the real defects of peer review and an alternative to it</article-title>. <source>Sci Technol Hum Values</source>. <year>1985</year>; <volume>10</volume>: <fpage>73</fpage>–<lpage>81</lpage>.</mixed-citation></ref>
<ref id="pone.0120838.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Benda</surname> <given-names>WGG</given-names></name>, <name name-style="western"><surname>Engels</surname> <given-names>TCE</given-names></name>. <article-title>The predictive validity of peer review: A selective review of the judgmental forecasting qualities of peers, and implications for innovation in science</article-title>. <source>Int J Forecast</source>. <year>2011</year>; <volume>27</volume>: <fpage>166</fpage>–<lpage>182</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.ijforecast.2010.03.003" xlink:type="simple">10.1016/j.ijforecast.2010.03.003</ext-link></comment></mixed-citation></ref>
<ref id="pone.0120838.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bornmann</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Daniel</surname> <given-names>H-D</given-names></name>. <article-title>Reliability, fairness and predictive validity of committee peer review. Evaluation of the selection of post-graduate fellowship holders by the Boehringer Ingelheim Fonds</article-title>. <source>BIF Futura</source>. <year>2004</year>; <volume>19</volume>: <fpage>7</fpage>–<lpage>19</lpage>.</mixed-citation></ref>
<ref id="pone.0120838.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Langfeldt</surname> <given-names>L</given-names></name>. <article-title>The policy challenges of peer review: managing bias, conflict of interests and interdisciplinary assessments</article-title>. <source>Res Eval</source>. <year>2006</year>; <volume>15</volume>: <fpage>31</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3152/147154406781776039" xlink:type="simple">10.3152/147154406781776039</ext-link></comment></mixed-citation></ref>
<ref id="pone.0120838.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bornmann</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Daniel</surname> <given-names>H-D</given-names></name>. <article-title>Potential sources of bias in research fellowship assessments: effects of university prestige and field of study</article-title>. <source>Res Eval</source>. <year>2006</year>; <volume>15</volume>: <fpage>209</fpage>–<lpage>219</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3152/147154406781775850" xlink:type="simple">10.3152/147154406781775850</ext-link></comment></mixed-citation></ref>
<ref id="pone.0120838.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Sugimoto</surname> <given-names>CR</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Cronin</surname> <given-names>B</given-names></name>. <article-title>Bias in peer review</article-title>. <source>J Am Soc Inf Sci Technol</source>. <year>2013</year>; <volume>64</volume>: <fpage>2</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/asi.22784" xlink:type="simple">10.1002/asi.22784</ext-link></comment></mixed-citation></ref>
<ref id="pone.0120838.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wessely</surname> <given-names>S.</given-names></name> <article-title>Peer review of grant applications: what do we know?</article-title> <source>Lancet</source>. <year>1998</year>; <volume>352</volume>: <fpage>301</fpage>–<lpage>305</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0140-6736(97)11129-1" xlink:type="simple">10.1016/S0140-6736(97)11129-1</ext-link></comment> <object-id pub-id-type="pmid">9690424</object-id></mixed-citation></ref>
<ref id="pone.0120838.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luukkonen</surname> <given-names>T</given-names></name>. <article-title>Conservatism and risk-taking in peer review: Emerging ERC practices</article-title>. <source>Res Eval</source>. <year>2012</year>; <volume>21</volume>: <fpage>48</fpage>–<lpage>60</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/reseval/rvs001" xlink:type="simple">10.1093/reseval/rvs001</ext-link></comment></mixed-citation></ref>
<ref id="pone.0120838.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Graves</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Barnett</surname> <given-names>AG</given-names></name>, <name name-style="western"><surname>Clarke</surname> <given-names>P</given-names></name>. <article-title>Funding grant proposals for scientific research: retrospective analysis of scores by members of grant review panel</article-title>. <source>BMJ</source>. <year>2011</year>; <volume>343</volume>: <fpage>d4797</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1136/bmj.d4797" xlink:type="simple">10.1136/bmj.d4797</ext-link></comment> <object-id pub-id-type="pmid">21951756</object-id></mixed-citation></ref>
<ref id="pone.0120838.ref024"><label>24</label><mixed-citation publication-type="other" xlink:type="simple">Guthrie S, Guerin B, Wu H, Ismail S, Wooding S. Alternatives to peer review in research project funding 2013 update. RR-139-DH. RAND Corporation; 2013.</mixed-citation></ref>
<ref id="pone.0120838.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Birukou</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wakeling</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Bartolini</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Casati</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Marchese</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Mirylenka</surname> <given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Alternatives to peer review: novel approaches for research evaluation</article-title>. <source>Front Comput Neurosci</source>. <year>2011</year>; <volume>5</volume>: <fpage>1</fpage>–<lpage>12</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/fncom.2011.00056" xlink:type="simple">10.3389/fncom.2011.00056</ext-link></comment> <object-id pub-id-type="pmid">21267396</object-id></mixed-citation></ref>
<ref id="pone.0120838.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bollen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Crandall</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Junk</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ding</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Borner</surname> <given-names>K</given-names></name>. <article-title>From funding agencies to scientific agency. Collective allocation of science funding as an alternative to peer review</article-title>. <source>EMBO Rep</source>. <year>2014</year>; <volume>15</volume>: <fpage>131</fpage>–<lpage>133</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/embr.201338068" xlink:type="simple">10.1002/embr.201338068</ext-link></comment> <object-id pub-id-type="pmid">24397931</object-id></mixed-citation></ref>
<ref id="pone.0120838.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roebber</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Schultz</surname> <given-names>DM</given-names></name>. <article-title>Peer review, program officers and science funding</article-title>. <source>PLoS ONE</source>. <year>2011</year>; <volume>6</volume>(<issue>4</issue>): <fpage>e18680</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0018680" xlink:type="simple">10.1371/journal.pone.0018680</ext-link></comment> <object-id pub-id-type="pmid">21533268</object-id></mixed-citation></ref>
<ref id="pone.0120838.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Squazzoni</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Bravo</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Takács</surname> <given-names>K</given-names></name>. <article-title>Does incentive provision increase the quality of peer review? An experimental study</article-title>. <source>Res Policy</source>. <year>2013</year>; <volume>42</volume>: <fpage>287</fpage>–<lpage>294</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.respol.2012.04.014" xlink:type="simple">10.1016/j.respol.2012.04.014</ext-link></comment></mixed-citation></ref>
<ref id="pone.0120838.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lipworth</surname> <given-names>WL</given-names></name>, <name name-style="western"><surname>Kerridge</surname> <given-names>IH</given-names></name>, <name name-style="western"><surname>Carter</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Little</surname> <given-names>M</given-names></name>. <article-title>Journal peer review in context: a qualitative study of the social and subjective dimensions of manuscript review in biomedical publishing</article-title>. <source>Soc Sci Med</source>. <year>2011</year>; <volume>72</volume>: <fpage>1056</fpage>–<lpage>1063</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.socscimed.2011.02.002" xlink:type="simple">10.1016/j.socscimed.2011.02.002</ext-link></comment> <object-id pub-id-type="pmid">21388730</object-id></mixed-citation></ref>
<ref id="pone.0120838.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tite</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Schroter</surname> <given-names>S</given-names></name>. <article-title>Why do peer reviewers decline to review? A survey</article-title>. <source>J Epidemiol Community Health</source>. <year>2007</year>; <volume>61</volume>: <fpage>9</fpage>–<lpage>12</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1136/jech.2006.049817" xlink:type="simple">10.1136/jech.2006.049817</ext-link></comment> <object-id pub-id-type="pmid">17183008</object-id></mixed-citation></ref>
<ref id="pone.0120838.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kaplan</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Lacetera</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Kaplan</surname> <given-names>C</given-names></name>. <article-title>Sample size and precision in NIH peer review</article-title>. <source>PLoS ONE</source>. <year>2008</year>; <volume>3</volume>: <fpage>e2761</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0002761" xlink:type="simple">10.1371/journal.pone.0002761</ext-link></comment> <object-id pub-id-type="pmid">18648494</object-id></mixed-citation></ref>
<ref id="pone.0120838.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Landis</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Koch</surname> <given-names>GG</given-names></name>. <article-title>The measurement of observer agreement for categorical data</article-title>. <source>Biometrics</source>. <year>1977</year>; <volume>33</volume>: <fpage>159</fpage>–<lpage>174</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2307/2529310" xlink:type="simple">10.2307/2529310</ext-link></comment> <object-id pub-id-type="pmid">843571</object-id></mixed-citation></ref>
<ref id="pone.0120838.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koehler</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Haneuse</surname> <given-names>SJ-PA</given-names></name>. <article-title>On the assessment of Monte Carlo error in simulation-based statistical analyses.</article-title> <source>Am Stat</source>. <year>2009</year>; <volume>63</volume>: <fpage>155</fpage>–<lpage>162</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1198/tast.2009.0030" xlink:type="simple">10.1198/tast.2009.0030</ext-link></comment> <object-id pub-id-type="pmid">22544972</object-id></mixed-citation></ref>
<ref id="pone.0120838.ref034"><label>34</label><mixed-citation publication-type="book" xlink:type="simple"><collab xlink:type="simple">R Core Team</collab>. <chapter-title>R: A language and environment for statistical computing</chapter-title>. <publisher-loc>Vienna, Austria</publisher-loc>: <publisher-name>R Foundation for Statistical Computing</publisher-name>; <year>2014</year>.</mixed-citation></ref>
<ref id="pone.0120838.ref035"><label>35</label><mixed-citation publication-type="other" xlink:type="simple">Canadian Institutes of Health Research Natural Sciences and Engineering Research Council of Canada and Social Sciences and Humanities Research Council of Canada. TCPS2 Tri-Council Policy Statement: Ethical Conduct for Research Involving Humans; 2010. Available: <ext-link ext-link-type="uri" xlink:href="http://www.pre.ethics.gc.ca" xlink:type="simple">www.pre.ethics.gc.ca</ext-link>. Accessed 2014 July 22.</mixed-citation></ref>
<ref id="pone.0120838.ref036"><label>36</label><mixed-citation publication-type="other" xlink:type="simple">Canadian Institutes of Health Research. Designing for the future: The new Open Suite of programs and peer review process; 2012. Available: <ext-link ext-link-type="uri" xlink:href="http://www.cihr-irsc.gc.ca/e/46099.html" xlink:type="simple">www.cihr-irsc.gc.ca/e/46099.html</ext-link>. Accessed 22 July 2014.</mixed-citation></ref>
<ref id="pone.0120838.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Peres-Neto</surname> <given-names>PR</given-names></name>, <name name-style="western"><surname>Jackson</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Somers</surname> <given-names>KM</given-names></name>. <article-title>How many principal components? Stopping rules for determining the number of non-trivial axes revisited</article-title>. <source>Comput Stat Data Anal</source>. <year>2005</year>; <volume>49</volume>: <fpage>974</fpage>–<lpage>997</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.csda.2004.06015" xlink:type="simple">10.1016/j.csda.2004.06015</ext-link></comment></mixed-citation></ref>
<ref id="pone.0120838.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buffin-Bélanger</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Roy</surname> <given-names>AG</given-names></name>. <article-title>1 min in the life of a river: selecting the optimal record length for the measurement of turbulence in fluvial boundary layers</article-title>. <source>Geomorphology</source>. <year>2005</year>; <volume>68</volume>: <fpage>77</fpage>–<lpage>94</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.geomorph.2004.09.032" xlink:type="simple">10.1016/j.geomorph.2004.09.032</ext-link></comment></mixed-citation></ref>
<ref id="pone.0120838.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Holbrook</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Frodeman</surname> <given-names>R</given-names></name>. <article-title>Peer review and the ex ante assessment of societal impacts</article-title>. <source>Res Eval</source>. <year>2011</year>; <volume>20</volume>: <fpage>239</fpage>–<lpage>246</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3152/095820211X12941371876788" xlink:type="simple">10.3152/095820211X12941371876788</ext-link></comment></mixed-citation></ref>
</ref-list>
</back>
</article>