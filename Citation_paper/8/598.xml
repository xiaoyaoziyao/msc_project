<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-16-32553</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0181790</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Test statistics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Test statistics</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Invertebrates</subject><subj-group><subject>Arthropoda</subject><subj-group><subject>Crustaceans</subject><subj-group><subject>Copepods</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Statistical distributions</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Ecology</subject><subj-group><subject>Theoretical ecology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Ecology and environmental sciences</subject><subj-group><subject>Ecology</subject><subj-group><subject>Theoretical ecology</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Invertebrates</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Invertebrates</subject><subj-group><subject>Arthropoda</subject><subj-group><subject>Crustaceans</subject><subj-group><subject>Crabs</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>The PIT-trap—A “model-free” bootstrap procedure for inference about regression models with discrete, multivariate responses</article-title>
<alt-title alt-title-type="running-head">The PIT-trap—A residual resampling approach for any parametric model</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9441-6645</contrib-id>
<name name-style="western">
<surname>Warton</surname> <given-names>David I.</given-names></name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Thibaut</surname> <given-names>Loïc</given-names></name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Wang</surname> <given-names>Yi Alice</given-names></name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>School of Mathematics and Statistics and the Evolution &amp; Ecology Research Centre, UNSW Sydney, NSW, Australia</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>School of Mathematics and Statistics, UNSW Sydney, NSW, Australia</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Institute of Computational and Theoretical Studies, Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR, China</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Politis</surname> <given-names>Dimitris N.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University of California, San Diego, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">David.Warton@unsw.edu.au</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<year>2017</year>
</pub-date>
<pub-date pub-type="epub">
<day>24</day>
<month>7</month>
<year>2017</year>
</pub-date>
<volume>12</volume>
<issue>7</issue>
<elocation-id>e0181790</elocation-id>
<history>
<date date-type="received">
<day>15</day>
<month>8</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>7</day>
<month>7</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Warton et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0181790"/>
<abstract>
<p>Bootstrap methods are widely used in statistics, and bootstrapping of residuals can be especially useful in the regression context. However, difficulties are encountered extending residual resampling to regression settings where residuals are not identically distributed (thus not amenable to bootstrapping)—common examples including logistic or Poisson regression and generalizations to handle clustered or multivariate data, such as generalised estimating equations. We propose a bootstrap method based on probability integral transform (PIT-) residuals, which we call the PIT-trap, which assumes data come from some marginal distribution <italic>F</italic> of known parametric form. This method can be understood as a type of “model-free bootstrap”, adapted to the problem of discrete and highly multivariate data. PIT-residuals have the key property that they are (asymptotically) pivotal. The PIT-trap thus inherits the key property, not afforded by any other residual resampling approach, that the marginal distribution of data can be preserved under PIT-trapping. This in turn enables the derivation of some standard bootstrap properties, including second-order correctness of pivotal PIT-trap test statistics. In multivariate data, bootstrapping rows of PIT-residuals affords the property that it preserves correlation in data without the need for it to be modelled, a key point of difference as compared to a parametric bootstrap. The proposed method is illustrated on an example involving multivariate abundance data in ecology, and demonstrated via simulation to have improved properties as compared to competing resampling methods.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000923</institution-id>
<institution>Australian Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>FT120100501</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9441-6645</contrib-id>
<name name-style="western">
<surname>Warton</surname> <given-names>David I.</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100000923</institution-id>
<institution>Australian Research Council</institution>
</institution-wrap>
</funding-source>
<award-id>DP0987729</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-9441-6645</contrib-id>
<name name-style="western">
<surname>Warton</surname> <given-names>David I.</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was supported by the Australian Research Council (FT120100501, DP0987729) to David Warton. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="4"/>
<table-count count="1"/>
<page-count count="18"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The bootstrap is a generally applicable inferential tool that is so intuitive and widely used in statistics that it not only appears in most university courses, but it has also been proposed for teaching in high schools. A key challenge however arises when applying the bootstrap to regression models with a non-Gaussian response. Common examples include logistic and Poisson regression, and extensions to handle clustered or multivariate data such as generalised estimating equations [<xref ref-type="bibr" rid="pone.0181790.ref001">1</xref>]. Specifically, regression models are specified conditionally on the observed explanatory variables, so bootstrapped values should usually be generated conditionally, with the set of design points remaining fixed in resamples. One way to achieve this is to use a parametric bootstrap [<xref ref-type="bibr" rid="pone.0181790.ref002">2</xref>], although this requires specification of a fully parametric model for the data, which can be a challenge in high-dimensional settings. A non-parametric bootstrap that keeps the design fixed can be achieved in models with additive errors, by estimating residuals and then resampling them in some way [<xref ref-type="bibr" rid="pone.0181790.ref003">3</xref>, <xref ref-type="bibr" rid="pone.0181790.ref004">4</xref>]. But for non-Gaussian regression models, it is sometimes not obvious how residuals should be defined. For example, when bootstrapping generalized linear models, Pearson, deviance or Anscombe residuals have been considered [<xref ref-type="bibr" rid="pone.0181790.ref002">2</xref>] but none of these are identically distributed, even in large samples. Some have proposed resampling quantities in the estimating equations [<xref ref-type="bibr" rid="pone.0181790.ref005">5</xref>–<xref ref-type="bibr" rid="pone.0181790.ref007">7</xref>], but for non-normal responses these are also not identically distributed, even in large samples. We will see later that resampling from non-identical distributions can lead to undesirable properties in resultant resampling procedures.</p>
<p>
<xref ref-type="table" rid="pone.0181790.t001">Table 1</xref> is a well-known example data set from ecology [<xref ref-type="bibr" rid="pone.0181790.ref008">8</xref>] which serves to highlight the problems of current bootstrap methodology. The data are multivariate counts of invertebrates (copepods) collected in a randomized blocks design along beaches in Tasmania, Australia, to study the nature of the effect of crab exclusion on communities of small invertebrates. The design has two treatment replicates and two control replicates in each of four blocks, and we would like to start by testing the hypothesis of no interaction between treatment and block. Notice that the number of variables (<italic>p</italic> = 12) is not small compared to the number of observations (<italic>n</italic> = 16), meaning that we cannot rely on standard large-<italic>n</italic>-fixed-<italic>p</italic> inference. One way to address this issue is to use a resampling approach for inference, where we resample rows of data to make inferences that are robust to misspecification of the correlation between the twelve species [<xref ref-type="bibr" rid="pone.0181790.ref009">9</xref>, <xref ref-type="bibr" rid="pone.0181790.ref010">10</xref>].</p>
<table-wrap id="pone.0181790.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0181790.t001</object-id>
<label>Table 1</label>
<caption>
<title>A 16 × 12 matrix of copepod abundances, classified to species.</title>
<p>Data taken from a study at Eagle Neck, Tasmania [<xref ref-type="bibr" rid="pone.0181790.ref008">8</xref>], in which twelve species of copepods (a type of small crustacean) were sampled at 16 transects in each of 4 sites. At each site, crabs had disturbed two transects and left two undisturbed. It is of interest to determine if there is an effect of disturbance and if it interacts with block, collectively across all twelve species.</p>
</caption>
<alternatives>
<graphic id="pone.0181790.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0181790.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Treatment</th>
<th align="left">Block</th>
<th align="right"><italic>Am</italic></th>
<th align="right"><italic>Ad</italic></th>
<th align="right"><italic>Ec</italic>(a)</th>
<th align="right"><italic>Ec</italic>(b)</th>
<th align="right"><italic>Ha</italic></th>
<th align="right"><italic>Le</italic>(a)</th>
<th align="right"><italic>Le</italic>(b)</th>
<th align="right"><italic>Le</italic>(c)</th>
<th align="right"><italic>Mi</italic></th>
<th align="right"><italic>Pa</italic></th>
<th align="right"><italic>Qu</italic></th>
<th align="right"><italic>Rh</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Disturbed</td>
<td align="left">A</td>
<td align="right">43</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">30</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
</tr>
<tr>
<td align="left">Disturbed</td>
<td align="left">A</td>
<td align="right">63</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">15</td>
<td align="right">0</td>
<td align="right">97</td>
<td align="right">11</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr>
<td align="left">Undisturbed</td>
<td align="left">A</td>
<td align="right">124</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">7</td>
<td align="right">2</td>
<td align="right">151</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">6</td>
</tr>
<tr>
<td align="left">Undisturbed</td>
<td align="left">A</td>
<td align="right">105</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">7</td>
<td align="right">0</td>
<td align="right">117</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">6</td>
</tr>
<tr>
<td align="left">Disturbed</td>
<td align="left">B</td>
<td align="right">4</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">14</td>
<td align="right">0</td>
<td align="right">27</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">8</td>
<td align="right">2</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr>
<td align="left">Disturbed</td>
<td align="left">B</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">4</td>
<td align="right">0</td>
<td align="right">35</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr>
<td align="left">Undisturbed</td>
<td align="left">B</td>
<td align="right">91</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">4</td>
<td align="right">0</td>
<td align="right">15</td>
<td align="right">2</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr>
<td align="left">Undisturbed</td>
<td align="left">B</td>
<td align="right">57</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="right">88</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr>
<td align="left">Disturbed</td>
<td align="left">C</td>
<td align="right">7</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">10</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr>
<td align="left">Disturbed</td>
<td align="left">C</td>
<td align="right">6</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">180</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr>
<td align="left">Undisturbed</td>
<td align="left">C</td>
<td align="right">10</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr>
<td align="left">Undisturbed</td>
<td align="left">C</td>
<td align="right">60</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">10</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr>
<td align="left">Disturbed</td>
<td align="left">D</td>
<td align="right">69</td>
<td align="right">4</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">29</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr>
<td align="left">Disturbed</td>
<td align="left">D</td>
<td align="right">5</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">47</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr>
<td align="left">Undisturbed</td>
<td align="left">D</td>
<td align="right">142</td>
<td align="right">3</td>
<td align="right">6</td>
<td align="right">2</td>
<td align="right">0</td>
<td align="right">6</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0</td>
</tr>
<tr>
<td align="left">Undisturbed</td>
<td align="left">D</td>
<td align="right">96</td>
<td align="right">2</td>
<td align="right">7</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The key stumbling block is developing a resampling algorithm that preserves in resamples both the non-normality of the data and the fixed nature of the sampling design. Permutation tests, which are exact in some designs [<xref ref-type="bibr" rid="pone.0181790.ref003">3</xref>], are not directly applicable for the example of <xref ref-type="table" rid="pone.0181790.t001">Table 1</xref> as the null hypothesis (in a test for interaction) does not imply that observations in different treatment groups are exchangeable. Case resampling [<xref ref-type="bibr" rid="pone.0181790.ref002">2</xref>], where we resample rows of the design matrix and response matrix jointly, creates singularities in the design matrix with high probability (each treatment-block combination having a 12% chance of containing no replicates). Residual resampling is a way forward, since it keeps the sampling design fixed in resamples, but it requires identically distributed residuals to be available. What is needed is a general method of calculating residuals for data from any parametric distribution, that will produce approximately independently and identically distributed (iid) residuals under the null hypothesis.</p>
<p>There is a generally applicable definition of residuals for parametric regression models that can produce approximately iid observations—the probability integral transform (PIT) residual [<xref ref-type="bibr" rid="pone.0181790.ref011">11</xref>]. PIT-residuals have the key property that if the regression model is correct and the true values of parameters are known, they are exactly an iid sample from the standard uniform distribution. In practice, PIT-residuals can only approximately satisfy these properties, because of sampling error estimating parameters, which are usually not known. PIT-residuals, or variants thereof, have hence been proposed primarily for use as diagnostic tools, and referred to by a variety of names, including forecast distribution transformed residuals [<xref ref-type="bibr" rid="pone.0181790.ref011">11</xref>], randomized quantile residuals [<xref ref-type="bibr" rid="pone.0181790.ref012">12</xref>] and universal residuals [<xref ref-type="bibr" rid="pone.0181790.ref013">13</xref>]. A particular part of their appeal is that they have the same distribution (when the model is true) irrespective of the values of model parameters, and irrespective of discreteness in the raw data. Hence, for example, when comparing residual vs fits plots for the data of <xref ref-type="table" rid="pone.0181790.t001">Table 1</xref>, it is difficult to assess model fit using Pearson residuals (<xref ref-type="fig" rid="pone.0181790.g001">Fig 1a</xref>), but the model fit appears reasonable upon study of the PIT residuals (<xref ref-type="fig" rid="pone.0181790.g001">Fig 1b</xref>).</p>
<fig id="pone.0181790.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0181790.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Residual plots for a negative binomial regression model fitted to the copepod data of <xref ref-type="table" rid="pone.0181790.t001">Table 1</xref>, using (a) Pearson residuals; (b) PIT-residuals.</title>
<p>Different colours used for different species. Notice that the predominant patterns in (a) are the line of points towards the left (corresponding to zeros) and asymmetry about the horizontal line <italic>y</italic> = 0 (marked in red). These trends, due to the discreteness of the data rather than lack of fit, have been removed in (b) such that the reader can focus on the question of goodness-of-fit.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0181790.g001" xlink:type="simple"/>
</fig>
<p>In this paper we propose the PIT-residual bootstrap, which we refer to for conciseness as the “PIT-trap”. The name is a reference to pit traps, or pitfall traps, a method of sampling invertebrates in ecology that is commonly used to collect data of the form of <xref ref-type="table" rid="pone.0181790.t001">Table 1</xref>. This method can also be understood as a special type of “model-free bootstrap” [<xref ref-type="bibr" rid="pone.0181790.ref014">14</xref>] for discrete, highly multivariate data. The idea behind our method is also very similar to the residual bootstrap proposed by [<xref ref-type="bibr" rid="pone.0181790.ref015">15</xref>], a key distinction being our extension of the method to handle regression models for responses from any distribution, including the discrete case, and our derivation of some key properties of the PIT-trap. One particularly useful property is that standard results from the classical bootstrap apply quite generally to the PIT-trap, including second order accuracy of pivotal test statistics, a benefit not afforded to competing residual resampling approaches [<xref ref-type="bibr" rid="pone.0181790.ref002">2</xref>]. A second useful property, that does apply to any residual resampling approach, is that when applied to multivariate data, the PIT-trap can preserve the correlation in the data without explicitly modelling it via row-resampling. This is the key point of difference as compared to a parametric bootstrap.</p>
<p>First PIT-residuals are reviewed, then the PIT-trap is proposed and some of its key properties discussed, then the approach is applies to the data of <xref ref-type="table" rid="pone.0181790.t001">Table 1</xref>, and simulations are reported which verify some desirable properties of the method.</p>
</sec>
<sec id="sec002">
<title>PIT-residuals</title>
<p>The key innovation in this paper makes use of probability integral transform (PIT-) residuals, which are reviewed below. PIT-residuals have been used by others to develop related bootstrap algorithms for continuous data [<xref ref-type="bibr" rid="pone.0181790.ref014">14</xref>, <xref ref-type="bibr" rid="pone.0181790.ref015">15</xref>].</p>
<p>It is well-known that for a univariate, continuous <italic>Y</italic>, which has cumulative distribution function <italic>F</italic>(<italic>y</italic>; <bold><italic>θ</italic></bold>), <inline-formula id="pone.0181790.e001"><alternatives><graphic id="pone.0181790.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:mi>U</mml:mi> <mml:mo>=</mml:mo> <mml:mi>F</mml:mi> <mml:mo>(</mml:mo> <mml:mi>Y</mml:mi> <mml:mo>;</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">U</mml:mi> <mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> where <inline-formula id="pone.0181790.e002"><alternatives><graphic id="pone.0181790.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mrow><mml:mi mathvariant="script">U</mml:mi> <mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> denotes the standard uniform random variable. A multivariate version of this result maps a <italic>p</italic>-variate variable <bold>Y</bold> onto the uniform distribution on the <italic>p</italic>-variate unit cube [<xref ref-type="bibr" rid="pone.0181790.ref016">16</xref>], but in this paper only the univariate version of this result will be used.</p>
<p>This result can be generalized to discrete data as follows [<xref ref-type="bibr" rid="pone.0181790.ref016">16</xref>]:
<disp-formula id="pone.0181790.e003"><alternatives><graphic id="pone.0181790.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>U</mml:mi> <mml:mo>=</mml:mo> <mml:mi>Q</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>Y</mml:mi> <mml:mo>;</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>Y</mml:mi> <mml:mo>-</mml:mo></mml:msup> <mml:mo>;</mml:mo> <mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">U</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <italic>F</italic>(<italic>y</italic><sup>−</sup>) denotes the limiting value of <italic>F</italic>(<italic>y</italic>) as <italic>y</italic> is approached from the negative direction, and <italic>Q</italic> is a standard uniform random variable independent of <italic>Y</italic>. The random variable <italic>Q</italic> is introduced to handle the discreteness in the data, by uniformly distributing the probability mass from the point <italic>F</italic>(<italic>y</italic>) across all real values between <italic>F</italic>(<italic>y</italic>) and the previous allowable value of this function <italic>F</italic>(<italic>y</italic><sup>−</sup>).</p>
<p>The above results can be used to define probability integral transform residuals (PIT-residuals) as follows. Consider a parametric regression model for <italic>Y</italic>, conditional on explanatory variables <bold>x</bold>, which has cumulative distribution function <italic>F</italic>(<italic>y</italic>; <bold><italic>θ</italic></bold>, <bold>x</bold>). The PIT-residual <italic>u</italic><sub><italic>i</italic></sub> corresponding to the observation <italic>y</italic><sub><italic>i</italic></sub> conditional on <bold>x</bold><sub><italic>i</italic></sub> is defined as follows:
<disp-formula id="pone.0181790.e004"><alternatives><graphic id="pone.0181790.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>u</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>F</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:mover accent="true"><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:msub><mml:mi>Y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="4.pt"/><mml:mtext>is</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>continuous</mml:mtext></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>q</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:mover accent="true"><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>q</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>y</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mo>-</mml:mo></mml:msubsup> <mml:mo>;</mml:mo> <mml:mover accent="true"><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:msub><mml:mi>Y</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mspace width="4.pt"/><mml:mtext>is</mml:mtext> <mml:mspace width="4.pt"/><mml:mtext>discrete</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where <italic>q</italic><sub><italic>i</italic></sub> is an observation from the standard uniform distribution.</p>
<p>While the <italic>u</italic><sub><italic>i</italic></sub> are typically referred to as residuals [<xref ref-type="bibr" rid="pone.0181790.ref011">11</xref>, <xref ref-type="bibr" rid="pone.0181790.ref013">13</xref>], they do not behave like residuals in the usual sense—they are centred around a value of 0.5 rather than a value of 0, and are bounded between 0 and 1. To address this the <italic>u</italic><sub><italic>i</italic></sub> can be mapped onto the standard normal distribution, <italic>z</italic><sub><italic>i</italic></sub> = Φ<sup>−1</sup>(<italic>u</italic><sub><italic>i</italic></sub>) [<xref ref-type="bibr" rid="pone.0181790.ref012">12</xref>], to improve interpretability (as in <xref ref-type="fig" rid="pone.0181790.g001">Fig 1</xref>). Whether or not this is done is irrelevant to the development of the PIT-trap, in the next section.</p>
<p>Note that if <inline-formula id="pone.0181790.e005"><alternatives><graphic id="pone.0181790.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mover><mml:mo>→</mml:mo> <mml:mi>P</mml:mi></mml:mover> <mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> then the <italic>u</italic><sub><italic>i</italic></sub> come from a distribution whose limit is <inline-formula id="pone.0181790.e006"><alternatives><graphic id="pone.0181790.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:mi mathvariant="script">U</mml:mi> <mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, provided that <italic>F</italic>(<italic>y</italic>; <bold><italic>θ</italic></bold>, <bold>x</bold><sub><italic>i</italic></sub>) has been specified correctly. In this sense, PIT-residuals are pivotal quantities. Further, if the <italic>y</italic><sub><italic>i</italic></sub> are independent, then the only dependence in the <italic>u</italic><sub><italic>i</italic></sub> is via <inline-formula id="pone.0181790.e007"><alternatives><graphic id="pone.0181790.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, and this dependence decays to zero as sample size increases. So in large samples the <italic>u</italic><sub><italic>i</italic></sub> are iid—this is the key property that makes PIT-residuals amenable to bootstrapping.</p>
</sec>
<sec id="sec003">
<title>The PIT-trap</title>
<p>The fundamental idea of this paper is to use a PIT-residual bootstrap, or PIT-trap, as a basis for inference. The idea of a residual resampling technique that makes use of the probability integral transform has been proposed previously in the context of survival analysis [<xref ref-type="bibr" rid="pone.0181790.ref015">15</xref>, <xref ref-type="bibr" rid="pone.0181790.ref017">17</xref>]. [<xref ref-type="bibr" rid="pone.0181790.ref015">15</xref>] considered Cox proportional hazard models for univariate data, and later generalized his algorithm to multivariate survival analysis [<xref ref-type="bibr" rid="pone.0181790.ref017">17</xref>] by making use of the marginal cumulative distribution and row resampling. We do the same in the below, but apply the technique more generally beyond proportional hazard models, and study some of its theoretical properties. As [<xref ref-type="bibr" rid="pone.0181790.ref017">17</xref>] note, the idea of making inferences based on models for the marginal distribution, but which are robust to correlation in responses, is very much in the spirit of generalized estimating equations [<xref ref-type="bibr" rid="pone.0181790.ref001">1</xref>] methodology, previously adapted to problems such as the analysis of <xref ref-type="table" rid="pone.0181790.t001">Table 1</xref> [<xref ref-type="bibr" rid="pone.0181790.ref010">10</xref>].</p>
<p>Another related idea is the “model-free” approach [<xref ref-type="bibr" rid="pone.0181790.ref014">14</xref>, <xref ref-type="bibr" rid="pone.0181790.ref018">18</xref>] to fitting and prediction, based on analysis of residuals and searching for transformations to “iid-ness” that imply particular models for the observed data. [<xref ref-type="bibr" rid="pone.0181790.ref014">14</xref>] proposed a “model-free bootstrap”, based on resampling transformed quantities that play the role of PIT-residuals, for continuously distributed data. This is referred to as model-free on the grounds that it is motivated via transformation of the response to iid-ness rather than from fitting a given model to data. Thus PIT-residual resampling, as proposed below, could be understood as a type of model-free bootstrap, where the PIT-residuals are considered as the data transformation to “iid-ness”. While the model-free bootstrap of [<xref ref-type="bibr" rid="pone.0181790.ref014">14</xref>] required the response to be continuous, such that there was a one-to-one transformation between response and residuals, in the discrete data case [<xref ref-type="bibr" rid="pone.0181790.ref014">14</xref>] advocated a “limit model-free bootstrap”, related to the parametric bootstrap, to be discussed later. Below we propose an alternative but related approach appropriate for discrete and highly multivariate data. Specifically, discreteness will be handled by using PIT-residuals defined with “jittering” via <italic>Q</italic>, such that algorithms along the lines of [<xref ref-type="bibr" rid="pone.0181790.ref015">15</xref>] and [<xref ref-type="bibr" rid="pone.0181790.ref014">14</xref>] can be extended directly to discrete data. Further, multivariate data will be handled using block resampling so that we are not required to model the correlation structure in the data—a useful property when analysing data such as in <xref ref-type="table" rid="pone.0181790.t001">Table 1</xref>, where data are not very informative about the correlation structure, especially given the large number of response relative to the number of observations.</p>
<p>The PIT-residual bootstrap, or PIT-trap, can be applied whenever we observe a <italic>n</italic> × <italic>p</italic> matrix of responses <inline-formula id="pone.0181790.e008"><alternatives><graphic id="pone.0181790.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">y</mml:mi> <mml:mn>1</mml:mn> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>n</mml:mi> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> for which marginal distribution functions are available for each variable in <bold>y</bold><sub><italic>i</italic></sub> = (<italic>y</italic><sub><italic>i</italic>1</sub>, …, <italic>y</italic><sub><italic>ip</italic></sub>). Denote as <italic>F</italic>(<italic>y</italic>; <bold><italic>θ</italic></bold><sub><italic>j</italic></sub>, <bold>x</bold><sub><italic>i</italic></sub>) the distribution of <italic>y</italic><sub><italic>ij</italic></sub> that is marginal with respect to <italic>y</italic><sub><italic>ik</italic></sub>, <italic>k</italic> ≠ <italic>j</italic>, but conditional on covariates <bold>x</bold><sub><italic>i</italic></sub>. The focus in this paper will be on the case where the <italic>p</italic>-variate observations <bold>y</bold><sub><italic>i</italic></sub> are independent of each other, but dependence can also be handled if the <italic>F</italic>(<bold>y</bold>; <bold><italic>θ</italic></bold><sub><italic>j</italic></sub>, <bold>x</bold><sub><italic>i</italic></sub>) specify the <italic>conditional</italic> distribution of <bold>y</bold><sub><italic>i</italic></sub> given previous responses <bold>y</bold><sub>1</sub>, …, <bold>y</bold><sub><italic>i</italic>−1</sub> as in [<xref ref-type="bibr" rid="pone.0181790.ref013">13</xref>]. Each response observation <italic>y</italic><sub><italic>ij</italic></sub> in the cluster <bold>y</bold><sub><italic>i</italic></sub> has been assumed in our notation to be related to the same set of covariates <bold>x</bold><sub><italic>i</italic></sub>, as that is most relevant to our situation, but if this assumption were relaxed the below results would still apply.</p>
<p>The PIT-trap is simply a bootstrap method that resamples PIT-residuals {<bold>u</bold><sub>1</sub>, …,<bold>u</bold><sub><italic>n</italic></sub>}, then uses these to construct resampled values <inline-formula id="pone.0181790.e009"><alternatives><graphic id="pone.0181790.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mrow><mml:mo>{</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">y</mml:mi> <mml:mn>1</mml:mn> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>n</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> by inverting the cumulative distribution function. The PIT-trap algorithm computes resamples <italic>T</italic>* to approximate the sampling distribution of some statistic <italic>T</italic> = <italic>g</italic>(<bold>y</bold>) as follows:</p>
<list list-type="order">
<list-item>
<p>Estimate <inline-formula id="pone.0181790.e010"><alternatives><graphic id="pone.0181790.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> by fitting the regression model to the observed <bold>y</bold>.</p>
</list-item>
<list-item>
<p>Generate an <italic>n</italic> × <italic>p</italic> matrix, <bold>q</bold>, of independent random values from the standard uniform distribution.</p>
</list-item>
<list-item>
<p>Calculate an <italic>n</italic> × <italic>p</italic> matrix of PIT residuals <inline-formula id="pone.0181790.e011"><alternatives><graphic id="pone.0181790.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:mi mathvariant="bold">u</mml:mi> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">u</mml:mi> <mml:mn>1</mml:mn> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">u</mml:mi> <mml:mi>n</mml:mi> <mml:mi>T</mml:mi></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> by applying <xref ref-type="disp-formula" rid="pone.0181790.e004">Eq (1)</xref> element-wise to {<bold>y</bold>, <bold>q</bold>}. Optionally, each column can be centered and rescaled (see below).</p>
</list-item>
<list-item>
<p>For <italic>b</italic> = 1 … <italic>B</italic>:</p>
<list list-type="alpha-lower">
<list-item>
<p>Resample with replacement the <italic>n</italic> vectors {<bold>u</bold><sub>1</sub>, …, <bold>u</bold><sub><italic>n</italic></sub>}, to obtain <inline-formula id="pone.0181790.e012"><alternatives><graphic id="pone.0181790.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:mo>{</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">u</mml:mi> <mml:mn>1</mml:mn> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">u</mml:mi> <mml:mi>n</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</list-item>
<list-item>
<p>Calculate resampled <italic>p</italic>-variate observations <inline-formula id="pone.0181790.e013"><alternatives><graphic id="pone.0181790.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mrow><mml:mo>{</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">y</mml:mi> <mml:mn>1</mml:mn> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>n</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> by solving for each <inline-formula id="pone.0181790.e014"><alternatives><graphic id="pone.0181790.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:msubsup><mml:mi>y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> as a function of <inline-formula id="pone.0181790.e015"><alternatives><graphic id="pone.0181790.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:msubsup><mml:mi>u</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> (the <italic>j</italic>th elements of <inline-formula id="pone.0181790.e016"><alternatives><graphic id="pone.0181790.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:msubsup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0181790.e017"><alternatives><graphic id="pone.0181790.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:msubsup><mml:mi mathvariant="bold">u</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> respectively):
<disp-formula id="pone.0181790.e018"><alternatives><graphic id="pone.0181790.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e018" xlink:type="simple"/><mml:math display="block" id="M18"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo>;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if </mml:mtext><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext> is continuous</mml:mtext></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>−</mml:mo></mml:msup><mml:mo>;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:msubsup><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo>&lt;</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo>;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathsize="normal" mathvariant="bold"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if </mml:mtext><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext> is discrete</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives></disp-formula></p>
</list-item>
<list-item>
<p>Compute the required statistic, <italic>T</italic>* = <italic>g</italic>(<bold>y</bold>*).</p>
</list-item>
</list>
</list-item>
</list>
<p specific-use="continuation">If data are discrete, then some randomness has been introduced into residual calculation by <bold>q</bold>. This can be accounted for in the PIT-trap algorithm by recalculating <bold>q</bold> and <bold>u</bold> for each resample, i.e. moving steps 2-3 inside the loop at step 4 rather than leaving them outside, at some (usually small) computational cost.</p>
<p>This algorithm is especially suited to situations where we have a reasonable idea of the nature of the marginal distribution of the <italic>y</italic><sub><italic>ij</italic></sub>, but relatively little knowledge of the correlation within clusters. If the marginal distribution <inline-formula id="pone.0181790.e019"><alternatives><graphic id="pone.0181790.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:mi>F</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is correct then PIT-residuals may be correlated across response variables but they will (asymptotically) be marginally standard uniform. By resampling rows (clusters) of data <inline-formula id="pone.0181790.e020"><alternatives><graphic id="pone.0181790.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:mo>{</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">u</mml:mi> <mml:mn>1</mml:mn> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">u</mml:mi> <mml:mi>n</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> as proposed, the correlation structure in the <italic>p</italic>-variate observations is preserved in resamples and subsequently accounted for in inferences.</p>
<p>In small samples, we have found improved performance when centering and rescaling residuals, as in [<xref ref-type="bibr" rid="pone.0181790.ref019">19</xref>] and [<xref ref-type="bibr" rid="pone.0181790.ref020">20</xref>]. The method used was to map them onto the standard normal distribution (Φ<sup>−1</sup>(<italic>u</italic><sub><italic>ij</italic></sub>)) and divide by their sample standard deviation, denoted as <italic>s</italic><sub>Φ<sup>−1</sup>(<italic>u</italic><sub><italic>j</italic></sub>)</sub>. That is, we calculated rescaled residuals <inline-formula id="pone.0181790.e021"><alternatives><graphic id="pone.0181790.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:msub><mml:mover accent="true"><mml:mi>u</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula> to satisfy
<disp-formula id="pone.0181790.e022"><alternatives><graphic id="pone.0181790.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mo>Φ</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>u</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msup><mml:mo>Φ</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>u</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>/</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:msup><mml:mo>Φ</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>u</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
This rescaling can be understood as an empirical correction for the fact that the model typically overfits the data, thus underestimates the magnitude of residuals. This is especially the case in small samples and the effect tends to vanish as sample size increases.</p>
<sec id="sec004">
<title>Properties of the PIT-trap</title>
<p>In the below we will derive some attractive properties of the PIT-trap algorithm.</p>
<p><bold>Theorem 1</bold> <italic>Consider a PIT-trap sample of</italic> <bold>Y</bold>, <italic>where the</italic> (<italic>i</italic>, <italic>j</italic>)<italic>th PIT-trap value</italic> <inline-formula id="pone.0181790.e023"><alternatives><graphic id="pone.0181790.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:msubsup><mml:mi>Y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> <italic>is computed using a plug-in estimate of the marginal distribution of</italic> <italic>Y</italic><sub><italic>ij</italic></sub>, <inline-formula id="pone.0181790.e024"><alternatives><graphic id="pone.0181790.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mrow><mml:mi>F</mml:mi> <mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, <italic>and the true marginal distribution of</italic> <italic>Y</italic><sub><italic>ij</italic></sub> is <italic>F</italic>(<italic>y</italic>; <bold><italic>θ</italic></bold><sub><italic>j</italic></sub>, <bold>x</bold><sub><italic>i</italic></sub>). <italic>Assume</italic> <italic>F</italic>(<italic>y</italic>; <bold><italic>θ</italic></bold><sub><italic>j</italic></sub>, <bold>x</bold><sub><italic>i</italic></sub>) <italic>is twice differentiable with respect to</italic> <bold><italic>θ</italic></bold><sub><italic>j</italic></sub>.</p>
<p><italic>For each j</italic>, <italic>if</italic> <inline-formula id="pone.0181790.e025"><alternatives><graphic id="pone.0181790.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula> <italic>is a</italic> <inline-formula id="pone.0181790.e026"><alternatives><graphic id="pone.0181790.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math></alternatives></inline-formula>-<italic>consistent estimator of</italic> <bold><italic>θ</italic></bold><sub><italic>j</italic></sub>, <italic>then</italic>:
<disp-formula id="pone.0181790.e027"><alternatives><graphic id="pone.0181790.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e027" xlink:type="simple"/><mml:math display="block" id="M27"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mo>*</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>Y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>≤</mml:mo> <mml:mi>y</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>O</mml:mi> <mml:mi>p</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula> <italic>where</italic> <inline-formula id="pone.0181790.e028"><alternatives><graphic id="pone.0181790.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mo>*</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="script">A</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> <italic>denotes the probability of</italic> <inline-formula id="pone.0181790.e029"><alternatives><graphic id="pone.0181790.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mi mathvariant="script">A</mml:mi></mml:math></alternatives></inline-formula> <italic>from repeated PIT-trapping</italic>.</p>
<p specific-use="continuation">The proof, and indeed all proofs, can be found in on-line supplementary material (<xref ref-type="supplementary-material" rid="pone.0181790.s001">S1 File</xref>).</p>
<p>Theorem 1 shows that when applied to a large data set, the distribution of a PIT-trapped observation <inline-formula id="pone.0181790.e030"><alternatives><graphic id="pone.0181790.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:msubsup><mml:mi>y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> approximates the true marginal distribution of the corresponding original observation <italic>y</italic><sub><italic>ij</italic></sub>, converging to the target marginal distribution asymptotically. The conditions under which this result was derived—a known parametric form for the marginal distribution <italic>F</italic>(<italic>y</italic>; <bold><italic>θ</italic></bold><sub><italic>j</italic></sub>, <bold>x</bold><sub><italic>i</italic></sub>), and a <inline-formula id="pone.0181790.e031"><alternatives><graphic id="pone.0181790.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math></alternatives></inline-formula>-consistent estimate of the parameter <inline-formula id="pone.0181790.e032"><alternatives><graphic id="pone.0181790.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>—considerably relaxes the conditions previously required for residual resampling to be applicable, to the point where the method can readily be applied to most common parametric regression models, including the important case of generalized linear models for discrete data.</p>
<p>Theorem 1 can be generalized to handle misspecification of the marginal distribution, if the “working” marginal distribution leads to pivotal PIT-residuals. This follows since the proof to Theorem 1 does not use the fact that the PIT-residuals <italic>u</italic><sub><italic>ij</italic></sub> come from a standard uniform distribution, it only requires them to be iid.</p>
<p>The PIT-trap, when applied to clusters of correlated data, will preserve the correlation in the data, as in the following theorem.</p>
<p><bold>Theorem 2</bold> <italic>Consider p-variate PIT-trap residuals</italic> <inline-formula id="pone.0181790.e033">
<alternatives>
<graphic id="pone.0181790.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e033" xlink:type="simple"/>
<mml:math display="inline" id="M33">
<mml:msubsup>
<mml:mi mathvariant="bold">U</mml:mi>
<mml:mi>i</mml:mi>
<mml:mo>*</mml:mo>
</mml:msubsup>
</mml:math>
</alternatives>
</inline-formula>, <italic>i</italic> = 1, …, <italic>n</italic>, <italic>obtained by resampling PIT-residuals with replacement. Let</italic> <inline-formula id="pone.0181790.e034"><alternatives><graphic id="pone.0181790.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mover accent="true"><mml:mo>Σ</mml:mo> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> <italic>be the sample variance-covariance matrix of PIT-residuals</italic>, <inline-formula id="pone.0181790.e035"><alternatives><graphic id="pone.0181790.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:mrow><mml:mover accent="true"><mml:mo>Σ</mml:mo> <mml:mo>^</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>n</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:msubsup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">u</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">u</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">u</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">u</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. <italic>Then</italic>: 
<disp-formula id="pone.0181790.e036"><alternatives><graphic id="pone.0181790.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e036" xlink:type="simple"/><mml:math display="block" id="M36"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>v</mml:mi> <mml:mi>a</mml:mi> <mml:msub><mml:mi>r</mml:mi> <mml:mo>*</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">U</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mover accent="true"><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula> <italic>where var</italic><sub>*</sub>(⋅) <italic>denotes the variance-covariance matrix under repeated PIT-trapping</italic>.</p>
<p specific-use="continuation">The proof is straightforward and is omitted.</p>
<p>Note that Theorem 2 does not necessarily imply that <inline-formula id="pone.0181790.e037"><alternatives><graphic id="pone.0181790.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e037" xlink:type="simple"/><mml:math display="inline" id="M37"><mml:mrow><mml:mi>v</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">U</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>→</mml:mo> <mml:mi>v</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">U</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> because it may not be the case that the <bold>U</bold><sub><italic>i</italic></sub> share the same variance-covariance matrix <bold>Σ</bold>. If they do not, then the <bold>U</bold><sub><italic>i</italic></sub> are not identically distributed and the PIT-trap might not have desirable properties for multivariate inference. On the other hand, when the <bold>U</bold><sub><italic>i</italic></sub> are identically distributed, we can show the following result.</p>
<p><bold>Theorem 3</bold> <italic>Let T</italic> = <italic>g</italic>(<bold>Y</bold>) <italic>be an asymptotically standard normal statistic calculated from some multivariate sample</italic> <bold>Y</bold> <italic>characterized by its marginal distributions F</italic>(<italic>y</italic>; <bold><italic>θ</italic></bold><sub><italic>j</italic></sub>, <bold>x</bold><sub><italic>i</italic></sub>) <italic>and the variance-covariance matrix of PIT-residuals</italic>, <italic>var</italic>(<bold>U</bold><sub><italic>i</italic></sub>) = <bold>Σ</bold>. <italic>Let T</italic>* = <italic>g</italic>(<bold>Y</bold>*) <italic>be the same statistic calculated from the PIT-trap sample</italic> <bold>Y</bold>* <italic>using</italic> <inline-formula id="pone.0181790.e038"><alternatives><graphic id="pone.0181790.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:mrow><mml:mi>F</mml:mi> <mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. <italic>Assume</italic> <bold>Y</bold> <italic>and g</italic>(⋅) <italic>are such that T admits an Edgeworth expansion. If</italic> <inline-formula id="pone.0181790.e039"><alternatives><graphic id="pone.0181790.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula> <italic>is</italic> <inline-formula id="pone.0181790.e040"><alternatives><graphic id="pone.0181790.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math></alternatives></inline-formula>-<italic>consistent for each</italic> <italic>j</italic> <italic>then</italic> <disp-formula id="pone.0181790.e041"><alternatives><graphic id="pone.0181790.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e041" xlink:type="simple"/><mml:math display="block" id="M41"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mo>*</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>T</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>≤</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>T</mml:mi> <mml:mo>≤</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>O</mml:mi> <mml:mi>p</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula> <disp-formula id="pone.0181790.e042"><alternatives><graphic id="pone.0181790.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e042" xlink:type="simple"/><mml:math display="block" id="M42"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>P</mml:mi> <mml:mo>*</mml:mo></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mi>t</mml:mi> <mml:mo>≤</mml:mo> <mml:msup><mml:mi>T</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>≤</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mi>t</mml:mi> <mml:mo>≤</mml:mo> <mml:mi>T</mml:mi> <mml:mo>≤</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>O</mml:mi> <mml:mi>p</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>n</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>3</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
This theorem parallels standard bootstrap results [<xref ref-type="bibr" rid="pone.0181790.ref021">21</xref>] and can be proved in a similar way, using Edgeworth expansions (with some modification to handle the discrete case).</p>
<p>The conditions under which <italic>T</italic> admits an Edgeworth expansion, required for Theorem 3, can be found in [<xref ref-type="bibr" rid="pone.0181790.ref021">21</xref>], but briefly, <bold>Y</bold> must satisfy Cramér’s condition, the first four moments of <bold>Y</bold> must be finite, and <italic>g</italic> must be able to be expressed as a smooth function of means of independent random variables (in the sense that it is four times differentiable in the neighbourhood of the mean).</p>
<p>We use simulation later to consider the question of how robust the performance of the PIT-trap is to settings where the conditions of Theorem 3 are not satisfied—specifically, we are concerned about situations where the distributional assumptions are mildly misspecified.</p>
<p>The requirement that the joint distribution of <bold>Y</bold> be characterized by its marginal distributions and the variance-covariance matrix of PIT-residuals <bold>Σ</bold> is not unlike working assumptions often made in the generalized estimating equations literature [<xref ref-type="bibr" rid="pone.0181790.ref001">1</xref>], but it also has an interesting connection to the literature on copula models [<xref ref-type="bibr" rid="pone.0181790.ref022">22</xref>]. Copula models are specified via a similar structure to the PIT-trap, and one way to view the PIT-trap is as a bootstrap method for a type of copula model. In the simulation section we use a copula model to generate correlated data in order to assess the performance of the PIT-trap.</p>
</sec>
<sec id="sec005">
<title>Relation to other bootstrap methods</title>
<p>Beyond [<xref ref-type="bibr" rid="pone.0181790.ref014">14</xref>, <xref ref-type="bibr" rid="pone.0181790.ref015">15</xref>, <xref ref-type="bibr" rid="pone.0181790.ref017">17</xref>], the PIT-trap has relationships to other bootstrap methods, most closely, the parametric bootstrap [<xref ref-type="bibr" rid="pone.0181790.ref002">2</xref>]. Roughly speaking, the PIT-trap can be understood as a compromise between the parametric bootstrap and residual resampling, inheriting the attractive features of each method—the ability to generate data (approximately) from <italic>F</italic>(<italic>y</italic>; <bold><italic>θ</italic></bold><sub><italic>j</italic></sub>, <bold>x</bold><sub><italic>i</italic></sub>) is inherited from the former, and the ability to preserve correlation in clustered data is inherited from the latter.</p>
<p>The parametric bootstrap follows a very similar algorithm to the PIT-trap, the key difference being that it obtains PIT-residuals (<inline-formula id="pone.0181790.e043"><alternatives><graphic id="pone.0181790.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:msubsup><mml:mi>u</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula>) by simulating them instead of resampling them. That is, the parametric bootstrap can be understood as replacing step 4a of the PIT-trap algorithm as follows:</p>
<list list-type="simple">
<list-item>
<label>(a)</label>
<p>Generate <inline-formula id="pone.0181790.e044"><alternatives><graphic id="pone.0181790.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e044" xlink:type="simple"/><mml:math display="inline" id="M44"><mml:msubsup><mml:mi>u</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> from <inline-formula id="pone.0181790.e045"><alternatives><graphic id="pone.0181790.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:mrow><mml:mi mathvariant="script">U</mml:mi> <mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</list-item>
</list>
<p specific-use="continuation">Hence the inverse transformation of step 4b generates samples directly from <inline-formula id="pone.0181790.e046"><alternatives><graphic id="pone.0181790.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mrow><mml:mi>F</mml:mi> <mml:mo>(</mml:mo> <mml:mi>y</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mi>j</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. (Step 3 is redundant for the parametric bootstrap and could be removed.)</p>
<p>[<xref ref-type="bibr" rid="pone.0181790.ref014">14</xref>] proposed the limit model-free bootstrap, and advocated the approach for bootstrapping discrete data. The procedure requires as a starting point a transformation <italic>G</italic>(<bold>y</bold><sub><italic>i</italic></sub>) of the data to “iid-ness”. Then one simulates standard uniform values <inline-formula id="pone.0181790.e047"><alternatives><graphic id="pone.0181790.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e047" xlink:type="simple"/><mml:math display="inline" id="M47"><mml:msubsup><mml:mi>u</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> and obtains bootstrapped values as <inline-formula id="pone.0181790.e048"><alternatives><graphic id="pone.0181790.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">y</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>=</mml:mo> <mml:msup><mml:mi>G</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold">u</mml:mi> <mml:mi>i</mml:mi> <mml:mo>*</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Note that, by the probability integral transform, this generates independent data with distribution function <italic>G</italic>(<bold>y</bold><sub><italic>i</italic></sub>), hence the limit model-free bootstrap can be understood as a parametric bootstrap assuming <italic>G</italic>(<bold>y</bold><sub><italic>i</italic></sub>).</p>
<p>In the case of multivariate or clustered data, step 4a of a parametric bootstrap involves simulating vectors of <inline-formula id="pone.0181790.e049"><alternatives><graphic id="pone.0181790.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:msubsup><mml:mi mathvariant="bold">u</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> with covariance matrix <inline-formula id="pone.0181790.e050"><alternatives><graphic id="pone.0181790.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e050" xlink:type="simple"/><mml:math display="inline" id="M50"><mml:mover accent="true"><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, but where each element is marginally <inline-formula id="pone.0181790.e051"><alternatives><graphic id="pone.0181790.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e051" xlink:type="simple"/><mml:math display="inline" id="M51"><mml:mrow><mml:mi mathvariant="script">U</mml:mi> <mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. <inline-formula id="pone.0181790.e052"><alternatives><graphic id="pone.0181790.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:mover accent="true"><mml:mo mathvariant="bold">Σ</mml:mo> <mml:mo>^</mml:mo></mml:mover></mml:math></alternatives></inline-formula> should be a consistent estimate of Σ.</p>
<p>A key point of difference between the parametric bootstrap and the PIT-trap is in their assumptions. The PIT-trap resamples <italic>u</italic><sub><italic>ij</italic></sub> under the assumption that the PIT-residuals <italic>u</italic><sub><italic>ij</italic></sub> are iid for <italic>i</italic> = 1, …, <italic>n</italic> in the limit as <italic>n</italic> → ∞. An important situation where this assumption can be satisfied is when the marginal distribution of the <italic>y</italic><sub><italic>ij</italic></sub> has been correctly specified and PIT-residuals share a common correlation structure. The parametric bootstrap, in contrast, requires the full joint distribution of data to be specified correctly—i.e. the correlation structure must be known in order to simulate <inline-formula id="pone.0181790.e053"><alternatives><graphic id="pone.0181790.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e053" xlink:type="simple"/><mml:math display="inline" id="M53"><mml:msubsup><mml:mi>u</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mo>*</mml:mo></mml:msubsup></mml:math></alternatives></inline-formula> with the required correlation structure. Hence the PIT-trap is applicable in some settings where the parametric bootstrap is not—in particular, generalized estimating equations, for which a joint distribution is not specified, and situations where data are clustered but the precise form of within-cluster correlation is not well understood. An important example of where the within-cluster correlation is not well understood is when <bold>y</bold> is high-dimensional [<xref ref-type="bibr" rid="pone.0181790.ref010">10</xref>], or more generally, when <italic>p</italic> is not small compared to <italic>n</italic>.</p>
<p>A different way to view the PIT-trap is as a special type of residual resampling, indeed it inherits some advantages from residual resampling—in particular, the ability to resample clustered data in such a way that any within-cluster correlation between residuals can be preserved. Also, residual resampling methods are often advocated in the regression context [<xref ref-type="bibr" rid="pone.0181790.ref002">2</xref>, <xref ref-type="bibr" rid="pone.0181790.ref021">21</xref>] because they preserve the conditioning on <bold>x</bold><sub><italic>i</italic></sub>. The key issue when implementing residual resampling for non-Gaussian regression models, most commonly using Pearson and deviance residuals [<xref ref-type="bibr" rid="pone.0181790.ref002">2</xref>, <xref ref-type="bibr" rid="pone.0181790.ref023">23</xref>], has been that they do not result in pivotal quantities in general. Subsequent residual-resampled data <bold>Y</bold>* does not adequately approximate the sampling distribution of <bold>Y</bold> in general. For discrete and sparse data, as in <xref ref-type="table" rid="pone.0181790.t001">Table 1</xref>, subsequent bootstrap samples would deviate conspicuously from the desired distribution, some values being non-integer, negative or even undefined. Correcting for this issue could introduce considerable bias. In contrast, the PIT-trap circumvents these difficulties, provided that the parametric form of the marginal distribution of data is known.</p>
<p>A final set of relations of interest link the PIT-trap to classical resampling methods for models with iid errors. Consider first the model <italic>y</italic><sub><italic>ij</italic></sub> = <italic>μ</italic><sub><italic>ij</italic></sub> + <italic>ϵ</italic><sub><italic>ij</italic></sub>, where the <italic>μ</italic><sub><italic>ij</italic></sub> are fixed and the random errors <italic>ϵ</italic><sub><italic>ij</italic></sub> are parameterized by their standard deviation <italic>σ</italic> only, linear regression being an important special case. For such models, raw residuals <inline-formula id="pone.0181790.e054"><alternatives><graphic id="pone.0181790.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e054" xlink:type="simple"/><mml:math display="inline" id="M54"><mml:mrow><mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> are monotonically related to PIT-residuals:
<disp-formula id="pone.0181790.e055"><alternatives><graphic id="pone.0181790.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e055" xlink:type="simple"/><mml:math display="block" id="M55"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>u</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>σ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>F</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>r</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>;</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>σ</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
This monotonicity implies that bootstrapping PIT-residuals is equivalent to the standard residual resampling approach where raw residuals are bootstrapped [<xref ref-type="bibr" rid="pone.0181790.ref002">2</xref>], if one assumes errors are iid. Now consider the situation where we wish to test the null hypothesis that all observations are iid. In this case, and by a similar argument, the PIT-trap reduces to resampling the <bold>y</bold><sub><italic>i</italic></sub> with replacement. Further, if resampling PIT-residuals without replacement, this would reduce to the usual permutation test [<xref ref-type="bibr" rid="pone.0181790.ref003">3</xref>]. Hence many classical resampling methods can be understood as special cases of the PIT-trap, the key innovation of the PIT-trap being its ability to extend these well-known resampling methods to parametric modeling where errors are no longer iid.</p>
</sec>
</sec>
<sec id="sec006">
<title>Practical application</title>
<p>In this section the PIT-trap will be applied to the data of <xref ref-type="table" rid="pone.0181790.t001">Table 1</xref> [<xref ref-type="bibr" rid="pone.0181790.ref008">8</xref>]. This data set consists of counts of the abundance of twelve species of copepod (small crustaceans) in 16 sites. The study was conducted to explore the effect of crab disturbance on copepod communities on intertidal sandflats on the Tasman Peninsular, Australia. A randomized blocks design was used where two disturbed and two undisturbed sites were sampled at each of four sites (“blocks”).</p>
<p>The purpose of analysis was to test for evidence of an effect of disturbance (treatment effect), and whether the effect was different at different sites (block×treatment interaction). When testing for interaction, difficulties arise for resampling-based hypothesis testing because observations are not exchangeable under the null hypothesis, and instead residual resampling has been proposed [<xref ref-type="bibr" rid="pone.0181790.ref003">3</xref>].</p>
<p>Three important properties of the data are evident in <xref ref-type="table" rid="pone.0181790.t001">Table 1</xref>. Firstly, there are many zeros in the data, because not every species is observed in every site. Secondly, abundance appears to be strongly right-skewed. Each of these features is problematic for most types of residuals that have previously been defined [<xref ref-type="bibr" rid="pone.0181790.ref002">2</xref>, <xref ref-type="bibr" rid="pone.0181790.ref023">23</xref>]. Thirdly, the number of variables (<italic>p</italic> = 12) is not small compared to the number of observations (<italic>n</italic> = 16), which motivates the use of resampling for inference as in [<xref ref-type="bibr" rid="pone.0181790.ref009">9</xref>, <xref ref-type="bibr" rid="pone.0181790.ref010">10</xref>].</p>
<p>Negative binomial regression models [<xref ref-type="bibr" rid="pone.0181790.ref024">24</xref>] were fitted to the data, to account for overdispersion in the counts. Let <italic>Y</italic><sub><italic>ijkl</italic></sub> be the abundance of the <italic>l</italic>th replicate (<italic>l</italic> ∈ {1, 2}) at site <italic>k</italic> ∈ {1, 2, 3, 4} of species <italic>j</italic> ∈ {1, …, 12} in treatment <italic>i</italic> ∈ {1, 2}. We assumed <italic>Y</italic><sub><italic>ijkl</italic></sub> have a negative binomial marginal distribution with mean <italic>μ</italic><sub><italic>ijk</italic></sub> satisfying:
<disp-formula id="pone.0181790.e056"><alternatives><graphic id="pone.0181790.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e056" xlink:type="simple"/><mml:math display="block" id="M56"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo form="prefix">log</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mn>0</mml:mn> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>γ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where the <italic>α</italic><sub><italic>ij</italic></sub>, <italic>β</italic><sub><italic>kj</italic></sub> and <italic>γ</italic><sub><italic>ijk</italic></sub> respectively represent treatment, block and treatment×block effects for the <italic>j</italic>th species, and for identifiability, each of these terms is zero whenever it is indexed by an <italic>i</italic> or <italic>k</italic> which equals one.</p>
<p>The variance was assumed to be a quadratic function of the mean:
<disp-formula id="pone.0181790.e057"><alternatives><graphic id="pone.0181790.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e057" xlink:type="simple"/><mml:math display="block" id="M57"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>V</mml:mi> <mml:mi>a</mml:mi> <mml:mi>r</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>Y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi> <mml:mi>k</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>ψ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:msubsup><mml:mi>μ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
and a residual plot (<xref ref-type="fig" rid="pone.0181790.g001">Fig 1b</xref>) suggested this model accounted for overdispersion in the data reasonably well.</p>
<p>Model parameters were estimated separately for each species via maximum likelihood. This corresponds to a working assumption of independence across species, but we expect that there is in fact correlation across species, despite it not being explicitly modelled in the above.</p>
<p>We tested the hypothesis of no interaction:
<disp-formula id="pone.0181790.e058"><alternatives><graphic id="pone.0181790.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e058" xlink:type="simple"/><mml:math display="block" id="M58"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>:</mml:mo> <mml:msub><mml:mi>γ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mspace width="4pt"/><mml:mo>∀</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi> <mml:mspace width="1.em"/><mml:msub><mml:mi>H</mml:mi> <mml:mi>a</mml:mi></mml:msub> <mml:mo>:</mml:mo> <mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
using a score statistic based on a ridge-regularized estimate of the correlation matrix of residuals [<xref ref-type="bibr" rid="pone.0181790.ref010">10</xref>]. This approach incorporates correlation between species into the test statistic, using linear shrinkage towards an identity matrix in order to obtain a more numerically stable statistic which has better properties [<xref ref-type="bibr" rid="pone.0181790.ref009">9</xref>, <xref ref-type="bibr" rid="pone.0181790.ref010">10</xref>]. We assessed the significance of this test statistic using 1000 resamples from each of a residual bootstrap using Pearson residuals, the PIT-trap with negative binomial marginals, and a parametric bootstrap which simulated from a copula model with an unstructured correlation matrix, and again assumed negative binomial marginals. The copula was fitted in a two-step approach, using negative binomial regression again to estimate parameters in the marginal distribution, and a ridge-regularised estimate of the correlation matrix [<xref ref-type="bibr" rid="pone.0181790.ref010">10</xref>] of PIT-residuals mapped onto the standard normal, Φ<sup>−1</sup>(<italic>u</italic><sub><italic>ij</italic></sub>)</p>
<p>The observed test statistic for a test for no interaction was 40.99, and the bootstrap estimates of the <italic>P</italic>-value for the PIT-trap, parametric bootstrap and Pearson residual bootstrap were (to three decimal places) 0.039, 0.046, and 0.014, respectively. The difference between the Pearson result and the other two is larger than would be expected by Monte Carlo error, and while all suggest some evidence of an interaction effect, only marginally so for the PIT-trap and parametric bootstrap.</p>
<p>We have developed freely available software in the <monospace>R</monospace> package <monospace>mvabund</monospace> [<xref ref-type="bibr" rid="pone.0181790.ref025">25</xref>], indeed our PIT-trap and Pearson residual bootstrap results can be easily replicated using the following code:</p>
<p specific-use="line">
<monospace>data(Tasmania)</monospace>
</p>
<p specific-use="line">
<monospace>attach(Tasmania)</monospace>
</p>
<p specific-use="line">
<monospace>abund = mvabund(copepods)</monospace>
</p>
<p specific-use="line">
<monospace>ftMain = manyglm(abund~block+treatment, family = “negative.binomial”)</monospace>
</p>
<p specific-use="line">
<monospace>plot(ftMain)</monospace>
</p>
<p specific-use="line">
<monospace>ftInter = manyglm(abund~block*treatment, family = “negative.binomial”)</monospace>
</p>
<p specific-use="line">
<monospace>anova(ftMain, ftInter, cor.type = “shrink”, test = “score”, p.uni = “adjust”, resamp=***)</monospace>
</p>
<p specific-use="continuation">where <monospace>***</monospace> is chosen to be “<monospace>pit.trap</monospace>” or “<monospace>residual</monospace>” (Pearson residuals).</p>
</sec>
<sec id="sec007">
<title>Simulations</title>
<p>We conducted two simulation studies to investigate the small sample properties of the PIT-trap, in comparison to its immediate competitors.</p>
<sec id="sec008">
<title>Logistic regression</title>
<p>First we studied logistic regression in small to moderate samples. We compared the properties of likelihood ratio tests where the null distribution was estimated using the PIT-trap, the Pearson residual bootstrap, the parametric bootstrap, case resampling, and the usual chi-squared approximation from classical statistics.</p>
<p>The sampling design, inspired by <xref ref-type="table" rid="pone.0181790.t001">Table 1</xref>, was a randomized blocks design with four blocks, two treatments, and balanced sampling. We generated Bernoulli random variables with the mean in the <italic>i</italic>th treatment and the <italic>k</italic>th block given by:
<disp-formula id="pone.0181790.e059"><alternatives><graphic id="pone.0181790.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e059" xlink:type="simple"/><mml:math display="block" id="M59"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo form="prefix">log</mml:mo> <mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>μ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mi>k</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>γ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula>
where as previously the <italic>α</italic><sub><italic>i</italic></sub>, <italic>β</italic><sub><italic>k</italic></sub> and <italic>γ</italic><sub><italic>ik</italic></sub> respectively represent treatment, block and treatment×block effects, and for identifiability, each of these terms is zero whenever it is indexed by an <italic>i</italic> or <italic>k</italic> which equals one. We tested for no interaction:
<disp-formula id="pone.0181790.e060"><alternatives><graphic id="pone.0181790.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e060" xlink:type="simple"/><mml:math display="block" id="M60"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>H</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>:</mml:mo> <mml:msub><mml:mi>γ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mspace width="4pt"/><mml:mo>∀</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>k</mml:mi> <mml:mspace width="1.em"/><mml:msub><mml:mi>H</mml:mi> <mml:mi>a</mml:mi></mml:msub> <mml:mo>:</mml:mo> <mml:mtext>otherwise</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
This hypothesis is of biological interest, while also being difficult to resample data from, since response observations <italic>Y</italic><sub><italic>i</italic></sub> are not exchangeable under <italic>H</italic><sub>0</sub>.</p>
<p>We simulated 1000 data sets such that (<italic>α</italic><sub>0</sub>, <italic>α</italic><sub>2</sub>, <italic>β</italic><sub>2</sub>, <italic>β</italic><sub>3</sub>, <italic>β</italic><sub>4</sub>, <italic>γ</italic><sub>22</sub>, <italic>γ</italic><sub>23</sub>, <italic>γ</italic><sub>24</sub>) = (−1, 1, 0, −1, 1, 0, 0, 0) and varied the number of replicates such that total sample size ranged from 16 to 128. Each bootstrap test was conducted using 1000 bootstrap samples, and we compared Type I error at the 0.05 significance level.</p>
<p>The PIT-trap and parametric bootstrap converged rapidly to nominal Type I error levels (<xref ref-type="fig" rid="pone.0181790.g002">Fig 2</xref>), as expected from Theorem 3, with close to nominal levels when there were as few as four replicates per treatment group. The classical test for interaction, comparing the likelihood ratio statistic to a <inline-formula id="pone.0181790.e061"><alternatives><graphic id="pone.0181790.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e061" xlink:type="simple"/><mml:math display="inline" id="M61"><mml:msubsup><mml:mi>χ</mml:mi> <mml:mn>3</mml:mn> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> null distribution, was too liberal for small-moderate sample sizes, and was relatively slow to converge to nominal levels. Hence these results provide some motivation for using the PIT-trap or parametric bootstrap to test hypotheses concerning logistic regression parameters using small-moderate samples.</p>
<fig id="pone.0181790.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0181790.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Type I error of different resampling methods in logistic regression simulations varying <italic>n</italic>.</title>
<p>The shaded grey region represents a 95% confidence band around the nominal Type I error rate of 0.05. Note that both the PIT-trap and parametric regression performed well for <italic>n</italic> ≥ 32, the Pearson residual bootstrap was very poor for all <italic>n</italic>, and (for small-moderate <italic>n</italic>) classical tests via the <italic>χ</italic><sup>2</sup> distribution had inflated Type I error, motivating the use of resampling.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0181790.g002" xlink:type="simple"/>
</fig>
<p>A striking pattern in results (<xref ref-type="fig" rid="pone.0181790.g002">Fig 2</xref>) was that the Pearson residual bootstrap was grossly inadequate, with Type I error always in excess of four times the nominal level, and not converging to the nominal level as <italic>n</italic> increased. As explained previously, Pearson residual resamples (<bold>Y</bold>*) do not have the correct marginal distribution—resampled values typically are not integers and are not always between 0 and 1. The latter we believe is the greater problem, because it is necessary in test statistic construction to truncate data to the unit interval [0, 1], which introduces bias and changes the mean-variance relationship of the bootstrap samples.</p>
<p>Case resampling also had very highly inflated Type I error at small sample sizes, but converged towards correct values as <italic>n</italic> increased. Case resampling treats the number of replicates per treatment-block combination as random, which leads to unbalanced designs in resamples, with some treatment-block combinations often empty. This is the likely reason for the poor small-sample performance, with randomness in the design introducing additional uncertainty, considerably so for small <italic>n</italic>.</p>
</sec>
<sec id="sec009">
<title>Multivariate counts</title>
<p>A second simulation generated multivariate count data to mimic the properties of the data of <xref ref-type="table" rid="pone.0181790.t001">Table 1</xref>, looking at the effect of increasing dimension of the data as well as increasing sample size.</p>
<p>Correlated, overdispersed count data were generated via two methods—using a copula model, and a Poisson lognormal model. In the copula approach, we generated <bold>z</bold><sub><italic>i</italic></sub> ∼ <italic>MVN</italic>(<bold>0</bold>, <bold>R</bold>) for some correlation matrix <bold>R</bold>, then <italic>u</italic><sub><italic>ij</italic></sub> = Φ(<italic>z</italic><sub><italic>ij</italic></sub>) and <italic>Y</italic><sub><italic>ijkl</italic></sub> = <italic>F</italic><sup>−1</sup>(<italic>u</italic><sub><italic>ij</italic></sub>; <italic>μ</italic><sub><italic>ijk</italic></sub>, <italic>ψ</italic><sub><italic>j</italic></sub>) where Φ(<italic>x</italic>) and <italic>F</italic>(<italic>x</italic>; ⋅, ⋅) are the cumulative distribution functions of the standard normal and negative binomial distribution, respectively, and <italic>μ</italic><sub><italic>ijk</italic></sub> is defined in <xref ref-type="disp-formula" rid="pone.0181790.e056">Eq 2</xref>. Hence copula data had constant <italic>cov</italic>(<bold>U</bold><sub><italic>i</italic></sub>), as was required for Theorem 3. The Poisson lognormal model simulated counts as <italic>Y</italic><sub><italic>ijkl</italic></sub> ∼ <italic>Poisson</italic>(<italic>m</italic><sub><italic>ijk</italic></sub>) and
<disp-formula id="pone.0181790.e062"><alternatives><graphic id="pone.0181790.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0181790.e062" xlink:type="simple"/><mml:math display="block" id="M62"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo form="prefix">log</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mn>0</mml:mn> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>5</mml:mn> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mrow><mml:mi>k</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>γ</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi> <mml:mi>k</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>z</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <italic>z</italic><sub><italic>ij</italic></sub> was the <italic>j</italic>th element of <bold>z</bold><sub><italic>i</italic></sub> ∼ <italic>MVN</italic>(<bold>0</bold>, Σ<sub><italic>d</italic></sub><bold>R</bold>Σ<sub><italic>d</italic></sub>) and <italic>σ</italic><sub><italic>j</italic></sub> played the role of <italic>ψ</italic><sub><italic>j</italic></sub> in controlling the extent of overdispersion. The values of <italic>σ</italic><sub><italic>j</italic></sub> for the <italic>p</italic> response variables were stored in the diagonal matrix Σ<sub><italic>d</italic></sub>. This model maintained the same mean model and mean-variance relationship as for the negative binomial copula model, but data were no longer marginally negative binomial and no longer had constant <italic>cov</italic>(<bold>U</bold><sub><italic>i</italic></sub>). Hence this second simulation gives some insight into the robustness of the PIT-trap to (modest) violations of assumptions.</p>
<p>The values for slope parameters to be used in simulations were taken from the fit of the null (main effects) model to the sample data of <xref ref-type="table" rid="pone.0181790.t001">Table 1</xref>. To look at the effect of average species abundance on performance, we multiplied the matrix of the means <bold><italic>μ</italic></bold> by a factor <italic>δ</italic> ∈ {1, 2.5, 5}. The correlation matrix was set using an AR(1) structure with the autocorrelation parameter <italic>ρ</italic> ∈ {0.5, 0.6, 0.7, 0.8, 0.9} to look at the effect of strength of correlation structure on performance. Results were similar across different values of <italic>ρ</italic> so we only report <italic>ρ</italic> = 0.7 here. We varied the sample size <italic>n</italic> ∈ {16, 32, 64, 96, 128, 160} and number of variables <italic>p</italic> ∈ {12, 24, 36, 48, 60} by replicating the design matrix and the matrix of means <bold><italic>μ</italic></bold> as required.</p>
<p>We used the same testing procedure here as in the practical application described previously: fitting negative binomial distributions to each species, then constructing a score statistic which estimates correlation between variables using a ridge-regularized correlation matrix [<xref ref-type="bibr" rid="pone.0181790.ref010">10</xref>]. We compared results when significance of this statistic was assessed using Pearson residual resampling, the PIT-trap, and the parametric bootstrap assuming either an unstructured correlation matrix or incorrectly assuming an exchangeable correlation structure. The latter choice looks at the question of robustness of the parametric bootstrap to misspecification of the correlation structure.</p>
<p>As before, for each of 1000 sample data sets, 1000 resamples were used to estimate the <italic>P</italic>-value of a test for interaction, and the Type I error rate at the 0.05 level was recorded. This was very computationally intensive, taking a total of over a year of computation time on 2.8 GHz processors.</p>
<p>Type I error rates of the PIT-trap approached nominal levels as <italic>n</italic> increased (<xref ref-type="fig" rid="pone.0181790.g003">Fig 3</xref>), converged faster for larger mean abundances (<xref ref-type="fig" rid="pone.0181790.g003">Fig 3b and 3c</xref>), and remained close to nominal levels as <italic>n</italic> increased. Problems arose in simulations with low abundances (<xref ref-type="fig" rid="pone.0181790.g003">Fig 3a</xref>). When abundance was low and sample size small, all tests had inflated Type I error, although this settled down at larger sample sizes. Further, as <italic>p</italic> increased, all tests had inflated Type I error increasing with <italic>p</italic>, if abundance was low. For medium or high abundance this problem did not seem to arise for the PIT-trap.</p>
<fig id="pone.0181790.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0181790.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Type I error of different resampling methods in multivariate count simulations from a copula model varying <italic>n</italic> (left) and <italic>p</italic> (right).</title>
<p>Mean abundances were manipulated by multiplying the matrix of the means <bold><italic>μ</italic></bold> by a factor (a) <italic>δ</italic> = 1, (b) <italic>δ</italic> = 2.5, (c) <italic>δ</italic> = 5. The shaded grey region represents a 95% confidence band around the nominal Type I error rate of 0.05. The PIT-trap performed reasonably well in all contexts, although for low abundances it deviated from nominal levels for small <italic>n</italic> and large <italic>p</italic>. The parametric bootstrap also performed well, provided that the correlation structure was taken to be unstructured, it was less reliable if incorrectly assuming an exchangeable correlation structure.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0181790.g003" xlink:type="simple"/>
</fig>
<p>The PIT-trap seemed to perform better than alternatives. The Pearson residual bootstrap often had highly inflated Type I error, emphasising the costs of bootstrapping quantities that are not pivotal. The parametric bootstrap became problematic if the correlation structure was not correctly specified. When incorrectly assuming an exchangeable correlation structure, it became too liberal as <italic>n</italic> increased (<xref ref-type="fig" rid="pone.0181790.g003">Fig 3</xref>, left), and was highly conservative when <italic>p</italic> was not small for large mean abundances (<xref ref-type="fig" rid="pone.0181790.g003">Fig 3c</xref>, right). Using an unstructured correlation matrix in combination with the parametric bootstrap performed about as well as the PIT-trap in most cases.</p>
<p>Poisson-lognormal simulations suggested that under violations of the underlying data model, the PIT-trap maintained close to nominal Type I error rates as <italic>n</italic> increased, but Type I error became noticeably inflated at large <italic>p</italic> when mean abundances were low (<xref ref-type="fig" rid="pone.0181790.g004">Fig 4</xref>). There seemed to be little difference between the PIT-trap and parametric bootstrap in robustness to violation of underlying model assumptions.</p>
<fig id="pone.0181790.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0181790.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Type I error of different resampling methods in multivariate count simulations from a lognormal-Poisson model, varying <italic>n</italic> (left) and <italic>p</italic> (right).</title>
<p>Mean abundances were manipulated by multiplying the matrix of the means <bold><italic>μ</italic></bold> by a factor (a) <italic>δ</italic> = 1, (b) <italic>δ</italic> = 2.5, (c) <italic>δ</italic> = 5. The shaded grey region represents a 95% confidence band around the nominal Type I error rate of 0.05. The PIT-trap performed reasonably well in simulations varying <italic>n</italic>, but became inflated at large <italic>p</italic> because of failure of model assumptions, for low-medium abundances (a-b). The parametric bootstrap, assuming an unstructured correlation matrix, performed similarly.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0181790.g004" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec010" sec-type="conclusions">
<title>Discussion</title>
<p>In this paper we have described a very general residual resampling approach, based on probability integral transform, which we refer to as the PIT-trap. This can be understood as a special case of the model-free bootstrap [<xref ref-type="bibr" rid="pone.0181790.ref014">14</xref>], adapted to the problem of discrete, highly multivariate data. The method was demonstrated by theory and simulation to perform well even in a very challenging situation arising in ecology, where data were sparse, overdispersed, and high dimensional. Simulations suggest the method can perform reasonably well when <italic>p</italic> &gt; <italic>n</italic>, and under mild forms of model misspecification, although problems can arise when both of these elements may be present.</p>
<p>The PIT-trap is most closely related to the parametric bootstrap, and in simulations, these two methods behaved similarly (Figs <xref ref-type="fig" rid="pone.0181790.g002">2</xref>–<xref ref-type="fig" rid="pone.0181790.g004">4</xref>). A key distinction however is that the PIT-trap only requires knowledge of the marginal distribution of data. In contrast, the parametric bootstrap requires knowledge of the joint distribution, and can perform poorly when assumptions of correlation structure are incorrect. Further, one might expect the PIT-trap to have greater robustness to failure of assumptions in the marginal model (see on-line Appendix, Lemma 2), although this was not borne out in simulations (<xref ref-type="fig" rid="pone.0181790.g004">Fig 4</xref>).</p>
<p>Because it is a residual resampling method, the PIT-trap can be used to model clustered observations without explicitly specifying a model for how they are correlated. This is somewhat analogous to the setup under which the generalized estimating equations method was derived [<xref ref-type="bibr" rid="pone.0181790.ref001">1</xref>], and indeed the PIT-trap is readily applicable to problems where generalized estimating equations are used. Another important application, and the one which inspired this work, is the analysis of discrete multivariate data in ecology, when <italic>p</italic> may not be sufficiently small compared to <italic>n</italic> to adequately estimate the correlation structure (<xref ref-type="table" rid="pone.0181790.t001">Table 1</xref>). However simulations suggest that when <italic>p</italic> is too large compared to <italic>n</italic> the PIT-trap might not reliably maintain nominal levels. This is likely to arise in part because the PIT-trap will become increasingly sensitive to assumption violations in the marginal model as <italic>p</italic> increases (<xref ref-type="fig" rid="pone.0181790.g004">Fig 4</xref>). But it is also likely to arise in part because errors in the PIT-trap distribution accumulate across response variables, so for example the asymptotic one-tailed approximation of Theorem 3 can be written as a function of <italic>p</italic> as well as <italic>n</italic> as <italic>O</italic><sub><italic>p</italic></sub>(<italic>pn</italic><sup>−1</sup>). This quantity is not negligible when <italic>p</italic> is large.</p>
<p>PIT-residuals are pivotal measures of the agreement between observed and fitted values for any parametric regression model, and their pivotal nature is a particularly useful property. It is this property that makes them so useful in diagnostic tools [<xref ref-type="bibr" rid="pone.0181790.ref012">12</xref>, <xref ref-type="fig" rid="pone.0181790.g001">Fig 1b</xref>], and in this paper, this property has been exploited to develop a very general residual resampling scheme with desirable properties. We speculate that there may be other opportunities to improve methodology for parametric regression modelling via PIT-residuals.</p>
</sec>
<sec id="sec011">
<title>Supporting information</title>
<supplementary-material id="pone.0181790.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pone.0181790.s001" xlink:type="simple">
<label>S1 File</label>
<caption>
<title>Proofs of theorems.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>This work was supported by the Australian Research Council Future Fellowship (FT120100501) Discovery Scheme (DP0987729). For comments on the manuscript thanks to Gery Geenens and Francis Hui.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0181790.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Liang</surname> <given-names>KY</given-names></name>, <name name-style="western"><surname>Zeger</surname> <given-names>SL</given-names></name>. <article-title>Longitudinal data analysis using generalized linear models</article-title>. <source>Biometrika</source>. <year>1986</year>;<volume>73</volume>:<fpage>13</fpage>–<lpage>22</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/biomet/73.1.13" xlink:type="simple">10.1093/biomet/73.1.13</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0181790.ref002">
<label>2</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Davison</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Hinkley</surname> <given-names>DV</given-names></name>. <source>Bootstrap methods and their application</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>; <year>1997</year>.</mixed-citation>
</ref>
<ref id="pone.0181790.ref003">
<label>3</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Edgington</surname> <given-names>ES</given-names></name>. <source>Randomization tests</source>. <edition>3rd ed</edition>. <publisher-loc>New York</publisher-loc>: <publisher-name>Marcel Dekker</publisher-name>; <year>1995</year>.</mixed-citation>
</ref>
<ref id="pone.0181790.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wu</surname> <given-names>CFJ</given-names></name>. <article-title>Jackknife, bootstrap and other resampling methods in regression analysis</article-title>. <source>The Annals of Statistics</source>. <year>1986</year>;<volume>14</volume>(<issue>4</issue>):<fpage>1261</fpage>–<lpage>1295</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/aos/1176350161" xlink:type="simple">10.1214/aos/1176350161</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0181790.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hu</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Zidek</surname> <given-names>JV</given-names></name>. <article-title>A bootstrap based on the estimating equations of the linear model</article-title>. <source>Biometrika</source>. <year>1995</year>;<volume>82</volume>(<issue>2</issue>):<fpage>263</fpage>–<lpage>275</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/biomet/82.2.263" xlink:type="simple">10.1093/biomet/82.2.263</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0181790.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hu</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Kalbfleisch</surname> <given-names>JD</given-names></name>. <article-title>The estimating function bootstrap</article-title>. <source>Canadian Journal of Statistics</source>. <year>2000</year>;<volume>28</volume>(<issue>3</issue>):<fpage>449</fpage>–<lpage>481</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2307/3315958" xlink:type="simple">10.2307/3315958</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0181790.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>He</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Hu</surname> <given-names>F</given-names></name>. <article-title>Markov chain marginal bootstrap</article-title>. <source>Journal of the American Statistical Association</source>. <year>2002</year>;<volume>97</volume>(<issue>459</issue>):<fpage>783</fpage>–<lpage>795</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1198/016214502388618591" xlink:type="simple">10.1198/016214502388618591</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0181790.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Warwick</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Clarke</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Gee</surname> <given-names>JM</given-names></name>. <article-title>The effect of disturbance by soldier crabs, <italic>Mictyris platycheles</italic> H. Milne Edwards, on meiobenthic community structure</article-title>. <source>Journal of Experimental Marine Biology and Ecology</source>. <year>1990</year>;<volume>135</volume>:<fpage>19</fpage>–<lpage>33</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0022-0981(90)90196-J" xlink:type="simple">10.1016/0022-0981(90)90196-J</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0181790.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Warton</surname> <given-names>DI</given-names></name>. <article-title>Penalized normal likelihood and ridge regularization of correlation and covariance matrices</article-title>. <source>Journal of the American Statistical Association</source>. <year>2008</year>;<volume>103</volume>:<fpage>340</fpage>–<lpage>349</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1198/016214508000000021" xlink:type="simple">10.1198/016214508000000021</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0181790.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Warton</surname> <given-names>DI</given-names></name>. <article-title>Regularized sandwich estimators for analysis of high dimensional data using generalized estimating equations</article-title>. <source>Biometrics</source>. <year>2011</year>;<volume>67</volume>:<fpage>116</fpage>–<lpage>123</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1541-0420.2010.01438.x" xlink:type="simple">10.1111/j.1541-0420.2010.01438.x</ext-link></comment> <object-id pub-id-type="pmid">20528857</object-id></mixed-citation>
</ref>
<ref id="pone.0181790.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Smith</surname> <given-names>JQ</given-names></name>. <article-title>Diagnostic checks of non-standard time series models</article-title>. <source>Journal of Forecasting</source>. <year>1985</year>;<volume>4</volume>(<issue>3</issue>):<fpage>283</fpage>–<lpage>291</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/for.3980040305" xlink:type="simple">10.1002/for.3980040305</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0181790.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dunn</surname> <given-names>PK</given-names></name>, <name name-style="western"><surname>Smyth</surname> <given-names>GK</given-names></name>. <article-title>Randomized quantile residuals</article-title>. <source>Journal of Computational and Graphical Statistics</source>. <year>1996</year>;<volume>5</volume>(<issue>3</issue>):<fpage>236</fpage>–<lpage>244</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2307/1390802" xlink:type="simple">10.2307/1390802</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0181790.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brockwell</surname> <given-names>AE</given-names></name>. <article-title>Universal residuals: A multivariate transformation</article-title>. <source>Statistics and Probability Letters</source>. <year>2007</year>;<volume>77</volume>(<issue>14</issue>):<fpage>1473</fpage>—<lpage>1478</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.spl.2007.02.008" xlink:type="simple">10.1016/j.spl.2007.02.008</ext-link></comment> <object-id pub-id-type="pmid">18670587</object-id></mixed-citation>
</ref>
<ref id="pone.0181790.ref014">
<label>14</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Politis</surname> <given-names>DN</given-names></name>. <source>Model-Free Prediction and Regression: A Transformation-Based Approach to Inference</source>; <year>2015</year>.</mixed-citation>
</ref>
<ref id="pone.0181790.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Loughin</surname> <given-names>TM</given-names></name>. <article-title>A residual bootstrap for regression parameters in proportional hazards models</article-title>. <source>Journal of Statistical Computation and Simulation</source>. <year>1995</year>;<volume>52</volume>(<issue>4</issue>):<fpage>367</fpage>–<lpage>384</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/00949659508811686" xlink:type="simple">10.1080/00949659508811686</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0181790.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rüschendorf</surname> <given-names>L</given-names></name>. <article-title>On the distributional transform, Sklar’s theorem, and the empirical copula process</article-title>. <source>Journal of Statistical Planning and Inference</source>. <year>2009</year>;<volume>139</volume>:<fpage>3921</fpage>–<lpage>3927</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.jspi.2009.05.030" xlink:type="simple">10.1016/j.jspi.2009.05.030</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0181790.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Loughin</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Koehler</surname> <given-names>K</given-names></name>. <article-title>Bootstrapping Regression Parameters in Multivariate Survival Analysis</article-title>. <source>Lifetime Data Analysis</source>. <year>1997</year>;<volume>3</volume>:<fpage>157</fpage>–<lpage>177</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/A:1009609218622" xlink:type="simple">10.1023/A:1009609218622</ext-link></comment> <object-id pub-id-type="pmid">9384620</object-id></mixed-citation>
</ref>
<ref id="pone.0181790.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Politis</surname> <given-names>DN</given-names></name>. <article-title>Model-free model-fitting and predictive distributions</article-title>. <source>TEST</source>. <year>2013</year>;<volume>22</volume>(<issue>2</issue>):<fpage>183</fpage>–<lpage>221</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s11749-013-0323-9" xlink:type="simple">10.1007/s11749-013-0323-9</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0181790.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Field</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Welsh</surname> <given-names>AH</given-names></name>. <article-title>Bootstrapping clustered data</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>. <year>2007</year>;<volume>69</volume>(<issue>3</issue>):<fpage>369</fpage>–<lpage>390</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1467-9868.2007.00593.x" xlink:type="simple">10.1111/j.1467-9868.2007.00593.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0181790.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chambers</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Chandra</surname> <given-names>H</given-names></name>. <article-title>A Random Effect Block Bootstrap for Clustered Data</article-title>. <source>Journal of Computational and Graphical Statistics</source>. <year>2013</year>;<volume>22</volume>(<issue>2</issue>):<fpage>452</fpage>–<lpage>470</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/10618600.2012.681216" xlink:type="simple">10.1080/10618600.2012.681216</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0181790.ref021">
<label>21</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Hall</surname> <given-names>P</given-names></name>. <source>The bootstrap and Edgeworth expansion</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>; <year>1992</year>.</mixed-citation>
</ref>
<ref id="pone.0181790.ref022">
<label>22</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Cherubini</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Luciano</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Vecchiato</surname> <given-names>W</given-names></name>. <source>Copula methods in finance</source>. <publisher-loc>Chichester, UK</publisher-loc>: <publisher-name>John Wiley &amp; Sons</publisher-name>; <year>2004</year>.</mixed-citation>
</ref>
<ref id="pone.0181790.ref023">
<label>23</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>McCullagh</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Nelder</surname> <given-names>JA</given-names></name>. <source>Generalized linear models</source>. <edition>2nd ed</edition>. <publisher-loc>London</publisher-loc>: <publisher-name>Chapman &amp; Hall</publisher-name>; <year>1989</year>.</mixed-citation>
</ref>
<ref id="pone.0181790.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lawless</surname> <given-names>JF</given-names></name>. <article-title>Negative binomial and mixed Poisson regression</article-title>. <source>Canadian Journal of Statistics</source>. <year>1987</year>;<volume>15</volume>:<fpage>209</fpage>–<lpage>225</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2307/3314912" xlink:type="simple">10.2307/3314912</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0181790.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Naumann</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Wright</surname> <given-names>ST</given-names></name>, <name name-style="western"><surname>Warton</surname> <given-names>DI</given-names></name>. <article-title><monospace>mvabund</monospace>—an <monospace>R</monospace> package for model-based analysis of multivariate abundance data</article-title>. <source>Methods in Ecology and Evolution</source>. <year>2012</year>;<volume>3</volume>:<fpage>471</fpage>–<lpage>474</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.2041-210X.2012.00190.x" xlink:type="simple">10.1111/j.2041-210X.2012.00190.x</ext-link></comment></mixed-citation>
</ref>
</ref-list>
</back>
</article>