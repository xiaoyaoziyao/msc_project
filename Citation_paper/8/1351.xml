<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">plos</journal-id>
      <journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
      <journal-id journal-id-type="pmc">ploscomp</journal-id>
      <journal-title-group>
        <journal-title>PLoS Computational Biology</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1553-734X</issn>
      <issn pub-type="epub">1553-7358</issn>
      <publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">PCOMPBIOL-D-12-00520</article-id>
      <article-id pub-id-type="doi">10.1371/journal.pcbi.1002836</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Computational biology</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Neurophysiology</subject>
              <subj-group>
                <subject>Synapses</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subject>Learning and memory</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Computational Biology</subject>
          <subject>Neuroscience</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Soft-bound Synaptic Plasticity Increases Storage Capacity</article-title>
        <alt-title alt-title-type="running-head">Soft-bound Plasticity Increases Storage Capacity</alt-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>van Rossum</surname>
            <given-names>Mark C. W.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Shippi</surname>
            <given-names>Maria</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Barrett</surname>
            <given-names>Adam B.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1"/>
          <xref ref-type="fn" rid="fn1">
            <sup>¤</sup>
          </xref>
        </contrib>
      </contrib-group>
      <aff id="aff1">
        <addr-line>Institute for Adaptive and Neural Computation, School of Informatics, University of Edinburgh, Edinburgh, United Kingdom</addr-line>
      </aff>
      <contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Latham</surname>
            <given-names>Peter E.</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group>
      <aff id="edit1">
        <addr-line>Gatsby Computational Neuroscience Unit, University College London, United Kingdom</addr-line>
      </aff>
      <author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">mvanross@inf.ed.ac.uk</email></corresp>
        <fn fn-type="conflict">
          <p>The authors have declared that no competing interests exist.</p>
        </fn>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: MCWvR ABB. Performed the experiments: MS MCWvR. Wrote the paper: ABB MCWvR MS.</p>
        </fn>
        <fn id="fn1" fn-type="current-aff">
          <label>¤</label>
          <p>Current address: School of Informatics, University of Sussex, Brighton, United Kingdom.</p>
        </fn>
      </author-notes>
      <pub-date pub-type="collection">
        <month>12</month>
        <year>2012</year>
      </pub-date>
      <pub-date pub-type="epub">
        <day>20</day>
        <month>12</month>
        <year>2012</year>
      </pub-date>
      <volume>8</volume>
      <issue>12</issue>
      <elocation-id>e1002836</elocation-id>
      <history>
        <date date-type="received">
          <day>30</day>
          <month>3</month>
          <year>2012</year>
        </date>
        <date date-type="accepted">
          <day>24</day>
          <month>10</month>
          <year>2012</year>
        </date>
      </history>
      <permissions>
        <copyright-year>2012</copyright-year>
        <copyright-holder>van Rossum et al</copyright-holder>
        <license xlink:type="simple">
          <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
        </license>
      </permissions>
      <abstract>
        <p>Accurate models of synaptic plasticity are essential to understand the adaptive properties of the nervous system and for realistic models of learning and memory. Experiments have shown that synaptic plasticity depends not only on pre- and post-synaptic activity patterns, but also on the strength of the connection itself. Namely, weaker synapses are more easily strengthened than already strong ones. This so called soft-bound plasticity automatically constrains the synaptic strengths. It is known that this has important consequences for the dynamics of plasticity and the synaptic weight distribution, but its impact on information storage is unknown. In this modeling study we introduce an information theoretic framework to analyse memory storage in an online learning setting. We show that soft-bound plasticity increases a variety of performance criteria by about 18% over hard-bound plasticity, and likely maximizes the storage capacity of synapses.</p>
      </abstract>
      <abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>It is generally believed that our memories are stored in the synaptic connections between neurons. Numerous experimental studies have therefore examined when and how the synaptic connections change. In parallel, many computational studies have examined the properties of memory and synaptic plasticity, aiming to better understand human memory and allow for neural network models of the brain. However, the plasticity rules used in most studies are highly simplified and do not take into account the rich behaviour found in experiments. For instance, it has been observed in experiments that it is hard to make strong synapses even stronger. Here we show that this saturation of plasticity enhances the number of memories that can be stored and introduce a general framework to calculate information storage in online learning paradigms.</p>
      </abstract>
      <funding-group>
        <funding-statement>ABB was supported by the EPSRC through grant EP/G007543/1 to Anil Seth. MS was supported the Eurospin Erasmus Mundus Doctoral programme. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
      </funding-group>
      <counts>
        <page-count count="11"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Long term synaptic plasticity has been established as one of the most important components for learning and memory. In parallel with experimental investigations, numerous computational models of synaptic plasticity have been developed to simulate network function and to establish the fundamental characteristics and limitations of plasticity. Despite the complexity of the underlying neurobiology, theoretical studies have in the interest of tractability mostly focused on highly simplified plasticity rules <xref ref-type="bibr" rid="pcbi.1002836-Hertz1">[1]</xref>. However, more realistic models are now becoming possible in the light of more detailed experimental characterization of synaptic plasticity <xref ref-type="bibr" rid="pcbi.1002836-Clopath1">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1002836-Kotaleski1">[3]</xref>.</p>
      <p>One such experimental finding is that strong synapses are harder to potentiate than weak ones, that is, the percentage increase in strength is significantly smaller for strong synapses than for weak synapses <xref ref-type="bibr" rid="pcbi.1002836-Debanne1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1002836-Montgomery1">[5]</xref>. Meanwhile, synaptic depression protocols lead to a percentage decrease in strength independent of strength itself <xref ref-type="bibr" rid="pcbi.1002836-Debanne2">[6]</xref>. This phenomenon has been observed under both classical and spike timing dependent plasticity protocols <xref ref-type="bibr" rid="pcbi.1002836-Bi1">[7]</xref>, and is known as soft-bound or weight-dependent plasticity (see Discussion for possible biophysical correlates). Soft-bound plasticity contributes to saturation of LTP when one tries to induce it repeatedly. Observation of LTP saturation has been used as evidence that synaptic plasticity did occur during some earlier learning protocol <xref ref-type="bibr" rid="pcbi.1002836-Whitlock1">[8]</xref>, <xref ref-type="bibr" rid="pcbi.1002836-RioultPedotti1">[9]</xref>.</p>
      <p>Soft-bound plasticity automatically constrains the synaptic weights, and thereby resolves simply, but effectively, the danger of unconstrained plasticity, namely that on repeated activation, synaptic strength would grow indefinitely. In many modeling studies weight dependence is ignored, instead hard-bounds are typically introduced that cap the minimal and maximal synaptic weights (also known as weight clipping), which are often supplemented with constraints on the total weight <xref ref-type="bibr" rid="pcbi.1002836-Parisi1">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1002836-Miller1">[11]</xref>. In other plasticity rules, such as Oja's rule <xref ref-type="bibr" rid="pcbi.1002836-Oja1">[12]</xref>, weight dependence might be present but it is not biologically motivated. However, including weight dependence in plasticity rules is not just a minor fix noticeable only if synapses reach extreme values. It has profound consequences for plasticity and its dynamics: First, it leads to unimodal synaptic weight distributions <xref ref-type="bibr" rid="pcbi.1002836-vanRossum1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1002836-Rubin1">[14]</xref>, consistent with distributions observed both in electro-physiological <xref ref-type="bibr" rid="pcbi.1002836-Song1">[15]</xref> and in spine size data <xref ref-type="bibr" rid="pcbi.1002836-Loewenstein1">[16]</xref>. Second, it weakens competition between synaptic inputs <xref ref-type="bibr" rid="pcbi.1002836-Gtig1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002836-Meffin1">[18]</xref> and instead causes the synaptic weight to depend smoothly on the correlation between inputs <xref ref-type="bibr" rid="pcbi.1002836-vanRossum2">[19]</xref>, consistent with recent data <xref ref-type="bibr" rid="pcbi.1002836-Perin1">[20]</xref>. Finally, as a result of the weaker competition, for identically sized synaptic updates soft-bound plasticity is less stable compared to hard-bound plasticity <xref ref-type="bibr" rid="pcbi.1002836-Billings1">[21]</xref>.</p>
      <p>Despite the experimental evidence for soft-bound plasticity rules, the effect of weight dependence on information storage is not well understood <xref ref-type="bibr" rid="pcbi.1002836-Loewenstein2">[22]</xref>. A priori it is not clear whether soft-bound plasticity is better or worse for information storage compared to hard-bound plasticity. In the case of <italic>discrete</italic> synapses it has been suggested that soft-bounds fundamentally limit memory lifetime <xref ref-type="bibr" rid="pcbi.1002836-Fusi1">[23]</xref>. Analysis of soft-bound plasticity is complicated by the fact that when plasticity depends on the synaptic weight, it will depend on the history of the synapse. Here we study a plasticity process that is continually on-going and which has started a long time ago, so that the distribution of synaptic weights has reached an equilibrium. We introduce an information measure for such on-line learning schemes. We show that soft-bound plasticity leads to a 18% higher information capacity and find strong evidence that soft-bound plasticity optimizes storage capacity. Moreover, the memory lifetime of soft-bound plasticity is longer than for hard-bound plasticity. Thus, soft-bound plasticity not only helps to constrain plasticity, but it also increases capacity.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <p>To understand how different plasticity rules determine information capacity, we consider a simple, single neuron learning paradigm. The setup is shown in <xref ref-type="fig" rid="pcbi-1002836-g001">Fig. 1A</xref>. The neuron receives <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e001" xlink:type="simple"/></inline-formula> synaptic inputs. Each input has a plastic synaptic weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e002" xlink:type="simple"/></inline-formula>. Every time-step we present a different synaptic input pattern. The pattern's elements <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e003" xlink:type="simple"/></inline-formula>, are uncorrelated binary variables, with +1 and −1 occurring with 50% probability (see below for variations). The neuron's output equals the weighted sum of the inputs, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e004" xlink:type="simple"/></inline-formula>. The plasticity rule depresses synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e005" xlink:type="simple"/></inline-formula> with an amount <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e006" xlink:type="simple"/></inline-formula> (with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e007" xlink:type="simple"/></inline-formula>) when its input is low, so that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e008" xlink:type="simple"/></inline-formula>,, and a high input potentiates the synapse, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e009" xlink:type="simple"/></inline-formula>. These updates are independent of post-synaptic activity (but see below). As a result the next time the same pattern is encountered the response of the neuron will be higher.</p>
      <fig id="pcbi-1002836-g001" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1002836.g001</object-id>
        <label>Figure 1</label>
        <caption>
          <title>Diagram of the single neuron recognition task.</title>
          <p>A) A neuron receives binary pattern inputs. At each time-step a new pattern is presented and the weights are updated according to the input value. The neuron's output equals the weighted sum of the inputs. B) The neuron has to remember the presented patterns. When tested, learned patterns lead to a larger output (solid curve) than lures (dashed curve). As the memory of the pattern ages and is overwritten by new patterns, the output of the neuron in response to the pattern becomes less distinct and the signal-to-noise ratio decays. The performance is measured by the signal-to-noise ratio, a measure of the distance between the two output distributions. C) The decay of the signal-to-noise ratio for soft-bound and hard-bound plasticity rules as a function of the age of the pattern. The synaptic updates were set so that both rules led to an initial SNR of 100 right after the pattern was presented (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e010" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e011" xlink:type="simple"/></inline-formula>). For both plasticity rules the SNR decays, but it decays slower for soft-bound plasticity.</p>
        </caption>
        <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002836.g001" position="float" xlink:type="simple"/>
      </fig>
      <p>The task of the neuron is to recognize the patterns that it has encountered previously. To measure the performance we periodically interrupt the learning and test the neuron with both previously presented patterns (labeled <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e012" xlink:type="simple"/></inline-formula>) and lures (labeled <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e013" xlink:type="simple"/></inline-formula>) that were not presented before. Based on the output it has to be decided whether the pattern has been seen before or not. This very simplest of tasks can straightforwardly be extended to a supervised associative learning scheme in which some patterns are associated to a high output and others to a low output. Hereto patterns that should give a high output follow the above scheme, while patterns that should give a low output, potentiate synapses with low inputs and depress synapses with high inputs <xref ref-type="bibr" rid="pcbi.1002836-Fusi2">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1002836-Barrett1">[25]</xref>.</p>
      <p>The model is agnostic about the precise timescale and brain area involved - cortex and hippocampus come to mind. It is also possible to adopt a variant in which only a fraction of all presented patterns is learned. For instance, the synaptic plasticity might only occur with a certain probability, or plasticity might occur only if some additional signal (for instance signalling reward or relevance), lifts the postsynaptic activity above a certain plasticity threshold. Either mechanism would slow down the learning and forgetting equally, but would not otherwise change our analysis <xref ref-type="bibr" rid="pcbi.1002836-Amit1">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1002836-Brunel1">[27]</xref>.</p>
      <p>Because the neuron sums many inputs, the output distribution is well approximated with a Gaussian distribution. This holds independently of the <italic>weight</italic> distribution (by the law of large numbers, even with a uniform weight distribution the output distribution will still tend to a Gaussian). Indeed, simulations with and without this Gaussian assumption gave virtually identical results. Using the Gaussian approximation, a signal to noise ratio (SNR) can be used to characterize the difference in the response between patterns and lures. With <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e014" xlink:type="simple"/></inline-formula> we denote the SNR of a pattern presented <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e015" xlink:type="simple"/></inline-formula> time-steps ago, <xref ref-type="fig" rid="pcbi-1002836-g001">Fig. 1B</xref>,<disp-formula id="pcbi.1002836.e016"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e016" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e017" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e018" xlink:type="simple"/></inline-formula> denote the mean and the variance of the output in response to a pattern learned <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e019" xlink:type="simple"/></inline-formula> time-steps ago; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e020" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e021" xlink:type="simple"/></inline-formula> denote the mean and variance of the output in response to a lure.</p>
      <p>In many storage capacity studies weights are initialized to zero, the number of items to be learned is fixed and learning stops after all the items have been presented, or once the task as been learned <xref ref-type="bibr" rid="pcbi.1002836-Hertz1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1002836-Hopfield1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1002836-Willshaw1">[29]</xref>. In such schemes the memory for each item is typically equally strong. Thus a single number characterizes the performance, and plasticity rules can be designed to optimize it <xref ref-type="bibr" rid="pcbi.1002836-Tsodyks1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1002836-Dayan1">[31]</xref>. In contrast, we consider an on-line learning scheme. In on-line learning the plasticity never stops, which is arguably more relevant biologically. Old patterns are continuously forgotten as new ones are learned, known as the ‘palimpsest’ property <xref ref-type="bibr" rid="pcbi.1002836-Nadal1">[32]</xref>. The definition of memory capacity requires more care for on-line learning. The quality of a memory, expressed by the signal-to-noise ratio of the neuron's output, decays with age of the pattern. <xref ref-type="fig" rid="pcbi-1002836-g001">Fig. 1B</xref> shows the probability distribution of the output of the neuron. As the memory of the pattern ages and gets overwritten, the output of the neuron becomes indistinguishable from the response to a lure, <xref ref-type="fig" rid="pcbi-1002836-g001">Fig. 1C</xref>.</p>
      <sec id="s2a">
        <title>Soft-bound versus hard-bound plasticity</title>
        <p>The central question we address is which plasticity rules lead to the best performance. We focus on the effect of the weight dependence and restrict ourselves to plasticity rules that are local (only dependent on the pre- and post-synaptic activity at that synapse) and incremental (have no access to the patterns presented earlier). We implemented first two plasticity rules, a hard-bound and a soft-bound one.</p>
        <p>For the hard-bound plasticity rule, potentiation occurs when the input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e022" xlink:type="simple"/></inline-formula> is high and when the input is low, the synapse depresses: For the hard-bound rule we use<disp-formula id="pcbi.1002836.e023"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e023" xlink:type="simple"/><label>(2)</label></disp-formula>For these plasticity rules hard-bounds on the synaptic weight need to be imposed to prevent unlimited growth; these were set at 0 and +1. The results do not depend on the choice of boundaries, as long as there is feedforward inhibition tuned to the mean weight (see below). The magnitude of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e024" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e025" xlink:type="simple"/></inline-formula> determines how much the weight is updated per potentiation or depression event. We balance potentiation and depression, i.e. we set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e026" xlink:type="simple"/></inline-formula>, which is optimal in our scenario (see below for unbalanced parameters). In this case, the weight distribution is uniform. We can include dependence of the plasticity on the level of post-synaptic activity, which, dependent on parameters, can lead to a bi-modal weight distribution, as in STDP <xref ref-type="bibr" rid="pcbi.1002836-vanRossum1">[13]</xref>. This bi-modality is weak as the inputs are uncorrelated in our setup. Performance is decreased when such dependence on post-synaptic activity is included (not shown).</p>
        <p>Secondly, we implement a soft-bound plasticity rule. Here the absolute amount of potentiation is weight independent, while the depression is proportional to the weight,<disp-formula id="pcbi.1002836.e027"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e027" xlink:type="simple"/><label>(3)</label></disp-formula>This mimics experimental data <xref ref-type="bibr" rid="pcbi.1002836-Debanne1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1002836-Debanne2">[6]</xref>, <xref ref-type="bibr" rid="pcbi.1002836-Bi1">[7]</xref>. Note that in experimental studies the <italic>relative</italic> amount of plasticity is typically reported. It was found that relative amount of depression is approximately constant (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e028" xlink:type="simple"/></inline-formula>) and potentiation is inversely proportional to weight (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e029" xlink:type="simple"/></inline-formula>). This leads to the above plasticity rule for the absolute amounts. No bounds need to be imposed with the soft-bound rule, the plasticity is intrinsically bounded. For small updates (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e030" xlink:type="simple"/></inline-formula>) the soft-bound plasticity yields a Gaussian weight distribution, centered around a mean weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e031" xlink:type="simple"/></inline-formula>, and a variance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e032" xlink:type="simple"/></inline-formula>.</p>
        <p>The decay of the SNR for both hard- and soft-bound plasticity rules is shown in <xref ref-type="fig" rid="pcbi-1002836-g001">Fig. 1C</xref>. Here the plasticity parameters were set such that the signal-to-noise at time 0, i.e. the initial strength of the memory tested immediately after presentation, was 100 in both cases. For soft-bound plasticity the decay is exactly exponential, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e033" xlink:type="simple"/></inline-formula>, where the time-constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e034" xlink:type="simple"/></inline-formula> is the memory's decay time, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e035" xlink:type="simple"/></inline-formula> is the initial memory strength per synapse and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e036" xlink:type="simple"/></inline-formula> is the number of synapses. For hard-bound plasticity rules, the decay is not exactly exponential (see Models for the exact expression), although an exponential fit can still be used to characterize performance. Importantly, the soft-bound plasticity decays more slowly and thus retains the memory longer.</p>
        <p>Ideally one has a slow forgetting and a strong initial memory, however this is impossible to achieve. High plasticity rates (large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e037" xlink:type="simple"/></inline-formula>) lead to a strong memory of recent patterns (large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e038" xlink:type="simple"/></inline-formula>) but also rapid forgetting (short <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e039" xlink:type="simple"/></inline-formula>), as old memories are overwritten by new ones. On the contrary, small plasticity rates will extend the memory time but will also lead to a reduced strength of the memory. Thus, in online learning, these two competing quantities characterize the memory performance and there is a trade-off between them. Here we will use two different approaches to solve this trade-off and express memory capacity as a single number, thus enabling quantitative comparison between hard-bound and soft-bound plasticity. First, we use information theory to calculate the information per synapse and, second, we use a more traditional signal-to-noise argument to calculate the memory lifetime.</p>
      </sec>
      <sec id="s2b">
        <title>Information theory</title>
        <p>The first way to resolve the trade-off uses mutual information. The mutual information expresses how many bits of information are gained about the novelty of a pattern by inspecting the output of the neuron when it is tested by lures and learned patterns. We pass the output of the neuron through a threshold with value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e040" xlink:type="simple"/></inline-formula>. This thresholded response, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e041" xlink:type="simple"/></inline-formula>, equals zero when the summed input is less than the threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e042" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e043" xlink:type="simple"/></inline-formula> when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e044" xlink:type="simple"/></inline-formula>. The mutual information between the response and the pattern is given by<disp-formula id="pcbi.1002836.e045"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e045" xlink:type="simple"/><label>(4)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e046" xlink:type="simple"/></inline-formula> is the probability for a pattern of either class: previously presented (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e047" xlink:type="simple"/></inline-formula>) or lure (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e048" xlink:type="simple"/></inline-formula>). <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e049" xlink:type="simple"/></inline-formula> is the probability for a certain response, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e050" xlink:type="simple"/></inline-formula> is the conditional probability for a given response on a given pattern class. Concretely, the expression contains the probabilities that a learned pattern is correctly recognized <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e051" xlink:type="simple"/></inline-formula>, that a lure pattern is correctly identified <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e052" xlink:type="simple"/></inline-formula> and the two error probabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e053" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e054" xlink:type="simple"/></inline-formula>. To obtain the total information stored per synapse, we sum the information over all presented patterns and normalize it by the number of synapses. We call this the information per synapse, termed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e055" xlink:type="simple"/></inline-formula>. Thus, when a neuron with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e056" xlink:type="simple"/></inline-formula> synapses would be able to perfectly recognize <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e057" xlink:type="simple"/></inline-formula> patterns, the information per synapse would be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e058" xlink:type="simple"/></inline-formula>.</p>
        <p>Using the Gaussian distribution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e059" xlink:type="simple"/></inline-formula> (denoted <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e060" xlink:type="simple"/></inline-formula>) and the fact that the variances for lures and patterns become identical for small updates, the probabilities are calculated as a function of the Signal-to-Noise ratio. For instance, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e061" xlink:type="simple"/></inline-formula>. For the optimal threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e062" xlink:type="simple"/></inline-formula>, halfway between the Gaussians, one has for the probability of either mis-classification <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e063" xlink:type="simple"/></inline-formula>, where the error rate equals <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e064" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e065" xlink:type="simple"/></inline-formula> the signal-to-noise ratio, <xref ref-type="disp-formula" rid="pcbi.1002836.e016">Eq.(1)</xref>. The probability for correct responses is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e066" xlink:type="simple"/></inline-formula>. This yields the information as function of the SNR as<disp-formula id="pcbi.1002836.e067"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e067" xlink:type="simple"/><label>(5)</label></disp-formula>This relation is plotted in <xref ref-type="fig" rid="pcbi-1002836-g002">Fig. 2</xref>(middle). If the output distributions totally overlap, the SNR is zero and the information is zero as well. For small SNR (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e068" xlink:type="simple"/></inline-formula>), the information is linear in the SNR. Taylor expansion of <xref ref-type="disp-formula" rid="pcbi.1002836.e067">Eq.(5)</xref> yields <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e069" xlink:type="simple"/></inline-formula>. Importantly, the information is a saturating function of the SNR. For very high SNR, the two output distributions are almost perfectly separated, but because a pattern is either a learned pattern or a lure, there is maximally one bit of information per pattern. For example, doubling an already high SNR, only brings slightly more information.</p>
        <fig id="pcbi-1002836-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002836.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Relation between the information and the SNR.</title>
            <p>Top: The SNR decay curves versus pattern age for soft-bound plasticity with a large synaptic update (thin curve), and soft-bound plasticity with a small update (thick curve). Although the rules trade off between slow decay and the high initial SNR differently, the area under the curve is identical. Middle: The relation between SNR and Information, <xref ref-type="disp-formula" rid="pcbi.1002836.e067">Eq.(5)</xref>. Bottom: The Information versus pattern age calculated from the top and middle graph. The total information stored, equal to the area under the curve, is clearly larger when using small updates (thick curve) than when using large updates.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002836.g002" position="float" xlink:type="simple"/>
        </fig>
        <p>The saturation has an important consequence when one wants to maximize information, illustrated in <xref ref-type="fig" rid="pcbi-1002836-g002">Fig. 2</xref>. The SNR decay is shown for two soft-bound learning settings, one with large updates, one with small updates. The total information is the sum of the information about all patterns; each pattern has a different age and hence SNR and information associated to it. Although the integral under the SNR curves is identical, the integral under the information curves, corresponding to the total information stored, is clearly smaller for the large updates. In other words, a high SNR wastes synaptic information capacity that otherwise could have been used to store more patterns. As an example, an initial SNR of 10 will achieve only 78% of maximal capacity (assuming exponential decay). The information capacity is maximized when this saturation is avoided and many patterns are stored with a low SNR, that is, when the synaptic updates are small.</p>
        <p>This setup in principle requires a threshold precisely between the average output to pattern and lure, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e070" xlink:type="simple"/></inline-formula>. Hence the threshold should depend on the age of test pattern, but it would be difficult to imagine how this could be implemented. In the limit of small SNR, however, the information becomes independent of the precise threshold setting, and instead the threshold can be fixed, at say, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e071" xlink:type="simple"/></inline-formula>.</p>
      </sec>
      <sec id="s2c">
        <title>Soft and hard bound information capacity</title>
        <p>Both soft-bound information capacity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e072" xlink:type="simple"/></inline-formula> and hard-bound information capacity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e073" xlink:type="simple"/></inline-formula> are calculated exactly in the limit of small updates in the Models section. We find<disp-formula id="pcbi.1002836.e074"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e074" xlink:type="simple"/></disp-formula>Thus the soft-bound plasticity can store more information. The improvement in performance is moderate though, some 18%. <xref ref-type="fig" rid="pcbi-1002836-g003">Fig. 3A</xref> shows the outcome of simulations that confirm these theoretical results, the soft-bound rule outperforms the hard-bound rule. These results raise the question whether other plasticity rules could increase capacity even further. That does not appear to be the case as we argue next.</p>
        <fig id="pcbi-1002836-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002836.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Comparison of hard-bound to soft-bound plasticity.</title>
            <p>A) The information capacity per synapse in the recognition task for a variety of plasticity rules. Up to numerical error, the soft-bound, log-normal and Gutig rule perform identically. B) Simulation of the lifetime of a memory for various plasticity rules. The lifetime was defined as the number of memories stored with a SNR above 30. Again soft-bound plasticity outperforms hard-bounds. C) The trade-off between memory decay time and initial memory strength for soft- and hard-bound plasticity. The amount of synaptic update was varied and the resulting fitted decay time-constant and the initial SNR was plotted. Ideally initial strength is high and memory decay time is long (top-right corner), but increasing one decreases the other. Soft-bound plasticity always leads to a superior trade-off.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002836.g003" position="float" xlink:type="simple"/>
        </fig>
      </sec>
      <sec id="s2d">
        <title>Alternative soft-bound plasticity rules</title>
        <p>We simulated two additional soft-bound plasticity rules. The first comes from the empirical observation that the synaptic weight distribution can be fitted to a log-normal distribution <xref ref-type="bibr" rid="pcbi.1002836-Song1">[15]</xref>. Also the distribution of spine volumes, which correlates strongly with the synaptic strength, follows a log-normal distribution <xref ref-type="bibr" rid="pcbi.1002836-Loewenstein1">[16]</xref>. One way to obtain a log-normal distribution as a stationary weight distribution is to use an exponentiated Ornstein-Uhlenbeck process, where a decay term continuously pulls the weights back to the mean value <xref ref-type="bibr" rid="pcbi.1002836-Loewenstein1">[16]</xref>. Such a mechanism is difficult to reconcile with our setup. Instead we use that for small synaptic updates the soft-bound rule above yields a normal distribution. Exponentiation of the soft-bound plasticity rules yields<disp-formula id="pcbi.1002836.e075"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e075" xlink:type="simple"/><label>(6)</label></disp-formula>For small updates this yields a log-normally distributed weight, with a mean equal to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e076" xlink:type="simple"/></inline-formula>.</p>
        <p>The second rule is a polynomial plasticity rule <xref ref-type="bibr" rid="pcbi.1002836-Gtig1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002836-Morrison1">[33]</xref>, which can be viewed as an interpolation between hard-and soft-bound.<disp-formula id="pcbi.1002836.e077"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e077" xlink:type="simple"/><label>(7)</label></disp-formula>If the exponent <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e078" xlink:type="simple"/></inline-formula> equals 1, one retrieves a soft-bound rule very similar to the soft-bound rule above (although not identical). The case <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e079" xlink:type="simple"/></inline-formula> leads to <xref ref-type="disp-formula" rid="pcbi.1002836.e023">Eq.(2)</xref> if hard bounds are imposed at 0 and 1. The performance of this rule improves gradually from hard-bound and soft-bound as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e080" xlink:type="simple"/></inline-formula> increases from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e081" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e082" xlink:type="simple"/></inline-formula>, interpolating from the hard- to soft-bound case. To examine if this rule can outperform the earlier soft-bound rule, we choose a value outside this range, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e083" xlink:type="simple"/></inline-formula>.</p>
        <p>Both the log-normal and polynomial rule perform as well as the original soft-bound rule, but neither did better, <xref ref-type="fig" rid="pcbi-1002836-g003">Fig. 3A</xref>. In the Model section we show that a large class of soft-bound rules indeed have the same capacity and that this does not depend on the precise parameter values, as long as the updates are small. To further corroborate the optimality of soft-bound plasticity, we numerically optimized plasticity rules for which both potentiation and depression are general second order polynomials in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e084" xlink:type="simple"/></inline-formula><disp-formula id="pcbi.1002836.e085"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e085" xlink:type="simple"/><label>(8)</label></disp-formula>To examine if the soft-bound plasticity could be outperformed, the coefficients <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e086" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e087" xlink:type="simple"/></inline-formula> were varied to find numerically their best values. For numerical performance, the weight wa discretized in 200 bins and the number of synapses was limited to 100 so that there was no saturation from discretization <xref ref-type="bibr" rid="pcbi.1002836-Barrett1">[25]</xref>. The information capacity was identical to the soft-bound rules, but no improvement could be achieved by allowing these polynomial update rules. Finally, the result matches the maximum information capacity of discrete synapses for which a more exhaustive optimization is possible <xref ref-type="bibr" rid="pcbi.1002836-Barrett1">[25]</xref>. Together these results suggest that for this class of learning rules, soft-bound plasticity performs optimally.</p>
      </sec>
      <sec id="s2e">
        <title>Memory lifetime</title>
        <p>Although the use of small synaptic updates maximizes information capacity, in practice there are issues with using small updates. First, the low SNR of each memory renders the pattern/lure discrimination sensitive to noise, such as noise from synaptic variability or other inputs to the neuron. Secondly, in recurrent networks, such as the Hopfield network, errors made by a single neuron can be amplified by the network. Moreover, experimental evidence suggests that synaptic plasticity protocols can induce substantial changes in a synapse. Finally, soft-bound plasticity with small updates leads to narrow weight distributions (i.e. with a small variance), while the observed synaptic weight distributions are relatively broad, consistent with larger updates.</p>
        <p>We therefore define a second storage measure to compare hard and soft-bounds. We use the same single neuron paradigm used above but define the memory lifetime as the number of recent patterns that the neuron stores with SNR above a predefined threshold <xref ref-type="bibr" rid="pcbi.1002836-Sterratt1">[34]</xref>. Similar measures have been defined for networks <xref ref-type="bibr" rid="pcbi.1002836-Leibold1">[35]</xref>. For instance, in the Hopfield network, the single neuron error rate should stay below <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e088" xlink:type="simple"/></inline-formula>0.36% to prevent an avalanche of errors in the recurrent activity <xref ref-type="bibr" rid="pcbi.1002836-Hertz1">[1]</xref>. This error rate corresponds to a SNR of about 30, and corresponds to a regime where the synaptic updates are no longer small. A full analysis of capacity of recurrent networks is rather more involved <xref ref-type="bibr" rid="pcbi.1002836-Mzard1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1002836-Huang1">[37]</xref>, but the current approach is sufficient for our purposes.</p>
        <p>Because the SNR decays exponentially with soft-bound plasticity (Models), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e089" xlink:type="simple"/></inline-formula>, the number of memories stored above threshold is easily calculated. To find the maximum lifetime the plasticity parameters have to be optimized, as with too small updates the SNR might never become high enough, while too large updates would lead to rapid over-writing. One finds the optimal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e090" xlink:type="simple"/></inline-formula> to be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e091" xlink:type="simple"/></inline-formula>, leading to a life-time<disp-formula id="pcbi.1002836.e092"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e092" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e093" xlink:type="simple"/></inline-formula> is the imposed threshold and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e094" xlink:type="simple"/></inline-formula> is Euler's number. The memory lifetime increases linearly with the number of synapses and decreases with SNR threshold.</p>
        <p>The lifetime in the hard-bound case can be approximated by taking only the lowest order term in the expression for the SNR decay (see Models), yielding<disp-formula id="pcbi.1002836.e095"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e095" xlink:type="simple"/></disp-formula>Thus again soft-bound plasticity is superior to hard-bound plasticity, on this measure by some 20%.</p>
        <p>The theory is confirmed by the simulations in which we numerically maximized the lifetime by changing the synaptic update. Too small updates would lead to none or few patterns above the threshold, while too large updates speed up the decay. The lifetime is plotted normalized by the number of synapses. The soft-bound plasticity, as well as the other soft-bound variants, outperform hard-bound, <xref ref-type="fig" rid="pcbi-1002836-g003">Fig. 3B</xref>.</p>
        <p>As indicated, the SNR decay leads to a trade-off between initial SNR and memory decay time. The current performance measure sets a somewhat arbitrary threshold to resolve this. However, the result is independent of the precise threshold setting. <xref ref-type="fig" rid="pcbi-1002836-g003">Fig. 3C</xref> shows the initial SNR versus the forgetting time, as we parametrically vary the size of the synaptic update. For both hard and soft-bound plasticity, SNR and decay time are inversely proportional over a large range. The soft-bound plasticity is superior, as it always gives a longer lifetime for the same initial SNR, no matter what the preferred trade-off between memory strength and retention time. Note that the information capacity analyzed in the previous section is reflected in <xref ref-type="fig" rid="pcbi-1002836-g003">Fig. 3C</xref> as well. Apart from a constant, the information capacity approximately equals the product of decay-time and initial SNR in the limit of small SNR, i.e. in the right region of the graph.</p>
      </sec>
      <sec id="s2f">
        <title>Feed-forward inhibition and sparse codes</title>
        <p>In the simulations and the analysis we made two assumptions. First, the input patterns take positive and negative values, but in biological systems it would seem more natural to assume that the inputs are spikes, that is, 1s and 0s. Secondly, we tacitly used fixed feedforward inhibition to balance the excitatory inputs and ensure that the effective synaptic weight has zero mean. Without these assumptions the SNR, and hence the information capacity, for both hard- and soft-bound plasticity decreases significantly.</p>
        <p>The reason for the decreased SNR is easiest seen by analyzing the variance in the output in response to a lure pattern. Because a lure pattern will be completely uncorrelated to the weights, the variance can be written as<disp-formula id="pcbi.1002836.e096"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e096" xlink:type="simple"/><label>(9)</label></disp-formula>The higher this quantity, the smaller the SNR, the worse the performance of both the information measure and the memory lifetime. We now examine the second and third term in detail.</p>
        <p>In the above results the second term in <xref ref-type="disp-formula" rid="pcbi.1002836.e096">Eq.(9)</xref> was zero, because we used <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e097" xlink:type="simple"/></inline-formula> inputs with a coding density of 1/2 (equal probability of high and low input) so that mean input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e098" xlink:type="simple"/></inline-formula> was zero. When using 0/1 inputs this term is no longer zero. This strongly reduces the capacity at high coding density, <xref ref-type="fig" rid="pcbi-1002836-g004">Fig. 4A</xref>. However, when coding is sparse so that most inputs are zero and only a small fraction are one, the mean input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e099" xlink:type="simple"/></inline-formula> is again close to zero and the information capacity approaches the theoretical maximum, <xref ref-type="fig" rid="pcbi-1002836-g004">Fig. 4A</xref>. This is also the case for the memory lifetime, <xref ref-type="fig" rid="pcbi-1002836-g004">Fig. 4D</xref>. Note that in simulations with sparse codes, we scale the plasticity such that the balance between depression and potentiation is maintained. For instance, for the hard-bound plasticity, we use <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e100" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e101" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e102" xlink:type="simple"/></inline-formula> denotes the probability for a high input. Not doing so, would decrease capacity.</p>
        <fig id="pcbi-1002836-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002836.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Effect of coding density and inhibition on performance.</title>
            <p>A. The synaptic information capacity versus the coding density for soft-bound (solid line) and hard-bound (dashed line) plasticity, when taking 0s and 1s as inputs. When coding density is 1/2, the capacity is approximately half of what it is when using <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e103" xlink:type="simple"/></inline-formula> as inputs. However, low coding density improves the synaptic information capacity and in the limit of very sparse codes (utmost left in the graph) the capacity reaches that of <xref ref-type="fig" rid="pcbi-1002836-g003">Figure 3A</xref>. B. Effect of feed-forward inhibition on capacity under soft-bound plasticity. Using excitatory synapses (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e104" xlink:type="simple"/></inline-formula>) without inhibition, capacity is strongly reduced (‘No inhibition’). Adding feed-forward inhibition maximizes information capacity (‘Feedforward inhibition’). Equivalently, high capacity is achieved when the plasticity rules are defined such as to allow for negative weights (‘Unrestricted’). C. Possible circuit to implement feed-forward inhibition. D+E. Effects of coding density and inhibition on the information (panel A+B) on the information are mirrored by the effects on the memory lifetime.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002836.g004" position="float" xlink:type="simple"/>
        </fig>
        <p>The third term in <xref ref-type="disp-formula" rid="pcbi.1002836.e096">Eq. (9)</xref> is proportional to the mean weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e105" xlink:type="simple"/></inline-formula> and reflects changes in the output due to changes in the number of active inputs. When only excitatory inputs are used and hence the mean weight is non-zero, the capacity is reduced, <xref ref-type="fig" rid="pcbi-1002836-g004">Fig. 4B</xref>. In the simulations zero mean weight was effectively achieved by implementing feed-forward inhibition. Hereto we introduced an inhibitory partner neuron that receives the same inputs as the original neuron and that calculates the un-weighted sum <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e106" xlink:type="simple"/></inline-formula>. This neuron then inhibits the output neuron with an inhibitory weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e107" xlink:type="simple"/></inline-formula>, <xref ref-type="fig" rid="pcbi-1002836-g004">Fig. 4C</xref>. The total output of the neuron is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e108" xlink:type="simple"/></inline-formula>. If the inhibitory weight is adjusted to balance the mean excitatory weight (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e109" xlink:type="simple"/></inline-formula>), optimal capacity is obtained, <xref ref-type="fig" rid="pcbi-1002836-g004">Fig. 4B</xref>. Similar arguments have been made in a variety of memory models <xref ref-type="bibr" rid="pcbi.1002836-Horner1">[38]</xref>–<xref ref-type="bibr" rid="pcbi.1002836-Leibold2">[40]</xref>.</p>
        <p>In this idealized setup, feed-forward inhibition is mathematically equivalent to making the mean weight zero by allowing negative weights and adjusting the plasticity rules. For instance, for the hard-bound plasticity, this is achieved by just moving the lower weights bound to −1; for soft-bound plasticity, the depression rule is redefined to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e110" xlink:type="simple"/></inline-formula>. Indeed, the information per synapse is identical, <xref ref-type="fig" rid="pcbi-1002836-g004">Fig. 4B</xref>. This same argument applies again to the memory lifetime and thus it behaves in parallel, <xref ref-type="fig" rid="pcbi-1002836-g004">Fig. 4E</xref>.</p>
      </sec>
      <sec id="s2g">
        <title>Imbalanced potentiation and depression</title>
        <p>A further potential problem with hard-bound plasticity is that highest capacity is only attained when potentiation and depression are exactly balanced <xref ref-type="bibr" rid="pcbi.1002836-Fusi1">[23]</xref>. In contrast imbalance in soft-bound plasticity shift the mean weight but do not affect the capacity or the lifetime. Our theory allows for an exact analysis of imbalance (see Models). We set the potentiation strength to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e111" xlink:type="simple"/></inline-formula> and the depression to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e112" xlink:type="simple"/></inline-formula>, so that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e113" xlink:type="simple"/></inline-formula> recovers the balanced case. Capacity is indeed diminished if the amount of the potentiation event does not exactly balance the depression event. Simulation and theory are shown in <xref ref-type="fig" rid="pcbi-1002836-g005">Fig. 5A</xref> and reveal an interesting stupa-like shape: The information is maximal in the balanced case (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e114" xlink:type="simple"/></inline-formula>). For small imbalances favouring either LTD or LTP, the information decreases rapidly, following the theory (black curve, see Models). However, for larger imbalance the decrease in information is moderate. In this region, the weight distribution approaches a narrow exponential. The fast decay of the signal caused by the imbalance is counter-acted by the reduction of the variance of the weight distribution as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e115" xlink:type="simple"/></inline-formula> increases. Due to this effect, the initial SNR increases but the information saturates unavoidably, even for small updates. This is reflected in the fact that the simulations deviate from the theory and show progressively less information per synapse as the total number of synapses of the neuron increases.</p>
        <fig id="pcbi-1002836-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002836.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>The effect of imbalance between potentiation and depression on the capacity measures for hard-bound plasticity.</title>
            <p>A. The Information capacity showing the theory (black) as well as simulations for 10, 100, 1000, and 10000 synapses for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e116" xlink:type="simple"/></inline-formula> (blue, violet and magenta). In contrast to the balanced case (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e117" xlink:type="simple"/></inline-formula>) the capacity depends on the number of synapses. A larger synaptic update always decreases capacity in the balanced case, but can improve capacity in the imbalanced case (dashed curve, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e118" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e119" xlink:type="simple"/></inline-formula>). B. Memory lifetime decreases when potentiation and depression are imbalanced. The memory lifetime was optimized w.r.t <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e120" xlink:type="simple"/></inline-formula> for every setting of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e121" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e122" xlink:type="simple"/></inline-formula>.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002836.g005" position="float" xlink:type="simple"/>
        </fig>
        <p>The width of the peak around <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e123" xlink:type="simple"/></inline-formula> is given by the condition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e124" xlink:type="simple"/></inline-formula>. Thus the peak can be widened by increasing the event size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e125" xlink:type="simple"/></inline-formula> (an example is shown with the dashed line). However, a larger event size also increases saturation when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e126" xlink:type="simple"/></inline-formula>. Thus unlike the balanced case, where a small update always maximized mutual information, the optimal synaptic update in the imbalanced case is dependent on the imbalance parameter and the number of synapses. In parallel, the memory lifetime decreases when potentiation and depression are imbalanced, <xref ref-type="fig" rid="pcbi-1002836-g005">Fig. 5B</xref>.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>We have studied plasticity rules that include the experimental observation that plasticity depends on the synaptic weight itself. Namely, the relative amount of potentiation decreases as the synapse gets stronger, while the relative amount of depression shows no such dependence. This means that the synaptic weight is automatically bounded. Using an information theoretic framework we have compared these plasticity rules to hard-bound plasticity in an online learning setting. We found that the information storage is higher for soft-bound plasticity rules. Contrasting the prototypical soft-bound and hard-bound plasticity rules, the improvement can be calculated analytically. In addition, a wide class of soft-bound plasticity rules lead to the same increased capacity and we suggest that soft-bound rules in fact maximize the synaptic capacity. Furthermore, we examined an alternative capacity measure that determines how many patterns can be stored above a certain threshold. This memory lifetime measure also improved for soft-bound plasticity.</p>
      <p>The improvement in performance is moderate for both capacity measures, some 18%. However, it should be stressed that <italic>a priori</italic> there was no reason to assume that weight dependence would actually improve capacity, it could have led to a lower capacity. Moreover, naively one might have guessed, errorneously, that the uniform weight distribution associated with hard-bound plasticity would be optimal. The difference in capacity means that for hard-bound plasticity to perform as well as the soft-bound one, 18% more synapses would be required. This is a significant space and metabolic cost for the nervous system as synapses consume about 60% of the brain's energy <xref ref-type="bibr" rid="pcbi.1002836-Sengupta1">[41]</xref>.</p>
      <p>Given that the difference between hard and soft-bound is only quantitative, it is difficult to point to the cause for the difference, besides our mathematical derivation. Our intuition is that the hard-bounds lead to a deformation of the weight distribution. As a result, the decay back to equilibrium is a sum of exponentials. This means that the decay is always somewhat faster than for the soft-bound case for identical initial SNR, <xref ref-type="fig" rid="pcbi-1002836-g001">Fig. 1c</xref>.</p>
      <p>Our study reveals the following criteria to optimize synaptic storage capacity: 1) use soft-bound rules, 2) ensure that the mean input is close to zero, for instance by using sparse codes, 3) ensure that the mean weight is close to zero, for instance by implementing balanced feed-forward inhibition, and finally, if one wants to maximize the mutual information, 4) update the synapse in small amounts.</p>
      <p>There have recently been a number of studies on information storage and forgetting in synapses with a discrete number of states. The continuous synapses studied here have an equal or superior capacity, as they can always effectively act as discrete ones. An earlier study of discrete synapses argued that balanced hard-bound rules are superior to soft-bound rules when long memory lifetime is required <xref ref-type="bibr" rid="pcbi.1002836-Fusi1">[23]</xref>. These concerns don't apply to continuous synapses, and we believe that our performance measure based on Shannon information is a more fundamental one. That said, the precise biological objectives and constraints for synaptic plasticity are unknown. Furthermore it should be noted that, as for any optimization argument, another criterion would likely yield a different optimal rule. For instance, one-shot learning would require large updates and thus yields low information, while sensitivity to input correlations has been shown to require a rule intrapolating between hard and soft-bound <xref ref-type="bibr" rid="pcbi.1002836-Gtig1">[17]</xref>.</p>
      <p>The results do not depend on the precise form of the soft-bound plasticity rules. Biophysically, numerous mechanisms with any kind of saturation, could lead to soft-bound plasticity. At the level of a single synapse one could even argue that weight-<italic>independent</italic> plasticity would be difficult to achieve bio-physically, as many biophysical signals such as Ca influx and AMPA insertion are likely affected by the synaptic weight itself. Interestingly, the spine volume grows substantially as the synapse undergoes potentiation <xref ref-type="bibr" rid="pcbi.1002836-Matsuzaki1">[42]</xref>. At first glance this suggests that the spine readies itself for potential further strengthening, but it has been suggested that actually the increase in spine volume reduces the calcium transients, limiting further potentiation, and therefore giving rise to soft-bound plasticity <xref ref-type="bibr" rid="pcbi.1002836-Kalantzis1">[43]</xref>, <xref ref-type="bibr" rid="pcbi.1002836-ODonnell1">[44]</xref>.</p>
      <p>Apart from the increased capacity, weight dependence has other important consequences for plasticity. First, the weight dependence leads to central weight distributions, consistent with data, both measured electro-physiologically and microscopically. Second, competition between synapses is weaker for soft-bound rules because depressed synapses never completely disappear. Finally, in soft-bound plasticity the mean weight remains sensitive to correlations <xref ref-type="bibr" rid="pcbi.1002836-vanRossum2">[19]</xref>, in line with recent evidence in <xref ref-type="bibr" rid="pcbi.1002836-Perin1">[20]</xref>.</p>
      <p>In an earlier study we reported that soft-bound STDP plasticity leads to shorter retention times than hard-bound plasticity in both single neurons and networks <xref ref-type="bibr" rid="pcbi.1002836-Billings1">[21]</xref>. In that study the update size was not optimized as done here, but instead the average synaptic update per pre-post spike pairing was set the same for hard-bound and soft-bound plasticity. Fully consistent with the results in the Models section, for the same update soft-bound plasticity leads to quicker decay (proportional to the update size) than hard-bound (proportional to the update size squared) <xref ref-type="bibr" rid="pcbi.1002836-Fusi1">[23]</xref>. Because of the difference in the setup, the SNR measure used there can not directly be compared with the one derived here. Characterization of the information storage was not carried out in that setting, but we see no reason why the current results would not hold for STDP learning.</p>
      <p>Soft-bound plasticity is certainly not the only way to prevent run-away plasticity. Apart from hard-bounds, BCM theory <xref ref-type="bibr" rid="pcbi.1002836-Bienenstock1">[45]</xref>, and normalization models <xref ref-type="bibr" rid="pcbi.1002836-Miller1">[11]</xref> are some of the best known alternatives. Soft-bound plasticity however provides one of the easiest solutions to run away plasticity, as it does not require a running average of activity (needed for BCM) or knowledge of the other synapses onto the neuron (as needed in normalization models). Yet, soft-bound plasticity can co-exist with those mechanisms, as well as with homeostatic processes, and thus can be part of a larger set of mechanisms to keep neural activity and plasticity in check. This study suggests that this can be done without losing any storage capacity, but instead gaining some.</p>
    </sec>
    <sec id="s4">
      <title>Models</title>
      <sec id="s4a">
        <title>Calculation of information capacity</title>
        <p>In this section we calculate the capacity of soft and hard-bound plasticity analytically. To calculate the capacity we concentrate on one single synapse, as the Signal-to-Noise scales linearly with the number of synapses. We artificially distinguish the pattern that is to be learned from all the other patterns that are presented subsequently and erase the memory of this pattern, although of course, no pattern stands above the others; the same plasticity rules underlie both the storage and the forgetting processes. Throughout we assume that the synaptic updates are small. This prevents saturation in the relation between SNR and Information, <xref ref-type="fig" rid="pcbi-1002836-g002">Fig. 2B</xref>, ensuring maximal information.</p>
        <p>To calculate the information storage we need to study how the synaptic weight decays after learning. As the weight updates are small, a Fokker-Planck equation for the weight distribution describes the decay of the synapse as it is subject to the learning of other patterns. The synaptic weight distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e127" xlink:type="simple"/></inline-formula> evolves as <xref ref-type="bibr" rid="pcbi.1002836-vanRossum1">[13]</xref><disp-formula id="pcbi.1002836.e128"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e128" xlink:type="simple"/></disp-formula>where the drift is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e129" xlink:type="simple"/></inline-formula>, the diffusion term is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e130" xlink:type="simple"/></inline-formula>, and where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e131" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e132" xlink:type="simple"/></inline-formula> denote the weight change associated with potentiation and depression. We denote an average over many trials with angular brackets, the variance is denoted <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e133" xlink:type="simple"/></inline-formula>, and the equilibrium mean weight is denoted <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e134" xlink:type="simple"/></inline-formula>.</p>
      </sec>
      <sec id="s4b">
        <title>Soft-bound plasticity rule</title>
        <p>We first calculate the information capacity for the soft-bound rule. The solution Fokker-Planck is complicated, but when the jumps are much smaller than the standard deviation of the distribution, that is, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e135" xlink:type="simple"/></inline-formula>, the diffusion term is approximately constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e136" xlink:type="simple"/></inline-formula>. In this limit, the equilibrium weight distribution, defined by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e137" xlink:type="simple"/></inline-formula>, becomes a narrow Gaussian distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e138" xlink:type="simple"/></inline-formula>, with mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e139" xlink:type="simple"/></inline-formula> and variance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e140" xlink:type="simple"/></inline-formula>. Note, that validity of the Fokker-Planck approximation itself only requires the weaker condition that the updates themselves are small, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e141" xlink:type="simple"/></inline-formula>.</p>
        <p>We consider what happens to a given synapse when a pattern is learned. Suppose that at time zero, the synapse gets a high input and is thus potentiated (the calculation in the case of depression is analogous). Right after the potentiation event, the weight distribution is displaced with an amount <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e142" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e143" xlink:type="simple"/></inline-formula> denotes the displacement of the weight. For small updates one has <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e144" xlink:type="simple"/></inline-formula>.</p>
        <p>After this event, the synaptic weight is subject to random potentiation and depression events caused by the learning of patterns presented subsequently. During this the perturbation of the synaptic weight distribution will decay, until finally the distribution equals the equilibrium again and the potentiation event will have lost its trace. The perturbed distribution can be plugged into the Fokker-Planck equation to study its decay back to equilibrium. Because <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e145" xlink:type="simple"/></inline-formula> is linear in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e146" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e147" xlink:type="simple"/></inline-formula> is approximately constant, it can be shown that the probability obeys<disp-formula id="pcbi.1002836.e148"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e148" xlink:type="simple"/></disp-formula>This is a transport equation, which means that during the overwriting of the synapse by the subsequent patterns the distribution shifts back to its equilibrium value, but maintains its shape, <xref ref-type="fig" rid="pcbi-1002836-g006">Fig. 6A</xref>. From this equation it follows that the mean weight obeys <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e149" xlink:type="simple"/></inline-formula>. The mean weight decays back exponentially with a time-constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e150" xlink:type="simple"/></inline-formula> as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e151" xlink:type="simple"/></inline-formula>.</p>
        <fig id="pcbi-1002836-g006" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002836.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>Illustration of decay of the weight distributions after a potentiation event.</title>
            <p>The distribution right after the potentiation is shown by the magenta curve; as time progresses (indicated by the arrow) it decays back to the equilibrium distribution (thick black curve). A) With soft-bound plasticity the distribution is displaced but maintains its shape. During the overwriting it shifts back to the equilibrium distribution. B) With hard-bound plasticity, the distribution distorts after the potentiation due to the presence of the bounds. As it decays back to the equilibrium this distortion flattens out.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002836.g006" position="float" xlink:type="simple"/>
        </fig>
        <p>The output signal is found by probing the synapse with a ‘1’ (high input). Assuming perfectly tuned feed-forward inhibition, the mean output signal is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e152" xlink:type="simple"/></inline-formula>. As the variance of the weight distribution and hence the variance of the output is to first approximation not affected by the plasticity, the signal-to-noise is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e153" xlink:type="simple"/></inline-formula>. The information follows as<disp-formula id="pcbi.1002836.e154"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e154" xlink:type="simple"/></disp-formula>which matches simulations.</p>
        <p>In the simulations we found the same information for other soft-bound plasticity rules. For small updates, a Taylor expansion of the drift <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e155" xlink:type="simple"/></inline-formula> can always be made, yielding the linear term in the drift, furthermore the diffusion term becomes independent of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e156" xlink:type="simple"/></inline-formula> close enough to the center of the distribution. These approximations become perfect in the limit of small update updates. Therefore most soft-bound rules can be mapped to the above one, yield a narrow Gaussian as equilibrium distribution and have the same capacity. Finally, one can construct soft-bound rules in which the linear term in the drift is absent <xref ref-type="bibr" rid="pcbi.1002836-Fusi1">[23]</xref>. However, also for those cases we numerically found the same capacity.</p>
      </sec>
      <sec id="s4c">
        <title>Calculation of information capacity hard-bound rule</title>
        <p>We repeat the calculation for hard-bound plasticity. We impose hard-bounds at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e157" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e158" xlink:type="simple"/></inline-formula>, that prevent the weights from crossing minimal and maximal values. This corresponds to imposing the boundary conditions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e159" xlink:type="simple"/></inline-formula> to the Fokker-Planck equation, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e160" xlink:type="simple"/></inline-formula> is the probability flux. As shown below, the capacity is optimal when potentiation and depression are matched. In this case, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e161" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e162" xlink:type="simple"/></inline-formula>. The equilibrium distribution is the uniform distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e163" xlink:type="simple"/></inline-formula>, with mean weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e164" xlink:type="simple"/></inline-formula>. The boundary conditions simplify to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e165" xlink:type="simple"/></inline-formula>.</p>
        <p>At time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e166" xlink:type="simple"/></inline-formula> the synapse is potentiated with a small amount <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e167" xlink:type="simple"/></inline-formula>. The average weight increases by an amount <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e168" xlink:type="simple"/></inline-formula>. The weight distribution, which expresses the probability to find the weight of a certain value, is displaced, creating a bump at the upper boundary and a dip at the lower boundary, <xref ref-type="fig" rid="pcbi-1002836-g006">Fig. 6B</xref>. The distribution becomes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e169" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e170" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e171" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e172" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e173" xlink:type="simple"/></inline-formula> otherwise. We approximate this perturbation as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e174" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e175" xlink:type="simple"/></inline-formula> is the Dirac-delta distribution.</p>
        <p>As above, the synaptic weight is subject to random potentiation and depression events caused by the learning of patterns presented subsequently. Solving the Fokker-Planck equation gives that the perturbation of the synaptic weight distribution decays as<disp-formula id="pcbi.1002836.e176"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e176" xlink:type="simple"/></disp-formula>where the Green's function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e177" xlink:type="simple"/></inline-formula> is the solution to the diffusion equation in infinite space when all weights are initially concentrated at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e178" xlink:type="simple"/></inline-formula>. The sum over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e179" xlink:type="simple"/></inline-formula> represents the ‘mirror charges’, needed to satisfy the boundary conditions, preventing the synaptic weight escaping from the interval between 0 and 1; similar equations arise in neural cable theory, e.g. <xref ref-type="bibr" rid="pcbi.1002836-Gerstner1">[46]</xref>.</p>
        <p>The derivative of the mean weight follows from the Fokker-Planck equation after integration by parts as<disp-formula id="pcbi.1002836.e180"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e180" xlink:type="simple"/></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e181" xlink:type="simple"/></inline-formula>. Integration gives that the weight decays as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e182" xlink:type="simple"/></inline-formula>. Note that the inverse of the slowest time-constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e183" xlink:type="simple"/></inline-formula> is proportional to the update squared, in contrast to the soft-bound case were it is linearly proportional to the synaptic update <xref ref-type="bibr" rid="pcbi.1002836-Fusi1">[23]</xref>.</p>
        <p>The variance in the output equals that of the uniform distribution and is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e184" xlink:type="simple"/></inline-formula>. Thus, assuming perfect feed-forward inhibition, the signal-to-noise ratio is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e185" xlink:type="simple"/></inline-formula>. Hence for small plasticity events the information is<disp-formula id="pcbi.1002836.e186"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e186" xlink:type="simple"/></disp-formula>where the sum was calculated numerically. For practical purposes the sums over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e187" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e188" xlink:type="simple"/></inline-formula> can be truncated above <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e189" xlink:type="simple"/></inline-formula> as the contributions to the sum rapidly diminish.</p>
      </sec>
      <sec id="s4d">
        <title>Imbalanced plasticity</title>
        <p>For the soft-bound plasticity, the ratio between potentiation and depression determines the mean synaptic weight, but the capacity does not depend on it. For hard-bound plasticity, however, the situation is more complicated. Imbalance between potentiation and depression leads to a deformation of the steady state distribution and speeds up the decay after perturbations, changing the capacity.</p>
        <p>We assume that the size of the potentiation event is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e190" xlink:type="simple"/></inline-formula>, and depression <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e191" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e192" xlink:type="simple"/></inline-formula> parameterizes the imbalance. To calculate the capacity one again needs to calculate the reaction to a perturbation (potentiation or depression) away from the equilibrium. But as now <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e193" xlink:type="simple"/></inline-formula> is no longer zero, the mirror charges trick does not work. Instead using standard Sturm-Liouville theory, the solution to the Fokker-Planck with boundary conditions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e194" xlink:type="simple"/></inline-formula> can be written as a series expansion<disp-formula id="pcbi.1002836.e195"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e195" xlink:type="simple"/><label>(10)</label></disp-formula>where the steady state distribution is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e196" xlink:type="simple"/></inline-formula> and where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e197" xlink:type="simple"/></inline-formula> is the re-scaled drift. The eigenfunctions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e198" xlink:type="simple"/></inline-formula> are<disp-formula id="pcbi.1002836.e199"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e199" xlink:type="simple"/></disp-formula>with eigenvalues <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e200" xlink:type="simple"/></inline-formula>. The imbalance, expressed by the factor <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e201" xlink:type="simple"/></inline-formula>, speeds up the decay to equilibrium. The coefficients <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e202" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1002836.e195">Eq.10</xref> follow from the initial condition. As above we first consider a single potentiation event of size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e203" xlink:type="simple"/></inline-formula>. This will shift the weight distribution as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e204" xlink:type="simple"/></inline-formula>; the coefficients <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e205" xlink:type="simple"/></inline-formula> (normalized to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e206" xlink:type="simple"/></inline-formula>) follow as<disp-formula id="pcbi.1002836.e207"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e207" xlink:type="simple"/></disp-formula>From this we calculate the evolution of the mean weight <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e208" xlink:type="simple"/></inline-formula> using <xref ref-type="disp-formula" rid="pcbi.1002836.e195">Eq.10</xref>. Each eigenfunction contributes an amount <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e209" xlink:type="simple"/></inline-formula> to the perturbation of the mean weight. Averaging potentiation and depression events, the Information becomes<disp-formula id="pcbi.1002836.e210"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002836.e210" xlink:type="simple"/></disp-formula>This expression is plotted in <xref ref-type="fig" rid="pcbi-1002836-g005">Fig. 5A</xref> (black line) as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e211" xlink:type="simple"/></inline-formula>.</p>
      </sec>
      <sec id="s4e">
        <title>Computer simulations</title>
        <p>The computer code is made available on the first author's website. To examine the various plasticity rules, we presented typically one million patterns to the neuron, one pattern at every time-step. Every pattern led to an update of the synaptic weights according to the plasticity rules. After each time-step, the output of the neuron in response to this pattern, to its predecessors, as well as to lures was measured. The mean and the variance of the neuron's output were collected online as a function of the pattern's age, that is how many time-steps ago the pattern was presented. From this the SNR and information was calculated at the end of the simulation using <xref ref-type="disp-formula" rid="pcbi.1002836.e067">Eq.(5)</xref> and <xref ref-type="disp-formula" rid="pcbi.1002836.e016">(1)</xref>.</p>
        <p>Importantly, unlike the analysis above, the simulations are not restricted to small synaptic updates. Furthermore, the Gaussian assumption can be dropped in the simulations. In that case all the neuron's responses were stored, and the information was calculated at the end of the simulation using the full response distributions to patterns and lures. We found results were the virtually identical. It required, however, much more computer memory and time than the first method.</p>
        <p>Unless indicated otherwise, for the figures we used <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e212" xlink:type="simple"/></inline-formula> synapses for the lifetime calculations. For efficiency reasons we used <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002836.e213" xlink:type="simple"/></inline-formula> to calculate the information measure as the scaling with the number of synapses is trivial.</p>
      </sec>
    </sec>
  </body>
  <back>
    <ack>
      <p>The authors would like to thank Matthias Hennig and Cian O'Donnell for discussions.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002836-Hertz1">
        <label>1</label>
        <mixed-citation publication-type="other" xlink:type="simple">Hertz J, Krogh A, Palmer RG (1991) Introduction to the theory of neural computation. Reading, MA: Perseus.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Clopath1">
        <label>2</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clopath</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Büsing</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Vasilaki</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2010</year>) <article-title>Connectivity reflects coding: a model of voltage-based stdp with homeostasis</article-title>. <source>Nat Neurosci</source> <volume>13</volume>: <fpage>344</fpage>–<lpage>352</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Kotaleski1">
        <label>3</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kotaleski</surname><given-names>JH</given-names></name>, <name name-style="western"><surname>Blackwell</surname><given-names>KT</given-names></name> (<year>2010</year>) <article-title>Modelling the molecular mechanisms of synaptic plasticity using systems biology approaches</article-title>. <source>Nat Rev Neurosci</source> <volume>11</volume>: <fpage>239</fpage>–<lpage>251</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Debanne1">
        <label>4</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Debanne</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Gähwiler</surname><given-names>BH</given-names></name>, <name name-style="western"><surname>Thompson</surname><given-names>SM</given-names></name> (<year>1999</year>) <article-title>Heterogeneity of synaptic plasticity at unitary CA1-CA3 and CA3-CA3 connections in rat hippocampal slice cultures</article-title>. <source>J Neurosci</source> <volume>19</volume>: <fpage>10664</fpage>–<lpage>10671</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Montgomery1">
        <label>5</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Montgomery</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Pavlidis</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Madison</surname><given-names>DV</given-names></name> (<year>2001</year>) <article-title>Pair recordings reveal all-silent synaptic connections and the postsynaptic expression of long-term potentiation</article-title>. <source>Neuron</source> <volume>29</volume>: <fpage>691</fpage>–<lpage>701</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Debanne2">
        <label>6</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Debanne</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Gähwiler</surname><given-names>BH</given-names></name>, <name name-style="western"><surname>Thompson</surname><given-names>SM</given-names></name> (<year>1996</year>) <article-title>Cooperative interactions in the induction of long-term potentiation and depression of synaptic excitation between hippocampal CA3-CA1 cell pairs in vitro</article-title>. <source>Proc Natl Acad Sci</source> <volume>93</volume>: <fpage>11225</fpage>–<lpage>11230</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Bi1">
        <label>7</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bi</surname><given-names>Gq</given-names></name>, <name name-style="western"><surname>Poo</surname><given-names>Mm</given-names></name> (<year>1998</year>) <article-title>Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type</article-title>. <source>J Neurosci</source> <volume>18</volume>: <fpage>10464</fpage>–<lpage>10472</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Whitlock1">
        <label>8</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Whitlock</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Heynen</surname><given-names>AJ</given-names></name>, <name name-style="western"><surname>Shuler</surname><given-names>MG</given-names></name>, <name name-style="western"><surname>Bear</surname><given-names>MF</given-names></name> (<year>2006</year>) <article-title>Learning induces long-term potentiation in the hippocampus</article-title>. <source>Science</source> <volume>313</volume>: <fpage>1093</fpage>–<lpage>1097</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-RioultPedotti1">
        <label>9</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rioult-Pedotti</surname><given-names>MS</given-names></name>, <name name-style="western"><surname>Donoghue</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Dunaevsky</surname><given-names>A</given-names></name> (<year>2007</year>) <article-title>Plasticity of the synaptic modification range</article-title>. <source>J Neurophysiol</source> <volume>98</volume>: <fpage>3688</fpage>–<lpage>3695</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Parisi1">
        <label>10</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parisi</surname><given-names>G</given-names></name> (<year>1986</year>) <article-title>A memory which forgets</article-title>. <source>J Phys A: Math Gen</source> <volume>19</volume>: <fpage>L617</fpage>–<lpage>M20</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Miller1">
        <label>11</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miller</surname><given-names>KD</given-names></name>, <name name-style="western"><surname>MacKay</surname><given-names>DJC</given-names></name> (<year>1994</year>) <article-title>The role of constraints in Hebbian learning</article-title>. <source>Neural Comp</source> <volume>6</volume>: <fpage>100</fpage>–<lpage>126</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Oja1">
        <label>12</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oja</surname><given-names>E</given-names></name> (<year>1982</year>) <article-title>A simplified neuron model as a principal component analyzer</article-title>. <source>J Math Biol</source> <volume>15</volume>: <fpage>267</fpage>–<lpage>273</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-vanRossum1">
        <label>13</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Rossum</surname><given-names>MCW</given-names></name>, <name name-style="western"><surname>Bi</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name> (<year>2000</year>) <article-title>Stable Hebbian learning from spike timing dependent plasticity</article-title>. <source>J Neurosci</source> <volume>20</volume>: <fpage>8812</fpage>–<lpage>8821</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Rubin1">
        <label>14</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rubin</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>DD</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>2001</year>) <article-title>Equilibrium properties of temporally asymmetric Hebbian plasticity</article-title>. <source>Phys Rev Lett</source> <volume>86</volume>: <fpage>364</fpage>–<lpage>367</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Song1">
        <label>15</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Song</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Sjostrom</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Reigl</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Nelson</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Chklovskii</surname><given-names>DB</given-names></name> (<year>2005</year>) <article-title>Highly nonrandom features of synaptic connectivity in local cortical circuits</article-title>. <source>PLoS Biol</source> <volume>3</volume>: <fpage>e68</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Loewenstein1">
        <label>16</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Loewenstein</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Kuras</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Rumpel</surname><given-names>S</given-names></name> (<year>2011</year>) <article-title>Multiplicative dynamics underlie the emergence of the log-normal distribution of spine sizes in the neocortex in vivo</article-title>. <source>J Neurosci</source> <volume>31</volume>: <fpage>9481</fpage>–<lpage>9488</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Gtig1">
        <label>17</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gütig</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Aharonov</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Rotter</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>2003</year>) <article-title>Learning input correlations through nonlinear temporally asymmetric Hebbian plasticity</article-title>. <source>J Neurosci</source> <volume>23</volume>: <fpage>3697</fpage>–<lpage>3714</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Meffin1">
        <label>18</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meffin</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Besson</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Burkitt</surname><given-names>AN</given-names></name>, <name name-style="western"><surname>Grayden</surname><given-names>DB</given-names></name> (<year>2006</year>) <article-title>Learning the structure of correlated synaptic subgroups using stable and competitive spike-timing-dependent plasticity</article-title>. <source>Phys Rev E Stat Nonlin Soft Matter Phys</source> <volume>73</volume>: <fpage>041911</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-vanRossum2">
        <label>19</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Rossum</surname><given-names>MCW</given-names></name>, <name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name> (<year>2001</year>) <article-title>Correlation based learning from spike timing dependent plasticity</article-title>. <source>Neuro Computing</source> <volume>38–40</volume>: <fpage>409</fpage>–<lpage>415</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Perin1">
        <label>20</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perin</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Berger</surname><given-names>TK</given-names></name>, <name name-style="western"><surname>Markram</surname><given-names>H</given-names></name> (<year>2011</year>) <article-title>A synaptic organizing principle for cortical neuronal groups</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>108</volume>: <fpage>5419</fpage>–<lpage>5424</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Billings1">
        <label>21</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Billings</surname><given-names>G</given-names></name>, <name name-style="western"><surname>van Rossum</surname><given-names>MCW</given-names></name> (<year>2009</year>) <article-title>Memory retention and spike-timing-dependent plasticity</article-title>. <source>J Neurophysiol</source> <volume>101</volume>: <fpage>2775</fpage>–<lpage>2788</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Loewenstein2">
        <label>22</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Loewenstein</surname><given-names>Y</given-names></name> (<year>2008</year>) <article-title>Robustness of learning that is based on covariance-driven synaptic plasticity</article-title>. <source>PLoS Comput Biol</source> <volume>4</volume>: <fpage>e1000007</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Fusi1">
        <label>23</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name> (<year>2007</year>) <article-title>Limits on the memory storage capacity of bounded synapses</article-title>. <source>Nat Neurosci</source> <volume>10</volume>: <fpage>485</fpage>–<lpage>493</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Fusi2">
        <label>24</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name> (<year>2002</year>) <article-title>Hebbian spike-driven synaptic plasticity for learning patterns of mean firing rates</article-title>. <source>Biological Cybernetics</source> <volume>87</volume>: <fpage>459</fpage>–<lpage>470</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Barrett1">
        <label>25</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barrett</surname><given-names>AB</given-names></name>, <name name-style="western"><surname>van Rossum</surname><given-names>MCW</given-names></name> (<year>2008</year>) <article-title>Optimal learning rules for discrete synapses</article-title>. <source>PLoS Comput Biol</source> <volume>4</volume>: <fpage>e1000230</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Amit1">
        <label>26</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name> (<year>1994</year>) <article-title>Learning in neural networks with material synapses</article-title>. <source>Neural Computation</source> <volume>6</volume>: <fpage>957</fpage>–<lpage>982</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Brunel1">
        <label>27</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Carusi</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name> (<year>1998</year>) <article-title>Slow stochastic hebbian learning of classes of stimuli in a recurrent neural network</article-title>. <source>Network</source> <volume>9</volume>: <fpage>123</fpage>–<lpage>152</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Hopfield1">
        <label>28</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name> (<year>1982</year>) <article-title>Neural Networks and Physical Systems with Emergent Collective Computational Abilities</article-title>. <source>Proc Natl Acad Sci</source> <volume>79</volume>: <fpage>2554</fpage>–<lpage>2558</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Willshaw1">
        <label>29</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Willshaw</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Buneman</surname><given-names>OP</given-names></name>, <name name-style="western"><surname>Longuet-Higgins</surname><given-names>HC</given-names></name> (<year>1969</year>) <article-title>Non-holographic associative memory</article-title>. <source>Nature</source> <volume>222</volume>: <fpage>960</fpage>–<lpage>993</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Tsodyks1">
        <label>30</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsodyks</surname><given-names>MV</given-names></name>, <name name-style="western"><surname>Feigelman</surname><given-names>MV</given-names></name> (<year>1988</year>) <article-title>The enhanced storage capacity in neural networks with low activity level</article-title>. <source>Europhys Lett</source> <volume>6</volume>: <fpage>101</fpage>–<lpage>105</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Dayan1">
        <label>31</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Willshaw</surname><given-names>DJ</given-names></name> (<year>1991</year>) <article-title>Optimising synaptic learning rules in linear associative memories</article-title>. <source>Biol Cybern</source> <volume>65</volume>: <fpage>253</fpage>–<lpage>265</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Nadal1">
        <label>32</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nadal</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Toulouse</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Changeux</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Dehaene</surname><given-names>S</given-names></name> (<year>1986</year>) <article-title>Networks of Formal Neurons and Memory Palimpsests</article-title>. <source>Europhysics Letters (EPL)</source> <volume>1</volume>: <fpage>535</fpage>–<lpage>542</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Morrison1">
        <label>33</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morrison</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Aertsen</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Diesmann</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>Spike-timing-dependent plasticity in balanced random networks</article-title>. <source>Neural Comput</source> <volume>19</volume>: <fpage>1437</fpage>–<lpage>1467</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Sterratt1">
        <label>34</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sterratt</surname><given-names>DC</given-names></name>, <name name-style="western"><surname>Willshaw</surname><given-names>D</given-names></name> (<year>2008</year>) <article-title>Inhomogeneities in heteroassociative memories with linear learning rules</article-title>. <source>Neural Comput</source> <volume>20</volume>: <fpage>311</fpage>–<lpage>344</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Leibold1">
        <label>35</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leibold</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Kempter</surname><given-names>R</given-names></name> (<year>2008</year>) <article-title>Sparseness constrains the prolongation of memory lifetime via synaptic metaplasticity</article-title>. <source>Cerebral Cortex</source> <volume>18</volume>: <fpage>67</fpage>–<lpage>77</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Mzard1">
        <label>36</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mézard</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Nadal</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Toulouse</surname><given-names>G</given-names></name> (<year>1986</year>) <article-title>Solvable models of working memories</article-title>. <source>J Phys</source> <volume>47</volume>: <fpage>1457</fpage>–<lpage>1462</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Huang1">
        <label>37</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huang</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Amit</surname><given-names>Y</given-names></name> (<year>2011</year>) <article-title>Capacity analysis in multi-state synaptic models: a retrieval probability perspective</article-title>. <source>J Comput Neurosci</source> <volume>30</volume>: <fpage>699</fpage>–<lpage>720</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Horner1">
        <label>38</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Horner</surname><given-names>H</given-names></name> (<year>1989</year>) <article-title>Neural networks with low levels of activity: Ising vs. McCulloch-Pitts neurons</article-title>. <source>Zeitschrift für Physik B Condensed Matter</source> <volume>75</volume>: <fpage>133</fpage>–<lpage>136</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Legenstein1">
        <label>39</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Legenstein</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2008</year>) <article-title>On the classification capability of sign-constrained perceptrons</article-title>. <source>Neural Comput</source> <volume>20</volume>: <fpage>288</fpage>–<lpage>309</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Leibold2">
        <label>40</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leibold</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Bendels</surname><given-names>MHK</given-names></name> (<year>2009</year>) <article-title>Learning to discriminate through long-term changes of dynamical synaptic transmission</article-title>. <source>Neural Comput</source> <volume>21</volume>: <fpage>3408</fpage>–<lpage>3428</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Sengupta1">
        <label>41</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sengupta</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Stemmler</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Laughlin</surname><given-names>SB</given-names></name>, <name name-style="western"><surname>Niven</surname><given-names>JE</given-names></name> (<year>2010</year>) <article-title>Action potential energy efficiency varies among neuron types in vertebrates and invertebrates</article-title>. <source>PLoS Comput Biol</source> <volume>6</volume>: <fpage>e1000840</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Matsuzaki1">
        <label>42</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Matsuzaki</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Honkura</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Ellis-Davies</surname><given-names>GCR</given-names></name>, <name name-style="western"><surname>Kasai</surname><given-names>H</given-names></name> (<year>2004</year>) <article-title>Structural basis of long-term potentiation in single dendritic spines</article-title>. <source>Nature</source> <volume>429</volume>: <fpage>761</fpage>–<lpage>766</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Kalantzis1">
        <label>43</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kalantzis</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Shouval</surname><given-names>HZ</given-names></name> (<year>2009</year>) <article-title>Structural plasticity can produce metaplasticity</article-title>. <source>PLoS One</source> <volume>4</volume>: <fpage>e8062</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-ODonnell1">
        <label>44</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>O'Donnell</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Nolan</surname><given-names>M</given-names></name>, <name name-style="western"><surname>van Rossum</surname><given-names>MCW</given-names></name> (<year>2011</year>) <article-title>Dendritic spine dynamics regulate the long-term stability of synaptic plasticity</article-title>. <source>Journal of Neuroscience</source> <volume>31</volume>: <fpage>16142</fpage>–<lpage>16156</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Bienenstock1">
        <label>45</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bienenstock</surname><given-names>EL</given-names></name>, <name name-style="western"><surname>Cooper</surname><given-names>LN</given-names></name>, <name name-style="western"><surname>Munro</surname><given-names>PW</given-names></name> (<year>1982</year>) <article-title>Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex</article-title>. <source>J Neurosci</source> <volume>2</volume>: <fpage>32</fpage>–<lpage>48</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002836-Gerstner1">
        <label>46</label>
        <mixed-citation publication-type="other" xlink:type="simple">Gerstner W, Kistler W (2002) Spiking Neuron Models: Single Neurons, Populations, Plasticity. Cambridge University Press. 496 pp.</mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>