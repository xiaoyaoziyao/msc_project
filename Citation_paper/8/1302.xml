<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-15-00533</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004439</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A Three-Threshold Learning Rule Approaches the Maximal Capacity of Recurrent Neural Networks</article-title>
<alt-title alt-title-type="running-head">A Learning Rule for Optimal Storage in Recurrent Neural Networks</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Alemi</surname> <given-names>Alireza</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Baldassi</surname> <given-names>Carlo</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Brunel</surname> <given-names>Nicolas</given-names></name>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Zecchina</surname> <given-names>Riccardo</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Human Genetics Foundation (HuGeF), Turin, Italy</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>DISAT, Politecnico di Torino, Turin, Italy</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Departments of Statistics and Neurobiology, University of Chicago, Chicago, Illinois, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Latham</surname> <given-names>Peter E.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University College London, UNITED KINGDOM</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: CB NB RZ. Performed the experiments: AA CB. Analyzed the data: AA CB. Contributed reagents/materials/analysis tools: AA CB. Wrote the paper: AA CB NB.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">alireza.alemi@hugef-torino.it</email>, <email xlink:type="simple">alireza.alemi@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>8</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="epub">
<day>20</day>
<month>8</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>8</issue>
<elocation-id>e1004439</elocation-id>
<history>
<date date-type="received">
<day>30</day>
<month>3</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>19</day>
<month>6</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Alemi et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004439" xlink:type="simple"/>
<abstract>
<p>Understanding the theoretical foundations of how memories are encoded and retrieved in neural populations is a central challenge in neuroscience. A popular theoretical scenario for modeling memory function is the attractor neural network scenario, whose prototype is the Hopfield model. The model simplicity and the locality of the synaptic update rules come at the cost of a poor storage capacity, compared with the capacity achieved with perceptron learning algorithms. Here, by transforming the perceptron learning rule, we present an online learning rule for a recurrent neural network that achieves near-maximal storage capacity without an explicit supervisory error signal, relying only upon locally accessible information. The fully-connected network consists of excitatory binary neurons with plastic recurrent connections and non-plastic inhibitory feedback stabilizing the network dynamics; the memory patterns to be memorized are presented online as strong afferent currents, producing a bimodal distribution for the neuron synaptic inputs. Synapses corresponding to active inputs are modified as a function of the value of the local fields with respect to three thresholds. Above the highest threshold, and below the lowest threshold, no plasticity occurs. In between these two thresholds, potentiation/depression occurs when the local field is above/below an intermediate threshold. We simulated and analyzed a network of binary neurons implementing this rule and measured its storage capacity for different sizes of the basins of attraction. The storage capacity obtained through numerical simulations is shown to be close to the value predicted by analytical calculations. We also measured the dependence of capacity on the strength of external inputs. Finally, we quantified the statistics of the resulting synaptic connectivity matrix, and found that both the fraction of zero weight synapses and the degree of symmetry of the weight matrix increase with the number of stored patterns.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Recurrent neural networks have been shown to be able to store memory patterns as fixed point attractors of the dynamics of the network. The prototypical learning rule for storing memories in attractor neural networks is Hebbian learning, which can store up to 0.138<italic>N</italic> uncorrelated patterns in a recurrent network of <italic>N</italic> neurons. This is very far from the maximal capacity 2<italic>N</italic>, which can be achieved by supervised rules, e.g. by the perceptron learning rule. However, these rules are problematic for neurons in the neocortex or the hippocampus, since they rely on the computation of a supervisory error signal for each neuron of the network. We show here that the total synaptic input received by a neuron during the presentation of a sufficiently strong stimulus contains implicit information about the error, which can be extracted by setting three thresholds on the total input, defining depression and potentiation regions. The resulting learning rule implements basic biological constraints, and our simulations show that a network implementing it gets very close to the maximal capacity, both in the dense and sparse regimes, across all values of storage robustness. The rule predicts that when the total synaptic inputs goes beyond a threshold, no potentiation should occur.</p>
</abstract>
<funding-group>
<funding-statement>AA, CB and RZ acknowledge the European Research Council (<ext-link ext-link-type="uri" xlink:type="simple" xlink:href="http://erc.europa.eu">http://erc.europa.eu</ext-link>) for grant No. 267915. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="10"/>
<table-count count="1"/>
<page-count count="23"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>One of the fundamental challenges in neuroscience is to understand how we store and retrieve memories for a long period of time. Such long-term memory is fundamental for a variety of our cognitive functions. A popular theoretical framework for storing and retrieving memories in recurrent neural networks is the attractor network model framework [<xref ref-type="bibr" rid="pcbi.1004439.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1004439.ref003">3</xref>]. Attractors, i.e. stable states of the dynamics of a recurrent network, are set by modification of synaptic efficacies in a recurrent network. Synaptic plasticity rules specify how the efficacy of a synapse is affected by pre- and post-synaptic neural activity. In particular, Hebbian synaptic plasticity rules lead to long-term potentiation (LTP) for correlated pre- and post-synaptic activities, and long-term depression (LTD) for anticorrelated activities. These learning rules build excitatory feedback loops in the synaptic connectivity, resulting in the emergence of attractors that are correlated with the patterns of activity that were imposed on the network through external inputs. Once a set of patterns become attractors of a network (in other words when the network “learns” the patterns), upon a brief initial activation of a subpopulation of neurons, the network state evolves towards the learned stable state (the network “retrieves” a past stored memory), and remains in that state after removal of the external inputs (and hence maintains the information in short-term memory). The set of initial network states leading to a memorized state is called the <italic>basin of attraction</italic>, whose size determines how robust a memory is. The attractor neural network scenario was originally explored in networks of binary neurons [<xref ref-type="bibr" rid="pcbi.1004439.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004439.ref002">2</xref>], and then extended from the 90s to networks of spiking neurons [<xref ref-type="bibr" rid="pcbi.1004439.ref004">4</xref>–<xref ref-type="bibr" rid="pcbi.1004439.ref007">7</xref>].</p>
<p>Experimental evidence in different areas of the brain, including inferotemporal cortex [<xref ref-type="bibr" rid="pcbi.1004439.ref008">8</xref>–<xref ref-type="bibr" rid="pcbi.1004439.ref011">11</xref>] and prefrontal cortex [<xref ref-type="bibr" rid="pcbi.1004439.ref012">12</xref>–<xref ref-type="bibr" rid="pcbi.1004439.ref014">14</xref>], has provided support for the attractor neural network framework, using electrophysiological recordings in awake monkeys performing delayed response tasks. In such experiments, the monkey has to maintain information in short-term (working) memory in a ‘delay period’ to be able to perform the task. Consistent with the attractor network scenario, some neurons exhibit selective persistent activity during the delay period. This persistent activity of ensembles of cortical neurons has thus been hypothesized to form the basis of the working memory of stimuli shown in these tasks.</p>
<p>One of the most studied properties of attractor neural network as a model of memory is its storage capacity, i.e. how many random patterns can be learned in a recurrent network of <italic>N</italic> neurons in the large <italic>N</italic> limit. Storage capacity depends both on the network architecture and on the synaptic learning rule. In many models, the storage capacity scales with <italic>N</italic>. In particular, the Hopfield network [<xref ref-type="bibr" rid="pcbi.1004439.ref001">1</xref>] that uses a Hebbian learning rule has a storage capacity of 0.138<italic>N</italic> in the limit of <italic>N</italic> → ∞ [<xref ref-type="bibr" rid="pcbi.1004439.ref015">15</xref>]. Later studies showed how the capacity depends on the connection probability in a randomly connected network [<xref ref-type="bibr" rid="pcbi.1004439.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1004439.ref017">17</xref>] and on the coding level (fraction of active neurons in a pattern) [<xref ref-type="bibr" rid="pcbi.1004439.ref018">18</xref>, <xref ref-type="bibr" rid="pcbi.1004439.ref019">19</xref>]. A natural question is, what is the maximal capacity of a given network architecture, over all possible learning rules? This question was answered by Elizabeth Gardner, who showed that the capacity of fully connected networks of binary neurons with dense patterns scales as 2<italic>N</italic> [<xref ref-type="bibr" rid="pcbi.1004439.ref020">20</xref>], a storage capacity which is much larger than the one of the Hopfield model. The next question is what learning rules are able to saturate the Gardner bound? A simple learning rule that is guaranteed to achieve this bound is the perceptron learning rule (PLR) [<xref ref-type="bibr" rid="pcbi.1004439.ref021">21</xref>] applied to each neuron independently. However, unlike the rule used in the Hopfield model, the perceptron learning rule is a supervised rule that needs an explicit “error signal” in order to achieve the Gardner bound. While such an error signal might be available in the cerebellum [<xref ref-type="bibr" rid="pcbi.1004439.ref022">22</xref>–<xref ref-type="bibr" rid="pcbi.1004439.ref024">24</xref>], it is unclear how error signals targeting individual neurons might be implemented in cortical excitatory synapses. Therefore, it remains unclear whether and how networks with realistic learning rules might approach the Gardner bound.</p>
<p>The goal of the present paper is to propose a learning rule whose capacity approaches the maximal capacity of recurrent neural networks by transforming the original perceptron learning rule such that the new rule does not explicitly use an error signal. The perceptron learning rule modifies the synaptic weights by comparing the desired output with the actual output to obtain an error signal, subsequently changing the weights in the opposite direction of the error signal. We argue that the total synaptic inputs (‘local fields’) received by a neuron during the presentation of a stimulus contain some information about the current error (i.e. whether the neuron will end up in the right state after the stimulus is removed). We use this insight to build a field dependent learning rule that contains three thresholds separating no plasticity, LTP and LTD regions. This rule implements basic biological constraints: (a) it uses only information local to the synapse; (b) the new patterns can be learned incrementally, i.e. it is an online rule; (c) it does not need an explicit error signal; (d) synapses obey Dale’s principle, i.e. excitatory synapses are not allowed to have negative weights. We studied the capacity and the size of the basins of attraction for a binary recurrent neural network in which excitatory synapses are endowed with this rule, while a global inhibition term controls the global activity level. We investigated how the strength of external fields and the presence of correlations in the inputs affect the memory capacity. Finally, we investigated the statistical properties of the connectivity matrix (distribution of synaptic weights, degree of symmetry).</p>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec003">
<title>The network</title>
<p>We simulated a network of <italic>N</italic> binary (McCulloch-Pitts) neurons, fully-connected with excitatory synapses (<xref ref-type="fig" rid="pcbi.1004439.g001">Fig 1A</xref>). All the neurons feed a population of inhibitory neurons which is modeled as a single aggregated inhibitory unit. This state-dependent global inhibition projects back onto all the neurons, stabilizing the network and controlling its activity level. At each time step, the activity (or the state) of neuron <italic>i</italic> (<italic>i</italic> = 1…<italic>N</italic>) is described by a binary variable <italic>s</italic><sub><italic>i</italic></sub> ∈ {0,1}. The state is a step function of the <italic>local field</italic> <italic>v</italic><sub><italic>i</italic></sub> of the neuron:
<disp-formula id="pcbi.1004439.e001"><alternatives><graphic id="pcbi.1004439.e001g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e001"/><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>v</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(1)</label></disp-formula>
where Θ is the Heaviside function (Θ(<italic>x</italic>) = 1 if <italic>x</italic> &gt; 0 and 0 otherwise) and <italic>θ</italic> is a neuronal threshold. The local field <italic>v</italic><sub><italic>i</italic></sub> represents the overall input received by the neuron from its excitatory and inhibitory connections (<xref ref-type="fig" rid="pcbi.1004439.g001">Fig 1B</xref>). The excitatory connections are of two kinds: recurrent connections from within the excitatory population, and external inputs.</p>
<fig id="pcbi.1004439.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004439.g001</object-id>
<label>Fig 1</label>
<caption>
<title>A sketch of the network and the neuron model.</title>
<p><bold>A.</bold> Structure of the network. The fully-connected network consists of <italic>N</italic> binary (<italic>s</italic><sub><italic>i</italic></sub> ∈ {0,1}) neurons and an aggregated inhibitory unit. The global inhibition is a function of the state of the network and the external fields, i.e. <inline-formula id="pcbi.1004439.e002"><alternatives><graphic id="pcbi.1004439.e002g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e002"/><mml:math id="M2" display="inline" overflow="scroll"><mml:mi mathvariant="script">I</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mspace width="1pt"/> <mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo stretchy="false">)</mml:mo></mml:math></alternatives></inline-formula>. A memory pattern <inline-formula id="pcbi.1004439.e003"><alternatives><graphic id="pcbi.1004439.e003g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e003"/><mml:math id="M3" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>ξ</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is encoded as strong external fields, i.e. <inline-formula id="pcbi.1004439.e004"><alternatives><graphic id="pcbi.1004439.e004g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e004"/><mml:math id="M4" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mspace width="1pt"/> <mml:mo>=</mml:mo> <mml:mspace width="1pt"/> <mml:mi>X</mml:mi> <mml:mspace width="1pt"/> <mml:mover accent="true"><mml:mi>ξ</mml:mi> <mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula> and presented to the network during the learning phase. <bold>B.</bold> Each neuron receives excitatory recurrent inputs (thin black arrows) from the other neurons, a global inhibitory input (red connections), and a strong binary external field (<italic>x</italic><sub><italic>i</italic></sub> ∈ {0, <italic>X</italic>}; thick black arrows). All these inputs are summed to obtain the total field, which is then compared to a neuronal threshold <italic>θ</italic>; the output of the neuron is a step function of the result.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004439.g001"/>
</fig>
<p>The recurrent excitatory connections are mediated by synaptic weights, denoted by a matrix <italic>W</italic> whose elements <italic>w</italic><sub><italic>ij</italic></sub> (the weight of the synapse from neuron <italic>j</italic> to <italic>i</italic>) are continuous non-negative variables (<italic>w</italic><sub><italic>ij</italic></sub> ∈ [0,∞); <italic>w</italic><sub><italic>ii</italic></sub> = 0). In the following, and in all our simulations, we assume that the weights are initialized randomly before the training takes place (see <xref ref-type="sec" rid="sec011">Materials and Methods</xref>).</p>
<p>Therefore, in the absence of external inputs, the local field of each neuron <italic>i</italic> is given by:
<disp-formula id="pcbi.1004439.e005"><alternatives><graphic id="pcbi.1004439.e005g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e005"/><mml:math id="M5" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>s</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="script">I</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(2)</label></disp-formula>
where <inline-formula id="pcbi.1004439.e006"><alternatives><graphic id="pcbi.1004439.e006g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e006"/><mml:math id="M6" display="inline" overflow="scroll"><mml:msub><mml:mi mathvariant="script">I</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mspace width="1pt"/> <mml:mo stretchy="false">(</mml:mo> <mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo stretchy="false">)</mml:mo></mml:math></alternatives></inline-formula> represents the inhibitory input.</p>
<p>For the sake of simplicity, we simulated a synchronous update process, in which the activity of each neuron <italic>s</italic><sub><italic>i</italic></sub> is computed from the local field <italic>v</italic><sub><italic>i</italic></sub> at the previous time step, and all updates happen in parallel.</p>
<p>The network was designed so that, in absence of external input and prior to the training process, it should spontaneously stabilize itself to some fixed overall average activity level <italic>f</italic> (fraction of active neurons, or sparseness), regardless of the initial conditions. In particular, we aimed at avoiding trivial attractors (the all-off and all-on states). To this end, we model the inhibitory feedback (in absence of external inputs) as a linear function of the overall excitatory activity:
<disp-formula id="pcbi.1004439.e007"><alternatives><graphic id="pcbi.1004439.e007g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e007"/><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">I</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>λ</mml:mi> <mml:mo>(</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>f</mml:mi> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(3)</label></disp-formula>
The parameters <italic>H</italic><sub>0</sub> and <italic>λ</italic> can be understood as follows: <italic>H</italic><sub>0</sub> is the average inhibitory activity when the excitatory network has the desired activity level <italic>f</italic>, i.e. when <inline-formula id="pcbi.1004439.e008"><alternatives><graphic id="pcbi.1004439.e008g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e008"/><mml:math id="M8" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mi>N</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>; <italic>λ</italic> measures the strength of the inhibitory feedback onto the excitatory network. This expression can be interpreted as a first-order approximation of the inhibitory activity as a function of the excitatory activity around some reference value <italic>fN</italic>, which is reasonable under the assumption that the deviations from <italic>fN</italic> are small enough. Indeed, by properly setting these two parameters in relation to the other network parameters (such as <italic>θ</italic> and the average connection strength) it is possible to achieve the desired goal of a self-stabilizing network.</p>
<p>In the training process, the network is presented a set of <italic>p</italic> patterns in the form of strong external inputs, representing the memories which need to be stored. We denote the patterns as <inline-formula id="pcbi.1004439.e009"><alternatives><graphic id="pcbi.1004439.e009g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e009"/><mml:math id="M9" display="inline" overflow="scroll"><mml:mo>{</mml:mo> <mml:mover accent="true"><mml:msup><mml:mi>ξ</mml:mi> <mml:mi>μ</mml:mi></mml:msup> <mml:mo>→</mml:mo></mml:mover> <mml:mo>}</mml:mo></mml:math></alternatives></inline-formula> (where <italic>μ</italic> = 1…<italic>p</italic> and <inline-formula id="pcbi.1004439.e010"><alternatives><graphic id="pcbi.1004439.e010g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e010"/><mml:math id="M10" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>ξ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>∈</mml:mo> <mml:mo stretchy="false">{</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>), and assume that each entry <inline-formula id="pcbi.1004439.e011"><alternatives><graphic id="pcbi.1004439.e011g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e011"/><mml:math id="M11" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>ξ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>μ</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is drawn randomly and independently. For simplicity, the coding level <italic>f</italic> for the patterns was set equal to the spontaneous activity level of the network, i.e. <inline-formula id="pcbi.1004439.e012"><alternatives><graphic id="pcbi.1004439.e012g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e012"/><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>ξ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> with probability <italic>f</italic>, 0 otherwise. During the presentation of a pattern <italic>μ</italic>, each neuron <italic>i</italic> receives an external binary input <inline-formula id="pcbi.1004439.e013"><alternatives><graphic id="pcbi.1004439.e013g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e013"/><mml:math id="M13" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>X</mml:mi> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>μ</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>X</italic> denotes the strength of the external inputs, which we parameterized as <inline-formula id="pcbi.1004439.e014"><alternatives><graphic id="pcbi.1004439.e014g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e014"/><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi> <mml:mo>=</mml:mo> <mml:mi>γ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>. In addition, the external input also affects the inhibitory part of the network, eliciting a response which indirectly downregulates the excitatory neurons. We model this effect as an additional term <italic>H</italic><sub>1</sub> in the expression for the inhibitory term (<xref ref-type="disp-formula" rid="pcbi.1004439.e007">Eq 3</xref>), which therefore becomes:
<disp-formula id="pcbi.1004439.e015"><alternatives><graphic id="pcbi.1004439.e015g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e015"/><mml:math id="M15" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:mrow><mml:mi>f</mml:mi> <mml:mi>N</mml:mi> <mml:mi>X</mml:mi></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mi>λ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>f</mml:mi> <mml:mi>N</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(4)</label></disp-formula>
The general expression for the local field <italic>v</italic><sub><italic>i</italic></sub> then reads:
<disp-formula id="pcbi.1004439.e016"><alternatives><graphic id="pcbi.1004439.e016g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e016"/><mml:math id="M16" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>v</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>s</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(5)</label></disp-formula>
In the absence of external fields, <italic>x</italic><sub><italic>i</italic></sub> = 0 for all <italic>i</italic>, and thus Eqs <xref ref-type="disp-formula" rid="pcbi.1004439.e015">4</xref> and <xref ref-type="disp-formula" rid="pcbi.1004439.e016">5</xref> reduce to the previous expressions Eqs <xref ref-type="disp-formula" rid="pcbi.1004439.e007">3</xref> and <xref ref-type="disp-formula" rid="pcbi.1004439.e005">2</xref>.</p>
<p>The goal of the learning process is to find values of <italic>w</italic><sub><italic>ij</italic></sub>’s such that the patterns <inline-formula id="pcbi.1004439.e017"><alternatives><graphic id="pcbi.1004439.e017g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e017"/><mml:math id="M17" display="inline" overflow="scroll"><mml:mo>{</mml:mo> <mml:mover accent="true"><mml:msup><mml:mi>ξ</mml:mi> <mml:mi>μ</mml:mi></mml:msup> <mml:mo>→</mml:mo></mml:mover> <mml:mo>}</mml:mo></mml:math></alternatives></inline-formula> become attractors of the network dynamics. Qualitatively, this means that, if the training process is successful, then whenever the network state gets sufficiently close to one of the stored patterns, i.e. whenever the Hamming distance <inline-formula id="pcbi.1004439.e018"><alternatives><graphic id="pcbi.1004439.e018g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e018"/><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:mi>d</mml:mi> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:mrow><mml:mo stretchy="true">∣</mml:mo> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>−</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo stretchy="true">∣</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> between the current network state and a pattern <italic>μ</italic> is sufficiently small, the network dynamics in the absence of external inputs should drive the network state towards a fixed point equal to the pattern itself (or very close to it). The general underlying idea is that, after a pattern is successfully learned, some brief external input which initializes the network close to the learned state would be sufficient for the network to recognize and retrieve the pattern. The maximum value of <italic>d</italic> for which this property holds is then called the basin of attraction size (or just basin size hereafter for simplicity); indeed, there is generally a trade-off between the number of patterns which can be stored according to this criterion and the size of their basin of attraction.</p>
<p>More precisely, the requirement that a pattern <inline-formula id="pcbi.1004439.e019"><alternatives><graphic id="pcbi.1004439.e019g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e019"/><mml:math id="M19" display="inline" overflow="scroll"><mml:mover accent="true"><mml:msup><mml:mi>ξ</mml:mi> <mml:mi>μ</mml:mi></mml:msup> <mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is a fixed point of the network dynamics in the absence of external fields can be reduced to a condition for each neuron <italic>i</italic> (cfr. Eqs <xref ref-type="disp-formula" rid="pcbi.1004439.e015">4</xref> and <xref ref-type="disp-formula" rid="pcbi.1004439.e016">5</xref>):
<disp-formula id="pcbi.1004439.e020"><alternatives><graphic id="pcbi.1004439.e020g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e020"/><mml:math id="M20" display="block" overflow="scroll"><mml:mrow><mml:mo>∀</mml:mo> <mml:mi>i</mml:mi> <mml:mo>:</mml:mo> <mml:mspace width="4pt"/><mml:mo>Θ</mml:mo> <mml:mo stretchy="true">(</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup></mml:mrow> <mml:mo>-</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mover accent="true"><mml:mn>0</mml:mn> <mml:mo>→</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:msup><mml:mi>ξ</mml:mi> <mml:mi>μ</mml:mi></mml:msup> <mml:mo>→</mml:mo></mml:mover> <mml:mo stretchy="false">)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>θ</mml:mi> <mml:mo stretchy="true">)</mml:mo> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>.</mml:mo></mml:mrow></mml:math></alternatives> <label>(6)</label></disp-formula>
This condition only guarantees that, if the network is initialized into a state <inline-formula id="pcbi.1004439.e021"><alternatives><graphic id="pcbi.1004439.e021g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e021"/><mml:math id="M21" display="inline" overflow="scroll"><mml:mover accent="true"><mml:mi>s</mml:mi> <mml:mo>→</mml:mo></mml:mover> <mml:mspace width="1pt"/> <mml:mo>=</mml:mo> <mml:mspace width="1pt"/> <mml:mover accent="true"><mml:msup><mml:mi>ξ</mml:mi> <mml:mi>μ</mml:mi></mml:msup> <mml:mo>→</mml:mo></mml:mover></mml:math></alternatives></inline-formula>, then it will not spontaneously change its state, i.e. it implements a zero-size basin of attraction. A simple way to enlarge the basin size is to make the requirement in <xref ref-type="disp-formula" rid="pcbi.1004439.e020">Eq 6</xref> more stringent, by enforcing a more stringent constraint for local fields:
<disp-formula id="pcbi.1004439.e022"><alternatives><graphic id="pcbi.1004439.e022g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e022"/><mml:math id="M22" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>∀</mml:mo> <mml:mi>i</mml:mi> <mml:mo>:</mml:mo> <mml:mspace width="4pt"/><mml:mo>{</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup></mml:mrow> <mml:mo>-</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mn>0</mml:mn> <mml:mo>→</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:msup><mml:mi>ξ</mml:mi> <mml:mi>μ</mml:mi></mml:msup> <mml:mo>→</mml:mo></mml:mover> <mml:mo>)</mml:mo> <mml:mo>&gt;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>f</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mi>ϵ</mml:mi></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:msubsup><mml:mi>ξ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup></mml:mrow> <mml:mo>-</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mn>0</mml:mn> <mml:mo>→</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:msup><mml:mi>ξ</mml:mi> <mml:mi>μ</mml:mi></mml:msup> <mml:mo>→</mml:mo></mml:mover> <mml:mo>)</mml:mo> <mml:mo>&lt;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>-</mml:mo> <mml:mi>f</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mi>ϵ</mml:mi></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext> <mml:mspace width="4.pt"/><mml:msubsup><mml:mi>ξ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo/></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
where <italic>ϵ</italic> ≥ 0 is a robustness parameter. When <italic>ϵ</italic> = 0, we recover the previous zero-basin-size scenario; increasing <italic>ϵ</italic> we make the neurons’ response more robust towards noise in their inputs, and thus we enlarge the basin of attraction of the stored patterns (but then fewer patterns can be stored, as noted above).</p>
</sec>
<sec id="sec004">
<title>The three-threshold learning rule (3TLR)</title>
<p>In the training phase, the network is presented with patterns as strong external fields <italic>x</italic><sub><italic>i</italic></sub>. Patterns are presented sequentially in random order. For each pattern <italic>μ</italic>, we simulated the following scheme:
<list list-type="simple"><list-item><p><bold>Step 1</bold>: The pattern is presented (i.e. the external inputs <italic>x</italic><sub><italic>i</italic></sub> are set to <inline-formula id="pcbi.1004439.e023"><alternatives><graphic id="pcbi.1004439.e023g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e023"/><mml:math id="M23" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>μ</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>). A single step of synchronous updating is performed (Eqs <xref ref-type="disp-formula" rid="pcbi.1004439.e001">1</xref>, <xref ref-type="disp-formula" rid="pcbi.1004439.e015">4</xref> and <xref ref-type="disp-formula" rid="pcbi.1004439.e016">5</xref>). If the external inputs are strong enough, i.e. <italic>γ</italic> is large enough, this updating sets the network in a state corresponding to the presented pattern.</p></list-item> <list-item><p><bold>Step 2</bold>: Learning occurs. Each neuron <italic>i</italic> may update its synaptic weights depending on 1) their current value <inline-formula id="pcbi.1004439.e024"><alternatives><graphic id="pcbi.1004439.e024g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e024"/><mml:math id="M24" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>, 2) the state of the pre-synaptic neurons, and 3) the value of the local field <italic>v</italic><sub><italic>i</italic></sub>. Therefore, all the information required is locally accessible, and no explicit error signals are used. The new synaptic weights <inline-formula id="pcbi.1004439.e025"><alternatives><graphic id="pcbi.1004439.e025g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e025"/><mml:math id="M25" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> are set to:
<disp-formula id="pcbi.1004439.e026"><alternatives><graphic id="pcbi.1004439.e026g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e026"/><mml:math id="M26" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext> <mml:mspace width="1pt"/> <mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mspace width="1pt"/> <mml:mi>θ</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>otherwise,</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow> </mml:mrow></mml:mrow></mml:math></alternatives> <label>(8)</label></disp-formula>
where <italic>η</italic> is the learning rate, and <italic>θ</italic><sub>0</sub> and <italic>θ</italic><sub>1</sub> are two auxiliary learning thresholds set as
<disp-formula id="pcbi.1004439.e027"><alternatives><graphic id="pcbi.1004439.e027g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e027"/><mml:math id="M27" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>θ</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>θ</mml:mi> <mml:mo>-</mml:mo> <mml:mo>(</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>)</mml:mo> <mml:mi>f</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula> <disp-formula id="pcbi.1004439.e028"><alternatives><graphic id="pcbi.1004439.e028g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e028"/><mml:math id="M28" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>θ</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>θ</mml:mi> <mml:mo>+</mml:mo> <mml:mo>(</mml:mo> <mml:mi>γ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>ϵ</mml:mi> <mml:mo>)</mml:mo> <mml:mi>f</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula></p></list-item></list></p>
<p>We refer to this update scheme as the “three-threshold learning rule” (3TRL). After some number of presentations, we checked whether the patterns are learned by presenting a noisy version of these patterns, and checking whether the patterns (or network states which are very close to the patterns) are fixed points of the network dynamics.</p>
<p>When <italic>N</italic> ≫ 1, <italic>γ</italic> is large enough, and <italic>H</italic><sub>1</sub> = <italic>fX</italic>, the update rule described by <xref ref-type="disp-formula" rid="pcbi.1004439.e026">Eq 8</xref> is essentially equivalent to the perceptron learning rule for the task described in <xref ref-type="disp-formula" rid="pcbi.1004439.e022">Eq 7</xref>. This can be shown as follows (see also <xref ref-type="fig" rid="pcbi.1004439.g002">Fig 2</xref> for a graphical representation of the case <italic>f</italic> = 0.5 and <italic>ϵ</italic> = 0): when a stimulus is presented, the population of neurons is divided in two groups, one for which <italic>x</italic><sub><italic>i</italic></sub> = 0 and one for which <italic>x</italic><sub><italic>i</italic></sub> = <italic>X</italic>. The net effect of the stimulus presentation on the local field has to take into account the indirect effect through the inhibitory part of the network (see <xref ref-type="disp-formula" rid="pcbi.1004439.e015">Eq 4</xref>), and thus is equal to −<italic>fX</italic> for the <italic>x</italic><sub><italic>i</italic></sub> = 0 population and to (1 − <italic>f</italic>)<italic>X</italic> for the <italic>x</italic><sub><italic>i</italic></sub> = <italic>X</italic> population. Before learning, the distribution of the local fields across the excitatory population, in the limit <italic>N</italic> → ∞, is a Gaussian whose standard deviation is proportional to <inline-formula id="pcbi.1004439.e029"><alternatives><graphic id="pcbi.1004439.e029g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e029"/><mml:math id="M29" display="inline" overflow="scroll"><mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>, due to the central limit theorem; moreover, the parameter <italic>H</italic><sub>0</sub> is set so that the average activity level of the network is <italic>f</italic>, which means that the center of the Gaussian will be within a distance of order <inline-formula id="pcbi.1004439.e030"><alternatives><graphic id="pcbi.1004439.e030g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e030"/><mml:math id="M30" display="inline" overflow="scroll"><mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula> from the neuronal threshold <italic>θ</italic> (this also applies if we use different values for the spontaneous activity level and the pattern activity level). Therefore, if <inline-formula id="pcbi.1004439.e031"><alternatives><graphic id="pcbi.1004439.e031g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e031"/><mml:math id="M31" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi> <mml:mo>=</mml:mo> <mml:mi>γ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula> is large enough, the state of the network during stimulus presentation will be effectively clamped to the desired output, i.e. <inline-formula id="pcbi.1004439.e032"><alternatives><graphic id="pcbi.1004439.e032g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e032"/><mml:math id="M32" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>μ</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> for all <italic>i</italic>. This fact has two consequences: 1) the local field potential can be used to detect the desired output by just comparing it to the threshold, and 2) each neuron <italic>i</italic> will receive, as its recurrent inputs {<italic>s</italic><sub><italic>j</italic></sub>}<sub><italic>j</italic> ≠ <italic>i</italic></sub>, the rest of the pattern <inline-formula id="pcbi.1004439.e033"><alternatives><graphic id="pcbi.1004439.e033g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e033"/><mml:math id="M33" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="true">{</mml:mo> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mi>j</mml:mi> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo stretchy="true">}</mml:mo></mml:mrow> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>≠</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>. Furthermore, due to the choice of the secondary thresholds <italic>θ</italic><sub>0</sub> and <italic>θ</italic><sub>1</sub> in Eqs <xref ref-type="disp-formula" rid="pcbi.1004439.e027">9</xref> and <xref ref-type="disp-formula" rid="pcbi.1004439.e028">10</xref>, the difference between the local field and <italic>θ</italic><sub>0</sub> (or <italic>θ</italic><sub>1</sub>) during stimulus presentation for the <italic>x</italic><sub><italic>i</italic></sub> = 0 population (or <italic>x</italic><sub><italic>i</italic></sub> = <italic>X</italic>, respectively) is equal to the difference between the local field and <inline-formula id="pcbi.1004439.e034"><alternatives><graphic id="pcbi.1004439.e034g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e034"/><mml:math id="M34" display="inline" overflow="scroll"><mml:mrow><mml:mi>θ</mml:mi> <mml:mo>−</mml:mo> <mml:mi>f</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mi>ϵ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> (or <inline-formula id="pcbi.1004439.e035"><alternatives><graphic id="pcbi.1004439.e035g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e035"/><mml:math id="M35" display="inline" overflow="scroll"><mml:mrow><mml:mi>θ</mml:mi> <mml:mo>+</mml:mo> <mml:mi>f</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mi>ϵ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>, respectively) in the absence of external stimuli, provided the recurrent inputs are the same. Therefore, the value of the local field <italic>v</italic><sub><italic>i</italic></sub> during stimulus presentation in relation to the three thresholds <italic>θ</italic>, <italic>θ</italic><sub>0</sub> and <italic>θ</italic><sub>1</sub> is sufficient to determine whether an error is made with respect to the constraints of <xref ref-type="disp-formula" rid="pcbi.1004439.e022">Eq 7</xref>, and which kind of error is made. Following these observations, it is straightforward to map the standard perceptron learning rule on the 4 different cases which may occur (see <xref ref-type="fig" rid="pcbi.1004439.g002">Fig 2</xref>), resulting in <xref ref-type="disp-formula" rid="pcbi.1004439.e026">Eq 8</xref>.</p>
<fig id="pcbi.1004439.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004439.g002</object-id>
<label>Fig 2</label>
<caption>
<title>The three-threshold learning rule (3TLR), and its relationship with the standard perceptron learning rule (PLR).</title>
<p>The perceptron learning rule modifies the synaptic weights by comparing the desired output with the actual output to obtain an error signal, subsequently changing the weights in the opposite direction of the error signal (see the table in the left panel). For a pattern which is uncorrelated with the current synaptic weights, the distribution is Gaussian (in the limit of large <italic>N</italic>), due to the central limit theorem. <italic>H</italic><sub>0</sub> is set such that, on average, a fraction <italic>f</italic> of the local fields are above the neuronal threshold <italic>θ</italic>; in the case of <italic>f</italic> = 0.5, this means that the Gaussian is centered on <italic>θ</italic> (left panel). In our model (<xref ref-type="fig" rid="pcbi.1004439.g001">Fig 1B</xref>), the desired output is given as a strong external input, whose distribution across the population is bimodal (with two delta functions on <italic>x</italic><sub><italic>i</italic></sub> = 0 and <italic>x</italic><sub><italic>i</italic></sub> = <italic>X</italic>); therefore, the distribution of the local fields during stimulus presentation becomes bimodal as well (right panel). The left and right bumps of this distribution correspond to cases where the desired outputs are zero and one, respectively. Note that, since the external input also elicits an inhibitory response, the neurons in the network which are not directly affected by the external input (i.e. those with desired output equal to zero) are effectively hyperpolarized. If <italic>X</italic> is sufficiently large, the two distributions do not overlap, and the four cases of the PLR can be mapped to the four regions determined from the three thresholds, indicated by vertical dashed lines (see text).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004439.g002"/>
</fig>
<p>In <xref ref-type="fig" rid="pcbi.1004439.g003">Fig 3</xref> we demonstrate the effect of the learning rule on the distribution of the local field potentials as measured from a simulation (with <italic>f</italic> = 0.5 and <italic>ϵ</italic> = 1.2): the initial distribution of the local fields of the neurons, before the learning process takes place and in the absence of external fields, is well described by a Gaussian distribution centered on the neuronal threshold <italic>θ</italic> (see <xref ref-type="fig" rid="pcbi.1004439.g003">Fig 3A</xref>) with a standard deviation which scales as <inline-formula id="pcbi.1004439.e036"><alternatives><graphic id="pcbi.1004439.e036g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e036"/><mml:math id="M36" display="inline" overflow="scroll"><mml:mrow><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>. During a pattern presentation, the resulting distribution becomes a bimodal one; before learning takes place, the distribution is given by the sum of two Gaussians of equal width, centered around <inline-formula id="pcbi.1004439.e037"><alternatives><graphic id="pcbi.1004439.e037g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e037"/><mml:math id="M37" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>θ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>f</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mi>ϵ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004439.e038"><alternatives><graphic id="pcbi.1004439.e038g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e038"/><mml:math id="M38" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>θ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>−</mml:mo> <mml:mi>f</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mi>ϵ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> (<xref ref-type="fig" rid="pcbi.1004439.g003">Fig 3B</xref>). The left Gaussian corresponds to the cases where <italic>x</italic><sub><italic>i</italic></sub> = 0 and the right one to the cases where <italic>x</italic><sub><italic>i</italic></sub> = <italic>X</italic>. Having applied the learning rule, we observe that the depression region (i.e. the interval (<italic>θ</italic><sub>0</sub>, <italic>θ</italic>)) and the potentiation region (i.e. (<italic>θ</italic>, <italic>θ</italic><sub>1</sub>)) gets depleted (<xref ref-type="fig" rid="pcbi.1004439.g003">Fig 3C</xref>). In the testing phase, when the external inputs are absent, the left and right parts of the distribution come closer, such that the distance between the two peaks is equal to at least <inline-formula id="pcbi.1004439.e039"><alternatives><graphic id="pcbi.1004439.e039g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e039"/><mml:math id="M39" display="inline" overflow="scroll"><mml:mrow><mml:mn>2</mml:mn> <mml:mi>ϵ</mml:mi> <mml:mi>f</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula> (<xref ref-type="fig" rid="pcbi.1004439.g003">Fig 3D</xref>). This margin between the local fields of the ON and OFF neurons makes the attractors more robust.</p>
<fig id="pcbi.1004439.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004439.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Distribution of local fields before and after learning for <italic>f</italic> = 0.5 and non-zero robustness.</title>
<p><bold>A.</bold> Before learning begins, the distribution of local field of neurons is a Gaussian distribution (due to central limit theorem) centered around neuronal threshold <italic>θ</italic> both for neurons with the desired output zero (OFF neurons) and with the desired output one (ON neurons). The goal is to have the local field distribution of ON neurons (red curve) to be above the threshold <italic>θ</italic>, and that of OFF neurons to be below <italic>θ</italic>. <bold>B.</bold> Once any of the to-be-stored patterns are presented as strong external fields, right before the learning process starts, the local field distribution of the OFF neuron shifts toward the left-side centered around <inline-formula id="pcbi.1004439.e040"><alternatives><graphic id="pcbi.1004439.e040g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e040"/><mml:math id="M40" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>θ</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mi>f</mml:mi> <mml:mi>ϵ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>, whereas the distribution of the ON neurons moves toward the right-side, centered around <inline-formula id="pcbi.1004439.e041"><alternatives><graphic id="pcbi.1004439.e041g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e041"/><mml:math id="M41" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>θ</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>−</mml:mo> <mml:mi>f</mml:mi> <mml:mi>ϵ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>, with a negligible overlap between the two curves if the external field is strong enough. Thanks to the strong external fields and global inhibition, the local fields of the ON and OFF neurons are well separated. <bold>C.</bold> Due to the learning process, the local fields within the depression region [i.e. (<italic>θ</italic><sub>0</sub>, <italic>θ</italic>)] get pushed to the left-side, below <italic>θ</italic><sub>0</sub>, whereas those within the potentiation region get pushed further to the right-side, above <italic>θ</italic><sub>1</sub>. If the learning process is successful, it will result in a region (<italic>θ</italic><sub>0</sub>, <italic>θ</italic><sub>1</sub>) which no longer contain local fields, with two sharp peaks on <italic>θ</italic><sub>0</sub> and <italic>θ</italic><sub>1</sub>. <bold>D.</bold> After successful learning, once the external fields are removed, the blue and red curves come closer, with a gap equal to <inline-formula id="pcbi.1004439.e042"><alternatives><graphic id="pcbi.1004439.e042g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e042"/><mml:math id="M42" display="inline" overflow="scroll"><mml:mrow><mml:mn>2</mml:mn> <mml:mi>f</mml:mi> <mml:mi>ϵ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>. The larger the robustness parameter <italic>ϵ</italic>, the more the gap between the left- and right-side of the distribution. Notice that now the red curve is fully above <italic>θ</italic> which means those neurons remain stably ON, while the the blue curve is fully below <italic>θ</italic>, which means those neurons are stably OFF. Therefore the corresponding pattern is successfully stored by the network.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004439.g003"/>
</fig>
</sec>
<sec id="sec005">
<title>Storage capacity</title>
<p>Since our proposed learning rule is able to mimic (or approximate, depending on the parameters) the perceptron learning rule, which is known to be able to solve the task posed by <xref ref-type="disp-formula" rid="pcbi.1004439.e022">Eq 7</xref> whenever a solution exists, we expect that a network implementing such rule can get close to maximal capacity in terms of the number of memories which it can store at a given robustness level. The storage capacity, denoted by <italic>α</italic> = <italic>p</italic>/<italic>N</italic>, is measured as a ratio of the maximum number of patterns <italic>p</italic> which can successfully be stored to the number of neurons <italic>N</italic>, in the limit of large <italic>N</italic>. As mentioned above, it is a function of the basin size.</p>
<p>We used the following definition for the basin size: a set of <italic>p</italic> patterns is said to be successfully stored at a size <italic>b</italic> if, for each pattern, the retrieval rate when starting from a state in which a fraction <italic>b</italic> of the pattern was randomized is at least 90%. The retrieval rate is measured by the probability that the network dynamics is able to bring the network state to an attractor within 1% distance from the pattern, in at most 30 steps. The distance between the state of the network and a pattern <italic>μ</italic> is measured by the normalized Hamming distance <inline-formula id="pcbi.1004439.e043"><alternatives><graphic id="pcbi.1004439.e043g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e043"/><mml:math id="M43" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>N</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:msubsup> <mml:mrow><mml:mo stretchy="true">∣</mml:mo> <mml:msub><mml:mi>s</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>−</mml:mo> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo stretchy="true">∣</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Therefore, at coding level <italic>f</italic> = 0.5, reaching a basin size <italic>b</italic> means that the network can successfully recover patterns starting from a state at distance <italic>b</italic>/2.</p>
<p>
<xref ref-type="fig" rid="pcbi.1004439.g004">Fig 4A</xref> shows the maximal capacity as a function of the basin size for a simulated network of <italic>N</italic> = 1001 neurons. We simulated many pairs of (<italic>α</italic>, <italic>ϵ</italic>) with different random seeds, obtaining a probability of success for each pair. The red line shows the points for which the probability of successful storage is 0.5, and the error bars span 0.95 to 0.05 success probability. The capacity was optimized over the robustness parameter <italic>ϵ</italic>. The maximal capacity (the Gardner bound) in the limit of <italic>N</italic> → ∞ at the zero basin size is <italic>α</italic><sub><italic>c</italic></sub> = 2 for our model (see <xref ref-type="sec" rid="sec011">Materials and Methods</xref> for the calculation), as for a network with unconstrained synaptic weights [<xref ref-type="bibr" rid="pcbi.1004439.ref020">20</xref>]. In <xref ref-type="fig" rid="pcbi.1004439.g004">Fig 4A</xref>, we also compare our network with the Hopfield model. Our network stores close to the maximal capacity at zero basin size, at least eleven times more than the Hopfield model. Across the range of basin sizes, 3TLR achieves more than twice the capacity that can be achieved with the Hopfield model.</p>
<fig id="pcbi.1004439.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004439.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Critical capacity as a function of the basin size and the robustness parameter.</title>
<p><bold>A.</bold> The red plot shows the critical capacity as a function of the size of the basins of attraction (<italic>N</italic> = 1001 neurons in the dense regime <italic>f</italic> = 0.5) when the strength of the external field is large (<italic>γ</italic> = 6) such that the ON and OFF neuronal populations are well separated. The points indicate 0.5 probability of successful storage at a given basin size, optimized over the robustness parameter <italic>ϵ</italic>. The error bars show the [0.95,0.05] probability interval for successful storage. The blue plot shows the performance of the Hopfield model with <italic>N</italic> = 1001 neurons. The maximal capacity at zero basin size (the Gardner bound) is equal to 2. <bold>B.</bold> To compare the result of simulation of our model with the analytical results, we plotted the critical capacity as a function of the robustness parameter <italic>ϵ</italic>. The dark red curve is the critical capacity versus <italic>ϵ</italic> for our model obtained form analytical calculations (see <xref ref-type="sec" rid="sec011">Materials and Methods</xref>), the cyan line shows the result of simulations of our model, and the dark blue shows the Gardner bound for a network with no constraints on synaptic weights. The difference between the two theoretical curves is due to the constraints on the weights in our network.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004439.g004"/>
</fig>
<p>The enlargement of the basin of attraction was achieved by increasing the robustness parameter <italic>ϵ</italic>. We computed the maximal theoretical capacity as a function of <italic>ϵ</italic> at <italic>N</italic> → ∞ (see <xref ref-type="sec" rid="sec011">Materials and Methods</xref>) and compared it to our simulations, and to the maximal theoretical capacity of the Hopfield network. The results are shown in <xref ref-type="fig" rid="pcbi.1004439.g004">Fig 4B</xref>. For any given value of <italic>ϵ</italic>, the cyan curve shows the maximum <italic>α</italic> for which the success ratio with our network was at least 0.5 across different runs. The difference between the theory and the experiments in our model can be ascribed to several factors: the finite size of the network; the choice of the finite learning rate <italic>η</italic>, and the fact that we imposed a hard limit on the number of pattern presentations (see number of iterations in <xref ref-type="table" rid="pcbi.1004439.t001">Table 1</xref>), while the perceptron rule for excitatory synaptic connectivity is only guaranteed to be optimal in the limit of <italic>η</italic> → 0, with a number of presentations inversely proportional to <italic>η</italic> [<xref ref-type="bibr" rid="pcbi.1004439.ref025">25</xref>]. Note that the correspondence between the PLR and the 3TLR is only perfect in the large <italic>γ</italic> limit, and is only approximate otherwise, as can be shown by comparing explicitly the synaptic matrices obtained by both algorithms on the same set of patterns (see <xref ref-type="sec" rid="sec011">Materials and Methods</xref>).</p>
<table-wrap id="pcbi.1004439.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004439.t001</object-id>
<label>Table 1</label>
<caption>
<title>Table of parameters in the simulation.</title>
</caption>
<alternatives>
<graphic id="pcbi.1004439.t001g" mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004439.t001"/>
<table frame="box" rules="all" border="0">
<colgroup span="1">
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
<col align="left" valign="top" span="1"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="1" colspan="1"><bold>Parameter name</bold></th>
<th align="left" rowspan="1" colspan="1"><bold>Value in dense regime</bold></th>
<th align="left" rowspan="1" colspan="1"><bold>Value in sparse regime</bold></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>N</italic></td>
<td align="left" rowspan="1" colspan="1">1001</td>
<td align="left" rowspan="1" colspan="1">1001</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">
<inline-formula id="pcbi.1004439.e044">
<alternatives>
<graphic id="pcbi.1004439.e044g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e044"/>
<mml:math id="M44" display="inline" overflow="scroll">
<mml:mrow>
<mml:mi>λ</mml:mi>
<mml:mo>=</mml:mo>
<mml:msubsup>
<mml:mover>
<mml:mi>w</mml:mi>
<mml:mo accent="true">‾</mml:mo>
</mml:mover>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
<mml:mtext mathvariant="normal">init</mml:mtext>
</mml:msubsup>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula>
</td>
<td align="left" rowspan="1" colspan="1">≈ 1.08</td>
<td align="left" rowspan="1" colspan="1">≈ 1.08</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>f</italic></td>
<td align="left" rowspan="1" colspan="1">0.5</td>
<td align="left" rowspan="1" colspan="1">0.2</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>ψ</italic></td>
<td align="left" rowspan="1" colspan="1">0.35</td>
<td align="left" rowspan="1" colspan="1">0.35</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>θ</italic></td>
<td align="left" rowspan="1" colspan="1">350</td>
<td align="left" rowspan="1" colspan="1">350</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>η</italic></td>
<td align="left" rowspan="1" colspan="1">0.01 [0.001 when <italic>ϵ</italic> = 0]</td>
<td align="left" rowspan="1" colspan="1">0.01 [0.001 when <italic>ϵ</italic> = 0]</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>γ</italic></td>
<td align="left" rowspan="1" colspan="1">6.0</td>
<td align="left" rowspan="1" colspan="1">12.0</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"># of interations (learning)</td>
<td align="left" rowspan="1" colspan="1">1000 [10000 when <italic>ϵ</italic> = 0]</td>
<td align="left" rowspan="1" colspan="1">1000 [10000 when <italic>ϵ</italic> = 0]</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"># of trials in test phase</td>
<td align="left" rowspan="1" colspan="1">50</td>
<td align="left" rowspan="1" colspan="1">50</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>A crucial ingredient of the 3TLR is having a strong external input which effectively acts as a supervisory signal. How strong do the external fields need to be? How much does the capacity depend on this strength? To answer these questions, we measured the maximum number of stored patterns as a function of the parameter <italic>γ</italic> which determines the strength of external fields as <inline-formula id="pcbi.1004439.e045"><alternatives><graphic id="pcbi.1004439.e045g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e045"/><mml:math id="M45" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi> <mml:mo>=</mml:mo> <mml:mi>γ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>. This parameter, in fact, determines how far the two Gaussian distributions of the local field are; as shown in <xref ref-type="fig" rid="pcbi.1004439.g002">Fig 2</xref>, the distance between the two peaks of the distribution is <italic>X</italic>. For large enough <italic>γ</italic>, the overlap of these two distributions is negligible and the capacity is maximal; but as we lower <italic>γ</italic>, the overlap increases, causing the learning rule to make mistakes, i.e. when it should potentiate, it depresses the synapses and vice versa. In our simulations with <italic>N</italic> = 1001 neurons in the dense regime <italic>f</italic> = 0.5 at a fixed epsilon <italic>ϵ</italic> = 0.3, we varied <italic>γ</italic> and computed the maximum <italic>α</italic> that can be achieved with a fixed number of iterations (1000). The capacity indeed gradually decreases as <italic>γ</italic> decreases, until it reaches a threshold, below which there is a sharp drop of capacity (see <xref ref-type="fig" rid="pcbi.1004439.g005">Fig 5</xref>). With the above values for the parameters, this transition occurs at <italic>γ</italic> ≈ 2.4.</p>
<fig id="pcbi.1004439.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004439.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Dependence of the critical capacity on the strength of the external input.</title>
<p>We varied the strength of the external field (<italic>γ</italic>) in order to quantify its effect on the learning process. The critical capacity is plotted as a function of <italic>γ</italic> at a fixed robustness <italic>ϵ</italic> = 0.3 in the dense regime <italic>f</italic> = 0.5. The simulations show that there is a very sharp drop in the maximum <italic>α</italic> when <italic>γ</italic> goes below ≈ 2.4.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004439.g005"/>
</fig>
<p>The 3TLR can also be adapted to work in a sparser regime, at a coding level lower than 0.5. However, the average activity level of the network is determined by <italic>H</italic><sub>0</sub>, and their relationship also involves the variance of the distribution of the synaptic weights when <italic>f</italic> ≠ 0.5 (see <xref ref-type="sec" rid="sec011">Materials and Methods</xref>). During the learning process, the variance of the weights changes, which implies that the parameter <italic>H</italic><sub>0</sub> must adapt correspondingly. In our simulations, this adaptation was performed after each complete presentation of the whole pattern set. In practice, this additional self-stabilizing mechanism could still be performed in an unsupervised fashion along with (or in alternation with) the learning process. Using this adjustment, we simulated the network at <italic>f</italic> = 0.2 and compared the results with the theoretical calculations. As shown in <xref ref-type="fig" rid="pcbi.1004439.g006">Fig 6</xref>, we can achieve at least 70% of the critical capacity across different values of the robustness parameter <italic>ϵ</italic>.</p>
<fig id="pcbi.1004439.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004439.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Capacity as a function of the robustness parameter <italic>ϵ</italic> at sparseness <italic>f</italic> = 0.2.</title>
<p>The theoretical calculations is compared with the simulations for <italic>f</italic> = 0.2. Note that the capacity in the sparse regime is higher than in the dense regime.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004439.g006"/>
</fig>
<p>We also investigated numerically the effect of correlations in the input patterns. The PLR is able to learn correlated patterns as long as a solution to the learning problem exists. As the 3TLR approximates the PLR, we expect the 3TLR to be able to learn correlated patterns as well. As a simple model of correlation, we tested patterns organized in <italic>L</italic> categories [<xref ref-type="bibr" rid="pcbi.1004439.ref026">26</xref>, <xref ref-type="bibr" rid="pcbi.1004439.ref027">27</xref>]. Each category was defined by a randomly generated prototype. Prototypes were uncorrelated from category to category. For each category, we then generated <italic>p</italic>/<italic>L</italic> patterns independently with a specified correlation coefficient <italic>c</italic> with the corresponding prototype. We show in <xref ref-type="fig" rid="pcbi.1004439.g007">Fig 7</xref> the results of simulations with <italic>L</italic> = 5, <italic>f</italic> = 0.2 and <italic>ϵ</italic> = 3. The figure shows that the learning rule reaches a capacity that is essentially independent of <italic>c</italic>, in the range 0 ≤ <italic>c</italic> ≤ 0.75.</p>
<fig id="pcbi.1004439.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004439.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Capacity as a function of correlations in the input patterns, for <italic>f</italic> = 0.2 at <italic>ϵ</italic> = 3.0.</title>
<p>Patterns are organized in categories, with a correlation <italic>c</italic> with the prototype of the corresponding category (see text).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004439.g007"/>
</fig>
</sec>
<sec id="sec006">
<title>Statistical properties of the connectivity matrix</title>
<p>We next investigated the statistical properties of the connectivity matrix after the learning process. Previous studies have shown that the distribution of synaptic weights in perceptrons with excitatory synapses becomes at maximal capacity a delta function at zero weight, plus a truncated Gaussian for strictly positive weights [<xref ref-type="bibr" rid="pcbi.1004439.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1004439.ref028">28</xref>–<xref ref-type="bibr" rid="pcbi.1004439.ref030">30</xref>]. Our model differs from this setting because of the global inhibitory feedback. Despite this difference, the distribution of weights in our network bear similarities with the results obtained in these previous studies: the distribution exhibits a peak at zero weight (‘silent’, or ‘potential’ synapses), while the distribution of strictly positive weights resembles a truncated Gaussian. Finally, the fraction of silent synapses increases with the robustness parameter (see <xref ref-type="fig" rid="pcbi.1004439.g008">Fig 8</xref>).</p>
<fig id="pcbi.1004439.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004439.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Synaptic weight distributions.</title>
<p>Comparing the distributions of the synaptic weights at critical capacity for three different values of robustness obtained from simulation. The distribution of weights approaches a Dirac-delta distribution at zero plus a truncated Gaussian. As the patterns become more robust, the center of the partial Gaussian shifts towards the left, and the number of silent synapses increases.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004439.g008"/>
</fig>
<p>We have also computed the degree of symmetry of the weight matrix. The symmetry degree is computed as the Pearson correlation coefficient between the reciprocal weights in pairs of neurons. We observe a general trend towards an increasingly symmetric weight matrix as more patterns are stored, for all values of the robustness parameter <italic>ϵ</italic> (see <xref ref-type="fig" rid="pcbi.1004439.g009">Fig 9</xref>).</p>
<fig id="pcbi.1004439.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004439.g009</object-id>
<label>Fig 9</label>
<caption>
<title>The degree of symmetry of the weight matrix.</title>
<p>The Pearson correlation coefficient between <italic>w</italic><sub><italic>ij</italic></sub> and <italic>w</italic><sub><italic>ji</italic></sub> is computed at different values of <italic>α</italic> for three values of <italic>ϵ</italic>. As <italic>α</italic> increases the weight matrix tends to be more symmetric, but gets saturated for high <italic>α</italic>. For the same values of <italic>α</italic>, as the robustness increases, the correlation also increases, so the weight matrix becomes more symmetric. Error bars (across 10 runs) are smaller than the symbols.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004439.g009"/>
</fig>
</sec>
</sec>
<sec id="sec007" sec-type="conclusions">
<title>Discussion</title>
<p>We presented a biologically-plausible learning rule that is characterized by three thresholds, and is able to store memory patterns close to the maximal storage capacity in a recurrent neural networks without the need of an explicit “error signal”. We demonstrated how the learning rule can be considered a transformed version of the PLR in the limit of a strong external field. Our network implements the separation between excitatory and inhibitory neurons, with learning occurring only at excitatory-to-excitatory synapses. We simulated a recurrent network with <italic>N</italic> = 1001 binary neurons, reaching to <italic>α</italic><sub><italic>c</italic></sub> = 1.6 at zero basin size. We then used a robustness parameter <italic>ϵ</italic> to enlarge the basin size. The simulations showed that we are close to the theoretical capacity across the whole investigated range of values of <italic>ϵ</italic>. We expect that as <italic>N</italic> increases and the learning rate gets smaller, this difference would go to zero.</p>
<p>Two crucial ingredients of the 3TLR are necessary: (1) strong external inputs, (2) three learning thresholds which are set according to the statistics of inputs to the neuron. The learning rule only uses information that is local to a synapse and corresponding neurons. Like classic Hebbian learning rules, our 3TLR works in an online fashion. In addition, it can also perform as a ‘palimpsest’ [<xref ref-type="bibr" rid="pcbi.1004439.ref031">31</xref>–<xref ref-type="bibr" rid="pcbi.1004439.ref033">33</xref>]: in case the total number of patterns exceeds the maximal capacity (at a certain basin size) the network begins to forget patterns that are not being presented anymore.</p>
<sec id="sec008">
<title>Comparison with other learning rules</title>
<p>The 3TLR can be framed in the setting of the classic Bienenstock-Cooper-Munro (BCM) theory [<xref ref-type="bibr" rid="pcbi.1004439.ref034">34</xref>, <xref ref-type="bibr" rid="pcbi.1004439.ref035">35</xref>], with additional requirements to adapt it to the attractor network scenario. The original BCM theory uses firing-rate units, and prescribes that synaptic modifications should be proportional to (1) the synaptic input, and (2) a function <italic>ϕ</italic>(<italic>v</italic>) of the total input <italic>v</italic> (or, equivalently, of the total output). The function <italic>ϕ</italic>(<italic>v</italic>) is subject to two conditions: (1) <italic>ϕ</italic>(<italic>v</italic>) ≥ 0 (or ≤ 0) when <italic>v</italic> &gt; <italic>θ</italic> (or &lt; <italic>θ</italic>, respectively); (2) <italic>ϕ</italic>(0) = 0. The parameter <italic>θ</italic> is also assumed to change, but on a longer time scale (such that the changes reflect the statistics of the inputs); this (metaplastic) adaptation has the goal of avoiding the trivial situations in which all inputs elicit indistinguishable responses. This (loosely specified) framework ensures that, under reasonable conditions, the resulting units become highly selective to a subset of the inputs, and has been mainly used to model the developmental stages of primary sensory cortex. The arising selectivity is spontaneous and completely unsupervised: in absence of further specifications, the units become selective to a random subset of the inputs (e.g. depending on random initial conditions).</p>
<p>Our model is defined on simpler (binary) units; however, if we define <italic>ϕ</italic>(<italic>v</italic>) = Θ (<italic>v</italic> − <italic>θ</italic>) Θ (<italic>θ</italic><sub>1</sub> − <italic>v</italic>) − Θ (<italic>θ</italic> − <italic>v</italic>) Θ (<italic>v</italic> − <italic>θ</italic><sub>0</sub>), then <italic>ϕ</italic> behaves according to the prescriptions of the BCM theory. Furthermore, we have essentially assumed the same slow metaplastic adaptation mechanism of BCM, even though we have assigned this role explicitly to the inhibitory part of the network (see <xref ref-type="sec" rid="sec011">Materials and Methods</xref>). On the other hand, our model has additional requirements: (1) <italic>ϕ</italic>(<italic>v</italic>) = 0 when <italic>v</italic> &lt; <italic>θ</italic><sub>0</sub> or <italic>v</italic> &gt; <italic>θ</italic><sub>1</sub>, (2) plasticity occurs during presentation of external inputs, which in turn are strong enough to drive the network towards a desired state. The second requirement ensures that the network units become selective to a specific subset of the inputs, as opposed to a random subset as in the original BCM theory, and thus that they are able to collectively behave as an attractor network. The first requirement ensures that each unit operates close to critical capacity. Indeed, these additional requirements involve extra parameters with respect to the BCM theory, and we implicitly assume these parameters to also slowly adapt according to the statistics of the inputs during network formation and development.</p>
<p>A variant of the BCM theory, known as ABS rule [<xref ref-type="bibr" rid="pcbi.1004439.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1004439.ref037">37</xref>] introduced a lower threshold for LTD, analogous to our <italic>θ</italic><sub>0</sub>, motivated by experimental evidence; however, a high threshold for LTP, analogous to our <italic>θ</italic><sub>1</sub>, was not used there, or—to our knowledge—in any other BCM variant. The idea of stopping plasticity above some value of the ‘local field’ has been introduced previously to stabilize the learning process in feed-forward networks with discrete synapses [<xref ref-type="bibr" rid="pcbi.1004439.ref038">38</xref>–<xref ref-type="bibr" rid="pcbi.1004439.ref040">40</xref>]. Our study goes beyond these previous works in generalizing such a high threshold to recurrent networks, and showing that the resulting networks achieve close to maximal capacity.</p>
</sec>
<sec id="sec009">
<title>Comparison with data and experimental predictions</title>
<p>In vitro experiments have characterized how synaptic plasticity depends on voltage [<xref ref-type="bibr" rid="pcbi.1004439.ref041">41</xref>] and firing rate [<xref ref-type="bibr" rid="pcbi.1004439.ref042">42</xref>], both variables that are expected to have a monotonic relationship with the total excitatory synaptic inputs received by a neuron. In both cases, a low value of the controlling variable leads to no changes; intermediate values lead to depression; and high values to potentiation. These three regimes are consistent with the three regions for <italic>v</italic> &lt; <italic>θ</italic><sub>1</sub> in <xref ref-type="fig" rid="pcbi.1004439.g002">Fig 2</xref>. The 3TLR predicts that a fourth region should occur at sufficiently high values of the voltage and/or firing rates. Most of the studies investigating the dependence of plasticity on firing rate or voltage have not reported a decrease in plasticity at high values of the controlling variables, but these studies might have not increased sufficiently such variables. To our knowledge, a single study has found that at high rates, the plasticity vs rate curve is a decreasing function of the input rate [<xref ref-type="bibr" rid="pcbi.1004439.ref043">43</xref>].</p>
<p>Another test of the model consists in comparing the statistics of the synaptic connectivity with experimental data. As it has been argued in several recent studies [<xref ref-type="bibr" rid="pcbi.1004439.ref025">25</xref>, <xref ref-type="bibr" rid="pcbi.1004439.ref028">28</xref>, <xref ref-type="bibr" rid="pcbi.1004439.ref030">30</xref>, <xref ref-type="bibr" rid="pcbi.1004439.ref044">44</xref>, <xref ref-type="bibr" rid="pcbi.1004439.ref045">45</xref>], networks with plastic excitatory synapses are generically sparse close to maximal capacity, with a connection probability that decreases with the robustness of information storage, consistent with short range cortical connectivity [<xref ref-type="bibr" rid="pcbi.1004439.ref046">46</xref>, <xref ref-type="bibr" rid="pcbi.1004439.ref047">47</xref>]. Our network is no exception, though the fraction of silent synapses that we observe is significantly lower than in models that lack inhibition. Furthermore, network that are close to maximal capacity tends to have a connectivity matrix that has a significant degree of symmetry, as illustrated by the over-representation of bidirectionally connected pairs of neurons, and the tendency of bidirectionally connected pairs to form stronger synapses than unidirectionally connected pairs as observed in cortex [<xref ref-type="bibr" rid="pcbi.1004439.ref047">47</xref>, <xref ref-type="bibr" rid="pcbi.1004439.ref048">48</xref>], except in barrel cortex [<xref ref-type="bibr" rid="pcbi.1004439.ref049">49</xref>]. Again, the 3TLR we have proposed here reproduces this feature (<xref ref-type="fig" rid="pcbi.1004439.g009">Fig 9</xref>), consistent with the fact that the rule approaches the optimal capacity.</p>
</sec>
<sec id="sec010">
<title>Future directions</title>
<p>Our network uses the simplest possible single neuron model [<xref ref-type="bibr" rid="pcbi.1004439.ref050">50</xref>]. One obvious direction for future work would be to implement the learning rule in a network of more realistic neuron models such as firing rate models or spiking neuron models. Another potential direction would be to understand the biophysical mechanisms leading to the high threshold in the 3TLR. In any case, we believe the results discussed here provide a significant step in the quest for understanding how learning rules in cortical networks can optimize information storage capacity.</p>
</sec>
</sec>
<sec id="sec011" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="sec012">
<title>Simulation</title>
<p>The main equations of the network, the neuron model, the learning rule, and the criteria for stopping the learning algorithm are outlined in the Results section, Eqs <xref ref-type="disp-formula" rid="pcbi.1004439.e001">1</xref>–<xref ref-type="disp-formula" rid="pcbi.1004439.e022">7</xref>. We present here additional details about network simulations.</p>
<sec id="sec013">
<title>Network setup before learning process</title>
<p>Before applying the learning rule, we required the network to have stable dynamics around a desired activity level <italic>f</italic>. A network with only excitatory neurons is highly unstable and typically converges towards the trivial all-off and all-on states; therefore, we implemented a global inhibition such that the network operates around activity level <italic>f</italic>. The basal inhibitory term (<italic>H</italic><sub>0</sub>) and the inhibitory reaction term (<italic>H</italic><sub>1</sub>) are defined as:
<disp-formula id="pcbi.1004439.e046"><alternatives><graphic id="pcbi.1004439.e046g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e046"/><mml:math id="M46" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>H</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>f</mml:mi> <mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>-</mml:mo> <mml:mi>ψ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msup><mml:mi>H</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msqrt><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>f</mml:mi></mml:mrow></mml:msqrt> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula> <disp-formula id="pcbi.1004439.e047"><alternatives><graphic id="pcbi.1004439.e047g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e047"/><mml:math id="M47" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>H</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>f</mml:mi> <mml:mi>γ</mml:mi> <mml:msqrt><mml:mrow><mml:mi>N</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msqrt></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
where <inline-formula id="pcbi.1004439.e048"><alternatives><graphic id="pcbi.1004439.e048g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e048"/><mml:math id="M48" display="inline" overflow="scroll"><mml:mrow><mml:mi>H</mml:mi> <mml:mrow><mml:mo stretchy="true">(</mml:mo> <mml:mi>x</mml:mi> <mml:mo stretchy="true">)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mtext mathvariant="normal">erfc</mml:mtext> <mml:mrow><mml:mo stretchy="true">(</mml:mo> <mml:mfrac><mml:mi>x</mml:mi> <mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mfrac> <mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and <italic>H</italic><sup>−1</sup> is the inverse of <italic>H</italic>, <italic>ψ</italic> is defined as <italic>θ</italic> = (<italic>N</italic> − 1)<italic>ψ</italic>; <inline-formula id="pcbi.1004439.e049"><alternatives><graphic id="pcbi.1004439.e049g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e049"/><mml:math id="M49" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mi>w</mml:mi> <mml:mo accent="true">‾</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> and <italic>σ</italic><sub><italic>w</italic></sub> are the mean and standard deviation of the synaptic weights, respectively. With these definitions the network dynamics is stable in the sense that the activity level converges to <italic>f</italic> very fast, regardless of the initial condition.</p>
<p>In <xref ref-type="disp-formula" rid="pcbi.1004439.e046">Eq 11</xref>, we see that <italic>H</italic><sub>0</sub> depends on the activity level <italic>f</italic> and on the standard deviation of the weights <italic>σ</italic><sub><italic>w</italic></sub>. In the dense regime, <italic>f</italic> = 0.5, we have <italic>H</italic><sup>−1</sup>(0.5) = 0, therefore the rightmost term of <xref ref-type="disp-formula" rid="pcbi.1004439.e046">Eq 11</xref> vanishes, which means that in this regime <italic>H</italic><sub>0</sub> is independent of <italic>σ</italic><sub><italic>w</italic></sub>. However, in sparser regimes, the network must be endowed with a mechanism to adjust for the changes in standard deviation, otherwise the learning process would bring the network out of the stable state, changing the basal activity level. In contrast, the mean synaptic efficacy <inline-formula id="pcbi.1004439.e050"><alternatives><graphic id="pcbi.1004439.e050g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e050"/><mml:math id="M50" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mi>w</mml:mi> <mml:mo accent="true">‾</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> does not change significantly during the learning process.</p>
<p>In all our simulations, the initial values for {<italic>w</italic><sub><italic>ij</italic></sub>} were sampled from a Gaussian distribution with mean and standard deviation equal to one, after which negative values were set to zero. This has the effect the <inline-formula id="pcbi.1004439.e051"><alternatives><graphic id="pcbi.1004439.e051g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e051"/><mml:math id="M51" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mover><mml:mi>w</mml:mi> <mml:mo accent="true">‾</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mtext mathvariant="normal">init</mml:mtext></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula> is slightly higher than one. We also set <italic>w</italic><sub><italic>ii</italic></sub> = 0 for all <italic>i</italic>.</p>
<p><xref ref-type="table" rid="pcbi.1004439.t001">Table 1</xref> shows the values of the parameters used in the simulations, in the dense and sparse regimes.</p>
</sec>
<sec id="sec014">
<title>Direct comparison between the 3TLR and the PLR</title>
<p>In order to determine the degree to which the 3TLR is able to mimic the PRL, and the effect of deviations from the latter rule, we tested both rules on the same tasks. In these simulations, every part of the simulation code was kept identical—including the pseudo-random numbers used to choose the initial state and the arbitrary permutations for the update order of the units—except for the learning rule. We tested the network in the dense case <italic>f</italic> = 0.5, at <italic>ϵ</italic> = 3, varying the storage load <italic>α</italic>, using 10 samples for each point. We compared the probability of solving the learning task and the distribution of the discrepancies (absolute value of the differences) in the values of the resulting synaptic weights. We tested two values of the parameter <italic>γ</italic>, 6 (as in <xref ref-type="fig" rid="pcbi.1004439.g004">Fig 4</xref>) and 12. We found that at <italic>γ</italic> = 12 there was absolutely no difference between the two rules, while at <italic>γ</italic> = 6 the 3TLR performed slightly worse, and significant deviations from the PLR started to appear close to the maximal capacity of the 3TLR (see <xref ref-type="fig" rid="pcbi.1004439.g010">Fig 10</xref>).</p>
<fig id="pcbi.1004439.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004439.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Direct comparions of the 3TLR and the PLR.</title>
<p>Success probability for the 3TLR at <italic>γ</italic> = 6 (blue curve, left axis) and the PLR (red curve) at f=0.5 and ϵ=3; the results for the 3TLR at <italic>γ</italic> = 12 are identical to those of the PLR (red curve). The orange points show the absolute difference of weights between the final values of the weights for the PLR at <italic>γ</italic> = 6 and the PLR (right axis): the points show the median of the distribution, while the error bars span the 5th-95th percentiles, showing that, while the distribution is concentrated at near-zero values, outliers appear at the critical capacity of the 3TLR algorithm. (Note that the average value of the weights is in all cases approximately 1.08; also compare the discrepancies with the overall distribution of the weights, <xref ref-type="fig" rid="pcbi.1004439.g008">Fig 8</xref>).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004439.g010"/>
</fig>
</sec>
</sec>
<sec id="sec015">
<title>Analytical calculation of the storage capacity at infinite <italic>N</italic></title>
<sec id="sec016">
<title>Entropy calculation</title>
<p>In this section, we present the details of the calculations for the typical storage capacity of our network in the limit of <italic>N</italic> → ∞, using the Gardner analysis [<xref ref-type="bibr" rid="pcbi.1004439.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1004439.ref028">28</xref>].</p>
<p>The capacity is defined as the maximum value of <italic>α</italic> = <italic>p</italic>/<italic>N</italic> such that a solution to <xref ref-type="disp-formula" rid="pcbi.1004439.e022">Eq 7</xref> can typically be found.</p>
<p>We can rewrite <xref ref-type="disp-formula" rid="pcbi.1004439.e022">Eq 7</xref> as
<disp-formula id="pcbi.1004439.e052"><alternatives><graphic id="pcbi.1004439.e052g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e052"/><mml:math id="M52" display="block" overflow="scroll"><mml:mrow><mml:mo>∀</mml:mo> <mml:mi>i</mml:mi> <mml:mo>:</mml:mo> <mml:mspace width="1.em"/><mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>μ</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mi>α</mml:mi> <mml:mi>N</mml:mi></mml:mrow></mml:munderover> <mml:mo>Θ</mml:mo> <mml:mo stretchy="true">(</mml:mo> <mml:mo stretchy="false">(</mml:mo> <mml:mn>2</mml:mn> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="true">(</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>-</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:mi>λ</mml:mi> <mml:mo stretchy="true">(</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>-</mml:mo> <mml:mi>f</mml:mi> <mml:mi>N</mml:mi> <mml:mo stretchy="true">)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>θ</mml:mi> <mml:mo stretchy="true">)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>f</mml:mi> <mml:mi>ϵ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mo stretchy="true">)</mml:mo> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives> <label>(13)</label></disp-formula>
where
<disp-formula id="pcbi.1004439.e053"><alternatives><graphic id="pcbi.1004439.e053g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e053"/><mml:math id="M53" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msub><mml:mi>H</mml:mi> <mml:mn>0</mml:mn></mml:msub></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>N</mml:mi> <mml:mi>f</mml:mi> <mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>-</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>+</mml:mo> <mml:msup><mml:mi>H</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>(</mml:mo> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>w</mml:mi></mml:msub> <mml:msqrt><mml:mrow><mml:mi>f</mml:mi> <mml:mi>N</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula> <disp-formula id="pcbi.1004439.e054"><alternatives><graphic id="pcbi.1004439.e054g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e054"/><mml:math id="M54" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>λ</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula></p>
<p>
<xref ref-type="disp-formula" rid="pcbi.1004439.e052">Eq 13</xref> becomes:
<disp-formula id="pcbi.1004439.e055"><alternatives><graphic id="pcbi.1004439.e055g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e055"/><mml:math id="M55" display="block" overflow="scroll"><mml:mrow><mml:mo>∀</mml:mo> <mml:mi>i</mml:mi> <mml:mo>:</mml:mo> <mml:mspace width="1.em"/><mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>μ</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mi>α</mml:mi> <mml:mi>N</mml:mi></mml:mrow></mml:munderover> <mml:mo>Θ</mml:mo> <mml:mo stretchy="true">(</mml:mo> <mml:mo stretchy="false">(</mml:mo> <mml:mn>2</mml:mn> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="true">(</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:mo stretchy="false">(</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo stretchy="false">)</mml:mo> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>i</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>-</mml:mo> <mml:msup><mml:mi>H</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo stretchy="false">(</mml:mo> <mml:mi>f</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:msub><mml:mi>σ</mml:mi> <mml:mi>w</mml:mi></mml:msub> <mml:msqrt><mml:mrow><mml:mi>f</mml:mi> <mml:mi>N</mml:mi></mml:mrow></mml:msqrt> <mml:mo stretchy="true">)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>f</mml:mi> <mml:mi>ϵ</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mo stretchy="true">)</mml:mo> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives> <label>(16)</label></disp-formula></p>
<p>Let us now consider a single unit <italic>i</italic>. We write <inline-formula id="pcbi.1004439.e056"><alternatives><graphic id="pcbi.1004439.e056g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e056"/><mml:math id="M56" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>=</mml:mo> <mml:mrow><mml:mo stretchy="true">(</mml:mo> <mml:mn>2</mml:mn> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mi>i</mml:mi> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn> <mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, and re-parametrize the weights as <inline-formula id="pcbi.1004439.e057"><alternatives><graphic id="pcbi.1004439.e057g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e057"/><mml:math id="M57" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>W</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mover><mml:mi>w</mml:mi> <mml:mo accent="true">‾</mml:mo></mml:mover></mml:mfrac> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn> <mml:mo>∈</mml:mo> <mml:mrow><mml:mo stretchy="true">[</mml:mo> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mo>∞</mml:mo> <mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, and also define
<disp-formula id="pcbi.1004439.e058"><alternatives><graphic id="pcbi.1004439.e058g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e058"/><mml:math id="M58" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>T</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi>H</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>(</mml:mo> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo> <mml:msqrt><mml:mi>f</mml:mi></mml:msqrt></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula> <disp-formula id="pcbi.1004439.e059"><alternatives><graphic id="pcbi.1004439.e059g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e059"/><mml:math id="M59" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>K</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mi>ϵ</mml:mi> <mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(18)</label></disp-formula>
Dropping the index <italic>i</italic> and neglecting terms of order 1, we obtain:
<disp-formula id="pcbi.1004439.e060"><alternatives><graphic id="pcbi.1004439.e060g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e060"/><mml:math id="M60" display="block" overflow="scroll"><mml:mrow><mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>μ</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mi>α</mml:mi> <mml:mi>N</mml:mi></mml:mrow></mml:munderover> <mml:mo>Θ</mml:mo> <mml:mo>(</mml:mo> <mml:msup><mml:mi>σ</mml:mi> <mml:mi>μ</mml:mi></mml:msup> <mml:mo>(</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>N</mml:mi></mml:munderover> <mml:msub><mml:mi>W</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:msubsup><mml:mi>ξ</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>μ</mml:mi></mml:msubsup> <mml:mo>-</mml:mo> <mml:mi>T</mml:mi> <mml:mfrac><mml:msub><mml:mi>σ</mml:mi> <mml:mi>w</mml:mi></mml:msub> <mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:mfrac> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>f</mml:mi> <mml:mi>K</mml:mi> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives> <label>(19)</label></disp-formula></p>
<p>Our goal is to compute the quenched entropy of this problem, i.e. the scaled average of the logarithm of the volume of <italic>W</italic> which satisfies the above equation:
<disp-formula id="pcbi.1004439.e061"><alternatives><graphic id="pcbi.1004439.e061g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e061"/><mml:math id="M61" display="block" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mi>S</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo> <mml:mrow><mml:mtext>log</mml:mtext><mml:mi>V</mml:mi></mml:mrow> <mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:msup><mml:mi>ξ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msub><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo> <mml:mrow><mml:mtext>log</mml:mtext><mml:mstyle displaystyle="true"><mml:mrow><mml:mo>∫</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>Θ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:mstyle><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo>Θ</mml:mo></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:msubsup><mml:mi>ξ</mml:mi><mml:mi>j</mml:mi><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mfrac><mml:mi>T</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mi>K</mml:mi><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>⟩</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>{</mml:mo> <mml:mrow><mml:msup><mml:mi>ξ</mml:mi><mml:mi>μ</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mi>μ</mml:mi></mml:msup></mml:mrow> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(20)</label></disp-formula></p>
<p>The computation proceeds along the lines of [<xref ref-type="bibr" rid="pcbi.1004439.ref020">20</xref>, <xref ref-type="bibr" rid="pcbi.1004439.ref028">28</xref>], by using the so-called replica trick to perform the average of the logarithm of <italic>V</italic>, exploiting the identity:
<disp-formula id="pcbi.1004439.e062"><alternatives><graphic id="pcbi.1004439.e062g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e062"/><mml:math id="M62" display="block" overflow="scroll"><mml:mrow><mml:mo>⟨</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mi>V</mml:mi> <mml:mo>⟩</mml:mo> <mml:mo>=</mml:mo> <mml:munder><mml:mo movablelimits="true" form="prefix">lim</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>→</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:munder> <mml:mfrac><mml:mrow><mml:mo>⟨</mml:mo> <mml:msup><mml:mi>V</mml:mi> <mml:mi>n</mml:mi></mml:msup> <mml:mo>⟩</mml:mo> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives> <label>(21)</label></disp-formula>
performing the computation for integer values of <italic>n</italic> and using an analytical continuation to perform the limit <italic>n</italic> → 0. We perform the calculation using the replica-symmetric (RS) Ansatz, which is believed to give exact results in the case of perceptron models with continuous weights. The final expression for the entropy depends on six order parameters; the first three are <italic>Q</italic>, <italic>q</italic> and <italic>M</italic>, whose meaning is
<disp-formula id="pcbi.1004439.e063"><alternatives><graphic id="pcbi.1004439.e063g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e063"/><mml:math id="M63" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>Q</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>N</mml:mi></mml:mfrac> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:munder> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>W</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula> <disp-formula id="pcbi.1004439.e064"><alternatives><graphic id="pcbi.1004439.e064g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e064"/><mml:math id="M64" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>q</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>N</mml:mi></mml:mfrac> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:munder> <mml:msubsup><mml:mi>W</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>a</mml:mi></mml:msubsup> <mml:msubsup><mml:mi>W</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mi>b</mml:mi></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula> <disp-formula id="pcbi.1004439.e065"><alternatives><graphic id="pcbi.1004439.e065g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e065"/><mml:math id="M65" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>M</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mfrac> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>j</mml:mi></mml:munder> <mml:msub><mml:mi>W</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where we used <italic>W</italic><sup><italic>a</italic></sup> and <italic>W</italic><sup><italic>b</italic></sup> to denote two different <italic>replicas</italic> of the system, which can simply be interpreted as two independent solutions to the constraint equation. <italic>Q</italic> is called the self-overlap, and is equal to <inline-formula id="pcbi.1004439.e066"><alternatives><graphic id="pcbi.1004439.e066g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e066"/><mml:math id="M66" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo> <mml:mfrac><mml:msub><mml:mi>σ</mml:mi> <mml:mi>w</mml:mi></mml:msub> <mml:mover><mml:mi>w</mml:mi> <mml:mo accent="true">‾</mml:mo></mml:mover></mml:mfrac> <mml:mo stretchy="true">)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> in our case, while <italic>q</italic> is the mutual-overlap. The remaining order parameters are the conjugate quantities <inline-formula id="pcbi.1004439.e067"><alternatives><graphic id="pcbi.1004439.e067g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e067"/><mml:math id="M67" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mi>Q</mml:mi> <mml:mo accent="true">^</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pcbi.1004439.e068"><alternatives><graphic id="pcbi.1004439.e068g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e068"/><mml:math id="M68" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mi>q</mml:mi> <mml:mo accent="true">^</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pcbi.1004439.e069"><alternatives><graphic id="pcbi.1004439.e069g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e069"/><mml:math id="M69" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mi>M</mml:mi> <mml:mo accent="true">^</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>. The entropy expression is:
<disp-formula id="pcbi.1004439.e070"><alternatives><graphic id="pcbi.1004439.e070g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e070"/><mml:math id="M70" display="block" overflow="scroll"><mml:mrow><mml:mi>S</mml:mi> <mml:mo>(</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi> <mml:mo>,</mml:mo> <mml:mi>M</mml:mi> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>M</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mo>-</mml:mo> <mml:mo>(</mml:mo> <mml:mi>Q</mml:mi> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mi>q</mml:mi> <mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mi>α</mml:mi> <mml:msub><mml:mi mathvariant="script">Z</mml:mi> <mml:mi>A</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi> <mml:mo>,</mml:mo> <mml:mi>M</mml:mi> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="script">Z</mml:mi> <mml:mi>W</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>,</mml:mo> <mml:mover accent="true"><mml:mi>M</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives> <label>(22)</label></disp-formula>
where
<disp-formula id="pcbi.1004439.e071"><alternatives><graphic id="pcbi.1004439.e071g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e071"/><mml:math id="M71" display="block" overflow="scroll"><mml:msub><mml:mi mathvariant="script">Z</mml:mi><mml:mi>A</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mstyle displaystyle="true"><mml:mrow><mml:mo>∫</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mstyle><mml:mi>u</mml:mi><mml:msub><mml:mrow><mml:mo>⟨</mml:mo> <mml:mrow><mml:mtext>ln</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mi>T</mml:mi><mml:msqrt><mml:mi>Q</mml:mi></mml:msqrt></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msqrt><mml:mi>q</mml:mi></mml:msqrt></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msqrt><mml:mrow><mml:mi>Q</mml:mi><mml:mo>−</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>⟩</mml:mo></mml:mrow><mml:mi>σ</mml:mi></mml:msub></mml:math></alternatives> <label>(23)</label></disp-formula> <disp-formula id="pcbi.1004439.e072"><alternatives><graphic id="pcbi.1004439.e072g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e072"/><mml:math id="M72" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="script">Z</mml:mi><mml:mi>W</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>Q</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msup><mml:mrow><mml:msup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mtext>​</mml:mtext></mml:msup></mml:mrow><mml:mtext>​</mml:mtext></mml:msup><mml:mi>D</mml:mi><mml:mi>u</mml:mi><mml:mtext> </mml:mtext><mml:mi>ln</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:munderover><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>W</mml:mi><mml:mtext> </mml:mtext><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mover accent="true"><mml:mi>Q</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>W</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:msqrt><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:msqrt><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>M</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(24)</label></disp-formula>
We used the usual notation <inline-formula id="pcbi.1004439.e073"><alternatives><graphic id="pcbi.1004439.e073g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e073"/><mml:math id="M73" display="inline" overflow="scroll"><mml:mrow><mml:mi>D</mml:mi> <mml:mi>u</mml:mi> <mml:mo>≡</mml:mo> <mml:mi>d</mml:mi> <mml:mi>u</mml:mi> <mml:mspace width="0.167em"/><mml:mfrac><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>−</mml:mo> <mml:mfrac><mml:msup><mml:mi>u</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi></mml:mrow></mml:msqrt></mml:mfrac> <mml:mo>=</mml:mo> <mml:mi>d</mml:mi> <mml:mi>u</mml:mi> <mml:mspace width="0.167em"/><mml:mi>G</mml:mi> <mml:mrow><mml:mo stretchy="true">(</mml:mo> <mml:mi>u</mml:mi> <mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> to denote Gaussian integrals, and defined <inline-formula id="pcbi.1004439.e074"><alternatives><graphic id="pcbi.1004439.e074g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e074"/><mml:math id="M74" display="inline" overflow="scroll"><mml:mrow><mml:mi>H</mml:mi> <mml:mrow><mml:mo stretchy="true">(</mml:mo> <mml:mi>x</mml:mi> <mml:mo stretchy="true">)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∫</mml:mo> <mml:mi>x</mml:mi> <mml:mo>∞</mml:mo></mml:msubsup> <mml:mi>D</mml:mi> <mml:mi>u</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mtext mathvariant="normal">erfc</mml:mtext> <mml:mrow><mml:mo stretchy="true">(</mml:mo> <mml:mfrac><mml:mi>x</mml:mi> <mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mfrac> <mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. In the following, we will also use the shorthand <inline-formula id="pcbi.1004439.e075"><alternatives><graphic id="pcbi.1004439.e075g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e075"/><mml:math id="M75" display="inline" overflow="scroll"><mml:mo mathvariant="italic">𝒢</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>G</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mi>H</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>. We also used the notation ⟨ ⋅ ⟩<sub><italic>σ</italic></sub> to denote the average over the output <italic>σ</italic>, i.e. ⟨<italic>φ</italic>(<italic>σ</italic>)⟩<sub><italic>σ</italic></sub> = <italic>fφ</italic>(1) + (1 − <italic>f</italic>) <italic>φ</italic> (−1) for any function <italic>φ</italic>. The value of the order parameters is found by extremizing <italic>S</italic>. The notation and the following computations can be simplified using:
<disp-formula id="pcbi.1004439.e076"><alternatives><graphic id="pcbi.1004439.e076g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e076"/><mml:math id="M76" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mi>Q</mml:mi></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>Q</mml:mi> <mml:mo>-</mml:mo> <mml:mi>q</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(25)</label></disp-formula> <disp-formula id="pcbi.1004439.e077"><alternatives><graphic id="pcbi.1004439.e077g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e077"/><mml:math id="M77" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>t</mml:mi> <mml:mi>σ</mml:mi></mml:msub> <mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mfrac><mml:mrow><mml:mi>K</mml:mi> <mml:mo>-</mml:mo> <mml:mi>σ</mml:mi> <mml:mo>(</mml:mo> <mml:mi>M</mml:mi> <mml:mo>-</mml:mo> <mml:mi>T</mml:mi> <mml:msqrt><mml:mi>Q</mml:mi></mml:msqrt> <mml:mo>)</mml:mo> <mml:mo>+</mml:mo> <mml:mi>u</mml:mi> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo> <mml:msqrt><mml:mi>q</mml:mi></mml:msqrt></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo> <mml:msqrt><mml:mrow><mml:mo>Δ</mml:mo> <mml:mi>Q</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(26)</label></disp-formula> <disp-formula id="pcbi.1004439.e078"><alternatives><graphic id="pcbi.1004439.e078g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e078"/><mml:math id="M78" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>-</mml:mo> <mml:mn>2</mml:mn> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(27)</label></disp-formula> <disp-formula id="pcbi.1004439.e079"><alternatives><graphic id="pcbi.1004439.e079g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e079"/><mml:math id="M79" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>ν</mml:mi> <mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>,</mml:mo> <mml:mi>W</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>Δ</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:msup><mml:mi>W</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mi>W</mml:mi> <mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:msqrt><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:msqrt> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>M</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(28)</label></disp-formula></p>
<p>The extremization of <italic>S</italic> then results in the system of equations:
<disp-formula id="pcbi.1004439.e080"><alternatives><graphic id="pcbi.1004439.e080g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e080"/><mml:math id="M80" display="block" overflow="scroll"><mml:mo>Δ</mml:mo><mml:mover accent="true"><mml:mi>Q</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mi>α</mml:mi><mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>−</mml:mo><mml:mo>Δ</mml:mo><mml:mi>Q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>Δ</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:mrow><mml:mo>∫</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mstyle><mml:mi>u</mml:mi><mml:mspace width="1pt"/><mml:mi>u</mml:mi><mml:msub><mml:mrow><mml:mo>〈</mml:mo> <mml:mrow><mml:mi mathvariant="script">G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>σ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>〉</mml:mo></mml:mrow><mml:mi>σ</mml:mi></mml:msub></mml:math></alternatives> <label>(29)</label></disp-formula> <disp-formula id="pcbi.1004439.e081"><alternatives><graphic id="pcbi.1004439.e081g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e081"/><mml:math id="M81" display="block" overflow="scroll"><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mi>α</mml:mi><mml:mrow><mml:mo>Δ</mml:mo><mml:mi>Q</mml:mi></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:mrow><mml:mo>∫</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mstyle><mml:mi>u</mml:mi><mml:msub><mml:mrow><mml:mo>〈</mml:mo> <mml:mrow><mml:mi mathvariant="script">G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>σ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>σ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>〉</mml:mo></mml:mrow><mml:mi>σ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo>Δ</mml:mo><mml:mover accent="true"><mml:mi>Q</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></alternatives> <label>(30)</label></disp-formula> <disp-formula id="pcbi.1004439.e082"><alternatives><graphic id="pcbi.1004439.e082g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e082"/><mml:math id="M82" display="block" overflow="scroll"><mml:mn>0</mml:mn><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mstyle displaystyle="true"><mml:mrow><mml:mo>∫</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mstyle><mml:mi>u</mml:mi><mml:msub><mml:mrow><mml:mo>〈</mml:mo> <mml:mrow><mml:mi mathvariant="script">G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>σ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>σ</mml:mi></mml:mrow> <mml:mo>〉</mml:mo></mml:mrow><mml:mi>σ</mml:mi></mml:msub></mml:math></alternatives> <label>(31)</label></disp-formula> <disp-formula id="pcbi.1004439.e083"><alternatives><graphic id="pcbi.1004439.e083g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e083"/><mml:math id="M83" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>Q</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∫</mml:mo> <mml:mi>D</mml:mi> <mml:mi>u</mml:mi> <mml:mspace width="0.222222em"/><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>W</mml:mi> <mml:mspace width="0.166667em"/><mml:msup><mml:mi>W</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi>ν</mml:mi> <mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>,</mml:mo> <mml:mi>W</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>W</mml:mi> <mml:mi>ν</mml:mi> <mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>,</mml:mo> <mml:mi>W</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(32)</label></disp-formula> <disp-formula id="pcbi.1004439.e084"><alternatives><graphic id="pcbi.1004439.e084g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e084"/><mml:math id="M84" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mi>Q</mml:mi></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:msqrt><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:msqrt></mml:mfrac> <mml:mo>∫</mml:mo> <mml:mi>D</mml:mi> <mml:mi>u</mml:mi> <mml:mspace width="0.222222em"/><mml:mi>u</mml:mi> <mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>W</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>W</mml:mi> <mml:mi>ν</mml:mi> <mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>,</mml:mo> <mml:mi>W</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>W</mml:mi> <mml:mi>ν</mml:mi> <mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>,</mml:mo> <mml:mi>W</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(33)</label></disp-formula> <disp-formula id="pcbi.1004439.e085"><alternatives><graphic id="pcbi.1004439.e085g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e085"/><mml:math id="M85" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>∫</mml:mo> <mml:mi>D</mml:mi> <mml:mi>u</mml:mi> <mml:mspace width="0.222222em"/><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>W</mml:mi> <mml:mspace width="0.166667em"/><mml:mi>W</mml:mi> <mml:mi>ν</mml:mi> <mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>,</mml:mo> <mml:mi>W</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:msubsup><mml:mo>∫</mml:mo> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>∞</mml:mi></mml:msubsup> <mml:mi>d</mml:mi> <mml:mi>W</mml:mi> <mml:mi>ν</mml:mi> <mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:mo>,</mml:mo> <mml:mi>W</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(34)</label></disp-formula></p>
<p>The integrals over <italic>dW</italic> in the last three equations can be performed explicitly, yielding:
<disp-formula id="pcbi.1004439.e086"><alternatives><graphic id="pcbi.1004439.e086g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e086"/><mml:math id="M86" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>Q</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>+</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>M</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mo>Δ</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow> <mml:mrow><mml:mo>Δ</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mo>Δ</mml:mo> <mml:msup><mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mfrac><mml:mn>3</mml:mn> <mml:mn>2</mml:mn></mml:mfrac></mml:msup></mml:mrow></mml:mfrac> <mml:mo>∫</mml:mo> <mml:mi>D</mml:mi> <mml:mi>u</mml:mi> <mml:mspace width="0.222222em"/><mml:mo>(</mml:mo> <mml:mi>u</mml:mi> <mml:msqrt><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:msqrt> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>M</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>-</mml:mo> <mml:mo>Δ</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>)</mml:mo> <mml:mi>𝒢</mml:mi> <mml:mspace width="1pt"/><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mi>u</mml:mi> <mml:msqrt><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:msqrt> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>M</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>+</mml:mo> <mml:mo>Δ</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow> <mml:msqrt><mml:mrow><mml:mo>Δ</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:msqrt></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(35)</label></disp-formula> <disp-formula id="pcbi.1004439.e087"><alternatives><graphic id="pcbi.1004439.e087g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e087"/><mml:math id="M87" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mi>Q</mml:mi></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mo>Δ</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msqrt><mml:mrow><mml:mo>Δ</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:msqrt></mml:mfrac> <mml:mo>∫</mml:mo> <mml:mi>D</mml:mi> <mml:mi>u</mml:mi> <mml:mspace width="0.222222em"/><mml:mi>u</mml:mi> <mml:mi mathvariant="script">G</mml:mi> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mi>u</mml:mi> <mml:msqrt><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:msqrt> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>M</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>+</mml:mo> <mml:mo>Δ</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow> <mml:msqrt><mml:mrow><mml:mo>Δ</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:msqrt></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(36)</label></disp-formula> <disp-formula id="pcbi.1004439.e088"><alternatives><graphic id="pcbi.1004439.e088g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e088"/><mml:math id="M88" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>-</mml:mo> <mml:mfrac><mml:mover accent="true"><mml:mi>M</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mo>Δ</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msqrt><mml:mrow><mml:mo>Δ</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:msqrt></mml:mfrac> <mml:mo>∫</mml:mo> <mml:mi>D</mml:mi> <mml:mi>u</mml:mi> <mml:mspace width="0.222222em"/><mml:mi mathvariant="script">G</mml:mi> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:mi>u</mml:mi> <mml:msqrt><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:msqrt> <mml:mo>-</mml:mo> <mml:mover accent="true"><mml:mi>M</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mo>+</mml:mo> <mml:mo>Δ</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow> <mml:msqrt><mml:mrow><mml:mo>Δ</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:msqrt></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(37)</label></disp-formula></p>
</sec>
<sec id="sec017">
<title>Critical capacity</title>
<p>At critical capacity, the space of the solutions shrinks to a point, and the mutual overlap tends to become equal to the self overlap: <italic>q</italic> → <italic>Q</italic>, i.e. Δ<italic>Q</italic> → 0. In this limit, the conjugate order parameters diverge as:
<disp-formula id="pcbi.1004439.e089"><alternatives><graphic id="pcbi.1004439.e089g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e089"/><mml:math id="M89" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mover accent="true"><mml:mi>q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mfrac><mml:mi>C</mml:mi> <mml:mrow><mml:mo>Δ</mml:mo> <mml:msup><mml:mi>Q</mml:mi> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(38)</label></disp-formula> <disp-formula id="pcbi.1004439.e090"><alternatives><graphic id="pcbi.1004439.e090g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e090"/><mml:math id="M90" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mover accent="true"><mml:mi>Q</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mfrac><mml:mi>A</mml:mi> <mml:mrow><mml:mo>Δ</mml:mo> <mml:mi>Q</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(39)</label></disp-formula> <disp-formula id="pcbi.1004439.e091"><alternatives><graphic id="pcbi.1004439.e091g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e091"/><mml:math id="M91" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mover accent="true"><mml:mi>M</mml:mi> <mml:mo>^</mml:mo></mml:mover></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mfrac><mml:mrow><mml:mi>B</mml:mi> <mml:msqrt><mml:mi>C</mml:mi></mml:msqrt></mml:mrow> <mml:mrow><mml:mo>Δ</mml:mo> <mml:mi>Q</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(40)</label></disp-formula></p>
<p>Using these conditions, and calling <italic>α</italic><sub><italic>c</italic></sub> the critical value of <italic>α</italic>, the saddle point equations, <xref ref-type="disp-formula" rid="pcbi.1004439.e080">29</xref> to <xref ref-type="disp-formula" rid="pcbi.1004439.e085">34</xref>, become:
<disp-formula id="pcbi.1004439.e092"><alternatives><graphic id="pcbi.1004439.e092g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e092"/><mml:math id="M92" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>Q</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>A</mml:mi></mml:mfrac> <mml:mo>(</mml:mo> <mml:mi>C</mml:mi> <mml:mo>-</mml:mo> <mml:mi>B</mml:mi> <mml:msqrt><mml:mi>C</mml:mi></mml:msqrt> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(41)</label></disp-formula> <disp-formula id="pcbi.1004439.e093"><alternatives><graphic id="pcbi.1004439.e093g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e093"/><mml:math id="M93" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mi>A</mml:mi></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi>H</mml:mi> <mml:mo>(</mml:mo> <mml:mi>B</mml:mi> <mml:mo>-</mml:mo> <mml:mfrac><mml:mi>A</mml:mi> <mml:msqrt><mml:mi>C</mml:mi></mml:msqrt></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(42)</label></disp-formula> <disp-formula id="pcbi.1004439.e094"><alternatives><graphic id="pcbi.1004439.e094g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e094"/><mml:math id="M94" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:msqrt><mml:mi>C</mml:mi></mml:msqrt> <mml:mi>A</mml:mi></mml:mfrac> <mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo>(</mml:mo> <mml:mi>B</mml:mi> <mml:mo>-</mml:mo> <mml:mfrac><mml:mi>A</mml:mi> <mml:msqrt><mml:mi>C</mml:mi></mml:msqrt></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>B</mml:mi> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(43)</label></disp-formula> <disp-formula id="pcbi.1004439.e095"><alternatives><graphic id="pcbi.1004439.e095g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e095"/><mml:math id="M95" display="block" overflow="scroll"><mml:mi>C</mml:mi><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mi>Q</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mo>〈</mml:mo> <mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>σ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>σ</mml:mi></mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>σ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>〉</mml:mo></mml:mrow><mml:mi>σ</mml:mi></mml:msub></mml:math></alternatives> <label>(44)</label></disp-formula> <disp-formula id="pcbi.1004439.e096"><alternatives><graphic id="pcbi.1004439.e096g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e096"/><mml:math id="M96" display="block" overflow="scroll"><mml:mi>A</mml:mi><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>α</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mo>〈</mml:mo> <mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>σ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>〉</mml:mo></mml:mrow><mml:mi>σ</mml:mi></mml:msub></mml:math></alternatives> <label>(45)</label></disp-formula> <disp-formula id="pcbi.1004439.e097"><alternatives><graphic id="pcbi.1004439.e097g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e097"/><mml:math id="M97" display="block" overflow="scroll"><mml:mn>0</mml:mn><mml:mtext> </mml:mtext><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mrow><mml:mo>〈</mml:mo> <mml:mrow><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>σ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>σ</mml:mi></mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>σ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>〉</mml:mo></mml:mrow><mml:mi>σ</mml:mi></mml:msub></mml:math></alternatives> <label>(46)</label></disp-formula>
where we defined
<disp-formula id="pcbi.1004439.e098"><alternatives><graphic id="pcbi.1004439.e098g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e098"/><mml:math id="M98" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>τ</mml:mi> <mml:mi>σ</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>σ</mml:mi> <mml:mo>(</mml:mo> <mml:mi>M</mml:mi> <mml:mo>-</mml:mo> <mml:mi>T</mml:mi> <mml:msqrt><mml:mi>Q</mml:mi></mml:msqrt> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>K</mml:mi></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>f</mml:mi> <mml:mo>)</mml:mo> <mml:msqrt><mml:mi>Q</mml:mi></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives> <label>(47)</label></disp-formula></p>
<p>These equations can be solved numerically to find the six parameters <italic>α</italic><sub><italic>c</italic></sub>, <italic>Q</italic>, <italic>A</italic>, <italic>B</italic>, <italic>C</italic> and <italic>M</italic>.</p>
<p>Note that in the special case <italic>K</italic> = 0 these equations have a degenerate solution with <italic>Q</italic> = 0 and the same <italic>α</italic><sub><italic>c</italic></sub> as in the case of unbounded synaptic weights (e.g. <italic>α</italic><sub><italic>c</italic></sub> = 2 for <italic>f</italic> = 0.5). This is because in that case the original problem has the property that scaling all weights by a factor of <italic>x</italic> is equivalent to scaling the boundary <inline-formula id="pcbi.1004439.e099"><alternatives><graphic id="pcbi.1004439.e099g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004439.e099"/><mml:math id="M99" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mi>w</mml:mi> <mml:mo accent="true">‾</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula> by a factor of <italic>x</italic><sup>−1</sup> (see <xref ref-type="disp-formula" rid="pcbi.1004439.e055">Eq 16</xref>); therefore, the optimal strategy is to exploit this property by setting <italic>x</italic> → 0, i.e. effectively reducing the problem to the unbounded case. Of course, this strategy can only be pursued up to the available precision in a practical setting.</p>
</sec>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pcbi.1004439.ref001">
<label>1</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hopfield</surname> <given-names>JJ</given-names></name>. <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>1982</year>;<volume>79</volume>:<fpage>2554</fpage>–<lpage>2558</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.79.8.2554" xlink:type="simple">10.1073/pnas.79.8.2554</ext-link></comment> <object-id pub-id-type="pmid">6953413</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref002">
<label>2</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Amit</surname> <given-names>DJ</given-names></name>. <source>Modeling brain function</source>. <publisher-name>Cambridge University Press</publisher-name>; <year>1989</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004439.ref003">
<label>3</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Hertz</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Krogh</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Palmer</surname> <given-names>RG</given-names></name>. <source>Introduction to the Theory of Neural Computation</source>. <publisher-name>Addison-Wesley</publisher-name>, <publisher-loc>Redwood City</publisher-loc>; <year>1991</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004439.ref004">
<label>4</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Amit</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>. <article-title>Model of global spontaneous activity and local structured activity during delay periods in the cerebral cortex</article-title>. <source>Cerebral Cortex</source>. <year>1997</year>;<volume>7</volume>:<fpage>237</fpage>–<lpage>252</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/cercor/7.3.237" xlink:type="simple">10.1093/cercor/7.3.237</ext-link></comment> <object-id pub-id-type="pmid">9143444</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref005">
<label>5</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>XJ</given-names></name>. <article-title>Effects of neuromodulation in a cortical network model of object working memory dominated by recurrent inhibition</article-title>. <source>J Comput Neurosci</source>. <year>2001</year>;<volume>11</volume>:<fpage>63</fpage>–<lpage>85</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/A:1011204814320" xlink:type="simple">10.1023/A:1011204814320</ext-link></comment> <object-id pub-id-type="pmid">11524578</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref006">
<label>6</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mongillo</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Tsodyks</surname> <given-names>M</given-names></name>. <article-title>Synaptic Theory of Working Memory</article-title>. <source>Science</source>. <year>2008</year>;<volume>319</volume>:<fpage>1543</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1150769" xlink:type="simple">10.1126/science.1150769</ext-link></comment> <object-id pub-id-type="pmid">18339943</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref007">
<label>7</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Barak</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Tsodyks</surname> <given-names>M</given-names></name>. <article-title>Working models of working memory</article-title>. <source>Curr Opin Neurobiol</source>. <year>2014</year>;<volume>25</volume>:<fpage>20</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.conb.2013.10.008" xlink:type="simple">10.1016/j.conb.2013.10.008</ext-link></comment> <object-id pub-id-type="pmid">24709596</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref008">
<label>8</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Fuster</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Jervey</surname> <given-names>JP</given-names></name>. <article-title>Inferotemporal neurons distinguish and retain behaviourally relevant features of visual stimuli</article-title>. <source>Science</source>. <year>1981</year>;<volume>212</volume>:<fpage>952</fpage>–<lpage>955</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.7233192" xlink:type="simple">10.1126/science.7233192</ext-link></comment> <object-id pub-id-type="pmid">7233192</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref009">
<label>9</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Miyashita</surname> <given-names>Y</given-names></name>. <article-title>Neuronal correlate of visual associative long-term memory in the primate temporal cortex</article-title>. <source>Nature</source>. <year>1988</year>;<volume>335</volume>:<fpage>817</fpage>–<lpage>820</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/335817a0" xlink:type="simple">10.1038/335817a0</ext-link></comment> <object-id pub-id-type="pmid">3185711</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref010">
<label>10</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Miyashita</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Chang</surname> <given-names>HS</given-names></name>. <article-title>Neuronal correlate of pictorial short-term memory in the primate temporal cortex</article-title>. <source>Nature</source>. <year>1988</year>;<volume>331</volume>:<fpage>68</fpage>–<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/331068a0" xlink:type="simple">10.1038/331068a0</ext-link></comment> <object-id pub-id-type="pmid">3340148</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref011">
<label>11</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Nakamura</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kubota</surname> <given-names>K</given-names></name>. <article-title>Mnemonic firing of neurons in the monkey temporal pole during a visual recognition memory task</article-title>. <source>J Neurophysiol</source>. <year>1995</year>;<volume>74</volume>:<fpage>162</fpage>–<lpage>178</lpage>. <object-id pub-id-type="pmid">7472321</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref012">
<label>12</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Fuster</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Alexander</surname> <given-names>G</given-names></name>. <article-title>Neuron activity related to short-term memory</article-title>. <source>Science</source>. <year>1971</year>;<volume>173</volume>:<fpage>652</fpage>–<lpage>654</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.173.3997.652" xlink:type="simple">10.1126/science.173.3997.652</ext-link></comment> <object-id pub-id-type="pmid">4998337</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref013">
<label>13</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Funahashi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Bruce</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Goldman-Rakic</surname> <given-names>PS</given-names></name>. <article-title>Mnemonic coding of visual space in the monkey’s dorsolateral prefrontal cortex</article-title>. <source>J Neurophysiol</source>. <year>1989</year>;<volume>61</volume>:<fpage>331</fpage>–<lpage>349</lpage>. <object-id pub-id-type="pmid">2918358</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref014">
<label>14</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Romo</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Brody</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Hernández</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lemus</surname> <given-names>L</given-names></name>. <article-title>Neuronal correlates of parametric working memory in the prefrontal cortex</article-title>. <source>Nature</source>. <year>1999</year>;<volume>399</volume>:<fpage>470</fpage>–<lpage>474</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/20939" xlink:type="simple">10.1038/20939</ext-link></comment> <object-id pub-id-type="pmid">10365959</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref015">
<label>15</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Amit</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Gutfreund</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>. <article-title>Storing infinite numbers of patterns in a spin-glass model of neural networks</article-title>. <source>Phys Rev Lett</source>. <year>1985</year>;<volume>55</volume>:<fpage>1530</fpage>–<lpage>1531</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.55.1530" xlink:type="simple">10.1103/PhysRevLett.55.1530</ext-link></comment> <object-id pub-id-type="pmid">10031847</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref016">
<label>16</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name>. <article-title>Neural networks with nonlinear synapses and a static noise</article-title>. <source>Phys Rev A</source>. <year>1986</year>;<volume>34</volume>:<fpage>2571</fpage>–<lpage>2574</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevA.34.2571" xlink:type="simple">10.1103/PhysRevA.34.2571</ext-link></comment> <object-id pub-id-type="pmid">9897569</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref017">
<label>17</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Derrida</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Gardner</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Zippelius</surname> <given-names>A</given-names></name>. <article-title>An exactly solvable asymmetric neural network model</article-title>. <source>Europhys Lett</source>. <year>1987</year>;<volume>4</volume>:<fpage>167</fpage>–<lpage>173</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1209/0295-5075/4/2/007" xlink:type="simple">10.1209/0295-5075/4/2/007</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref018">
<label>18</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Tsodyks</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Feigel’man</surname> <given-names>MV</given-names></name>. <article-title>The enhanced storage capacity in neural networks with low activity level</article-title>. <source>Europhys Lett</source>. <year>1988</year>;<volume>6</volume>:<fpage>101</fpage>–<lpage>105</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1209/0295-5075/6/2/002" xlink:type="simple">10.1209/0295-5075/6/2/002</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref019">
<label>19</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Buhmann</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Divko</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Schulten</surname> <given-names>K</given-names></name>. <article-title>Associative memory with high information content</article-title>. <source>Phys Rev A</source>. <year>1989</year>;<volume>39</volume>:<fpage>2689</fpage>–<lpage>2692</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevA.39.2689" xlink:type="simple">10.1103/PhysRevA.39.2689</ext-link></comment> <object-id pub-id-type="pmid">9901541</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref020">
<label>20</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Gardner</surname> <given-names>EJ</given-names></name>. <article-title>The space of interactions in neural network models</article-title>. <source>J Phys A: Math Gen</source>. <year>1988</year>;<volume>21</volume>:<fpage>257</fpage>–<lpage>270</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/0305-4470/21/1/030" xlink:type="simple">10.1088/0305-4470/21/1/030</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref021">
<label>21</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Rosenblatt</surname> <given-names>F</given-names></name>. <source>Principles of neurodynamics</source>. <publisher-name>Spartan Books</publisher-name>, <publisher-loc>New York</publisher-loc>; <year>1962</year>.</mixed-citation>
</ref>
<ref id="pcbi.1004439.ref022">
<label>22</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Marr</surname> <given-names>D</given-names></name>. <article-title>A theory of cerebellar cortex</article-title>. <source>J Physiol</source>. <year>1969</year>;<volume>202</volume>:<fpage>437</fpage>–<lpage>470</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1113/jphysiol.1969.sp008820" xlink:type="simple">10.1113/jphysiol.1969.sp008820</ext-link></comment> <object-id pub-id-type="pmid">5784296</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref023">
<label>23</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Albus</surname> <given-names>JS</given-names></name>. <article-title>A theory of cerebellar function</article-title>. <source>Mathematical Biosciences</source>. <year>1971</year>;<volume>10</volume>:<fpage>26</fpage>–<lpage>51</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0025-5564(71)90051-4" xlink:type="simple">10.1016/0025-5564(71)90051-4</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref024">
<label>24</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ito</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sakurai</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Tongroach</surname> <given-names>P</given-names></name>. <article-title>Climbing fibre induced depression of both mossy fibre responsiveness and glutamate sensitivity of cerebellar Purkinje cells</article-title>. <source>J Physiol</source>. <year>1982</year>;<volume>324</volume>:<fpage>113</fpage>–<lpage>134</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1113/jphysiol.1982.sp014103" xlink:type="simple">10.1113/jphysiol.1982.sp014103</ext-link></comment> <object-id pub-id-type="pmid">7097592</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref025">
<label>25</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Clopath</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Nadal</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>. <article-title>Storage of correlated patterns in standard and bistable Purkinje cell models</article-title>. <source>PLoS Comput Biol</source>. <year>2012</year>;<volume>8</volume>:<fpage>e1002448</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002448" xlink:type="simple">10.1371/journal.pcbi.1002448</ext-link></comment> <object-id pub-id-type="pmid">22570592</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref026">
<label>26</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Parga</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Virasoro</surname> <given-names>MA</given-names></name>. <article-title>The ultrametric organization of memories in a neural network</article-title>. <source>J Phys France</source>. <year>1986</year>;<volume>47</volume>:<fpage>1857</fpage>–<lpage>1864</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1051/jphys:0198600470110185700" xlink:type="simple">10.1051/jphys:0198600470110185700</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref027">
<label>27</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Carusi</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>. <article-title>Slow stochastic Hebbian learning of classes in recurrent neural networks</article-title>. <source>Network</source>. <year>1998</year>;<volume>9</volume>:<fpage>123</fpage>–<lpage>152</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/0954-898X/9/1/007" xlink:type="simple">10.1088/0954-898X/9/1/007</ext-link></comment> <object-id pub-id-type="pmid">9861982</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref028">
<label>28</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hakim</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Isope</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Nadal</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Barbour</surname> <given-names>B</given-names></name>. <article-title>Optimal information storage and the distribution of synaptic weights: perceptron versus Purkinje cell</article-title>. <source>Neuron</source>. <year>2004</year>;<volume>43</volume>:<fpage>745</fpage>–<lpage>57</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0896-6273(04)00528-8" xlink:type="simple">10.1016/S0896-6273(04)00528-8</ext-link></comment> <object-id pub-id-type="pmid">15339654</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref029">
<label>29</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>van Rossum</surname> <given-names>MC</given-names></name>. <article-title>Lapicque’s 1907 paper: from frogs to integrate-and-fire</article-title>. <source>Biol Cybern</source>. <year>2007</year>;<volume>97</volume>:<fpage>337</fpage>–<lpage>339</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00422-007-0190-0" xlink:type="simple">10.1007/s00422-007-0190-0</ext-link></comment> <object-id pub-id-type="pmid">17968583</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref030">
<label>30</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Clopath</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>. <article-title>Optimal properties of analog perceptrons with excitatory weights</article-title>. <source>PLoS Comput Biol</source>. <year>2013</year>;<volume>9</volume>:<fpage>e1002919</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1002919" xlink:type="simple">10.1371/journal.pcbi.1002919</ext-link></comment> <object-id pub-id-type="pmid">23436991</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref031">
<label>31</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Mézard</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Nadal</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Toulouse</surname> <given-names>G</given-names></name>. <article-title>Solvable models of working memories</article-title>. <source>J Physique</source>. <year>1986</year>;<volume>47</volume>:<fpage>1457</fpage>– <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1051/jphys:019860047090145700" xlink:type="simple">10.1051/jphys:019860047090145700</ext-link></comment></mixed-citation></ref>
<ref id="pcbi.1004439.ref032">
<label>32</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Parisi</surname> <given-names>G</given-names></name>. <article-title>A memory which forgets</article-title>. <source>J Phys A: Math Gen</source>. <year>1986</year>;<volume>19</volume>:<fpage>L617</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/0305-4470/19/11/005" xlink:type="simple">10.1088/0305-4470/19/11/005</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref033">
<label>33</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Amit</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>. <article-title>Dynamic learning in neural networks with material synapses</article-title>. <source>Neural Computation</source>. <year>1994</year>;<volume>6</volume>:<fpage>957</fpage>–<lpage>982</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.1994.6.5.957" xlink:type="simple">10.1162/neco.1994.6.5.957</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref034">
<label>34</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bienenstock</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Cooper</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Munro</surname> <given-names>P</given-names></name>. <article-title>Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex</article-title>. <source>J Neurosci</source>. <year>1982</year>;<volume>2</volume>:<fpage>32</fpage>–<lpage>48</lpage>. <object-id pub-id-type="pmid">7054394</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref035">
<label>35</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Jedlicka</surname> <given-names>P</given-names></name>. <article-title>Synaptic plasticity, metaplasticity and BCM theory</article-title>. <source>Bratislavské lekárske listy</source>. <year>2002</year>;<volume>103</volume>(<issue>4/5</issue>):<fpage>137</fpage>–<lpage>143</lpage>. <object-id pub-id-type="pmid">12413200</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref036">
<label>36</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Bröcher</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Artola</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Singer</surname> <given-names>W</given-names></name>. <article-title>Intracellular injection of Ca2+ chelators blocks induction of long-term depression in rat visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1992</year>;<volume>89</volume>(<issue>1</issue>):<fpage>123</fpage>–<lpage>127</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.89.1.123" xlink:type="simple">10.1073/pnas.89.1.123</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref037">
<label>37</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Artola</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Singer</surname> <given-names>W</given-names></name>. <article-title>Long-term depression of excitatory synaptic transmission and its relationship to long-term potentiation</article-title>. <source>Trends in neurosciences</source>. <year>1993</year>;<volume>16</volume>(<issue>11</issue>):<fpage>480</fpage>–<lpage>487</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0166-2236(93)90081-V" xlink:type="simple">10.1016/0166-2236(93)90081-V</ext-link></comment> <object-id pub-id-type="pmid">7507622</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref038">
<label>38</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Amit</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Mascaro</surname> <given-names>M</given-names></name>. <article-title>Attractor networks for shape recognition</article-title>. <source>Neural Comput</source>. <year>2001</year>;<volume>13</volume>:<fpage>1415</fpage>–<lpage>1442</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/08997660152002906" xlink:type="simple">10.1162/08997660152002906</ext-link></comment> <object-id pub-id-type="pmid">11387051</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref039">
<label>39</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Drew</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>LF</given-names></name>. <article-title>Cascade models of synaptically stored memories</article-title>. <source>Neuron</source>. <year>2005</year>;<volume>45</volume>:<fpage>599</fpage>–<lpage>611</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2005.02.001" xlink:type="simple">10.1016/j.neuron.2005.02.001</ext-link></comment> <object-id pub-id-type="pmid">15721245</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref040">
<label>40</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Brader</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Senn</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Fusi</surname> <given-names>S</given-names></name>. <article-title>Learning real-world stimuli in a neural network with spike-driven synaptic dynamics</article-title>. <source>Neural Comput</source>. <year>2007</year>;<volume>19</volume>:<fpage>2881</fpage>–<lpage>2912</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2007.19.11.2881" xlink:type="simple">10.1162/neco.2007.19.11.2881</ext-link></comment> <object-id pub-id-type="pmid">17883345</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref041">
<label>41</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ngezahayo</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schachner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Artola</surname> <given-names>A</given-names></name>. <article-title>Synaptic activity modulates the induction of bidirectional synaptic changes in adult mouse hippocampus</article-title>. <source>J Neurosci</source>. <year>2000</year>;<volume>20</volume>:<fpage>2451</fpage>–<lpage>2458</lpage>. <object-id pub-id-type="pmid">10729325</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref042">
<label>42</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kirkwood</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rioult</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Bear</surname> <given-names>MF</given-names></name>. <article-title>Experience-dependent modification of synaptic plasticity in visual cortex</article-title>. <source>Nature</source>. <year>1996</year>;<volume>381</volume>:<fpage>526</fpage>–<lpage>528</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/381526a0" xlink:type="simple">10.1038/381526a0</ext-link></comment> <object-id pub-id-type="pmid">8632826</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref043">
<label>43</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wang</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Wagner</surname> <given-names>JJ</given-names></name>. <article-title>Priming-induced shift in synaptic plasticity in the rat hippocampus</article-title>. <source>J Neurophysiol</source>. <year>1999</year>;<volume>82</volume>:<fpage>2024</fpage>–<lpage>2028</lpage>. <object-id pub-id-type="pmid">10515995</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref044">
<label>44</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Barbour</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Brunel</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hakim</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Nadal</surname> <given-names>JP</given-names></name>. <article-title>What can we learn from synaptic weight distributions?</article-title> <source>Trends Neurosci</source>. <year>2007</year>;<volume>30</volume>:<fpage>622</fpage>–<lpage>629</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tins.2007.09.005" xlink:type="simple">10.1016/j.tins.2007.09.005</ext-link></comment> <object-id pub-id-type="pmid">17983670</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref045">
<label>45</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Chapeton</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Fares</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>LaSota</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Stepanyants</surname> <given-names>A</given-names></name>. <article-title>Efficient associative memory storage in cortical circuits of inhibitory and excitatory neurons</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2012</year>;<volume>109</volume>:<fpage>E3614</fpage>–<lpage>3622</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1211467109" xlink:type="simple">10.1073/pnas.1211467109</ext-link></comment> <object-id pub-id-type="pmid">23213221</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref046">
<label>46</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kalisman</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Silberberg</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Markram</surname> <given-names>H</given-names></name>. <article-title>The neocortical microcircuit as a tabula rasa</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2005</year>;<volume>102</volume>:<fpage>880</fpage>–<lpage>885</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0407088102" xlink:type="simple">10.1073/pnas.0407088102</ext-link></comment> <object-id pub-id-type="pmid">15630093</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref047">
<label>47</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Song</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sjostrom</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Reigl</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name>. <article-title>Highly nonrandom features of synaptic connectivity in local cortical circuits</article-title>. <source>PLoS Biol</source>. <year>2005</year>;<volume>3</volume>:<fpage>e68</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pbio.0030068" xlink:type="simple">10.1371/journal.pbio.0030068</ext-link></comment> <object-id pub-id-type="pmid">15737062</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref048">
<label>48</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Markram</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Goodman</surname> <given-names>PH</given-names></name>, <name name-style="western"><surname>Berger</surname> <given-names>TK</given-names></name>, <name name-style="western"><surname>Ma</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Goldman-Rakic</surname> <given-names>PS</given-names></name>. <article-title>Heterogeneity in the pyramidal network of the medial prefrontal cortex</article-title>. <source>Nat Neurosci</source>. <year>2006</year>;<volume>9</volume>:<fpage>534</fpage>–<lpage>542</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1670" xlink:type="simple">10.1038/nn1670</ext-link></comment> <object-id pub-id-type="pmid">16547512</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref049">
<label>49</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Lefort</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tomm</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Floyd Sarria</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Petersen</surname> <given-names>CC</given-names></name>. <article-title>The excitatory neuronal network of the C2 barrel column in mouse primary somatosensory cortex</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>61</volume>:<fpage>301</fpage>–<lpage>316</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neuron.2008.12.020" xlink:type="simple">10.1016/j.neuron.2008.12.020</ext-link></comment> <object-id pub-id-type="pmid">19186171</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004439.ref050">
<label>50</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>McCulloch</surname> <given-names>WS</given-names></name>, <name name-style="western"><surname>Pitts</surname> <given-names>WA</given-names></name>. <article-title>A logical calculus of the ideas immanent in nervous activity</article-title>. <source>Bull Math Biophys</source>. <year>1943</year>;<volume>5</volume>:<fpage>115</fpage>–<lpage>133</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF02478259" xlink:type="simple">10.1007/BF02478259</ext-link></comment></mixed-citation>
</ref>
</ref-list>
</back>
</article>