<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article article-type="research-article" dtd-version="3.0" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-15-41235</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0149874</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Model organisms</subject><subj-group><subject>Animal models</subject><subj-group><subject>Caenorhabditis elegans</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Invertebrates</subject><subj-group><subject>Nematoda</subject><subj-group><subject>Caenorhabditis</subject><subj-group><subject>Caenorhabditis elegans</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject><subj-group><subject>Scale-free networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular types</subject><subj-group><subject>Animal cells</subject><subj-group><subject>Neurons</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Artificial neural networks</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Artificial neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Artificial neural networks</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Artificial neural networks</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Learning curves</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Learning curves</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Learning</subject><subj-group><subject>Learning curves</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Learning</subject><subj-group><subject>Learning curves</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cell motility</subject><subj-group><subject>Chemotaxis</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cell physiology</subject><subj-group><subject>Junctional complexes</subject><subj-group><subject>Electrical synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject><subj-group><subject>Electrical synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Nervous system</subject><subj-group><subject>Synapses</subject><subj-group><subject>Electrical synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject><subj-group><subject>Electrical synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject><subj-group><subject>Electrical synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Synapses</subject><subj-group><subject>Electrical synapses</subject></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>A Model for Improving the Learning Curves of Artificial Neural Networks</article-title>
<alt-title alt-title-type="running-head">Improving the ANN Learning Curves</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Monteiro</surname> <given-names>Roberto L. S.</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Carneiro</surname> <given-names>Tereza Kelly G.</given-names></name>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Fontoura</surname> <given-names>José Roberto A.</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>da Silva</surname> <given-names>Valéria L.</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Moret</surname> <given-names>Marcelo A.</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Pereira</surname> <given-names>Hernane Borges de Barros</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Programa de Modelagem Computational, SENAI CIMATEC, Av. Orlando Gomes 1845, Salvador, 41.650-010, Brazil</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Universidade do Estado da Bahia, Salvador, Brasil</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Universidade Estadual de Ciências da Saúde de Alagoas, Maceió, Brazil</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Li</surname> <given-names>Daqing</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Beihang University, CHINA</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: RLSM TKGC JRAF VLS MAM HBBP. Performed the experiments: RLSM TKGC JRAF VLS MAM HBBP. Analyzed the data: RLSM TKGC JRAF VLS MAM HBBP. Contributed reagents/materials/analysis tools: RLSM TKGC JRAF VLS MAM HBBP. Wrote the paper: RLSM TKGC JRAF VLS MAM HBBP.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">roberto@souzamonteiro.com</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<year>2016</year>
</pub-date>
<pub-date pub-type="epub">
<day>22</day>
<month>2</month>
<year>2016</year>
</pub-date>
<volume>11</volume>
<issue>2</issue>
<elocation-id>e0149874</elocation-id>
<history>
<date date-type="received">
<day>19</day>
<month>9</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>6</day>
<month>2</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Monteiro et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0149874"/>
<abstract>
<p>In this article, the performance of a hybrid artificial neural network (i.e. scale-free and small-world) was analyzed and its learning curve compared to three other topologies: random, scale-free and small-world, as well as to the chemotaxis neural network of the nematode Caenorhabditis Elegans. One hundred equivalent networks (same number of vertices and average degree) for each topology were generated and each was trained for one thousand epochs. After comparing the mean learning curves of each network topology with the C. elegans neural network, we found that the networks that exhibited preferential attachment exhibited the best learning curves.</p>
</abstract>
<funding-group>
<funding-statement>This work was supported by Conselho Nacional de Desenvolvimento Científico e Tecnológico-CNPq, a Federal Brazilian funding agency, grant no. 304454/2014-1 MAM. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="5"/>
<table-count count="1"/>
<page-count count="11"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Emmert-Streib [<xref ref-type="bibr" rid="pone.0149874.ref001">1</xref>] demonstrated the effect of topology on the performance of neural networks. they compared the performances of random-topology networks [<xref ref-type="bibr" rid="pone.0149874.ref002">2</xref>], scale-free networks [<xref ref-type="bibr" rid="pone.0149874.ref003">3</xref>], and small-world networks [<xref ref-type="bibr" rid="pone.0149874.ref004">4</xref>]. Bohland and Minai [<xref ref-type="bibr" rid="pone.0149874.ref005">5</xref>] highlighted that small-world networks are more economical because these networks have fewer connections and perform as fast as denser networks when applied to associative memory systems.</p>
<p>Watts and Strogatz [<xref ref-type="bibr" rid="pone.0149874.ref004">4</xref>] analyzed the properties (mean shortest path and mean clustering coefficient) of the neural network of the nematode <italic>Caenorhabditis elegans</italic>[<xref ref-type="bibr" rid="pone.0149874.ref006">6</xref>, <xref ref-type="bibr" rid="pone.0149874.ref007">7</xref>] and found that this network exhibits small-work network characteristics. Latora and Marchiori [<xref ref-type="bibr" rid="pone.0149874.ref008">8</xref>] also reached this conclusion when analyzing the efficiency of the neural network of <italic>C. elegans</italic>. Chen et al. [<xref ref-type="bibr" rid="pone.0149874.ref009">9</xref>] also studied the efficiency of this network and argued that this characteristic is an evolutionary trait.</p>
<p>Although the aforementioned authors classified the neural network of <italic>C. elegans</italic> as a small-world network, Morita et al. [<xref ref-type="bibr" rid="pone.0149874.ref010">10</xref>] argued that the Watts and Strogatz [<xref ref-type="bibr" rid="pone.0149874.ref004">4</xref>] model is insufficient to explain its properties. It should be emphasized that <italic>C. elegans</italic> was used as a benchmark for these studies because it is the only animal whose neural network has been fully mapped and is used as a model for various studies involving neurodegeneration and neuroplasticity (e.g., [<xref ref-type="bibr" rid="pone.0149874.ref011">11</xref>–<xref ref-type="bibr" rid="pone.0149874.ref013">13</xref>]).</p>
<p>These studies utilized simplified models to simulate the neural network of the animal. In this paper, we proposed a method that allows the original neural network of the animal (represented by an augmented adjacency matrix, called learning matrix in this article, <xref ref-type="fig" rid="pone.0149874.g001">Fig 1b</xref>) to be trained and compares its performance with random, small-world, scale-free and hybrid topology networks.</p>
<fig id="pone.0149874.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149874.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Artificial neural network and its learning matrix.</title>
<p>a) Two-layer perceptron based on Rosenblatt [<xref ref-type="bibr" rid="pone.0149874.ref021">21</xref>] and Nazzal et al. [<xref ref-type="bibr" rid="pone.0149874.ref022">22</xref>]. b) Learning matrix elements.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149874.g001" xlink:type="simple"/>
</fig>
<p>Furthermore, we compare the learning curves of four network topologies (random [<xref ref-type="bibr" rid="pone.0149874.ref002">2</xref>], scale-free [<xref ref-type="bibr" rid="pone.0149874.ref003">3</xref>], small-world [<xref ref-type="bibr" rid="pone.0149874.ref004">4</xref>], and hybrid [<xref ref-type="bibr" rid="pone.0149874.ref014">14</xref>, <xref ref-type="bibr" rid="pone.0149874.ref015">15</xref>]) with the performance of the neural network for chemotaxis in <italic>C. elegans</italic>[<xref ref-type="bibr" rid="pone.0149874.ref016">16</xref>–<xref ref-type="bibr" rid="pone.0149874.ref019">19</xref>]. This is the first time, to the best of our knowledge, that a comparative analysis of the performance of a hybrid neural network was done.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Materials and Methods</title>
<p>We selected a sub-network of the main component of the neural network of <italic>C. elegans</italic> to perform this study: the chemotaxis network. This network, studied by Ward [<xref ref-type="bibr" rid="pone.0149874.ref016">16</xref>], Segev and Ben-Jacob [<xref ref-type="bibr" rid="pone.0149874.ref018">18</xref>], Pierce-Shimomura et al. [<xref ref-type="bibr" rid="pone.0149874.ref017">17</xref>], and Dunn et al. [<xref ref-type="bibr" rid="pone.0149874.ref019">19</xref>], among others, consists of 15 neurons that are interconnected by chemical and electrical synapses (there are two pairs of each neuron; thus, two identical networks are formed for chemotaxis). In this study, we made no distinction between chemical and electrical synapses and only used one neuron from each pair to simplify modeling. This simplification does not lead to any loss of information, since we investigate the efficiency of the topological structure of the neural network regarding the flow of information in terms of learning correctness and epochs. Simulation results of the C. elegans network with the electrical synapses removed validate this assumption and are shown in <xref ref-type="supplementary-material" rid="pone.0149874.s001">S1 Appendix</xref>.</p>
<p>The model introduced by Dunn et al. [<xref ref-type="bibr" rid="pone.0149874.ref019">19</xref>] contains one input neuron, ASE, and one output neuron, which combines neurons AVA and AVB into a single neuron. We chose to treat the two neurons separately in this study. So, we drawn the directed graph shown in <xref ref-type="fig" rid="pone.0149874.g002">Fig 2</xref>. This network is similar to that presented by [<xref ref-type="bibr" rid="pone.0149874.ref019">19</xref>], except that the loops have been removed and the chemical and electrical synapses are represented by a single oriented line segment. [<xref ref-type="bibr" rid="pone.0149874.ref019">19</xref>] and Varshney et al. [<xref ref-type="bibr" rid="pone.0149874.ref007">7</xref>]</p>
<fig id="pone.0149874.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149874.g002</object-id>
<label>Fig 2</label>
<caption>
<title>The chemotaxis neural network of <italic>C. elegans</italic>, based on [<xref ref-type="bibr" rid="pone.0149874.ref006">6</xref>, <xref ref-type="bibr" rid="pone.0149874.ref007">7</xref>, <xref ref-type="bibr" rid="pone.0149874.ref019">19</xref>].</title>
<p>The line type identifies the arcs leaving the neurons (vertices) of each layer. The solid line arcs leave the ASE sensory neuron; the dotted line arcs leave the AWC, AFD, AIY and AIA interneurons; the short dashed line arcs leave the AIB, RIA and RIF interneurons; and the dashed line arcs leave the SAAD, DVC, FLP, RIM and RIB interneurons.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149874.g002" xlink:type="simple"/>
</fig>
<p>Based on this graph, we created 100 equivalent artificial networks (same number of vertices and average degree) of each topology: 100 random networks, 100 scale-free networks, 100 small-world networks, and 100 hybrid networks. The random, scale-free, and small-world networks were created using algorithms adapted from Batagelj and Brandes [<xref ref-type="bibr" rid="pone.0149874.ref020">20</xref>]. To create the hybrid networks, we initially created small-world networks with the same number of vertices and an initial average degree slightly smaller than the one for the <italic>C. elegans</italic> (the initial average degree is obtained empirically by starting with a value of one or two units lower than desired and increasing this value in 0.1 steps to obtain a network with the average degree nearest the desired); then, new edges were added to the networks according to the probability <inline-formula id="pone.0149874.e001"><alternatives><graphic id="pone.0149874.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149874.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>n</mml:mi> <mml:mrow><mml:mi>e</mml:mi> <mml:mi>d</mml:mi> <mml:mi>g</mml:mi> <mml:mi>e</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>k</italic><sub><italic>i</italic></sub> is the vertex degree, and <italic>n</italic><sub><italic>edges</italic></sub> is the number of edges existing in the network. Barabási and Albert [<xref ref-type="bibr" rid="pone.0149874.ref003">3</xref>] proposed <inline-formula id="pone.0149874.e002"><alternatives><graphic id="pone.0149874.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149874.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mrow><mml:msub><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>k</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:msubsup> <mml:msub><mml:mi>k</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula> for preferential attachment. However, this formula results in an extremely small number of preferential connections given the small size of the network. These networks were saved in Pajek format files for subsequent use in the simulations.</p>
<p>Each network was trained 1000 times or until learning reached 100% using a set of 100 pairs of input and output values, which correspond to the rules shown in <xref ref-type="table" rid="pone.0149874.t001">Table 1</xref>. This table was defined based on the analysis of the experiment conducted by Dunn et al. [<xref ref-type="bibr" rid="pone.0149874.ref019">19</xref>]. The ASE value corresponds to the variation in the <italic>NH</italic><sub>4</sub> <italic>Cl</italic> concentration detected by this neuron, which is expressed as 10<sup>−3</sup> <italic>mM</italic>/<italic>s</italic>. We set the value of ±5 × 10<sup>−3</sup> <italic>mM</italic>/<italic>s</italic> for the lower and upper limits of the variation range of <italic>NH</italic><sub>4</sub> <italic>Cl</italic> concentration based on the analysis of the graphs shown in Dunn et al. [<xref ref-type="bibr" rid="pone.0149874.ref019">19</xref>]. This value is an approximation required for our simulations.</p>
<table-wrap id="pone.0149874.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149874.t001</object-id>
<label>Table 1</label>
<caption>
<title>Rules for the simulation of chemotaxis in <italic>C. elegans</italic> based on Dunn et al. [<xref ref-type="bibr" rid="pone.0149874.ref019">19</xref>].</title>
</caption>
<alternatives>
<graphic id="pone.0149874.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149874.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">ASE</th>
<th align="center">AVA</th>
<th align="center">AVB</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"><italic>dC</italic>/<italic>dt</italic> &lt; −5</td>
<td align="center">0</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">−5 ≤ <italic>dC</italic>/<italic>dt</italic> ≤ 5</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>
<tr>
<td align="center"><italic>dC</italic>/<italic>dt</italic> &gt; 5</td>
<td align="center">1</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The mathematical model that was used to construct the artificial neural networks was based on the perceptron created by Rosenblatt [<xref ref-type="bibr" rid="pone.0149874.ref021">21</xref>] and generalized for multiple layers by Nazzal et al. [<xref ref-type="bibr" rid="pone.0149874.ref022">22</xref>]. <xref ref-type="fig" rid="pone.0149874.g001">Fig 1a</xref> shows a two-layer perceptron. The perceptron is an easily implemented artificial neuron. However, code development for the construction of an artificial neural network becomes laborious as the number of layers increases. To facilitate our study, we developed an algorithm that enables training and running a neural network using a learning matrix, which is constructed based on the adjacency matrix of the network. <xref ref-type="fig" rid="pone.0149874.g001">Fig 1b</xref> shows the learning matrix elements for the two-layer perceptron in <xref ref-type="fig" rid="pone.0149874.g001">Fig 1a</xref>. These algorithms are presented in detail in <xref ref-type="supplementary-material" rid="pone.0149874.s002">S2 Appendix</xref>.</p>
<p>A perceptron consists of four elements: input signals, adder, activation function, and output signal. Multi-layer perceptrons consist of several artificial neurons arranged in layers, wherein a neuron output is the neuron input of the next layer. Feedback, wherein a neuron output returns to the same layer, may exist if necessary. Our experiment used five-layer perceptrons without feedback, wherein the first layer (input) consisted of the ASE neuron; the second layer consisted of the AWC, AFD, AIY, and AIA neurons; the third layer consisted of the AIB, RIA, and RIF neurons; the fourth layer consisted of the SAAD, DVC, FLP, RIM, and RIB neurons, and the fifth layer (output) consisted of the AVA and AVB neurons.</p>
<p>To calculate the output value of a neuron, we used the function <italic>y</italic><sub><italic>j</italic></sub> = <italic>f</italic>(<italic>x</italic><sub><italic>j</italic></sub>), where <italic>f</italic>(<italic>x</italic><sub><italic>j</italic></sub>) is the neuron activation function, and <italic>x</italic><sub><italic>j</italic></sub> is the value of the weighted sum of the inputs in this neuron defined by <inline-formula id="pone.0149874.e003"><alternatives><graphic id="pone.0149874.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149874.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:msubsup> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, where <italic>x</italic><sub><italic>i</italic></sub> is the input value at synapse <italic>i</italic> of neuron <italic>j</italic>, and <italic>w</italic><sub><italic>i</italic>,<italic>j</italic></sub> is the weight of this synapse. We chose the sigmoid function, <inline-formula id="pone.0149874.e004"><alternatives><graphic id="pone.0149874.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149874.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>, as the activation function because this function is commonly used to simulate the output signal of neurons in <italic>C. elegans</italic> (e.g., [<xref ref-type="bibr" rid="pone.0149874.ref019">19</xref>]).</p>
<p>The process of training a perceptron, be it a single layer or multiple layers, occurs by adjusting the weights of the neural synapses. For this purpose, we used <xref ref-type="disp-formula" rid="pone.0149874.e005">Eq 1</xref>, which is based on the study by Nazzal et al. [<xref ref-type="bibr" rid="pone.0149874.ref022">22</xref>].
<disp-formula id="pone.0149874.e005"><alternatives><graphic id="pone.0149874.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149874.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:mi>η</mml:mi> <mml:mo>·</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:msup><mml:mi>f</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where <italic>η</italic> is a real number between 0 and 1. We used <italic>η</italic> = 0.45 in our study after testing various values between 0.05 and 0.95 with 0.05 increments.</p>
<p>To calculate <italic>δ</italic><sub><italic>j</italic></sub>, the output error of neuron <italic>j</italic>, we used two equations, <italic>δ</italic><sub><italic>j</italic></sub> = <italic>z</italic><sub><italic>j</italic></sub> − <italic>y</italic><sub><italic>j</italic></sub>, for the weights of the last layer, and <inline-formula id="pone.0149874.e006"><alternatives><graphic id="pone.0149874.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149874.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mrow><mml:msub><mml:mi>δ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mi>n</mml:mi></mml:mrow> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mi>n</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>·</mml:mo> <mml:msub><mml:mi>δ</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>, for the weights of the intermediate layers. In this formulas <italic>z</italic><sub><italic>j</italic></sub> is the expected output value of neuron <italic>j</italic> and <italic>δ</italic><sub><italic>i</italic></sub> is the error value of input neuron <italic>i</italic> of the layer after neuron <italic>j</italic>. Furthermore, <inline-formula id="pone.0149874.e007"><alternatives><graphic id="pone.0149874.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149874.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mrow><mml:msup><mml:mi>f</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mo>·</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the derivative of the activation function of neuron <italic>j</italic>.</p>
<p>For each network, we ran the algorithm 1000 times and saved the hit percentage of the input and output set at each epoch.</p>
<p>The algorithms described in <xref ref-type="supplementary-material" rid="pone.0149874.s002">S2 Appendix</xref> were developed in order to facilitate performing simulations using neural networks with complex topologies, such as those studied herein. These algorithms were implemented in the programming language GuaráScript, which we also designed to facilitate the construction of scientific applications. All software programs that were used as the basis for this study are available for download on the GuaráScript project website: <ext-link ext-link-type="uri" xlink:href="http://www.guarascript.org" xlink:type="simple">http://www.guarascript.org</ext-link>.</p>
<p>The entire dataset used to perform the simulations are included in <xref ref-type="supplementary-material" rid="pone.0149874.s004">S1 Dataset</xref>.</p>
</sec>
<sec id="sec003" sec-type="results">
<title>Results</title>
<p>The results were divided into two groups. In group 1, we have considered only those simulations where there was 100% of learning. In group 2, we have considered all the simulations, even when there was less than 100% of learning.</p>
<p>Considering the simulations of group 2, Figs <xref ref-type="fig" rid="pone.0149874.g003">3</xref> and <xref ref-type="fig" rid="pone.0149874.g004">4</xref> show the results of the simulations performed with 400 artificial neural networks, composed by 100 random, 100 scale-free, 100 small-world and 100 hybrid (scale-free and small-world) networks, in terms of the number of epochs and correctness of the networks, comparing them with the values obtained from the original network of <italic>C. elegans</italic>. <xref ref-type="fig" rid="pone.0149874.g003">Fig 3</xref> shows that the networks with preferential attachment (i.e. free-scale and hybrid) learn more rapidly than the networks where those characteristics are not present (i.e. random and small-world). In <xref ref-type="fig" rid="pone.0149874.g004">Fig 4</xref>, we observe that the network with hybrid topology has a mean number of epochs to learn that is close to the <italic>C. elegans</italic> network. These results provide evidence that the neural network of <italic>C. elegans</italic> can have an hybrid topology with characteristics of scale-free and small-world networks, reinforcing the observations made by [<xref ref-type="bibr" rid="pone.0149874.ref010">10</xref>].</p>
<fig id="pone.0149874.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149874.g003</object-id>
<label>Fig 3</label>
<caption>
<title>The number of epochs.</title>
<p>(CE) <italic>C. elegans</italic>, (RD) random, (SW) small-world, (SF) scale-free and (HY) hybrid networks necessary to learn to interpret 100 input signals. Mean of 100 samples.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149874.g003" xlink:type="simple"/>
</fig>
<fig id="pone.0149874.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149874.g004</object-id>
<label>Fig 4</label>
<caption>
<title>The correctness.</title>
<p>(CE) <italic>C. elegans</italic>, (RD) random, (SW) small-world, (SF) scale-free and (HY) hybrid networks, when attempting to learn to interpret 100 input signals. Mean of 100 samples.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149874.g004" xlink:type="simple"/>
</fig>
<p>
<xref ref-type="fig" rid="pone.0149874.g005">Fig 5</xref> compares the mean learning curves of the <italic>C. elegans</italic>, hybrid, random, scale-free, and small-world networks. The scale-free, hybrid, and <italic>C. elegans</italic> networks learned faster than the random and small-world networks. Conversely, the learning curve of the animal neural network approaches the hybrid neural network at approximately the hundredth epoch.</p>
<fig id="pone.0149874.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149874.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Learning curves of the neural network of <italic>C. elegans</italic> and the random, small-world, scale-free and hybrid artificial neural networks.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149874.g005" xlink:type="simple"/>
</fig>
<p>Considering that although the neural network of <italic>C. elegans</italic> has characteristics of a small-world network [<xref ref-type="bibr" rid="pone.0149874.ref023">23</xref>], its properties may not be explained using only this model [<xref ref-type="bibr" rid="pone.0149874.ref010">10</xref>]. Furthermore, Chatterjee and Sinha [<xref ref-type="bibr" rid="pone.0149874.ref024">24</xref>] argued that there is a correlation between the degree centrality of the neurons of the <italic>C. elegans</italic> network and its neurological importance, which characterizes the preferential attachment. Within this context, we have evidence that this network has the characteristics of a scale-free network, and the networks where preferential attachment occurred were those that exhibited the best learning curves.</p>
<p>As observed in <xref ref-type="fig" rid="pone.0149874.g005">Fig 5</xref>, there is an evidence that the neural network of the animal exhibits an hybrid neural network (i.e. small-world and scale-free properties).</p>
<p>We also noticed that the theoretical hybrid network behaved like the original network of the animal, in terms of its ability to properly learn (correctness) the rules imposed on the model (<xref ref-type="fig" rid="pone.0149874.g004">Fig 4</xref>).</p>
<p>In order to validate our conclusion, we performed a similar experiment using a semantic network for controlling a gas sniffer robot. The results are similar to the ones obtained with the C. Elegans (i.e. networks with preferential attachment have better learning curves). More details on this experiment are presented in <xref ref-type="supplementary-material" rid="pone.0149874.s003">S3 Appendix</xref>.</p>
</sec>
<sec id="sec004" sec-type="conclusions">
<title>Conclusions</title>
<p>In this study, we analyzed the performance of four network topologies, including random, small-world, scale-free and hybrid. These topologies were used to compare their results with the results of the neural network of <italic>C. elegans</italic>. Further, we presented two algorithms that were suitable for the implementation of artificial neural networks with complex topologies (random, small-world, scale-free and hybrid).</p>
<p>We compared the learning curves of four different network topologies that are used in modeling artificial neural networks. We observed that the scale-free, hybrid, and <italic>C. elegans</italic> networks learned faster than the other topologies because they displayed preferential attachment.</p>
<p>We used the neural network for chemotaxis in the nematode <italic>Caenorhabditis elegans</italic> as the benchmark and found that near the hundredth epoch, its learning curve distances itself from the random and small-world networks and approaches the hybrid network curve. This result provides evidence that the neural network of the animal exhibits an hybrid neural network (i.e. small-world and scale-free properties).</p>
<p>When analyzing the structure and function of the neural network of <italic>C. elegans</italic>, [<xref ref-type="bibr" rid="pone.0149874.ref009">9</xref>] emphasized that the network is highly optimized and that this optimization is an evolutionary trait. This hypothesis is reinforced by the results observed in Figs <xref ref-type="fig" rid="pone.0149874.g003">3</xref> and <xref ref-type="fig" rid="pone.0149874.g004">4</xref>, which show that the random and small-world networks have the lowest correctness and the worst time to learn, while the scale-free network features 100% correctness.</p>
<p>On the other hand, when studying the efficiency of the neural network of <italic>C. elegans</italic>, [<xref ref-type="bibr" rid="pone.0149874.ref008">8</xref>] emphasized that the neural network behaves as a small-world network and that this type of network has the property of being highly resistant to failure. Thus, it is natural that in its evolutionary process, the animal has experienced various network topologies and that natural selection has favored individuals with extremely fast learning, accuracy in their responses and the ability to withstand failures in its neurological structure (e.g., diseases and injuries caused by predators).</p>
<p>In fact, in addition to displaying characteristics of a small-world network, the neural network of <italic>C. elegans</italic> has other properties that suggest that this network may also behave as a scale-free network, i.e., a hybrid network.</p>
</sec>
<sec id="sec005">
<title>Supporting Information</title>
<supplementary-material id="pone.0149874.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pone.0149874.s001" xlink:type="simple">
<label>S1 Appendix</label>
<caption>
<title>Results of the simulation after removal of the electrical synapses (gap junctions) of the C. elegans chemotaxis neural network.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0149874.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pone.0149874.s002" xlink:type="simple">
<label>S2 Appendix</label>
<caption>
<title>Training Process Summary and Algorithms.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0149874.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pone.0149874.s003" xlink:type="simple">
<label>S3 Appendix</label>
<caption>
<title>Simulation results of a gas sniffer robot.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0149874.s004" mimetype="application/zip" position="float" xlink:href="info:doi/10.1371/journal.pone.0149874.s004" xlink:type="simple">
<label>S1 Dataset</label>
<caption>
<title>Complete dataset.</title>
<p>(ZIP)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>Conselho Nacional de Desenvolvimento Científico e Tecnológico—CNPq, a Federal Brazilian funding agency, Grant No. 304454/2014-1 MAM. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0149874.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Emmert-Streib</surname> <given-names>F</given-names></name>. <article-title>Influence of the neural network topology on the learning dynamics</article-title>. <source>Neurocomputing</source>. <year>2006</year>;<volume>69</volume>(<issue>10–12</issue>):<fpage>1179</fpage>–<lpage>1182</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neucom.2005.12.070" xlink:type="simple">10.1016/j.neucom.2005.12.070</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149874.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Erdos</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Rényi</surname> <given-names>A</given-names></name>. <article-title>On the evolution of random graphs</article-title>. <source>Publications of the Matematical Institute of the Hungarian Academy of Sciences</source>. <year>1960</year>;<volume>5</volume>:<fpage>17</fpage>–<lpage>61</lpage>.</mixed-citation>
</ref>
<ref id="pone.0149874.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Barabási</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Albert</surname> <given-names>R</given-names></name>. <article-title>Emergence of Scaling in Random Networks</article-title>. <source>Science</source>. <year>1999</year>;<volume>286</volume>(<issue>5439</issue>):<fpage>509</fpage>–<lpage>512</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.286.5439.509" xlink:type="simple">10.1126/science.286.5439.509</ext-link></comment> <object-id pub-id-type="pmid">10521342</object-id></mixed-citation>
</ref>
<ref id="pone.0149874.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Watts</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Strogatz</surname> <given-names>SH</given-names></name>. <article-title>Collective dynamics of ‘small-world’ networks</article-title>. <source>Nature</source>. <year>1998</year>;<volume>393</volume>:<fpage>440</fpage>–<lpage>442</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/30918" xlink:type="simple">10.1038/30918</ext-link></comment> <object-id pub-id-type="pmid">9623998</object-id></mixed-citation>
</ref>
<ref id="pone.0149874.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bohland</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Minai</surname> <given-names>AA</given-names></name>. <article-title>Efficient associative memory using small-world architecture</article-title>. <source>Neurocomputing</source>. <year>2001</year>;<volume>38–40</volume>(<issue>0</issue>):<fpage>489</fpage>–<lpage>496</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0925-2312(01)00378-2" xlink:type="simple">10.1016/S0925-2312(01)00378-2</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149874.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>White</surname> <given-names>JG</given-names></name>, <name name-style="western"><surname>Southgate</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Thomson</surname> <given-names>JN</given-names></name>, <name name-style="western"><surname>Brenner</surname> <given-names>S</given-names></name>. <article-title>The Structure of the Nervous System of the Nematode Caenorhabditis elegans</article-title>. <source>Phil Trans R Soc Lond B</source>. <year>1986</year>;<volume>314</volume>(<issue>1165</issue>):<fpage>1</fpage>–<lpage>340</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rstb.1986.0056" xlink:type="simple">10.1098/rstb.1986.0056</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149874.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Varshney</surname> <given-names>LR</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>Paniagua</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Hall</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name>. <article-title>Structural Properties of the Caenorhabditis elegans Neuronal Network</article-title>. <source>PLOS COMPUTATIONAL BIOLOGY</source>. <year>2011</year>;<volume>7</volume>(<issue>2</issue>):<fpage>1</fpage>–<lpage>21</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1001066" xlink:type="simple">10.1371/journal.pcbi.1001066</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149874.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Latora</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Marchiori</surname> <given-names>M</given-names></name>. <article-title>Efficient Behavior of Small-World Networks</article-title>. <source>Physical Review Letters</source>. <year>2001</year>;<volume>87</volume>(<issue>19</issue>):<fpage>1</fpage>–<lpage>4</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.87.198701" xlink:type="simple">10.1103/PhysRevLett.87.198701</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149874.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chen</surname> <given-names>BL</given-names></name>, <name name-style="western"><surname>Hall</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Chklovskii</surname> <given-names>DB</given-names></name>. <article-title>Wiring optimization can relate neuronal structure and function</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2006</year>;<volume>103</volume>(<issue>12</issue>):<fpage>4723</fpage>–<lpage>4728</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0506806103" xlink:type="simple">10.1073/pnas.0506806103</ext-link></comment> <object-id pub-id-type="pmid">16537428</object-id></mixed-citation>
</ref>
<ref id="pone.0149874.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Morita</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>ichi Oshio</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Osana</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Funabashi</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Oka</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kawamura</surname> <given-names>K</given-names></name>. <article-title>Geometrical structure of the neuronal network of Caenorhabditis elegans</article-title>. <source>Physica A: Statistical Mechanics and its Applications</source>. <year>2001</year>;<volume>298</volume>(<issue>3–4</issue>):<fpage>553</fpage>–<lpage>561</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0378-4371(01)00266-7" xlink:type="simple">10.1016/S0378-4371(01)00266-7</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149874.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Horn</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ruppin</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Usher</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Herrmann</surname> <given-names>M</given-names></name>. <article-title>Neural Network Modeling of Memory Deterioration in Alzheimer’s Disease</article-title>. <source>Neural Computation</source>. <year>1993</year>;<volume>5</volume>(<issue>5</issue>):<fpage>736</fpage>–<lpage>749</lpage>.</mixed-citation>
</ref>
<ref id="pone.0149874.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Puri</surname> <given-names>IK</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>L</given-names></name>. <article-title>Mathematical Modeling for the Pathogenesis of Alzheimer’s Disease</article-title>. <source>PLoS ONE</source>. <year>2010</year>;<volume>5</volume>(<issue>12</issue>):<fpage>e15176+</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0015176" xlink:type="simple">10.1371/journal.pone.0015176</ext-link></comment> <object-id pub-id-type="pmid">21179474</object-id></mixed-citation>
</ref>
<ref id="pone.0149874.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lublin</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Link</surname> <given-names>C</given-names></name>. <article-title>Alzheimerś Disease Drug Discovery: In-vivo screening using C. elegans as a model for <italic>β</italic>-amyloid peptide-induced toxicity</article-title>. <source>Drug Discov Today Technol</source>. <year>2013</year>;<volume>10</volume>(<issue>1</issue>):<fpage>e115</fpage>–<lpage>e119</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.ddtec.2012.02.002" xlink:type="simple">10.1016/j.ddtec.2012.02.002</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149874.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pereira</surname> <given-names>HBB</given-names></name>, <name name-style="western"><surname>Fadigas</surname> <given-names>IS</given-names></name>, <name name-style="western"><surname>Senna</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Moret</surname> <given-names>MA</given-names></name>. <article-title>Semantic networks based on titles of scientific papers</article-title>. <source>Physica A: Statistical Mechanics and its Applications</source>. <year>2011</year>;<volume>390</volume>(<issue>6</issue>):<fpage>1192</fpage>–<lpage>1197</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.physa.2010.12.001" xlink:type="simple">10.1016/j.physa.2010.12.001</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149874.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Monteiro</surname> <given-names>RLS</given-names></name>, <name name-style="western"><surname>Fontoura</surname> <given-names>JRA</given-names></name>, <name name-style="western"><surname>Carneiro</surname> <given-names>TKG</given-names></name>, <name name-style="western"><surname>Moret</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Pereira</surname> <given-names>HBB</given-names></name>. <article-title>Evolution basded on chromosome affinity from a network perspective</article-title>. <source>Physica A: Statistical Mechanics and its Applications</source>. <year>2014</year>;<volume>403</volume>:<fpage>276</fpage>–<lpage>283</lpage>.</mixed-citation>
</ref>
<ref id="pone.0149874.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ward</surname> <given-names>S</given-names></name>. <article-title>Chemotaxis by the Nematode Caenorhabditis elegans: Identification of Attractants and Analysis of the Response by Use of Mutants</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1973</year>;<volume>70</volume>(<issue>3</issue>):<fpage>817</fpage>–<lpage>821</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.70.3.817" xlink:type="simple">10.1073/pnas.70.3.817</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149874.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pierce-Shimomura</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Morse</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Lockery</surname> <given-names>SR</given-names></name>. <article-title>The fundamental role of pirouettes in Caenorhabditis elegans chemotaxis</article-title>. <source>Journal of Neuroscience</source>. <year>1999</year>;<volume>19</volume>(<issue>21</issue>):<fpage>9557</fpage>–<lpage>9569</lpage>. <object-id pub-id-type="pmid">10531458</object-id></mixed-citation>
</ref>
<ref id="pone.0149874.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Segev</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Ben-Jacob</surname> <given-names>E</given-names></name>. <article-title>Generic modeling of chemotactic based self-wiring of neural networks</article-title>. <source>Neural Networks</source>. <year>2000</year>;<volume>13</volume>(<issue>2</issue>):<fpage>185</fpage>–<lpage>199</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0893-6080(99)00084-2" xlink:type="simple">10.1016/S0893-6080(99)00084-2</ext-link></comment> <object-id pub-id-type="pmid">10935760</object-id></mixed-citation>
</ref>
<ref id="pone.0149874.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dunn</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Lockery</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pierce-Shimomura</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Conery</surname> <given-names>J</given-names></name>. <article-title>A Neural Network Model of Chemotaxis Predicts Functions of Synaptic Connections in the Nematode Caenorhabditis elegans</article-title>. <source>Journal of Computational Neuroscience</source>. <year>2004</year>;<volume>17</volume>(<issue>2</issue>):<fpage>137</fpage>–<lpage>147</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/B:JCNS.0000037679.42570.d5" xlink:type="simple">10.1023/B:JCNS.0000037679.42570.d5</ext-link></comment> <object-id pub-id-type="pmid">15306736</object-id></mixed-citation>
</ref>
<ref id="pone.0149874.ref020">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Batagelj</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Brandes</surname> <given-names>U</given-names></name>. <article-title>Efficient generation of large random networks</article-title>. <source>Physcal Review E</source>. <year>2005</year>;<volume>71</volume>(<issue>036113</issue>):<fpage>1</fpage>–<lpage>5</lpage>.</mixed-citation>
</ref>
<ref id="pone.0149874.ref021">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rosenblatt</surname> <given-names>F</given-names></name>. <article-title>The perceptron: A probabilistic model for information storage and organization in the brain</article-title>. <source>Psychological Review</source>. <year>1958</year>;<volume>65</volume>(<issue>6</issue>):<fpage>386</fpage>–<lpage>408</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/h0042519" xlink:type="simple">10.1037/h0042519</ext-link></comment> <object-id pub-id-type="pmid">13602029</object-id></mixed-citation>
</ref>
<ref id="pone.0149874.ref022">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nazzal</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>El-emary</surname> <given-names>IM</given-names></name>, <name name-style="western"><surname>Najim</surname> <given-names>SA</given-names></name>. <article-title>Multilayer Perceptron Neural Network (MLPs) For Analyzing the Properties of Jordan Oil Shale</article-title>. <source>World Applied Sciences Journal</source>. <year>2008</year>;<volume>5</volume>(<issue>5</issue>):<fpage>546</fpage>–<lpage>552</lpage>.</mixed-citation>
</ref>
<ref id="pone.0149874.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Watts</surname> <given-names>DJ</given-names></name>. <article-title>Networks, dynamics, and the small-world phenomenon</article-title>. <source>The American Journal of Sociology</source>. <year>1999</year>;<volume>105</volume>(<issue>2</issue>):<fpage>493</fpage>–<lpage>527</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1086/210318" xlink:type="simple">10.1086/210318</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149874.ref024">
<label>24</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Chatterjee</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Sinha</surname> <given-names>S</given-names></name>. In: <source>Understanding the mind of a worm: hierarchical network structure underlying nervous system function in C. elegans. vol. 168 of Progress in Brain Research</source>. <publisher-name>Elsevier</publisher-name>; <year>2007</year>. p. <fpage>145</fpage>–<lpage>153</lpage>.</mixed-citation>
</ref>
</ref-list>
</back>
</article>