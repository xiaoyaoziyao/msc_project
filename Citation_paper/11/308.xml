<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-15-03918</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0150738</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Endocrinology</subject><subj-group><subject>Endocrine physiology</subject><subj-group><subject>Menstrual cycle</subject><subj-group><subject>Ovulation</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Endocrine physiology</subject><subj-group><subject>Menstrual cycle</subject><subj-group><subject>Ovulation</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Endocrine physiology</subject><subj-group><subject>Menstrual cycle</subject><subj-group><subject>Ovulation</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Reproductive physiology</subject><subj-group><subject>Menstrual cycle</subject><subj-group><subject>Ovulation</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Reproductive physiology</subject><subj-group><subject>Menstrual cycle</subject><subj-group><subject>Ovulation</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Information technology</subject><subj-group><subject>Data reduction</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Mental health and psychiatry</subject><subj-group><subject>Dementia</subject><subj-group><subject>Alzheimer disease</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Neurology</subject><subj-group><subject>Dementia</subject><subj-group><subject>Alzheimer disease</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Neurology</subject><subj-group><subject>Neurodegenerative diseases</subject><subj-group><subject>Alzheimer disease</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Organisms</subject><subj-group><subject>Animals</subject><subj-group><subject>Vertebrates</subject><subj-group><subject>Amniotes</subject><subj-group><subject>Mammals</subject><subj-group><subject>Dogs</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Plant science</subject><subj-group><subject>Plant anatomy</subject><subj-group><subject>Leaves</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Political science</subject><subj-group><subject>Elections</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Database and informatics methods</subject></subj-group></subj-group></article-categories>
<title-group>
<article-title>kmlShape: An Efficient Method to Cluster Longitudinal Data (Time-Series) According to Their Shapes</article-title>
<alt-title alt-title-type="running-head">kmlShape: To Cluster Longitudinal Data According to Their Shapes</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Genolini</surname> <given-names>Christophe</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Ecochard</surname> <given-names>René</given-names></name>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Benghezal</surname> <given-names>Mamoun</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Driss</surname> <given-names>Tarak</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Andrieu</surname> <given-names>Sandrine</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Subtil</surname> <given-names>Fabien</given-names></name>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Inserm UMR 1027, University of Toulouse III, Toulouse, France</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>CeRSM (EA 2931), UFR STAPS, University Paris Ouest-Nanterre-La Défense, Nanterre, France</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Service de Biostatistique, Université Lyon 1, Villeurbanne, France</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>CNRS, UMR5558, Equipe Biotatistique-Santé, Laboratoire de Biométrie et Biologie Evolutive, Villeurbanne, France</addr-line>
</aff>
<aff id="aff005">
<label>5</label>
<addr-line>Department of Epidemiology and Public Health, CHU Toulouse, Toulouse, France</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Huang</surname> <given-names>Chun-Hsi</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University of Connecticut, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: CG RE TD. Performed the experiments: CG MB. Analyzed the data: CG. Contributed reagents/materials/analysis tools: RE SA. Wrote the paper: CG RE TD SA FS MB.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">christophe.genolini@u-paris10.fr</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<year>2016</year>
</pub-date>
<pub-date pub-type="epub">
<day>3</day>
<month>6</month>
<year>2016</year>
</pub-date>
<volume>11</volume>
<issue>6</issue>
<elocation-id>e0150738</elocation-id>
<history>
<date date-type="received">
<day>27</day>
<month>1</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>18</day>
<month>2</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Genolini et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0150738"/>
<abstract>
<sec id="sec001">
<title>Background</title>
<p>Longitudinal data are data in which each variable is measured repeatedly over time. One possibility for the analysis of such data is to cluster them. The majority of clustering methods group together individual that have close trajectories at given time points. These methods group trajectories that are locally close but not necessarily those that have similar shapes. However, in several circumstances, the progress of a phenomenon may be more important than the moment at which it occurs. One would thus like to achieve a partitioning where each group gathers individuals whose trajectories have similar shapes whatever the time lag between them.</p>
</sec>
<sec id="sec002">
<title>Method</title>
<p>In this article, we present a longitudinal data partitioning algorithm based on the shapes of the trajectories rather than on classical distances. Because this algorithm is time consuming, we propose as well two data simplification procedures that make it applicable to high dimensional datasets.</p>
</sec>
<sec id="sec003">
<title>Results</title>
<p>In an application to Alzheimer disease, this algorithm revealed a “rapid decline” patient group that was not found by the classical methods. In another application to the feminine menstrual cycle, the algorithm showed, contrarily to the current literature, that the luteinizing hormone presents two peaks in an important proportion of women (22%).</p>
</sec>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>ANR (project IDoL)</institution>
</funding-source>
<award-id>ANR-12-BSV1-0036</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Genolini</surname> <given-names>Christophe</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This research was funded by the Agence National de la Recherche grant IDoL: ANR-12-BSV1-0036 (CG received the funding). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="13"/>
<table-count count="5"/>
<page-count count="24"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec004" sec-type="intro">
<title>1 Introduction</title>
<sec id="sec005">
<title>1.1 Clustering longitudinal data</title>
<p>Longitudinal data are data in which each variable is measured repeatedly over time. One way of analyzing this type of data is to cluster them; i.e., divide the population into homogeneous subgroups. For this, different methods were proposed among which variants of k-means [<xref ref-type="bibr" rid="pone.0150738.ref001">1</xref>–<xref ref-type="bibr" rid="pone.0150738.ref006">6</xref>] and various model-based classification methods relying on mixture models [<xref ref-type="bibr" rid="pone.0150738.ref007">7</xref>–<xref ref-type="bibr" rid="pone.0150738.ref011">11</xref>]. The pros and cons of these approaches are regularly discussed [<xref ref-type="bibr" rid="pone.0150738.ref012">12</xref>, <xref ref-type="bibr" rid="pone.0150738.ref013">13</xref>] though there are no current recommendations on which method to prefer in a specific context.</p>
<p>The general idea behind partitioning is to group similar individuals within the same cluster. Different approaches to the concept of “similarity” are possible. They may be based on the concept of distance, resemblance, or likelihood. In the majority of the currently available approaches, two individuals are considered similar when they have close trajectories at each time point. This approach takes into account local similarities but not necessarily the general shapes of the trajectories. In particular, two identical trajectories but shifted in time are considered different and may be potentially assigned to distinct clusters. The immediate consequence is that the mean of the group does not inform on the shapes whereas, in a number of cases, the progress of a phenomenon may be more important than the moment at which it occurs. In such circumstances, one would prefer a partitioning that groups individuals whose trajectories have similar shapes whatever the shift in time. An example of this is shown <xref ref-type="fig" rid="pone.0150738.g001">Fig 1</xref>. With classical techniques, trajectories <italic>i</italic><sub>1</sub> and <italic>i</italic><sub>2</sub> (in orange) belong to the same cluster A while <italic>i</italic><sub>3</sub> and <italic>i</italic><sub>4</sub> (light blue) belong to another cluster B. The mean of cluster A is in red; that of cluster B is in deep blue. Using “shape-respecting clustering”, <italic>i</italic><sub>1</sub> and <italic>i</italic><sub>3</sub> (in orange) belong to cluster A while <italic>i</italic><sub>2</sub> and <italic>i</italic><sub>4</sub> (light blue) belong to cluster B. The shape-respecting mean is in red for cluster A and in deep blue for cluster B.</p>
<fig id="pone.0150738.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150738.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Cluster longitudinal data according to their shapes.</title>
<p>(a) four trajectories. (b) With classical techniques, trajectories <italic>i</italic><sub>1</sub> and <italic>i</italic><sub>2</sub> (in orange) belong to the same cluster A while <italic>i</italic><sub>3</sub> and <italic>i</italic><sub>4</sub> (light blue) belong to another cluster B. The mean of cluster A is in red; that of cluster B is in deep blue. (c) Using “shape-respecting clustering”, <italic>i</italic><sub>1</sub> and <italic>i</italic><sub>3</sub> (in orange) belong to cluster A while <italic>i</italic><sub>2</sub> and <italic>i</italic><sub>4</sub> (light blue) belong to cluster B. The shape-respecting mean is in red for cluster A and in deep blue for cluster B.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150738.g001" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec006">
<title>1.2 Shape respecting tools</title>
<p>The problems of trajectories with similar shapes were mainly addressed in two ways: i) distances and ii) means.</p>
<p>Intuitively, a distance is a function that takes two individuals and returns a number. The number should have a low value when the two individuals are close and a high value when the two individuals are distant from each other. A shape-respecting distance is a distance that takes a small value when individuals have trajectories with similar shapes but a big value otherwise.</p>
<p>Several shape-respecting distances have been proposed in the literature. The most studied are the Fréchet distance [<xref ref-type="bibr" rid="pone.0150738.ref014">14</xref>] and the Dynamic Time Warping [<xref ref-type="bibr" rid="pone.0150738.ref015">15</xref>–<xref ref-type="bibr" rid="pone.0150738.ref018">18</xref>] but there are many other alternatives like HCCA [<xref ref-type="bibr" rid="pone.0150738.ref019">19</xref>] or EDR (Edit Distance is Real sequence) [<xref ref-type="bibr" rid="pone.0150738.ref020">20</xref>] or longest common subsequence [<xref ref-type="bibr" rid="pone.0150738.ref021">21</xref>].</p>
<p>The problem of the mean respecting the form of trajectories is more complex. Many solutions exist. Curve alignment consists in deforming the trajectories so as to align some specific points (minimums, maximums, inflexion points) [<xref ref-type="bibr" rid="pone.0150738.ref022">22</xref>–<xref ref-type="bibr" rid="pone.0150738.ref027">27</xref>]. In a second step, the deformed curves are modeled according to mixture modeling. Or a simple Euclidean means is computed on the deformed trajectory.</p>
<p>More recently, for a higher efficiency, a number of authors [<xref ref-type="bibr" rid="pone.0150738.ref028">28</xref>, <xref ref-type="bibr" rid="pone.0150738.ref029">29</xref>] have chosen to partition and align jointly. Currently, they are only few articles on method performance comparisons [<xref ref-type="bibr" rid="pone.0150738.ref027">27</xref>, <xref ref-type="bibr" rid="pone.0150738.ref030">30</xref>]; most articles tend to show that one of the most efficient methods is fdakma [<xref ref-type="bibr" rid="pone.0150738.ref031">31</xref>].</p>
<p>Unfortunately, most methods suffer from various weaknesses; mainly, they are efficient only in populations with well-separated clusters and limited shifts.</p>
</sec>
<sec id="sec007">
<title>1.3 Clustering according to shape</title>
<p>Using k-means with a classic distance does not allow solving the similar-shape clustering problem. But using a shape distance does not allow solving it either. Indeed, the use of the shape distance will form correctly the clusters by grouping individuals whose trajectories have similar shapes, but the mean trajectory of each cluster will not necessarily be representative of the group. Thus, the following iterations are affected.</p>
<p>
<xref ref-type="fig" rid="pone.0150738.g002">Fig 2</xref> gives an illustration of the impact of the methods on the partitioning process. The population is shown <xref ref-type="fig" rid="pone.0150738.g002">Fig 2.a</xref>. It is a mixture of two groups of trajectories: one whose tops are high (between 0.75 and 0.85) and the other whose tops are lower (between 0.35 and 0.45). The objective of the algorithm is to identify the two groups. During the initialization phase, two individuals are randomly chosen (red and blue, <xref ref-type="fig" rid="pone.0150738.g002">Fig 2.b</xref>). The expectation phase assigns each individual to the closest cluster. By using the Euclidean distance, both individuals <italic>i</italic><sub>1</sub> and <italic>i</italic><sub>2</sub> are close to the red individual and will be classified in the red group whereas <italic>i</italic><sub>3</sub> and <italic>i</italic><sub>4</sub> will be classified in the blue group. This method leads to the partition presented <xref ref-type="fig" rid="pone.0150738.g002">Fig 2.c</xref>, then to the mean trajectories shown <xref ref-type="fig" rid="pone.0150738.g002">Fig 2.e</xref>. This partition does not find the two groups that constituted the initial population. Using a shape-respecting distance, individuals <italic>i</italic><sub>1</sub> and <italic>i</italic><sub>3</sub> are close to the individual in blue and will be classified in the blue group whereas <italic>i</italic><sub>2</sub> and <italic>i</italic><sub>4</sub> will be classified in the red group. This method leads to the partition presented <xref ref-type="fig" rid="pone.0150738.g002">Fig 2.d</xref>. Now, using a conventional way to compute the mean leads to find the mean trajectories presented <xref ref-type="fig" rid="pone.0150738.g002">Fig 2.f</xref>. The groups identified this way are correct, but the mean trajectories are not representative. The use of a shape-respecting mean leads to find the mean trajectories shown in <xref ref-type="fig" rid="pone.0150738.g002">Fig 2.g</xref>. The groups are correct and the mean trajectories are representative of the groups.</p>
<fig id="pone.0150738.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150738.g002</object-id>
<label>Fig 2</label>
<caption>
<title>The impact of using the classical distance, the classical mean, the Fr´echet distance and the Fr´echet mean.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150738.g002" xlink:type="simple"/>
</fig>
<p>In the present article, we introduce kmlShape, a new partitioning method that clusters trajectories according to their shapes.</p>
<p>This method is based on a variation of the k-means algorithms in which we use a “shape-respecting distance” and a “shape-respecting mean”. Regarding the shape-respecting distance, we define a new method, the “generalized distance of Fréchet” which is a generalization of both the Fréchet distance and the Dynamic Time Warping. Regarding the shape-respecting mean, we define a new curve alignment solution. It is based on the construction of the Fréchet mean between two curves, then between n curves.</p>
<p>This method can be time consuming in case of large datasets. We introduce thus two methods that reduce the data size while keeping the essential information contained in the initial trajectories. The use of both data reduction and kmlShape yields a partitioning method that preserves the shapes of the trajectories and may be used with high-dimensional data.</p>
<p>The sections below are organized as follows: we present first the methods used to partition the trajectories according to their shapes. Next, the performances of the methods are evaluated with artificial and real data. Then we discuss the results, the quality of the partitioning on the artificial data, and the originality of the method with real data; i.e., the ability of the algorithm to reveal clusters that are undetectable with the classical methods.</p>
</sec>
</sec>
<sec id="sec008" sec-type="materials|methods">
<title>2 Methods</title>
<sec id="sec009">
<title>2.1 General considerations</title>
<sec id="sec010">
<title>2.1.1 Notation</title>
<p>Let us consider a set <italic>S</italic> of <italic>n</italic> subjects. For each subject <italic>i</italic>, an outcome variable <italic>Y</italic> is measured <italic>t</italic> times. For the sake of simplicity, we consider that all trajectories have the same number of measurements <italic>t</italic> though trajectories with different numbers of measurements do not add complexity to the algorithm. The time of the <italic>j</italic><sup><italic>th</italic></sup> measurement for subject <italic>i</italic> is noted <italic>x</italic><sub><italic>ij</italic></sub>. The value of the <italic>j</italic><sup><italic>th</italic></sup> measurement for subject <italic>i</italic> is noted <italic>y</italic><sub><italic>ij</italic></sub>. The sequence <inline-formula id="pone.0150738.e001"><alternatives><graphic id="pone.0150738.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:msub><mml:mi>Y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msub><mml:mi>y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msub><mml:mi>y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msub><mml:mi>y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> is called a trajectory.</p>
</sec>
<sec id="sec011">
<title>2.1.2 k-means using shape-respecting distances</title>
<p>k-means is a partitioning algorithm that belongs to Classification-Expectation-Maximization (CEM) methods [<xref ref-type="bibr" rid="pone.0150738.ref032">32</xref>]. This algorithm was first used with classical data [<xref ref-type="bibr" rid="pone.0150738.ref033">33</xref>, <xref ref-type="bibr" rid="pone.0150738.ref034">34</xref>] but is now widely used with longitudinal data in various fields [<xref ref-type="bibr" rid="pone.0150738.ref005">5</xref>, <xref ref-type="bibr" rid="pone.0150738.ref035">35</xref>–<xref ref-type="bibr" rid="pone.0150738.ref040">40</xref>]. The principle of k-means is to alternate two steps: i) an Expectation step that calculates the distances between the individual trajectories and the mean trajectories of each cluster; then each individual is assigned to the closest cluster; ii) a Maximization step that estimates the mean trajectory of each cluster. Before alternating these two steps, an initialization phase defines the “mean trajectories of each cluster” that will be used in the first maximization step. Various initialization methods are possible, as detailed in [<xref ref-type="bibr" rid="pone.0150738.ref041">41</xref>–<xref ref-type="bibr" rid="pone.0150738.ref044">44</xref>]. Here, we use the classical method that selects randomly <italic>k</italic> inidividuals and considers them as the <italic>k</italic> first clusters’ centers.</p>
<p>kmlShape is a new clustering algorithm that clusters trajectories according to their shape. It applies k-means within the context of a shape-respecting partitioning. As briefly reminded here, method k-means uses two tools: a distance and a mean. kmlShape requires both a distance and a mean that take the shapes into account. These tools (Fréchet distance and Fréchet means) are presented in the next section.</p>
<p>Overall, kmlShape is a variant of k-means using: i) the Fréchet distance to calculate the distances between individuals and cluster centers; ii) Fréchet mean to construct the centers of the clusters. The stopping condition is the stability of the algorithm: when the clusters are identical at step <italic>s</italic> and step <italic>s</italic> − 1, the algorithm is terminated (with a limitation of the number of iterations to avoid very long times before convergence). The pseudo code of the algorithm is given in Algorithm 1.</p>
<p specific-use="line"><bold>Data:</bold> Population: <italic>n</italic> individuals <italic>Y</italic><sub>1</sub>, … <italic>Y</italic><sub><italic>n</italic></sub></p>
<p specific-use="line"><bold>Result:</bold> Partition: <italic>Cluster</italic> vector of size <italic>n</italic> taking values in [1..<italic>k</italic>]</p>
<p specific-use="line">/* Step 0: Initialization                                      */</p>
<p specific-use="line"><italic>k</italic> individuals <italic>C</italic><sub>1</sub>, <italic>C</italic><sub>2</sub>, …, <italic>C</italic><sub><italic>k</italic></sub> are randomly chosen in <italic>Y</italic><sub>1</sub>, … <italic>Y</italic><sub><italic>n</italic></sub></p>
<p specific-use="line"><italic>s</italic> ← 0</p>
<p specific-use="line"><italic>Cluster</italic><sub>0</sub> ← (0, 0, …, 0) /* vector of size <italic>n</italic>                              */</p>
<p specific-use="line"><bold>repeat</bold></p>
<p specific-use="line"> <italic>s</italic> ← <italic>s</italic> + 1</p>
<p specific-use="line"> /* Step s.1, phase expectation                                  */</p>
<p specific-use="line">  <bold>for</bold> <italic>i in 1</italic>..<italic>n</italic> <bold>do</bold></p>
<p specific-use="line">   <bold>for</bold> <italic>j in 1</italic>..<italic>k</italic> <bold>do</bold></p>
<p specific-use="line">    Compute <italic>DistF</italic><sub><italic>i</italic>, <italic>j</italic></sub> (The Fréchet distance between <italic>Y</italic><sub><italic>i</italic></sub> and <italic>C</italic><sub><italic>j</italic></sub>)</p>
<p specific-use="line">    <italic>Clusters</italic><sub><italic>S</italic></sub>(<italic>i</italic>)←<italic>j</italic> such that <italic>DistF</italic><sub><italic>i</italic>, <italic>j</italic></sub> is smaller than <italic>Dist</italic><sub><italic>i, j</italic>′</sub> for <italic>j</italic>′ ≠ <italic>j</italic></p>
<p specific-use="line">   <bold>end</bold></p>
<p specific-use="line">  <bold>end</bold></p>
<p specific-use="line">  /* Step s.2, phase maximization                                */</p>
<p specific-use="line">  <bold>for</bold> <italic>j in 1</italic>..<italic>k</italic> <bold>do</bold></p>
<p specific-use="line">   Compute <italic>M</italic><sub><italic>j</italic></sub>, the Fréchet mean of clusters <italic>j</italic> (that is the Fréchet mean of all the <italic>Y</italic><sub><italic>i</italic></sub> such that <italic>Cluster</italic><sub><italic>S</italic></sub>(<italic>i</italic>) == <italic>j</italic>)</p>
<p specific-use="line">  <bold>end</bold></p>
<p specific-use="line"><bold>until</bold> <italic>Cluster</italic><sub><italic>S</italic></sub> == <italic>Cluster</italic><sub><italic>S</italic> − 1</sub> or <italic>s</italic> &gt;<italic>Max_Iteration</italic></p>
<p specific-use="line">            <bold>Algorithm 1:</bold> kmlShape</p>
</sec>
</sec>
<sec id="sec012">
<title>2.2 Extension to the Fréchet distance</title>
<sec id="sec013">
<title>2.2.1 Fréchet distance</title>
<p>The Fréchet distance was introduced by Maurice Fréchet in [<xref ref-type="bibr" rid="pone.0150738.ref014">14</xref>]. Informally, it is often compared to a leash between two trajectories. The Fréchet distance is the minimum length of a leash that would separate a master from his dog walking at different speeds along two trajectories. In other words, each point of each trajectory is associated with the nearest point on the other trajectory. The Fréchet distance is then the longest link between the two trajectories.</p>
<p>Mathematically: let <inline-formula id="pone.0150738.e002"><alternatives><graphic id="pone.0150738.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mrow><mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>x</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msub><mml:mi>y</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msub><mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula> be the Euclidian distance between points <inline-formula id="pone.0150738.e003"><alternatives><graphic id="pone.0150738.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>y</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0150738.e004"><alternatives><graphic id="pone.0150738.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. Let <italic>P</italic> and <italic>Q</italic> be two curves from [0, <italic>t</italic>] to <inline-formula id="pone.0150738.e005"><alternatives><graphic id="pone.0150738.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mi mathvariant="double-struck">R</mml:mi></mml:math></alternatives></inline-formula>. A reparameterization of the interval [0, <italic>t</italic>] is a continuous function, increasing and surjective from [0, <italic>t</italic>] to [0, <italic>t</italic>]. We denote <inline-formula id="pone.0150738.e006"><alternatives><graphic id="pone.0150738.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:mi mathvariant="script">A</mml:mi></mml:math></alternatives></inline-formula> the set of all reparameterizations of [0, <italic>t</italic>]. Let <italic>α</italic> and <italic>β</italic> two reparameterizations in <inline-formula id="pone.0150738.e007"><alternatives><graphic id="pone.0150738.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mi mathvariant="script">A</mml:mi></mml:math></alternatives></inline-formula> and let <italic>s</italic> be a real belonging to [0, <italic>t</italic>].</p>
<p>Intuitively, curve <italic>P</italic> can be regarded as the mobile trajectory that would travel at constant speed (e.g., two centimeter per second). So <italic>P</italic> ∘ <italic>α</italic> is the same trajectory as <italic>P</italic>, but covered by the mobile with a variable speed, speed defined by <italic>α</italic> (e.g., <italic>α</italic> can set 1 centimeter per second as 1 ≤ <italic>s</italic> ≤ 3 and then 3 centimeters per second as 3 &lt; <italic>s</italic> ≤ <italic>t</italic>).</p>
<p>The distance between curves <italic>P</italic> and <italic>Q</italic> reparameterized by <italic>α</italic> and <italic>β</italic> at time <italic>s</italic> is the distance between <inline-formula id="pone.0150738.e008"><alternatives><graphic id="pone.0150738.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>α</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>s</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mi>P</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>α</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>s</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0150738.e009"><alternatives><graphic id="pone.0150738.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>β</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>s</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mi>Q</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>β</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>s</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, that is <inline-formula id="pone.0150738.e010"><alternatives><graphic id="pone.0150738.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:mrow><mml:msub><mml:mi>d</mml:mi> <mml:mrow><mml:mi>α</mml:mi> <mml:mo>,</mml:mo> <mml:mi>β</mml:mi> <mml:mo>,</mml:mo> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>P</mml:mi> <mml:mo>,</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>d</mml:mi> <mml:mo>(</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>α</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>α</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>β</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mi>Q</mml:mi> <mml:mo>(</mml:mo> <mml:mi>β</mml:mi> <mml:mo>(</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The distance between <italic>P</italic> and <italic>Q</italic> reparameterized by <italic>α</italic> and <italic>β</italic> is the maximum of the distances <italic>d</italic><sub><italic>α</italic>, <italic>β</italic>, <italic>s</italic></sub>(<italic>P</italic>, <italic>Q</italic>) while <italic>s</italic> varies from 0 to <italic>t</italic>: <italic>d</italic><sub><italic>α</italic>, <italic>β</italic></sub>(<italic>P</italic>, <italic>Q</italic>) = <italic>Max</italic><sub><italic>s</italic></sub>(<italic>d</italic><sub><italic>α</italic>, <italic>β</italic>, <italic>s</italic></sub>(<italic>P</italic>, <italic>Q</italic>)). Then the Fréchet distance between <italic>P</italic> and <italic>Q</italic> is the smallest possible maximum between <italic>P</italic> and <italic>Q</italic> after reparameterization of <italic>P</italic> and <italic>Q</italic>: <italic>DistFrechet</italic>(<italic>P</italic>, <italic>Q</italic>) = <italic>d</italic><sub><italic>α</italic>, <italic>β</italic></sub>(<italic>P</italic>, <italic>Q</italic>).</p>
<p>The definition is the same in the discrete case with the exception that <italic>s</italic> takes values between 0 and <italic>t</italic> by intervals. Note that, contrarily to several classical distances, the calculation of Fréchet distance does not require the same number of measurements or the same time points on the two trajectories. Therefore, it can be used to cluster irregular trajectories, the use of imputation methods for longitudinal data [<xref ref-type="bibr" rid="pone.0150738.ref045">45</xref>–<xref ref-type="bibr" rid="pone.0150738.ref048">48</xref>] is not necessary.</p>
<p>From a computational point of view, the Fréchet distance is rather easy to determine [<xref ref-type="bibr" rid="pone.0150738.ref049">49</xref>] but the calculation time is longer than that required for Euclidian distance: <italic>O</italic>(<italic>t</italic><sup>2</sup>) (the details of all the computational complexity are given in Appendix).</p>
</sec>
<sec id="sec014">
<title>The generalized Fréchet distance</title>
<p>Fréchet has given the seminal definition within the context of two mathematical curves <italic>P</italic> and <italic>Q</italic>. Within the context of real data, there is a relative-scale issue. The variable of interest and the time variable are not measured using the same unit. This can be an important issue since a scale changes impact the Fréchet distance. <xref ref-type="fig" rid="pone.0150738.g003">Fig 3.a</xref> shows three trajectories. According to the Fréchet distance, <italic>i</italic><sub>1</sub> is closer to <italic>i</italic><sub>2</sub> than to <italic>i</italic><sub>3</sub> (the segments that materialize the distances between the trajectories are dotted). If the scale of the X-axis is changed (<xref ref-type="fig" rid="pone.0150738.g003">Fig 3.b</xref>), <italic>i</italic><sub>1</sub> will be closer to <italic>i</italic><sub>3</sub> than to <italic>i</italic><sub>2</sub>.</p>
<fig id="pone.0150738.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150738.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Scale change.</title>
<p>(a) <italic>i</italic><sub>1</sub> is closer to <italic>i</italic><sub>2</sub> than to <italic>i</italic><sub>3</sub> (b) <italic>i</italic><sub>1</sub> is closer to <italic>i</italic><sub>3</sub> than to <italic>i</italic><sub>2</sub>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150738.g003" xlink:type="simple"/>
</fig>
<p>This scale-change is not trivial because it impacts the partitioning. This lead to the following definition: the generalized Fréchet distance of parameter lambda between two curves <italic>P</italic> and <italic>Q</italic> is the Fréchet distance obtained after an affine transformation <inline-formula id="pone.0150738.e011"><alternatives><graphic id="pone.0150738.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mrow><mml:mi>A</mml:mi> <mml:mo>:</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>x</mml:mi></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mi>y</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>→</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>λ</mml:mi> <mml:mo>.</mml:mo> <mml:mi>x</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mi>y</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>; that is, <inline-formula id="pone.0150738.e012"><alternatives><graphic id="pone.0150738.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:mrow><mml:mi>D</mml:mi> <mml:mi>i</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mi>F</mml:mi> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>c</mml:mi> <mml:mi>h</mml:mi> <mml:mi>e</mml:mi> <mml:msub><mml:mi>t</mml:mi> <mml:mi>λ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>P</mml:mi> <mml:mo>,</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>I</mml:mi> <mml:mi>n</mml:mi> <mml:msub><mml:mi>f</mml:mi> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>α</mml:mi> <mml:mo>,</mml:mo> <mml:mi>β</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∈</mml:mo> <mml:msup><mml:mrow><mml:mi mathvariant="script">A</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msub> <mml:mi>M</mml:mi> <mml:mi>a</mml:mi> <mml:mi>x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>P</mml:mi> <mml:mo>∘</mml:mo> <mml:mi>α</mml:mi> <mml:mo>∘</mml:mo> <mml:mi>A</mml:mi> <mml:mo>,</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>∘</mml:mo> <mml:mi>β</mml:mi> <mml:mi>o</mml:mi> <mml:mi>A</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>. This is what we called <italic>the generalized Fréchet distance</italic>. <italic>λ</italic> is the time-scale parameters.</p>
<p>One should notice that when <italic>λ</italic> = 0, the Fréchet distance matches with the Dynamic Time Warping (DTW) distance (i.e., as in DTW, horizontal shifts have no impacts. See <xref ref-type="sec" rid="sec034">appendix A</xref> for more details). On the opposite, when <italic>λ</italic> tends to +∞, then <italic>DistFrechet</italic><sub><italic>λ</italic></sub> tends toward the classical maximum distance.</p>
<p>Therefore, the generalized Fréchet distance is a generalization of shape-respecting distance (like DTW) but also of other classical distances (Maximum). Herein, for the sake of simplicity, the generalized Fréchet distance will be referred as to the Fréchet distance.</p>
</sec>
<sec id="sec015">
<title>The Fréchet mean between two trajectories</title>
<p>As mentioned above, Classification-Expectation-Maximization algorithms require the calculation of a mean. Informally, the Fréchet mean between two trajectories is the middle of the leash that links the dog to the master when each goes along its own way.</p>
<p>More precisely, calculating the Fréchet distance requires the explicit calculation of the two reparameterizations <italic>α</italic> and <italic>β</italic> that minimize <italic>DistFrechet</italic><sub><italic>λ</italic></sub>(<italic>P</italic>, <italic>Q</italic>). Using these two functions, it is obvious to define the Fréchet mean as the mean of the distances between the points of the two trajectories when these trajectories are run at speeds <italic>α</italic> and <italic>β</italic>: <inline-formula id="pone.0150738.e013"><alternatives><graphic id="pone.0150738.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mrow><mml:mi>M</mml:mi> <mml:mi>e</mml:mi> <mml:mi>a</mml:mi> <mml:mi>n</mml:mi> <mml:mi>F</mml:mi> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>c</mml:mi> <mml:mi>h</mml:mi> <mml:mi>e</mml:mi> <mml:msub><mml:mi>t</mml:mi> <mml:mi>λ</mml:mi></mml:msub> <mml:mo stretchy="false">(</mml:mo> <mml:mi>P</mml:mi> <mml:mo>,</mml:mo> <mml:mi>Q</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>∘</mml:mo> <mml:mi>α</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>λ</mml:mi> <mml:mo>.</mml:mo> <mml:mi>x</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mn>1</mml:mn> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="false">)</mml:mo> <mml:mo>+</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>∘</mml:mo> <mml:mi>β</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>λ</mml:mi> <mml:mo>.</mml:mo> <mml:mi>x</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mn>1</mml:mn> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="false">)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>∘</mml:mo> <mml:mi>α</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>y</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mn>1</mml:mn> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="false">)</mml:mo> <mml:mo>+</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>∘</mml:mo> <mml:mi>β</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>y</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mn>1</mml:mn> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="false">)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> </mml:mrow> <mml:mo>,</mml:mo> <mml:mo>…</mml:mo> <mml:mo>,</mml:mo> <mml:mrow> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>∘</mml:mo> <mml:mi>α</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>λ</mml:mi> <mml:mo>.</mml:mo> <mml:mi>x</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>a</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="false">)</mml:mo> <mml:mo>+</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>∘</mml:mo> <mml:mi>β</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>λ</mml:mi> <mml:mo>.</mml:mo> <mml:mi>x</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>a</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="false">)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>∘</mml:mo> <mml:mi>α</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>y</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>a</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="false">)</mml:mo> <mml:mo>+</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>∘</mml:mo> <mml:mi>β</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>y</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mi>a</mml:mi> <mml:mo stretchy="false">)</mml:mo> <mml:mo stretchy="false">)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> that we will write <inline-formula id="pone.0150738.e014"><alternatives><graphic id="pone.0150738.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:mi>M</mml:mi> <mml:mi>e</mml:mi> <mml:mi>a</mml:mi> <mml:mi>n</mml:mi> <mml:mi>F</mml:mi> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>c</mml:mi> <mml:mi>h</mml:mi> <mml:mi>e</mml:mi> <mml:msub><mml:mi>t</mml:mi> <mml:mi>λ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>P</mml:mi> <mml:mo>,</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>∘</mml:mo> <mml:mi>α</mml:mi> <mml:mo>∘</mml:mo> <mml:mi>A</mml:mi> <mml:mo>+</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>∘</mml:mo> <mml:mi>β</mml:mi> <mml:mo>∘</mml:mo> <mml:mi>A</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula></p>
<p>An example of the Fréchet mean is given <xref ref-type="fig" rid="pone.0150738.g004">Fig 4</xref>.</p>
<fig id="pone.0150738.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150738.g004</object-id>
<label>Fig 4</label>
<caption>
<title>The Fréchet mean.</title>
<p>Two trajectories <italic>P</italic> and <italic>Q</italic> are in red and blue, respectively. The segments linking <italic>P</italic> to <italic>Q</italic> after reparameterization are in black. The mean trajectory as defined by the middles of these segments is in violet.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150738.g004" xlink:type="simple"/>
</fig>
<p>The Fréchet mean of the two curves <italic>P</italic> and <italic>Q</italic> weighted by coefficients <italic>p</italic> and <italic>q</italic> works on the same principle with a weighting on each curve: <inline-formula id="pone.0150738.e015"><alternatives><graphic id="pone.0150738.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e015" xlink:type="simple"/><mml:math display="inline" id="M15"><mml:mrow><mml:mi>M</mml:mi> <mml:mi>e</mml:mi> <mml:mi>a</mml:mi> <mml:mi>n</mml:mi> <mml:mi>F</mml:mi> <mml:mi>r</mml:mi> <mml:mi>e</mml:mi> <mml:mi>c</mml:mi> <mml:mi>h</mml:mi> <mml:mi>e</mml:mi> <mml:msub><mml:mi>t</mml:mi> <mml:mi>λ</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>P</mml:mi> <mml:mo>,</mml:mo> <mml:mi>p</mml:mi> <mml:mo>,</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>,</mml:mo> <mml:mi>q</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>.</mml:mo> <mml:mi>P</mml:mi> <mml:mo>∘</mml:mo> <mml:mi>α</mml:mi> <mml:mo>∘</mml:mo> <mml:mi>A</mml:mi> <mml:mo>+</mml:mo> <mml:mi>q</mml:mi> <mml:mo>.</mml:mo> <mml:mi>Q</mml:mi> <mml:mo>∘</mml:mo> <mml:mi>β</mml:mi> <mml:mo>∘</mml:mo> <mml:mi>A</mml:mi></mml:mrow> <mml:mrow><mml:mi>p</mml:mi> <mml:mo>+</mml:mo> <mml:mi>q</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</sec>
<sec id="sec016">
<title>Generalization to n curves</title>
<p>The definition of the Fréchet mean may be extended to <italic>n</italic> curves. However, the complexity of the algorithm (<italic>O</italic>(<italic>t</italic><sup><italic>n</italic></sup>)) would not be realistic for the analysis of real data, even of very small size.</p>
<p>However, the Fréchet mean with <italic>n</italic> curves may be approximated with less complexity. The calculation of the Fréchet mean between two curves is reasonable (<italic>O</italic>(<italic>t</italic><sup>2</sup>)). In a population of <italic>n</italic> individuals, it is possible to combine pairs of individuals (with weight 1), then combine the so-obtained means (weighted by the number of individuals that generated them) until obtaining a unique mean. The calculation cost of this “step by step” mean is <italic>O</italic>(<italic>n</italic>.<italic>t</italic><sup>2</sup>).</p>
<p>Obviously, the order in which the combinations are made has an impact on the final result. Let us mention three possible variants:</p>
<list list-type="bullet">
<list-item>
<p><bold>RandomAll</bold>: the <italic>n</italic> individuals are randomly scattered on the leaves of a complete binary tree (each knots has either zero or two leaves) having depth <italic>h</italic> with 2<sup><italic>h</italic></sup> ≤ <italic>n</italic> &lt; 2<sup><italic>h</italic>+1</sup>. The value of each non-terminal leaf is the mean of the two children-leaves. The Fréchet mean is thus the value of the tree root. (Informally, this structure is close to that of a tennis tournament). The complexity of this method is <italic>O</italic>(<italic>nt</italic><sup>2</sup>).</p>
</list-item>
<list-item>
<p><bold>Hierarchical</bold>: the combination order between individuals is fixed in a deterministic way through an ascending hierarchical classification; the closest individuals being combined first. The complexity of this method is <italic>O</italic>(<italic>n</italic><sup>2</sup> <italic>t</italic><sup>2</sup>).</p>
</list-item>
<list-item>
<p><bold>RandomSubset</bold>: This method is the RandomAll method applied to a sample of randomly selected individuals. The complexity of the method is <italic>O</italic>(<italic>n</italic><sub>0</sub> <italic>t</italic><sup>2</sup>), <italic>n</italic><sub>0</sub> being the size of the random sample.</p>
</list-item>
</list>
<p>The means obtained through RandomAll and Hierarchical are very close and, in the case of simulations with artificial data, are also very close to the real mean. The choice of one of theses two methods has thus no impact on the final partitioning. On the contrary, the performance of RandomSubset is dependent on the sample size. Besides, the Hierarchical method is deterministic, which, in the case of an algorithm run several time (such as k-means) is a disadvantage because, in case of convergence toward a local maximum, an additional run of the algorithm will lead to the same maximum. Finally, its complexity is <italic>O</italic>(<italic>n</italic><sup>2</sup> <italic>t</italic><sup>2</sup>) whereas that of randomAll is <italic>O</italic>(<italic>nt</italic><sup>2</sup>). Thus, it is RandomAll that should be preferred.</p>
</sec>
</sec>
<sec id="sec017">
<title>2.3 Data size reduction</title>
<sec id="sec018">
<title>Reduction of the number of individuals</title>
<p>The use of the Fréchet mean approximation shifts the complexity of our first algorithm from <italic>O</italic>(<italic>t</italic><sup><italic>n</italic></sup>) to <italic>O</italic>(<italic>nt</italic><sup>2</sup>). This is an important gain, however insufficient for applying the method to large databases (thousand or tens of thousand individuals). One optimization option is to reduce the number of individuals by an identification of a small number of comparable trajectories. This suggestion of simplification is based on two facts: i) in large populations, some groups of individuals have close trajectories (because the limited number of typical trajectories); this is all the more true as the population becomes larger; ii) when two trajectories are very close, the Euclidian mean and the Fréchet mean are close (see <xref ref-type="fig" rid="pone.0150738.g005">Fig 5</xref>). It becomes then locally satisfactory to approximate the Fréchet mean through the Euclidian mean.</p>
<fig id="pone.0150738.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150738.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Comparison between Euclidian mean and the Fréchet mean in case of two close curves.</title>
<p>The means are almost identical.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150738.g005" xlink:type="simple"/>
</fig>
<p>With these facts, in case of large populations, it is convenient to replace close groups of individuals by representatives (in the same way senators represent populations of states). In addition, the Fréchet mean may be approximated through the Euclidian mean without changing the forms of the trajectories. In the end, this reduction of the number of individuals may be obtained using a classical classification algorithm such as k-means with a Euclidian distance.</p>
<p>Practically, k-means is carried out on a population of size <italic>n</italic>, with, say, <italic>n</italic><sub><italic>S</italic></sub> = 128 groups. This is equivalent to make the not very constraining hypothesis that there is a set of 128 representative trajectories so that each individual trajectory is close to at least one of them. The cost of this preliminary classification (we would conveniently name “election”) is <italic>O</italic>(<italic>n</italic><sub><italic>S</italic></sub> <italic>nt</italic>). Afterwards, kmlShape with weighting may be used with <italic>n</italic><sub><italic>S</italic></sub> senators stemming from the election. The cost of kmlShape is then <italic>O</italic>(<italic>n</italic><sub><italic>S</italic></sub> <italic>t</italic><sup>2</sup>). The overall complexity is <italic>O</italic>(<italic>n</italic><sub><italic>S</italic></sub> <italic>nt</italic> + <italic>n</italic><sub><italic>S</italic></sub> <italic>t</italic><sup>2</sup>).</p>
</sec>
<sec id="sec019">
<title>Reduction of the number of measurements</title>
<p>In an orthogonal way, it is generally possible to simplify the trajectories by reducing the number of measurements made without much loss of information. These techniques are known as “Segmentation Time Series” [<xref ref-type="bibr" rid="pone.0150738.ref050">50</xref>, <xref ref-type="bibr" rid="pone.0150738.ref051">51</xref>], “Line-simplification” [<xref ref-type="bibr" rid="pone.0150738.ref052">52</xref>] or “Trajectories compression” [<xref ref-type="bibr" rid="pone.0150738.ref053">53</xref>] In his survey, Keogh proposes three kinds of methods: the Sliding Windows, the Top-Down, and the Bottom-up. For our purpose, the Top-Down are the ones that have the best complexity. In this article, we will focus on Douglas-Peuker algorithm [<xref ref-type="bibr" rid="pone.0150738.ref054">54</xref>], also known as Ramers algorithm [<xref ref-type="bibr" rid="pone.0150738.ref055">55</xref>] or “Iterative End-Points Fits” [<xref ref-type="bibr" rid="pone.0150738.ref056">56</xref>].</p>
<p>Let us consider a trajectory <italic>Y</italic> of length <italic>t</italic> and an <italic>ϵ</italic>. The Douglas-Peuker algorithm [<xref ref-type="bibr" rid="pone.0150738.ref054">54</xref>] allows finding a curve <italic>Y</italic><sup><italic>DP</italic></sup> of length <italic>t</italic><sub><italic>DP</italic></sub> ≤ <italic>t</italic> so that the distance (projection of one point of one curve on the other curve) between <italic>Y</italic> and <italic>Y</italic><sup><italic>DP</italic></sup> is, in each point, less than <italic>ϵ</italic>. The Douglas-Peuker algorithm is recursive; as long as the simplified trajectory <italic>Y</italic><sup><italic>DP</italic></sup> is not at a distance less than epsilon from the original trajectory <italic>Y</italic>, the point of <italic>Y</italic> the farthest from <italic>Y</italic><sup><italic>DP</italic></sup> is added to <italic>Y</italic><sup><italic>DP</italic></sup>. This algorithm makes it possible to set the quality of the approximation of <italic>Y</italic> through <italic>Y</italic><sup><italic>DP</italic></sup>. Note that many amelioration of this algorithm exist [<xref ref-type="bibr" rid="pone.0150738.ref053">53</xref>].</p>
<p>In our present problematic, it may be more interesting to set the adequate length <italic>t</italic><sub><italic>DP</italic></sub> for the simplified trajectory because this length has a direct impact on the computation time. This may be obtained through a simple modification of Douglas-Peuker algorithm. Instead of considering a calculation-stopping condition that depends on the distance between <italic>Y</italic> and <italic>Y</italic><sup><italic>DP</italic></sup>, we may choose to set the maximum number of points for <italic>Y</italic><sup><italic>DP</italic></sup>: as long as the simplified trajectory <italic>Y</italic><sup><italic>DP</italic></sup> has less than <italic>t</italic><sub><italic>DP</italic></sub> points, the point of <italic>Y</italic> the farthest from <italic>Y</italic><sup><italic>DP</italic></sup> is added to <italic>Y</italic><sup><italic>DP</italic></sup>. With <italic>n</italic><sub><italic>S</italic></sub> individuals, the complexity of this algorithm is <inline-formula id="pone.0150738.e016"><alternatives><graphic id="pone.0150738.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e016" xlink:type="simple"/><mml:math display="inline" id="M16"><mml:mrow><mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>t</mml:mi> <mml:mrow><mml:mi>D</mml:mi> <mml:mi>P</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:msub><mml:mi>n</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. <xref ref-type="fig" rid="pone.0150738.g006">Fig 6</xref> shows the two types of simplification with 5 and 15 points, respectively.</p>
<fig id="pone.0150738.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150738.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Approximation of a trajectory (in black).</title>
<p>(a) using 5 points; (b) using 15 points.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150738.g006" xlink:type="simple"/>
</fig>
<p>Note that the modification of the stopping condition induces that the error is no longer explicitly controlled. In some specific cases, this might lead to a simplified trajectory that is no longer close to the initial trajectory (for example, the trajectory sin(<italic>t</italic>) with <italic>t</italic> in [0, 3<italic>π</italic>] approximated with only 3 points). To inform the user on the size of the error, the modified Douglas-Peuker algorithm returns the greatest distance between the simplified curve and the initial curve. Thus, if the user has no direct control on the error, he/she has an estimation of it. He/She can then decide whether the size of the error seems too big for him/her to increase the number of points used by the Douglas-Pecker algorithm. The user may feel also free to use the classical Douglas-Pecker algorithm (control the error but not the number of points). In the latter case, the time complexity of the algorithm kmlShape is not guaranteed.</p>
</sec>
</sec>
<sec id="sec020">
<title>2.4 Overall complexity</title>
<p>In the end, the election cost is <italic>O</italic>(<italic>n</italic><sub><italic>S</italic></sub> <italic>n</italic><sub><italic>t</italic></sub>). The cost of senator simplification is <inline-formula id="pone.0150738.e017"><alternatives><graphic id="pone.0150738.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mrow><mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>t</mml:mi> <mml:mrow><mml:mi>D</mml:mi> <mml:mi>P</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:msub><mml:mi>n</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. One may then use kmlShape with the <italic>n</italic><sub><italic>S</italic></sub> simplified senators at cost <inline-formula id="pone.0150738.e018"><alternatives><graphic id="pone.0150738.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:msubsup><mml:mi>t</mml:mi> <mml:mrow><mml:mi>D</mml:mi> <mml:mi>P</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. The overall complexity is <inline-formula id="pone.0150738.e019"><alternatives><graphic id="pone.0150738.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mrow><mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>t</mml:mi> <mml:mrow><mml:mi>D</mml:mi> <mml:mi>P</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:msub><mml:mi>n</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:mi>t</mml:mi> <mml:mo>+</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mi>S</mml:mi></mml:msub> <mml:msubsup><mml:mi>t</mml:mi> <mml:mrow><mml:mi>D</mml:mi> <mml:mi>P</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, <italic>t</italic><sub><italic>DP</italic></sub> and <italic>n</italic><sub><italic>S</italic></sub> being constants set by the user. So, the final complexity is <italic>O</italic>(<italic>nt</italic>).</p>
<p>
<xref ref-type="fig" rid="pone.0150738.g007">Fig 7</xref> summarizes the steps needed to partition data using algorithm kmlShape in a reasonable time.</p>
<fig id="pone.0150738.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150738.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Steps for data partitioning with kmlShape the final complexity is in <italic>O</italic>(<italic>nt</italic>).</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150738.g007" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec021">
<title>3 Performance assessment</title>
<sec id="sec022">
<title>3.1 Simulation study</title>
<sec id="sec023">
<title>Generation of artificial data</title>
<p>Let us consider three populations of small (<italic>n</italic> = 20, <italic>t</italic> = 21), medium (<italic>n</italic> = 40, <italic>t</italic> = 41), and large size (<italic>n</italic> = 500, <italic>t</italic> = 501). Each population has <italic>k</italic> subgroups. Each subgroup <italic>G</italic> is defined by a typical trajectory <italic>y</italic> = <italic>f</italic><sub><italic>G</italic></sub>(<italic>x</italic>). We have considered two cases:</p>
<list list-type="bullet">
<list-item>
<p><bold>Case 1</bold>: two groups A and B (k = 2) with <italic>f</italic><sub><italic>A</italic></sub>(<italic>x</italic>) = <italic>ψ</italic>(<italic>x</italic>, 0.5, 0.1) × 0.125 and <italic>f</italic><sub><italic>B</italic></sub>(<italic>x</italic>) = <italic>ψ</italic>(<italic>x</italic>, 0.5, 0.1) × 0.25.</p>
</list-item>
<list-item>
<p><bold>Case 2</bold>: four groups A, B, C and D (k = 4) with <italic>f</italic><sub><italic>A</italic></sub>(<italic>x</italic>) = <italic>ψ</italic>(<italic>x</italic>, 0.5, 0.1) × 0.125, <italic>f</italic><sub><italic>B</italic></sub>(<italic>x</italic>) = <italic>Φ</italic>(<italic>x</italic>, 0.4, 0.1) × 0.5, <italic>f</italic><sub><italic>C</italic></sub>(<italic>x</italic>) = <italic>ψ</italic>(<italic>x</italic>, 0.5, 0.1) × 0.25 and <italic>f</italic><sub><italic>D</italic></sub>(<italic>x</italic>) = <italic>Φ</italic>(<italic>x</italic>, 0.4, 0.1).</p>
</list-item>
</list>
<p>with <italic>ψ</italic> the normal law distribution <inline-formula id="pone.0150738.e020"><alternatives><graphic id="pone.0150738.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mrow><mml:mi>ψ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mi>s</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mi>s</mml:mi> <mml:msqrt><mml:mrow><mml:mn>2</mml:mn> <mml:mi>π</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac> <mml:mi>e</mml:mi> <mml:mi>x</mml:mi> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mi>x</mml:mi> <mml:mo>-</mml:mo> <mml:mi>m</mml:mi></mml:mrow> <mml:mi>s</mml:mi></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and <italic>Φ</italic> its cumulative distribution function (see <xref ref-type="fig" rid="pone.0150738.g008">Fig 8</xref>).</p>
<fig id="pone.0150738.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150738.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Artificial data.</title>
<p>(a) Case 1: <italic>f</italic><sub><italic>A</italic></sub> is in red, <italic>f</italic><sub><italic>B</italic></sub> in green; (b) Case 2: <italic>f</italic><sub><italic>A</italic></sub> is in red, <italic>f</italic><sub><italic>B</italic></sub> in green, <italic>f</italic><sub><italic>C</italic></sub> in deep blue and <italic>f</italic><sub><italic>D</italic></sub> in light blue.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150738.g008" xlink:type="simple"/>
</fig>
<p>Then each trajectory <italic>y</italic><sub><italic>i</italic></sub> belonging to subgroup <italic>G</italic> is a distortion of <italic>f</italic><sub><italic>G</italic></sub>. To create <italic>y</italic><sub><italic>i</italic></sub>, we choose a coefficient of distortion <italic>σ</italic>. Three types of distortion may be considered:</p>
<list list-type="bullet">
<list-item>
<p><bold>Simple distortion</bold> it consists in a mere horizontal translation of <italic>f</italic><sub><italic>G</italic></sub>: <italic>y</italic><sub><italic>ij</italic></sub> = <italic>f</italic><sub><italic>G</italic></sub>(<italic>x</italic><sub><italic>ij</italic></sub> + <italic>b</italic><sub>1</sub>), with <inline-formula id="pone.0150738.e021"><alternatives><graphic id="pone.0150738.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:mrow><mml:msub><mml:mi>b</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">U</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mi>σ</mml:mi> <mml:mo>,</mml:mo> <mml:mo>+</mml:mo> <mml:mi>σ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula></p>
</list-item>
<list-item>
<p><bold>Multiple distortion</bold> it consists not only in vertical and horizontal translations but also in vertical and horizontal deformations (compression and stretching): <italic>y</italic><sub><italic>ij</italic></sub> = <italic>a</italic><sub>2</sub>.<italic>f</italic><sub><italic>G</italic></sub>(<italic>a</italic><sub>1</sub>.<italic>x</italic><sub><italic>ij</italic></sub> + <italic>b</italic><sub>1</sub>) + <italic>b</italic><sub>2</sub>, with <inline-formula id="pone.0150738.e022"><alternatives><graphic id="pone.0150738.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e022" xlink:type="simple"/><mml:math display="inline" id="M22"><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">U</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>σ</mml:mi> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>σ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0150738.e023"><alternatives><graphic id="pone.0150738.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">U</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mi>σ</mml:mi> <mml:mo>,</mml:mo> <mml:mo>+</mml:mo> <mml:mi>σ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</list-item>
<list-item>
<p><bold>Noisy distortion</bold> it consists in a multiple distortion (as described above) together with a Gaussian random noise: <italic>y</italic><sub><italic>ij</italic></sub> = <italic>a</italic><sub>2</sub>.<italic>f</italic><sub><italic>G</italic></sub>(<italic>a</italic><sub>1</sub>.<italic>x</italic><sub><italic>ij</italic></sub> + <italic>b</italic><sub>1</sub>) + <italic>b</italic><sub>2</sub> + <italic>e</italic><sub><italic>ij</italic></sub>, with <inline-formula id="pone.0150738.e024"><alternatives><graphic id="pone.0150738.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e024" xlink:type="simple"/><mml:math display="inline" id="M24"><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">U</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:mi>σ</mml:mi> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:mi>σ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0150738.e025"><alternatives><graphic id="pone.0150738.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e025" xlink:type="simple"/><mml:math display="inline" id="M25"><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>b</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">U</mml:mi> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mo>-</mml:mo> <mml:mi>σ</mml:mi> <mml:mo>,</mml:mo> <mml:mo>+</mml:mo> <mml:mi>σ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0150738.e026"><alternatives><graphic id="pone.0150738.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e026" xlink:type="simple"/><mml:math display="inline" id="M26"><mml:mrow><mml:msub><mml:mi>e</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mi>σ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</list-item>
</list>
<p>with <inline-formula id="pone.0150738.e027"><alternatives><graphic id="pone.0150738.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e027" xlink:type="simple"/><mml:math display="inline" id="M27"><mml:mrow><mml:mi mathvariant="script">U</mml:mi> <mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>,</mml:mo> <mml:mi>b</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> the uniform distribution with minimum <italic>a</italic> and maximum <italic>b</italic>. For each type of distortion, <italic>σ</italic> takes values in {0.05, 0.1, 0.25}. At the end, (3 possible populations) times (2 possible cases) times (3 possible distortions) times (3 possible <italic>σ</italic>) gives 54 possible datasets. Each dataset was generated 500 times.</p>
<p>Small datasets were partitioned using (1.a) kmlShape (methods randomAll, <italic>λ</italic> = 0.1), (1.b) kmlShape using DTW (methods randomAll, <italic>λ</italic> = 0), (1.d) classical Euclidian k-means, and (1.e) fdakma [<xref ref-type="bibr" rid="pone.0150738.ref027">27</xref>, <xref ref-type="bibr" rid="pone.0150738.ref031">31</xref>]. Medium datasets were partitioned using (2.a) kmlShape (randomAll, <italic>λ</italic> = 0.1), (2.b) kmlShape using DTW (randomAll, <italic>λ</italic> = 0), (2.c) kmlShape with simplification (<italic>n</italic><sub><italic>S</italic></sub> = 32, <italic>t</italic><sub><italic>DP</italic></sub> = 21, randomAll, <italic>λ</italic> = 0.1), (2.d) classical Euclidian k-means and (2.e) fdakma.</p>
<p>Large datasets were partitioned using (3.c) kmlShape with simplification (<italic>n</italic><sub><italic>S</italic></sub> = 32, <italic>t</italic><sub><italic>DP</italic></sub> = 21, randomAll, <italic>λ</italic> = 0.1) and (3.d) classical Euclidian k-means; the other methods were too time-consuming.</p>
<p>The study of (1.a, 1.b, 1.d, 1.e, 2.a, 2.b, 2.c, 2.d and 2.e) will allow us to compare kmlShape using Fréchet, kmlShape using DTW, classical k-means and fdakma in various conditions. The comparison of (2.a) and (2.c) will allow us to study the impact of the simplification procedures. (3.c) and (3.d) will allow the comparison of kmlShape with simplification and k-means performance on large data set.</p>
<p>The indicated parameters were chosen because they reflect an equilibrium between a slight deformation of the original data (that requires high <italic>n</italic><sub><italic>S</italic></sub> and <italic>t</italic><sub><italic>DP</italic></sub> values) and a reasonable calculation time (that requires low <italic>n</italic><sub><italic>S</italic></sub> and <italic>t</italic><sub><italic>DP</italic></sub> values).</p>
</sec>
</sec>
<sec id="sec024">
<title>3.2 Performance</title>
<p>To measure the performance, we used the Correct Classification Rate (cRate) which is the percentage of agreement between the partitioning found <italic>P</italic> and the true partitioning <italic>P</italic><sub><italic>T</italic></sub>. We have also used the adjusted Rand Index (aRand) [<xref ref-type="bibr" rid="pone.0150738.ref057">57</xref>] which is a variant of the Rand Index [<xref ref-type="bibr" rid="pone.0150738.ref058">58</xref>]; the cRand index being the proportion of pairs of individuals (<italic>i</italic>, <italic>j</italic>) who are either in the same cluster in <italic>P</italic> or in <italic>P</italic><sub><italic>T</italic></sub> or in separate clusters in <italic>P</italic> or in <italic>P</italic><sub><italic>T</italic></sub>. The adjusted rand index is simply aRand = (cRand − theoretical cRand) / (Maximum cRand − theoretical cRand). This adjustment makes the aRand take value 0 when it measures the agreement between two random partitions. These two measures of agreement between classifications have been already used by several authors [<xref ref-type="bibr" rid="pone.0150738.ref005">5</xref>, <xref ref-type="bibr" rid="pone.0150738.ref009">9</xref>, <xref ref-type="bibr" rid="pone.0150738.ref030">30</xref>, <xref ref-type="bibr" rid="pone.0150738.ref059">59</xref>].</p>
</sec>
</sec>
<sec id="sec025" sec-type="results">
<title>4 Results</title>
<sec id="sec026">
<title>4.1 Method comparisons</title>
<p>The respective performances of kmlShape, fdakma, and k-means with small and medium datasets, case 1 and 2, are shown <xref ref-type="table" rid="pone.0150738.t001">Table 1</xref>. We observed that kmlShape performs better than fdakma and k-means regarding the classification indices. The same was found when only one specific subgroup was analyzed (e.g., only Case 1 with <italic>σ</italic> = 0.05). However, the differences between kmlShape and the other methods were more marked in Case 1 than in Case 2. Also, these differences tended to decrease slightly when <italic>σ</italic> increased.</p>
<table-wrap id="pone.0150738.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150738.t001</object-id>
<label>Table 1</label>
<caption>
<title>Performance of classical k-means, fdakma, and kmlShape in case of small and medium datasets (mean value ± standard deviation).</title>
</caption>
<alternatives>
<graphic id="pone.0150738.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150738.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center">Classical k-means</th>
<th align="center">kmlShape</th>
<th align="center">fdakma</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Case 1</td>
<td align="left" colspan="3"/>
</tr>
<tr>
<td align="left"> cRate</td>
<td align="center">0.75 (± 0.12)</td>
<td align="center">0.84 (± 0.15)</td>
<td align="center">0.57 (± 0.07)</td>
</tr>
<tr>
<td align="left"> aRand</td>
<td align="center">0.56 (± 0.2)</td>
<td align="center">0.71 (± 0.26)</td>
<td align="center">0.37 (± 0.1)</td>
</tr>
<tr>
<td align="left">Case 2</td>
<td align="left" colspan="3"/>
</tr>
<tr>
<td align="left"> cRate</td>
<td align="center">0.66 (± 0.14)</td>
<td align="center">0.94 (± 0.12)</td>
<td align="center">0.57 (± 0.07)</td>
</tr>
<tr>
<td align="left"> aRand</td>
<td align="center">0.14 (± 0.27)</td>
<td align="center">0.84 (± 0.32)</td>
<td align="center">0.01 (± 0.06)</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Note that for these two examples, the results of kmlShape with <italic>λ</italic> = 0.1 and kmlShape using DTW (i.e. <italic>λ</italic> = 0) were identical. Thus we give only the results of kmlShape with <italic>λ</italic> = 0.1.</p>
</sec>
<sec id="sec027">
<title>4.2 Impact of simplification in terms of time or number of individuals</title>
<p>The respective performances of kmlShape, classical k-means, and simplified kmlShape with medium size data are shown <xref ref-type="table" rid="pone.0150738.t002">Table 2</xref>. We observed that the performance of simplified kmlShape was close to (similar or slightly lower) the performance of kmlShape without simplification. In all cases, the performance of kmlShape was clearly better than that of a classical partitioning.</p>
<table-wrap id="pone.0150738.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150738.t002</object-id>
<label>Table 2</label>
<caption>
<title>Performance of classical k-means, kmlShape, and simplified kmlShape with medium datasets (mean value ± standard deviation).</title>
</caption>
<alternatives>
<graphic id="pone.0150738.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150738.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center">Classical k-means</th>
<th align="center">kmlShape</th>
<th align="center">Simplified kmlShape</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Case 1</td>
<td align="left" colspan="3"/>
</tr>
<tr>
<td align="left"> cRate</td>
<td align="center">0.64 (± 0.13)</td>
<td align="center">0.94 (± 0.12)</td>
<td align="center">0.94 (± 0.12)</td>
</tr>
<tr>
<td align="left"> aRand</td>
<td align="center">0.13 (± 0.25)</td>
<td align="center">0.84 (± 0.31)</td>
<td align="center">0.82 (± 0.32)</td>
</tr>
<tr>
<td align="left">Case 2</td>
<td align="left" colspan="3"/>
</tr>
<tr>
<td align="left"> cRate</td>
<td align="center">0.75 (± 0.11)</td>
<td align="center">0.84 (± 0.15)</td>
<td align="center">0.82 (± 0.14)</td>
</tr>
<tr>
<td align="left"> aRand</td>
<td align="center">0.56 (± 0.19)</td>
<td align="center">0.72 (± 0.25)</td>
<td align="center">0.7 (± 0.24)</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The performances of classical k-means and simplified kmlShape with large datasets are shown <xref ref-type="table" rid="pone.0150738.t003">Table 3</xref>. The simplified kmlShape outperformed clearly the classical k-means. We also observed that the performance of kmlShape with simplification was quite close to that of kmlShape without simplification.</p>
<table-wrap id="pone.0150738.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150738.t003</object-id>
<label>Table 3</label>
<caption>
<title>Performances of classical k-means and simplified kmlShape with large data (mean value ± standard deviation).</title>
</caption>
<alternatives>
<graphic id="pone.0150738.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150738.t003" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center">Classical k-means</th>
<th align="center">Simplified kmlShape</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Case 1</td>
<td align="left" colspan="2"/>
</tr>
<tr>
<td align="left"> cRate</td>
<td align="center">0.59 (± 0.12)</td>
<td align="center">0.92 (± 0.15)</td>
</tr>
<tr>
<td align="left"> aRand</td>
<td align="center">0.09 (± 0.22)</td>
<td align="center">0.79 (± 0.38)</td>
</tr>
<tr>
<td align="left">Case 2</td>
<td align="left" colspan="2"/>
</tr>
<tr>
<td align="left"> cRate</td>
<td align="center">0.71 (± 0.11)</td>
<td align="center">0.8 (± 0.15)</td>
</tr>
<tr>
<td align="left"> aRand</td>
<td align="center">0.56 (± 0.18)</td>
<td align="center">0.68 (± 0.23)</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec028">
<title>4.3 Application to real data</title>
<sec id="sec029">
<title>Cohort ICTUS</title>
<p>Ictus (see <xref ref-type="supplementary-material" rid="pone.0150738.s001">S1 File</xref>) is a cohort of 1380 patients with Alzheimer disease followed-up in 12 European countries [<xref ref-type="bibr" rid="pone.0150738.ref060">60</xref>, <xref ref-type="bibr" rid="pone.0150738.ref061">61</xref>]. These patients were included between February 2003 and July 2005 in 29 centers specialized in neurology, geriatrics, psychiatry or psycho-geriatrics with a recognized experience in the diagnosis and management of Alzheimer disease. Most of these patients were seen during memory consultations and included consecutively. These patients were examined at six-month intervals over two years. Each examination included (though not exclusively) an Instrumental Activities of Daily Living (IADL) assessment.</p>
<p>A classical analysis of IADL trajectories using either mixture models or k-means revealed 4 groups (<xref ref-type="fig" rid="pone.0150738.g009">Fig 9.a</xref>). The main feature of these groups is to show close consistent declines. Using kmlShape (after using the data size reduction <italic>n</italic><sub><italic>s</italic></sub> = 128, no curve simplification) with 4 groups (<xref ref-type="fig" rid="pone.0150738.g009">Fig 9.b</xref>), three of these groups were similar to those found by other classical algorithms whereas a fourth “rapid decline” group was detected by kmlShape only.</p>
<fig id="pone.0150738.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150738.g009</object-id>
<label>Fig 9</label>
<caption>
<title>IADL trajectories, in 4 clusters.</title>
<p>(a) with a classical method; (b) with kmlShape. kmlShape is able to identify a “rapid decline” cluster that is not be found using the classical method.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150738.g009" xlink:type="simple"/>
</fig>
<p>The identification of this group makes it possible to predict and anticipate the needs of these patients in terms of informal help or professional care. Such a planning is of utmost importance for the health professionals and the families. Applying the same methods on other functions of these patients (cognition, behavior) would help clarifying the natural history of the disease. Applying these methods before diagnosis would help targeting the population to include in clinical trials for the prevention of Alzheimer disease. Indeed, such trials may use aggressive agents (e.g., monoclonal antibodies) which makes it necessary, from an ethical point of view, to target only the subpopulation with “rapid decline”, which is not possible with the classical classification methods.</p>
</sec>
<sec id="sec030">
<title>Quidel database</title>
<p>The QUIDEL database aims to gain better knowledge on hormone profiles of women without fertility problems. This database has been described as the largest existing database on hormone profiles in normally menstruating women and includes ovary ultrasound scans on the day of ovulation [<xref ref-type="bibr" rid="pone.0150738.ref062">62</xref>]. The database includes 107 women and 283 cycles with identification of the day of ovulation and daily titrations of the levels of the four main hormones of the ovulation cycle. It has been already the subject of publications [<xref ref-type="bibr" rid="pone.0150738.ref063">63</xref>, <xref ref-type="bibr" rid="pone.0150738.ref064">64</xref>]</p>
<p>The use of classical classification methods regarding the luteinizing hormone (LH) provided three typical trajectories with similar features but with slight shifts in time (<xref ref-type="fig" rid="pone.0150738.g010">Fig 10.a</xref>). This is a typical finding in medicine but is currently strongly questioned [<xref ref-type="bibr" rid="pone.0150738.ref062">62</xref>, <xref ref-type="bibr" rid="pone.0150738.ref065">65</xref>]. The use of kmlShape (after using the data size reduction <italic>n</italic><sub><italic>s</italic></sub> = 128, <italic>t</italic><sub><italic>DP</italic></sub> = 20) with three groups led to identifying: i) a classical profile that would concern 25% of the women; ii) a two LH-peak profile that would concern 22% of the women; and, iii) a profile with a single peak followed by a slow decline over several days that would concern the remaining 53% of the women (<xref ref-type="fig" rid="pone.0150738.g010">Fig 10.b</xref>).</p>
<fig id="pone.0150738.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150738.g010</object-id>
<label>Fig 10</label>
<caption>
<title>LH trajectories, in 3 clusters.</title>
<p>(a) with a classical method; (b) with kmlShape. kmlShape shows typical trajectories with two peaks that are not found with the classical method.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150738.g010" xlink:type="simple"/>
</fig>
<p>These results will enrich the current debate on the role of the LH peak in the maintenance of corpus luteum. Indeed, as indicated by its name, the LH was first described as luteinizing but, as LH peaks occur close to the day of ovulation, LH was made responsible for triggering ovulation. However, recent works [<xref ref-type="bibr" rid="pone.0150738.ref066">66</xref>] have demonstrated that the course of LH during the days that follow ovulation may be important to understand some abnormalities of corpus luteum and, thus, the implantation of the embryo in the uterus. The identification of 22% of women with LH double peak profiles is important for further research in reproductive biology.</p>
</sec>
<sec id="sec031">
<title>UCR-CBF</title>
<p>The two last examples (CBF and Trace, see below) are extracted from the “UCR Time Series Classification Archive” [<xref ref-type="bibr" rid="pone.0150738.ref067">67</xref>], a collection of real and artificial datasets dedicated to studies of time series and longitudinal data. Contrarily to the approach we have adopted in our simulation study, these datasets are generated once.</p>
<p>CBF (Cylinder-Bell-Funnel, [<xref ref-type="bibr" rid="pone.0150738.ref068">68</xref>]) is a dataset of 900 trajectories measured 128 times. These trajectories are divided into three clusters of sizes 302, 300, and 298. The mean trajectories of each cluster are shown <xref ref-type="fig" rid="pone.0150738.g011">Fig 11.a</xref>. We partitioned the data with different methods:</p>
<list list-type="bullet">
<list-item>
<p>classical Euclidian k-means;</p>
</list-item>
<list-item>
<p>kmlShape using DWT with simplification (<italic>n</italic><sub><italic>S</italic></sub> = 64, <italic>t</italic><sub><italic>DP</italic></sub> = 30, randomAll, <italic>λ</italic> = 0)</p>
</list-item>
<list-item>
<p>kmlShape with simplification (<italic>n</italic><sub><italic>S</italic></sub> = 64, <italic>t</italic><sub><italic>DP</italic></sub> = 30, randomAll, <italic>λ</italic> = 0.1) and</p>
</list-item>
</list>
<fig id="pone.0150738.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150738.g011</object-id>
<label>Fig 11</label>
<caption>
<title>CBF trajectories.</title>
<p>(a) real means (b) means found using a classical method; (c) means foung using kmlShape with DTW (d) means found using kmlShape with <italic>λ</italic> = 0.1. kmlShape found the real means.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150738.g011" xlink:type="simple"/>
</fig>
<p>The other methods were too time-consuming or did not converge. The classical classification methods found three groups with identical shapes shifted in time (<xref ref-type="fig" rid="pone.0150738.g011">Fig 11.b</xref>). kmlShape using DTW identified three groups with similar shapes but different heights (<xref ref-type="fig" rid="pone.0150738.g011">Fig 11.c</xref>). With these two classification methods, the number of misclassified trajectories was quite important (<xref ref-type="table" rid="pone.0150738.t004">Table 4</xref>) and the average trajectories obtained were quite different from the average trajectories used to generate groups (<xref ref-type="fig" rid="pone.0150738.g011">Fig 11.a</xref>). kmlShape with <italic>λ</italic> = 0.1 gave good results in terms of individual ranking as in terms of identification of the average trajectory (<xref ref-type="fig" rid="pone.0150738.g011">Fig 11.d</xref>).</p>
<table-wrap id="pone.0150738.t004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150738.t004</object-id>
<label>Table 4</label>
<caption>
<title>Performance of classical k-means, simplified kmlShape using DTW and simplified kmlShape with <italic>λ</italic> = 1 on CBF dataset.</title>
</caption>
<alternatives>
<graphic id="pone.0150738.t004g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150738.t004" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center">Classical k-means</th>
<th align="center">Simplified kmlShape with DTW</th>
<th align="center">Simplified kmlShape with <italic>λ</italic> = 1</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">cRate</td>
<td align="char" char=".">0.65</td>
<td align="char" char=".">0.59</td>
<td align="char" char=".">0.95</td>
</tr>
<tr>
<td align="left">aRand</td>
<td align="char" char=".">0.34</td>
<td align="char" char=".">0.25</td>
<td align="char" char=".">0.86</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Note that, in this example, the use of the DTW method gave a different (worse) result than the Fréchet mean with <italic>λ</italic> = 0.1. The reasons for this will be discussed in the appendix A.</p>
</sec>
<sec id="sec032">
<title>UCR-Trace</title>
<p>The Trace database [<xref ref-type="bibr" rid="pone.0150738.ref069">69</xref>] (also extracted from the “UCR Time Series Classification Archive”) was obtained from EDF (Electricité de France). The database includes 200 different transient classes (<xref ref-type="fig" rid="pone.0150738.g012">Fig 12.a</xref>). We clustered the data using:</p>
<list list-type="bullet">
<list-item>
<p>classical Euclidian k-means;</p>
</list-item>
<list-item>
<p>kmlShape using DWT with simplification (<italic>n</italic><sub><italic>S</italic></sub> = 64, <italic>t</italic><sub><italic>DP</italic></sub> = 30, randomAll, <italic>λ</italic> = 0)</p>
</list-item>
<list-item>
<p>kmlShape with simplification (<italic>n</italic><sub><italic>S</italic></sub> = 64, <italic>t</italic><sub><italic>DP</italic></sub> = 30, randomAll, <italic>λ</italic> = 0.1) and</p>
</list-item>
</list>
<fig id="pone.0150738.g012" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150738.g012</object-id>
<label>Fig 12</label>
<caption>
<title>Trace trajectories.</title>
<p>(a) real means (b) means found using a classical method; (c) means found using kmlShape with <italic>λ</italic> = 0.1. kmlShape found the real means.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150738.g012" xlink:type="simple"/>
</fig>
<p>The other methods were too time-consuming or did not converge. kmlShape <italic>λ</italic> = 0.1 and kmlShape using DTW gave exactly the same results (we report here only kmlShape with <italic>λ</italic> = 0.1). The classical classification method identified perfectly one of the three groups (the group in blue <xref ref-type="fig" rid="pone.0150738.g012">Fig 12.b</xref>) but failed to distinguish between the two others. The kmlShape identified three groups without errors and found the right average trajectories (<xref ref-type="fig" rid="pone.0150738.g012">Fig 12.c</xref> and <xref ref-type="table" rid="pone.0150738.t005">Table 5</xref>.)</p>
<table-wrap id="pone.0150738.t005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150738.t005</object-id>
<label>Table 5</label>
<caption>
<title>Performance of classical k-means, simplified kmlShape using DTW and simplified kmlShape with <italic>λ</italic> = 0.1 on Trace dataset.</title>
</caption>
<alternatives>
<graphic id="pone.0150738.t005g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150738.t005" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center">Classical k-means</th>
<th align="center">Simplified kmlShape</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">cRate</td>
<td align="char" char=".">0.77</td>
<td align="center">1</td>
</tr>
<tr>
<td align="left">eRand</td>
<td align="char" char=".">0.73</td>
<td align="center">1</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
</sec>
</sec>
<sec id="sec033" sec-type="conclusions">
<title>5 Discussion</title>
<p>In the present article, we introduce kmlShape, a novel method of data partitioning. This method provides clusters on the basis of trajectory shape. This allows especially grouping individuals whose trajectories have similar forms but shifted positions in time. Given the high algorithmic complexity of kmlShape, we present two data-simplification methods that allow reducing the lengths of the trajectories or the number of the individuals of the population under study.</p>
<p>In comparison with other shape-based partitioning methods, kmlShape demonstrated higher performances with the datasets tested whatever the variance, the population size, or the number of clusters. In addition, the search for loss of information due to data simplification has shown that the final partitioning is only slightly affected by this simplification (<xref ref-type="table" rid="pone.0150738.t002">Table 2</xref>). Another advantage was that each of the partition steps (reduction of the number of measurements or of the population size) is mathematically simple, graphically displayable, and easily checkable. At any moment, the user can decide to invalidate any excessive “data simplification”. Finally, with real datasets, kmlShape makes it possible to detect groups of individuals of non-negligible sizes that would not be detected by other classical methods. Thus, the method paves the way to new perspectives in terms of data analysis.</p>
<p>Within the general context of data partitioning, the problem of the optimal number of clusters is still an open issue. Numerous criteria exist, either parametric (BIC [<xref ref-type="bibr" rid="pone.0150738.ref070">70</xref>], AIC [<xref ref-type="bibr" rid="pone.0150738.ref071">71</xref>], AICc [<xref ref-type="bibr" rid="pone.0150738.ref072">72</xref>], global posterior probability [<xref ref-type="bibr" rid="pone.0150738.ref073">73</xref>], …) or non-parametric (Caliskin &amp; Harabatz [<xref ref-type="bibr" rid="pone.0150738.ref074">74</xref>], Ray &amp; Turi [<xref ref-type="bibr" rid="pone.0150738.ref075">75</xref>], Davies &amp; Bouldin [<xref ref-type="bibr" rid="pone.0150738.ref076">76</xref>], …). These criteria are regularly compared using artificial data [<xref ref-type="bibr" rid="pone.0150738.ref077">77</xref>, <xref ref-type="bibr" rid="pone.0150738.ref078">78</xref>]. With real data, they often suffer from bias. For example, Calinski &amp; Harbatz criteria (which is the best criterion according to both [<xref ref-type="bibr" rid="pone.0150738.ref077">77</xref>] and [<xref ref-type="bibr" rid="pone.0150738.ref078">78</xref>]) often select the lowest number of clusters. Also, different authors advise to choose the number of clusters on the basis of clinical relevance rather than an index [<xref ref-type="bibr" rid="pone.0150738.ref013">13</xref>].</p>
<p>In the case of partitioning using the Fréchet distance, the problem is more complicated because the classical criteria are designed to be used with classical distances. To date, there is no quality criterion that can help selecting the correct number of clusters within the context of respecting-shape partitioning. Finding such a quality index would be a non-negligible progress in the field of data partitioning.</p>
<p>Regarding the choices of <italic>n</italic><sub><italic>S</italic></sub> and <italic>t</italic><sub><italic>DP</italic></sub>, the present study showed that <italic>n</italic><sub><italic>S</italic></sub> = 32 and <italic>t</italic><sub><italic>DP</italic></sub> = 20 is a good compromise between a reasonable simplification and an acceptable calculation time. Obviously, these parameters may be adapted according to the type of data (with complex and long curves, <italic>t</italic><sub><italic>DP</italic></sub> = 20 seems to be insufficient; with simple curves <italic>t</italic><sub><italic>DP</italic></sub> = 10 may be sufficient) and the power of the computer involved. In a medium term, new high-performance statistical software programs will probably overcome the current limitations.</p>
<p>The choice of <italic>λ</italic> is more complex. As shown <xref ref-type="fig" rid="pone.0150738.g003">Fig 3</xref>, it changes the relative weight of the distance between two trajectories according to the x-axis and the y-axis. If the x-scale and the y-scale are identical, setting <italic>λ</italic> = 0.1 gives ten times more weight to a vertical offset than to a horizontal offset. This case is close to the one shown in the right panel <xref ref-type="fig" rid="pone.0150738.g003">Fig 3</xref>: <italic>i</italic><sub>1</sub> is close to <italic>i</italic><sub>3</sub> because the “horizontal offset” is very important. When <italic>λ</italic> is 1, the horizontal and the vertical offsets have the same importance. When <italic>λ</italic> = +∞, the horizontal offsets becomes very expensive, the Fréchet distance is then identical to the classical maximum distance. When <italic>λ</italic> = 0, the horizontal offsets becomes free, the Fréchet distance is then identical to the dynamic time warping distance. With our artificial examples, a value of <italic>λ</italic> = 0.1 or less allowed a correct identification of the groups. More detail about <italic>λ</italic> can be find appendix A.</p>
<p>When the scales are not the same (which is true in the majority of cases in the present study because one axis represents time and the other the variable of interest), the data can be standardized by dividing by the range of x and multiplying by the range of y. On our real examples, the value used was <inline-formula id="pone.0150738.e028"><alternatives><graphic id="pone.0150738.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:mi>λ</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>M</mml:mi> <mml:mi>a</mml:mi> <mml:mi>x</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow> <mml:mrow><mml:mi>M</mml:mi> <mml:mi>a</mml:mi> <mml:mi>x</mml:mi> <mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo> <mml:mo>-</mml:mo> <mml:mi>M</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi> <mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>×</mml:mo> <mml:mn>0</mml:mn> <mml:mo>.</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>. This value is the one that gave the most relevant results from the clinical point of view and has identified groups undetected by conventional techniques.</p>
</sec>
<sec id="sec034">
<title>A How to choose <italic>λ</italic>?</title>
<p>The choice of <italic>λ</italic> is of considerable impact on the final clustering. There is no “best” value for <italic>λ</italic> as there is no “best clustering techniques ever”, it must be selected depending on the problem. For that, according to the specific problem, the user has to define the curves that should be considered as “close curves”. To make this decision, it is important to keep in mind the principle of working of <italic>λ</italic>.</p>
<p>Consider <xref ref-type="fig" rid="pone.0150738.g013">Fig 13.a</xref>. The black trajectory represents an individual <italic>i</italic>. The three colored trajectories represent three cluster centers. The question is to decide which <italic>i</italic> should be the closest. Suppose that the trajectories represent the intensity of a disease. From a public health perspective, it is important to know when the vaccines should be ready so it is interesting to group <italic>i</italic> with A. For a researcher who wants to understand the disease, the type of disease progression is more important than the time of its outbreak so it will be more relevant to group <italic>i</italic> and <italic>C</italic> because. In some other problems, it might be interesting to group <italic>i</italic> and <italic>B</italic>.</p>
<fig id="pone.0150738.g013" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150738.g013</object-id>
<label>Fig 13</label>
<caption>
<title>The impact of <italic>λ</italic> on the distance between the trajectory <italic>i</italic> and the clusters mean’s trajectories <italic>A</italic>, <italic>B</italic> and <italic>C</italic>.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150738.g013" xlink:type="simple"/>
</fig>
<p>Consider now the Fréchet distance between <italic>i</italic> and the cluster centers (represented by the segments between the curves in dash). In this example, <italic>i</italic> is close to the average trajectory <italic>B</italic> (the dashed blue line is shortest than the green or the red lines). If we represent the same data but divide the scale of the <italic>x</italic> axis by two (<xref ref-type="fig" rid="pone.0150738.g013">Fig 13.b</xref>), <italic>i</italic> is closer to <italic>C</italic> (the green dashed line is the shortest). If we represent the same data but multiply the scale of the x by 2 (<xref ref-type="fig" rid="pone.0150738.g013">Fig 13.c</xref>), <italic>i</italic> is close to <italic>A</italic> (the red dashed line is the shortest).</p>
<p>From a mathematical perspective, this is easy to understand: the calculation of the length of a segment involves two components; the differences in values along axis <italic>x</italic> and along axis <italic>y</italic>. Changing the scale of <italic>x</italic> changes the relative importance of the two differences (reducing the <italic>x</italic>’ scale decreases the importance of the difference along the axis of <italic>x</italic>). In <xref ref-type="fig" rid="pone.0150738.g013">Fig 13.a</xref>, a horizontal shift has a great impact on the calculation of the distance. So <italic>i</italic> is close to <italic>A</italic>. In <xref ref-type="fig" rid="pone.0150738.g013">Fig 13.b</xref>, a horizontal shift has a little impact on the distance calculation. So <italic>i</italic> is close to <italic>C</italic>. In extreme cases, when <italic>λ</italic> tends to +∞, a small difference on the x-axis increases greatly the distance between the curves. The Fréchet distance is reached when “man and dog” keep the same abscissa at any point (as a difference of <italic>δ</italic> causes an increase in the distance of <italic>δ</italic> × <italic>λ</italic> which tends to +∞ as <italic>λ</italic> tends to +∞). The Fréchet distance is then identical to the max distance.</p>
<p>Conversely, when <italic>λ</italic> is 0, the distance between points (<italic>x</italic><sub>1</sub>, <italic>y</italic><sub>1</sub>) and (<italic>x</italic><sub>2</sub>, <italic>y</italic><sub>2</sub>) is just |<italic>y</italic><sub>1</sub> − <italic>y</italic><sub>2</sub>|. The difference along the x-axis has no impact on the distance between the curves. The Fréchet distance matches the DTW.</p>
<p>In summary, the role of <italic>λ</italic> is to allow the user to choose the case in which he wishes to be. Suppose that the initial population is shown <xref ref-type="fig" rid="pone.0150738.g013">Fig 13.b</xref>. If, according to the problem, it is relevant to cluster <italic>i</italic> with <italic>A</italic>, then <italic>λ</italic> should be small (<italic>λ</italic> &lt; &lt;1). If <italic>i</italic> should be clustered with <italic>B</italic>, then <italic>λ</italic> = 1 will be a correct choice. If <italic>i</italic> should be close to <italic>C</italic>, then <italic>λ</italic> should be big (<italic>λ</italic> &gt; &gt;1).</p>
</sec>
<sec id="sec035">
<title>B Appendix: Computational complexities</title>
<sec id="sec036">
<title>B.1 Euclidean distance</title>
<p>The formula for calculating the Euclidean distance is <inline-formula id="pone.0150738.e029"><alternatives><graphic id="pone.0150738.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e029" xlink:type="simple"/><mml:math display="inline" id="M29"><mml:mrow><mml:mi>D</mml:mi> <mml:mi>i</mml:mi> <mml:mi>s</mml:mi> <mml:mi>t</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>.</mml:mo></mml:mrow></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mrow><mml:msup><mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>.</mml:mo></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>t</mml:mi></mml:msubsup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>y</mml:mi> <mml:mrow><mml:msup><mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>, that is subtraction and a square for each <italic>j</italic>, then <italic>t</italic> − 1 additions. The overall complexity is <italic>O</italic>(<italic>t</italic>).</p>
</sec>
<sec id="sec037">
<title>B.2 The Fréchet distance</title>
<p>The calculation of the Fréchet distance needs the calculation of the distance matrix between each pair of points <inline-formula id="pone.0150738.e030"><alternatives><graphic id="pone.0150738.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mo>(</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mi>j</mml:mi></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msub><mml:mi>y</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:msup><mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msub><mml:mi>y</mml:mi> <mml:mrow><mml:msup><mml:mi>i</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>. This is a matrix of size <italic>t</italic><sup>2</sup>. The complexity of the calculation of the distance between each pair of points is a constant, so the complexity of the Fréchet distance is <italic>O</italic>(<italic>t</italic><sup>2</sup>).</p>
<p>The computing complexity of the Fréchet path is identical because, in addition to the computation of the matrix distance between each pair of points, it only requires browsing the matrix once to find the path.</p>
</sec>
<sec id="sec038">
<title>B.3 The Fréchet mean between two trajectories</title>
<p>The computing complexity of the Fréchet mean between two trajectories needs the calculation of the Fréchet path (cost: <italic>O</italic>(<italic>t</italic><sup>2</sup>)). The length of the Fréchet path is bounded by 2<italic>t</italic> − 2. Then the computation of the mean needs up to 2<italic>t</italic> − 2 additions and divisions. Thus the overall complexity is <italic>O</italic>(<italic>t</italic><sup>2</sup>).</p>
</sec>
<sec id="sec039">
<title>B.4 The Fréchet mean between two trajectories</title>
<p>The generalization of the Fréchet mean to <italic>n</italic> trajectories requires the calculation of an index for each tuple <inline-formula id="pone.0150738.e031"><alternatives><graphic id="pone.0150738.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:mo>(</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>j</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msub><mml:mi>y</mml:mi> <mml:mrow><mml:msub><mml:mi>i</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>j</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msub><mml:mi>y</mml:mi> <mml:mrow><mml:msub><mml:mi>i</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:mo>,</mml:mo> <mml:mo>(</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>j</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msub><mml:mi>y</mml:mi> <mml:mrow><mml:msub><mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>j</mml:mi> <mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable> <mml:mo>)</mml:mo> <mml:mo>)</mml:mo></mml:math></alternatives></inline-formula>, that is, a matrix of size <italic>t</italic><sup><italic>n</italic></sup>. The complexity of the calculation is therefore at least <italic>O</italic>(<italic>t</italic><sup><italic>n</italic></sup>).</p>
</sec>
<sec id="sec040">
<title>B.5 The Fréchet mean, method RandomAll</title>
<p>Method RandomAll merges <italic>n</italic> individuals two by two. It takes <italic>n</italic> − 1 merges (cost: <italic>O</italic>(<italic>t</italic><sup>2</sup>)). The overall complexity is <italic>O</italic>(<italic>nt</italic><sup>2</sup>).</p>
</sec>
<sec id="sec041">
<title>B.6 The Fréchet mean, method Hierarchical</title>
<p>Method Hierarchical computes the Fréchet distance (cost: <italic>O</italic>(<italic>t</italic><sup>2</sup>)) between all possible couples (<italic>n</italic>(<italic>n</italic> − 1)/2 couples) for a total cost of <italic>O</italic>(<italic>n</italic><sup>2</sup> <italic>t</italic><sup>2</sup>). Then, it merges the <italic>n</italic> individuals two by two. Each merging has a cost of <italic>O</italic>(<italic>t</italic><sup>2</sup>). The final complexity is <italic>O</italic>(<italic>n</italic><sup>2</sup> <italic>t</italic><sup>2</sup>).</p>
</sec>
<sec id="sec042">
<title>B.7 The Fréchet mean, method RandomSubset</title>
<p>Method RandomAll merges <italic>n</italic><sub>0</sub> individuals two by two. It takes <italic>n</italic><sub>0</sub> − 1 merges. Each merge costs <italic>O</italic>(<italic>t</italic><sup>2</sup>). The overall complexity is <italic>O</italic>(<italic>n</italic><sub>0</sub> <italic>t</italic><sup>2</sup>).</p>
</sec>
<sec id="sec043">
<title>B.8 k-means</title>
<p>At each stage of k-means, <italic>nk</italic> Euclidean distances (cost: <italic>O</italic>(<italic>t</italic>)) between <italic>n</italic> individuals and <italic>k</italic> groups centers are calculated (cost: <italic>O</italic>(<italic>tnk</italic>)). Then, in each group <italic>g</italic>, the mean of the <italic>n</italic><sub><italic>g</italic></sub> individuals belonging to the group is calculated; that is, <italic>tn</italic><sub><italic>g</italic></sub> additions per group with <italic>n</italic> = ∑<italic>n</italic><sub><italic>g</italic></sub>. The final complexity is <italic>O</italic>(<italic>tnk</italic>) (the number of iterations is neglected here because it is generally bounded, it is therefore a constant).</p>
</sec>
<sec id="sec044">
<title>B.9 kmlShape</title>
<p>At each stage of k-means, <italic>nk</italic> Fréchet distances (complexity <italic>O</italic>(<italic>t</italic><sup>2</sup>)) between <italic>n</italic> individuals and <italic>k</italic> groups centers are calculated (cost: <italic>O</italic>(<italic>t</italic><sup>2</sup> <italic>nk</italic>)). Then, in each group <italic>g</italic>, the mean of <italic>n</italic><sub><italic>g</italic></sub> individuals in the group is calculated. The cost is <italic>O</italic>(<italic>n</italic><sub><italic>g</italic></sub> <italic>t</italic><sup>2</sup>) per group with method RandomAll, <inline-formula id="pone.0150738.e032"><alternatives><graphic id="pone.0150738.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mrow><mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>n</mml:mi> <mml:mi>g</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:msup><mml:mi>t</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> with method hierarchical. The final complexity is <italic>O</italic>(<italic>knt</italic><sup>2</sup>) with method RandomAll and <italic>O</italic>(<italic>n</italic><sup>2</sup> <italic>t</italic><sup>2</sup>) with method Hierarchical.</p>
</sec>
<sec id="sec045">
<title>B.10 Douglas-Peuker algorithm</title>
<p>For a curve which must be simplified into <italic>t</italic><sub>0</sub> points, each iteration requires the calculation of the distance between the <italic>t</italic> points and the current curve (cost: <italic>O</italic>(<italic>t</italic>.<italic>t</italic><sub>0</sub>)). This has to be done <italic>t</italic><sub>0</sub> times. So, for <italic>n</italic> curves, the complexity is <inline-formula id="pone.0150738.e033"><alternatives><graphic id="pone.0150738.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150738.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mrow><mml:mi>O</mml:mi> <mml:mo>(</mml:mo> <mml:mi>n</mml:mi> <mml:mi>t</mml:mi> <mml:msubsup><mml:mi>t</mml:mi> <mml:mn>0</mml:mn> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</sec>
</sec>
<sec id="sec046">
<title>Supporting Information</title>
<supplementary-material id="pone.0150738.s001" mimetype="text/csv" position="float" xlink:href="info:doi/10.1371/journal.pone.0150738.s001" xlink:type="simple">
<label>S1 File</label>
<caption>
<title>A subset of the longitudinal study ICTUS.</title>
<p>(CSV)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The authors thank Jean Iwaz (Hospices Civils de Lyon) for the revision of the manuscript draft. We also thank the reveiwers whose careful readings and insighful comments allowed us to greatly improve the quality of this article.</p>
<p><bold>Funding:</bold> This research was founded by the ANR grant IDoL: ANR-12-BSV1-0036.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0150738.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tarpey</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Kinateder</surname> <given-names>KKJ</given-names></name>. <article-title>Clustering Functional Data</article-title>. <source>Journal of Classification</source>. <year>2003</year>;<volume>20</volume>(<issue>1</issue>):<fpage>93</fpage>–<lpage>114</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00357-003-0007-3" xlink:type="simple">10.1007/s00357-003-0007-3</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Garcia-Escudero</surname> <given-names>LA</given-names></name>, <name name-style="western"><surname>Gordaliza</surname> <given-names>A</given-names></name>. <article-title>A proposal for robust curve clustering</article-title>. <source>Journal of classification</source>. <year>2005</year>;<volume>22</volume>(<issue>2</issue>):<fpage>185</fpage>–<lpage>201</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00357-005-0013-8" xlink:type="simple">10.1007/s00357-005-0013-8</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Tarpey</surname> <given-names>T</given-names></name>. <article-title>A parametric k-means algorithm</article-title>. <source>Computational statistics</source>. <year>2007</year>;<volume>22</volume>(<issue>1</issue>):<fpage>71</fpage>–<lpage>89</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00180-007-0022-7" xlink:type="simple">10.1007/s00180-007-0022-7</ext-link></comment> <object-id pub-id-type="pmid">17917692</object-id></mixed-citation>
</ref>
<ref id="pone.0150738.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Elsensohn</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Klich</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ecochard</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bastard</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Genolini</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Etard</surname> <given-names>JF</given-names></name>, <etal>et al</etal>. <article-title>A graphical method to assess distribution assumption in group-based trajectory models</article-title>. <source>Statistical methods in medical research</source>. <year>2013</year>;p. <fpage>0962280213475643</fpage>.</mixed-citation>
</ref>
<ref id="pone.0150738.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Genolini</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Pingault</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Driss</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Côté</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tremblay</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Vitaro</surname> <given-names>F</given-names></name>, <etal>et al</etal>. <article-title>KmL3D: a non-parametric algorithm for clustering joint trajectories</article-title>. <source>Computer methods and programs in biomedicine</source>. <year>2013</year>;<volume>109</volume>(<issue>1</issue>):<fpage>104</fpage>–<lpage>111</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cmpb.2012.08.016" xlink:type="simple">10.1016/j.cmpb.2012.08.016</ext-link></comment> <object-id pub-id-type="pmid">23127283</object-id></mixed-citation>
</ref>
<ref id="pone.0150738.ref006">
<label>6</label>
<mixed-citation publication-type="other" xlink:type="simple">Lee JG, Han J, Whang KY. Trajectory clustering: a partition-and-group framework. In: Proceedings of the 2007 ACM SIGMOD international conference on Management of data. ACM; 2007. p. 593–604.</mixed-citation>
</ref>
<ref id="pone.0150738.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>James</surname> <given-names>GM</given-names></name>, <name name-style="western"><surname>Sugar</surname> <given-names>CA</given-names></name>. <article-title>Clustering for sparsely sampled functional data</article-title>. <source>Journal of the American Statistical Association</source>. <year>2003</year>;<volume>98</volume>(<issue>462</issue>):<fpage>397</fpage>–<lpage>408</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1198/016214503000189" xlink:type="simple">10.1198/016214503000189</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Luan</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>H</given-names></name>. <article-title>Clustering of time-course gene expression data using a mixed-effects model with B-splines</article-title>. <source>Bioinformatics</source>. <year>2003</year>;<volume>19</volume>(<issue>4</issue>):<fpage>474</fpage>–<lpage>482</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/btg014" xlink:type="simple">10.1093/bioinformatics/btg014</ext-link></comment> <object-id pub-id-type="pmid">12611802</object-id></mixed-citation>
</ref>
<ref id="pone.0150738.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Chiou</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>PL</given-names></name>. <article-title>Functional clustering and identifying substructures of longitudinal data</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>. <year>2007</year>;<volume>69</volume>(<issue>4</issue>):<fpage>679</fpage>–<lpage>699</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1467-9868.2007.00605.x" xlink:type="simple">10.1111/j.1467-9868.2007.00605.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nagin</surname> <given-names>DS</given-names></name>. <article-title>Analyzing developmental trajectories: a semiparametric, group-based approach</article-title>. <source>Psychological methods</source>. <year>1999</year>;<volume>4</volume>(<issue>2</issue>):<fpage>139</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/1082-989X.4.2.139" xlink:type="simple">10.1037/1082-989X.4.2.139</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Muthén</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Shedden</surname> <given-names>K</given-names></name>. <article-title>Finite mixture modeling with mixture outcomes using the EM algorithm</article-title>. <source>Biometrics</source>. <year>1999</year>;<volume>55</volume>(<issue>2</issue>):<fpage>463</fpage>–<lpage>469</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.0006-341X.1999.00463.x" xlink:type="simple">10.1111/j.0006-341X.1999.00463.x</ext-link></comment> <object-id pub-id-type="pmid">11318201</object-id></mixed-citation>
</ref>
<ref id="pone.0150738.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Magidson</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Vermunt</surname> <given-names>JK</given-names></name>. <article-title>Latent class models for clustering: A comparison with K-means</article-title>. <source>Canadian Journal of Marketing Research</source>. <year>2002</year>;<volume>20</volume>:<fpage>37</fpage>.</mixed-citation>
</ref>
<ref id="pone.0150738.ref013">
<label>13</label>
<mixed-citation publication-type="other" xlink:type="simple">Everitt B, Landau S, Leese M. Cluster Analysis. 4th. Arnold, London; 2001.</mixed-citation>
</ref>
<ref id="pone.0150738.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fréchet</surname> <given-names>MM</given-names></name>. <article-title>Sur quelques points du calcul fonctionnel</article-title>. <source>Rendiconti del Circolo Matematico di Palermo (1884–1940)</source>. <year>1906</year>;<volume>22</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>72</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF03018603" xlink:type="simple">10.1007/BF03018603</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lucero</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Munhall</surname> <given-names>KG</given-names></name>, <name name-style="western"><surname>Gracco</surname> <given-names>VL</given-names></name>, <name name-style="western"><surname>Ramsay</surname> <given-names>JO</given-names></name>. <article-title>On the registration of time and the patterning of speech movements</article-title>. <source>Journal of Speech, Language, and Hearing Research</source>. <year>1997</year>;<volume>40</volume>(<issue>5</issue>):<fpage>1111</fpage>–<lpage>1117</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1044/jslhr.4005.1111" xlink:type="simple">10.1044/jslhr.4005.1111</ext-link></comment> <object-id pub-id-type="pmid">9328881</object-id></mixed-citation>
</ref>
<ref id="pone.0150738.ref016">
<label>16</label>
<mixed-citation publication-type="other" xlink:type="simple">Al-Naymat G, Chawla S, Taheri J. SparseDTW: a novel approach to speed up dynamic time warping. In: Proceedings of the Eighth Australasian Data Mining Conference-Volume 101. Australian Computer Society, Inc.; 2009. p. 117–127.</mixed-citation>
</ref>
<ref id="pone.0150738.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Berndt</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Clifford</surname> <given-names>J</given-names></name> <name name-style="western"><surname>Seattle</surname> <given-names>WA</given-names></name>. <article-title>Using Dynamic Time Warping to Find Patterns in Time Series</article-title>. <source>KDD workshop</source>. <year>1994</year>;<volume>10</volume>(<issue>16</issue>):<fpage>359</fpage>–<lpage>370</lpage>.</mixed-citation>
</ref>
<ref id="pone.0150738.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Keogh</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Ratanamahatana</surname> <given-names>CA</given-names></name>. <article-title>Exact indexing of dynamic time warping</article-title>. <source>Knowledge and information systems</source>. <year>2005</year>;<volume>7</volume>(<issue>3</issue>):<fpage>358</fpage>–<lpage>386</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10115-004-0154-9" xlink:type="simple">10.1007/s10115-004-0154-9</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref019">
<label>19</label>
<mixed-citation publication-type="other" xlink:type="simple">Vlachos M, Kollios G, Gunopulos D. Discovering similar multidimensional trajectories. In: Data Engineering, 2002. Proceedings. 18th International Conference on. IEEE; 2002. p. 673–684.</mixed-citation>
</ref>
<ref id="pone.0150738.ref020">
<label>20</label>
<mixed-citation publication-type="other" xlink:type="simple">Chen L, Özsu MT, Oria V. Robust and fast similarity search for moving object trajectories. In: Proceedings of the 2005 ACM SIGMOD international conference on Management of data. ACM; 2005. p. 491–502.</mixed-citation>
</ref>
<ref id="pone.0150738.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Buchin</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Buchin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Van Kreveld</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Luo</surname> <given-names>J</given-names></name>. <article-title>Finding long and similar parts of trajectories</article-title>. <source>Computational Geometry</source>. <year>2011</year>;<volume>44</volume>(<issue>9</issue>):<fpage>465</fpage>–<lpage>476</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.comgeo.2011.05.004" xlink:type="simple">10.1016/j.comgeo.2011.05.004</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ramsay</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>X</given-names></name>. <article-title>Curve registration</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>. <year>1998</year>;<volume>60</volume>(<issue>2</issue>):<fpage>351</fpage>–<lpage>363</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/1467-9868.00129" xlink:type="simple">10.1111/1467-9868.00129</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref023">
<label>23</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Ramsay</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Silvermann</surname> <given-names>B</given-names></name>. <chapter-title>Functional Data Analysis</chapter-title>. <source>Springer Series in Statistics</source>. <publisher-name>Wiley Online Library</publisher-name>; <year>1998</year>.</mixed-citation>
</ref>
<ref id="pone.0150738.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dimeglio</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Gallón</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Loubes</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Maza</surname> <given-names>E</given-names></name>. <article-title>A robust algorithm for template curve estimation based on manifold embedding</article-title>. <source>Computational Statistics &amp; Data Analysis</source>. <year>2014</year>;<volume>70</volume>:<fpage>373</fpage>–<lpage>386</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.csda.2013.09.030" xlink:type="simple">10.1016/j.csda.2013.09.030</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>James</surname> <given-names>GM</given-names></name>, <etal>et al</etal>. <article-title>Curve alignment by moments</article-title>. <source>The Annals of Applied Statistics</source>. <year>2007</year>;<volume>1</volume>(<issue>2</issue>):<fpage>480</fpage>–<lpage>501</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/07-AOAS127" xlink:type="simple">10.1214/07-AOAS127</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref026">
<label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kaziska</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Srivastava</surname> <given-names>A</given-names></name>. <article-title>Gait-based human recognition by classification of cyclostationary processes on nonlinear shape manifolds</article-title>. <source>Journal of the American Statistical Association</source>. <year>2007</year>;<volume>102</volume>(<issue>480</issue>):<fpage>1114</fpage>–<lpage>1124</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1198/016214507000000464" xlink:type="simple">10.1198/016214507000000464</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sangalli</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>Secchi</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Vantini</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Vitelli</surname> <given-names>V</given-names></name>. <article-title>K-mean alignment for curve clustering</article-title>. <source>Computational Statistics &amp; Data Analysis</source>. <year>2010</year>;<volume>54</volume>(<issue>5</issue>):<fpage>1219</fpage>–<lpage>1233</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.csda.2009.12.008" xlink:type="simple">10.1016/j.csda.2009.12.008</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref028">
<label>28</label>
<mixed-citation publication-type="other" xlink:type="simple">Chudova D, Gaffney S, Mjolsness E, Smyth P. Translation-invariant mixture models for curve clustering. In: Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM; 2003. p. 79–88.</mixed-citation>
</ref>
<ref id="pone.0150738.ref029">
<label>29</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Gaffney</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Smyth</surname> <given-names>P</given-names></name>. <chapter-title>Joint probabilistic curve clustering and alignment</chapter-title>. In: <source>Advances in neural information processing systems</source>; <year>2004</year>. p. <fpage>473</fpage>–<lpage>480</lpage>.</mixed-citation>
</ref>
<ref id="pone.0150738.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Liu</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>MC</given-names></name>. <article-title>Simultaneous curve registration and clustering for functional data</article-title>. <source>Computational Statistics &amp; Data Analysis</source>. <year>2009</year>;<volume>53</volume>(<issue>4</issue>):<fpage>1361</fpage>–<lpage>1376</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.csda.2008.11.019" xlink:type="simple">10.1016/j.csda.2008.11.019</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref031">
<label>31</label>
<mixed-citation publication-type="other" xlink:type="simple">Patriarca M, Sangalli L, Secchi P, Vantini S, Vitelli V. fdakma: Clustering and alignment of a functional dataset; 2013. R package version 1.0. Available from: <ext-link ext-link-type="uri" xlink:href="http://CRAN.R-project.org/package=fdakma" xlink:type="simple">http://CRAN.R-project.org/package=fdakma</ext-link>.</mixed-citation>
</ref>
<ref id="pone.0150738.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Celeux</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Govaert</surname> <given-names>G</given-names></name>. <article-title>A Classification EM algorithm for Clustering and Two Stochastic Versions</article-title>. <source>Computational Statistics &amp; Data Analysis</source>. <year>1992</year>;<volume>14</volume>(<issue>3</issue>):<fpage>315</fpage>–<lpage>332</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0167-9473(92)90042-E" xlink:type="simple">10.1016/0167-9473(92)90042-E</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref033">
<label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hartigan</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Wong</surname> <given-names>MA</given-names></name>. <article-title>Algorithm AS 136: A K-means Clustering Algorithm</article-title>. <source>Journal of the Royal Statistical Society Series C (Applied Statistics)</source>. <year>1979</year>;<volume>28</volume>(<issue>1</issue>):<fpage>100</fpage>–<lpage>108</lpage>.</mixed-citation>
</ref>
<ref id="pone.0150738.ref034">
<label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>MacQueen</surname> <given-names>J</given-names></name>, <etal>et al</etal>.; USA California. <article-title>Some methods for classification and analysis of multivariate observations</article-title>. <source>Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</source>. <year>1967</year>;<volume>1</volume>(<issue>281–297</issue>):<fpage>14</fpage>.</mixed-citation>
</ref>
<ref id="pone.0150738.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Genolini</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Falissard</surname> <given-names>B</given-names></name>. kml: k-means for <article-title>Longitudinal Data</article-title>. <source>Computational Statistics</source>. <year>2010</year>;<volume>25</volume>(<issue>2</issue>):<fpage>317</fpage>–<lpage>328</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00180-009-0178-4" xlink:type="simple">10.1007/s00180-009-0178-4</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Divoux</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Tordjman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lacasa</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Veyrie</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Hugol</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Aissat</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Fibrosis in human adipose tissue: composition, distribution, and link with lipid metabolism and fat mass loss</article-title>. <source>Diabetes</source>. <year>2010</year>;<volume>59</volume>(<issue>11</issue>):<fpage>2817</fpage>–<lpage>2825</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2337/db10-0585" xlink:type="simple">10.2337/db10-0585</ext-link></comment> <object-id pub-id-type="pmid">20713683</object-id></mixed-citation>
</ref>
<ref id="pone.0150738.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pingault</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Côté</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Lacourse</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Galéra</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Vitaro</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Tremblay</surname> <given-names>RE</given-names></name>. <article-title>Childhood hyperactivity, physical aggression and criminality: a 19-year prospective population-based study</article-title>. <source>PloS one</source>. <year>2013</year>;<volume>8</volume>(<issue>5</issue>):<fpage>e62594</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0062594" xlink:type="simple">10.1371/journal.pone.0062594</ext-link></comment> <object-id pub-id-type="pmid">23658752</object-id></mixed-citation>
</ref>
<ref id="pone.0150738.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pingault</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Tremblay</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Vitaro</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Carbonneau</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Genolini</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Falissard</surname> <given-names>B</given-names></name>, <etal>et al</etal>. <article-title>Childhood trajectories of inattention and hyperactivity and prediction of educational attainment in early adulthood: a 16-year longitudinal population-based study</article-title>. <source>American Journal of Psychiatry</source>. <year>2011</year>;<volume>168</volume>(<issue>11</issue>):<fpage>1164</fpage>–<lpage>1170</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1176/appi.ajp.2011.10121732" xlink:type="simple">10.1176/appi.ajp.2011.10121732</ext-link></comment> <object-id pub-id-type="pmid">21799065</object-id></mixed-citation>
</ref>
<ref id="pone.0150738.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Mackelprang</surname> <given-names>RD</given-names></name>, <name name-style="western"><surname>Baeten</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Donnell</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Celum</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Farquhar</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>de Bruyn</surname> <given-names>G</given-names></name>, <etal>et al</etal>. <article-title>Quantifying Ongoing HIV-1 Exposure in HIV-1–Serodiscordant Couples to Identify Individuals With Potential Host Resistance to HIV-1</article-title>. <source>Journal of Infectious Diseases</source>. <year>2012</year>;<volume>206</volume>(<issue>8</issue>):<fpage>1299</fpage>–<lpage>1308</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/infdis/jis480" xlink:type="simple">10.1093/infdis/jis480</ext-link></comment> <object-id pub-id-type="pmid">22926009</object-id></mixed-citation>
</ref>
<ref id="pone.0150738.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rancière</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Nikasinovic</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Bousquet</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Momas</surname> <given-names>I</given-names></name>. <article-title>Onset and persistence of respiratory/allergic symptoms in preschoolers: new insights from the PARIS birth cohort</article-title>. <source>Allergy</source>. <year>2013</year>;<volume>68</volume>(<issue>9</issue>):<fpage>1158</fpage>–<lpage>1167</lpage>. <object-id pub-id-type="pmid">23919292</object-id></mixed-citation>
</ref>
<ref id="pone.0150738.ref041">
<label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pena</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Lozano</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Larranaga</surname> <given-names>P</given-names></name>. <article-title>An Empirical Comparison of Four Initialization Methods for the k-Means Algorithm</article-title>. <source>Pattern recognition letters</source>. <year>1999</year>;<volume>20</volume>(<issue>10</issue>):<fpage>1027</fpage>–<lpage>1040</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0167-8655(99)00069-0" xlink:type="simple">10.1016/S0167-8655(99)00069-0</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref042">
<label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Khan</surname> <given-names>SS</given-names></name>, <name name-style="western"><surname>Ahmad</surname> <given-names>A</given-names></name>. <article-title>Cluster Center Initialization Algorithm for k-means Clustering</article-title>. <source>Pattern Recognition Letters</source>. <year>2004</year>;<volume>25</volume>(<issue>11</issue>):<fpage>1293</fpage>–<lpage>1302</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.patrec.2004.04.007" xlink:type="simple">10.1016/j.patrec.2004.04.007</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Redmond</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Heneghan</surname> <given-names>C</given-names></name>. <article-title>A Method for Initialising the k-means Clustering Algorithm Using <italic>kd</italic>-trees</article-title>. <source>Pattern Recognition Letters</source>. <year>2007</year>;<volume>28</volume>(<issue>8</issue>):<fpage>965</fpage>–<lpage>973</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.patrec.2007.01.001" xlink:type="simple">10.1016/j.patrec.2007.01.001</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Steinley</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Brusco</surname> <given-names>MJ</given-names></name>. <article-title>Initializing k-means Batch Clustering: A Critical Evaluation of Several Techniques</article-title>. <source>Journal of Classification</source>. <year>2007</year>;<volume>24</volume>(<issue>1</issue>):<fpage>99</fpage>–<lpage>121</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00357-007-0003-0" xlink:type="simple">10.1007/s00357-007-0003-0</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Twisk</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>De Vente</surname> <given-names>W</given-names></name>. <article-title>Attrition in Longitudinal Studies: How to Deal With Missing Data</article-title>. <source>Journal of Clinical Epidemiology</source>. <year>2002</year>;<volume>55</volume>(<issue>4</issue>):<fpage>329</fpage>–<lpage>337</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0895-4356(01)00476-0" xlink:type="simple">10.1016/S0895-4356(01)00476-0</ext-link></comment> <object-id pub-id-type="pmid">11927199</object-id></mixed-citation>
</ref>
<ref id="pone.0150738.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Engels</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Diehr</surname> <given-names>P</given-names></name>. <article-title>Imputation of Missing Longitudinal Data: A Comparison of Methods</article-title>. <source>Journal of Clinical Epidemiology</source>. <year>2003</year>;<volume>56</volume>(<issue>10</issue>):<fpage>968</fpage>–<lpage>976</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0895-4356(03)00170-7" xlink:type="simple">10.1016/S0895-4356(03)00170-7</ext-link></comment> <object-id pub-id-type="pmid">14568628</object-id></mixed-citation>
</ref>
<ref id="pone.0150738.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Genolini</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Écochard</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Jacqmin-Gadda</surname> <given-names>H</given-names></name>. <article-title>Copy Mean: A New Method to Impute Intermittent Missing Values in Longitudinal Studies</article-title>. <source>Open Journal of Statistics</source>. <year>2013</year>;<volume>3</volume>:<fpage>26</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.4236/ojs.2013.34A004" xlink:type="simple">10.4236/ojs.2013.34A004</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Genolini</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Lacombe</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Écochard</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Subtil</surname> <given-names>F</given-names></name>. <article-title>CopyMean: a new method to predict monotone missing values in longitudinal studies</article-title>. <source>Computer Methodes and Programs in biomedicine</source>. <year>2015</year>;In press:<fpage>1</fpage>–<lpage>22</lpage>.</mixed-citation>
</ref>
<ref id="pone.0150738.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Alt</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Godau</surname> <given-names>M</given-names></name>. <article-title>Computing the Fréchet distance between two polygonal curves</article-title>. <source>International Journal of Computational Geometry &amp; Applications</source>. <year>1995</year>;<volume>5</volume>(<issue>01n02</issue>):<fpage>75</fpage>–<lpage>91</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1142/S0218195995000064" xlink:type="simple">10.1142/S0218195995000064</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref050">
<label>50</label>
<mixed-citation publication-type="other" xlink:type="simple">Keogh E, Chu S, Hart D, Pazzani M. An online algorithm for segmenting time series. In: Data Mining, 2001. ICDM 2001, Proceedings IEEE International Conference on. IEEE; 2001. p. 289–296.</mixed-citation>
</ref>
<ref id="pone.0150738.ref051">
<label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Panagiotakis</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Pelekis</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Kopanakis</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Ramasso</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Theodoridis</surname> <given-names>Y</given-names></name>. <article-title>Segmentation and sampling of moving object trajectories based on representativeness</article-title>. <source>Knowledge and Data Engineering, IEEE Transactions on</source>. <year>2012</year>;<volume>24</volume>(<issue>7</issue>):<fpage>1328</fpage>–<lpage>1343</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TKDE.2011.39" xlink:type="simple">10.1109/TKDE.2011.39</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cao</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Wolfson</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Trajcevski</surname> <given-names>G</given-names></name>. <article-title>Spatio-temporal data reduction with deterministic error bounds</article-title>. <source>The VLDB Journal The International Journal on Very Large Data Bases</source>. <year>2006</year>;<volume>15</volume>(<issue>3</issue>):<fpage>211</fpage>–<lpage>228</lpage>. Line simplification. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00778-005-0163-7" xlink:type="simple">10.1007/s00778-005-0163-7</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref053">
<label>53</label> <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gudmundsson</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Katajainen</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Merrick</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ong</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Wolle</surname> <given-names>T</given-names></name>. <article-title>Compressing spatio-temporal trajectories</article-title>. <source>Computational geometry</source>. <year>2009</year>;<volume>42</volume>(<issue>9</issue>):<fpage>825</fpage>–<lpage>841</lpage>. Amelioration de Douglas-peker. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.comgeo.2009.02.002" xlink:type="simple">10.1016/j.comgeo.2009.02.002</ext-link></comment></mixed-citation></ref>
<ref id="pone.0150738.ref054">
<label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Douglas</surname> <given-names>DH</given-names></name>, <name name-style="western"><surname>Peucker</surname> <given-names>TK</given-names></name>. <article-title>Algorithms for the reduction of the number of points required to represent a digitized line or its caricature</article-title>. <source>Cartographica: The International Journal for Geographic Information and Geovisualization</source>. <year>1973</year>;<volume>10</volume>(<issue>2</issue>):<fpage>112</fpage>–<lpage>122</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3138/FM57-6770-U75U-7727" xlink:type="simple">10.3138/FM57-6770-U75U-7727</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref055">
<label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ramer</surname> <given-names>U</given-names></name>. <article-title>An iterative procedure for the polygonal approximation of plane curves</article-title>. <source>Computer Graphics and Image Processing</source>. <year>1972</year>;<volume>1</volume>(<issue>3</issue>):<fpage>244</fpage>–<lpage>256</lpage>. Douglas-pecker, sous un autre nom. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0146-664X(72)80017-0" xlink:type="simple">10.1016/S0146-664X(72)80017-0</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref056">
<label>56</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Duda</surname> <given-names>RO</given-names></name>, <name name-style="western"><surname>Hart</surname> <given-names>PE</given-names></name>, <etal>et al</etal>. <source>Pattern classification and scene analysis</source>. <volume>vol. 3</volume>. <publisher-name>Wiley</publisher-name> <publisher-loc>New York</publisher-loc>; <year>1973</year>. Douglas-pecker, sous un autre nom.</mixed-citation>
</ref>
<ref id="pone.0150738.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hubert</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Arabie</surname> <given-names>P</given-names></name>. <article-title>Comparing partitions</article-title>. <source>Journal of classification</source>. <year>1985</year>;<volume>2</volume>(<issue>1</issue>):<fpage>193</fpage>–<lpage>218</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF01908075" xlink:type="simple">10.1007/BF01908075</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rand</surname> <given-names>WM</given-names></name>. <article-title>Objective criteria for the evaluation of clustering methods</article-title>. <source>Journal of the American Statistical association</source>. <year>1971</year>;<volume>66</volume>(<issue>336</issue>):<fpage>846</fpage>–<lpage>850</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/01621459.1971.10482356" xlink:type="simple">10.1080/01621459.1971.10482356</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Genolini</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Alacoque</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Sentenac</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Arnaud</surname> <given-names>C</given-names></name>. <article-title>kml and kml3d: R Packages to Cluster Longitudinal Data</article-title>. <source>Journal of Statistical Software</source>. <year>2015</year>;<volume>65</volume>(<issue>4</issue>):<fpage>1</fpage>–<lpage>34</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.jstatsoft.org/v65/i04/" xlink:type="simple">http://www.jstatsoft.org/v65/i04/</ext-link>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.18637/jss.v065.i04" xlink:type="simple">10.18637/jss.v065.i04</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref060">
<label>60</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Reynish</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Cortes</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Andrieu</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Cantet</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Olde Rikkert</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Melis</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>The ICTUS Study: A prospective longitudinal observational study of 1,380 AD patients in Europe</article-title>. <source>Neuroepidemiology</source>. <year>2007</year>;<volume>29</volume>(<issue>1–2</issue>):<fpage>29</fpage>–<lpage>38</lpage>.</mixed-citation>
</ref>
<ref id="pone.0150738.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vellas</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Hausner</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Frolich</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Cantet</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Gardette</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Reynish</surname> <given-names>E</given-names></name>, <etal>et al</etal>. <article-title>Progression of Alzheimer disease in Europe: Data from the European ICTUS study</article-title>. <source>Current Alzheimer Research</source>. <year>2012</year>;<volume>9</volume>(<issue>8</issue>):<fpage>902</fpage>–<lpage>912</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2174/156720512803251066" xlink:type="simple">10.2174/156720512803251066</ext-link></comment> <object-id pub-id-type="pmid">22742853</object-id></mixed-citation>
</ref>
<ref id="pone.0150738.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ecochard</surname> <given-names>R</given-names></name>. <article-title>Heterogeneity in Fecundability Studies: Issues and Modelling</article-title>. <source>Statistical Methods in Medical Research</source>. <year>2006</year>;<volume>15</volume>(<issue>2</issue>):<fpage>141</fpage>–<lpage>160</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1191/0962280206sm436oa" xlink:type="simple">10.1191/0962280206sm436oa</ext-link></comment> <object-id pub-id-type="pmid">16615654</object-id></mixed-citation>
</ref>
<ref id="pone.0150738.ref063">
<label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ecochard</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Gougeon</surname> <given-names>A</given-names></name>. <article-title>Side of Ovulation and Cycle Characteristics in Normally Fertile Women</article-title>. <source>Human Reproduction</source>. <year>2000</year>;<volume>15</volume>(<issue>4</issue>):<fpage>752</fpage>–<lpage>755</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/humrep/15.4.752" xlink:type="simple">10.1093/humrep/15.4.752</ext-link></comment> <object-id pub-id-type="pmid">10739814</object-id></mixed-citation>
</ref>
<ref id="pone.0150738.ref064">
<label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ecochard</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Boehringer</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Rabilloud</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Marret</surname> <given-names>H</given-names></name>. <article-title>Chronological Aspects of Ultrasonic, Hormonal, and Other Indirect Indices of Ovulation</article-title>. <source>BJOG: An International Journal of Obstetrics &amp; Gynaecology</source>. <year>2001</year>;<volume>108</volume>(<issue>8</issue>):<fpage>822</fpage>–<lpage>829</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1471-0528.2001.00194.x" xlink:type="simple">10.1111/j.1471-0528.2001.00194.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref065">
<label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Alliende</surname> <given-names>ME</given-names></name>. <article-title>Mean versus individual hormonal profiles in the menstrual cycle</article-title>. <source>Fertility and sterility</source>. <year>2002</year>;<volume>78</volume>(<issue>1</issue>):<fpage>90</fpage>–<lpage>95</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0015-0282(02)03167-9" xlink:type="simple">10.1016/S0015-0282(02)03167-9</ext-link></comment> <object-id pub-id-type="pmid">12095496</object-id></mixed-citation>
</ref>
<ref id="pone.0150738.ref066">
<label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Direito</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bailly</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Mariani</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ecochard</surname> <given-names>R</given-names></name>. <article-title>Relationships between the luteinizing hormone surge and other characteristics of the menstrual cycle in normally ovulating women</article-title>. <source>Fertility and sterility</source>. <year>2013</year>;<volume>99</volume>(<issue>1</issue>):<fpage>279</fpage>–<lpage>285</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.fertnstert.2012.08.047" xlink:type="simple">10.1016/j.fertnstert.2012.08.047</ext-link></comment> <object-id pub-id-type="pmid">22999798</object-id></mixed-citation>
</ref>
<ref id="pone.0150738.ref067">
<label>67</label>
<mixed-citation publication-type="other" xlink:type="simple">Chen Y, Keogh E, Hu B, Begum N, Bagnall A, Mueen A, et al. The UCR Time Series Classification Archive; 2015. <ext-link ext-link-type="uri" xlink:href="http://www.cs.ucr.edu/~eamonn/time_series_data/" xlink:type="simple">www.cs.ucr.edu/~eamonn/time_series_data/</ext-link>.</mixed-citation>
</ref>
<ref id="pone.0150738.ref068">
<label>68</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Kadous</surname> <given-names>MW</given-names></name>. <source>Learning Comprehensible Descriptions of Multivariate Time Series</source>. In: <publisher-name>ICML</publisher-name>; <year>1999</year>. p. <fpage>454</fpage>–<lpage>463</lpage>.</mixed-citation>
</ref>
<ref id="pone.0150738.ref069">
<label>69</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Roverso</surname> <given-names>D</given-names></name>. <chapter-title>Multivariate temporal classification by windowed wavelet decomposition and recurrent neural networks</chapter-title>. In: <source>3rd ANS international topical meeting on nuclear plant instrumentation, control and human-machine interface</source>. <volume>vol. 20</volume>; <year>2000</year>. p. <fpage>1</fpage>–<lpage>8</lpage>.</mixed-citation>
</ref>
<ref id="pone.0150738.ref070">
<label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schwarz</surname> <given-names>G</given-names></name>. <article-title>Estimating the Dimension of a Model</article-title>. <source>The Annals of Statistics</source>. <year>1978</year>;<volume>6</volume>(<issue>2</issue>):<fpage>461</fpage>–<lpage>464</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/aos/1176344136" xlink:type="simple">10.1214/aos/1176344136</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref071">
<label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Akaike</surname> <given-names>H</given-names></name>. <article-title>A New Look at the Statistical Model Identification</article-title>. <source>Automatic Control, IEEE Transactions On</source>. <year>1974</year>;<volume>19</volume>(<issue>6</issue>):<fpage>716</fpage>–<lpage>723</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TAC.1974.1100705" xlink:type="simple">10.1109/TAC.1974.1100705</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref072">
<label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hurvich</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Tsai</surname> <given-names>CL</given-names></name>. <article-title>Regression and Time Series Model Selection in Small Samples</article-title>. <source>Biometrika</source>. <year>1989</year>;<volume>76</volume>(<issue>2</issue>):<fpage>297</fpage>–<lpage>307</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/biomet/76.2.297" xlink:type="simple">10.1093/biomet/76.2.297</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref073">
<label>73</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Bolstad</surname> <given-names>WM</given-names></name>. <source>Introduction to Bayesian Statistics</source>. <publisher-name>John Wiley &amp; Sons-Interscience</publisher-name>; <year>2007</year>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/9780470181188" xlink:type="simple">10.1002/9780470181188</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref074">
<label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Calinski</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Harabasz</surname> <given-names>J</given-names></name>. <article-title>A dendrite method for cluster analysis</article-title>. <source>Communications in Statistics</source>. <year>1974</year>;<volume>3</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>27</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/03610927408827101" xlink:type="simple">10.1080/03610927408827101</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref075">
<label>75</label>
<mixed-citation publication-type="other" xlink:type="simple">Ray S, Turi RH. Determination of number of clusters in k-means clustering and application in colour image segmentation. In: Proceedings of the 4th International Conference on Advances in Pattern Recognition and Digital Techniques (ICAPRDT’99), Calcutta, India; 1999. p. 137–143.</mixed-citation>
</ref>
<ref id="pone.0150738.ref076">
<label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Davies</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Bouldin</surname> <given-names>DW</given-names></name>. <article-title>A cluster separation measure</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>1979</year>;<volume>1</volume>(<issue>2</issue>):<fpage>224</fpage>–<lpage>227</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TPAMI.1979.4766909" xlink:type="simple">10.1109/TPAMI.1979.4766909</ext-link></comment> <object-id pub-id-type="pmid">21868852</object-id></mixed-citation>
</ref>
<ref id="pone.0150738.ref077">
<label>77</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Milligan</surname> <given-names>GW</given-names></name>, <name name-style="western"><surname>Cooper</surname> <given-names>MC</given-names></name>. <article-title>An examination of procedures for determining the number of clusters in a data set</article-title>. <source>Psychometrika</source>. <year>1985</year>;<volume>50</volume>(<issue>2</issue>):<fpage>159</fpage>–<lpage>179</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF02294245" xlink:type="simple">10.1007/BF02294245</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150738.ref078">
<label>78</label>
<mixed-citation publication-type="other" xlink:type="simple">Shim Y, Chung J, Choi IC. A Comparison Study of Cluster Validity Indices Using a Nonhierarchical Clustering Algorithm. In: Proceedings of CIMCA-IAWTIC’05-Volume 01. IEEE Computer Society Washington, DC, USA; 2005. p. 199–204.</mixed-citation>
</ref>
</ref-list>
</back>
</article>