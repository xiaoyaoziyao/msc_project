<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group>
<journal-title>PLoS ONE</journal-title></journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-13-55217</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0100805</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Developmental neuroscience</subject><subject>Neural networks</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Measuring Symmetry, Asymmetry and Randomness in Neural Network Connectivity</article-title>
<alt-title alt-title-type="running-head">Measuring Symmetry, Asymmetry and Randomness in Synaptic Connectivity</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Esposito</surname><given-names>Umberto</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Giugliano</surname><given-names>Michele</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>van Rossum</surname><given-names>Mark</given-names></name><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Vasilaki</surname><given-names>Eleni</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Department of Computer Science, University of Sheffield, Sheffield, United Kingdom</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Theoretical Neurobiology and Neuroengineering Laboratory, Department of Biomedical Sciences, University of Antwerp, Wilrijk, Belgium</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Laboratory of Neural Microcircuitry, Brain Mind Institute, École polytechnique fédérale de Lausanne, Lausanne, Switzerland</addr-line></aff>
<aff id="aff4"><label>4</label><addr-line>School of Informatics, University of Edinburgh, Edinburgh, United Kingdom</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Martinoia</surname><given-names>Sergio</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Genova, Italy</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">e.vasilaki@sheffield.ac.uk</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: EV UE. Performed the experiments: UE EV. Analyzed the data: UE EV. Contributed reagents/materials/analysis tools: MG MvR. Wrote the paper: UE EV. Edited the manuscript: MG MvR.</p></fn>
</author-notes>
<pub-date pub-type="collection"><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>9</day><month>7</month><year>2014</year></pub-date>
<volume>9</volume>
<issue>7</issue>
<elocation-id>e100805</elocation-id>
<history>
<date date-type="received"><day>31</day><month>12</month><year>2013</year></date>
<date date-type="accepted"><day>29</day><month>5</month><year>2014</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Esposito et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Cognitive functions are stored in the connectome, the wiring diagram of the brain, which exhibits non-random features, so-called <italic>motifs</italic>. In this work, we focus on bidirectional, symmetric motifs, i.e. two neurons that project to each other via connections of equal strength, and unidirectional, non-symmetric motifs, i.e. within a pair of neurons only one neuron projects to the other. We hypothesise that such motifs have been shaped via activity dependent synaptic plasticity processes. As a consequence, learning moves the distribution of the synaptic connections away from randomness. Our aim is to provide a global, macroscopic, single parameter characterisation of the statistical occurrence of bidirectional and unidirectional motifs. To this end we define a symmetry measure that does not require any <italic>a priori</italic> thresholding of the weights or knowledge of their maximal value. We calculate its mean and variance for random uniform or Gaussian distributions, which allows us to introduce a confidence measure of how significantly symmetric or asymmetric a specific configuration is, i.e. how likely it is that the configuration is the result of chance. We demonstrate the discriminatory power of our symmetry measure by inspecting the eigenvalues of different types of connectivity matrices. We show that a Gaussian weight distribution biases the connectivity motifs to more symmetric configurations than a uniform distribution and that introducing a random synaptic pruning, mimicking developmental regulation in synaptogenesis, biases the connectivity motifs to more asymmetric configurations, regardless of the distribution. We expect that our work will benefit the computational modelling community, by providing a systematic way to characterise symmetry and asymmetry in network structures. Further, our symmetry measure will be of use to electrophysiologists that investigate symmetry of network connectivity.</p>
</abstract>
<funding-group><funding-statement>This work was partly supported by the European Commission (FP7 Marie Curie Initial Training Network “NAMASEN”, grant n. 264872 (EV, MG), the Royal Society travel grant JP091330-2009/R4 (EV, MG), the Interuniversity Attraction Poles Program (IUAP), initiated by the Belgian Science Policy Office (MG), the Future Emerging Technology programme project “BRAINLEAP” grant n. 306502 (MG), the Engineering and Physical Sciences Research Council (EPSRC), grant n. EP/J019534/1 (EV) and the EPSRC e-futures award EFXD12003/EFXD12004 (EV). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="16"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>It is widely believed that cognitive functions are stored in the so-called <italic>connectome</italic> <xref ref-type="bibr" rid="pone.0100805-Sporns1">[1]</xref>, <xref ref-type="bibr" rid="pone.0100805-Lichtman1">[2]</xref>, the wiring diagram of the brain. Due to improvements in technology, experimental techniques and computational paradigms <xref ref-type="bibr" rid="pone.0100805-Smith1">[3]</xref>, <xref ref-type="bibr" rid="pone.0100805-Luo1">[4]</xref>, the investigation of the connectome, known as <italic>connectomics</italic>, has generated great excitement <xref ref-type="bibr" rid="pone.0100805-Seung1">[5]</xref> and has made significant progress <xref ref-type="bibr" rid="pone.0100805-White1">[6]</xref>–<xref ref-type="bibr" rid="pone.0100805-Bock1">[9]</xref> resulting in a rapid proliferation of neuroscience datasets <xref ref-type="bibr" rid="pone.0100805-Koetter1">[10]</xref>–<xref ref-type="bibr" rid="pone.0100805-Briggman2">[13]</xref>.</p>
<p>Studies on the brain wiring diagram have shown that connectivity is non-random, highlighting the existence of specific connectivity motifs at the microcircuit level, see for instance <xref ref-type="bibr" rid="pone.0100805-Song1">[14]</xref>–<xref ref-type="bibr" rid="pone.0100805-Perin1">[17]</xref>. Of particular interest are the motifs that exhibit bidirectional (reciprocal) and unidirectional (non-reciprocal) connections between pairs of neurons. More specifically, theoretical work <xref ref-type="bibr" rid="pone.0100805-Clopath1">[18]</xref> studied the development of unidirectional connectivity due to long-term plasticity in an artificial network of spiking neurons under a <italic>temporal coding scheme</italic>, where it is assumed that the time at which neurons fire carries out important information. This finding is correlated to unidirectional connectivity observed in somatosensory cortex, see <xref ref-type="bibr" rid="pone.0100805-Lefort1">[19]</xref>. In <xref ref-type="bibr" rid="pone.0100805-Clopath1">[18]</xref> the development of bidirectional connectivity in the same network under a <italic>frequency coding scheme</italic>, where information is transmitted in the firing rate of the neurons, was also studied and correlated to bidirectional connectivity found in the visual cortex <xref ref-type="bibr" rid="pone.0100805-Song1">[14]</xref>. Complementary to this work, in <xref ref-type="bibr" rid="pone.0100805-Vasilaki1">[20]</xref>, <xref ref-type="bibr" rid="pone.0100805-Vasilaki2">[21]</xref> the authors explored the experimentally identified correlation of bidirectional and unidirectional connectivity to short-term synaptic dynamics, see <xref ref-type="bibr" rid="pone.0100805-Pignatelli1">[22]</xref>, by studying the development of connectivity in networks with facilitating and depressing synapses due to the interaction of short-term and long-term plasticities. The role of synaptic long-term plasticity in structures formation within networks has been also investigated in <xref ref-type="bibr" rid="pone.0100805-Babadi1">[23]</xref>–<xref ref-type="bibr" rid="pone.0100805-Bourjaily2">[25]</xref>.</p>
<p>Similar to <xref ref-type="bibr" rid="pone.0100805-Clopath1">[18]</xref> and <xref ref-type="bibr" rid="pone.0100805-Vasilaki1">[20]</xref>, <xref ref-type="bibr" rid="pone.0100805-Vasilaki2">[21]</xref>, we hypothesise that the above mentioned motifs have been shaped via activity dependent synaptic plasticity processes, and that learning moves the distribution of the synaptic connections away from randomness. Our aim is to provide a global, macroscopic, single parameter characterisation of the statistical occurrence of bidirectional and unidirectional motifs. To this end:</p>
<list list-type="order"><list-item>
<p>We define a symmetry measure that does not require any a priori thresholding of the weights or knowledge of their maximal value, and hence is applicable to both simulations and experimental data.</p>
</list-item><list-item>
<p>We calculate the mean and variance of this symmetry measure for random uniform or Gaussian distributions, which allows us to introduce a confidence measure of how significantly symmetric or asymmetric is a specific configuration, i.e. how likely it is that the configuration is the result of chance.</p>
</list-item><list-item>
<p>We demonstrate the discriminatory power of our symmetry measure by inspecting the eigenvalues of different types of connectivity matrices, given that symmetric matrices are known to have real eigenvalues.</p>
</list-item><list-item>
<p>We show that a Gaussian distribution biases the connectivity motifs to more symmetric configurations than a uniform distribution and that introducing a random synaptic pruning, mimicking developmental regulation in synaptogenesis, biases the connectivity motifs to more asymmetric configurations, regardless of the distribution. Our statistics of the symmetry measure allows us to correctly evaluate the significance of a symmetric or asymmetric network configuration in both these cases.</p>
</list-item><list-item>
<p>Our symmetry measure allows us to observe the evolution of a specific network configuration, as we exemplify in our results.</p>
</list-item></list>
<p>We expect that our work will benefit the computational modelling community, by providing a systematic way to characterise symmetry and asymmetry in network structures. Further, our symmetry measure will be of use to electrophysiologists that may investigate symmetric or asymmetric network connectivity.</p>
</sec><sec id="s2" sec-type="methods">
<title>Methods</title>
<p>In what follows, we first define a novel measure that quantifies the degree of symmetry in a neuronal network with excitatory synaptic connections. More specifically, we describe the strength of the synaptic efficacies between the neurons by the elements of a square matrix, i.e. the connectivity matrix, to which we associate a number that quantifies the similarity of the elements above the matrix diagonal to those below the diagonal. We further study this measure from a statistical point of view, by means of both analytical tools and numerical simulations. Aiming to associate a significance value to the measure, i.e. the probability that a certain symmetric or non-symmetric configuration is the result of chance, we consider random synaptic efficacies drawn from uniform and Gaussian distributions. We also study how our symmetry measure is affected by the anatomical disconnection of neurons in a random manner, i.e. zeroing some entries in the connectivity matrix. Finally, we anticipate that connectivity distributions are modified by activity-dependent processes and we describe the structure of the network we use as a demonstrative example in the <xref ref-type="sec" rid="s3">Results</xref> section.</p>
<sec id="s2a">
<title>Definitions</title>
<p>Let us consider the adjacency (or connectivity) matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e001" xlink:type="simple"/></inline-formula> of a weighted directed network <xref ref-type="bibr" rid="pone.0100805-Newman1">[26]</xref>, composed of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e002" xlink:type="simple"/></inline-formula> vertices and without self-edges. The <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e003" xlink:type="simple"/></inline-formula> vertices represent the neurons, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e004" xlink:type="simple"/></inline-formula> possible synaptic connections among them. The synaptic efficacy between two neurons is expressed as a positive element <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e005" xlink:type="simple"/></inline-formula> in the adjacency matrix. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e006" xlink:type="simple"/></inline-formula> is thus composed by positive elements off-diagonal, taking values in the bounded range <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e007" xlink:type="simple"/></inline-formula> and by zero diagonal entries. We define <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e008" xlink:type="simple"/></inline-formula> as a measure of the symmetry of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e009" xlink:type="simple"/></inline-formula><disp-formula id="pone.0100805.e010"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e010" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e011" xlink:type="simple"/></inline-formula> is the number of instances where both <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e012" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e013" xlink:type="simple"/></inline-formula> are zero, i.e. there is no connection between two neurons. The term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e014" xlink:type="simple"/></inline-formula> is a normalisation factor that represents the total number of synaptic connection pairs that have at least one non-zero connection. A value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e015" xlink:type="simple"/></inline-formula> near 0 indicates that there are virtually no reciprocal connections in the network, while a value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e016" xlink:type="simple"/></inline-formula> near 1 indicates that virtually all connections are reciprocal. We exclude (0,0) pairs from our definition of the symmetry measure. Mathematically such pairs would introduce undefined terms to <xref ref-type="disp-formula" rid="pone.0100805.e010">Eq. (1)</xref>. In addition, conceptually, we expect that small weights will not be experimentally measurable. It is then reasonable to exclude them, expecting to effectively increase the signal to noise ratio.</p>
<sec id="s2a1">
<title>Pruning and Plasticity</title>
<p>We assume that a connection <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e017" xlink:type="simple"/></inline-formula> is permanently disconnected and set to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e018" xlink:type="simple"/></inline-formula> with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e019" xlink:type="simple"/></inline-formula> Consequently, the probability that two neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e020" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e021" xlink:type="simple"/></inline-formula> are mutually disconnected, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e022" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e023" xlink:type="simple"/></inline-formula> When a connection is permanently <italic>pruned</italic> in such a way, its efficacy remains <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e024" xlink:type="simple"/></inline-formula> all the time, whereas the off-diagonal non-pruned values of the adjacency matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e025" xlink:type="simple"/></inline-formula> change slowly in time, as a result of activity-dependent synaptic plasticity. We consider that this procedure correlates with developmental mechanisms associated with or following synaptogenesis.</p>
</sec><sec id="s2a2">
<title>Unidirectional and Bidirectional connection pairs</title>
<p>We associate the quantity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e026" xlink:type="simple"/></inline-formula> i.e. the term of the summation in the <xref ref-type="disp-formula" rid="pone.0100805.e010">Eq. (1)</xref> to the neuronal pair <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e027" xlink:type="simple"/></inline-formula> This term maps the strength of the connections between two neurons to a single variable. Each connection pair can therefore be bidirectional if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e028" xlink:type="simple"/></inline-formula> unidirectional if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e029" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e030" xlink:type="simple"/></inline-formula> or none of the two. As a consequence, a network can be dominated by bidirectional connectivity, by unidirectional connectivity, or it may exhibit random features.</p>
</sec><sec id="s2a3">
<title>Weight Bounds</title>
<p>In what follows we consider the case of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e031" xlink:type="simple"/></inline-formula> Due to the term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e032" xlink:type="simple"/></inline-formula> this can be done without loss of generality.</p>
</sec></sec><sec id="s2b">
<title>Statistics of s</title>
<p>Let us consider a large number of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e033" xlink:type="simple"/></inline-formula> instances of a network whose connection weights are randomly distributed. Each adjacency matrix can be evaluated via our symmetry measure. We rewrite <xref ref-type="disp-formula" rid="pone.0100805.e010">Eq. 1</xref> as:<disp-formula id="pone.0100805.e034"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e034" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e035" xlink:type="simple"/></inline-formula> is a linear index running over all the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e036" xlink:type="simple"/></inline-formula> non-zero “connection pairs” within the network. We can then estimate the mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e037" xlink:type="simple"/></inline-formula> and variance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e038" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e039" xlink:type="simple"/></inline-formula> over all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e040" xlink:type="simple"/></inline-formula> networks as:<disp-formula id="pone.0100805.e041"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e041" xlink:type="simple"/><label>(3)</label></disp-formula><disp-formula id="pone.0100805.e042"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e042" xlink:type="simple"/><label>(4)</label></disp-formula>where the notation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e043" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e044" xlink:type="simple"/></inline-formula> implies that the expected value and variance are computed along the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e045" xlink:type="simple"/></inline-formula> different representations of the network.</p>
<p><xref ref-type="disp-formula" rid="pone.0100805.e041">Eq. (3)</xref>, <xref ref-type="disp-formula" rid="pone.0100805.e042">(4)</xref> allow us to transfer the statistical analysis from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e046" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e047" xlink:type="simple"/></inline-formula> To derive theoretical formulas for mean value and variance of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e048" xlink:type="simple"/></inline-formula> we use the fact that its probability density function (PDF), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e049" xlink:type="simple"/></inline-formula> can be written as a joint distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e050" xlink:type="simple"/></inline-formula> where we have introduced the notation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e051" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e052" xlink:type="simple"/></inline-formula><disp-formula id="pone.0100805.e053"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e053" xlink:type="simple"/><label>(5)</label></disp-formula>within the range <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e054" xlink:type="simple"/></inline-formula> defined by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e055" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e056" xlink:type="simple"/></inline-formula> Similarly, we can calculate the variance as follows:<disp-formula id="pone.0100805.e057"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e057" xlink:type="simple"/><label>(6)</label></disp-formula></p>
<p>We note that mean value and variance of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e058" xlink:type="simple"/></inline-formula> can be numerically estimated either by using a large set of small networks or on a single very large network: What matters is that the total number of connection pairs, given by the product <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e059" xlink:type="simple"/></inline-formula> is sufficiently large to guarantee good statistics and that connection pairs are independent of each other. In the calculations below, we assume a very large adjacency matrix.</p>
</sec><sec id="s2c">
<title>Adjacency matrix with uniform random values</title>
<p>We first consider a network with randomly distributed connections without pruning, followed by the more general case where pruning is taken into account.</p>
<sec id="s2c1">
<title>Fully connected network</title>
<p>For the uniform distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e060" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e061" xlink:type="simple"/></inline-formula> see <xref ref-type="fig" rid="pone-0100805-g001">Fig. 1A</xref>. The probability of having <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e062" xlink:type="simple"/></inline-formula> for at least one pair <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e063" xlink:type="simple"/></inline-formula> is negligible, hence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e064" xlink:type="simple"/></inline-formula> It is straightforward to derive the distributions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e065" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e066" xlink:type="simple"/></inline-formula> depicted in <xref ref-type="fig" rid="pone-0100805-g001">Fig. 1B,C</xref> correspondingly:<disp-formula id="pone.0100805.e067"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e067" xlink:type="simple"/><label>(7)</label></disp-formula><disp-formula id="pone.0100805.e068"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e068" xlink:type="simple"/><label>(8)</label></disp-formula></p>
<fig id="pone-0100805-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0100805.g001</object-id><label>Figure 1</label><caption>
<title>Probability density functions for the case of uniformly distributed connections.</title>
<p><bold>A</bold> Distribution of the uniform variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e069" xlink:type="simple"/></inline-formula> <bold>B</bold> Distribution of the sum <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e070" xlink:type="simple"/></inline-formula> of two uniform variables. <bold>C</bold> Distribution of the absolute difference <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e071" xlink:type="simple"/></inline-formula> of two uniform variables. <bold>D</bold> Joint distribution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e072" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e073" xlink:type="simple"/></inline-formula> <bold>E, F, G</bold> The same as A, B and C but with pruning <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e074" xlink:type="simple"/></inline-formula> <bold>H</bold> The same as D but with pruning <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e075" xlink:type="simple"/></inline-formula> In all figures, <italic>Grey shaded area</italic>: histograms from simulations, <italic>Black lines and surfaces</italic>: theoretical results (see <xref ref-type="disp-formula" rid="pone.0100805.e067">Eq. (7)</xref>–<xref ref-type="disp-formula" rid="pone.0100805.e108">(13)</xref>).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0100805.g001" position="float" xlink:type="simple"/></fig>
<p>We can therefore obtain the joint PDF (<xref ref-type="fig" rid="pone-0100805-g001">Fig. 1D</xref>):<disp-formula id="pone.0100805.e076"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e076" xlink:type="simple"/><label>(9)</label></disp-formula></p>
</sec><sec id="s2c2">
<title>Pruning</title>
<p>Introducing pruning to the elements of the adjacency matrix, with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e077" xlink:type="simple"/></inline-formula> corresponds to a discontinuous probability distribution function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e078" xlink:type="simple"/></inline-formula> that can be written as a sum of a continuous function and of a Dirac's Delta centred in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e079" xlink:type="simple"/></inline-formula> (see also <xref ref-type="fig" rid="pone-0100805-g001">Fig. 1E</xref>):<disp-formula id="pone.0100805.e080"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e080" xlink:type="simple"/><label>(10)</label></disp-formula></p>
<p>Now the (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e081" xlink:type="simple"/></inline-formula>) pairs have to be explicitly excluded from the distributions of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e082" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e083" xlink:type="simple"/></inline-formula> Also, the number of pairs of the type (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e084" xlink:type="simple"/></inline-formula>) increases, resulting in the appearance of a uniform contribution in the region <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e085" xlink:type="simple"/></inline-formula> in both the PDF of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e086" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e087" xlink:type="simple"/></inline-formula> Their final exact profile can be obtained by considering the possible combinations of drawing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e088" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e089" xlink:type="simple"/></inline-formula> from the above pruned distribution and their corresponding probability of occurrence. There are four contributions: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e090" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e091" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e092" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e093" xlink:type="simple"/></inline-formula> The last term, which describes the (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e094" xlink:type="simple"/></inline-formula>) pairs, has to be subtracted and the remaining expression has to be renormalised. The results are graphically shown in <xref ref-type="fig" rid="pone-0100805-g001">Fig. 1F, 1G</xref> and are mathematically described by the following expressions:<disp-formula id="pone.0100805.e095"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e095" xlink:type="simple"/><label>(11)</label></disp-formula><disp-formula id="pone.0100805.e096"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e096" xlink:type="simple"/><label>(12)</label></disp-formula></p>
<p>The joint PDF is a mixture of two uniform distributions: the unpruned distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e097" xlink:type="simple"/></inline-formula> and the contribution from the pruning, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e098" xlink:type="simple"/></inline-formula> which is a delta peak along the line <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e099" xlink:type="simple"/></inline-formula> see <xref ref-type="fig" rid="pone-0100805-g001">Fig. 1H</xref>. To obtain <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e100" xlink:type="simple"/></inline-formula> the two unitary distributions are mixed with some coefficients <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e101" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e102" xlink:type="simple"/></inline-formula> satisfying the normalisation condition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e103" xlink:type="simple"/></inline-formula> With the same arguments used for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e104" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e105" xlink:type="simple"/></inline-formula> we can derive the relation between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e106" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e107" xlink:type="simple"/></inline-formula> so that we can finally write:<disp-formula id="pone.0100805.e108"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e108" xlink:type="simple"/><label>(13)</label></disp-formula></p>
</sec><sec id="s2c3">
<title>Expected value and variance of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e109" xlink:type="simple"/></inline-formula></title>
<p>We can calculate mean value and variance of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e110" xlink:type="simple"/></inline-formula> by plugging <xref ref-type="disp-formula" rid="pone.0100805.e108">Eq. (13)</xref> into <xref ref-type="disp-formula" rid="pone.0100805.e053">Eqs (5)</xref> and <xref ref-type="disp-formula" rid="pone.0100805.e057">(6)</xref>:<disp-formula id="pone.0100805.e111"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e111" xlink:type="simple"/><label>(14)</label></disp-formula><disp-formula id="pone.0100805.e112"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e112" xlink:type="simple"/><label>(15)</label></disp-formula></p>
</sec><sec id="s2c4">
<title>Expected value and variance of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e113" xlink:type="simple"/></inline-formula></title>
<p>By combining the above results with <xref ref-type="disp-formula" rid="pone.0100805.e041">Eqs (3)</xref> and <xref ref-type="disp-formula" rid="pone.0100805.e042">(4)</xref>, we can derive the final formulas for the expected value and variance of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e114" xlink:type="simple"/></inline-formula><disp-formula id="pone.0100805.e115"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e115" xlink:type="simple"/><label>(16)</label></disp-formula><disp-formula id="pone.0100805.e116"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e116" xlink:type="simple"/><label>(17)</label></disp-formula></p>
</sec></sec><sec id="s2d">
<title>Adjacency matrix with Gaussian-distributed random values</title>
<p>The procedure described above to derive the joint PDF of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e117" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e118" xlink:type="simple"/></inline-formula> is applicable to any distribution. In what follows, we consider a network with initial connections drawn by a truncated Gaussian distribution.</p>
<sec id="s2d1">
<title>Distribution of connections</title>
<p>Whereas the uniform distribution is well defined in any finite interval, the Gaussian distribution requires some considerations. Strictly speaking, any Gaussian distribution is defined over the entire real axes. For practical reasons, however, for any finite network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e119" xlink:type="simple"/></inline-formula> the maximum and the minimum values of the weights, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e120" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e121" xlink:type="simple"/></inline-formula> are always well defined, and therefore the actual distribution is a truncated Gaussian. To be able to consider the truncated Gaussian distribution as Gaussian with satisfactory accuracy, we require that the portion of the Gaussian enclosed in the region <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e122" xlink:type="simple"/></inline-formula> is as close as possible to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e123" xlink:type="simple"/></inline-formula> This means that the distribution has to be narrow enough with respect to the interval of definition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e124" xlink:type="simple"/></inline-formula> Also, by definition, the distribution has to be symmetric in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e125" xlink:type="simple"/></inline-formula> Because we are considering only excitatory connections then <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e126" xlink:type="simple"/></inline-formula> so as the mean value has to be <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e127" xlink:type="simple"/></inline-formula> On the other hand, the narrowness imposes a condition on the standard deviation of the distribution: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e128" xlink:type="simple"/></inline-formula>. Since we can set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e129" xlink:type="simple"/></inline-formula> without loss of generalization, the entire study on all the possible Gaussian distributions can be limited to a special class, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e130" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s2d2">
<title>The choice of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e131" xlink:type="simple"/></inline-formula></title>
<p>To guarantee a good approximation of a Gaussian distribution, we define the truncated Gaussian distribution such that points within <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e132" xlink:type="simple"/></inline-formula> fall in [0, 1] leading to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e133" xlink:type="simple"/></inline-formula> and a truncation error <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e134" xlink:type="simple"/></inline-formula></p>
</sec><sec id="s2d3">
<title>Fully connected network</title>
<p>For the truncated Gaussian distribution defined above, the distribution of connections without pruning is (see also <xref ref-type="fig" rid="pone-0100805-g002">Fig. 2A</xref>):<disp-formula id="pone.0100805.e135"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e135" xlink:type="simple"/><label>(18)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e136" xlink:type="simple"/></inline-formula> denotes the normal distribution. Since combinations of Gaussian distributions are also Gaussian distributions, we can immediately derive the PDF of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e137" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e138" xlink:type="simple"/></inline-formula> Then, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e139" xlink:type="simple"/></inline-formula> is simply the positive half of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e140" xlink:type="simple"/></inline-formula> but scaled by a factor of two because of the normalization. We obtain (<xref ref-type="fig" rid="pone-0100805-g002">Fig. 2B,C</xref>):<disp-formula id="pone.0100805.e141"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e141" xlink:type="simple"/><label>(19)</label></disp-formula><disp-formula id="pone.0100805.e142"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e142" xlink:type="simple"/><label>(20)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e143" xlink:type="simple"/></inline-formula> identifies the normalised (positive) half of a normal distribution. Similarly, the joint distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e144" xlink:type="simple"/></inline-formula> can be easily derived from the bivariate Gaussian of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e145" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e146" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pone-0100805-g002">Fig. 2D</xref>):<disp-formula id="pone.0100805.e147"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e147" xlink:type="simple"/><label>(21)</label></disp-formula>with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e148" xlink:type="simple"/></inline-formula> being the normalised half (where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e149" xlink:type="simple"/></inline-formula>) of a bivariate normal distribution.</p>
<fig id="pone-0100805-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0100805.g002</object-id><label>Figure 2</label><caption>
<title>Probability density functions for the case of Gaussian-distributed connections.</title>
<p><bold>A</bold> Distribution of the Gaussian variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e150" xlink:type="simple"/></inline-formula> <bold>B</bold> Distribution of the sum <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e151" xlink:type="simple"/></inline-formula> of two Gaussian-distributed variables. <bold>C</bold> Distribution of the absolute difference <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e152" xlink:type="simple"/></inline-formula> of two Gaussian-distributed variables. <bold>D</bold> Joint distribution of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e153" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e154" xlink:type="simple"/></inline-formula> <bold>E, F, G, H</bold> The same as A, B, C and D but with pruning <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e155" xlink:type="simple"/></inline-formula> In all the figures, <italic>Grey shaded area</italic>: histograms from simulations, <italic>Black lines and surfaces</italic>: theoretical results (see <xref ref-type="disp-formula" rid="pone.0100805.e141">Eq. (19)</xref>–<xref ref-type="disp-formula" rid="pone.0100805.e164">(25)</xref>).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0100805.g002" position="float" xlink:type="simple"/></fig></sec><sec id="s2d4">
<title>Pruning</title>
<p>When taking pruning into account, each PDF can be considered as a mixture of the unpruned distribution and the contribution coming from the pruning. We can therefore write:<disp-formula id="pone.0100805.e156"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e156" xlink:type="simple"/><label>(22)</label></disp-formula><disp-formula id="pone.0100805.e157"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e157" xlink:type="simple"/><label>(23)</label></disp-formula><disp-formula id="pone.0100805.e158"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e158" xlink:type="simple"/><label>(24)</label></disp-formula></p>
<p>The above distributions are plotted in <xref ref-type="fig" rid="pone-0100805-g002">Fig. 2E, 2F, 2G</xref>.</p>
<p>Finally, the joint PDF is again a mixture model, with a univariate Gaussian peak profile on the line <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e159" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pone-0100805-g002">Fig. 2H</xref>). Note that this peak can be described by the intersection of the plane <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e160" xlink:type="simple"/></inline-formula> with the full unpruned bivariate normal distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e161" xlink:type="simple"/></inline-formula> transformed to have its mean in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e162" xlink:type="simple"/></inline-formula> This operation implies a re-normalisation by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e163" xlink:type="simple"/></inline-formula> of the resulting univariate Gaussian. Then, we can write:<disp-formula id="pone.0100805.e164"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e164" xlink:type="simple"/><label>(25)</label></disp-formula></p>
</sec><sec id="s2d5">
<title>Correlation in the bivariate Gaussian</title>
<p>The correlation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e165" xlink:type="simple"/></inline-formula> between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e166" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e167" xlink:type="simple"/></inline-formula> appearing in the off-diagonal terms of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e168" xlink:type="simple"/></inline-formula>, can be computed by running a numerical simulation. We estimated <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e169" xlink:type="simple"/></inline-formula> as a mean value over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e170" xlink:type="simple"/></inline-formula> representations of a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e171" xlink:type="simple"/></inline-formula>-neuron network with random connections distributed according to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e172" xlink:type="simple"/></inline-formula> and with no pruning, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e173" xlink:type="simple"/></inline-formula> The result is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e174" xlink:type="simple"/></inline-formula> which allows to treat <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e175" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e176" xlink:type="simple"/></inline-formula> as independent variables and then to factorise the bivariate normal distribution <xref ref-type="disp-formula" rid="pone.0100805.e147">Eq. (21)</xref> in the product of the two single distributions. Indeed, by introducing the Heaviside step function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e177" xlink:type="simple"/></inline-formula> and the re-normalisation parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e178" xlink:type="simple"/></inline-formula> we can write:<disp-formula id="pone.0100805.e179"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e179" xlink:type="simple"/><label>(26)</label></disp-formula></p>
<p>We note that the pruning case does not require a different calculation and can be treated as the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e180" xlink:type="simple"/></inline-formula> case. This is because we are describing the effect of the pruning with a separate (univariate) function, i.e. the halved bivariate normal distribution describes only the unpruned part of the network, see <xref ref-type="disp-formula" rid="pone.0100805.e164">Eq. (25)</xref>.</p>
<p>The suitability of this approximation is also certified by <xref ref-type="fig" rid="pone-0100805-g002">Fig. 2D,H</xref>, where the agreement between simulation results and theoretical fit with <xref ref-type="disp-formula" rid="pone.0100805.e179">Eq. (26)</xref> is excellent.</p>
</sec><sec id="s2d6">
<title>Expected value and variance of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e181" xlink:type="simple"/></inline-formula></title>
<p>Now we can insert the expression of the joint distribution, <xref ref-type="disp-formula" rid="pone.0100805.e164">Eq. (25)</xref>, into <xref ref-type="disp-formula" rid="pone.0100805.e041">Eq. (3)</xref> and <xref ref-type="disp-formula" rid="pone.0100805.e042">Eq. (4)</xref>:<disp-formula id="pone.0100805.e182"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e182" xlink:type="simple"/><label>(27)</label></disp-formula><disp-formula id="pone.0100805.e183"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e183" xlink:type="simple"/><label>(28)</label></disp-formula></p>
<p>To calculate the above expression we use symbolic integration.</p>
</sec><sec id="s2d7">
<title>Expected value and variance of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e184" xlink:type="simple"/></inline-formula></title>
<p>By plugging the above results into <xref ref-type="disp-formula" rid="pone.0100805.e041">Eq. (3)</xref>, <xref ref-type="disp-formula" rid="pone.0100805.e042">(4)</xref>, we obtain:<disp-formula id="pone.0100805.e185"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e185" xlink:type="simple"/><label>(29)</label></disp-formula><disp-formula id="pone.0100805.e186"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e186" xlink:type="simple"/><label>(30)</label></disp-formula></p>
<p>The four formulas <xref ref-type="disp-formula" rid="pone.0100805.e115">Eq. (16)</xref>, <xref ref-type="disp-formula" rid="pone.0100805.e116">(17)</xref>, <xref ref-type="disp-formula" rid="pone.0100805.e185">(29)</xref>, <xref ref-type="disp-formula" rid="pone.0100805.e186">(30)</xref> are the final result of the statistical analysis and they will be discussed in the <xref ref-type="sec" rid="s3">Results</xref> section.</p>
</sec></sec><sec id="s2e">
<title>Model network with plastic weights</title>
<p>Below we describe the model neural network on which we will apply our symmetry measure.</p>
<sec id="s2e1">
<title>Single-neuron dynamics</title>
<p>We simulated <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e187" xlink:type="simple"/></inline-formula> leaky integrate-and-fire neurons <xref ref-type="bibr" rid="pone.0100805-Dayan1">[27]</xref> with a firing threshold of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e188" xlink:type="simple"/></inline-formula> The sub-threshold dynamics of the electrical potential <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e189" xlink:type="simple"/></inline-formula> is given by:<disp-formula id="pone.0100805.e190"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e190" xlink:type="simple"/><label>(31)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e191" xlink:type="simple"/></inline-formula> is the membrane time constant, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e192" xlink:type="simple"/></inline-formula> is the resting potential, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e193" xlink:type="simple"/></inline-formula> is the membrane resistance and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e194" xlink:type="simple"/></inline-formula> is the input signal. We chose <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e195" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e196" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e197" xlink:type="simple"/></inline-formula> To introduce noise in the firing process of neurons, we implemented the escape noise model <xref ref-type="bibr" rid="pone.0100805-Gerstner1">[28]</xref>. At each time-step <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e198" xlink:type="simple"/></inline-formula> the probability that the neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e199" xlink:type="simple"/></inline-formula> fires is given by:<disp-formula id="pone.0100805.e200"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e200" xlink:type="simple"/><label>(32)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e201" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e202" xlink:type="simple"/></inline-formula> Once a neuron fires, its membrane potential is reset to the resting value.</p>
</sec><sec id="s2e2">
<title>Synaptic and External Inputs</title>
<p>The input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e203" xlink:type="simple"/></inline-formula> to each neuron has two components: a synaptic part, coming from the action potentials of the other neurons, and an external part, which is defined by the applied protocol:<disp-formula id="pone.0100805.e204"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e204" xlink:type="simple"/><label>(33)</label></disp-formula></p>
<p>In the synaptic term, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e205" xlink:type="simple"/></inline-formula> are the synaptic weights, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e206" xlink:type="simple"/></inline-formula> is the firing time of the presynaptic neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e207" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e208" xlink:type="simple"/></inline-formula> is a small positive number accounting for the delivering time of the electrical signal from the presynaptic to the postsynaptic neuron. The term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e209" xlink:type="simple"/></inline-formula> is the time course of the injected input, which is different from neuron to neuron and depends on the protocol we use (see <xref ref-type="sec" rid="s3">Results</xref> section). Finally, the amplitudes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e210" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e211" xlink:type="simple"/></inline-formula> are fixed to the same value for all neurons. We chose <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e212" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e213" xlink:type="simple"/></inline-formula> so that each external input forces the neurons to fire.</p>
</sec><sec id="s2e3">
<title>Plasticity</title>
<p>The efficacy of the synaptic connections is activity-dependent. Therefore, the unpruned elements of the adjacency matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e214" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pone.0100805.e204">Eq. (33)</xref> change in time by Spike-Timing Dependent Plasticity (STDP) mechanisms, i.e. passively driven by the input protocol and emerging internal dynamics, without the presence of a supervisory or reinforcement learning signal <xref ref-type="bibr" rid="pone.0100805-Vasilaki3">[29]</xref>–<xref ref-type="bibr" rid="pone.0100805-Richmond1">[31]</xref>. More specifically, we implemented the triplet STDP rule <xref ref-type="bibr" rid="pone.0100805-Clopath1">[18]</xref>, <xref ref-type="bibr" rid="pone.0100805-Pfister1">[32]</xref>, <xref ref-type="bibr" rid="pone.0100805-Clopath2">[33]</xref> with parameters from <xref ref-type="bibr" rid="pone.0100805-Pfister1">[32]</xref> (Visual cortex, nearest neighbour dataset), see <xref ref-type="table" rid="pone-0100805-t001">Table 1</xref>, and we constrain the connections in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e215" xlink:type="simple"/></inline-formula> In this model, each neuron has two presynaptic variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e216" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e217" xlink:type="simple"/></inline-formula> and two postsynaptic variables <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e218" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e219" xlink:type="simple"/></inline-formula> In the absence of any activity, these variables exponentially decay towards zero with different time constants:<disp-formula id="pone.0100805.e220"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e220" xlink:type="simple"/><label>(34)</label></disp-formula>whereas when the neuron elicits a spike they increase by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e221" xlink:type="simple"/></inline-formula><disp-formula id="pone.0100805.e222"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e222" xlink:type="simple"/><label>(35)</label></disp-formula></p>
<table-wrap id="pone-0100805-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0100805.t001</object-id><label>Table 1</label><caption>
<title>List of parameters used for the case study.</title>
</caption><alternatives><graphic id="pone-0100805-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0100805.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Symbol</td>
<td align="left" rowspan="1" colspan="1">Description</td>
<td align="left" rowspan="1" colspan="1">Value</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e223" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Number of neurons</td>
<td align="left" rowspan="1" colspan="1">30</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e224" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Membrane time constant</td>
<td align="left" rowspan="1" colspan="1">10 ms</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e225" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Membrane resistance</td>
<td align="left" rowspan="1" colspan="1">1 K<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e226" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e227" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Resting and after-spike reset potential</td>
<td align="left" rowspan="1" colspan="1">−70 mV</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e228" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Threshold potential for spike emission</td>
<td align="left" rowspan="1" colspan="1">−50 mV</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e229" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Voltage increase due to a presynaptic event</td>
<td align="left" rowspan="1" colspan="1">1 mV</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e230" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Voltage increase due to an external event</td>
<td align="left" rowspan="1" colspan="1">30 mV</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e231" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Lower bound for synaptic weights</td>
<td align="left" rowspan="1" colspan="1">0</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e232" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Higher bound for synaptic weights</td>
<td align="left" rowspan="1" colspan="1">1</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e233" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Mean value of Gaussian-distributed initial weights</td>
<td align="left" rowspan="1" colspan="1">0.5</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e234" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Variance of Gaussian-distributed initial weights</td>
<td align="left" rowspan="1" colspan="1">0.01</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e235" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Amplitude of weights change - pair term in Long-Term Potentiation</td>
<td align="left" rowspan="1" colspan="1">4.6<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e236" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e237" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Amplitude of weights change - triplet term in Long-Term Potentiation</td>
<td align="left" rowspan="1" colspan="1">9.1<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e238" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e239" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Amplitude of weights change - pair term in Long-Term Depression</td>
<td align="left" rowspan="1" colspan="1">3.0<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e240" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e241" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Amplitude of weights change - triplet term in Long-Term Depression</td>
<td align="left" rowspan="1" colspan="1">7.5<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e242" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e243" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Decay constant of presynaptic indicator <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e244" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">16.8 ms</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e245" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Decay constant of presynaptic indicator <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e246" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">575 ms</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e247" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Decay constant of postsynaptic indicator <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e248" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">33.7 ms</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e249" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Decay constant of postsynaptic indicator <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e250" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">47 ms</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e251" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Learning rate for STDP</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e252" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e253" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Discretisation time step</td>
<td align="left" rowspan="1" colspan="1">1 ms</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e254" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">Number of independent repetitions of the experiment</td>
<td align="left" rowspan="1" colspan="1">50</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt101"><label/><p>STDP parameters are as in the nearest-spike triplet-model, described in <xref ref-type="bibr" rid="pone.0100805-Pfister1">[32]</xref>.</p></fn></table-wrap-foot></table-wrap>
<p>Then, assuming that neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e255" xlink:type="simple"/></inline-formula> fires a spike, the STDP implementation of the triplet rule can be written as follows:<disp-formula id="pone.0100805.e256"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e256" xlink:type="simple"/><label>(36)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e257" xlink:type="simple"/></inline-formula> is the learning rate and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e258" xlink:type="simple"/></inline-formula> is an infinitesimal time constant to ensure that the values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e259" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e260" xlink:type="simple"/></inline-formula> used are the ones right before the update due to the spike of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e261" xlink:type="simple"/></inline-formula> The learning rate used is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e262" xlink:type="simple"/></inline-formula> for the frequency protocol, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e263" xlink:type="simple"/></inline-formula> for the sequential protocol (see <xref ref-type="sec" rid="s3">Results</xref>).</p>
</sec><sec id="s2e4">
<title>Reproducibility of results</title>
<p>All simulations were performed in MATLAB (The Mathworks, Natick, USA). Code is available from ModelDB <xref ref-type="bibr" rid="pone.0100805-Hines1">[34]</xref>, accession number: 151692.</p>
</sec></sec></sec><sec id="s3">
<title>Results</title>
<p>We recall the definition of the symmetry measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e264" xlink:type="simple"/></inline-formula> (<xref ref-type="disp-formula" rid="pone.0100805.e010">Eq. 1</xref>):<disp-formula id="pone.0100805.e265"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e265" xlink:type="simple"/><label>(37)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e266" xlink:type="simple"/></inline-formula> is the positive synaptic connection from neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e267" xlink:type="simple"/></inline-formula> to neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e268" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e269" xlink:type="simple"/></inline-formula> is the total number of neurons and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e270" xlink:type="simple"/></inline-formula> is the number of instances where both <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e271" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e272" xlink:type="simple"/></inline-formula> are zero, i.e. there is no connection between two neurons. The term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e273" xlink:type="simple"/></inline-formula> is a normalisation factor that represents the total number of synaptic connection pairs that have at least one non-zero connection.</p>
<p>By using this definition, we were able to estimate the expected value and the variance of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e274" xlink:type="simple"/></inline-formula> on random matrices (uniform and truncated Gaussian), see <xref ref-type="disp-formula" rid="pone.0100805.e115">Eq. (16)</xref>–<xref ref-type="disp-formula" rid="pone.0100805.e116">(17)</xref> and <xref ref-type="disp-formula" rid="pone.0100805.e185">(29)</xref>–<xref ref-type="disp-formula" rid="pone.0100805.e186">(30)</xref> correspondingly. This provides us a tool to estimate the significance of the “symmetry” or “asymmetry” of the adjacency matrix of a given network, shaped by learning, given the initial distribution of the synaptic connections prior to the learning process. The statistical analysis is particularly useful in cases where the developed configuration is not “clear-cut”, i.e. all connections have been turned to either bidirectional or unidirectional resulting in a symmetry measure almost 1 or 0, which is probably an artificial scenario, but rather in the intermediate cases, where we need a measure of how far away the value of the symmetry measure of a specific configuration is from that of a random configuration. Though here we focused on two specific random distributions, our methodology is applicable to other distribution choices.</p>
<sec id="s3a">
<title>Hypothesis test</title>
<p>Having calculated the mean and variance of the symmetry measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e275" xlink:type="simple"/></inline-formula> over random networks of a specific connectivity distribution, we are now able to directly evaluate the symmetry measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e276" xlink:type="simple"/></inline-formula> of a specific connectivity structure and conclude whether the symmetric or asymmetric structure observed is due to chance or it is indeed significant. A simple test is, for instance, to calculate how many standard deviations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e277" xlink:type="simple"/></inline-formula> is away from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e278" xlink:type="simple"/></inline-formula> Equivalently, we can form the hypothesis that the configuration <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e279" xlink:type="simple"/></inline-formula> is non-random and calculate the <italic>p</italic>-value by:<disp-formula id="pone.0100805.e280"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0100805.e280" xlink:type="simple"/><label>(38)</label></disp-formula>where we implicitly assume that the distribution of the symmetry measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e281" xlink:type="simple"/></inline-formula> over all random networks is Gaussian. We can compare this result with the significance level we fixed, typically <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e282" xlink:type="simple"/></inline-formula> and we can then conclude the nature of the symmetry of the network with a confidence level equal to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e283" xlink:type="simple"/></inline-formula> or reject the hypothesis.</p>
</sec><sec id="s3b">
<title>Pruning biases the network towards asymmetry</title>
<p>To demonstrate the validity of our analytical results, we compare them to simulation results. We generated a sample of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e284" xlink:type="simple"/></inline-formula> networks with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e285" xlink:type="simple"/></inline-formula> neurons with random connections with synaptic efficacies varying from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e286" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e287" xlink:type="simple"/></inline-formula> We evaluated the symmetry measure on each network by applying directly the definition of <xref ref-type="disp-formula" rid="pone.0100805.e265">Eq. (37)</xref>, and then we computed the mean value and variance of that sample. This process was repeated ten times, each one for a different value of the pruning parameter, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e288" xlink:type="simple"/></inline-formula> The final results are shown in <xref ref-type="fig" rid="pone-0100805-g003">Fig. 3A,B</xref>, together with the analytical results, see <xref ref-type="disp-formula" rid="pone.0100805.e115">Eq. (16)</xref> and <xref ref-type="disp-formula" rid="pone.0100805.e185">Eq. (29)</xref>. Since numerical and analytical results overlap, we used a thicker (black) line for the latter. The agreement between theoretical findings, listed in <xref ref-type="table" rid="pone-0100805-t002">Table 2</xref>, and numerical evaluations is excellent.</p>
<fig id="pone-0100805-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0100805.g003</object-id><label>Figure 3</label><caption>
<title>Final statistics of the symmetry measure.</title>
<p><bold>A</bold> Expected value and standard deviation of the symmetry measure as a function of the pruning for different types of networks with uniform weights distribution. The total length of each bar is two times the standard deviation. <italic>Dashed light grey line</italic>: simulations for symmetric networks, <italic>Dash dotted light grey line</italic>: simulations for asymmetric networks, <italic>Solid dark grey line</italic>: simulations for random networks, <italic>Dashed black line</italic>: theoretical results for random networks. <bold>B</bold> The same as A but with Gaussian-distributed random weights. <bold>C</bold> Example of an adjacency matrix in a particular random network with uniform weights distribution and pruning parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e289" xlink:type="simple"/></inline-formula> For this example <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e290" xlink:type="simple"/></inline-formula> <bold>D</bold> The same as C but with Gaussian-distributed random weights. For this example <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e291" xlink:type="simple"/></inline-formula></p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0100805.g003" position="float" xlink:type="simple"/></fig><table-wrap id="pone-0100805-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0100805.t002</object-id><label>Table 2</label><caption>
<title>Mean value and standard deviation of the symmetry measure as obtained from the theoretical analysis.</title>
</caption><alternatives><graphic id="pone-0100805-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0100805.t002" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e292" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e293" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e294" xlink:type="simple"/></inline-formula></td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e295" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e296" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e297" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e298" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e299" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e300" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e301" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e302" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e303" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e304" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e305" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e306" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e307" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e308" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e309" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e310" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e311" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e312" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e313" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e314" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e315" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e316" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e317" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e318" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e319" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e320" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e321" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e322" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e323" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e324" xlink:type="simple"/></inline-formula></td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt102"><label/><p><italic>Column 1.</italic> Value of the pruning parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e325" xlink:type="simple"/></inline-formula> <italic>Column 2.</italic> Uniform distribution. <italic>Column 3.</italic> Gaussian distribution. These values are obtained with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e326" xlink:type="simple"/></inline-formula> random networks of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e327" xlink:type="simple"/></inline-formula> neurons and are plotted in <xref ref-type="fig" rid="pone-0100805-g003">Fig. 3</xref>.</p></fn></table-wrap-foot></table-wrap>
<p>We also considered two extreme cases, symmetric and asymmetric random networks, which respectively represent the upper and lower bound for the symmetry measure defined in <xref ref-type="disp-formula" rid="pone.0100805.e265">Eq. (37)</xref>. Symmetric random networks have been generated as follows: we filled the upper triangular part of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e328" xlink:type="simple"/></inline-formula> weights matrix with random values from the uniform/Gaussian distribution. We then mirrored the elements around the diagonal so as to have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e329" xlink:type="simple"/></inline-formula> In the asymmetric case, instead, we generated a random adjacency matrix with values in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e330" xlink:type="simple"/></inline-formula> for the upper triangular part and in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e331" xlink:type="simple"/></inline-formula> for the lower triangular part, so as to have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e332" xlink:type="simple"/></inline-formula> Then, we shuffled the adjacency matrix.</p>
<p>In <xref ref-type="fig" rid="pone-0100805-g003">Fig. 3A,B</xref> we contrast our results on random networks with numerical simulations of symmetric and asymmetric random networks: the dashed, light grey line (top line) shows the upper extreme case of a symmetric random network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e333" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e334" xlink:type="simple"/></inline-formula> whereas the dash-dotted, light grey line (bottom line) shows the lower extreme case of a asymmetric random network <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e335" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e336" xlink:type="simple"/></inline-formula></p>
<p>When we introduce pruning, the lower bound of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e337" xlink:type="simple"/></inline-formula> remains unchanged, whereas the more we prune the more a symmetric network appears as asymmetric.</p>
</sec><sec id="s3c">
<title>Gaussian-distributed synaptic efficacies bias the network towards symmetry</title>
<p>In <xref ref-type="fig" rid="pone-0100805-g003">Fig. 3C,D</xref>, we show the adjacency matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e338" xlink:type="simple"/></inline-formula> for a random pruned network with pruning parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e339" xlink:type="simple"/></inline-formula> A network with uniformly distributed initial connectivity is shown in <xref ref-type="fig" rid="pone-0100805-g003">Fig. 3C</xref> and a network with Gaussian-distributed initial connectivity is shown in <xref ref-type="fig" rid="pone-0100805-g003">Fig. 3D</xref>. Black areas represent zero connection, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e340" xlink:type="simple"/></inline-formula> The “Gaussian” network has most of the connections close to the mean value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e341" xlink:type="simple"/></inline-formula> resulting in higher values for the symmetry measure than in the case of a uniform distribution, compare <xref ref-type="fig" rid="pone-0100805-g003">Fig. 3B</xref> with <xref ref-type="fig" rid="pone-0100805-g003">Fig. 3A</xref>.</p>
<p>This difference in the mean values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e342" xlink:type="simple"/></inline-formula> depending on the shape of the distribution implies that for example a weight configuration that would be classified as non-random under the hypothesis that the initial connectivity, before learning, is uniform, is classified as random under the hypothesis that the initial distribution of the connections is Gaussian. To more emphasise this point, we show in <xref ref-type="fig" rid="pone-0100805-g004">Fig. 4</xref> the adjacency matrix of two different networks of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e343" xlink:type="simple"/></inline-formula> neurons. The first network, <xref ref-type="fig" rid="pone-0100805-g004">Fig. 4A</xref>, is a non-pruned network with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e344" xlink:type="simple"/></inline-formula> According with the values obtained from the statistical analysis (<xref ref-type="table" rid="pone-0100805-t002">Table 2</xref>), if we assume that the connections of this network are randomly drawn from a uniform distribution, the <italic>p</italic>-value test (<xref ref-type="disp-formula" rid="pone.0100805.e280">Eq. (38)</xref>) gives us <italic>p</italic>-value<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e345" xlink:type="simple"/></inline-formula> With the usual confidence level of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e346" xlink:type="simple"/></inline-formula> this is a significant result, implying that the network configuration is unlikely to be random. On the other hand, if we assume that the initial connectivity is drawn from a Gaussian distribution, we obtain <italic>p</italic>-value<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e347" xlink:type="simple"/></inline-formula> meaning that the network configuration should be considered random.</p>
<fig id="pone-0100805-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0100805.g004</object-id><label>Figure 4</label><caption>
<title>Symmetry and asymmetry depends on the distribution of the initial connectivity.</title>
<p><bold>A</bold> Example of an adjacency matrix in a random network with pruning parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e348" xlink:type="simple"/></inline-formula> and symmetry measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e349" xlink:type="simple"/></inline-formula> According with the <italic>p</italic>-value test with the null hypothesis of random connectivity and with a level of confidence of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e350" xlink:type="simple"/></inline-formula> the symmetry of this network is significant if the distribution of the initial connections is uniform but is non-significant if the initial distribution of the connections is Gaussian. Therefore, in the first case it should be regarded as a non-random network whereas in the second case as a random network. <bold>B</bold> The same as A but with pruning parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e351" xlink:type="simple"/></inline-formula> and symmetry measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e352" xlink:type="simple"/></inline-formula> In this case, with the same hypothesis test, the situation is reversed: the network should be considered random for initial uniform distribution of connections, but non-random for initial Gaussian-distributed connections (see the discussion in the text).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0100805.g004" position="float" xlink:type="simple"/></fig>
<p>In <xref ref-type="fig" rid="pone-0100805-g004">Fig. 4B</xref>, we show a pruned network with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e353" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e354" xlink:type="simple"/></inline-formula> In this case the opposite is true: under the hypothesis of uniform random initial connectivity, the network should be considered random, as <italic>p</italic>-value<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e355" xlink:type="simple"/></inline-formula> Under the hypothesis of Gaussian-distributed random initial connectivity, the network should be considered asymmetric, as <italic>p</italic>-value<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e356" xlink:type="simple"/></inline-formula></p>
</sec><sec id="s3d">
<title>Relation between symmetry measure and motifs</title>
<p>In what follows, we demonstrate the relation between our symmetry measure and unidirectional and bidirectional motifs. From the definition <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e357" xlink:type="simple"/></inline-formula> <xref ref-type="disp-formula" rid="pone.0100805.e265">Eq. 37</xref>, we can deduct that in the extreme case of a network with unidirectional motifs, i.e. pairs of the form (0, x), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e358" xlink:type="simple"/></inline-formula> the symmetry measure will result in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e359" xlink:type="simple"/></inline-formula> while in the case of bidirectional motifs i.e. pairs of the form (x, x), the symmetry measure will result in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e360" xlink:type="simple"/></inline-formula> By inverting <xref ref-type="disp-formula" rid="pone.0100805.e041">Eq. 3</xref>, we can derive the mean value for connection pairs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e361" xlink:type="simple"/></inline-formula> We can use now this value to define connection pairs in a network as unidirectional or bidirectional: if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e362" xlink:type="simple"/></inline-formula> than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e363" xlink:type="simple"/></inline-formula> is a unidirectional motif, otherwise it is a bidirectional motif. In this way we relate unidirectional and bidirectional motifs to what is traditionally called single edge motif and second-order reciprocal motif, respectively. It is then expected that when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e364" xlink:type="simple"/></inline-formula> increases, the fraction of bidirectional motifs increases towards <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e365" xlink:type="simple"/></inline-formula> whereas the percentage of unidirectional motifs decreases towards <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e366" xlink:type="simple"/></inline-formula></p>
<p>We show this relation in simulations by generating <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e367" xlink:type="simple"/></inline-formula> networks of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e368" xlink:type="simple"/></inline-formula> neurons each, with uniformly distributed random connections in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e369" xlink:type="simple"/></inline-formula> and no pruning. In this case the mean value of the symmetry measure is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e370" xlink:type="simple"/></inline-formula> Using <xref ref-type="disp-formula" rid="pone.0100805.e041">Eq. 3</xref>, we have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e371" xlink:type="simple"/></inline-formula> which is the value used to decide whether a connection pair is unidirectional or bidirectional. For each of these networks, we calculated the value of the symmetry measure and the fraction of unidirectional and bidirectional motifs and we plotted the results in <xref ref-type="fig" rid="pone-0100805-g005">Fig. 5A</xref> as a scatter plot (black circles - bidirectional motifs, grey circles - unidirectional motifs). Also, un <xref ref-type="fig" rid="pone-0100805-g005">Fig. 5B</xref> we show the analogous results obtained when we prune the connections with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e372" xlink:type="simple"/></inline-formula> In both cases, a linear relation between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e373" xlink:type="simple"/></inline-formula> and motifs is evident.</p>
<fig id="pone-0100805-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0100805.g005</object-id><label>Figure 5</label><caption>
<title>Symmetry measure reflects motifs formation.</title>
<p><bold>A</bold> Scatter plot of fraction of unidirectional and bidirectional motifs as a function of the symmetry measure for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e374" xlink:type="simple"/></inline-formula> networks with uniform random connections and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e375" xlink:type="simple"/></inline-formula> <italic>Black dots</italic>: bidirectional motifs, <italic>Grey dots</italic>: unidirectional motifs. For this typology <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e376" xlink:type="simple"/></inline-formula> <bold>B</bold> The same as A but with pruning parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e377" xlink:type="simple"/></inline-formula> In this case <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e378" xlink:type="simple"/></inline-formula> <bold>C</bold> Mean value and standard deviation (each bar is twice the standard deviation) of fraction of unidirectional and bidirectional motifs as a function of the symmetry measure for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e379" xlink:type="simple"/></inline-formula> networks with half of the connections uniformly distributed and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e380" xlink:type="simple"/></inline-formula> The second half of the connections were derived from the values of connection pairs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e381" xlink:type="simple"/></inline-formula> drawn from a Gaussian distribution with mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e382" xlink:type="simple"/></inline-formula> and standard deviation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e383" xlink:type="simple"/></inline-formula> <italic>Black line</italic>: bidirectional motifs, <italic>Grey line</italic>: unidirectional motifs. <bold>D</bold> The same as C but with pruning parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e384" xlink:type="simple"/></inline-formula></p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0100805.g005" position="float" xlink:type="simple"/></fig>
<p>Note that in both figures the restricted domain on the <italic>s</italic>-axis: this is determined by the range of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e385" xlink:type="simple"/></inline-formula> values that correspond to random networks. If we want to extend this range, we need to consider networks that are not random any more. We achieve this by fixing a distribution for connection pairs <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e386" xlink:type="simple"/></inline-formula> Once we decide on the desirable value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e387" xlink:type="simple"/></inline-formula> in our case the whole zero to one spectrum, we can use a distribution (e.g. Gaussian) with mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e388" xlink:type="simple"/></inline-formula> and a chosen variance to draw the values of all the connection pairs in the network. Following this procedure, we fill the upper triangular part of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e389" xlink:type="simple"/></inline-formula> weights matrix with random values from the uniform/Gaussian distribution, and derive the other half of the weights by inverting the definition of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e390" xlink:type="simple"/></inline-formula> As a PDF(<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e391" xlink:type="simple"/></inline-formula>) we chose a Gaussian distribution around <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e392" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e393" xlink:type="simple"/></inline-formula> except for the extreme cases (near <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e394" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e395" xlink:type="simple"/></inline-formula>) where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e396" xlink:type="simple"/></inline-formula> With this technique of creating networks, we sampled the entire domain of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e397" xlink:type="simple"/></inline-formula> in steps of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e398" xlink:type="simple"/></inline-formula> For each value, we again generated <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e399" xlink:type="simple"/></inline-formula> networks of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e400" xlink:type="simple"/></inline-formula> neurons with (half of the) weights uniformly distributed, and then we computed the mean value and standard deviation. Results are shown in <xref ref-type="fig" rid="pone-0100805-g005">Fig. 5C,D</xref> respectively for unpruned and pruned (with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e401" xlink:type="simple"/></inline-formula>) networks (black line - bidirectional motifs, grey line - unidirectional motifs). We can see that <xref ref-type="fig" rid="pone-0100805-g005">Fig. 5C,D</xref> correctly reproduce the linear regime observed in <xref ref-type="fig" rid="pone-0100805-g005">Fig. 5A,B</xref> for values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e402" xlink:type="simple"/></inline-formula> close enough to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e403" xlink:type="simple"/></inline-formula></p>
<p>Due to the method by which we generated networks, the shape of the distribution of half of the weights does not affect the shape of the dependence in <xref ref-type="fig" rid="pone-0100805-g005">Fig. 5C,D</xref>. Indeed, if we choose half of the connections to be Gaussian-distributed, we will observe only a shift in both curves as they have to cross at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e404" xlink:type="simple"/></inline-formula> (results not shown).</p>
</sec><sec id="s3e">
<title>Symmetry measure and eigenvalues</title>
<p>In the definition of our symmetry measure we have deliberately excluded <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e405" xlink:type="simple"/></inline-formula> connection pairs. This was a conscious decision for mathematical and practical reasons, see <xref ref-type="sec" rid="s2">Methods</xref>. As a consequence, pairs of the form <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e406" xlink:type="simple"/></inline-formula> do not contribute to the evaluation of the symmetry of the network. Instead, pairs of the form <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e407" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e408" xlink:type="simple"/></inline-formula> very small, contribute to the asymmetry of the network according to our specific choice of symmetry measure (leading to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e409" xlink:type="simple"/></inline-formula> see <xref ref-type="sec" rid="s2">Methods</xref>). Here we further motivate this choice via a comparison of our measure to the evaluation of the symmetry via the matrix eigenvalues, for three types of networks: (i) symmetric, where each connection pair consists of synapses of the same value, (ii) asymmetric, where every connection pair has one connection set to a small value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e410" xlink:type="simple"/></inline-formula> and (iii) random, where connections are uniformly distributed. We demonstrate that our measure has a clear advantage over the eigenvalues method, in particular when pruning is introduced. This difference in performance lays in the different ways that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e411" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e412" xlink:type="simple"/></inline-formula> are treated by our measure.</p>
<p>A crucial property of the real symmetric matrices is that all their eigenvalues are real. <xref ref-type="fig" rid="pone-0100805-g006">Fig. 6A</xref> depicts the fraction of complex eigenvalues vs the pruning parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e413" xlink:type="simple"/></inline-formula> for a symmetric (dash-dotted, light grey line) asymmetric (dotted, dark grey line) and random (dashed, black line) matrix with uniformly distributed values, similar to <xref ref-type="fig" rid="pone-0100805-g003">Fig. 3A</xref>, with the same statistics (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e414" xlink:type="simple"/></inline-formula> networks of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e415" xlink:type="simple"/></inline-formula> neurons). As expected, if no pruning takes place (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e416" xlink:type="simple"/></inline-formula>), symmetric matrices have no complex eigenvalues and are clearly distinguishable from random and asymmetric matrices. On the contrary, both random and asymmetric matrices have a non-zero number of complex eigenvalues, which increases with a higher degree of asymmetry, leading to a considerable overlap between these two cases, differently from what happens with our measure in <xref ref-type="fig" rid="pone-0100805-g003">Fig. 3A</xref>.</p>
<fig id="pone-0100805-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0100805.g006</object-id><label>Figure 6</label><caption>
<title>Eigenvalues and network structure.</title>
<p><bold>A</bold> Expected value and standard deviation of the fraction of complex eigenvalues as a function of the pruning for different types of networks of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e417" xlink:type="simple"/></inline-formula> neurons with uniform weight distribution. The total length of each bar is two times the standard deviation. <italic>Dotted, dark grey line</italic>: simulations for asymmetric networks, <italic>Dashed, black line</italic>: simulations for random networks, <italic>Dash-dotted, light grey line</italic>: simulations for symmetric networks. <bold>B</bold> Fraction of complex eigenvalues as a function of network size for random networks with uniform weights distribution. Pruning parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e418" xlink:type="simple"/></inline-formula></p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0100805.g006" position="float" xlink:type="simple"/></fig>
<p>As we introduce pruning, the mean of the complex eigenvalues of the three distinctive types of network moves towards the same value, an increase for the symmetric network and decrease for the random and non-symmetric networks. This is expected as pruning specific elements will make the symmetric network more asymmetric while it will increase the symmetry of the asymmetric network by introducing pairs of the form <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e419" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e420" xlink:type="simple"/></inline-formula> The <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e421" xlink:type="simple"/></inline-formula> pairs are due to the construction of the asymmetric network, where half of the connections are stochastically set to very low values. This continues till <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e422" xlink:type="simple"/></inline-formula> after which further pruning reduces the number of complex eigenvalues of all networks: a high level of pruning implies the formation of more <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e423" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e424" xlink:type="simple"/></inline-formula> pairs for the asymmetric network and more <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e425" xlink:type="simple"/></inline-formula> pairs for the symmetric network. In <xref ref-type="fig" rid="pone-0100805-g006">Fig. 6B</xref> we show the dependence of the fraction of complex eigenvalues for uniform random matrices on their size.</p>
<p>Comparing <xref ref-type="fig" rid="pone-0100805-g006">Fig. 6A to F</xref>ig. 3A, we observe that our symmetry measure offers excellent discrimination between the symmetric, asymmetric and random matrices for e.g. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e426" xlink:type="simple"/></inline-formula> This is despite the fact that the structure of the asymmetric matrix <italic>per se</italic> has become less asymmetric and the structure of the symmetric matrix has become more asymmetric due to the pruning, as it is confirmed by the overlapping fraction of complex eigenvalues for asymmetric and random matrices (<xref ref-type="fig" rid="pone-0100805-g006">Fig. 6A</xref>). In our measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e427" xlink:type="simple"/></inline-formula> pairs are treated as asymmetric, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e428" xlink:type="simple"/></inline-formula> pairs are ignored, and the bias that pruning introduces is taken into account allowing for good discrimination for all types of matrices, even beyond <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e429" xlink:type="simple"/></inline-formula></p>
</sec><sec id="s3f">
<title>Case study: Monitoring the connectivity evolution in neural networks</title>
<p>We demonstrate the application of the symmetry measure to a network of neurons evolving in time according to a Spike-Timing Dependent Plasticity (STDP) “triplet rule” <xref ref-type="bibr" rid="pone.0100805-Pfister1">[32]</xref> by adopting the protocols of <xref ref-type="bibr" rid="pone.0100805-Clopath1">[18]</xref>. These protocols are designed to evolve a network with connections modified according to the “triplet rule”, to either a unidirectional configuration or bidirectional configuration, with the weights being stable under the presence of hard bounds. We have deliberately chosen a small size network as a “toy-model” that will allow for visual inspection and characterisation at the mesoscopic scale.</p>
<p>We simulated <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e430" xlink:type="simple"/></inline-formula> integrate-and-fire neurons (see <xref ref-type="sec" rid="s2">Methods</xref> section for simulation details) initially connected with random weights <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e431" xlink:type="simple"/></inline-formula> drawn from either a uniform (<xref ref-type="fig" rid="pone-0100805-g007">Fig. 7</xref>) or a Gaussian (<xref ref-type="fig" rid="pone-0100805-g008">Fig. 8</xref>) distribution (see <xref ref-type="table" rid="pone-0100805-t001">Table 1</xref> for parameters). Where a pruning parameter is mentioned, the pruning took place prior to the learning procedure: with a fixed probability some connections were set to zero and were not allowed to grow during the simulation.</p>
<fig id="pone-0100805-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0100805.g007</object-id><label>Figure 7</label><caption>
<title>Evolution of networks with STDP and initially uniform weights distribution.</title>
<p><bold>A</bold> Time evolution of the symmetry measure when a frequency protocol is applied on a network, shown as average over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e432" xlink:type="simple"/></inline-formula> representations. The shaded light grey areas represent the standard deviation (the total length of height of each band is twice the standard deviation). <italic>Solid black line</italic>: no pruning, <italic>Dashed grey line</italic>: with pruning <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e433" xlink:type="simple"/></inline-formula> <bold>B</bold> Example of an adjacency matrix at the end of the learning process for a network with the frequency protocol and no pruning. For this example <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e434" xlink:type="simple"/></inline-formula> <bold>C</bold> The same as B but with pruning <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e435" xlink:type="simple"/></inline-formula> For this example <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e436" xlink:type="simple"/></inline-formula> <bold>D, E, F</bold> The same as A, B and C but with the sequential protocol applied. The connectivity matrix in panel E has <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e437" xlink:type="simple"/></inline-formula> The connectivity matrix in panel F has <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e438" xlink:type="simple"/></inline-formula></p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0100805.g007" position="float" xlink:type="simple"/></fig><fig id="pone-0100805-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0100805.g008</object-id><label>Figure 8</label><caption>
<title>Evolution of networks with STDP and initially Gaussian-distributed weights.</title>
<p><bold>A</bold> Time evolution of the symmetry measure when a frequency protocol is applied on a network, shown as average over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e439" xlink:type="simple"/></inline-formula> representations. The shaded light grey areas represent the standard deviation (the total length of height of each band is twice the standard deviation). <italic>Solid black line</italic>: no pruning, <italic>Dashed grey line</italic>: with pruning <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e440" xlink:type="simple"/></inline-formula> <bold>B</bold> Example of an adjacency matrix at the end of the evolution for a network with frequency protocol and no pruning. For this example <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e441" xlink:type="simple"/></inline-formula> <bold>C</bold> The same as B but with pruning <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e442" xlink:type="simple"/></inline-formula> For this example <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e443" xlink:type="simple"/></inline-formula> <bold>D, E, F</bold> The same as A, B and C but with the sequential protocol applied. The connectivity matrix in panel E has <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e444" xlink:type="simple"/></inline-formula> The connectivity matrix in panel F has <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e445" xlink:type="simple"/></inline-formula></p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0100805.g008" position="float" xlink:type="simple"/></fig>
<p>Our choice allows us to produce an asymmetric or a symmetric network depending on the external stimulation protocol applied to the network. Since the amplitude of the external stimulation we chose (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e446" xlink:type="simple"/></inline-formula>) is large enough to make a neuron fire every time it is presented with an input, the firing pattern of neurons reflects the input pattern and we can indifferently refer to one or another. The asymmetric network has been obtained by using a “sequential protocol”, in which neurons fire with the same frequency in a precise order one after the other, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e447" xlink:type="simple"/></inline-formula> delay, see also <xref ref-type="bibr" rid="pone.0100805-Clopath1">[18]</xref>. The symmetric network is produced by applying a “frequency protocol”, in which each neuron fires with a different frequency from the values <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e448" xlink:type="simple"/></inline-formula> In both cases, the input signals were jittered in time randomly with zero mean and standard deviation equal to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e449" xlink:type="simple"/></inline-formula> of the period of the input itself for the frequency protocol, to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e450" xlink:type="simple"/></inline-formula> of the delay for the sequential protocol. Depending on the protocol, we expect the neurons to form mostly unidirectional or bidirectional connections during the evolution.</p>
<p>The time evolution for both protocols and initial distributions is shown in <xref ref-type="fig" rid="pone-0100805-g007">Figs 7A,D</xref> (uniform) and 8A,D (Gaussian). Each panel represents the evolution of the symmetry measure averaged over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e451" xlink:type="simple"/></inline-formula> different representations for both fully connected networks (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e452" xlink:type="simple"/></inline-formula> solid black line) and pruned networks (e.g. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e453" xlink:type="simple"/></inline-formula> dashed grey line). The shaded area represents the standard deviation. The time course of the symmetry measure can be better understood with the help of the <xref ref-type="fig" rid="pone-0100805-g003">Fig. 3</xref>. At the beginning, the values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e454" xlink:type="simple"/></inline-formula> reflect what we expect from a random network. Afterwards, as the time passes, the learning process leads to the evolution of the connectivity. As expected, the frequency protocol induces the formation of mostly bidirectional connections, leading to the saturation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e455" xlink:type="simple"/></inline-formula> towards its maximum value, depending on the degree of pruning. On the other hand, when we apply the sequential protocol, connection pairs develop a high degree of asymmetry, the values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e456" xlink:type="simple"/></inline-formula> decreasing towards its minimum. Connections were constrained to remain inside the interval <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e457" xlink:type="simple"/></inline-formula></p>
<p>The final connectivity pattern can be inspected by plotting the adjacency matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e458" xlink:type="simple"/></inline-formula> In <xref ref-type="fig" rid="pone-0100805-g007">Fig. 7B,C</xref> and <xref ref-type="fig" rid="pone-0100805-g008">8B,C</xref> we give an example of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e459" xlink:type="simple"/></inline-formula> at the end of the evolution for one particular instance of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e460" xlink:type="simple"/></inline-formula> networks when the frequency protocol is applied. Similarly, in <xref ref-type="fig" rid="pone-0100805-g007">Fig. 7E,F</xref> and <xref ref-type="fig" rid="pone-0100805-g008">8E,F</xref> we show the results for the sequential protocol. The corresponding values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e461" xlink:type="simple"/></inline-formula> for each of the examples in the figures are listed in <xref ref-type="table" rid="pone-0100805-t003">Table 3</xref>. In the case that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e462" xlink:type="simple"/></inline-formula> a careful inspection of <xref ref-type="fig" rid="pone-0100805-g007">Fig. 7B</xref>, <xref ref-type="fig" rid="pone-0100805-g008">8B</xref> indicates that connectivity is bidirectional: all-to-all strong connections have been formed. On the other hand, In <xref ref-type="fig" rid="pone-0100805-g007">Fig. 7E</xref>, <xref ref-type="fig" rid="pone-0100805-g008">8E</xref>, trying to determine if there is a particular connectivity emerging in the network starts to be considerably tough. However, by using our symmetry measure (see values in <xref ref-type="table" rid="pone-0100805-t003">Table 3</xref>) we can infer that the connectivity is unidirectional. In the pruned networks, however, see <xref ref-type="fig" rid="pone-0100805-g007">Fig. 7C</xref>, <xref ref-type="fig" rid="pone-0100805-g008">8C</xref> and <xref ref-type="fig" rid="pone-0100805-g007">Fig. 7F</xref>, <xref ref-type="fig" rid="pone-0100805-g008">8F</xref>, the formation of bidirectional and unidirectional connection pairs is not as obvious as for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e463" xlink:type="simple"/></inline-formula> We therefore refer again to the <xref ref-type="table" rid="pone-0100805-t003">Table 3</xref> and compare the values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e464" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e465" xlink:type="simple"/></inline-formula> and with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e466" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e467" xlink:type="simple"/></inline-formula> depending on the case. We can then verify that the learning process has significantly changed the network and its inner connections from the initial random state.</p>
<table-wrap id="pone-0100805-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0100805.t003</object-id><label>Table 3</label><caption>
<title>Symmetry measure and <italic>p</italic>-value for different types of network.</title>
</caption><alternatives><graphic id="pone-0100805-t003-3" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0100805.t003" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Type</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e468" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e469" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><italic>p</italic>-value</td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e470" xlink:type="simple"/></inline-formula></td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">UF<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e471" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e472" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e473" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e474" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e475" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">UF<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e476" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e477" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e478" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e479" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e480" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">US<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e481" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e482" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e483" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e484" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e485" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">US<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e486" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e487" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e488" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e489" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e490" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">GF<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e491" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e492" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e493" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e494" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e495" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">GF<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e496" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e497" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e498" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e499" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e500" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">GS<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e501" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e502" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e503" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e504" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e505" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">GS<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e506" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e507" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e508" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e509" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e510" xlink:type="simple"/></inline-formula></td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt103"><label/><p><italic>Column 1.</italic> Network type. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e511" xlink:type="simple"/></inline-formula> = Uniform distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e512" xlink:type="simple"/></inline-formula> = Gaussian distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e513" xlink:type="simple"/></inline-formula> = Frequency protocol, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e514" xlink:type="simple"/></inline-formula> = sequential protocol, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e515" xlink:type="simple"/></inline-formula> = No prune, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e516" xlink:type="simple"/></inline-formula> = pruning of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e517" xlink:type="simple"/></inline-formula> <italic>Column 2.</italic> Value of the symmetry measure for one instance of each type. <italic>Column 3.</italic> Results from the previous statistical analysis on random networks. <italic>Column 4.</italic> Corresponding <italic>p</italic>-value from <xref ref-type="disp-formula" rid="pone.0100805.e280">Eq. 38</xref>. <italic>Column 5.</italic> Results from the previous statistical analysis for the corresponding closest extreme case – symmetric network for frequency protocol and asymmetric network for sequential protocol. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e518" xlink:type="simple"/></inline-formula> means symmetric and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e519" xlink:type="simple"/></inline-formula> asymmetric.</p></fn></table-wrap-foot></table-wrap>
<p>We can rigorously verify the above conclusions via a statistical hypothesis test such as the <italic>p</italic>-value test, which in essence quantifies how far away the value of our symmetry measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e520" xlink:type="simple"/></inline-formula> of our final configuration is from the initial, random configuration (see also <xref ref-type="sec" rid="s2">Methods</xref>). In <xref ref-type="table" rid="pone-0100805-t003">Table 3</xref> we show the <italic>p</italic>-values corresponding to the null hypothesis of random connectivity for the examples in the <xref ref-type="fig" rid="pone-0100805-g007">Fig. 7</xref>, <xref ref-type="fig" rid="pone-0100805-g008">8</xref>. Once we set the significance level at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e521" xlink:type="simple"/></inline-formula> we can verify that, except for the case of pruned network with initially Gaussian-distributed connections where a frequency protocol has been applied (i.e. GF<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e522" xlink:type="simple"/></inline-formula> the <italic>p</italic>-values are significant, implying the rejection of the null hypothesis. This is also justified by <xref ref-type="fig" rid="pone-0100805-g003">Fig. 3A, B:</xref> when we increase the pruning, the mean value of the symmetry measure of the fully symmetric network approaches that of the pruned random network and in particular for the case where the weight are randomly Gaussian-distributed.</p>
</sec></sec><sec id="s4">
<title>Summary</title>
<p>The study of the human brain reveals that neurons sharing the same cognitive functions or coding tend to form clusters, which appear to be characterised by the formation of specific connectivity patterns, called motifs. We, therefore, introduced a mathematical tool, a symmetry measure <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0100805.e523" xlink:type="simple"/></inline-formula> which computes the mean value of the connection pairs in a network, and allows us to monitor the evolution of the network structure due to the synaptic dynamics. In this context, we applied it to a number of evolving networks with plastic connections that are modified according a learning rule. After the network connectivity reaches a steady state as a consequence of the learning process, connectivity patterns develop. The use of the symmetry measure together with the statistical analysis and the <italic>p</italic>-value test allow us both to quantify the connectivity structure of the network, which has changed due to the learning process, and observe its development. It also allows for some interesting observations. (i) Introducing a fixed amount of pruning in the network prior to the learning process biases the adjacency matrix towards an asymmetric configuration. (ii) A network configuration that appears to be symmetric under the assumption of a uniform initial distribution is random under the assumption of a Gaussian initial distribution.</p>
<p>Statements on non-random connectivity in motifs experimental work, e.g. <xref ref-type="bibr" rid="pone.0100805-Song1">[14]</xref>, <xref ref-type="bibr" rid="pone.0100805-Vasilaki1">[20]</xref> are supported by calculating the probability of connectivity in a random network and then distributing it uniformly: this becomes the null hypothesis. This was a most suitable approach given the paucity of data. If, however, the null hypothesis consisted of a Gaussian-distributed connectivity, then a higher number of bidirectional connections would be expected, as suggested by our analysis.</p>
<p>It is also possible that in a large network, learning processes are only modifying a subset of the connections, forming motifs that might be unobserved if the symmetry measure is applied to the whole adjacency matrix. In such cases, algorithms of detecting potential symmetric or asymmetric clusters would detect the area of interest and the symmetry measure presented here reveals the evolution of the structure and its significance.</p>
</sec></body>
<back>
<ack>
<p>We are grateful to the anonymous reviewers for their comments.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0100805-Sporns1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sporns</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Tononi</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Ktter</surname><given-names>R</given-names></name> (<year>2005</year>) <article-title>The human connectome: A structural description of the human brain</article-title>. <source>PLoS Comput Biol</source> <volume>1</volume>: <fpage>e42</fpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Lichtman1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lichtman</surname><given-names>JW</given-names></name>, <name name-style="western"><surname>Sanes</surname><given-names>JR</given-names></name> (<year>2008</year>) <article-title>Ome sweet ome: what can the genome tell us about the connectome?</article-title> <source>Curr Opin Neurobiol</source> <volume>18</volume>: <fpage>346</fpage>–<lpage>353</lpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Smith1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname><given-names>SJ</given-names></name> (<year>2007</year>) <article-title>Circuit reconstruction tools today</article-title>. <source>Curr Opin Neurobiol</source> <volume>17</volume>: <fpage>601</fpage>–<lpage>608</lpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Luo1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luo</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Callaway</surname><given-names>EM</given-names></name>, <name name-style="western"><surname>Svoboda</surname><given-names>K</given-names></name> (<year>2008</year>) <article-title>Genetic dissection of neural circuits</article-title>. <source>Neuron</source> <volume>57</volume>: <fpage>634</fpage>–<lpage>660</lpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Seung1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Seung</surname><given-names>HS</given-names></name> (<year>2009</year>) <article-title>Reading the book of memory: sparse sampling versus dense mapping of connectomes</article-title>. <source>Neuron</source> <volume>62</volume>: <fpage>17</fpage>–<lpage>29</lpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-White1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>White</surname><given-names>JG</given-names></name>, <name name-style="western"><surname>Southgate</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Thomson</surname><given-names>JN</given-names></name>, <name name-style="western"><surname>Brenner</surname><given-names>S</given-names></name> (<year>1986</year>) <article-title>The structure of the nervous system of the nematode caenorhabditis elegans</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source> <volume>314</volume>: <fpage>1</fpage>–<lpage>340</lpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Varshney1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Varshney</surname><given-names>LR</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>BL</given-names></name>, <name name-style="western"><surname>Paniagua</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Hall</surname><given-names>DH</given-names></name>, <name name-style="western"><surname>Chklovskii</surname><given-names>DB</given-names></name> (<year>2011</year>) <article-title>Structural properties of the caenorhabditis elegans neuronal network</article-title>. <source>PLoS Comput Biol</source> <volume>7</volume>: <fpage>e1001066</fpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Briggman1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Briggman</surname><given-names>KL</given-names></name>, <name name-style="western"><surname>Helmstaedter</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Denk</surname><given-names>W</given-names></name> (<year>2011</year>) <article-title>Wiring specificity in the direction-selectivity circuit of the retina</article-title>. <source>Nature</source> <volume>471</volume>: <fpage>183</fpage>–<lpage>188</lpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Bock1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bock</surname><given-names>DD</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>WCA</given-names></name>, <name name-style="western"><surname>Kerlin</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Andermann</surname><given-names>ML</given-names></name>, <name name-style="western"><surname>Hood</surname><given-names>G</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Network anatomy and in vivo physiology of visual cortical neurons</article-title>. <source>Nature</source> <volume>471</volume>: <fpage>177</fpage>–<lpage>182</lpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Koetter1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koetter</surname><given-names>R</given-names></name> (<year>2001</year>) <article-title>Neuroscience databases: tools for exploring brain structure-function relationships</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source> <volume>356</volume>: <fpage>1111</fpage>–<lpage>1120</lpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Koslow1"><label>11</label>
<mixed-citation publication-type="other" xlink:type="simple">Koslow SH, Subramanian S (2005) Databasing the brain: From data to knowledge (Neuroinformatics). Wiley.</mixed-citation>
</ref>
<ref id="pone.0100805-Insel1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Insel</surname><given-names>TR</given-names></name>, <name name-style="western"><surname>Volkow</surname><given-names>ND</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>TK</given-names></name>, <name name-style="western"><surname>Battey</surname><given-names>JF</given-names></name>, <name name-style="western"><surname>Landis</surname><given-names>SC</given-names></name> (<year>2003</year>) <article-title>Neuroscience networks: data-sharing in an information age</article-title>. <source>PLoS Biol</source> <volume>1</volume>: <fpage>E17</fpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Briggman2"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Briggman</surname><given-names>KL</given-names></name>, <name name-style="western"><surname>Denk</surname><given-names>W</given-names></name> (<year>2006</year>) <article-title>Towards neural circuit reconstruction with volume electron microscopy techniques</article-title>. <source>Curr Opin Neurobiol</source> <volume>16</volume>: <fpage>562</fpage>–<lpage>570</lpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Song1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Song</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Sjstrm</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Reigl</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Nelson</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Chklovskii</surname><given-names>DB</given-names></name> (<year>2005</year>) <article-title>Highly nonrandom features of synaptic connectivity in local cortical circuits</article-title>. <source>PLoS Biol</source> <volume>3</volume>: <fpage>e68</fpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Wang1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Markram</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Goodman</surname><given-names>PH</given-names></name>, <name name-style="western"><surname>Berger</surname><given-names>TK</given-names></name>, <name name-style="western"><surname>Ma</surname><given-names>J</given-names></name>, <etal>et al</etal>. (<year>2006</year>) <article-title>Heterogeneity in the pyramidal network of the medial prefrontal cortex</article-title>. <source>Nat Neurosci</source> <volume>9</volume>: <fpage>534</fpage>–<lpage>542</lpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Silberberg1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Silberberg</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Markram</surname><given-names>H</given-names></name> (<year>2007</year>) <article-title>Disynaptic inhibition between neocortical pyramidal cells mediated by martinotti cells</article-title>. <source>Neuron</source> <volume>53</volume>: <fpage>735</fpage>–<lpage>746</lpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Perin1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perin</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Berger</surname><given-names>TK</given-names></name>, <name name-style="western"><surname>Markram</surname><given-names>H</given-names></name> (<year>2011</year>) <article-title>A synaptic organizing principle for cortical neuronal groups</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>108</volume>: <fpage>5419</fpage>–<lpage>5424</lpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Clopath1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clopath</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Buesing</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Vasilaki</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2010</year>) <article-title>Connectivity reects coding: a model of voltage-based stdp with homeostasis</article-title>. <source>Nat Neurosci</source> <volume>13</volume>: <fpage>344</fpage>–<lpage>352</lpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Lefort1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lefort</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Tomm</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Sarria</surname><given-names>JCF</given-names></name>, <name name-style="western"><surname>Petersen</surname><given-names>CCH</given-names></name> (<year>2009</year>) <article-title>The excitatory neuronal network of the c2 barrel column in mouse primary somatosensory cortex</article-title>. <source>Neuron</source> <volume>61</volume>: <fpage>301</fpage>–<lpage>316</lpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Vasilaki1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vasilaki</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Giugliano</surname><given-names>M</given-names></name> (<year>2014</year>) <article-title>Emergence of connectivity motifs in networks of model neurons with short- and long-term plastic synapses</article-title>. <source>PLoS One</source> <volume>9</volume>: <fpage>e84626</fpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Vasilaki2"><label>21</label>
<mixed-citation publication-type="other" xlink:type="simple">Vasilaki E, Giugliano M (2012) Emergence of connectivity patterns from long-term and shortterm plasticities. In: ICANN 2012 - 22nd International Conference on Artificial Neural Networks, Lausanne, Switzerland.</mixed-citation>
</ref>
<ref id="pone.0100805-Pignatelli1"><label>22</label>
<mixed-citation publication-type="other" xlink:type="simple">Pignatelli (2009) Structure and Function of the Olfactory Bulb Microcircuit. Ph.D. thesis, Ecole Polytechnique Federale de Lausanne.</mixed-citation>
</ref>
<ref id="pone.0100805-Babadi1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Babadi</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name> (<year>2013</year>) <article-title>Pairwise analysis can account for network structures arising from spike-timing dependent plasticity</article-title>. <source>PLoS Comput Biol</source> <volume>9</volume>: <fpage>e1002906</fpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Bourjaily1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bourjaily</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Miller</surname><given-names>P</given-names></name> (<year>2011</year>) <article-title>Excitatory, inhibitory, and structural plasticity produce correlated connectivity in random networks trained to solve paired-stimulus tasks</article-title>. <source>Front Comput Neurosci</source> <volume>5</volume>: <fpage>37</fpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Bourjaily2"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bourjaily</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Miller</surname><given-names>P</given-names></name> (<year>2011</year>) <article-title>Synaptic plasticity and connectivity requirements to produce stimulus-pair specific responses in recurrent networks of spiking neurons</article-title>. <source>PLoS Comput Biol</source> <volume>7</volume>: <fpage>e1001091</fpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Newman1"><label>26</label>
<mixed-citation publication-type="other" xlink:type="simple">Newman MEJ (2010) Networks: an Introduction. New York: Oxford University Press.</mixed-citation>
</ref>
<ref id="pone.0100805-Dayan1"><label>27</label>
<mixed-citation publication-type="other" xlink:type="simple">Dayan P, Abbott L (2001) Theoretical neuroscience: Computational and mathematical modeling of neural systems. The MIT Press: Cambridge, Massachusetts.</mixed-citation>
</ref>
<ref id="pone.0100805-Gerstner1"><label>28</label>
<mixed-citation publication-type="other" xlink:type="simple">Gerstner Kistler (2002) Spiking Neuron Models. Cambridge University Press.</mixed-citation>
</ref>
<ref id="pone.0100805-Vasilaki3"><label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vasilaki</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>XJ</given-names></name>, <name name-style="western"><surname>Senn</surname><given-names>W</given-names></name> (<year>2009</year>) <article-title>Learning exible sensori-motor mappings in a complex network</article-title>. <source>Biol Cybern</source> <volume>100</volume>: <fpage>147</fpage>–<lpage>158</lpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Vasilaki4"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vasilaki</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Fremaux</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Urbanczik</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Senn</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2009</year>) <article-title>Spike-based reinforcement learning in continuous state and action space: when policy gradient methods fail</article-title>. <source>PLoS Comput Biol</source> <volume>5</volume>: <fpage>e1000586</fpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Richmond1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Richmond</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Buesing</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Giugliano</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Vasilaki</surname><given-names>E</given-names></name> (<year>2011</year>) <article-title>Democratic population decisions result in robust policy-gradient learning: a parametric study with gpu simulations</article-title>. <source>PLoS One</source> <volume>6</volume>: <fpage>e18539</fpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Pfister1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pfister</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2006</year>) <article-title>Triplets of spikes in a model of spike timing-dependent plasticity</article-title>. <source>J Neurosci</source> <volume>26</volume>: <fpage>9673</fpage>–<lpage>9682</lpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Clopath2"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Clopath</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Ziegler</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Vasilaki</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Bsing</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2008</year>) <article-title>Tag-trigger-consolidation: a model of early and late long-term-potentiation and depression</article-title>. <source>PLoS Comput Biol</source> <volume>4</volume>: <fpage>e1000248</fpage>.</mixed-citation>
</ref>
<ref id="pone.0100805-Hines1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hines</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Morse</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Migliore</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Carnevale</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Shepherd</surname><given-names>G</given-names></name> (<year>2004</year>) <article-title>ModelDB: A database to support computational neuroscience</article-title>. <source>J Comput Neurosci</source> <volume>17</volume>: <fpage>7</fpage>–<lpage>11</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>