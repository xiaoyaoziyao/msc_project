<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pone.0166953</article-id>
<article-id pub-id-type="publisher-id">PONE-D-16-16145</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Speech signal processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject><subj-group><subject>Bioacoustics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Bioacoustics</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Verbal behavior</subject><subj-group><subject>Verbal communication</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Speech</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Animal signaling and communication</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Animal signaling and communication</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Recreation</subject><subj-group><subject>Games</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Prosody Predicts Contest Outcome in Non-Verbal Dialogs</article-title>
<alt-title alt-title-type="running-head">Non-Verbal Communication in Negotiation Process</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Dreiss</surname>
<given-names>Amélie N.</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Chatelain</surname>
<given-names>Philippe G.</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Roulin</surname>
<given-names>Alexandre</given-names>
</name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Richner</surname>
<given-names>Heinz</given-names>
</name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Ecology and Evolution, University of Lausanne, Lausanne, Switzerland</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Institute of Ecology and Evolution, University of Bern, Bern, Switzerland</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Center for Plant Molecular Biology, University of Tübingen, Tübingen, Germany</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Cipresso</surname>
<given-names>Pietro</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>IRCCS Istituto Auxologico Italiano, ITALY</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p><list list-type="simple"><list-item><p><bold>Conceptualization:</bold> AND PGC AR HR.</p></list-item> <list-item><p><bold>Formal analysis:</bold> AND.</p></list-item> <list-item><p><bold>Investigation:</bold> AND PGC.</p></list-item> <list-item><p><bold>Writing – original draft:</bold> AND.</p></list-item></list>
</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">Amelie.n.dreiss@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>1</day>
<month>12</month>
<year>2016</year>
</pub-date>
<pub-date pub-type="collection">
<year>2016</year>
</pub-date>
<volume>11</volume>
<issue>12</issue>
<elocation-id>e0166953</elocation-id>
<history>
<date date-type="received">
<day>28</day>
<month>4</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>7</day>
<month>11</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Dreiss et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0166953"/>
<abstract>
<p>Non-verbal communication has important implications for inter-individual relationships and negotiation success. However, to what extent humans can spontaneously use rhythm and prosody as a sole communication tool is largely unknown. We analysed human ability to resolve a conflict without verbal dialogs, independently of semantics. We invited pairs of subjects to communicate non-verbally using whistle sounds. Along with the production of more whistles, participants unwittingly used a subtle prosodic feature to compete over a resource (ice-cream scoops). Winners can be identified by their propensity to accentuate the first whistles blown when replying to their partner, compared to the following whistles. Naive listeners correctly identified this prosodic feature as a key determinant of which whistler won the interaction. These results suggest that in the absence of other communication channels, individuals spontaneously use a subtle variation of sound accentuation (prosody), instead of merely producing exuberant sounds, to impose themselves in a conflict of interest. We discuss the biological and cultural bases of this ability and their link with verbal communication. Our results highlight the human ability to use non-verbal communication in a negotiation process.</p>
</abstract>
<funding-group>
<funding-statement>The author(s) received no specific funding for this work.</funding-statement>
</funding-group>
<counts>
<fig-count count="3"/>
<table-count count="1"/>
<page-count count="13"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All excel data files are available at: <ext-link ext-link-type="uri" xlink:href="https://figshare.com/s/6589f41fc10f7e221590" xlink:type="simple">https://figshare.com/s/6589f41fc10f7e221590</ext-link>. DOI: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.6084/m9.figshare.4239239" xlink:type="simple">10.6084/m9.figshare.4239239</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>1. Introduction</title>
<p>Many animals communicate to reduce the cost of physical confrontation in conflict resolution. The winner of the conflict has priority access to limited resources, theoretically because the way individuals communicate reliably reflects their motivation or their competitive ability [<xref ref-type="bibr" rid="pone.0166953.ref001">1</xref>,<xref ref-type="bibr" rid="pone.0166953.ref002">2</xref>]. Transferring information on the motivation to compete does not necessarily require a complex language. In non-human animals, competitors use conventional stereotyped signals, with highly motivated individuals producing extravagant displays such as intense or numerous conspicuous calls, which indicate the willingness to compete [<xref ref-type="bibr" rid="pone.0166953.ref003">3</xref>].</p>
<p>Conflict of interest takes place whenever the attainment of the goal by one party excludes or limits its attainment by the other [<xref ref-type="bibr" rid="pone.0166953.ref004">4</xref>]. During human conflicts, beside verbal communication, non-verbal features play a substantial role in the inter-individual interaction (e.g. [<xref ref-type="bibr" rid="pone.0166953.ref005">5</xref>]). Many studies highlighted that non-verbal behavioural rules allow individuals to detect situations of conflict (e.g. [<xref ref-type="bibr" rid="pone.0166953.ref006">6</xref>]), but also to assess dominance, motivation or hierarchy status—parameters indicating who holds the power and has the strongest influence during interactions [<xref ref-type="bibr" rid="pone.0166953.ref007">7</xref>,<xref ref-type="bibr" rid="pone.0166953.ref008">8</xref>,<xref ref-type="bibr" rid="pone.0166953.ref009">9</xref>,<xref ref-type="bibr" rid="pone.0166953.ref010">10</xref>]. The way each participant of a social interaction views a conflict of interest is conveyed by non-verbal signals, such as facial expressions, posture, voice intensity and prosody (intonation, tone, stress and rhythm of vocalizations) [<xref ref-type="bibr" rid="pone.0166953.ref011">11</xref>], which change according to status and conflict intensity. For instance, during a conflict, individual variability in speech pitch and loudness increases [<xref ref-type="bibr" rid="pone.0166953.ref010">10</xref>], while speakers interrupt each other more frequently [<xref ref-type="bibr" rid="pone.0166953.ref012">12</xref>]. Observers are thus able to evaluate speakers’ affect and dominance on the basis of content-filtered speeches, using visual and/or vocal cues alone [<xref ref-type="bibr" rid="pone.0166953.ref013">13</xref>,<xref ref-type="bibr" rid="pone.0166953.ref014">14</xref>]. The study of non-verbal signals has allowed the development of automatic tools to identify dominance and related negotiation patterns from audio-visual media (e.g. [<xref ref-type="bibr" rid="pone.0166953.ref015">15</xref>], and finally to predict conflict outcome [<xref ref-type="bibr" rid="pone.0166953.ref016">16</xref>].</p>
<p>Previous works conducted on human non-verbal signals during conflict are mostly based on regular conversations and/or role-play interactions, recorded in the laboratory or extracted from media. In these settings, verbal factors, such as syntax and words’ meaning, covary with non-verbal cues. Hence, it is difficult to disentangle the spontaneous use and perception of non-verbal cues, independently of verbal communication. In the present study, we chose an alternative way to remove the effects induced by visual contacts and speech content (i.e. meaning of words) on the contest process. Using human ability to perform conversation play with simple noise or visual signs [<xref ref-type="bibr" rid="pone.0166953.ref017">17</xref>,<xref ref-type="bibr" rid="pone.0166953.ref018">18</xref>,<xref ref-type="bibr" rid="pone.0166953.ref019">19</xref>], we set up an experiment in which subjects used whistles as a means of communication. This setting permits us to get rid of the possible confounding factors of verbal communication. Because acoustic vocal cues seem particularly important to infer dominance [<xref ref-type="bibr" rid="pone.0166953.ref010">10</xref>], the whistle, which can be blown at different intensity and rhythm, is a suitable tool to study non-verbal communication. We aimed to tackle (1) the spontaneous use of acoustic features to resolve a contest over a resource and thereafter (2) the detection of such acoustic features by naive subjects listening to the recorded acoustic interactions. We predicted that competition would increase whistlers’ activity, in terms of loudness, duration and number of whistles, their propensity to whistle simultaneously with their partner, and would change the prosodic features of whistles, i.e. increase the change/variability in whistles’ loudness and duration along a sequence of whistles [<xref ref-type="bibr" rid="pone.0166953.ref010">10</xref>]. These acoustic features are also expected to determine the winner of a contest over a reward.</p>
<p>In a first experiment, referred to as the “Whistle Dialog Game”, pairs of subjects were asked to communicate using a whistle. We asked players to whistle during periods said to be non-competitive (i.e. there was no conflict of interest, implying that whatever the behaviour of players nobody would obtain a resource) and during periods said to be competitive (i.e. based on the behaviour of players, only one individual would receive a food reward in the form of an ice cream scoop). Contestants could not see each other and could only use whistle sounds, without any previous agreement of any sort on the way to communicate. Pairs of subjects competed for three successive rewards. At the end of each contest, whistlers indicated which one won the reward. The aim of this experiment was twofold. First, we determined to which extent non-verbal whistling communication was influenced by the need to compete, i.e. which acoustic features changed between non-competitive and competitive whistling dialogs. Second, we tested the ability of participants to resolve a conflict using whistle sounds. In particular, we analysed the acoustic characteristics of the dialogs leading to an agreement and the acoustic characteristics of the designated winner.</p>
<p>In a second experiment of playback, listeners, to whom we broadcast competitive interactions between two individuals recorded during the first experiment, were asked to identify the winner. Our aim was to assess whether naive listeners can infer competition success using acoustic cues, and whether it is concordant with the agreement reached by the dialog participants. Finally, we created a synthetic whistle dialog in which we manipulated one single acoustic cue, which was found in the first experiment to be correlated with the likelihood of winning the reward. This synthetic dialog was broadcast to the naive listeners who had to infer who won the contest.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>2. Methods</title>
<sec id="sec003">
<title>Experiment 1—Whistle Dialog Game</title>
<p>In July 2011, we invited subjects to participate to a communication experiment where two individuals blow a whistle to compete over ice-cream scoops. There were 90 participants representing 45 pairs in total aged on average 26 ± 4 years (all students or employees from the University of Lausanne, Switzerland). 50% of participants were native French-speakers (from Switzerland, France or Belgium) and for them the instructions were given in French. For other groups, in which one or both were non-French speakers (originated from Asia (7), Africa (1), South or North America (7) or Europe (30)), the instructions were in English. Whistlers saw each other before the beginning of the experiment and thus knew with whom they interacted. Most subjects knew their partner (55% were colleagues and 35% were friends or family). However, during the experiment they did not see each other and could only interact by blowing a whistle, as they were separated by a partition and did not talk or laugh.</p>
<p>The experiment consisted of a first non-competitive interaction lasting 3 min followed by three competitive rounds of 1 min, and a final non-competitive interaction of 2 min (<xref ref-type="fig" rid="pone.0166953.g001">Fig 1</xref>). The instructions were given at the beginning of each round. Subjects were not aware of the number and the length of experimental rounds so that this information did not influence their behaviour [<xref ref-type="bibr" rid="pone.0166953.ref020">20</xref>]. During non-competitive interactions, whistler pairs were asked to interact freely by whistling and we told them that the outcome of this non-competitive interaction had no influence on which of the two would get ice cream scoops later on. During each round of competition, they were asked to communicate in order to settle who would receive an ice cream reward (after each competitive round one person can win an ice cream scoop and hence participants can win up to three ice cream scoops). At the end of each competitive round, indicated by the experimenter, the person who assumed s/he had won had to indicate it by blowing the whistle. The first subject to whistle obtained one reward (which was given at the end of the experiment). If both subjects, or none of them, whistled at the end of a competitive round, the competition was considered “unresolved” and nobody obtained the reward. At the end of the whole experiment, the subjects were asked to write down whether rewards were fairly distributed or not. Contestants reached an “agreement” during a round if the two players approved the reward attribution, i.e. one partner reclaimed the reward by whistling (the “winner”) and his/her partner indicated that the reward distribution was fair.</p>
<fig id="pone.0166953.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0166953.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Schema of the experiments.</title>
<p>Experiment 1 –Whistle Dialog Game. Experiment 2 –Winner Designation Playback, based on (a) competitive rounds from the experiment 1 and (b) synthetic playback with utterance accentuated either on the first whistle (left) or in the middle of the sequence (right).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0166953.g001" xlink:type="simple"/>
</fig>
<p>Whistles were plastic toys renewed for each subject and placed at a fixed distance from a microphone (Microspot AR-666). The microphone of each participant was acoustically isolated in a tube, so that it did not record the whistles of the other participant. We automatically extracted the different components of the whistles from the recordings using Avisoft-SASlab Pro’s pulse train analysis (Avisoft Bioacoustics, Glienicke, Germany).</p>
<p>Participants whistled one after the other, one “utterance” defined as a sequence of whistles without partner’s interruption. We considered the following simple acoustic variables for the analyses: (1) mean whistle loudness (mV), (2) mean whistle duration (s), (3) total number of whistles, (4) pause intervals (s) between two successive whistles from the two participants (hereafter referred to as turn-taking gap) and (5) the proportion of overlapping whistles (number of whistles by a given player which started before the end of the partner’s whistle, over the total number whistles emitted by this player). We noticed that the first whistle of an utterance was usually more accentuated than the others, as it was significantly longer and louder (first vs. following whistles’ duration: 0.35±0.03 vs. 0.24±0.02 s, S = 1867, P &lt; 0.0001; first vs. following whistles’ loudness: 0.13±0.01 vs. 0.11±0.01 mV, S = 1438, P &lt; 0.0001). In order to study the prosody pattern used by whistlers and its variation, we hence analysed whistle blow accentuation (in duration and loudness) within an utterance. (6) Duration accentuation and (7) loudness accentuation were measured as the mean difference over all utterances between the first whistle of an utterance and the following whistles of the utterances, in duration (s) and loudness (mV), respectively. These prosodic traits were correlated with the coefficient of variation of whistle duration and loudness, respectively (Spearman correlation: r<sub>423</sub> = 0.31, P &gt; 0.0001, r<sub>423</sub> = 0.27, P &gt; 0.0001, with one line per individual and per round).</p>
</sec>
<sec id="sec004">
<title>Experiment 2 –Winner Designation Playback</title>
<p>In May 2015, we invited 30 subjects aged on average 23 ± 1 years to listen to four successive whistle dialogs. Three of these dialogs (Experiment 2a) were from competitive rounds recorded in Experiment 1 and one was a synthetic playback dialog (Experiment 2b) (<xref ref-type="fig" rid="pone.0166953.g001">Fig 1</xref>). The synthetic playback dialog was broadcast in 3<sup>rd</sup> position, but listeners were not aware that one of the dialogs was synthetic. Listeners were asked to indicate who won the competition at the end of an exchange between two persons competing for a resource using a whistle (Question form in <xref ref-type="supplementary-material" rid="pone.0166953.s001">S1 File</xref>). No further instructions were given. Listeners were asked to add whether they were “sure”, “not totally sure” or “not sure” about who won the interaction. Participants were students or assistants of the Science Faculty of the University of Bern (Switzerland) and did not know the protocol of the Experiment 1; 22 were native German- or Swiss-German-speakers, 2 from Asia, 1 from America or 5 from other parts of Europe than Germany or Switzerland. Instructions were given in English.</p>
<sec id="sec005">
<title>Experiment 2a. Natural Whistle Dialogs</title>
<p>We tested whether third-party listeners detected the winner of a “Whistle Dialog Game” (Exp. 1), when listening to whistle dialogs differing only in prosodic features. We hence selected competitive exchanges from the Whistle Dialog Game performed in 2011, with the criteria that the contestants reached an agreement and that the two contestants differed in less than 10% in the total number of whistles and in average whistle duration. We found 33 competitive rounds following these criteria and we randomly picked 11 of them to be broadcast. In these exchanges, the winners produced 48±2% of total whistles and the loser 52±2% on average (Wilcoxon signed rank test: <italic>P</italic> = 0.52), winners blew in the whistle for on average 0.17±0.02 s and the loser during 0.16±0.02 s (<italic>P</italic> = 0.32). The winners overlapped the whistles of their partner on 4.8±1.1% of the occasions and the loser on 3.7±1.0% (<italic>P</italic> = 0.57). Whistle loudness was on average slightly higher for winners than losers (0.08±0.01 vs. 0.05±0.01 mV, <italic>P</italic> = 0.01), and because we were interested in whether prosody can be used to identify the winner of a competition, we equalized the whistle loudness of both players using Audacity freeware (<ext-link ext-link-type="uri" xlink:href="http://audacity.sourceforge.net" xlink:type="simple">http://audacity.sourceforge.net</ext-link>).</p>
</sec>
<sec id="sec006">
<title>Experiment 2b. Accentuation Playback</title>
<p>We tested whether listeners perceived the prosodic feature found to predict the outcome of a Whistle Dialog Game (the “duration accentuation”, see <xref ref-type="sec" rid="sec014">Results</xref>). To this end, we created synthetic dialogs in which two whistler contestants differed only on this prosodic feature. We generated playbacks for which one whistler accentuated the first whistle of each utterance (high duration accentuation, i.e. the first whistle of each utterance was the longest), while for the other whistler, the longest whistle was in the middle of the utterance (low duration accentuation, <xref ref-type="fig" rid="pone.0166953.g001">Fig 1</xref>). Apart from this difference in duration accentuation, the two playback whistlers produced exactly the same 10 utterances one after the other, in a randomly assigned order. We built five different playbacks, by selecting 10 natural utterance sequences (containing 3 to 8 whistles) from five individuals of the Experiment 1. We modified the order of the whistles in the selected 10 utterances using Audacity software, so that each utterance had a version in which the longest whistle was at the beginning (for one whistler) and one version in which the longest whistle was in the middle (for the other whistler). The whistler ending the playback dialog was randomly assigned at each experiment.</p>
<p>Each whistler was emitted on a different loudspeaker (BeoPlay A2, Bang &amp; Olufsen, Denmark), allowing listeners to easily distinguish the two players. The loudspeakers’ side (left or right) was randomly assigned at each experiment.</p>
</sec>
</sec>
<sec id="sec007">
<title>Statistical Analyses</title>
<sec id="sec008">
<title>Effect of competition on Whistle Dialog Game (Exp. 1)</title>
<p>In order to test whether individuals whistled differently during the competitive and non-competitive dialogs, we averaged the acoustic features of each of the three competitive rounds and of the two non-competitive interactions for each whistler in the Whistle Dialog Game. For each pair of individuals, we then separately averaged the six means for competitive rounds and four means for non-competitive interactions. We compared the mean final values of the competitive and non-competitive whistle dialogs using Wilcoxon signed rank tests (paired difference tests).</p>
</sec>
<sec id="sec009">
<title>Achieving an agreement in the Whistle Dialog Game (Exp. 1)</title>
<p>To analyse which acoustic features were related to the likelihood of reaching an agreement during the competitive rounds of the Whistle Dialog Game, we defined agreement/disagreement in each round as the dependent variable, in a generalized linear mixed model (GLMM) with binomial distribution (d.f. = 1,56). Because each pair competed during three rounds, we set pair as random factor. As independent terms, we set the absolute difference between the two partners for the different acoustic features, round number (1<sup>st</sup>, 2<sup>nd</sup> or 3<sup>rd</sup>) and the interactions between round number and other independent terms.</p>
<p>Some intrinsic factors related to the pair may have influenced the likelihood of reaching an agreement (e.g. if they were acquainted, or if they relied on a hierarchy status or personality perceived before the experiment). To control that acoustic communication <italic>per se</italic> influenced the outcome of the contest, we analysed the within pair change in acoustic features according to whether partners found an agreement or not (for the 28 pairs which did not reach an agreement during all the rounds). We separately averaged the acoustic features of rounds leading to an agreement and rounds leading to a disagreement and compared the values with Wilcoxon signed rank test.</p>
</sec>
<sec id="sec010">
<title>Winning success in the Whistle Dialog Game (Exp. 1)</title>
<p>In order to analyse which factors predicted who won the reward in the Whistle Dialog Game, we compared the acoustic features of winners and losers during resolved competitive rounds (in which only one whistler claimed for the reward at the end of the round). As dependent variable, we set the contest outcome (winner or loser of a round) of a randomly chosen individual of each pair in a GLMM with binomial distribution. As independent variables, we set the mean differences between the two whistlers in the acoustic features, round number (1<sup>st</sup>, 2<sup>nd</sup> or 3<sup>rd</sup>) and the interactions between round number and other independent terms. Pair was set as random factor. The results of the simple effects of GLMMs are given from models in which the interactions are removed (as they appeared to be non-significant, see <xref ref-type="sec" rid="sec014">results</xref>). The covariates were not collinear (all r &lt; 0.4 in Pearson correlations).</p>
<p>As for agreement achievement, some intrinsic factors related to the pair may have influenced the likelihood of winning (e.g. if pairs relied on an assessment of dominance performed before the beginning of the experiment). We thus compared the change of whistling behaviour within an individual, during the three successive rounds, according to whether s/he was the loser or the winner (N = 36 individuals, who were alternatively winner and loser).</p>
</sec>
<sec id="sec011">
<title>Success of Winner Designation (Exp. 2.a)</title>
<p>To determine whether the listeners guessed the identity of the winner of the Whistle Dialog Game, we performed GLMM (with logit link), with designation response (1 for correct / 0 for incorrect designation of winner) as dependent binomial variable. Because the 30 subjects heard 3 different dialogs, listener identity was set as random factor, as well as the playback dialog (in total 11 different dialogs were broadcast). The test of intercept effect in an empty model indicated whether the responses were significantly different from the random expectation of 50%.</p>
</sec>
<sec id="sec012">
<title>Effect of prosody on Winner Designation (Exp. 2.b)</title>
<p>In the artificial playback dialog, we examined whether the whistler with high duration accentuation (i.e. accentuation of the first whistle of utterances) or the other one (low duration accentuation, i.e. accentuation in the middle of utterances) was selected as winner by listeners, using a binomial test.</p>
</sec>
</sec>
<sec id="sec013">
<title>Ethics Statement</title>
<p>Before starting the first experiment, subjects signed a free consent form. All participants were over the age of legal majority. The first experiment was approved by the Human Research Ethics Committee (Vaud canton). The second experiment being a questionnaire, it does not fall within the competences of the Committee. Data were anonymised for both experiments and no identifying participant information was collected.</p>
</sec>
</sec>
<sec id="sec014" sec-type="results">
<title>3. Results</title>
<sec id="sec015">
<title>Experiment 1—Effect of competition on Whistle Dialog Game</title>
<p>When competing for a resource in the form of an ice-cream scoop, participants produced louder sounds, longer whistles and emitted their whistles quicker after their partner than during the pre- and post-competition phases (<xref ref-type="table" rid="pone.0166953.t001">Table 1a</xref>). Whistlers did not produce significantly more whistles per minute (<xref ref-type="table" rid="pone.0166953.t001">Table 1a</xref>) and did not overlap significantly more during competitive rounds than non-competing interactions.</p>
<table-wrap id="pone.0166953.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0166953.t001</object-id>
<label>Table 1</label> <caption><title/> <p>(a) Effect of conflict on whistle dialog. (b) Relation between the likelihood of winning a contest and whistlers’ behaviour.</p></caption>
<alternatives>
<graphic id="pone.0166953.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0166953.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="3"/>
<th align="left" colspan="4">a. Conflict effect</th>
<th align="center" colspan="4">b. Conflict outcome</th>
</tr>
<tr>
<th align="center" colspan="4">Comparison between competitive and non-competitive interactions</th>
<th align="center" colspan="4">Comparison between winners and losers of competitive rounds</th>
</tr>
<tr>
<th align="center">Competition</th>
<th align="center">Non-competition</th>
<th align="center"><italic>S</italic></th>
<th align="center"><italic>P</italic></th>
<th align="center">Winner</th>
<th align="center">Loser</th>
<th align="center"><italic>F</italic></th>
<th align="center"><italic>P</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><bold>Loudness (mV)</bold></td>
<td align="center"><bold>0.13±0.01</bold></td>
<td align="center"><bold>0.10±0.01</bold></td>
<td align="center"><bold>-234</bold></td>
<td align="char" char="."><bold>0.0035</bold></td>
<td align="center">0.13±0.1</td>
<td align="center">0.12±0.1</td>
<td align="char" char=".">1.43</td>
<td align="char" char=".">0.24</td>
</tr>
<tr>
<td align="left"><bold>Duration (s)</bold></td>
<td align="center"><bold>0.36±0.05</bold></td>
<td align="center"><bold>0.25±0.01</bold></td>
<td align="center"><bold>-298</bold></td>
<td align="char" char="."><bold>0.0002</bold></td>
<td align="center">0.29±0.3</td>
<td align="center">0.33±0.7</td>
<td align="char" char=".">0.58</td>
<td align="char" char=".">0.45</td>
</tr>
<tr>
<td align="left"><bold>Turn-taking gap (s)</bold></td>
<td align="center"><bold>1.4±0.1</bold></td>
<td align="center"><bold>1.8±0.1</bold></td>
<td align="center"><bold>204</bold></td>
<td align="char" char="."><bold>0.016</bold></td>
<td align="center">1.42±0.13</td>
<td align="center">1.68±0.16</td>
<td align="char" char=".">0.54</td>
<td align="char" char=".">0.47</td>
</tr>
<tr>
<td align="left"><bold>Number of whistles</bold></td>
<td align="center">37±2</td>
<td align="center">36±2</td>
<td align="center">30</td>
<td align="char" char=".">0.73</td>
<td align="center"><bold>41±3</bold></td>
<td align="center"><bold>36±2</bold></td>
<td align="char" char="."><bold>7.95</bold></td>
<td align="char" char="."><bold>0.007</bold></td>
</tr>
<tr>
<td align="left"><bold>Overlap (%)</bold></td>
<td align="center">4.2±0.6</td>
<td align="center">3.1±0.4</td>
<td align="center">-92</td>
<td align="char" char=".">0.20</td>
<td align="center">3.8±0.6</td>
<td align="center">3.9±0.6</td>
<td align="char" char=".">0.05</td>
<td align="char" char=".">0.82</td>
</tr>
<tr>
<td align="left"><bold>Duration accentuation (s)</bold></td>
<td align="center">0.11±0.02</td>
<td align="center">0.10±0.01</td>
<td align="center">-12</td>
<td align="char" char=".">0.88</td>
<td align="center"><bold>0.14±0.02</bold></td>
<td align="center"><bold>0.07±0.02</bold></td>
<td align="char" char="."><bold>10.18</bold></td>
<td align="char" char="."><bold>0.004</bold></td>
</tr>
<tr>
<td align="left"><bold>Loudness accentuation (mV)</bold></td>
<td align="center">0.05±0.01</td>
<td align="center">0.05±0.02</td>
<td align="center">49</td>
<td align="char" char=".">0.17</td>
<td align="center"><bold>0.03±0.01</bold></td>
<td align="center"><bold>0.04±0.01</bold></td>
<td align="char" char="."><bold>4.06</bold></td>
<td align="char" char="."><bold>0.049</bold></td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t001fn001"><p>Accentuation was estimated as the mean difference between the first whistle of an utterance and the mean of the following whistles, in duration or loudness. (a) Wilcoxon signed rank tests compared the difference in mean acoustic variables between competitive and non-competitive situations in pairs of whistlers. (b) A generalized linear mixed model evaluated the effects of individual acoustic variables on the likelihood of winning a contest.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>Accentuation was not significantly different between competitive and non-competitive rounds (<xref ref-type="table" rid="pone.0166953.t001">Table 1a</xref>).</p>
</sec>
<sec id="sec016">
<title>Experiment 1—Achieving an agreement in the Whistle Dialog Game</title>
<p>In 88% of all 135 competitive rounds, only one individual of the whistling pair claimed for the resource and in turn received an ice cream scoop. This settlement of the contest was considered “fair” by his/her partner in 82% of cases. Hence, in 72% of all rounds (82% of 88%), pairs reached an agreement over ice cream distribution. 12% of the rounds remained unresolved, 7% because both partners claimed the food and 5% because none of them claimed it.</p>
<p>A competitive round was more likely to lead to an agreement when the difference in utterance accentuation between the two partners was higher. In other words, when one individual accentuated the first whistle of utterances to a larger extent than his/her partner, they both agreed about which one should obtain the reward (absolute difference between partners in duration accentuation when competition lead to agreement <italic>vs</italic>. disagreement: 0.27±0.02 <italic>vs</italic>. 0.17±0.01 s; <italic>F</italic><sub>1,71</sub> = 9.40, <italic>P</italic> = 0.003). No other acoustic features predicted the agreement success (absolute difference in loudness: <italic>F</italic><sub>1,66</sub> = 0.09, <italic>P</italic> = 0.77; absolute difference in duration: <italic>F</italic><sub>1,66</sub> = 0.18, <italic>P</italic> = 0.68; absolute difference in turn-taking gap: <italic>F</italic><sub>1,66</sub> = 0.03, <italic>P</italic> = 0.85; absolute difference in number of whistles: <italic>F</italic><sub>1,66</sub> = 0.19, <italic>P</italic> = 0.67; absolute difference proportion of overlapping whistles: <italic>F</italic><sub>1,66</sub> = 0.16, <italic>P</italic> = 0.69). The order of competitive round (1<sup>st</sup>, 2<sup>nd</sup> or 3<sup>rd</sup>) was not significant, nor the interaction between round and acoustic features (all <italic>P</italic> &gt; 0.05). Pairs of competitors which did not always reach an agreement during competitive rounds were more different in duration accentuation when they reached an agreement than when they did not (S = 90, <italic>P</italic> = 0.038).</p>
</sec>
<sec id="sec017">
<title>Experiment 1—Winning success in the Whistle Dialog Game</title>
<p>In competitive rounds that were resolved and hence for which the resource was distributed, the winner accentuated the first whistle of utterances to a larger extent than losers (<xref ref-type="fig" rid="pone.0166953.g002">Fig 2</xref>; effect of accentuation difference between partners on winning probability: <italic>F</italic><sub>1,56</sub> = 10.18, <italic>P</italic> = 0.004; <xref ref-type="table" rid="pone.0166953.t001">Table 1b</xref>). Furthermore, the winners produced more whistles than their partner (effect of difference in number of whistles between partners on winning probability: <italic>F</italic><sub>1,56</sub> = 7.95, <italic>P</italic> = 0.007; <xref ref-type="table" rid="pone.0166953.t001">Table 1b</xref>). The differences between partners in whistle loudness, whistle duration, turn-taking gap or proportion of overlapping whistles did not predict the success of a whistler (<xref ref-type="table" rid="pone.0166953.t001">Table 1b</xref>). The order of competitive round (1<sup>st</sup>, 2<sup>nd</sup> or 3<sup>rd</sup>) was not significant, nor the interaction between round and acoustic features (all P &gt; 0.05). The sex of the whistler did not influence his/her propensity to win the reward (<italic>F</italic><sub>1,56</sub> = 0.07, <italic>P</italic> = 0.79).</p>
<fig id="pone.0166953.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0166953.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Difference in prosody between winners and losers.</title>
<p>Duration of the first and following whistles of utterances, for winners and losers who reached an agreement in whistle contests.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0166953.g002" xlink:type="simple"/>
</fig>
<p>In the 36 pairs where the winner was not always the same individual across the three rounds, the accentuation difference changed significantly between rounds where one or the other individual won (Wilcoxon signed rank test: S = 133, <italic>P</italic> = 0.034), as well as the difference in number of whistles (S = 187.5, <italic>P</italic> = 0.003).</p>
</sec>
<sec id="sec018">
<title>Experiment 2—Winner Designation</title>
<sec id="sec019">
<title>Success of Winner Designation (Exp. 2.a)</title>
<p>70% of the naive listeners designated the winner concordantly with the participants of the Whistle Dialog Game, which is significantly different from random (<xref ref-type="fig" rid="pone.0166953.g003">Fig 3</xref>; GLMM: t = 4.24, <italic>P</italic> &lt; 0.0001). The evaluation success was similar for the 3 dialogs (1<sup>st</sup> natural whistle dialog broadcast: 71% of success, 2<sup>nd</sup>: 68%, 3<sup>rd</sup>: 68%).</p>
<fig id="pone.0166953.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0166953.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Designation of winner by naïve listeners.</title>
<p>Proportion of time listeners designated the winner consistently with the players of the Whistle Dialog Game (left). Proportion of time listeners designated as winner the playback emitting the long whistles at the beginning of utterances rather than in the middle of utterances (right).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0166953.g003" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec020">
<title>Effect of prosody on Winner Designation (Exp. 2.b)</title>
<p>70% of listeners designated the whistler that accentuated the first whistle of each utterance as winner (significantly different from random, unilateral binomial test: Z = 2.19, P = 0.014).</p>
<p>For 16% of replies, listeners were “not sure” while in 40% of replies listeners were certain of their choice. However, the proportion of correct (or expected) choices did not differ significantly between people who said that they were sure or not sure of their response (70% <italic>vs</italic>. 68%).</p>
</sec>
</sec>
</sec>
<sec id="sec021" sec-type="conclusions">
<title>4. Discussion</title>
<p>Inter-individual communication can use other channels than verbal speech. Without any previous training, most participants were able to resolve a contest and reach an agreement solely by emitting sounds with a whistle. The effectiveness of this mode of communication is further demonstrated by the observation that naïve third-party listeners correctly identified the winner of a contest taking place between two subjects. Indeed, we found a consistency between how whistlers attributed the rewards of the contest and how listeners identified the winner, although naïve third-party listened to dialogs for which the winner and loser emitted a similar number of whistles and of similar intensity. In conclusion, our study has demonstrated that individuals can spontaneously use non-verbal acoustic signals to negotiate simple intentions in a conflictual situation. Our results also highlight the importance of non-verbal communication in negotiation process and the ability of people to use non-verbal cues to convince an opponent.</p>
<p>Our results confirm that non-verbal behaviour is a source of information on many aspects of inter-personal relationships [<xref ref-type="bibr" rid="pone.0166953.ref011">11</xref>]. Speaker’s affective state, such as nervousness or confidence, can indeed be perceived by non-verbal expressions [<xref ref-type="bibr" rid="pone.0166953.ref013">13</xref>,<xref ref-type="bibr" rid="pone.0166953.ref021">21</xref>]. The sensitivity to prosodic cues in particular appears early in life, even preverbal infants discern such cues in speech processing [<xref ref-type="bibr" rid="pone.0166953.ref022">22</xref>,<xref ref-type="bibr" rid="pone.0166953.ref023">23</xref>]. The prosody of speech highly influences how people perceive the speaker, although seemingly without being aware of it [<xref ref-type="bibr" rid="pone.0166953.ref024">24</xref>]. This is consistent with the fact that in our setting, listeners were often unaware of their ability to detect the winner of Whistle Dialog Games, as they were uncertain of their reply in 16% of trials, but still assessed the correct winner in 68% of these replies. Listeners could not justify their choice and were not able to describe the difference between whistlers in terms of acoustic features. In their comments after the experiment, they claimed having chosen the “more aggressive” or “more persuasive” contestant (A.D., personal observation). The confidence of individuals in their social judgment is generally not a good predictor in their actual ability, for instance when judging deception or assessing the relationship between unknown persons [<xref ref-type="bibr" rid="pone.0166953.ref025">25</xref>,<xref ref-type="bibr" rid="pone.0166953.ref026">26</xref>]. The lack of relationship between confidence and accuracy may be due to the fact that they are differently influenced by factors such as self-confidence, sex or age [<xref ref-type="bibr" rid="pone.0166953.ref026">26</xref>].</p>
<p>Two types of sound characteristics were modulated in Dialogs Games. The first type was related to “performance”. Individuals increased the duration, rhythm and loudness of whistles when competing and produced numerous whistles to persuade their partner and win the contest. Interestingly, contestants used a second type of sound characteristics, the accentuation of the first whistle of utterances, which can be related to “prosody”. Whistlers used this subtle sound variation to affirm their dominance or motivation to win the contest. When they took the turn, winners produced a longer initial whistle than losers. In contrast, lengthening whistles within an utterance did not affect the winning success. Listeners hearing two contestants differing only in this accentuation evaluated that the winner was the one lengthening the first whistle. Competition for resources can hence be solved through a prosodic feature only, recognised as a dominant cue by listeners.</p>
<p>In situations where verbal communication is impossible, people are known to improvise gestures [<xref ref-type="bibr" rid="pone.0166953.ref027">27</xref>] or vocal symbols [<xref ref-type="bibr" rid="pone.0166953.ref019">19</xref>]. This ability underlines human potential to create non-verbal signals to communicate various meanings and emotions. In our experiment, the fact that the sound characteristics used to win the Dialog Game were recognized by naïve listeners, suggests that these sound characteristics have a <italic>biological</italic> or <italic>cultural</italic> basis. The ability to use and understand “performance” and “prosodic” acoustic cues independently of verbal communication are likely to be related to different phenomenon. We hypothesise that “performance” acoustic features have biological basis, while the “prosodic” feature has a cultural basis. Indeed, communication during conflict may require “performance” signals, i.e. conspicuous and costly displays to influence the behaviour of conspecifics. Theoretically, this is due to only competitive or highly motivated individuals being willing to pay the cost of conspicuous displays [<xref ref-type="bibr" rid="pone.0166953.ref028">28</xref>,<xref ref-type="bibr" rid="pone.0166953.ref029">29</xref>]. Non-human animal communication and human language would share this common biological characteristic when individuals display their dominance during conflicts, as shown by human ability to detect the social meaning of male dominance vocalizations of a macaque monkey [<xref ref-type="bibr" rid="pone.0166953.ref030">30</xref>], although this ability might be related to experience-dependent cognitive mechanisms [<xref ref-type="bibr" rid="pone.0166953.ref031">31</xref>]. In the Whistle Dialog Game, whistlers produced longer, louder and more rapid whistles when competing. In the same vein, speakers engaged in a dispute usually talk loudly [<xref ref-type="bibr" rid="pone.0166953.ref032">32</xref>] and with a high rate of interruption [<xref ref-type="bibr" rid="pone.0166953.ref033">33</xref>]. In our experiment, whistlers produced more whistles to maximise their access to a reward. Vocal activity in speech, i.e. the proportion of time an individual is speaking, is often associated with dominance or hierarchy status [<xref ref-type="bibr" rid="pone.0166953.ref009">9</xref>,<xref ref-type="bibr" rid="pone.0166953.ref034">34</xref>]. Hence, although human communication is of totally different nature than non-human animal communication in many aspects, human competitors may follow fundamental rules shared by many animal signalling systems when they manipulate sounds to compete, that is to say the use of performance signals.</p>
<p>On the other hand, beside the number of emitted whistles (and contrary to most animal signals), whistlers used and perceived a prosodic cue, the accentuation of the first whistle of utterance, when contesting during Dialog Games. This signal is not a performance signal, as the accentuation of sound was not higher but differently distributed in winners than losers. The reason why this signal was used and understood by a majority of contestants can only be speculated. It might be related to whistlers mirroring prosodic features used during verbal interactions. Whistle blowing cannot be directly compared with voice production, however, whistlers might have mimicked speech, the most commonly used auditory signal. Adults indeed interpret/classify the variation of simple tone loudness and pitch according to the position of word stress in their native language [<xref ref-type="bibr" rid="pone.0166953.ref035">35</xref>]. Syllable accentuation or lengthening of some parts of an utterance is observed in various languages [<xref ref-type="bibr" rid="pone.0166953.ref036">36</xref>] and even in baby babbling [<xref ref-type="bibr" rid="pone.0166953.ref037">37</xref>]. However, although dominance is known to influence the variability of speech vocal pitch and amplitude [<xref ref-type="bibr" rid="pone.0166953.ref010">10</xref>], the relation between dominance and the specific stress of some part of a sentence has not been studied so far, as far as we know. Moreover, the tendency to lengthen the initial or the final part of utterances varies across languages [<xref ref-type="bibr" rid="pone.0166953.ref036">36</xref>,<xref ref-type="bibr" rid="pone.0166953.ref037">37</xref>]. The first experiment was mainly performed with French-speakers, but with a high variability in sample, and the listeners of second experiment were mostly German-speakers, which did not prevent them from detecting the contest outcome. The repeatability of our results suggests that the use of this prosodic feature—the accentuation of the first whistle—was independent of subjects’ mother tongue. Our experiment has hence provided evidence that a prosodic feature can be used spontaneously to solve a contest. The cultural or biological origin of this prosodic feature is hence to be studied. The next step would be to return to natural speech and test whether it is used in verbal speech and in which competitive and cultural context. Our work should trigger further studies to determine whether accentuation in the beginning of utterance is used in verbal negotiation, and to what extend it can help to predict negotiation outcome.</p>
</sec>
<sec id="sec022">
<title>Supporting Information</title>
<supplementary-material id="pone.0166953.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pone.0166953.s001" xlink:type="simple">
<label>S1 File</label>
<caption>
<title>Question form given to subjects of Experiment 2.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Charlène Ruppli for her help in collecting the data and three anonymous reviewers for helpful comments on a previous version of the paper.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0166953.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parker</surname> <given-names>GA</given-names></name> (<year>1974</year>) <article-title>Assessment strategy and evolution of fighting behavior</article-title>. <source>Journal of Theoretical Biology</source> <volume>47</volume>: <fpage>223</fpage>–<lpage>243</lpage>. <object-id pub-id-type="pmid">4477626</object-id></mixed-citation></ref>
<ref id="pone.0166953.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maynard Smith</surname> <given-names>J</given-names></name> (<year>1982</year>) <article-title>Do animals convey information about their intentions?</article-title> <source>Journal of Theoretical Biology</source> <volume>97</volume>: <fpage>1</fpage>–<lpage>5</lpage>.</mixed-citation></ref>
<ref id="pone.0166953.ref003"><label>3</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Bradbury</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Vehrencamp</surname> <given-names>SL</given-names></name> (<year>1998</year>) <source>Principles of animal communication</source>. <publisher-loc>Sunderland, Massachusetts</publisher-loc>: <publisher-name>Sinauer Associates</publisher-name>.</mixed-citation></ref>
<ref id="pone.0166953.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wall</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Callister</surname> <given-names>RR</given-names></name> (<year>1995</year>) <article-title>Conflict and its management</article-title>. <source>Journal of Management</source> <volume>21</volume>: <fpage>515</fpage>–<lpage>558</lpage>.</mixed-citation></ref>
<ref id="pone.0166953.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sternglanz</surname> <given-names>RW</given-names></name>, <name name-style="western"><surname>DePaulo</surname> <given-names>BM</given-names></name> (<year>2004</year>) <article-title>Reading nonverbal cues to emotions: The advantages and liabilities of relationship closeness</article-title>. <source>Journal of Nonverbal Behavior</source> <volume>28</volume>: <fpage>245</fpage>–<lpage>266</lpage>.</mixed-citation></ref>
<ref id="pone.0166953.ref006"><label>6</label><mixed-citation publication-type="other" xlink:type="simple">Kim S, Valente F, Vinciarelli A, Ieee (2012) Automatic detection of conflicts in spoken conversations: Ratings and analysis of broadcast political debates. Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing: 5089–5092.</mixed-citation></ref>
<ref id="pone.0166953.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jayagopi</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Hung</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Yeo</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Gatica-Perez</surname> <given-names>D</given-names></name> (<year>2009</year>) <article-title>Modeling dominance in group conversations using nonverbal activity cues</article-title>. <source>Ieee Transactions on Audio Speech and Language Processing</source> <volume>17</volume>: <fpage>501</fpage>–<lpage>513</lpage>.</mixed-citation></ref>
<ref id="pone.0166953.ref008"><label>8</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Rienks</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Heylen</surname> <given-names>D</given-names></name> (<year>2005</year>) <chapter-title>Dominance detection in meetings using easily obtainable features</chapter-title>. In: <name name-style="western"><surname>Renals</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>S</given-names></name>, editors. <source>Machine Learning for Multimodal Interaction</source>. pp. <fpage>76</fpage>–<lpage>86</lpage>.</mixed-citation></ref>
<ref id="pone.0166953.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ko</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Sadler</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Galinsky</surname> <given-names>AD</given-names></name> (<year>2015</year>) <article-title>The sound of power: Conveying and detecting hierarchical rank through voice</article-title>. <source>Psychological Science</source> <volume>26</volume>: <fpage>3</fpage>–<lpage>14</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1177/0956797614553009" xlink:type="simple">10.1177/0956797614553009</ext-link></comment> <object-id pub-id-type="pmid">25413877</object-id></mixed-citation></ref>
<ref id="pone.0166953.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hall</surname> <given-names>JA</given-names></name>, <name name-style="western"><surname>Coats</surname> <given-names>EJ</given-names></name>, <name name-style="western"><surname>LeBeau</surname> <given-names>LS</given-names></name> (<year>2005</year>) <article-title>Nonverbal behavior and the vertical dimension of social relations: A meta-analysis</article-title>. <source>Psychological Bulletin</source> <volume>131</volume>: <fpage>898</fpage>–<lpage>924</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-2909.131.6.898" xlink:type="simple">10.1037/0033-2909.131.6.898</ext-link></comment> <object-id pub-id-type="pmid">16351328</object-id></mixed-citation></ref>
<ref id="pone.0166953.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vinciarelli</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pantic</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bourlard</surname> <given-names>H</given-names></name> (<year>2009</year>) <article-title>Social signal processing: Survey of an emerging domain</article-title>. <source>Image and vision computing</source> <volume>27</volume>: <fpage>1743</fpage>–<lpage>1759</lpage>.</mixed-citation></ref>
<ref id="pone.0166953.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cooper</surname> <given-names>VW</given-names></name> (<year>1986</year>) <article-title>Participant and observer attribution of affect in interpersonal conflict: An examination of noncontent verbal-behavior</article-title> <source>Journal of Nonverbal Behavior</source> <volume>10</volume>: <fpage>134</fpage>–<lpage>144</lpage>.</mixed-citation></ref>
<ref id="pone.0166953.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Krauss</surname> <given-names>RM</given-names></name>, <name name-style="western"><surname>Apple</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Morency</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Wenzel</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Winton</surname> <given-names>W</given-names></name> (<year>1981</year>) <article-title>Verbal, vocal, and visible factors in judgments of another's affect</article-title>. <source>Journal of Personality and Social Psychology</source> <volume>40</volume>: <fpage>312</fpage>–<lpage>320</lpage>.</mixed-citation></ref>
<ref id="pone.0166953.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tusing</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Dillard</surname> <given-names>JP</given-names></name> (<year>2000</year>) <article-title>The sounds of dominance—Vocal precursors of perceived dominance during interpersonal influence</article-title>. <source>Human Communication Research</source> <volume>26</volume>: <fpage>148</fpage>–<lpage>171</lpage>.</mixed-citation></ref>
<ref id="pone.0166953.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gatica-Perez</surname> <given-names>D</given-names></name> (<year>2009</year>) <article-title>Automatic nonverbal analysis of social interaction in small groups: A review</article-title>. <source>Image and vision computing</source> <volume>27</volume>: <fpage>1775</fpage>–<lpage>1787</lpage>.</mixed-citation></ref>
<ref id="pone.0166953.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Curhan</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Pentland</surname> <given-names>A</given-names></name> (<year>2007</year>) <article-title>Thin slices of negotiation: Predicting outcomes from conversational dynamics within the first 5 minutes</article-title>. <source>Journal of Applied Psychology</source> <volume>92</volume>: <fpage>802</fpage>–<lpage>811</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0021-9010.92.3.802" xlink:type="simple">10.1037/0021-9010.92.3.802</ext-link></comment> <object-id pub-id-type="pmid">17484559</object-id></mixed-citation></ref>
<ref id="pone.0166953.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Steels</surname> <given-names>L</given-names></name> (<year>2006</year>) <article-title>Experiments on the emergence of human communication</article-title>. <source>Trends in Cognitive Sciences</source> <volume>10</volume>: <fpage>347</fpage>–<lpage>349</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.tics.2006.06.002" xlink:type="simple">10.1016/j.tics.2006.06.002</ext-link></comment> <object-id pub-id-type="pmid">16828573</object-id></mixed-citation></ref>
<ref id="pone.0166953.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kose-Bagci</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Dautenhahn</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Syrdal</surname> <given-names>DS</given-names></name>, <name name-style="western"><surname>Nehaniv</surname> <given-names>CL</given-names></name> (<year>2010</year>) <article-title>Drum-mate: interaction dynamics and gestures in human-humanoid drumming experiments</article-title>. <source>Connection Science</source> <volume>22</volume>: <fpage>103</fpage>–<lpage>134</lpage>.</mixed-citation></ref>
<ref id="pone.0166953.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perlman</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Dale</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Lupyan</surname> <given-names>G</given-names></name> (<year>2015</year>) <article-title>Iconicity can ground the creation of vocal symbols</article-title>. <source>Royal Society Open Science</source> <volume>2</volume>: <fpage>150152</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1098/rsos.150152" xlink:type="simple">10.1098/rsos.150152</ext-link></comment> <object-id pub-id-type="pmid">26361547</object-id></mixed-citation></ref>
<ref id="pone.0166953.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Axelrod</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Hamilton</surname> <given-names>WD</given-names></name> (<year>1981</year>) <article-title>The evolution of cooperation</article-title>. <source>Science</source> <volume>211</volume>: <fpage>1390</fpage>–<lpage>1396</lpage>. <object-id pub-id-type="pmid">7466396</object-id></mixed-citation></ref>
<ref id="pone.0166953.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Depaulo</surname> <given-names>BM</given-names></name> (<year>1992</year>) <article-title>Nonverbal behavior and self-presentation</article-title>. <source>Psychological Bulletin</source> <volume>111</volume>: <fpage>203</fpage>–<lpage>243</lpage>. <object-id pub-id-type="pmid">1557474</object-id></mixed-citation></ref>
<ref id="pone.0166953.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mandel</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Jusczyk</surname> <given-names>PW</given-names></name>, <name name-style="western"><surname>Kemler Nelson</surname> <given-names>DG</given-names></name> (<year>1994</year>) <article-title>Does sentential prosody help infants to organize and remember speech information?</article-title> <source>Cognition</source> <volume>53</volume>: <fpage>155</fpage>–<lpage>180</lpage>. <object-id pub-id-type="pmid">7805352</object-id></mixed-citation></ref>
<ref id="pone.0166953.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Soderstrom</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Seidl</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>DGK</given-names></name>, <name name-style="western"><surname>Jusczyk</surname> <given-names>PW</given-names></name> (<year>2003</year>) <article-title>The prosodic bootstrapping of phrases: Evidence from prelinguistic infants</article-title>. <source>Journal of Memory and Language</source> <volume>49</volume>: <fpage>249</fpage>–<lpage>267</lpage>.</mixed-citation></ref>
<ref id="pone.0166953.ref024"><label>24</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Nass</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Brave</surname> <given-names>S</given-names></name> (<year>2004</year>) <source>Voice activated: How people are wired for speech and how computers will speak with us</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</mixed-citation></ref>
<ref id="pone.0166953.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Patterson</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Foster</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Bellmer</surname> <given-names>CD</given-names></name> (<year>2001</year>) <article-title>Another look at accuracy and confidence in social judgments</article-title>. <source>Journal of Nonverbal Behavior</source> <volume>25</volume>: <fpage>207</fpage>–<lpage>219</lpage>.</mixed-citation></ref>
<ref id="pone.0166953.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>DePaulo</surname> <given-names>BM</given-names></name>, <name name-style="western"><surname>Charlton</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Cooper</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Lindsay</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Muhlenbruck</surname> <given-names>L</given-names></name> (<year>1997</year>) <article-title>The accuracy-confidence correlation in the detection of deception</article-title>. <source>Personality and Social Psychology Review</source> <volume>1</volume>: <fpage>346</fpage>–<lpage>357</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1207/s15327957pspr0104_5" xlink:type="simple">10.1207/s15327957pspr0104_5</ext-link></comment> <object-id pub-id-type="pmid">15661668</object-id></mixed-citation></ref>
<ref id="pone.0166953.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goldin-Meadow</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>McNeill</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Singleton</surname> <given-names>J</given-names></name> (<year>1996</year>) <article-title>Silence is liberating: removing the handcuffs on grammatical expression in the manual modality</article-title>. <source>Psychological Review</source> <volume>103</volume>: <fpage>34</fpage>–<lpage>55</lpage>. <object-id pub-id-type="pmid">8650298</object-id></mixed-citation></ref>
<ref id="pone.0166953.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zahavi</surname> <given-names>A</given-names></name> (<year>1975</year>) <article-title>Mate selection: a selection for a handicap</article-title>. <source>Journal of Theoretical Biology</source> <volume>53</volume>: <fpage>205</fpage>–<lpage>214</lpage>. <object-id pub-id-type="pmid">1195756</object-id></mixed-citation></ref>
<ref id="pone.0166953.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grafen</surname> <given-names>A</given-names></name> (<year>1990</year>) <article-title>Biological signals as handicaps</article-title>. <source>Journal of Theoretical Biology</source> <volume>144</volume>: <fpage>517</fpage>–<lpage>546</lpage>. <object-id pub-id-type="pmid">2402153</object-id></mixed-citation></ref>
<ref id="pone.0166953.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leinonen</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Linnankoski</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Laakso</surname> <given-names>ML</given-names></name>, <name name-style="western"><surname>Aulanko</surname> <given-names>R</given-names></name> (<year>1991</year>) <article-title>Vocal communication between species: Man and macaque</article-title>. <source>Language &amp; Communication</source> <volume>11</volume>: <fpage>241</fpage>–<lpage>262</lpage>.</mixed-citation></ref>
<ref id="pone.0166953.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Scheumann</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hasting</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Kotz</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Zimmermann</surname> <given-names>E</given-names></name> (<year>2014</year>) <article-title>The Voice of Emotion across Species: How Do Human Listeners Recognize Animals' Affective States?</article-title> <source>Plos One</source> <volume>9</volume>: <fpage>e91192</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0091192" xlink:type="simple">10.1371/journal.pone.0091192</ext-link></comment> <object-id pub-id-type="pmid">24621604</object-id></mixed-citation></ref>
<ref id="pone.0166953.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sillars</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Parry</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Coletti</surname> <given-names>SF</given-names></name>, <name name-style="western"><surname>Rogers</surname> <given-names>MA</given-names></name> (<year>1982</year>) <article-title>Coding verbal conflict tactics: nonverbal and perceptual correlates of the avoidance-distributive-integrative distinction</article-title>. <source>Human Communication Research</source> <volume>9</volume>: <fpage>83</fpage>–<lpage>95</lpage>.</mixed-citation></ref>
<ref id="pone.0166953.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pesarin</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Cristani</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Murino</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Vinciarelli</surname> <given-names>A</given-names></name> (<year>2012</year>) <article-title>Conversation analysis at work: detection of conflict in competitive discussions through semi-automatic turn-organization analysis</article-title>. <source>Cognitive Processing</source> <volume>13</volume>: <fpage>533</fpage>–<lpage>540</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s10339-011-0417-9" xlink:type="simple">10.1007/s10339-011-0417-9</ext-link></comment> <object-id pub-id-type="pmid">22009168</object-id></mixed-citation></ref>
<ref id="pone.0166953.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sanchez-Cortes</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Aran</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Mast</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Gatica-Perez</surname> <given-names>D</given-names></name> (<year>2012</year>) <article-title>A nonverbal behavior approach to identify emergent leaders in small groups</article-title>. <source>Ieee Transactions on Multimedia</source> <volume>14</volume>: <fpage>816</fpage>–<lpage>832</lpage>.</mixed-citation></ref>
<ref id="pone.0166953.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Iversen</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Patel</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Ohgushi</surname> <given-names>K</given-names></name> (<year>2008</year>) <article-title>Perception of rhythmic grouping depends on auditory experience</article-title>. <source>Journal of the Acoustical Society of America</source> <volume>124</volume>: <fpage>2263</fpage>–<lpage>2271</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1121/1.2973189" xlink:type="simple">10.1121/1.2973189</ext-link></comment> <object-id pub-id-type="pmid">19062864</object-id></mixed-citation></ref>
<ref id="pone.0166953.ref036"><label>36</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Saarni</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Hakokari</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Isoaho</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Aaltonen</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Salakoski</surname> <given-names>T</given-names></name> (<year>2006</year>) <chapter-title>Segmental duration in utterance-initial environment: Evidence from Finnish speech corpora</chapter-title>. In: <name name-style="western"><surname>Salakoski</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Ginter</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Pyysalo</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Pahikkala</surname> <given-names>T</given-names></name>, editors. <source>Advances in Natural Language Processing, Proceedings</source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer-Verlag Berlin</publisher-name>. pp. <fpage>576</fpage>–<lpage>584</lpage>.</mixed-citation></ref>
<ref id="pone.0166953.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hallé</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Deboyssonbardies</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Vihman</surname> <given-names>MM</given-names></name> (<year>1991</year>) <article-title>Beginnings of prosodic organization: Intonation and duration patterns of disyllables produced by Japanese and French infants</article-title>. <source>Language and Speech</source> <volume>34</volume>: <fpage>299</fpage>–<lpage>318</lpage>. <object-id pub-id-type="pmid">1843528</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>