<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
   <journal-meta>
      <journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
      <journal-id journal-id-type="publisher-id">plos</journal-id>
      <journal-id journal-id-type="pmc">plosone</journal-id>
      <journal-title-group>
         <journal-title>PLoS ONE</journal-title>
      </journal-title-group>
      <issn pub-type="epub">1932-6203</issn>
      <publisher>
         <publisher-name>Public Library of Science</publisher-name>
         <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher>
   </journal-meta>
   <article-meta>
      <article-id pub-id-type="publisher-id">PONE-D-13-48161</article-id>
      <article-id pub-id-type="doi">10.1371/journal.pone.0098146</article-id>
      <article-categories>
         <subj-group subj-group-type="heading">
            <subject>Research Article</subject>
         </subj-group>
<subj-group subj-group-type="Discipline-v2"><subject>Biology and life sciences</subject>
<subj-group>
<subject>Computational biology</subject>
<subj-group>
<subject>Genome analysis</subject>
<subj-group>
<subject>Sequence assembly tools</subject>
</subj-group>
</subj-group>
</subj-group>
<subj-group>
<subject>Genetics</subject>
<subj-group>
<subject>Genomics</subject>
</subj-group>
</subj-group>
</subj-group>
<subj-group subj-group-type="Discipline-v2"><subject>Computer and information sciences</subject>
<subj-group>
<subject>Computing methods</subject>
<subj-group>
<subject>Cloud computing</subject>
</subj-group>
</subj-group>
<subj-group>
<subject>Software engineering</subject>
<subj-group>
<subject>Software tools</subject>
</subj-group>
</subj-group>
</subj-group>
<subj-group subj-group-type="Discipline-v2"><subject>Engineering and technology</subject>
</subj-group>
      </article-categories>
      <title-group>
         <article-title>CloudDOE: A User-Friendly Tool for Deploying Hadoop Clouds and Analyzing High-Throughput Sequencing Data with MapReduce</article-title>
         <alt-title alt-title-type="running-head">A User-Friendly Tool for Analyzing Big NGS Data in Clouds</alt-title>
      </title-group>
      <contrib-group>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Chung</surname>
<given-names>Wei-Chun</given-names>
            </name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff2"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff3"><sup>3</sup></xref>
         </contrib>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Chen</surname>
<given-names>Chien-Chih</given-names>
            </name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff2"><sup>2</sup></xref>
         </contrib>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Ho</surname>
<given-names>Jan-Ming</given-names>
            </name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff3"><sup>3</sup></xref>
         </contrib>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Lin</surname>
<given-names>Chung-Yen</given-names>
            </name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
         </contrib>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Hsu</surname>
<given-names>Wen-Lian</given-names>
            </name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
         </contrib>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Wang</surname>
<given-names>Yu-Chun</given-names>
            </name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
         </contrib>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Lee</surname>
<given-names>D. T.</given-names>
            </name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff2"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff4"><sup>4</sup></xref>
         </contrib>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Lai</surname>
<given-names>Feipei</given-names>
            </name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref>
         </contrib>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Huang</surname>
<given-names>Chih-Wei</given-names>
            </name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
         </contrib>
         <contrib contrib-type="author" xlink:type="simple">
            <name name-style="western"><surname>Chang</surname><given-names>Yu-Jung</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref>
         </contrib>
      </contrib-group>
      <aff id="aff1"><label>1</label><addr-line>Institute of Information Science, Academia Sinica, Taipei, Taiwan</addr-line></aff>
      <aff id="aff2"><label>2</label><addr-line>Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan</addr-line></aff>
      <aff id="aff3"><label>3</label><addr-line>Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan</addr-line></aff>
      <aff id="aff4"><label>4</label><addr-line>Department of Computer Science and Information Engineering, National Chung Hsing University, Taichung, Taiwan</addr-line></aff>
      <contrib-group>
         <contrib contrib-type="editor" xlink:type="simple">
            <name name-style="western"><surname>Antoniewski</surname>
<given-names>Christophe</given-names>
            </name><role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
         </contrib>
      </contrib-group>
      <aff id="edit1"><addr-line>CNRS UMR7622 &amp; University Paris 6 Pierre-et-Marie-Curie, France</addr-line></aff>
      <author-notes>
         <corresp id="cor1">* E-mail: <email xlink:type="simple">yjchang@iis.sinica.edu.tw</email></corresp>
         <fn fn-type="conflict">
            <p>The authors have declared that no competing interests exist.</p>
         </fn>
         <fn fn-type="con">
            <p>Conceived and designed the experiments: WCC CCC JMH CYL WLH YCW DTL FL YJC. Performed the experiments: WCC CCC YCW YJC. Analyzed the data: WCC CCC JMH CYL WLH YJC. Contributed reagents/materials/analysis tools: JMH CYL WLH. Wrote the paper: WCC CCC JMH CYL WLH YCW DTL FL YJC. Developed the software and web: WCC CCC YCW CWH YJC.</p>
         </fn>
      </author-notes>
      <pub-date pub-type="collection">
         <year>2014</year>
      </pub-date>
      <pub-date pub-type="epub">
         <day>4</day>
         <month>6</month>
         <year>2014</year>
      </pub-date>
      <volume>9</volume>
      <issue>6</issue>
      <elocation-id>e98146</elocation-id>
      <history>
         <date date-type="received">
            <day>25</day>
            <month>11</month>
            <year>2013</year>
         </date>
         <date date-type="accepted">
            <day>29</day>
            <month>4</month>
            <year>2014</year>
         </date>
      </history>
      <permissions>
         <copyright-year>2014</copyright-year>
         <copyright-holder>Chung et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
      <abstract>
         <sec>
            <title>Background</title>
            <p>Explosive growth of next-generation sequencing data has resulted in ultra-large-scale data sets and ensuing computational problems. Cloud computing provides an on-demand and scalable environment for large-scale data analysis. Using a MapReduce framework, data and workload can be distributed via a network to computers in the cloud to substantially reduce computational latency. Hadoop/MapReduce has been successfully adopted in bioinformatics for genome assembly, mapping reads to genomes, and finding single nucleotide polymorphisms. Major cloud providers offer Hadoop cloud services to their users. However, it remains technically challenging to deploy a Hadoop cloud for those who prefer to run MapReduce programs in a cluster without built-in Hadoop/MapReduce.</p>
         </sec>
         <sec>
            <title>Results</title>
            <p>We present CloudDOE, a platform-independent software package implemented in Java. CloudDOE encapsulates technical details behind a user-friendly graphical interface, thus liberating scientists from having to perform complicated operational procedures. Users are guided through the user interface to deploy a Hadoop cloud within in-house computing environments and to run applications specifically targeted for bioinformatics, including CloudBurst, CloudBrush, and CloudRS. One may also use CloudDOE on top of a public cloud. CloudDOE consists of three wizards, i.e., Deploy, Operate, and Extend wizards. Deploy wizard is designed to aid the system administrator to deploy a Hadoop cloud. It installs Java runtime environment version 1.6 and Hadoop version 0.20.203, and initiates the service automatically. Operate wizard allows the user to run a MapReduce application on the dashboard list. To extend the dashboard list, the administrator may install a new MapReduce application using Extend wizard.</p>
         </sec>
         <sec>
            <title>Conclusions</title>
            <p>CloudDOE is a user-friendly tool for deploying a Hadoop cloud. Its smart wizards substantially reduce the complexity and costs of deployment, execution, enhancement, and management. Interested users may collaborate to improve the source code of CloudDOE to further incorporate more MapReduce bioinformatics tools into CloudDOE and support next-generation big data open source tools, e.g., Hadoop BigTop and Spark. Availability: CloudDOE is distributed under Apache License 2.0 and is freely available at <ext-link ext-link-type="uri" xlink:href="http://clouddoe.iis.sinica.edu.tw/" xlink:type="simple">http://clouddoe.iis.sinica.edu.tw/</ext-link>.</p>
         </sec>
      </abstract>
      <funding-group>
         <funding-statement>This research is partially supported by Digital Culture Center, Academia Sinica (<ext-link ext-link-type="uri" xlink:href="http://www.sinica.edu.tw/main_e.shtml" xlink:type="simple">http://www.sinica.edu.tw/main_e.shtml</ext-link>) under the project “System Management and Content Retrieval Technologies for Supporting Cloud-based Digital Archive Systems and Services,” and National Science Council (<ext-link ext-link-type="uri" xlink:href="http://web1.nsc.gov.tw/mp.aspx?mp=7" xlink:type="simple">http://web1.nsc.gov.tw/mp.aspx?mp=7</ext-link>), Taiwan, under 102-2221-E-001-013-MY3 dubbed as “Next Generation Content Delivery Network: Cloud and Mobile Internet.” The hicloud CaaS computing resources are supported by Chunghwa Telecom Co. and Networked Communications Program of Taiwan under the project “A Cloud-Based DNA Analysis Platform.” The Microsoft Azure computing resources are supported by Microsoft Co. and National Science Council of Taiwan under the project “World Cloud Research Collaboration Project hosted by Microsoft Research: Electronic Laboratory Notebook (Elegance) for Biomedical Research Community on Sharing, Co-working and Inspiriting in the Cloud” to Chung-Yen Lin. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
      </funding-group>
<counts>
<page-count count="7"/>
</counts>
</article-meta>
</front>
<body>
   <sec id="s1">
      <title>Introduction</title>
      <p>Progress in computer science and technology has vastly promoted the development of genetic research in the past few decades. Next-generation sequencing (NGS) is a particularly notable technology for genetics and computational biology research. The explosive growth of NGS data has already resulted in ultra-large-scale datasets and various computational problems for conventional NGS tools; for instance, insufficient computation resources and undesirably long execution times <xref ref-type="bibr" rid="pone.0098146-Zou1">[1]</xref>. To overcome the issues associated with processing of large-scale data, MapReduce <xref ref-type="bibr" rid="pone.0098146-Dean1">[2]</xref> and its Java implementation, Hadoop <xref ref-type="bibr" rid="pone.0098146-Welcome1">[3]</xref>, were introduced. MapReduce is a framework that processes huge datasets in parallel by utilizing a large number of computers simultaneously, in which the computing resources can be allocated dynamically. In the programming model of MapReduce, developers only need to focus on implementing their programs by writing appropriate mapper and reducer procedures. Data and computations within the framework are automatically stored and executed across all computers to obtain redundancy, fault tolerance, parallelization, and load balance. Therefore, an increasing number of tools in bioinformatics <xref ref-type="bibr" rid="pone.0098146-Zou1">[1]</xref>, <xref ref-type="bibr" rid="pone.0098146-Taylor1">[4]</xref> are successfully being adapted to fit within the MapReduce programming model in order to analyze large biological datasets using cloud computing, e.g., mapping reads to human genomes <xref ref-type="bibr" rid="pone.0098146-Schatz1">[5]</xref>, calculating expression of RNA data <xref ref-type="bibr" rid="pone.0098146-Langmead1">[6]</xref>, finding single nucleotide polymorphisms <xref ref-type="bibr" rid="pone.0098146-Langmead2">[7]</xref>, performing <italic>de novo</italic> genome assembly <xref ref-type="bibr" rid="pone.0098146-Chang1">[8]</xref>, and achieving error correction of reads <xref ref-type="bibr" rid="pone.0098146-Chen1">[9]</xref>. Some bioinformatics tools have also been developed for Hadoop ecosystems <xref ref-type="bibr" rid="pone.0098146-Nordberg1">[10]</xref>, <xref ref-type="bibr" rid="pone.0098146-Schumacher1">[11]</xref>, <xref ref-type="bibr" rid="pone.0098146-Jourdren1">[12]</xref>. However, usability remains one of the main obstacles for cloud computing <xref ref-type="bibr" rid="pone.0098146-Schatz2">[13]</xref>. The prerequisite procedures of running MapReduce programs, including deploying Hadoop environments on computer clusters and executing programs through a series of technical Hadoop commands, pose considerable challenges for biological research laboratories that are interested in using MapReduce.</p>
      <p>Several existing approaches have been developed in an attempt to ease the burden of deploying and managing a Hadoop cloud. The hicloud-hadoop <xref ref-type="bibr" rid="pone.0098146-hicloudhadoop1">[14]</xref> open-source project focuses on automatically deploying a Hadoop environment on Hinet hicloud <xref ref-type="bibr" rid="pone.0098146-Hinet1">[15]</xref>. Apache Whirr <xref ref-type="bibr" rid="pone.0098146-Apache1">[16]</xref> provides a unified application programming interface for users to initiate cloud services from providers, e.g., Amazon EC2 <xref ref-type="bibr" rid="pone.0098146-Amazon1">[17]</xref> and Rackspace Cloud Servers <xref ref-type="bibr" rid="pone.0098146-Rackspace1">[18]</xref>. Amazon EMR <xref ref-type="bibr" rid="pone.0098146-Amazon2">[19]</xref> is a well-known service for renting MapReduce computing resources on demand. Puppet <xref ref-type="bibr" rid="pone.0098146-Puppet1">[20]</xref> is designed as an automation software that aids system administrators in managing and quickly deploying critical applications on large-scale servers. Cloudera Manager <xref ref-type="bibr" rid="pone.0098146-Cloudera1">[21]</xref> is targeted for deploying Hadoop ecosystems for enterprise-class requirements, including additional enterprise management components and security enhancement packages. Apache Ambari <xref ref-type="bibr" rid="pone.0098146-Apache2">[22]</xref> is designed to simplify Hadoop management. Although these tools and services are useful, some common functionalities required for using Hadoop computing clouds, hereafter referred to as Hadoop clouds, are not user-friendly for scientists without computer science expertise and relevant technical skills. Such examples include constructing a Hadoop cloud on idle computers of a laboratory and integrating bioinformatics MapReduce tools for a Hadoop cloud or users.</p>
      <p>In this study, we present CloudDOE, a software package for deploying an on-demand computing cloud with minimal user intervention. CloudDOE integrates available MapReduce programs within a unified graphical interface, and extends their functions with the addition of new MapReduce programs. In addition, smart features are included in CloudDOE, e.g., an auto-configuring algorithm of the Deploy wizard and an isolation method of the Operate wizard. CloudDOE encapsulates the complicated and niggling procedures of manipulating a Hadoop cloud, and is hence suitable for users of MapReduce cloud computing tools.</p>
   </sec>
   <sec id="s2">
      <title>Results</title>
      <p>CloudDOE aims at providing an open and integrated platform for biology/bioinformatics laboratories seeking to analyze big data via cloud computing with Hadoop/MapReduce (<xref ref-type="fig" rid="pone-0098146-g001">Figure 1</xref>). CloudDOE provides straightforward and user-friendly graphical interfaces, and covers most of the complicated, technical, and difficult command-line operations a user may encounter in traditional approaches (<xref ref-type="fig" rid="pone-0098146-g002">Figure 2</xref>). Several MapReduce programs are currently integrated into CloudDOE (<xref ref-type="table" rid="pone-0098146-t001">Table 1</xref>). Since CloudDOE is implemented in Java, users can run it on various operating systems, e.g., Windows, Linux, and Mac OS, with Java runtime environments installed. Prerequisites of CloudDOE are provided in Supplementary section 1 of File S1.</p>
      <fig id="pone-0098146-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0098146.g001</object-id><label>Figure 1</label>
         <caption>
            <title>Software solutions of CloudDOE.</title>
            <p>A user can deploy a Hadoop Cloud, operate the supported bioinformatics MapReduce programs, and extend cloud functions through installing new tools.</p>
         </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0098146.g001" position="float" xlink:type="simple"/></fig>
      <fig id="pone-0098146-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0098146.g002</object-id><label>Figure 2</label>
         <caption>
            <title>Comparison of CloudDOE and traditional approaches.</title>
            <p>CloudDOE encapsulates complicated procedures of traditional approaches into graphical user-friendly interfaces. Nearly 50% of the manipulating steps are reduced compared to traditional approaches.</p>
         </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0098146.g002" position="float" xlink:type="simple"/></fig>
      <table-wrap id="pone-0098146-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0098146.t001</object-id><label>Table 1</label>
         <caption>
            <title>Currently integrated MapReduce programs.</title>
         </caption><alternatives><graphic id="pone-0098146-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0098146.t001" xlink:type="simple"/><table>
            <colgroup span="1">
               <col align="left" span="1"/>
               <col align="center" span="1"/>
            </colgroup>
            <thead>
               <tr>
                  <td align="left" rowspan="1" colspan="1">Name</td>
                  <td align="left" rowspan="1" colspan="1">Description</td>
               </tr>
            </thead>
            <tbody>
               <tr>
                  <td align="left" rowspan="1" colspan="1">CloudBurst</td>
                  <td align="left" rowspan="1" colspan="1">Highly sensitive short read mapping.</td>
               </tr>
               <tr>
                  <td align="left" rowspan="1" colspan="1">CloudBrush</td>
                  <td align="left" rowspan="1" colspan="1">A <italic>de novo</italic> genome assembler.</td>
               </tr>
               <tr>
                  <td align="left" rowspan="1" colspan="1">CloudRS</td>
                  <td align="left" rowspan="1" colspan="1">An error corrector of substitution sequencing.</td>
               </tr>
               <tr>
                  <td align="left" rowspan="1" colspan="1">Hadoop-Examples</td>
                  <td align="left" rowspan="1" colspan="1">Hadoop example applications, including WordCount and Grep programs. The streaming mode of WordCount program is also available.</td>
               </tr>
            </tbody>
         </table></alternatives>
      </table-wrap>
      <sec id="s2a">
         <title>Deploying a Hadoop Computing Cloud</title>
         <p>The Hadoop cloud deployment procedure involves installing runtime environments and configuring system parameters. A Java runtime environment and Hadoop distributions are the basic requirements for constructing a Hadoop cloud. To improve usability and to simplify the installation processes, we developed Deploy wizard, which guides users to build their own Hadoop cloud in only three steps. Users simply need to provide user credentials and network connection settings of each computer upon installation of the cloud. Thus, the otherwise complicated installation procedure is completed automatically.</p>
         <p>Configuring a functional Hadoop cloud requires a computer science background and relevant operating skills, since improper parameter settings may affect cloud performance and incorrect system settings may lead to a malfunctioning system. To minimize the complexity of configuring a Hadoop cloud, we designed an auto-configuring algorithm in the Deploy wizard. The algorithm generates Secure Shell (SSH) certificates for internal communication and a set of cloud settings. This information is stored in files distributed to the cloud nodes as well as in the local PC for further use, e.g., for modifying cloud settings and re-deploying the cloud.</p>
         <p>A Hadoop cloud consists of a master node and multiple slave nodes (<xref ref-type="fig" rid="pone-0098146-g003">Figure 3A</xref>). A user is prompted to fill in the IP address and user account/password for the master node (<xref ref-type="fig" rid="pone-0098146-g003">Figure 3B</xref>) and each slave node (<xref ref-type="fig" rid="pone-0098146-g003">Figure 3C</xref>). The deployment process often takes 10–15 minutes (<xref ref-type="fig" rid="pone-0098146-g003">Figure 3D</xref>). Users can also undeploy a Hadoop cloud installed by CloudDOE, and restore configurations using the uninstallation function of Deploy wizard. To understand the process quickly, users can watch the supplementary video of Deploy wizard for step-by-step instructions and useful tips (File S2).</p>
         <fig id="pone-0098146-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0098146.g003</object-id><label>Figure 3</label>
            <caption>
               <title>Screenshots of Deploy wizard.</title>
               <p>(A) Brief instructions to explain the system requirements and procedures that Deploy wizard will perform. A user is prompted (B) to provide information of the connection between the local PC and the Hadoop cloud and (C) to set up information of the Hadoop cloud, including IP addresses and a username/password. (D) Settings and configurations of the target cloud are generated automatically. The installation progress and logs can also be monitored on the wizard.</p>
            </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0098146.g003" position="float" xlink:type="simple"/></fig>
         <p>In addition, CloudDOE is applicable for use in multiple deployment environments, e.g., hybrid and private/public clouds. An in-depth discussion of Deploy wizard is provided in Supplementary section 2 of File S1, including a list of necessary service ports used by Hadoop services and CloudDOE (Table S1 in File S1), an example of simulated machine information of a hybrid cloud on Windows Azure <xref ref-type="bibr" rid="pone.0098146-Windows1">[23]</xref> (Figure S1 and Table S2 in File S1), and a list of files and directories affected during deployment (Table S3 in File S1). Advanced users can also download the development branches or manually change the configuration for deploying a Hadoop cloud with different Hadoop releases (Table S4 in File S1).</p>
      </sec>
      <sec id="s2b">
         <title>Operating with Existing MapReduce Programs</title>
         <p>Several NGS data analysis tools have been implemented on the MapReduce framework. To overcome the hurdle of manipulating a MapReduce program with complicated command-line interfaces, we proposed a graphical wizard dubbed Operate. Users can manipulate a program with customized interfaces generated from necessary information in a configuration file, which is composed by the program’s author or an advanced user (<xref ref-type="fig" rid="pone-0098146-g004">Figure 4</xref>). An isolation method is also introduced to create a dedicated workspace for storing experimental data, i.e., programs, input files, and experimental results, of each execution. With Operate wizard, users can benefit from (1) a graphical interface for the MapReduce program, (2) a streamlined method for manipulating input/output data and setting up program parameters, and (3) a status tracker and progress monitor for execution.</p>
         <fig id="pone-0098146-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0098146.g004</object-id><label>Figure 4</label>
            <caption>
               <title>A structured XML configuration file and the generated wizard.</title>
               <p>The configuration file contains a metadata section on general program information, a set of parameters and its default values that are necessary to execute the program, and sections on log files and result download methods. CloudDOE loads a configuration file and generates the specific wizard required.</p>
            </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0098146.g004" position="float" xlink:type="simple"/></fig>
         <p>The user can fill out or load the stored login information to log in to the Master node of a Hadoop cloud (<xref ref-type="fig" rid="pone-0098146-g005">Figure 5A</xref>). After a successful login, the user can upload data files to the Hadoop cloud (<xref ref-type="fig" rid="pone-0098146-g005">Figure 5B</xref>), select supported MapReduce programs, and specify parameters for execution (<xref ref-type="fig" rid="pone-0098146-g005">Figure 5C</xref>). We also designed two progress bars for monitoring the execution progress of the ongoing MapReduce step and the entire program. After the program execution is completed, the user can download experimental results to a local computer for further processing (<xref ref-type="fig" rid="pone-0098146-g005">Figure 5D</xref>). To understand the process quickly, users can watch the supplementary video of Operate wizard for step-by-step instruction and useful tips (File S3).</p>
         <fig id="pone-0098146-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0098146.g005</object-id><label>Figure 5</label>
            <caption>
               <title>Screenshots of Operate wizard.</title>
               <p>A user can (A) log in to their Hadoop cloud, (B) upload and manage input data, (C) configure program parameters, and thus submit and monitor an execution, and (D) download the results after execution is completed.</p>
            </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0098146.g005" position="float" xlink:type="simple"/></fig>
         <p>In addition, the tool-adding process of CloudDOE, which requires the MapReduce jar files and their configuration files in the same directory under the target Hadoop cloud, could only be carried out by advanced users. To further simplify the burden of adding tools to CloudDOE, we presented Extend wizard, which is an extension management center of a Hadoop cloud (Figure S2 in File S1). Note that the Extend wizard is currently a prototype, and detailed information is provided in Supplementary section 3 of File S1.</p>
      </sec>
   </sec>
   <sec id="s3">
      <title>Discussion</title>
      <p>Hadoop/MapReduce supports large-scale computing in a distributed parallel and robust manner, thus ushering in a new era of bioinformatics data analysis. More bioinformatics tools are adopting the Hadoop/MapReduce framework. However, there are only a few software packages that currently provide bottom-tier support of MapReduce applications for general audiences, including developers, system administrators, and users. We thus developed CloudDOE, which provides cross-platform and user-friendly graphical interfaces, allowing a wider user base to manipulate a Hadoop cloud.</p>
      <sec id="s3a">
         <title>Strengths and Limitations</title>
         <p>CloudDOE is suitable as a unified console to Hadoop clouds among various computing environments, e.g., an in-house private cloud or rented machines from public cloud providers. CloudDOE is also useful and applicable across different scenarios: (1) deploying a workable Hadoop cloud with the auto-configuring algorithm within three steps, (2) manipulating a supported MapReduce program with the isolation method, and (3) integrating a MapReduce program with the program configuration file.</p>
         <p>There are nonetheless several limitations of the current CloudDOE release. The auto-configuring algorithm is performed sequentially, and only supports Ubuntu Linux distribution. Program integration does not support constructing pipelines for multiple programs. The deploy function only supports deploying the most common Hadoop releases on machines without Hadoop-integrated environments (<xref ref-type="table" rid="pone-0098146-t002">Table 2</xref>). Note that the support of deploying Apache Hadoop version 2 is released as a development branch of CloudDOE.</p>
         <table-wrap id="pone-0098146-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0098146.t002</object-id><label>Table 2</label>
            <caption>
               <title>Supports of CloudDOE for various deployment environments.</title>
            </caption><alternatives><graphic id="pone-0098146-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0098146.t002" xlink:type="simple"/><table>
               <colgroup span="1">
                  <col align="left" span="1"/>
                  <col align="center" span="1"/>
                  <col align="center" span="1"/>
                  <col align="center" span="1"/>
                  <col align="center" span="1"/>
               </colgroup>
               <thead>
                  <tr>
                     <td colspan="2" align="left" rowspan="1">Deployment Environments</td>
                     <td align="left" rowspan="1" colspan="1"/>
                     <td align="left" rowspan="1" colspan="1"/>
                     <td align="left" rowspan="1" colspan="1"/>
                  </tr>
                  <tr>
                     <td align="left" rowspan="1" colspan="1">Category</td>
                     <td align="left" rowspan="1" colspan="1">Examples</td>
                     <td align="left" rowspan="1" colspan="1">Hadoop Installed?</td>
                     <td align="left" rowspan="1" colspan="1">Hadoop Configured?</td>
                     <td align="left" rowspan="1" colspan="1">Supported by CloudDOE<xref ref-type="table-fn" rid="nt101">#</xref>?</td>
                  </tr>
               </thead>
               <tbody>
                  <tr>
                     <td align="left" rowspan="1" colspan="1">Generic machines</td>
                     <td align="left" rowspan="1" colspan="1">PCs, servers, or VMs</td>
                     <td align="left" rowspan="1" colspan="1">No</td>
                     <td align="left" rowspan="1" colspan="1">No</td>
                     <td align="left" rowspan="1" colspan="1">Yes</td>
                  </tr>
                  <tr>
                     <td align="left" rowspan="1" colspan="1">Cloud providers</td>
                     <td align="left" rowspan="1" colspan="1">Amazon EC2</td>
                     <td align="left" rowspan="1" colspan="1">No</td>
                     <td align="left" rowspan="1" colspan="1">No</td>
                     <td align="left" rowspan="1" colspan="1">Yes</td>
                  </tr>
                  <tr>
                     <td align="left" rowspan="1" colspan="1"/>
                     <td align="left" rowspan="1" colspan="1">RackSpace</td>
                     <td align="left" rowspan="1" colspan="1"/>
                     <td align="left" rowspan="1" colspan="1"/>
                     <td align="left" rowspan="1" colspan="1"/>
                  </tr>
                  <tr>
                     <td align="left" rowspan="1" colspan="1"/>
                     <td align="left" rowspan="1" colspan="1">Microsoft Azure</td>
                     <td align="left" rowspan="1" colspan="1"/>
                     <td align="left" rowspan="1" colspan="1"/>
                     <td align="left" rowspan="1" colspan="1"/>
                  </tr>
                  <tr>
                     <td align="left" rowspan="1" colspan="1"/>
                     <td align="left" rowspan="1" colspan="1">Hinet hicloud</td>
                     <td align="left" rowspan="1" colspan="1"/>
                     <td align="left" rowspan="1" colspan="1"/>
                     <td align="left" rowspan="1" colspan="1"/>
                  </tr>
                  <tr>
                     <td align="left" rowspan="1" colspan="1">Hadoop-integrated environments</td>
                     <td align="left" rowspan="1" colspan="1">Cloud BioLinux</td>
                     <td align="left" rowspan="1" colspan="1">Yes</td>
                     <td align="left" rowspan="1" colspan="1">No</td>
                     <td align="left" rowspan="1" colspan="1">No</td>
                  </tr>
               </tbody>
            </table></alternatives>
            <table-wrap-foot>
               <fn id="nt101">
                  <label>#</label>
                  <p>CloudDOE supports deploying Apache Hadoop version 0.20.203 in the current release. The supports of deploying Apache Hadoop 1.2.1 and 2.2.0 are released as development branches.</p>
               </fn>
            </table-wrap-foot>
         </table-wrap>
      </sec>
      <sec id="s3b">
         <title>Comparison with Similar Deploying Tools</title>
         <p>Several existing projects aim at easing the burden of deploying and managing a Hadoop cloud. <xref ref-type="table" rid="pone-0098146-t003">Table 3</xref> shows a comparison of the main features of current projects. The hicloud-hadoop, Apache Whirr, and Puppet projects are based on command-line interface, whereas the Cloudera manger, Apache Ambari, and CloudDOE projects provide graphical user interfaces. Apache Whirr supports deploying a Hadoop cloud through composing proper deployment files, thus initiating machine instances from infrastructure-as-a-service providers. Puppet supplies functions for deploying, enhancing, and managing a Hadoop cloud through executing appropriate modules developed by experts. Cloudera Manager and Apache Ambari provide functions for manipulating a Hadoop cloud. However, computer science expertise is still necessary to accomplish technical operations, e.g., generate and exchange SSH key pairs and adapt system configuration files. CloudDOE presents functions for deploying and undeploying a Hadoop cloud for administrators, and encapsulates technical operations using wizards. It also supports the manipulation of available bioinformatics MapReduce programs on a Hadoop cloud for a bioinformatician.</p>
         <table-wrap id="pone-0098146-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0098146.t003</object-id><label>Table 3</label>
            <caption>
               <title>Features comparison of CloudDOE and similar deploying tools.</title>
            </caption><alternatives><graphic id="pone-0098146-t003-3" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0098146.t003" xlink:type="simple"/><table>
               <colgroup span="1">
                  <col align="left" span="1"/>
                  <col align="center" span="1"/>
                  <col align="center" span="1"/>
                  <col align="center" span="1"/>
                  <col align="center" span="1"/>
                  <col align="center" span="1"/>
               </colgroup>
               <thead>
                  <tr>
                     <td align="left" rowspan="1" colspan="1"/>
                     <td colspan="3" align="left" rowspan="1">Function<xref ref-type="table-fn" rid="nt102">#</xref></td>
                     <td align="left" rowspan="1" colspan="1"/>
                     <td align="left" rowspan="1" colspan="1"/>
                  </tr>
                  <tr>
                     <td align="left" rowspan="1" colspan="1">Project</td>
                     <td align="left" rowspan="1" colspan="1">Deploy</td>
                     <td align="left" rowspan="1" colspan="1">Operate</td>
                     <td align="left" rowspan="1" colspan="1">Extend</td>
                     <td align="left" rowspan="1" colspan="1">User Interface<xref ref-type="table-fn" rid="nt102">#</xref></td>
                     <td align="left" rowspan="1" colspan="1">IaaS Only?<xref ref-type="table-fn" rid="nt102">#</xref></td>
                  </tr>
               </thead>
               <tbody>
                  <tr>
                     <td align="left" rowspan="1" colspan="1">hicloud-hadoop</td>
                     <td align="left" rowspan="1" colspan="1">√</td>
                     <td align="left" rowspan="1" colspan="1">N/A</td>
                     <td align="left" rowspan="1" colspan="1">N/A</td>
                     <td align="left" rowspan="1" colspan="1">CLI</td>
                     <td align="left" rowspan="1" colspan="1">No</td>
                  </tr>
                  <tr>
                     <td align="left" rowspan="1" colspan="1">Apache Whirr</td>
                     <td align="left" rowspan="1" colspan="1">√</td>
                     <td align="left" rowspan="1" colspan="1">N/A</td>
                     <td align="left" rowspan="1" colspan="1">N/A</td>
                     <td align="left" rowspan="1" colspan="1">CLI</td>
                     <td align="left" rowspan="1" colspan="1">Yes</td>
                  </tr>
                  <tr>
                     <td align="left" rowspan="1" colspan="1">Puppet</td>
                     <td align="left" rowspan="1" colspan="1">√</td>
                     <td align="left" rowspan="1" colspan="1">N/A</td>
                     <td align="left" rowspan="1" colspan="1">√</td>
                     <td align="left" rowspan="1" colspan="1">CLI</td>
                     <td align="left" rowspan="1" colspan="1">No</td>
                  </tr>
                  <tr>
                     <td align="left" rowspan="1" colspan="1">Cloudera Manager</td>
                     <td align="left" rowspan="1" colspan="1">√</td>
                     <td align="left" rowspan="1" colspan="1">N/A</td>
                     <td align="left" rowspan="1" colspan="1">√</td>
                     <td align="left" rowspan="1" colspan="1">GUI (Web)</td>
                     <td align="left" rowspan="1" colspan="1">No</td>
                  </tr>
                  <tr>
                     <td align="left" rowspan="1" colspan="1">Apache Ambari</td>
                     <td align="left" rowspan="1" colspan="1">√</td>
                     <td align="left" rowspan="1" colspan="1">N/A</td>
                     <td align="left" rowspan="1" colspan="1">√</td>
                     <td align="left" rowspan="1" colspan="1">GUI (Web)</td>
                     <td align="left" rowspan="1" colspan="1">No</td>
                  </tr>
                  <tr>
                     <td align="left" rowspan="1" colspan="1">CloudDOE</td>
                     <td align="left" rowspan="1" colspan="1">√</td>
                     <td align="left" rowspan="1" colspan="1">√</td>
                     <td align="left" rowspan="1" colspan="1">√</td>
                     <td align="left" rowspan="1" colspan="1">GUI (Java)</td>
                     <td align="left" rowspan="1" colspan="1">No</td>
                  </tr>
               </tbody>
            </table></alternatives>
            <table-wrap-foot>
               <fn id="nt102">
                  <label>#</label>
                  <p>N/A, not available in the current release of the project; IaaS, infrastructure-as-a-service; CLI, command-line interface; GUI, graphical-user interface.</p>
               </fn>
            </table-wrap-foot>
         </table-wrap>
      </sec>
      <sec id="s3c">
         <title>Future Work</title>
         <p>Parallel processing utilizes non-blocking operations and job overlapping to reduce waiting latency, and has been applied to different situations. We would like to accelerate deploying progress by introducing a parallel dispatcher and a monitor mechanism in future CloudDOE releases. One of the most successful characteristics of the existing workflow or execution platforms is the ability for users to construct analysis pipelines from available programs. Thus, incorporating the MapReduce programs into a pipeline with stand-alone programs to replace time-consuming processes is a promising future direction. We plan to implement wrapper functions or tools to integrate the MapReduce programs into workflows of existing bioinformatics platforms, e.g., Galaxy <xref ref-type="bibr" rid="pone.0098146-Goecks1">[24]</xref>. To enhance and keep up with technology trends, we plan to support state-of-the-art big data computing platforms, e.g., Hadoop BigTop <xref ref-type="bibr" rid="pone.0098146-Apache3">[25]</xref> and Spark <xref ref-type="bibr" rid="pone.0098146-Zaharia1">[26]</xref>. We also welcome community efforts to collaborate in future developments and in the maintenance of CloudDOE for integrating more MapReduce bioinformatics tools, providing multivariate deploying environment support, e.g., Cloud BioLinux <xref ref-type="bibr" rid="pone.0098146-Krampis1">[27]</xref>, and supporting next-generation big data open source tools.</p>
      </sec>
   </sec>
   <sec id="s4">
      <title>Conclusion</title>
      <p>We have presented CloudDOE, a software package with user-friendly graphical wizards. CloudDOE supports users without an advanced computer science background in manipulating a Hadoop cloud, and thus reduces operation costs by encapsulating technical details and niggling command-line processes. CloudDOE also improves the usability of existing bioinformatics MapReduce programs by integrating these programs into a unified graphical user interface. We have also demonstrated that CloudDOE is useful and applicable for different scenarios and targeted users, including ordinary users, developers, and administrators. CloudDOE is an open-source project distributed under Apache License 2.0 and is freely available online.</p>
   </sec>
   <sec id="s5" sec-type="materials|methods">
      <title>Materials and Methods</title>
      <p>To operate a Hadoop cloud remotely, we employed the client-server model as the system architecture of CloudDOE. Client-side applications were developed by Java and encapsulated as Java archive (JAR) executable files designed to be executed smoothly across different operating systems and environments, e.g., Windows, Linux, and Mac OS. Server-side deploying agents were written in GNU Bourne-Again Shell (BASH) script language because of its flexibility and good support for most Linux distributions. <xref ref-type="fig" rid="pone-0098146-g006">Figure 6</xref> shows the system architecture of CloudDOE. Further details about the interaction of CloudDOE and a Hadoop cloud are provided in Supplementary section 4 of File S1, including the Deploy, Extend (Figure S3 in File S1), and Operate (Figure S4 in File S1) wizards.</p>
      <fig id="pone-0098146-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0098146.g006</object-id><label>Figure 6</label>
         <caption>
            <title>System architecture of CloudDOE.</title>
            <p>The solid square represents a machine or a computing resource, and the gray solid square is the master of the Hadoop cloud. CloudDOE establishes Secure Shell (SSH) channels for communication and acquires local resources for operations.</p>
         </caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0098146.g006" position="float" xlink:type="simple"/></fig>
      <p>Communications between clients and the server were conducted through SSH channels in a reliable and secure manner. SSH is a cryptographic network protocol that aims to communicate securely within an insecure network environment. We developed SSHadoop, a library inherited from JSch <xref ref-type="bibr" rid="pone.0098146-JSch1">[28]</xref>, to establish secure communication channels and execute commands. It also enables the ability to complete basic demands and operations of CloudDOE, including remote program execution and job monitoring, data import and management, and downloading of experimental results.</p>
      <p>An auto-configuring algorithm was a major component necessary for deploying a Hadoop cloud from CloudDOE. Runtime environments and dependent libraries were installed through our server-side agents, currently applied to the Ubuntu Linux distribution. A set of security credentials (e.g., SSH key pair) was generated for internal usage, e.g., communication and services control, for configuring a Hadoop cloud. Moreover, pre-formatting cloud settings were also produced and applied globally. The configuration files contain role types of a Hadoop cloud in each computer (i.e., master or slave), a number of data replicas and relevant configurations of Hadoop Distributed File System, and operating system-related settings.</p>
      <p>A unique isolation identifier (IID) was the core concept of the isolation method, which is aimed at constructing independent workspaces and distinguishing the operation scope of executions. An IID is composed of a magic number and a user identifier, i.e., a timestamp followed by the current username. It is generated and applied to Hadoop cloud the first time an integrated program is initiated. We also exploited the IID to implement a stateful interaction environment, e.g., execution status recovery, to improve the reliability of connection and usability.</p>
      <p>A structured extensible markup language (XML) configuration file was utilized to integrate a MapReduce program into CloudDOE. This XML file is composed of various information blocks, i.e., program, parameters, logs, and downloads (Figure S5 in File S1). The <italic>program</italic> block expresses general information of the program. In the <italic>parameters</italic> block, parameters and their default values are defined. The <italic>logs</italic> block lists a program log file provided by authors that can be used to monitor program execution. Output files and their corresponding download methods are defined in the <italic>downloads</italic> block. Detailed information of each configuration field is given in Supplementary section 5 of File S1 for interested users.</p>
   </sec>
   <sec id="s6">
      <title>Supporting Information</title>
<supplementary-material id="pone.0098146.s001" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pone.0098146.s001" position="float" xlink:type="simple">
<label>File S1</label>
<caption><p><bold>Supplementary information, figures, and tables.</bold> Figure S1. A Hadoop cloud environment simulated from real Microsoft Azure machine data. Figure S2. Screenshots of Extend wizard. Figure S3. Interactions between CloudDOE, Hadoop cloud and Internet when manipulating a Hadoop cloud with Deploy or Extend function. Figure S4. Interactions between CloudDOE and Hadoop cloud when manipulating a Hadoop cloud with Operate function. Figure S5. Format of the program integration configuration file of CloudDOE.</p>
      <p>(PDF)</p>
</caption></supplementary-material>
<supplementary-material id="pone.0098146.s002" mimetype="video/mp4" xlink:href="info:doi/10.1371/journal.pone.0098146.s002" position="float" xlink:type="simple">
<label>File S2</label>
<caption><p><bold>A step-by-step video of Deploy wizard with useful tips.</bold></p>
      <p>(MP4)</p>
</caption></supplementary-material>
<supplementary-material id="pone.0098146.s003" mimetype="video/mp4" xlink:href="info:doi/10.1371/journal.pone.0098146.s003" position="float" xlink:type="simple">
<label>File S3</label>
<caption><p><bold>A step-by-step video of Operate wizard with useful tips.</bold></p>
      <p>(MP4)</p>
</caption></supplementary-material>
   </sec>
</body>
<back>
   <ack>
      <p>The authors wish to thank anonymous reviewers, Dr. Laurent Jourdren, Dr. Christophe Antoniewski, Jazz Yao-Tsung Wang (National Center for High-Performance Computing, Taiwan), Dr. Laurent H. Lin and Jen-Hao Cheng (Institute of Information Science, Academia Sinica, Taiwan) for their help, suggestions, and valuable comments. They also wish to thank Dr. Dennis Gannon and Dr. Nien-Chen Liu from Microsoft Co. on World Cloud Research Collaboration Projects for helping this research through the project to Chung-Yen Lin.</p>
   </ack>
   <ref-list>
      <title>References</title>
      <ref id="pone.0098146-Zou1"><label>1</label><mixed-citation publication-type="other" xlink:type="simple">Zou Q, Li XB, Jiang WR, Lin ZY, Li GL, <etal>et al</etal>.. (2013) Survey of MapReduce frame operation in bioinformatics. Brief Bioinform.</mixed-citation></ref>
      <ref id="pone.0098146-Dean1"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dean</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Ghemawat</surname><given-names>S</given-names></name> (<year>2008</year>) <article-title>MapReduce: simplified data processing on large clusters</article-title>. <source>Commun ACM</source> <volume>51</volume>: <fpage>107</fpage>–<lpage>113</lpage>.</mixed-citation></ref>
      <ref id="pone.0098146-Welcome1"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Welcome to Apache Hadoop! Available: <ext-link ext-link-type="uri" xlink:href="http://hadoop.apache.org/" xlink:type="simple">http://hadoop.apache.org/</ext-link>.Accessed 2014 May 5.</mixed-citation></ref>
      <ref id="pone.0098146-Taylor1"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Taylor</surname><given-names>RC</given-names></name> (<year>2010</year>) <article-title>An overview of the Hadoop/MapReduce/HBase framework and its current applications in bioinformatics</article-title>. <source>BMC Bioinformatics</source> <volume>11</volume> Suppl 12<fpage>S1</fpage>.</mixed-citation></ref>
      <ref id="pone.0098146-Schatz1"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schatz</surname><given-names>MC</given-names></name> (<year>2009</year>) <article-title>CloudBurst: highly sensitive read mapping with MapReduce</article-title>. <source>Bioinformatics</source> <volume>25</volume>: <fpage>1363</fpage>–<lpage>1369</lpage>.</mixed-citation></ref>
      <ref id="pone.0098146-Langmead1"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Langmead</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Hansen</surname><given-names>KD</given-names></name>, <name name-style="western"><surname>Leek</surname><given-names>JT</given-names></name> (<year>2010</year>) <article-title>Cloud-scale RNA-sequencing differential expression analysis with Myrna</article-title>. <source>Genome Biol</source> <volume>11</volume>: <fpage>R83</fpage>.</mixed-citation></ref>
      <ref id="pone.0098146-Langmead2"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Langmead</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Schatz</surname><given-names>MC</given-names></name>, <name name-style="western"><surname>Lin</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Pop</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Salzberg</surname><given-names>SL</given-names></name> (<year>2009</year>) <article-title>Searching for SNPs with cloud computing</article-title>. <source>Genome Biol</source> <volume>10</volume>: <fpage>R134</fpage>.</mixed-citation></ref>
      <ref id="pone.0098146-Chang1"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chang</surname><given-names>YJ</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>CC</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>CL</given-names></name>, <name name-style="western"><surname>Ho</surname><given-names>JM</given-names></name> (<year>2012</year>) <article-title>A de novo next generation genomic sequence assembler based on string graph and MapReduce cloud computing framework</article-title>. <source>BMC Genomics</source> <volume>13</volume>: <fpage>1</fpage>–<lpage>17</lpage>.</mixed-citation></ref>
      <ref id="pone.0098146-Chen1"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chen</surname><given-names>CC</given-names></name>, <name name-style="western"><surname>Chang</surname><given-names>YJ</given-names></name>, <name name-style="western"><surname>Chung</surname><given-names>WC</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>DT</given-names></name>, <name name-style="western"><surname>Ho</surname><given-names>JM</given-names></name> (<year>2013</year>) <article-title>CloudRS: An error correction algorithm of high-throughput sequencing data based on scalable framework. 6–9 Oct</article-title>. <volume>2013</volume>: <fpage>717</fpage>–<lpage>722</lpage>.</mixed-citation></ref>
      <ref id="pone.0098146-Nordberg1"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nordberg</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Bhatia</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>Z</given-names></name> (<year>2013</year>) <article-title>BioPig: a Hadoop-based analytic toolkit for large-scale sequence data</article-title>. <source>Bioinformatics</source> <volume>29</volume>: <fpage>3014</fpage>–<lpage>3019</lpage>.</mixed-citation></ref>
      <ref id="pone.0098146-Schumacher1"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schumacher</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Pireddu</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Niemenmaa</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Kallio</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Korpelainen</surname><given-names>E</given-names></name>, <etal>et al</etal>. (<year>2014</year>) <article-title>SeqPig: simple and scalable scripting for large sequencing data sets in Hadoop</article-title>. <source>Bioinformatics</source> <volume>30</volume>: <fpage>119</fpage>–<lpage>120</lpage>.</mixed-citation></ref>
      <ref id="pone.0098146-Jourdren1"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jourdren</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Bernard</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Dillies</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Le Crom</surname><given-names>S</given-names></name> (<year>2012</year>) <article-title>Eoulsan: a cloud computing-based framework facilitating high throughput sequencing analyses</article-title>. <source>Bioinformatics</source> <volume>28</volume>: <fpage>1542</fpage>–<lpage>1543</lpage>.</mixed-citation></ref>
      <ref id="pone.0098146-Schatz2"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schatz</surname><given-names>MC</given-names></name>, <name name-style="western"><surname>Langmead</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Salzberg</surname><given-names>SL</given-names></name> (<year>2010</year>) <article-title>Cloud computing and the DNA data race</article-title>. <source>Nat Biotechnol</source> <volume>28</volume>: <fpage>691</fpage>–<lpage>693</lpage>.</mixed-citation></ref>
      <ref id="pone.0098146-hicloudhadoop1"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">hicloud-hadoop. Available: <ext-link ext-link-type="uri" xlink:href="https://github.com/jazzwang/hicloud-hadoop" xlink:type="simple">https://github.com/jazzwang/hicloud-hadoop</ext-link>. Accessed 2014 May 5.</mixed-citation></ref>
      <ref id="pone.0098146-Hinet1"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Hinet hicloud. Available: <ext-link ext-link-type="uri" xlink:href="http://hicloud.hinet.net/" xlink:type="simple">http://hicloud.hinet.net/</ext-link>. Accessed 2014 May 5.</mixed-citation></ref>
      <ref id="pone.0098146-Apache1"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Apache Whirr. Available: <ext-link ext-link-type="uri" xlink:href="http://whirr.apache.org/" xlink:type="simple">http://whirr.apache.org/</ext-link>. Accessed 2014 May 5.</mixed-citation></ref>
      <ref id="pone.0098146-Amazon1"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Amazon Elastic Compute Cloud (Amazon EC2). Available: <ext-link ext-link-type="uri" xlink:href="http://aws.amazon.com/ec2/" xlink:type="simple">http://aws.amazon.com/ec2/</ext-link>. Accessed 2014 May 5.</mixed-citation></ref>
      <ref id="pone.0098146-Rackspace1"><label>18</label><mixed-citation publication-type="other" xlink:type="simple">Rackspace Cloud Servers. Available: <ext-link ext-link-type="uri" xlink:href="http://www.rackspace.com/cloud/servers/" xlink:type="simple">http://www.rackspace.com/cloud/servers/</ext-link>. Accessed 2014 May 5.</mixed-citation></ref>
      <ref id="pone.0098146-Amazon2"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Amazon Elastic MapReduce (Amazon EMR). Available: <ext-link ext-link-type="uri" xlink:href="http://aws.amazon.com/elasticmapreduce/" xlink:type="simple">http://aws.amazon.com/elasticmapreduce/</ext-link>. Accessed 2014 May 5.</mixed-citation></ref>
      <ref id="pone.0098146-Puppet1"><label>20</label><mixed-citation publication-type="other" xlink:type="simple">Puppet Open Source. Available: <ext-link ext-link-type="uri" xlink:href="http://puppetlabs.com/puppet/puppet-open-source" xlink:type="simple">http://puppetlabs.com/puppet/puppet-open-source</ext-link>. Accessed 2014 May 5.</mixed-citation></ref>
      <ref id="pone.0098146-Cloudera1"><label>21</label><mixed-citation publication-type="other" xlink:type="simple">Cloudera Manager. Available: <ext-link ext-link-type="uri" xlink:href="http://www.cloudera.com/content/cloudera/en/products-and-services/cloudera-enterprise/cloudera-manager.html" xlink:type="simple">http://www.cloudera.com/content/cloudera/en/products-and-services/cloudera-enterprise/cloudera-manager.html</ext-link>. Accessed 2014 May 5.</mixed-citation></ref>
      <ref id="pone.0098146-Apache2"><label>22</label><mixed-citation publication-type="other" xlink:type="simple">Apache Ambari. Available: <ext-link ext-link-type="uri" xlink:href="http://ambari.apache.org/" xlink:type="simple">http://ambari.apache.org/</ext-link>. Accessed 2014 May 5.</mixed-citation></ref>
      <ref id="pone.0098146-Windows1"><label>23</label><mixed-citation publication-type="other" xlink:type="simple">Windows Azure. Available: <ext-link ext-link-type="uri" xlink:href="http://www.windowsazure.com/" xlink:type="simple">http://www.windowsazure.com/</ext-link>. Accessed 2014 May 5.</mixed-citation></ref>
      <ref id="pone.0098146-Goecks1"><label>24</label><mixed-citation publication-type="other" xlink:type="simple">Goecks J, Nekrutenko A, Taylor J, Team G (2010) Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences. Genome Biology 11.</mixed-citation></ref>
      <ref id="pone.0098146-Apache3"><label>25</label><mixed-citation publication-type="other" xlink:type="simple">Apache Bigtop. Available: <ext-link ext-link-type="uri" xlink:href="http://bigtop.apache.org/" xlink:type="simple">http://bigtop.apache.org/</ext-link>. Accessed 2014 May 5.</mixed-citation></ref>
      <ref id="pone.0098146-Zaharia1"><label>26</label><mixed-citation publication-type="other" xlink:type="simple">Zaharia M, Chowdhury M, Das T, Dave A, Ma J, <etal>et al</etal>.. (2012) Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing. Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation. San Jose, CA: USENIX Association. 2–2.</mixed-citation></ref>
      <ref id="pone.0098146-Krampis1"><label>27</label><mixed-citation publication-type="other" xlink:type="simple">Krampis K, Booth T, Chapman B, Tiwari B, Bicak M, <etal>et al</etal>.. (2012) Cloud BioLinux: pre-configured and on-demand bioinformatics computing for the genomics community. Bmc Bioinformatics 13.</mixed-citation></ref>
      <ref id="pone.0098146-JSch1"><label>28</label><mixed-citation publication-type="other" xlink:type="simple">JSch - Java Secure Channel. Available: <ext-link ext-link-type="uri" xlink:href="http://www.jcraft.com/jsch/" xlink:type="simple">http://www.jcraft.com/jsch/</ext-link>. Accessed 2014 May 5.</mixed-citation></ref>
   </ref-list>
</back>
</article>