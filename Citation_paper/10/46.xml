<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group>
<journal-title>PLoS ONE</journal-title></journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-13-41023</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0095576</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Computer and information sciences</subject><subj-group><subject>Geoinformatics</subject><subj-group><subject>Remote sensing imagery</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Earth sciences</subject><subj-group><subject>Geography</subject><subj-group><subject>Cartography</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Engineering and technology</subject><subj-group><subject>Aerospace engineering</subject><subj-group><subject>Avionics</subject><subj-group><subject>Synthetic vision systems</subject></subj-group></subj-group></subj-group><subj-group><subject>Signal processing</subject><subj-group><subject>Image processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Social sciences</subject></subj-group></article-categories>
<title-group>
<article-title>Image Registration Algorithm Using Mexican Hat Function-Based Operator and Grouped Feature Matching Strategy</article-title>
<alt-title alt-title-type="running-head">Novel Image Registration Algorithm</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Jin</surname><given-names>Feng</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Feng</surname><given-names>Dazheng</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>School of Computer Science and Technology, Xidian University, Xi’an, China</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>National Key Laboratory of Science and Technology on Radar Signal Processing, Xidian University, Xi’an, China</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Gómez</surname><given-names>Sergio</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Universitat Rovira i Virgili, Spain</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">xdjinfeng@163.com</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: FJ DF. Performed the experiments: FJ. Analyzed the data: FJ DF. Wrote the paper: FJ. Commented on the manuscript: DF.</p></fn>
</author-notes>
<pub-date pub-type="collection"><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>21</day><month>4</month><year>2014</year></pub-date>
<volume>9</volume>
<issue>4</issue>
<elocation-id>e95576</elocation-id>
<history>
<date date-type="received"><day>7</day><month>10</month><year>2013</year></date>
<date date-type="accepted"><day>27</day><month>3</month><year>2014</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Jin, Feng</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Feature detection and matching are crucial for robust and reliable image registration. Although many methods have been developed, they commonly focus on only one class of image features. The methods that combine two or more classes of features are still novel and significant. In this work, methods for feature detection and matching are proposed. A Mexican hat function-based operator is used for image feature detection, including the local area detection and the feature point detection. For the local area detection, we use the Mexican hat operator for image filtering, and then the zero-crossing points are extracted and merged into the area borders. For the feature point detection, the Mexican hat operator is performed in scale space to get the key points. After the feature detection, an image registration is achieved by using the two classes of image features. The feature points are grouped according to a standardized region that contains correspondence to the local area, precise registration is achieved eventually by the grouped points. An image transformation matrix is estimated by the feature points in a region and then the best one is chosen through competition of a set of the transformation matrices. This strategy has been named the Grouped Sample Consensus (GCS). The GCS has also ability for removing the outliers effectively. The experimental results show that the proposed algorithm has high registration accuracy and small computational volume.</p>
</abstract>
<funding-group><funding-statement>This study is supported by the National Natural Science Foundation of China. The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="9"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<sec id="s1a">
<title>A. Background</title>
<p>Image registration is the process of matching and fusion of multiple images taken from the same scene, at different times, by different sensors, and from different perspectives <xref ref-type="bibr" rid="pone.0095576-Zitova1">[1]</xref>. It is a hot spot on the computer vision, pattern recognition, medical image processing and remote sensing data processing. Image registration is widely used in the multi-source remote sensing data integration and analysis, motion tracking of small target under complex scenes, matching of landscape and map, image stitching and topographic height reconstruction. Currently, in a wide range of applications of image registration, ones often adopt methods based on the image feature extraction.</p>
<p>According to different classes of image features, the methods can be divided into the area-based and the point-based. A classical local area-based method is a combination of chain code and invariant moment proposed by Dai and Khorram <xref ref-type="bibr" rid="pone.0095576-Dai1">[2]</xref>. The improved Laplacian of Gaussian (LoG) operator is used for the extraction of the area contours, and the contours are further described by the chain code.</p>
<p>The feature points are also called the interesting points or key-points. The feature point-based methods are widely used, such as the scale invariant feature transform (SIFT) operator, proposed by Lowe <xref ref-type="bibr" rid="pone.0095576-Lowe1">[3]</xref>, and the Harris-Laplace operator which is the improved Harris operator with scale invariance proposed by Mikolajczyk and Schmid <xref ref-type="bibr" rid="pone.0095576-Mikolajczyk1">[4]</xref>. These two operators defined in the scale space are the most classical application of the Gaussian kernel filter.</p>
<p>These two classes of methods, however, both have inherent shortcomings that need to be dealt with. The performances of the local area-based methods are highly influenced by the accuracy of the LoG operator, and they behave when the shape of objects is seriously changed in the matching images. For example, the fields or lakes frequently change their area along with the time lapses. The even worse is that it cannot provide sufficient features to support the registration of images with complex texture or perform the 3-D object reconstruction. On the other hand, the point-based methods have higher accuracy, their ability for differentiating and localizing the points depend on the complex description of the point properties. For example, the SIFT operator describes each feature point with a 128-D vector.</p>
</sec><sec id="s1b">
<title>B. Literature Review</title>
<p>The SIFT and Harris-Laplace operator are the most classical methods of scale invariant points detection and matching. They are based on the theory of scale-space analysis. There are many other algorithms developed in the theory and techniques, such as the speedup robust features <xref ref-type="bibr" rid="pone.0095576-Bay1">[5]</xref> (SURF) and the PCA-SIFT <xref ref-type="bibr" rid="pone.0095576-Yang1">[6]</xref>. Recently, Padmavathi, Muthukumar and Thakur <xref ref-type="bibr" rid="pone.0095576-Padmavathi1">[7]</xref> proposed a method by combining the Kernel PCA (KPCA) and SIFT together (called KPCA- SIFT feature detection) for underwater images. The method focuses on the approaches to the KPCA using reproduced kernels. Hence, KPCA is used for feature extraction and dimension reduction of SIFT. Cui and Ngan <xref ref-type="bibr" rid="pone.0095576-Cui1">[8]</xref> developed multiple fan sub regions named Fan features depict the image neighborhood of a key point. The Fan features are made scale-invariant by using the automatic scale selection method based on Fan Laplacian of Gaussian (FLOG).</p>
<p>Instead of feature point-based approach, Tuytlelaars and Gool <xref ref-type="bibr" rid="pone.0095576-Tuytelaars1">[9]</xref> defined and extracted an intensity-based, local affine invariant region that is independent of the presence of edges or corners in the image. Such regions are also applied in wide baseline stereo matching. Reference <xref ref-type="bibr" rid="pone.0095576-Matas1">[10]</xref> proposed an efficient and practically fast detection algorithm for detecting the maximally stable extremal regions (MSER). And then the invariants from multiple measurement regions are used to establish tentative correspondences. Kadir, Brady and Zisserman <xref ref-type="bibr" rid="pone.0095576-Kadir1">[11]</xref> developed a novel algorithm called Scale Saliency for quantifying image region saliency. In their approach, regions are considered salient if they are simultaneously unpredictable both in some feature and scale-space.</p>
<p>There is also a class of image registration algorithms based on the spatial relations or constraints among points, which is receiving much more attention. These methods are widely used in image classification, pattern recognition and object recognition. Reference <xref ref-type="bibr" rid="pone.0095576-Wang1">[12]</xref> provides a matching method which is to find the correspondence between groups of contour points. Two groups are considered to be matched when the two point sequences formed by the two groups lead to a perfect one-to-one mapping. Myronenko and Song <xref ref-type="bibr" rid="pone.0095576-Myronenko1">[13]</xref> introduce a probabilistic method, called the Coherent Point Drift (CPD) algorithm, for both rigid and non-rigid point set registration. The CPD consider the alignment of two point sets as a probability density estimation problem. These methods have very low dependence with image information and do not need a complex description of the feature points; they turn the process of points matching to be an iteration of the objective function optimization. The objective function is usually a function about image transformation matrix with lower errors or time expense, such as <xref ref-type="bibr" rid="pone.0095576-Chui1">[14]</xref> <xref ref-type="bibr" rid="pone.0095576-Fitzgibbon1">[15]</xref>.</p>
<p>Daubechies introduced a typical choice for wavelet function in her well-known textbook <xref ref-type="bibr" rid="pone.0095576-Daubechies1">[16]</xref>. This function is represented by the second derivative of Gaussian, and sometimes called the Mexican hat function because it resembles a cross section of a Mexican hat. We find that the Mexican hat function is a function that can get good performance in both area detection and point detection, because of the relationship between the Mexican hat function and the difference of Gaussian (DoG). For example, the well-known LoG operator which is introduced by Marr and Hildreth as an edge detector <xref ref-type="bibr" rid="pone.0095576-Marr1">[17]</xref> produces a circularly symmetric Mexican hat.The Mexican hat wavelet is used for feature detection by Yasein <xref ref-type="bibr" rid="pone.0095576-Yasein1">[18]</xref>.</p>
</sec><sec id="s1c">
<title>C. Motivation and Contribution</title>
<p>The distinct objects play a pivotal role in the image registration. A distinct object usually contains obvious features that can be used for image registration, including the edge of the object and the key points on the special positions (the corners, the endpoints and so on). In the area of a distinct object, there would be a set of usable image features, and the positional correspondence is maintained when the image changes happen. The local areas usually have a clear physical meaning which means that they usually represent some distinct objects that contain dense key points. A matching of a local area pair implies a matching of a point group pair. We use the method that combines the two classes of features for a precise registration. We find the distinguishing feature through local area detection. In the local area, a group of key points is used for the description and matching of distinct objects. The point group is much more reliable than the border of the local area because the feature point descriptor contains more invariable properties.</p>
<p>In addition, it is well-known that the computational complexity of many algorithms is the higher-order function of problem size or data scale. Thus, if a large problem is decomposed into the multiple small problems, the whole computational time for solving it will be reduced. More concretely speaking, in the registration of two (multiple) images, the large number of feature points or the high complexity of feature describer seriously affects the computational volume of a matching algorithm. Thus, our objective is to find such an effective strategy for decomposing feature points into several sets that the number of matching operations or the computational volume of a matching algorithm is significantly decreased in the cost of the slight loss of performance. Interestingly, the proposed grouped matching method achieves this objective by grouping the feature points into the local areas. In order to keep the performance of the proposed algorithm, a good transformation matrix is chosen by a strategy named grouped sample consensus (GCS).</p>
<p>There are two contributions in our work. The first major contribution is to give a common simplification operator, which is called Mex operator. <xref ref-type="fig" rid="pone-0095576-g001">Figure 1</xref> shows three diagrams for the Mex, LoG and DoG operator (<xref ref-type="fig" rid="pone-0095576-g001">Figure 1 A–C</xref>). It can be seen that the Mex operator is well approximate to the LoG and DoG operator, since these three operators produce a circularly symmetric Mexican hat. Especially, when we require a method that combines two or more classes of features, it is necessary to use a common operator similar to the LOG. Interestingly, the DoG operator is generally used for extracting point features, while our operator can extract both area and point features. We give two methods that using the Mex operator for detecting the features and the methods achieve good performance.</p>
<fig id="pone-0095576-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095576.g001</object-id><label>Figure 1</label><caption>
<title>Wavelets of the Mex, LoG and DoG operator.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095576.g001" position="float" xlink:type="simple"/></fig>
<p>The second contribution is that we propose a fast and robust method for estimating the transformation matrix. An image transformation matrix is estimated by a group of points and then the best one is chosen through competition of a set of the transformation matrices. The transformation matrix is well selected by the Grouped Sample Consensus (GCS), which estimates the global transformation matrix through the local sample-data. Importantly, the GCS can also remove the outlier features.</p>
<p>The rest of this paper is organized as follows: Section II presents the Mex operator’s generating and its performance in the feature detection. Section III discusses the grouped feature matching and the transformation matrix estimation through GCS. The Experimental setup and performance evaluation are presented in section IV. Finally, we conclude our work in section V.</p>
</sec></sec><sec id="s2">
<title>Feature Point Detection</title>
<sec id="s2a">
<title>A. Mexican Hat Operator</title>
<p>In Lowe’s work, we can get the well known DoG operator:<disp-formula id="pone.0095576.e001"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095576.e001" xlink:type="simple"/><label>(1)</label></disp-formula>Where, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e002" xlink:type="simple"/></inline-formula> is the space factor, <italic>k</italic> is the factor using for varying scale space. The relationship between the DoG operator and the LoG operator is described by:<disp-formula id="pone.0095576.e003"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095576.e003" xlink:type="simple"/><label>(2)</label></disp-formula>Where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e004" xlink:type="simple"/></inline-formula> is the LoG operator.</p>
<p><disp-formula id="pone.0095576.e005"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095576.e005" xlink:type="simple"/><label>(3)</label></disp-formula>Hence, we have:<disp-formula id="pone.0095576.e006"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095576.e006" xlink:type="simple"/><label>(4)</label></disp-formula>Then we get a slightly simplified function that can replace the DoG:<disp-formula id="pone.0095576.e007"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095576.e007" xlink:type="simple"/><label>(5)</label></disp-formula>Where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e008" xlink:type="simple"/></inline-formula> is a scale factor and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e009" xlink:type="simple"/></inline-formula> is a variation factor of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e010" xlink:type="simple"/></inline-formula> using for varying scale space, here we take <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e011" xlink:type="simple"/></inline-formula>. k can be consecutive positive integers bigger than 1. Moreover, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e012" xlink:type="simple"/></inline-formula> is a Mexican hat function that can be called the Mex operator in this work. It is worth mentioning that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e013" xlink:type="simple"/></inline-formula> is also a simplification of the operator defined in <xref ref-type="bibr" rid="pone.0095576-Matas1">[10]</xref>. The two-dimensional waveform of the Mex operator, LoG operator and DoG operator are shown in <xref ref-type="fig" rid="pone-0095576-g001">figure 1</xref>. The Mexican hat function can be seen as the second order derivative of Gaussian function, and its local extreme points can be seen as the extreme points in DoG scale space. So the Mex operator can simplify the operation in the scale space feature detection.</p>
</sec><sec id="s2b">
<title>B. Local Area Detection</title>
<p>Zero-crossing is a common method for edge detection. The crossing of the second derivative curve and number axis reflect the dramatic intensity change in the image. As a result, pixel points with distinctive features could be detected on this basis. The function of Zero-crossing method is shown as:<disp-formula id="pone.0095576.e014"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095576.e014" xlink:type="simple"/><label>(6)</label></disp-formula>Where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e015" xlink:type="simple"/></inline-formula> is the second derivative operation, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e016" xlink:type="simple"/></inline-formula> is the smoothing of the image with Gaussian filtering. Owing to the fact that the maximum gradient can only be achieved in the vertical direction of the intensity change, it is not sufficient to choose zero-crossings of the second derivative in any direction. The second derivative is non-zero in the direction that are perpendicular to the direction of the intensity change. In order to solve this problem, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e017" xlink:type="simple"/></inline-formula> is replaced by operators that do not rely on the direction, such as the LoG operator in Marr’s work. In our work, we take Mex operator, whose distribution remains rotation invariance no matter in space domain or in the frequency domain. <xref ref-type="fig" rid="pone-0095576-g002">Figure 2</xref> shows the procedures of the local area detection on an input image (<xref ref-type="fig" rid="pone-0095576-g002">Figure 2 A</xref>).</p>
<fig id="pone-0095576-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095576.g002</object-id><label>Figure 2</label><caption>
<title>Procedures of the local area detection.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095576.g002" position="float" xlink:type="simple"/></fig>
<list list-type="order"><list-item>
<p>Image filtering is conducted using the Mex operator; thresholding is taken on the image grayscale (<xref ref-type="fig" rid="pone-0095576-g002">Figure 2 B</xref>). Since zero is represented by an intermediate gray after the filtering, the very positive values appear white, and the negative ones appear black. We give polar values to describe the white and black pixel in order to make the boundaries distinct.</p>
</list-item><list-item>
<p>Zero-crossing detection is conducted, and the closed contours are formed by connecting the zero crossing points (<xref ref-type="fig" rid="pone-0095576-g002">Figure 2 C</xref>). Zero-crossing patterns, which are composed of signs of pixel values of the filtered image, are detected along both vertical and horizontal directions. The pixel that we mark as an edge point satisfies the following two conditions: the pixel is a zero-crossing point (significant change of the convolved image); the pixel is the closest one to the virtual zero plane of the convolved image among its eight neighbors. When the detecting goes to an endpoint of the edge, a low-threshold satisfied point is chosen and the detection starts from the new point until a closed area is achieved. The low-threshold for edge strength (here we use the Mex value of the pixel) is set to 0 in our work.</p>
</list-item><list-item>
<p>If an area we get does not encompass sufficient feature points, this area is removable. For example, we eliminate the areas that contain less than 3 points, because the transformation matrix estimation needs at least 3 pairs of matching points. The bigger value the threshold is set as, the less number of the regions we get.</p>
</list-item></list>
</sec><sec id="s2c">
<title>C. Feature Point Detection</title>
<p>Lecture <xref ref-type="bibr" rid="pone.0095576-Manjunath1">[19]</xref> has proposed a general model which is based on the observation that the curvature response of the feature detectors roots in the difference of two low-pass responses of different bandwidths. The response of the feature detector, denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e018" xlink:type="simple"/></inline-formula> at location <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e019" xlink:type="simple"/></inline-formula> is defined as:<disp-formula id="pone.0095576.e020"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095576.e020" xlink:type="simple"/><label>(7)</label></disp-formula>Where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e021" xlink:type="simple"/></inline-formula> is the representation of the descriptions and the positioning by employing the difference of the filter response in the different scale space. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e022" xlink:type="simple"/></inline-formula> is the normalization of the low-pass filtering in discrete scale space, like <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e023" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e024" xlink:type="simple"/></inline-formula>, <italic>f</italic> is the modulation function, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e025" xlink:type="simple"/></inline-formula> is a constant. Using Mex operator in (6), we get the feature point detection method:<disp-formula id="pone.0095576.e026"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095576.e026" xlink:type="simple"/><label>(8)</label></disp-formula><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e027" xlink:type="simple"/></inline-formula> is taken for calculating the difference between two neighboring spaces.<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e028" xlink:type="simple"/></inline-formula> refers to the convolution of image <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e029" xlink:type="simple"/></inline-formula> with the Mex operator. We only concern about the extremum in a local area and do not care about whether the extreme value is positive or negative. So we take <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e030" xlink:type="simple"/></inline-formula> as the absolute value of the difference between the filter response in scale <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e031" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e032" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e033" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e034" xlink:type="simple"/></inline-formula> represent two neighboring space, and they are not specific scales that have to be determined by the user. <xref ref-type="fig" rid="pone-0095576-g003">Figure 3</xref> shows the result of the local extrema detection on an input image (<xref ref-type="fig" rid="pone-0095576-g003">Figure 3 A</xref>) in scale space with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e035" xlink:type="simple"/></inline-formula> = 2, 3 and 4 (<xref ref-type="fig" rid="pone-0095576-g003">Figure 3 B–D</xref>).</p>
<fig id="pone-0095576-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095576.g003</object-id><label>Figure 3</label><caption>
<title>Feature point detection in different scale spaces.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095576.g003" position="float" xlink:type="simple"/></fig></sec></sec><sec id="s3">
<title>Grouped Points Matching</title>
<sec id="s3a">
<title>A. Area Matching</title>
<p>The majority of the feature matching consists of three steps: local area matching, feature points grouping and points matching. In the step 1, the geometric centers of the detected areas are seen as the interesting points. The interesting points in the two images are matched through <xref ref-type="disp-formula" rid="pone.0095576.e026">equation (8</xref>). The equation is a fast and stable application method based on the voting strategy, which uses the spatial geometrical relations among the interesting points for image feature matching.<disp-formula id="pone.0095576.e036"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095576.e036" xlink:type="simple"/><label>(9)</label></disp-formula>Where, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e037" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e038" xlink:type="simple"/></inline-formula> are the number of standby registration points in the master image and the slave image respectively. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e039" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e040" xlink:type="simple"/></inline-formula> are the matrix formed by the distance of all these points in the images respectively. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e041" xlink:type="simple"/></inline-formula> is the voting matrix. If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e042" xlink:type="simple"/></inline-formula> is the maximum in row <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e043" xlink:type="simple"/></inline-formula> and line <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e044" xlink:type="simple"/></inline-formula> of matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e045" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e046" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e047" xlink:type="simple"/></inline-formula> is a constant, then the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e048" xlink:type="simple"/></inline-formula><italic>-</italic>th point in the master image and the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e049" xlink:type="simple"/></inline-formula>-th point in the slave image is a pair of matching points.</p>
</sec><sec id="s3b">
<title>B. Grouped Points Matching</title>
<p>Generally, feature points partitioning or grouping is an effective method to increase the matching efficiency. After the local area and feature points are achieved, image registration would be conducted by combining the two classes of features. First, the matching of the local area is conducted. Then the points are grouped according to the areas, and matched in each group.</p>
<p>In view of the fact that the accuracy of area detection is an effective factor on image partitioning and points grouping, we use a standard circular region that takes place of a detected area to encompass the pixels as shown in <xref ref-type="fig" rid="pone-0095576-g004">Figure 4</xref>. A circle’s center is the geometric center of a detected local area; its radius is the average of the distances from all the points on the area contour to the geometric center of the area. It is convenient because we only need to compare the distance from the point to the center with the radius when distinguishing the point in or not in the region.</p>
<fig id="pone-0095576-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095576.g004</object-id><label>Figure 4</label><caption>
<title>Image partitioning.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095576.g004" position="float" xlink:type="simple"/></fig>
<p><xref ref-type="fig" rid="pone-0095576-g005">Figure 5</xref> shows the points matched in a pair of matched regions. The image is partitioned into several circular regions and the points are grouped according to regions. The points in the same region are in the same group, and the points that do not in any region are not grouped. The computation speed can get faster since that, the number of the points is small in each group. We append the regional center to the point’s location properties so that the points in one region match with the points in the corresponding region.</p>
<fig id="pone-0095576-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095576.g005</object-id><label>Figure 5</label><caption>
<title>Points matched in a pair of matched regions.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095576.g005" position="float" xlink:type="simple"/></fig>
<p>The time complexity of the grouped feature point matching is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e050" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e051" xlink:type="simple"/></inline-formula> is the number of all the feature points and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e052" xlink:type="simple"/></inline-formula> is the dimensionality of the descriptor. So when the feature point descriptor is given and fixed, the time complexity depends on the number of the points. After points grouping, the time complexity is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e053" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e054" xlink:type="simple"/></inline-formula> is the biggest number of the points in all the groups. Obviously, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e055" xlink:type="simple"/></inline-formula>. So in this work, the time complexity of the feature point matching depends on the group with the biggest number of points rather than all the feature points.</p>
</sec><sec id="s3c">
<title>C. Transformation Matrix Estimation</title>
<p>After the areas matching and points matching, the transformation matrix would be achieved by optimizing the matching results in different groups, and the procedures are shown as follows.<list list-type="order"><list-item>
<p>Calculating the transformation matrices and the root mean square errors (RMSE) taken through the matching points in each group respectively.</p>
</list-item><list-item>
<p>The transformation matrix of a random group is employed to verify all the feature point pairs, and reserves the point pairs meeting the following requirements as the candidate point pairs set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e056" xlink:type="simple"/></inline-formula> whose size is <italic>sum</italic>:<disp-formula id="pone.0095576.e057"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0095576.e057" xlink:type="simple"/><label>(10)</label></disp-formula>Where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e058" xlink:type="simple"/></inline-formula> is the transformation matrix of a random point group, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e059" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e060" xlink:type="simple"/></inline-formula> are the points in the master and the slave image, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e061" xlink:type="simple"/></inline-formula> is the fault-tolerant error. If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e062" xlink:type="simple"/></inline-formula> is bigger than the threshold, let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e063" xlink:type="simple"/></inline-formula> be a candidate inner set, then return to 1); otherwise, return to 2).</p>
</list-item><list-item>
<p>After λ iterations, when the <italic>sum</italic> remains unchanged, takes the matrix with the biggest size of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e064" xlink:type="simple"/></inline-formula> and receivable RMSE as the transformation matrix of image matching.</p>
</list-item></list>          </p>
<p>Since the strategy is an application of the Random Sample Consensus (RANSAC) in the grouped points matching we call it Grouped Sample Consensus (GCS). The matrix in a random group which can satisfy the requirement (9), which has low RMSE and sufficient feature points, can be seen as a candidate matrix. The final transformation matrix is the one with the least RMSE and the largest number of matching points in all the candidate matrices. That is, the whole algorithm would not collapse if there is one matrix fulfill the conditions.</p>
<p>The time complexity of GCS is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e065" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e066" xlink:type="simple"/></inline-formula> is the probability in procedure 2). Interestingly, GCS does not just remove outlier points, it is also a solution on regions mismatching. When there is a mismatching of a region pair, the candidate matrix based on that pair can’t provide inner point pairs and the candidate matrix is rejected.</p>
</sec></sec><sec id="s4">
<title>Experimental Results</title>
<sec id="s4a">
<title>A. Data Set</title>
<p>We use 12 pairs of images for the matching work shown in <xref ref-type="fig" rid="pone-0095576-g006">Figure 6</xref>. Some are from a Mikolajczyk’s testing image set which are frequently used in image processing (<xref ref-type="fig" rid="pone-0095576-g006">Figure 6 A</xref> shows the image “Boat” and its three matching images. Similarly, the <xref ref-type="fig" rid="pone-0095576-g006">Figure 6 B</xref> denotes the image “Bark”). Some are optical pictures (<xref ref-type="fig" rid="pone-0095576-g006">Figure 6 C–F</xref>). All the images are taken due to the real changes through different focal distance and perspectives. So they contain the images with a great majority of rigid changes and some slightly perspective transformations.</p>
<fig id="pone-0095576-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095576.g006</object-id><label>Figure 6</label><caption>
<title>Text images.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095576.g006" position="float" xlink:type="simple"/></fig></sec><sec id="s4b">
<title>B. Performance</title>
<p><xref ref-type="fig" rid="pone-0095576-g007">Figure 7</xref> shows the result of the test image “Building” (<xref ref-type="fig" rid="pone-0095576-g006">Figure 6 E</xref>) registration, the “Building” contains optical images from different viewing angles.</p>
<fig id="pone-0095576-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095576.g007</object-id><label>Figure 7</label><caption>
<title>Performance of the image “Building” registration.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095576.g007" position="float" xlink:type="simple"/></fig>
<p>The master image and the slave image are both partitioned into 20 regions after local area detection and matching (<xref ref-type="fig" rid="pone-0095576-g007">Figure 7 A</xref>). Feature points are detected (<xref ref-type="fig" rid="pone-0095576-g007">Figure 7 B</xref>) and point matching is conducted in each region (<xref ref-type="fig" rid="pone-0095576-g007">Figure 7 C and D</xref>). Each region can get a candidate transformation matrix and the optimal matrix is achieved by GCS. Comparison on the registration accuracy and efficiency among our work, area-based LoG operator, and point-based SIFT operator has been carried out for <xref ref-type="table" rid="pone-0095576-t001">Table 1</xref>. The repetition is the ratio of matched points in the whole points.</p>
<table-wrap id="pone-0095576-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095576.t001</object-id><label>Table 1</label><caption>
<title>Registration result.</title>
</caption><alternatives><graphic id="pone-0095576-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095576.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Operator</td>
<td align="left" rowspan="1" colspan="1">Number</td>
<td align="left" rowspan="1" colspan="1">Repetition</td>
<td align="left" rowspan="1" colspan="1">RMSE</td>
<td align="left" rowspan="1" colspan="1">Time</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Mex</td>
<td align="left" rowspan="1" colspan="1">1078</td>
<td align="left" rowspan="1" colspan="1">72%</td>
<td align="left" rowspan="1" colspan="1">0.0726</td>
<td align="left" rowspan="1" colspan="1">120s</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">SIFT</td>
<td align="left" rowspan="1" colspan="1">1204</td>
<td align="left" rowspan="1" colspan="1">48%</td>
<td align="left" rowspan="1" colspan="1">0.0634</td>
<td align="left" rowspan="1" colspan="1">513s</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">LoG</td>
<td align="left" rowspan="1" colspan="1">73</td>
<td align="left" rowspan="1" colspan="1">/</td>
<td align="left" rowspan="1" colspan="1">0.7430</td>
<td align="left" rowspan="1" colspan="1">13s</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap>
<p>The local area obtained by the LoG operator is so small in number with precise positioning that there is little sense in comparing repetition rate, and it is difficult to achieve good results in registration accuracy. The Mex operator can provide more image features and higher accuracy than the LoG operator. When compared to the SIFT operator, the grouped matching strategy achieves a higher repetition rate and runs much faster. All the experiments in our work were executed on an Intel Pentium(R) Dual-Core CPU 2.5 GHz computer with 2 G RAM in a Matlab environment.</p>
</sec><sec id="s4c">
<title>C. Analysis of Area Detection</title>
<p>Since the local area detection and matching are crucial for the registration method, we discuss their influences as follows. In testing image pair “car” that contains scale change and clipping, we set different values for the threshold in local area detection to get different number of features.</p>
<p>For simplicity, we use the number of the pixels encompassed in the area for the threshold. Four different thresholds are chosen in the master image and the detection threshold is set to 200 in the slave image. The details of the matching are shown in <xref ref-type="table" rid="pone-0095576-t002">Table 2</xref>.</p>
<table-wrap id="pone-0095576-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095576.t002</object-id><label>Table 2</label><caption>
<title>Feature matching result.</title>
</caption><alternatives><graphic id="pone-0095576-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095576.t002" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Threshold</td>
<td align="left" rowspan="1" colspan="1">Matched area</td>
<td align="left" rowspan="1" colspan="1">Points in area</td>
<td align="left" rowspan="1" colspan="1">Matched points</td>
<td align="left" rowspan="1" colspan="1">Time</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">200</td>
<td align="left" rowspan="1" colspan="1">17</td>
<td align="left" rowspan="1" colspan="1">225</td>
<td align="left" rowspan="1" colspan="1">237</td>
<td align="left" rowspan="1" colspan="1">34s</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">400</td>
<td align="left" rowspan="1" colspan="1">9</td>
<td align="left" rowspan="1" colspan="1">162</td>
<td align="left" rowspan="1" colspan="1">246</td>
<td align="left" rowspan="1" colspan="1">48s</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">800</td>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">120</td>
<td align="left" rowspan="1" colspan="1">235</td>
<td align="left" rowspan="1" colspan="1">73s</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">1600</td>
<td align="left" rowspan="1" colspan="1">3</td>
<td align="left" rowspan="1" colspan="1">98</td>
<td align="left" rowspan="1" colspan="1">228</td>
<td align="left" rowspan="1" colspan="1">82s</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap>
<p>It can be seen that, the number of the detected area has an indirect relation to the feature point matching. However, when the matched areas are small, we still get sufficient inner points because of the GCS strategy that estimates the global transformation matrix through the local sample-data. Although the running time of our algorithm becomes longer because the cost on inner choosing gets higher, the algorithm would be useful even though only one detected area meets the condition under which this region encompass sufficient a number of the good feature points that can be used to estimate a transformation matrix with receivable accuracy.</p>
</sec><sec id="s4d">
<title>D. Comparison</title>
<p>In order to testify the algorithm further, we use four quantitative measures to evaluate its performance: the recall, the precision, the RMSE and the running time. The recall is defined as follows. If <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e067" xlink:type="simple"/></inline-formula> pairs of points are matched, and actually there are <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e068" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e069" xlink:type="simple"/></inline-formula> points with matching alternatives in the two images, then the recall is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e070" xlink:type="simple"/></inline-formula>. The precision is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e071" xlink:type="simple"/></inline-formula>, in which <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e072" xlink:type="simple"/></inline-formula> is the number of the wrong matched pairs. The higher the recall and the precision behave, the more stable and accurate the matching methods are.</p>
<p>Comparisons between our work and the most popular point detection and matching methods are taken. The alternatives are the SIFT, and the scale invariant Harris. These methods use RANSAC for transformation matrix estimation, while our work is exploiting the Mex operator with GCS. In our experiments, the factor <italic>k</italic> is taken as 2, 3 and 4, in order to construct a range of scale spaces. We determine the factor <italic>k</italic> based on only one fact that <italic>k</italic> should be consecutive positive integers bigger than 1. We chose the range 2, 3 and 4 because that the Mex operator is mostly similar to the LoG when <italic>k</italic> = 2. Since the Mex operator can approximate the second order derivative of Gaussian function, we actually detect key points in 4 levels of different Gaussian space like SIFT. And the other operators in our experiments use the same range scales for fairness and universality. Finally, all the three methods take the SIFT descriptor for the feature point description.</p>
<p>The results are shown in <xref ref-type="fig" rid="pone-0095576-g008">Figure 8</xref>.The numbers on the x-axes in <xref ref-type="fig" rid="pone-0095576-g008">Figure 8</xref> are corresponding to the 12 pairs of images in <xref ref-type="fig" rid="pone-0095576-g006">Figure 6</xref>. For example, the 1, 2 and 3 are three matching results for the “Boat”.</p>
<fig id="pone-0095576-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095576.g008</object-id><label>Figure 8</label><caption>
<title>Comparison results.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095576.g008" position="float" xlink:type="simple"/></fig>
<p>It can be seen that, the proportion of the outliers becomes higher when the scale transformation happens (<xref ref-type="fig" rid="pone-0095576-g008">Figure 8 A</xref>). The threshold in GCS depends on the robustness required in the application, and we set it to 50% of all detected points, which can sufficiently satisfy the image registrations in our experiments.</p>
<p>The S-Harris, short for the scale invariant Harris operator, achieves the best performance on recall (<xref ref-type="fig" rid="pone-0095576-g008">Figure 8 B</xref>). The Mex operator in our work and the SIFT get the similar result. That is because, the Mex operator and the SIFT detect the points in the scale space pyramid, and the points are a union of several space. The recall is not connected with the proportion of the outliers, but is influenced by the number of the feature points. The third pair has the lowest recall because the matching images in that pair have the most feature points. The matching images in “flower” also contain the scale changes, but the recall of the Mex is higher than the S-Harris since the images take the simple textures. It can be seen that, the Mex operator in our work achieves the same recall as the other scale space based-methods. Moreover, the Mex operator’s recall is higher than the SIFT’s in the matching of rotation changes.</p>
<p>Our method achieves the best performance on precision for some testing pairs (<xref ref-type="fig" rid="pone-0095576-g008">Figure 8 C</xref>), for example, the matching images “giraffe”. There are salient objects in the “giraffe” so that the features, both the local areas and the points, are detected and worked effectively. The GCS gets the best result when the two classes of features are combined; there is about 10% more on precision than that in the other methods. In some testing pairs, the “bark” for example, the textures in the images are discrete, so the GCS strategy is not efficient and regresses to the normal RANSAC as in the other methods.</p>
<p>The RMSE is directly connected with the precision and the detecting accuracy. The RMSE (<xref ref-type="fig" rid="pone-0095576-g008">Figure 8 D</xref>) shows that, the profile is corresponding to the precision curve (<xref ref-type="fig" rid="pone-0095576-g008">Figure 8 C</xref>). It is an evidence presents that the Mex operator achieves as accurate detecting result as the SIFT gets.</p>
<p>Another advantage of our method is on running time (<xref ref-type="fig" rid="pone-0095576-g008">Figure 8 E</xref>). Like the precision, the GCS strategy performs much better when the two classes of features are working together. The points are grouped and the running time decreases according to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0095576.e073" xlink:type="simple"/></inline-formula>.</p>
<p><xref ref-type="table" rid="pone-0095576-t003">Table 3</xref> shows the average results of the comparisons. It can be seen that, our algorithm gets better performance than the famous SIFT and S-Harris on the registration accuracy, and achieves much better results in running time.</p>
<table-wrap id="pone-0095576-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0095576.t003</object-id><label>Table 3</label><caption>
<title>Average result.</title>
</caption><alternatives><graphic id="pone-0095576-t003-3" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0095576.t003" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1">Operator</td>
<td align="left" rowspan="1" colspan="1">Recall</td>
<td align="left" rowspan="1" colspan="1">Precision</td>
<td align="left" rowspan="1" colspan="1">RMSE</td>
<td align="left" rowspan="1" colspan="1">Time (s/point)</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Mex</td>
<td align="left" rowspan="1" colspan="1">67%</td>
<td align="left" rowspan="1" colspan="1">92%</td>
<td align="left" rowspan="1" colspan="1">0.0642</td>
<td align="left" rowspan="1" colspan="1">0.1391</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">SIFT</td>
<td align="left" rowspan="1" colspan="1">66%</td>
<td align="left" rowspan="1" colspan="1">90%</td>
<td align="left" rowspan="1" colspan="1">0.0644</td>
<td align="left" rowspan="1" colspan="1">0.5371</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">S-Harris</td>
<td align="left" rowspan="1" colspan="1">71%</td>
<td align="left" rowspan="1" colspan="1">89%</td>
<td align="left" rowspan="1" colspan="1">0.0662</td>
<td align="left" rowspan="1" colspan="1">0.4686</td>
</tr>
</tbody>
</table>
</alternatives></table-wrap></sec></sec><sec id="s5">
<title>Conclusion</title>
<p>In this work, we provide a Mexican hat function-based operator on image feature detection. We use the operator for achieving an improved zero-crossing method of local area detection and detection of feature point is conducted in different scale space. By combining the two classes of image features, we propose a grouped feature point matching strategy and a grouped sample consensus strategy to achieve a fast and accurate image registration.</p>
<p>The future work for the research is that, the local area changes in the affine transformation so the regions we take to group the points can’t be circles. An affine invariant region should be an ellipse, the orientation and the accuracy of edge detection is crucial to the feature matching.</p>
</sec></body>
<back><ref-list>
<title>References</title>
<ref id="pone.0095576-Zitova1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zitova</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Flusser</surname><given-names>J</given-names></name> (<year>2003</year>) <article-title>Image registration methods: a survey</article-title>. <source>Image and Vision Computing</source> <volume>21</volume>: <fpage>977</fpage>–<lpage>1000</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095576-Dai1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dai</surname><given-names>XL</given-names></name>, <name name-style="western"><surname>Khorram</surname><given-names>S</given-names></name> (<year>1999</year>) <article-title>A feature-based image registration algorithm using improved chain-code representation combined with invariant moment</article-title>. <source>IEEE Transaction on Geoscience and Remote Sensing</source> <volume>37(5)</volume>: <fpage>2351</fpage>–<lpage>2362</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095576-Lowe1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lowe</surname><given-names>DG</given-names></name> (<year>2004</year>) <article-title>Distinctive image features from scale-invariant keypoints</article-title>. <source>International Journal of Computer Vision</source> <volume>60(2)</volume>: <fpage>91</fpage>–<lpage>110</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095576-Mikolajczyk1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mikolajczyk</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Schmid</surname><given-names>C</given-names></name> (<year>2004</year>) <article-title>Scale and affine invariant interest point detectors</article-title>. <source>International Journal of Computer Vision</source> <volume>60(1)</volume>: <fpage>63</fpage>–<lpage>68</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095576-Bay1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bay</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Tuytelaars</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Gool</surname><given-names>LV</given-names></name> (<year>2006</year>) <article-title>SURF: speeded up robust features</article-title>. <source>Computer Vision and Image Understanding</source> <volume>110(3)</volume>: <fpage>346</fpage>–<lpage>359</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095576-Yang1"><label>6</label>
<mixed-citation publication-type="other" xlink:type="simple">Yang K, Sukthanhar R (2004) PCA-SIFT: a more distinctive representation for local image descriptors. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</mixed-citation>
</ref>
<ref id="pone.0095576-Padmavathi1"><label>7</label>
<mixed-citation publication-type="other" xlink:type="simple">Padmavathi G, Muthukumar M, Thakur SK (2010) Kernel principal component analysis feature detection and classification for underwater images. Proceedings of the 3rd International Congress on Image and Signal Processing.</mixed-citation>
</ref>
<ref id="pone.0095576-Cui1"><label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cui</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Ngan</surname><given-names>K</given-names></name> (<year>2011</year>) <article-title>Scale and affine invariant fan feature</article-title>. <source>IEEE Transaction on Image Processing</source> <volume>20(6)</volume>: <fpage>1627</fpage>–<lpage>1640</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095576-Tuytelaars1"><label>9</label>
<mixed-citation publication-type="other" xlink:type="simple">Tuytelaars T, Gool LV (2000) Wide baseline stereo matching based on local affinely invariant regions. Proceedings of the 11th British Machine Vision Conference.</mixed-citation>
</ref>
<ref id="pone.0095576-Matas1"><label>10</label>
<mixed-citation publication-type="other" xlink:type="simple">Matas J, Chum O, Urban M, Pajdla T (2002) Robust wide-baseline stereo from maximally stable extremal regions. Proceedings of the British Machine Vision Conference Cardiff UK: 384–393.</mixed-citation>
</ref>
<ref id="pone.0095576-Kadir1"><label>11</label>
<mixed-citation publication-type="other" xlink:type="simple">Kadir T, Brady M, Zisserman A (2004) An affine invariant method for selecting salient regions in images. Proceedings of the European Conference on Computer Vision: 345–457.</mixed-citation>
</ref>
<ref id="pone.0095576-Wang1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Zhou</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Bai</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>WY</given-names></name> (<year>2012</year>) <article-title>Shape matching and recognition using group-wised points</article-title>. <source>Advances in Image and Video Technology LNCS</source> <volume>(7088)</volume>: <fpage>393</fpage>–<lpage>404</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095576-Myronenko1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Myronenko</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Song</surname><given-names>X</given-names></name> (<year>2010</year>) <article-title>Point-set registration: coherent point drift, IEEE Transaction on Pattern Analysis and Machine Intelligence</article-title>. <volume>12(32)</volume>: <fpage>2262</fpage>–<lpage>2275</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095576-Chui1"><label>14</label>
<mixed-citation publication-type="other" xlink:type="simple">Chui H, Rangarajan A (2003) A new point matching algorithm for non-rigid registration. Comput. Vis. Image Underst: 114–141.</mixed-citation>
</ref>
<ref id="pone.0095576-Fitzgibbon1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fitzgibbon</surname><given-names>AW</given-names></name> (<year>2001</year>) <article-title>Robust Registration of 2D and 3D Point Sets</article-title>. <source>Proceedings of the BMVC 43</source> <volume>1–43</volume>: <fpage>10</fpage>.</mixed-citation>
</ref>
<ref id="pone.0095576-Daubechies1"><label>16</label>
<mixed-citation publication-type="other" xlink:type="simple">Daubechies I (1992) Society for Industrial and Applied Mathematics. Philadelphia Pennsylvania, US.</mixed-citation>
</ref>
<ref id="pone.0095576-Marr1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marr</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Hildreth</surname><given-names>E</given-names></name> (<year>1980</year>) <article-title>Theory of edge detection</article-title>. <source>Proceedings of the Royal Society of London, Series B, Biological Sciences</source> <volume>207(1167)</volume>: <fpage>187</fpage>–<lpage>217</lpage>.</mixed-citation>
</ref>
<ref id="pone.0095576-Yasein1"><label>18</label>
<mixed-citation publication-type="other" xlink:type="simple">Yasein M, Agathoklis P (2008) A feature-based image registration technique for images of different scale. Proceedings of the IEEE International Symposium on Circuits and Systems: 3558–3561.</mixed-citation>
</ref>
<ref id="pone.0095576-Manjunath1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Manjunath</surname><given-names>BS</given-names></name>, <name name-style="western"><surname>Shekhar</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Chellappa</surname><given-names>R</given-names></name> (<year>1996</year>) <article-title>A new approach to image feature detection with application</article-title>. <source>Pattern Recognition</source> <volume>29(4)</volume>: <fpage>627</fpage>–<lpage>640</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>