<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-16-26753</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0179289</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject><subj-group><subject>Bioacoustics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Bioacoustics</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Linguistics</subject><subj-group><subject>Speech</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Music cognition</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Music cognition</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Music cognition</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Engineering and technology</subject><subj-group><subject>Signal processing</subject><subj-group><subject>Speech signal processing</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Verbal behavior</subject><subj-group><subject>Verbal communication</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Anthropology</subject><subj-group><subject>Cultural anthropology</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Shared acoustic codes underlie emotional communication in music and speech—Evidence from deep transfer learning</article-title>
<alt-title alt-title-type="running-head">Shared acoustic codes underlie emotional communication in music and speech</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-5234-1497</contrib-id>
<name name-style="western">
<surname>Coutinho</surname> <given-names>Eduardo</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Schuller</surname> <given-names>Björn</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Department of Music, University of Liverpool, Liverpool, United Kingdom</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Department of Computing, Imperial College London, London, United Kingdom</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Zhang</surname> <given-names>Yudong</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Nanjing Normal University, CHINA</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p><list list-type="simple">
<list-item><p><bold>Conceptualization:</bold> EC BS.</p></list-item>
<list-item><p><bold>Data curation:</bold> EC.</p></list-item>
<list-item><p><bold>Formal analysis:</bold> EC.</p></list-item>
<list-item><p><bold>Funding acquisition:</bold> BS.</p></list-item>
<list-item><p><bold>Investigation:</bold> EC.</p></list-item>
<list-item><p><bold>Methodology:</bold> EC BS.</p></list-item>
<list-item><p><bold>Project administration:</bold> EC.</p></list-item>
<list-item><p><bold>Resources:</bold> EC BS.</p></list-item>
<list-item><p><bold>Software:</bold> EC.</p></list-item>
<list-item><p><bold>Supervision:</bold> EC.</p></list-item>
<list-item><p><bold>Validation:</bold> EC.</p></list-item>
<list-item><p><bold>Visualization:</bold> EC.</p></list-item>
<list-item><p><bold>Writing – original draft:</bold> EC.</p></list-item>
<list-item><p><bold>Writing – review &amp; editing:</bold> EC.</p></list-item>
</list></p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">e.coutinho@liverpool.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<year>2017</year>
</pub-date>
<pub-date pub-type="epub">
<day>28</day>
<month>6</month>
<year>2017</year>
</pub-date>
<volume>12</volume>
<issue>6</issue>
<elocation-id>e0179289</elocation-id>
<history>
<date date-type="received">
<day>4</day>
<month>7</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>26</day>
<month>5</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Coutinho, Schuller</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0179289"/>
<abstract>
<p>Music and speech exhibit striking similarities in the communication of emotions in the acoustic domain, in such a way that the communication of specific emotions is achieved, at least to a certain extent, by means of shared acoustic patterns. From an Affective Sciences points of view, determining the degree of overlap between both domains is fundamental to understand the shared mechanisms underlying such phenomenon. From a Machine learning perspective, the overlap between acoustic codes for emotional expression in music and speech opens new possibilities to enlarge the amount of data available to develop music and speech emotion recognition systems. In this article, we investigate time-continuous predictions of emotion (Arousal and Valence) in music and speech, and the Transfer Learning between these domains. We establish a comparative framework including intra- (i.e., models trained and tested on the same modality, either music or speech) and cross-domain experiments (i.e., models trained in one modality and tested on the other). In the cross-domain context, we evaluated two strategies—the direct transfer between domains, and the contribution of Transfer Learning techniques (feature-representation-transfer based on Denoising Auto Encoders) for reducing the gap in the feature space distributions. Our results demonstrate an excellent cross-domain generalisation performance with and without feature representation transfer in both directions. In the case of music, cross-domain approaches outperformed intra-domain models for Valence estimation, whereas for Speech intra-domain models achieve the best performance. This is the first demonstration of shared acoustic codes for emotional expression in music and speech in the time-continuous domain.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100007601</institution-id>
<institution>Horizon 2020</institution>
</institution-wrap>
</funding-source>
<award-id>645378</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Schuller</surname> <given-names>Björn</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>This work was partially funded by European Union’s Horizon 2020 (<ext-link ext-link-type="uri" xlink:href="https://ec.europa.eu/programmes/horizon2020/" xlink:type="simple">https://ec.europa.eu/programmes/horizon2020/</ext-link>) research and innovation programme under grant agreement No. 645378 (ARIA-VALUSPA) awarded to Björn Schuller.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="6"/>
<page-count count="24"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Dataset and full explanation on how to obtain data not curated by the authors available here: <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/345944.WTZdgl2qNFQ" xlink:type="simple">https://zenodo.org/record/345944.WTZdgl2qNFQ</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>It is common knowledge that music has the remarkable capacity to stir human emotions and affect our moods in everyday life. Although this is an intuitive fact for most people, understanding the process of emotion induction through music proved to be a major challenge to research in various disciplines. It is not until the past few decades that a consistent body of evidence started unveiling key aspects of such a pervasive phenomenon and shedding new light on the underlying mechanisms supporting the link between music perception and emotion production (see [<xref ref-type="bibr" rid="pone.0179289.ref001">1</xref>, <xref ref-type="bibr" rid="pone.0179289.ref002">2</xref>]). It is now known that the emotions experienced by music listeners are influenced by a variety of parameters related to listener traits and states, musicians’ performance, and listening and cultural contexts, therefore, rendering the whole process of emotion production listener- and context-dependent (see [<xref ref-type="bibr" rid="pone.0179289.ref002">2</xref>, <xref ref-type="bibr" rid="pone.0179289.ref003">3</xref>] for detailed accounts). Nonetheless, researchers have also provided important evidence demonstrating that the emotions perceived (i.e., recognised) by listeners in a piece of music seem to depend mostly on particular configurations of acoustic and musical features—it is now well-known that modulations of speed and continuity, accentuation, pitch and range, timbre and dynamics are at the very centre of the communication of emotional meaning (see [<xref ref-type="bibr" rid="pone.0179289.ref004">4</xref>] for an overview), with specific configurations communicating similar emotions universally (e.g., [<xref ref-type="bibr" rid="pone.0179289.ref005">5</xref>–<xref ref-type="bibr" rid="pone.0179289.ref007">7</xref>]).</p>
<p>The fact that the most consistent relationships between musical structure and emotional qualities involve basic variables in human audition rather than complex, music-specific cues, indicates that the way emotions are conveyed through acoustic patterns embedded in music is remarkably similar to the expressive patterns characteristic of speech prosody (the nonverbal aspects of speech that, amongst other things, are crucial in the communication of emotional information). In a meta-analysis that reviews 104 studies of vocal expression and 41 studies of music performance and compared the acoustic characteristics of speech and music associated with particular emotions [<xref ref-type="bibr" rid="pone.0179289.ref008">8</xref>], the authors clearly demonstrate a great degree of overlap between the emotion-specific acoustic patterns of acoustic cues used to express discrete emotions in both domains. Subsequent empirical work, that compared directly both domains, has provided further evidence, both in terms of specific emotions ([<xref ref-type="bibr" rid="pone.0179289.ref009">9</xref>]) as well as in terms of affective dimensions ([<xref ref-type="bibr" rid="pone.0179289.ref010">10</xref>, <xref ref-type="bibr" rid="pone.0179289.ref011">11</xref>]). In sum, these and other studies provide a strong basis on which to purport the existence of a general mechanism for the expression and recognition of emotions in the acoustic domain (see also [<xref ref-type="bibr" rid="pone.0179289.ref012">12</xref>] for a discussion and account on the possible evolutionary roots of such mechanism).</p>
<p>In this article, we propose to investigate the level of overlap between the acoustic cues to emotion in music and speech. From an Affective Sciences perspective, it is important to explore the level of overlap between the acoustic cues to emotion in music and speech, because it can help explain the reasons why listeners perceive music as expressive of emotion (e.g., [<xref ref-type="bibr" rid="pone.0179289.ref008">8</xref>, <xref ref-type="bibr" rid="pone.0179289.ref013">13</xref>] by highlighting the underlying mechanisms supporting such phenomenon (e.g. [<xref ref-type="bibr" rid="pone.0179289.ref002">2</xref>, <xref ref-type="bibr" rid="pone.0179289.ref014">14</xref>]. Furthermore, it can provide support to theories of music origins suggesting that speech and music evolved from a common origin (e.g., [<xref ref-type="bibr" rid="pone.0179289.ref015">15</xref>, <xref ref-type="bibr" rid="pone.0179289.ref016">16</xref>]). From a Machine Learning perspective, it is advantageous to explore the existence of shared acoustic codes to emotions in both domains since the undifferentiated use of music and speech signals can enlarge the amount of available data which can be used to improve the performance of Speech Emotion Recognition (SER) and Music Emotion Recognition (MER) systems. In particular, the possibility of using speech to develop MER systems is of paramount importance given the fact that there is very little labelled data in this domain. On the other hand, music expresses a wider range of emotional states that are not necessarily present in available speech databases. In that sense, it is also beneficial to develop SER systems by providing a more complete sample of the full emotional spectrum. Additionally, it can lead to the development of hybrid systems, i.e., emotions recognition systems that are applicable to both music and speech signals, which may be particularly important for novel applications in Affective Computing where a holistic understanding of affective signals in the world would be highly beneficial.</p>
<sec id="sec002">
<title>Time-continuous predictions of emotion in music and speech</title>
<p>In [<xref ref-type="bibr" rid="pone.0179289.ref010">10</xref>, <xref ref-type="bibr" rid="pone.0179289.ref017">17</xref>, <xref ref-type="bibr" rid="pone.0179289.ref018">18</xref>], it was shown that the prediction of musical affect demands models sensitive to the temporal context of music structural features that continuously adapt the predictions of emotion qualities based on the present and past inputs. This is particularly important in everyday life scenarios where continuous streams of information are always available, and predicting emotion in a time-continuous way is essential to improve interactions between humans and machines. Moreover, it allows to detect both nuanced and evident differences in emotion continuously over time, without determining particular segmentations beforehand. For these reasons, we deal with time-continuous predictions of emotional responses to music and speech.</p>
<p>Only a few works have modelled time-continuous music and speech emotion recognition, and only two have directly compared both domains. In relation to music, the first basic attempt to model time-continuous emotional responses was proposed by [<xref ref-type="bibr" rid="pone.0179289.ref019">19</xref>], who has created multiple linear regression models to predict Arousal and Valence ratings for a set of six music pieces. Nonetheless, such models are not sensitive to the temporal context. In [<xref ref-type="bibr" rid="pone.0179289.ref017">17</xref>], the use of Recurrent Neural Networks (RNN) was introduced to model the same set of music pieces. The authors found that a significant part of the listener’s affective response could be predicted from a small set of six psychoacoustic features—loudness, tempo, texture, mean pitch, pitch variation, and sharpness. This methodology was later successfully applied to a new set of music [<xref ref-type="bibr" rid="pone.0179289.ref018">18</xref>]. Still, a major limitation of these works is the very small set of music pieces resultant from the time-consuming annotation process, which renders doubts about their generality. Such limitations have recently been partially addressed with the introduction of the MediaEval “Emotion in Music” task in 2013 [<xref ref-type="bibr" rid="pone.0179289.ref020">20</xref>], which has led to the creation of a large database of 1000 songs (extended in 2014 to almost 1500 [<xref ref-type="bibr" rid="pone.0179289.ref021">21</xref>]). In the speech domain, [<xref ref-type="bibr" rid="pone.0179289.ref022">22</xref>] applied LSTM-RNNs to the estimation of Arousal and Valence from a subset of natural speech recordings from the SEMAINE database [<xref ref-type="bibr" rid="pone.0179289.ref023">23</xref>] (see also [<xref ref-type="bibr" rid="pone.0179289.ref024">24</xref>]). Recently, the Audio/Visual Emotion Challenges (AVEC) have increasingly focused on time-continuous predictions of emotion from audio (and video) (e.g., [<xref ref-type="bibr" rid="pone.0179289.ref025">25</xref>]).</p>
<p>The first attempt to compare (time-continuous) emotions perceived in music and natural speech was conducted by [<xref ref-type="bibr" rid="pone.0179289.ref010">10</xref>]. The authors applied the same methodology proposed in [<xref ref-type="bibr" rid="pone.0179289.ref017">17</xref>] and [<xref ref-type="bibr" rid="pone.0179289.ref018">18</xref>] to model music and speech separately, and have shown that, an almost identical set of acoustic features allowed to predict the emotions perceived by human listeners in both domains. These variables were loudness, tempo/speech rate, melodic/prosodic contour, spectral centroid, spectral flux, sharpness, and roughness. Unfortunately, like in previous studies, the database size was very limited (8 music pieces and 9 speech samples), and speech and music were modelled separately.</p>
<p>The first direct comparison between both domains (i.e., using the same models and feature sets), albeit in the categorical domain, was presented by [<xref ref-type="bibr" rid="pone.0179289.ref011">11</xref>]. The authors demonstrated that a model trained to classify music or speech instances into a set of discrete emotions using a common set of 200 acoustic features, had a good performance when tested on speech or Music (respectively). Finally, within a time-continuous framework, [<xref ref-type="bibr" rid="pone.0179289.ref026">26</xref>] evaluated whether cross-domain predictions of emotion are a viable option for acoustic emotion recognition. Overall, results indicated a good cross-domain generalisation performance (especially for the model trained on speech and tested on music), but the data used included less than 14 minutes of music and 12 minutes of speech.</p>
<p>In the context of this paper, our aims are two-fold. First, we want to explore the extent to which time-continuous affective-acoustic patterns are generalisable across music and speech, and to predict emotion in music using models trained on emotional speech, and vice-versa. Second, we want to explore the use of knowledge transfer techniques to deal with differences in the feature spaces and distributions of both types of stimuli.</p>
</sec>
</sec>
<sec id="sec003">
<title>Background and methods</title>
<sec id="sec004">
<title>Emotion representation</title>
<p>When modelling emotion it is fundamental to define the conceptual model used for its numerical representation. Early work on music emotion recognition focused on classifying music pieces into categorical labels describing specific emotions [<xref ref-type="bibr" rid="pone.0179289.ref027">27</xref>]. The typical lists of emotions (or affect categories) used were the so-called basic emotions (e.g., anger, fear, surprise). Nonetheless, these terms are seldom adequate to describe the emotions present in a music piece and are very dependent on stylistic aspects [<xref ref-type="bibr" rid="pone.0179289.ref028">28</xref>, <xref ref-type="bibr" rid="pone.0179289.ref029">29</xref>]. In other cases freely chosen tags are used (e.g., [<xref ref-type="bibr" rid="pone.0179289.ref030">30</xref>]), although the quality of this annotation process is questionable (e.g., tags often do not describe emotional states, terms can be highly ambiguous, large number of tags). Moreover, emotions perceived and experienced with music are often ambiguous, mixed and dynamic (e.g., [<xref ref-type="bibr" rid="pone.0179289.ref010">10</xref>, <xref ref-type="bibr" rid="pone.0179289.ref031">31</xref>]), which demands a more flexible frameworks for emotion representation.</p>
<p>A framework that has proved to be particularly adequate to address these issues are the so-called dimensional models of affects. According to dimensional theorists, the subjective experience of emotion can be depicted by the combination of two or more underlying psychological ‘dimensions’, and therefore that human emotions can be represented as positions in a multidimensional space (the dimensions themselves are usually derived empirically through factorial analysis). A very popular dimensional taxonomy in this domain is the circumplex model of affect proposed by Russell [<xref ref-type="bibr" rid="pone.0179289.ref032">32</xref>], which construes emotions as linear combinations of two independent neurophysiological dimensions—arousal and valence. Arousal describes the level of activation or intensity of the emotions experienced. Valence depicts the hedonic tone of the emotion in a continuum from negative to positive affect. In the context of this work, we adopt Russell’s taxonomy for emotion representation. Apart from its intrinsic benefits, Russell’s taxonomy has the additional advantage of allowing to represent a wide range of emotions that are not specific to music or speech, and that can described by the same variables. This is fundamental because the emotions expressed in speech (triggered by events in everyday life) can be very different from those expressed by music (often of aesthetic nature [<xref ref-type="bibr" rid="pone.0179289.ref002">2</xref>]). Furthermore, dimensional models are nowadays commonly used in SER and MER (e.g., [<xref ref-type="bibr" rid="pone.0179289.ref025">25</xref>, <xref ref-type="bibr" rid="pone.0179289.ref033">33</xref>])</p>
</sec>
<sec id="sec005">
<title>Deep long-short term memory recurrent neural networks</title>
<p>From the machine learning point of view, time-continuous predictions of continuous dimensions in a two-dimensional space largely benefit from context-sensitive models. Given their proven ability to model time-continuous emotions in both music (e.g., [<xref ref-type="bibr" rid="pone.0179289.ref034">34</xref>]) and speech (e.g., [<xref ref-type="bibr" rid="pone.0179289.ref035">35</xref>]), we consider the contribution of Long Short-Term Memory RNNs (LSTM-RNNs) [<xref ref-type="bibr" rid="pone.0179289.ref036">36</xref>]. LSTM-RNNs are architecturally similar to the traditional recurrent neural networks (RNN; e.g., [<xref ref-type="bibr" rid="pone.0179289.ref037">37</xref>]) except that, the nonlinear hidden units are replaced by a special kind of memory blocks, which endow the network with the capacity of accessing long-range temporal contexts and predict the outputs based on such information. A single LSTM memory block comprises one (or several) self-connected memory cells and three multiplicative units—input, output and forget gates. These units set up the cells with analogues of write, read and reset operations, and allow LSTM memory cells to store and access information over long sequences (and corresponding periods of time) which permits to overcome the vanishing gradient problem of simple RNNs, whereby the influence of the network inputs on the hidden units (and therefore the outputs) decays or blows up exponentially as the information cycles through the network recurrent connections.</p>
<p>
<xref ref-type="fig" rid="pone.0179289.g001">Fig 1</xref> shows a LSTM memory block with one cell. The cell input is initially scaled by the activation function of the input gate. Then, the cell output is computed from the activation of the output gate, plus the memory cell values in the previous time step (controlled by the activation of the forget gates). For a given memory block, with <bold><italic>W</italic></bold> as the weight matrix, <bold><italic>x</italic></bold><sub><italic>t</italic></sub> the input vector, <bold><italic>h</italic></bold><sub><italic>t</italic></sub> the hidden vector, <bold><italic>b</italic></bold><sub><italic>t</italic></sub> the hidden bias vector, the activation vector of the input gate <bold><italic>i</italic></bold><sub><italic>t</italic></sub> can be expressed as follows:
<disp-formula id="pone.0179289.e001"><alternatives><graphic id="pone.0179289.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0179289.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>i</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mrow><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mrow><mml:mi>h</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi mathvariant="bold-italic">h</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi mathvariant="bold-italic">c</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where, <bold><italic>f</italic></bold><sub><italic>g</italic></sub> is the logistic sigmoid function.</p>
<fig id="pone.0179289.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0179289.g001</object-id>
<label>Fig 1</label>
<caption>
<title>LSTM memory block.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0179289.g001" xlink:type="simple"/>
</fig>
<p>Similarly, the activation of the forget gate <bold><italic>f</italic></bold><sub><italic>t</italic></sub> is expressed as:
<disp-formula id="pone.0179289.e002"><alternatives><graphic id="pone.0179289.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0179289.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>f</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mrow><mml:mi>x</mml:mi> <mml:mi>f</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mrow><mml:mi>h</mml:mi> <mml:mi>f</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi mathvariant="bold-italic">h</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>f</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi mathvariant="bold-italic">c</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mi>f</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula></p>
<p>The memory cell value <bold><italic>c</italic></bold><sub><italic>t</italic></sub> is then the sum of the input vector at time <italic>t</italic> and its own activation in the previous time step:
<disp-formula id="pone.0179289.e003"><alternatives><graphic id="pone.0179289.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0179289.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>c</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mrow><mml:mi>x</mml:mi> <mml:mi>c</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mrow><mml:mi>h</mml:mi> <mml:mi>c</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi mathvariant="bold-italic">h</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">f</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">c</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
where <italic>f</italic><sub><italic>i</italic></sub> is the <italic>tanh</italic> activation function.</p>
<p>The output of the memory cell is controlled by the output gate activation:
<disp-formula id="pone.0179289.e004"><alternatives><graphic id="pone.0179289.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0179289.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>o</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>g</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mrow><mml:mi>x</mml:mi> <mml:mi>o</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mrow><mml:mi>h</mml:mi> <mml:mi>o</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi mathvariant="bold-italic">h</mml:mi> <mml:mrow><mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>o</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi mathvariant="bold-italic">c</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">b</mml:mi> <mml:mi>o</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula></p>
<p>Finally, the output of the memory block is:
<disp-formula id="pone.0179289.e005"><alternatives><graphic id="pone.0179289.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0179289.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>h</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">o</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>·</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>o</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">c</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
where <italic>f</italic><sub><italic>o</italic></sub> is the <italic>tanh</italic> activation function.</p>
<p>Generally, LSTM-RNNs have shown remarkable performance in a variety of Machine Learning tasks, including, handwriting recognition [<xref ref-type="bibr" rid="pone.0179289.ref038">38</xref>], keyword spotting [<xref ref-type="bibr" rid="pone.0179289.ref039">39</xref>], or phoneme classification [<xref ref-type="bibr" rid="pone.0179289.ref040">40</xref>]. Furthermore, as mentioned earlier in this section, they have also been used in the context of time-continuous predictions of emotion in music and speech with considerable success. For more details on LSTM networks the reader is referred to [<xref ref-type="bibr" rid="pone.0179289.ref036">36</xref>].</p>
</sec>
<sec id="sec006">
<title>Acoustic features for emotion in speech and music</title>
<p>In order to promote the reproducibility of this work, we use a well-developed set for automatic recognition of paralinguistic phenomena—the official feature set of the 2013 INTERSPEECH Computational Paralinguistics Challenge (<italic>ComParE</italic>). This feature set is already a standard in SER (e.g., [<xref ref-type="bibr" rid="pone.0179289.ref041">41</xref>, <xref ref-type="bibr" rid="pone.0179289.ref042">42</xref>]) and MER (e.g., [<xref ref-type="bibr" rid="pone.0179289.ref021">21</xref>, <xref ref-type="bibr" rid="pone.0179289.ref033">33</xref>]). The <italic>ComParE</italic> feature set comprises 65 of low-level audio descriptors (LLDs; see <xref ref-type="table" rid="pone.0179289.t001">Table 1</xref>) and their first order derivates (<italic>Δ</italic> LLDs; a total of 130 features), which cover a broad set of descriptors from the fields of speech processing, Music Information Retrieval, and general sound analysis. For the computation of all LLDs, we used overlapping windows with a step size of 10 ms. LLDs related to voice were computed using 60 ms long time frames and Gaussian windows (<italic>σ</italic> = 0.4), whereas LLDs related to all other features were calculated using 25 ms long time frames and Hamming window functions (leading to 17% and 40% overlaps, respectively). In order to adapt the sampling rate of the feature set to that of the annotations, we also computed the mean and standard deviation functionals of each feature over 2 s time windows with 50% overlap (step size of 1.0 s). This resulted in a total of 260 features extracted at a rate of 1 Hz. The computation of these features was done using the open-source feature extractor openSMILE ([<xref ref-type="bibr" rid="pone.0179289.ref043">43</xref>]). For full details on the <italic>ComParE</italic> feature set please refer to [<xref ref-type="bibr" rid="pone.0179289.ref011">11</xref>].</p>
<table-wrap id="pone.0179289.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0179289.t001</object-id>
<label>Table 1</label>
<caption>
<title>ComParE feature set: List of 65 energy-, spectral- and voicing-related low-level descriptors (LLD).</title>
</caption>
<alternatives>
<graphic id="pone.0179289.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0179289.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<tbody>
<tr>
<td align="left"><bold>Energy related LLDs (4)</bold></td>
<td align="left"><bold>Group</bold></td>
</tr>
<tr>
<td align="left">Sum of auditory spectrum (loudness)</td>
<td align="left">prosodic</td>
</tr>
<tr>
<td align="left">Sum of RASTA-style filtered auditory spectrum</td>
<td align="left">prosodic</td>
</tr>
<tr>
<td align="left">RMS Energy</td>
<td align="left">prosodic</td>
</tr>
<tr>
<td align="left"><bold>Spectral LLDs (55)</bold></td>
<td align="left"><bold>Group</bold></td>
</tr>
<tr>
<td align="left">RASTA-style auditory spectrum, bands 1–26 (0–8 kHz)</td>
<td align="left">spectral</td>
</tr>
<tr>
<td align="left">MFCC 1–14</td>
<td align="left">cepstral</td>
</tr>
<tr>
<td align="left">Spectral energy 250–650 Hz, 1 k–4 kHz</td>
<td align="left">spectral</td>
</tr>
<tr>
<td align="left">Spectral Roll off point 0.25, 0.50, 0.75, 0.90</td>
<td align="left">spectral</td>
</tr>
<tr>
<td align="left">Spectral Flux, Centroid, Entropy, Slope</td>
<td align="left">spectral</td>
</tr>
<tr>
<td align="left">Psychoacoustic Sharpness, Harmonicity</td>
<td align="left">spectral</td>
</tr>
<tr>
<td align="left">Spectral Variance, Skewness, Kurtosis</td>
<td align="left">spectral</td>
</tr>
<tr>
<td align="left">Zero-Crossing Rate</td>
<td align="left">prosodic</td>
</tr>
<tr>
<td align="left"><bold>Voicing related LLDs (6)</bold></td>
<td align="left"><bold>Group</bold></td>
</tr>
<tr>
<td align="left"><italic>F</italic><sub>0</sub> (SHS &amp; Viterbi smoothing)</td>
<td align="left">prosodic</td>
</tr>
<tr>
<td align="left">Prob. of voice</td>
<td align="left">sound quality</td>
</tr>
<tr>
<td align="left">log. HNR, Jitter (local, delta), Shimmer (local)</td>
<td align="left">sound quality</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec007">
<title>Transfer learning with denoising auto-encoders</title>
<p>The concept of Transfer Learning (TL) is inspired by the fact that humans can use previously acquired knowledge in one domain to solve new problems, faster and efficiently, in new domains. In the realm of Machine Learning, TL has been proposed to deal with the problematic situation of having training and test samples derived from different feature spaces and data distributions. In this scenario, most statistical models need to be retrained with new data that matches the new distributions, which is an expensive process and often unrealistic. As an example, in the SER field, a common limitation of models trained on a specific speech corpus is the tendency to under-perform with recordings from other sources. This can be due to various reasons, but often is related to the characteristics of the speakers in each corpus, the type of emotions being conveyed, the level of portrayal vs spontaneity, the recording conditions, among others (see [<xref ref-type="bibr" rid="pone.0179289.ref044">44</xref>]). In point of fact, TL has recently started to be applied in this area with evident success ([<xref ref-type="bibr" rid="pone.0179289.ref045">45</xref>]).</p>
<p>In this work, we explore the existence of shared acoustic codes communicating emotions in music and speech, and evaluate the extent to which TL can benefit the generalisation of emotion recognition across domains. Considering that we are using a common taxonomy to represent emotions in both domains, we have a typical <italic>transductive transfer learning</italic> (<italic>tTL</italic>) setting [<xref ref-type="bibr" rid="pone.0179289.ref046">46</xref>]—the source (<italic>T</italic><sub><italic>S</italic></sub>) and target (<italic>T</italic><sub><italic>T</italic></sub>) tasks are the same (<italic>T</italic><sub><italic>S</italic></sub> = <italic>T</italic><sub><italic>T</italic></sub>; regression of emotion dimensions), but the source (<italic>D</italic><sub><italic>S</italic></sub>) and target (<italic>D</italic><sub><italic>T</italic></sub>) domains are different (<italic>D</italic><sub><italic>S</italic></sub> ≠ <italic>D</italic><sub><italic>T</italic></sub>). Specifically, given that in our case the source and target feature spaces are the same, but the marginal probability distributions of the inputs for each domain are different, our task is related to domain adaptation [<xref ref-type="bibr" rid="pone.0179289.ref047">47</xref>]. Given <italic>D</italic><sub><italic>S</italic></sub> and <italic>D</italic><sub><italic>T</italic></sub> (with respective learning tasks <italic>T</italic><sub><italic>S</italic></sub> and <italic>T</italic><sub><italic>T</italic></sub>), <italic>tTL</italic> aims to improve the learning of the target predictive function <italic>f</italic><sub><italic>T</italic></sub>(.) in <italic>D</italic><sub><italic>T</italic></sub> using the knowledge in <italic>D</italic><sub><italic>S</italic></sub> and <italic>T</italic><sub><italic>S</italic></sub> (<italic>D</italic><sub><italic>S</italic></sub> ≠ <italic>D</italic><sub><italic>T</italic></sub> and <italic>T</italic><sub><italic>S</italic></sub> = <italic>T</italic><sub><italic>T</italic></sub>).</p>
<p>For the transfer of knowledge related to emotion decoding from music (or speech) to speech (or music), our aim is to find adequate feature representations that maximise domain convergence and the regression model error. In particular, in our task only a relatively small amount of labelled data is available in the source domain <italic>D</italic><sub><italic>S</italic></sub> and no labelled data exists on the target domain <italic>D</italic><sub><italic>T</italic></sub>. Typically, under these settings, transferring knowledge of features representations (aka feature-representation-transfer) is achieved through unsupervised learning frameworks [<xref ref-type="bibr" rid="pone.0179289.ref046">46</xref>]. Several techniques have been proposed for unsupervised feature-representation-transfer (e.g., see [<xref ref-type="bibr" rid="pone.0179289.ref046">46</xref>] for a detailed overview). In this paper, we focus on representation learning [<xref ref-type="bibr" rid="pone.0179289.ref048">48</xref>] in order to learn transformations of the data in the source and target feature domains that ease the extraction of useful representations for inputting to the emotion regression models. In particular, we focus on directly learning a parametric map from the features to their representations which, unlike learned representations based on latent variables, does not require that the posterior distribution is known and allows extracting stable deterministic numerical feature values [<xref ref-type="bibr" rid="pone.0179289.ref048">48</xref>]. One such class of methods is the regularised version of the auto-encoder (AE) framework [<xref ref-type="bibr" rid="pone.0179289.ref049">49</xref>] that forces the AE to develop more general internal feature representations (as insensitive as possible with respect to changes in input). Popular techniques implementing this method are Sparse Auto-Encoders [<xref ref-type="bibr" rid="pone.0179289.ref050">50</xref>] and Denoising Auto-Encoders (DAE) [<xref ref-type="bibr" rid="pone.0179289.ref051">51</xref>], with the latter often associated with better outcomes [<xref ref-type="bibr" rid="pone.0179289.ref052">52</xref>]. DAEs are trained to reconstruct a clean, ‘repaired’ input from its corrupted version [<xref ref-type="bibr" rid="pone.0179289.ref051">51</xref>], that is, to denoise corrupted versions of their inputs. In so doing, the DAE must capture the structure of the input distribution in order to reduce the effect of the corruption process, and as a consequence learn (useful) higher level representations. This method has been shown to be efficient in a wide range of tasks (see [<xref ref-type="bibr" rid="pone.0179289.ref052">52</xref>]), including, recently, in SER (e.g., [<xref ref-type="bibr" rid="pone.0179289.ref045">45</xref>]).</p>
</sec>
</sec>
<sec id="sec008">
<title>Databases</title>
<p>In our TL test-bed, we established a (realistic) scenario in which a large pool of unlabelled data is pervasively available in both domains (e.g., speech and music from online sources), but only a relatively small amount of labelled data is available in the source domain <italic>D</italic><sub><italic>S</italic></sub> and none in the target domain <italic>D</italic><sub><italic>T</italic></sub>. In our experiments we further establish two sub-scenarios, whereby music is the source domain and speech the target domain, and vice-versa. The next subsections describe the four databases used (two of music and two of speech instances). Our choices for the labelled databases were driven by the quality and reliability of the annotations in order to have a robust set of data for our Machine Learning experiments, as well as their availability which can facilitate reproducibility of this work.</p>
<sec id="sec009">
<title>Speech</title>
<sec id="sec010">
<title>DAE pre-training: Semaine database</title>
<p>The SEMAINE corpus [<xref ref-type="bibr" rid="pone.0179289.ref053">53</xref>] was developed specifically to address the task of achieving emotion-rich interactions, and it is adequate for this task as it comprises a wide range of emotional speech. It includes video and speech recordings of spontaneous interactions between human and emotionally stereotyped ‘characters’ (Prudence, who is even-tempered and sensible; Poppy, who is happy and outgoing; Spike, who is angry and confrontational, and Obadiah, who is sad and depressive). Audio was recorded at 48 kHz with 24 bits per sample. In our experiments, we use a subset of this database (called <italic>Solid-SAL</italic>), which is freely available for scientific research purposes (see <ext-link ext-link-type="uri" xlink:href="http://semaine-db.eu" xlink:type="simple">http://semaine-db.eu</ext-link>), and it was used as the official database in the First International Audio/Visual Emotion Challenge (AVEC 2011) [<xref ref-type="bibr" rid="pone.0179289.ref054">54</xref>]. In this subset of SEMAINE, the characters are role-played by human operators. <xref ref-type="table" rid="pone.0179289.t002">Table 2</xref> shows the details of <italic>Solid-SAL</italic> database.</p>
<table-wrap id="pone.0179289.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0179289.t002</object-id>
<label>Table 2</label>
<caption>
<title>Overview of the speech databases used in this paper (f: female).</title>
</caption>
<alternatives>
<graphic id="pone.0179289.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0179289.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Database</th>
<th align="center">SEMAINE</th>
<th align="center">RECOLA</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Number of recordings</td>
<td align="center">95</td>
<td align="center">23</td>
</tr>
<tr>
<td align="center">Number of speakers</td>
<td align="center">28 (14 f)</td>
<td align="center">23 (12 f)</td>
</tr>
<tr>
<td align="center">Total duration (h:m:s)</td>
<td align="center">7:30:29</td>
<td align="center">1:55:00</td>
</tr>
<tr>
<td align="center">Total number of frames (1 s long)</td>
<td align="center">27 029</td>
<td align="center">6 900</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec011">
<title>Regression task: RECOLA database</title>
<p>The RECOLA database [<xref ref-type="bibr" rid="pone.0179289.ref055">55</xref>] (the official database of AVEC 2015, the 5th International Audio/Visual Emotion Challenge and Workshop [<xref ref-type="bibr" rid="pone.0179289.ref056">56</xref>]) consists of 9.5 hours of multimodal recordings (audio, video, and peripheral physiological activity) of spontaneous dyadic interactions between French adults whom were performing a remote collaborative task. Initially, each participant was asked to rank a number of items according to their importance for the survival of a group of crew members in a deserted and hostile area after a plain crash. Then, they had to discuss their ratings with another peer and reach a consensus on how to survive in the proposed disaster scenario (mean duration of the interactions was circa 15 minutes).</p>
<p>In this paper, we use the RECOLA-Audio module which consists of the audio recordings of each participant in the dyadic phase of the task. In particular, we use the non-segmented high-quality audio signals (WAV format, 44.1kHz, 16bits), obtained through unidirectional headset microphones, of the first five minutes of each interaction. These sections contain the most emotionally expressive moment of the interactions, and include annotations of perceived emotions by six French speakers. Annotations consist of time-continuous ratings of the level of Arousal and Valence dimensions of emotion perceived by each rater while seeing and listening the audio-visual recordings of each participant task.</p>
<p>The details of the sound files used are shown in <xref ref-type="table" rid="pone.0179289.t002">Table 2</xref>. Given that not all participants consented to the release of their recordings, and the authors only released part of the data from the annotated sound files, the total number of instances is 23. The time frame length used is this work is 1s given that no changes in time-continuous annotations are expected at lower rates [<xref ref-type="bibr" rid="pone.0179289.ref019">19</xref>].</p>
<p>It should be noted that the RECOLA database contains speech in the French language, whereas SEMAINE is an English database. In that respect it should noted that we are focusing on the non-linguistic aspects of speech (prosody) and that cross-cultural research has demonstrated that emotions conveyed by the human voice (and by music as well) can be communicated accurately across cultures and that convincing evidence points to similar prosodic codes underlying this phenomenon (e.g., [<xref ref-type="bibr" rid="pone.0179289.ref057">57</xref>, <xref ref-type="bibr" rid="pone.0179289.ref058">58</xref>]. This is apparent, for instance, in our capacity to understand how other feel even if they speak an unfamiliar language.</p>
</sec>
</sec>
<sec id="sec012">
<title>Music</title>
<sec id="sec013">
<title>DAE pre-training: MediaEval 2014 (ME14)</title>
<p>The MediaEval “Emotion in Music” task is dedicated to the estimation of Arousal and Valence scores continuously in time and value for song excerpts from the Free Music Archive. The whole corpus (development and test sets for the 2014 challenge) includes 1 744 songs belonging to 11 musical styles—Soul, Blues, Electronic, Rock, Classical, Hip-Hop, International, Folk, Jazz, Country, and Pop (maximum of five songs per artist). The numerical details of the <italic>ME14</italic> corpus are shown in <xref ref-type="table" rid="pone.0179289.t003">Table 3</xref>.</p>
<table-wrap id="pone.0179289.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0179289.t003</object-id>
<label>Table 3</label>
<caption>
<title>Overview of the Mediaeval “Emotion in Music” task corpus (<italic>ME14</italic>) and database derived from previous Music Psychology empirical studies (<italic>MP</italic>).</title>
</caption>
<alternatives>
<graphic id="pone.0179289.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0179289.t003" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Database</th>
<th align="center">ME14</th>
<th align="center">MP</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Number of pieces</td>
<td align="center">1000</td>
<td align="center">20</td>
</tr>
<tr>
<td align="center">Number of genres</td>
<td align="center">11</td>
<td align="center">8</td>
</tr>
<tr>
<td align="center">Total duration (h:m:s)</td>
<td align="center">8:19:59</td>
<td align="center">1:28:50</td>
</tr>
<tr>
<td align="center">Total number of frames (1 s long)</td>
<td align="center">30 000</td>
<td align="center">5 749</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec014">
<title>Regression task: Compilation of data from Music Psychology studies (MP)</title>
<p>The music database compiled for this study consists of emotionally diverse full music pieces from a variety of musical styles (“Classical” and contemporary Western Art, Baroque, Bossa Nova, Rock, Pop, Heavy Metal, and Film Music). Each piece was administered in the context of controlled laboratory experiments in which the emotional character of each piece was evaluated time-continuously by 35 to 52 participants using a computer mouse to control a cursor on the screen to indicate the continuous level of Arousal and Valence perceived at each moment [<xref ref-type="bibr" rid="pone.0179289.ref010">10</xref>, <xref ref-type="bibr" rid="pone.0179289.ref017">17</xref>, <xref ref-type="bibr" rid="pone.0179289.ref018">18</xref>, <xref ref-type="bibr" rid="pone.0179289.ref059">59</xref>]. In what follows, we describe each study. The numerical information pertaining to the various datasets is summarised in <xref ref-type="table" rid="pone.0179289.t004">Table 4</xref>.</p>
<table-wrap id="pone.0179289.t004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0179289.t004</object-id>
<label>Table 4</label>
<caption>
<title>Music database: Collection of music pieces used in five different Music Psychology studies and employed in our work for the Arousal and Valence regression tasks.</title>
</caption>
<alternatives>
<graphic id="pone.0179289.t004g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0179289.t004" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center">Dataset</th>
<th align="center"><italic>MP</italic><sub><italic>DB</italic><sub>1</sub></sub></th>
<th align="center"><italic>MP</italic><sub><italic>DB</italic><sub>2</sub></sub></th>
<th align="center"><italic>MP</italic><sub><italic>DB</italic><sub>3</sub></sub></th>
<th align="center"><italic>MP</italic><sub><italic>DB</italic><sub>4</sub></sub></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Number of pieces</td>
<td align="center">6</td>
<td align="center">9</td>
<td align="center">8</td>
<td align="center">7</td>
</tr>
<tr>
<td align="center">Number of genres</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">7</td>
</tr>
<tr>
<td align="center">Total duration (m:s)</td>
<td align="center">18:38</td>
<td align="center">23:40</td>
<td align="center">13:11</td>
<td align="center">33:21</td>
</tr>
<tr>
<td align="center">Number of time frames (1s long)</td>
<td align="center">1 118</td>
<td align="center">1 420</td>
<td align="center">791</td>
<td align="center">2 001</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p><italic>MP</italic><sub><italic>DB</italic><sub>1</sub></sub> This subset of our database consists of the data reported in [<xref ref-type="bibr" rid="pone.0179289.ref060">60</xref>], and gently made available by the author. This dataset includes six full (or long excerpts) music pieces ranging from 151 s to 315 s in length (only classical music). Each piece was annotated by 35 participants (14 females). The time series correspondents to each music piece were collected at 1 Hz. The golden standard for each piece was computed by averaging the individual time series across all raters.</p>
<p><italic>MP</italic><sub><italic>DB</italic><sub>2</sub></sub> The dataset by [<xref ref-type="bibr" rid="pone.0179289.ref018">18</xref>] includes 9 full pieces (43s to 240s long) of classical music (romantic repertoire) annotated by 39 subjects (19 females). Values were recorded every time the mouse was moved with a precision of 1 ms. The resultant timeseries were then resampled (moving average) to a synchronous rate of 1 Hz. The golden standard for each piece was computed by averaging the individual time series across all raters.</p>
<p><italic>MP</italic><sub><italic>DB</italic><sub>3</sub></sub> The music in the third study [<xref ref-type="bibr" rid="pone.0179289.ref010">10</xref>] consists of 8 pieces of film music (84 s to 130 s long) taken from the late 20<sup>th</sup> century Hollywood film repertoire. Emotion ratings were given by 52 participants (26 females). The annotation procedure, data processing, and golden standard calculations were identical to <italic>MP</italic><sub><italic>DB</italic><sub>2</sub></sub>.</p>
<p><italic>MP</italic><sub><italic>DB</italic><sub>4</sub></sub> The music used in [<xref ref-type="bibr" rid="pone.0179289.ref059">59</xref>] includes seven music pieces (127 s to 502 s in length) of heterogeneous styles (e.g., Rock, Pop, Heavy Metal, Classical). Each music piece was annotated by 38 participants (29 females) using an identical methodology to <italic>MP</italic><sub><italic>DB</italic><sub>2</sub></sub> and <italic>M</italic><sub><italic>DB</italic><sub>3</sub></sub>. Data processing and golden standard calculations were also identical.</p>
</sec>
</sec>
</sec>
<sec id="sec015">
<title>Experimental setup, procedure and measures</title>
<p>In the next subsections we describe our experimental procedure. The contents are organised in two subsections. In the first, we attempt to use DAEs for feature-representation-transfer with the aim of improving the cross-modal generalisation. The DAEs are trained on the large, unlabelled music and speech databases (<italic>SEMAINE</italic> and <italic>ME</italic>14). In the second part, we conduct the regression experiments on the target tasks (predicting Arousal and Valence) using a three-part comparative framework. First, we created a baseline to allow establishing a reference to compare against the performance of TL. The baseline consists of intra domain models, i.e., models developed and tested only on music or speech. Second, given that we use the same feature set for both domains, we establish a basic cross-domain setup whereby a model is trained with data from one modality and tested with data from another. Third, we attempt to use DAEs for feature-representation-transfer with the aim of improving the cross-modal generalisation. In all cases we conducted bidirectional experiments, that is, transfer learning from music to and from speech.</p>
<sec id="sec016">
<title>Pre-training of the first layer</title>
<p>The first step was to pre-train the first layer of the network using a DAE framework. The DAE takes as input time-continuous feature vectors of acoustic descriptors, and attempt to reproduce these time series at the output units from corrupted version of the inputs. In this paper, we inject Gaussian noise with zero mean and variable standard deviation (<italic>σ</italic>; see details below) at the input layer (only during training) in order to generate a corrupted version of inputs during training (see [<xref ref-type="bibr" rid="pone.0179289.ref061">61</xref>] for other methods of corruption). In all tests and trials the DAEs were trained on the <italic>ME</italic>14 (music) and <italic>SEMAINE</italic> (speech) databases in an unsupervised fashion. Also, we computed 10 trials (repetitions) in each test condition, each with randomised initial weights in the range [-0.1, 0.1] in order to obtain a more robust performance estimation. Training was stopped once there was no improvement after 20 consecutive learning iterations, or a maximum of 300 iterations was reached. The architecture implemented for each DAE consisted of an input layer composed of 260 units with sigmoidal activation functions, one hidden layer composed of a variable number of LSTM blocks (#<italic>HB</italic><sub><italic>layer</italic><sub>1</sub></sub>), and one output layer composed of 260 units also with sigmoidal activation functions.</p>
<p>In order to understand how the transfer learning process is affected by the size of the hidden layer (the dimensionality of the feature vector to be transferred) and the <italic>σ</italic> of the noise injected in the input layer (which will impact the generalisability of the feature vector), we trained different DAEs with a variable number of hidden layer sizes (#<italic>HB</italic><sub><italic>layer</italic><sub>1</sub></sub> ∈ [50, 75, 100, 150, 200]) and different noise parameters. Each combination of #<italic>HB</italic><sub><italic>layer</italic><sub>1</sub></sub> and <italic>σ</italic> will later be used tested in the regression experiments (as mentioned, for each parameter set combination (#<italic>HB</italic><sub><italic>layer</italic><sub>1</sub></sub>, <italic>σ</italic> and <italic>lr</italic><sub><italic>dae</italic></sub>), we computed 10 independent trials of the models with randomised initial weights in the range [-0.1, 0.1]).</p>
<p>The only parameter optimised in the pre-training phase was the learning rate (<italic>lr</italic><sub><italic>dae</italic></sub>). This optimisation was done in two steps. First, for each value of #<italic>HB</italic><sub><italic>layer</italic><sub>1</sub></sub>, we trained the model with different learning rates (<italic>lr</italic><sub><italic>dae</italic></sub> ∈ [1 × 10<sup>−7</sup>, 1 × 10<sup>−6</sup>, 5 × 10<sup>−6</sup>]; <italic>momentum</italic> = 0.9 and <italic>σ</italic> = 0.1 for all tests) (again this process was repeated 10 times for each #<italic>HB</italic><sub><italic>layer</italic><sub>1</sub></sub>/<italic>lr</italic><sub><italic>dae</italic></sub> pair). The optimal <italic>lr</italic><sub><italic>dae</italic></sub> for each #<italic>HB</italic><sub><italic>layer</italic><sub>1</sub></sub> was determined by selecting the lowest average reconstruction error (computed using the <italic>rmse</italic>) over the 10 trials. Second, for each #<italic>HB</italic><sub><italic>layer</italic><sub>1</sub></sub> (using the optimised <italic>lr</italic><sub><italic>dae</italic></sub> for each layer size), we varied the characteristics of the noise injected at the inputs (<italic>σ</italic> ∈ [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]) (also repeated 10 times for each #<italic>HB</italic><sub><italic>layer</italic><sub>1</sub></sub>/<italic>σ</italic> combination). As a result, we created 10 different DAEs (each with an optimised learning rate) that correspond to 30 different initialisation of the first layer to be used in the regression tests (each with 10 repeated trials).</p>
<p>Note that the size of the hidden layers is smaller than the number of inputs features given that we expect redundancy amongst features. By setting the hidden layer to be smaller than the input layer, our goal is to implicitly reduce the dimensionality of the feature space by eliminating redundancy and improving generalisation performance.</p>
</sec>
<sec id="sec017">
<title>Models training on the target task</title>
<p>We used a multi-task learning framework for the joint learning of Arousal and Valence time-continuous values. In all tests, the network’s basic architecture consisted of 260 inputs (the number of acoustic features), two hidden layers (with LSTM memory blocks) of variable size, and two output units with linear activation functions (corresponding to Arousal and Valence outputs). In all cases, mirroring the pre-training phase, we computed ten trials (repetitions) in each test condition, each with randomised initial weights in the range [-0.1, 0.1]. We conducted three separate blocks of experiments—intra-domain (<italic>ID</italic>), cross-domain (<italic>CD</italic>), and cross-domain with TL (<italic>CD</italic><sub><italic>TL</italic></sub>):
<list list-type="bullet">
<list-item>
<p><italic>ID</italic> experiments correspond to the traditional approaches—separate models were created for each domain (music and speech) using the extracted acoustic features. During training, for each domain, we used a leave-one-out nested cross-validation schema (<italic>N</italic><sub><italic>music</italic></sub> = 33 and <italic>N</italic><sub><italic>speech</italic></sub> = 23). In each fold, <italic>N</italic> − 2 instances are used for training (training set), one is used for parameter optimisation (validation set), and another instance is left out for testing (test set). The best parameter set is determined as the average performance over the left-out instances of each fold. Once the best model was found, its performance to unseen data was determined as the average performance over the test instances of each fold.</p>
</list-item>
<list-item>
<p><italic>CD</italic> experiments focused on developing the model with data from one domain (music or speech) and directly tested on the other domain (speech or music, respectively). In the model development and optimisation phase, we used a leave-one-out cross-validation strategy. The performance for each condition was estimated as the average performance over the left-out instances of each training fold. The optimised model was then tested on cross-domain data (music if trained on speech, and vice-versa).</p>
</list-item>
<list-item>
<p><italic>CD</italic><sub><italic>TL</italic></sub> experiments were also focused on creating the models with data from the source domain (music or speech) and test them on the target domain (speech or music, respectively). Again, we used a leave-one-out cross-validation strategy in the development phase using the source domain database, and the model performance was estimated on the database of the target domain. The key difference between <italic>CD</italic><sub><italic>TL</italic></sub> and <italic>CD</italic> is that the first layer was not randomly initialised. Instead, we initialised the first layer with the weights and biases matrix of the DAEs described in the previous sections. Tests were conducted with each (<italic>lr</italic><sub><italic>dae</italic></sub>, #<italic>HB</italic><sub><italic>layer</italic><sub>1</sub></sub>) pair.</p>
</list-item>
</list></p>
<sec id="sec018">
<title>Architecture and hyper-parameter optimisation</title>
<p>Each sequence (music or speech instance) was presented randomly to the model during training, and both the input and output data were standardised to the mean zero and unit variance (using the parameters of the correspondent training sets in each cross-validation fold).</p>
<p>For all models, we optimised the number of hidden LSTM blocks in each layer using a learning rate of 1<italic>e</italic><sup>−6</sup> (no Gaussian noise was applied to the inputs, and a momentum of 0.9 was used). The size of the first hidden layer was varied between 50 and 200 neurons (#<italic>HB</italic><sub><italic>layer</italic><sub>1</sub></sub> ∈ [50, 75, 100, 150, 200]). As noted, in the case of the <italic>CD</italic><sub><italic>TL</italic></sub> experiments, we initialised the weight and biases matrices of the first layer with the parameters of the previously trained DAEs (note that the sizes of the first hidden layer are the same). In practice, this was done by removing the output layer of the previously trained DAEs, and adding an extra hidden layer and a new output layer (composed by two units: Arousal and Valence). These weights were kept fixed while training the models on the target task. Given that we trained 6 DAEs with the same number of hidden units (due to the various <italic>σ</italic><sub><italic>DAE</italic></sub> used), unlike the <italic>ID</italic> and <italic>CD</italic> experiments, we conducted 6 tests (instead of one) per hidden layer in order to test the impact of the <italic>σ</italic><sub><italic>DAE</italic></sub> value used in the pre-training phase.</p>
<p>After determining the #<italic>HB</italic><sub><italic>layer</italic><sub>1</sub></sub> size (or #<italic>HB</italic><sub><italic>layer</italic><sub>1</sub></sub>/<italic>σ</italic><sub><italic>DAE</italic></sub> pair in the case of the <italic>CD</italic><sub><italic>TL</italic></sub> experiments) yielding the best performance on the target tasks, we optimised the size of the second hidden layer on the target task (#<italic>HB</italic><sub><italic>layer</italic><sub>2</sub></sub> ∈ [<xref ref-type="bibr" rid="pone.0179289.ref002">2</xref>, <xref ref-type="bibr" rid="pone.0179289.ref004">4</xref>, <xref ref-type="bibr" rid="pone.0179289.ref006">6</xref>, <xref ref-type="bibr" rid="pone.0179289.ref008">8</xref>, <xref ref-type="bibr" rid="pone.0179289.ref010">10</xref>]). Then, using the optimal architectures (#<italic>HB</italic><sub><italic>layer</italic><sub>1</sub></sub> and #<italic>HB</italic><sub><italic>layer</italic><sub>2</sub></sub> sizes) we optimised the learning rate (<italic>LR</italic><sub><italic>model</italic></sub> ∈ [1 × 10<sup>−7</sup>, 1 × 10<sup>−6</sup>, 5 × 10<sup>−6</sup>]). Finally, we injected Gaussian noise with different standard deviations (<italic>σ</italic><sub><italic>model</italic></sub>; mean was 0) in the input layer to regularise the training process (<italic>σ</italic><sub><italic>model</italic></sub> ∈ [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]). In addition to the input noise, we used an early stopping strategy to avoid over-fitting the training data. The training process was halted after 20 iterations without improvement to the performance of the validation set, and a maximum of 500 iterations of the learning algorithm was allowed.</p>
</sec>
</sec>
</sec>
<sec id="sec019">
<title>Performance measures</title>
<p>We use three measures to quantify the models’ performance: an index of precision—the root mean squared error (<italic>rmse</italic>), a measure of similarity—Pearson’s linear correlation coefficient (<italic>r</italic>), and a measure that combines both—the Concordance Correlation Coefficient (<italic>ccc</italic>; [<xref ref-type="bibr" rid="pone.0179289.ref062">62</xref>]). Given that the trade-off between precision and similarity of the temporal paths is fundamental for an accurate estimation of unfolding of the emotional expression (particularly in the case of music since in speech fast changes in emotional tone are less likely to occur in most circumstances), we optimised the model using the <italic>ccc</italic> cost function. We report <italic>rmse</italic> and <italic>r</italic> for clarity, and because it allows to better interpret the results given that <italic>ccc</italic> is a compound measure. The <italic>rmse</italic> is measured in the same units as the data, and should therefore be interpreted carefully. Given that model outputs (as well as the golden standard) vary between -1 and 1, an informative way is to consider a normalised version of the <italic>rmse</italic>, which can be achieved by diving the original <italic>rmse</italic> values by the range of observed values (in our case 1 − (−1) = 2. This value is an intuitive measure of the magnitude of the error. The interpretation of <italic>r</italic> is straightforward—larger values indicate a better linear correlation between model outputs and target values. Our analysis will be guided by the interpretation of <italic>ccc</italic>, <italic>r</italic> and <italic>rmse</italic>. <italic>ccc</italic> provides a global indication of the models’ performances in terms of precision (also depicted by <italic>rmse</italic>) and similarity (measured by <italic>r</italic>) of the affective trajectories for each piece. In order to determine whether the different conditions yielded results that are significantly different from each other, we conducted statistical comparisons between the performances in each experiment using Student’s <italic>t</italic>-tests with Bonferroni corrections for multiple comparisons (three tests, therefore the p-values were multiplied by three).</p>
</sec>
<sec id="sec020" sec-type="results">
<title>Results</title>
<sec id="sec021">
<title>DAE pre-training</title>
<p>In <xref ref-type="table" rid="pone.0179289.t005">Table 5</xref>, we show the statistics on the reconstruction errors of the DAEs with different number of hidden blocks and noise variability (<italic>σ</italic>) applied to the inputs. These results were obtained with the optimised learning rate for each hidden layer size (as described in the previous section; values also indicated in the table: <italic>LR</italic><sub><italic>dae</italic></sub>). As it can be observed, the DAEs could reproduce very well the input features for all parameters tested. The optimised learning rate tended to increase for higher values of <italic>σ</italic>, as well as the <italic>rmse</italic>. <italic>r</italic> tended to decrease for increasing <italic>σ</italic>. The best reproductions of the inputs were achieved for smaller sizes of the hidden layer.</p>
<table-wrap id="pone.0179289.t005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0179289.t005</object-id>
<label>Table 5</label>
<caption>
<title>DAE performance for different hidden layer sizes, and various variances of the Gaussian noise <italic>σ</italic> applied to the inputs during training.</title>
<p>For each combination of <italic>σ</italic> and #<italic>LSTMblocks</italic>, we indicate the mean-squared error <italic>rmse</italic> and Person’s linear correlation coefficient <italic>r</italic> (<italic>rmse</italic> / <italic>r</italic> for each cell).</p>
</caption>
<alternatives>
<graphic id="pone.0179289.t005g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0179289.t005" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center" rowspan="2"><italic>LR</italic><sub><italic>dae</italic></sub></th>
<th align="center" rowspan="2"><italic>σ</italic></th>
<th align="center" colspan="5">#<italic>LSTMblocks</italic></th>
</tr>
<tr>
<th align="center">50</th>
<th align="center">75</th>
<th align="center">100</th>
<th align="center">150</th>
<th align="center">200</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">5 × 10<sup>−7</sup></td>
<td align="char" char=".">0.1</td>
<td align="center">27 / 0.808</td>
<td align="center">21 / 0.880</td>
<td align="center">16 / 0.926</td>
<td align="center">12 / 0.950</td>
<td align="center">9 / 0.968</td>
</tr>
<tr>
<td align="center">1 × 10<sup>−6</sup></td>
<td align="char" char=".">0.2</td>
<td align="center">28 / 0.806</td>
<td align="center">21 / 0.881</td>
<td align="center">16 / 0.928</td>
<td align="center">13 / 0.950</td>
<td align="center">10 / 0.969</td>
</tr>
<tr>
<td align="center">1 × 10<sup>−6</sup></td>
<td align="char" char=".">0.3</td>
<td align="center">28 / 0.804</td>
<td align="center">21 / 0.881</td>
<td align="center">17 / 0.921</td>
<td align="center">14 / 0.942</td>
<td align="center">13 / 0.955</td>
</tr>
<tr>
<td align="center">1 × 10<sup>−6</sup></td>
<td align="char" char=".">0.4</td>
<td align="center">28 / 0.799</td>
<td align="center">22 / 0.872</td>
<td align="center">18 / 0.909</td>
<td align="center">16 / 0.930</td>
<td align="center">14 / 0.943</td>
</tr>
<tr>
<td align="center">1 × 10<sup>−6</sup></td>
<td align="char" char=".">0.5</td>
<td align="center">29 / 0.791</td>
<td align="center">22 / 0.863</td>
<td align="center">19 / 0.899</td>
<td align="center">17 / 0.916</td>
<td align="center">16 / 0.928</td>
</tr>
<tr>
<td align="center">5 × 10<sup>−6</sup></td>
<td align="char" char=".">0.6</td>
<td align="center">29 / 0.785</td>
<td align="center">23 / 0.851</td>
<td align="center">20 / 0.887</td>
<td align="center">19 / 0.902</td>
<td align="center">18 / 0.912</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec022">
<title>Regression task</title>
<p>In <xref ref-type="table" rid="pone.0179289.t006">Table 6</xref>, we summarise the complete set of results obtained for the MER (left) and SER (right) regression tasks in the three experimental conditions: intra-domain (<italic>M</italic>2<italic>M</italic> and <italic>S</italic>2<italic>S</italic>), cross-domain (<italic>M</italic>2<italic>S</italic> and <italic>S</italic>2<italic>M</italic>), and cross-domain with TL (<italic>M</italic>2<italic>S</italic><sub><italic>TL</italic></sub> and <italic>S</italic>2<italic>M</italic><sub><italic>TL</italic></sub>). <italic>D</italic><sub><italic>S</italic></sub>2<italic>D</italic><sub><italic>T</italic></sub> indicates the direction of knowledge transfer—<italic>D</italic><sub><italic>S</italic></sub> is the source domain (<italic>M</italic>usic or <italic>S</italic>peech), and <italic>D</italic><sub><italic>T</italic></sub> is the target domain (<italic>S</italic>peech or <italic>M</italic>usic). The performance figures shown correspond to the average of each measure across the five best trials in each experimental condition, and are shown separately for Arousal and Valence for better interpretability. In the table, we also indicate the values of the hyper-parameters optimised in the development phase.</p>
<table-wrap id="pone.0179289.t006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0179289.t006</object-id>
<label>Table 6</label>
<caption>
<title>Results obtained for the test sets of the various experimental conditions (Music on the left side, and Speech on the right).</title>
<p>The performance of the various approaches is quantified using the root-mean-squared-error (<italic>rmse</italic>), Pearson’s linear correlation coefficient <italic>r</italic>, and the concordance correlation coefficient <italic>ccc</italic>. For each experiment, the performance measures were averaged across the five best trials. <italic>ID</italic>: intra-domain (<italic>M</italic>2<italic>M</italic> and <italic>S</italic>2<italic>S</italic>, baseline models trained and tested on the same database); <italic>CD</italic>: cross-domain (<italic>S</italic>2<italic>M</italic> and <italic>M</italic>2<italic>S</italic>); <italic>CD</italic><sub><italic>TL</italic></sub>, cross-domain transfer learning (<italic>S</italic>2<italic>M</italic> and <italic>M</italic>2<italic>S</italic>); <italic>D</italic><sub><italic>S</italic></sub> ⇒ <italic>D</italic><sub><italic>T</italic></sub> (where <italic>D</italic><sub><italic>S</italic></sub> is the source domain, and <italic>D</italic><sub><italic>T</italic></sub> the target domain).</p>
</caption>
<alternatives>
<graphic id="pone.0179289.t006g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0179289.t006" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center" colspan="2" rowspan="3"/>
<th align="center" colspan="3">MUSIC</th>
<th align="center" colspan="3">SPEECH</th>
</tr>
<tr>
<th align="center">Intra-Domain</th>
<th align="center" colspan="2">Cross-Domain</th>
<th align="center">Intra-Domain</th>
<th align="center" colspan="2">Cross-Domain</th>
</tr>
<tr>
<th align="center"><italic>M</italic>2<italic>M</italic></th>
<th align="center"><italic>S</italic>2<italic>M</italic></th>
<th align="center"><italic>S</italic>2<italic>M</italic><sub><italic>TL</italic></sub></th>
<th align="center"><italic>S</italic>2<italic>S</italic></th>
<th align="center"><italic>M</italic>2<italic>S</italic></th>
<th align="center"><italic>M</italic>2<italic>S</italic><sub><italic>TL</italic></sub></th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" colspan="2">First layer size</td>
<td align="center">150</td>
<td align="center">200</td>
<td align="center">200</td>
<td align="center">150</td>
<td align="center">200</td>
<td align="center">150</td>
</tr>
<tr>
<td align="center" colspan="2">Second layer size</td>
<td align="center">10</td>
<td align="center">10</td>
<td align="center">10</td>
<td align="center">10</td>
<td align="center">8</td>
<td align="center">10</td>
</tr>
<tr>
<td align="center" colspan="2">Learning rate</td>
<td align="center">5 × 10<sup>−6</sup></td>
<td align="center">5 × 10<sup>−6</sup></td>
<td align="center">5 × 10<sup>−6</sup></td>
<td align="center">5 × 10<sup>−6</sup></td>
<td align="center">5 × 10<sup>−6</sup></td>
<td align="center">5 × 10<sup>−6</sup></td>
</tr>
<tr>
<td align="center" colspan="2">Gaussian noise <italic>σ</italic></td>
<td align="center">0.5</td>
<td align="center">0.5</td>
<td align="center">0.4</td>
<td align="center">0.4</td>
<td align="center">0.1</td>
<td align="center">0.1</td>
</tr>
<tr>
<td align="center" rowspan="3">Arousal</td>
<td align="center"><italic>rmse</italic></td>
<td align="center"><underline>0.194</underline></td>
<td align="center">0.223</td>
<td align="center">0.223</td>
<td align="center"><underline>0.105</underline></td>
<td align="center">0.142</td>
<td align="center">0.141</td>
</tr>
<tr>
<td align="center"><italic>r</italic></td>
<td align="center"><underline>0.517</underline></td>
<td align="center"><underline>0.524</underline></td>
<td align="center"><underline>0.542</underline></td>
<td align="center"><underline>0.821</underline></td>
<td align="center">0.693</td>
<td align="center">0.719</td>
</tr>
<tr>
<td align="center"><italic>ccc</italic></td>
<td align="center"><bold>0.317</bold></td>
<td align="center"><bold>0.276</bold></td>
<td align="center"><bold>0.269</bold></td>
<td align="center"><bold>0.749</bold></td>
<td align="center">0.545</td>
<td align="center">0.567</td>
</tr>
<tr>
<td align="center" rowspan="3">Valence</td>
<td align="center"><italic>rmse</italic></td>
<td align="center"><underline>0.265</underline></td>
<td align="center"><underline>0.260</underline></td>
<td align="center"><underline>0.265</underline></td>
<td align="center"><underline>0.109</underline></td>
<td align="center">0.143</td>
<td align="center">0.151</td>
</tr>
<tr>
<td align="center"><italic>r</italic></td>
<td align="center">0.175</td>
<td align="center"><underline>0.256</underline></td>
<td align="center">0.218</td>
<td align="center"><underline>0.448</underline></td>
<td align="center">0.228</td>
<td align="center">0.246</td>
</tr>
<tr>
<td align="center"><italic>ccc</italic></td>
<td align="center">0.090</td>
<td align="center"><bold>0.133</bold></td>
<td align="center">0.117</td>
<td align="center"><bold>0.332</bold></td>
<td align="center">0.164</td>
<td align="center">0.181</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Before describing the results pertaining to the focus of this article (the comparisons between the experimental conditions), it is important to make some considerations about the quality of the predictions. As it can be observed in <xref ref-type="table" rid="pone.0179289.t006">Table 6</xref>, results show very good fits to the Arousal data for both Music and Speech experiments, and moderate fits to Valence. Indeed, in relation to Arousal, <italic>r</italic> &gt; 0.5 in all experiments which suggest an excellent fit to the data. Furthermore, all <italic>rmse</italic>’s are smaller than 0.223, which, considering that the range of the outputs is [-1, 1], corresponds to a maximum normalised <italic>rmse</italic> of approximately 11% (0.223/2), and therefore also an excellent precision. The results concerning Valence are clearly less positive, which is not surprising given the fact that Valence is harder to be perceived by people and automatically estimated (see, for instance, [<xref ref-type="bibr" rid="pone.0179289.ref010">10</xref>, <xref ref-type="bibr" rid="pone.0179289.ref021">21</xref>]). This is particularly evident in terms of similarity (<italic>r</italic> can be as low as 0, 175), but not so accentuated in terms of precision (the worst normalised <italic>rmse</italic> was 13%, which is similar to Arousal). Generally, these results indicate a good fit the data in both domains, and confirm our expectations in terms of modelling emotional responses to music and speech using acoustic descriptors (in this case the ComPaRe 2013 feature set).</p>
<sec id="sec023">
<title>Music as the target domain</title>
<p>We focus now on the statistical analysis of the experiments where Music was used as the target domain, i.e., <italic>M</italic>2<italic>M</italic>, <italic>S</italic>2<italic>M</italic> and <italic>S</italic>2<italic>M</italic><sub><italic>TL</italic></sub>).</p>
<p>In relation to Arousal, results show that all conditions achieved statistically the same performance in terms of <italic>ccc</italic> and <italic>r</italic> (<italic>p</italic> &gt; 0.05 in all cases). In terms of precision, the performance of the cross-domain models was slightly worse than the model only trained on music (<italic>t</italic>(<italic>M</italic>, <italic>S</italic>2<italic>M</italic>) = −3.96, <italic>p</italic> = 0.004, <italic>t</italic>(<italic>M</italic>, <italic>S</italic>2<italic>M</italic><sub><italic>TL</italic></sub>) = −4.12, <italic>p</italic> = 0.003).</p>
<p>Regarding Valence, the two cross-modal models performed generally better than the baseline model. Indeed, the statistical analysis of the <italic>ccc</italic> values showed that the performances of <italic>S</italic>2<italic>M</italic> and <italic>S</italic>2<italic>M</italic><sub><italic>TL</italic></sub> were significantly higher than the baseline model (<italic>t</italic>(<italic>M</italic>, <italic>S</italic>2<italic>M</italic>) = −3.84, <italic>p</italic> = 0.005 and <italic>t</italic>(<italic>M</italic>, <italic>S</italic>2<italic>M</italic><sub><italic>TL</italic></sub>) = −2.86, <italic>p</italic> = 0.021). Furthermore, the <italic>S</italic>2<italic>M</italic> model performed better than <italic>S</italic>2<italic>M</italic><sub><italic>TL</italic></sub> (<italic>t</italic>(<italic>S</italic>2<italic>M</italic>, <italic>S</italic>2<italic>M</italic><sub><italic>TL</italic></sub>) = 2.36, <italic>p</italic> = 0.046), and therefore yielded the best performance. There were no differences in terms of precision, but the <italic>S</italic>2<italic>M</italic> lead to statistically better results than the <italic>M</italic>2<italic>M</italic> model in terms of <italic>r</italic> (<italic>t</italic>(<italic>M</italic>, <italic>S</italic>2<italic>M</italic>) = −3.51, <italic>p</italic> = 0.008).</p>
<p>In sum, the models trained on speech (with or without the pre-trained layer) delivered statistically the same performance of the baseline model trained and tested on music for Arousal, and the <italic>S</italic>2<italic>M</italic> model delivered the best results for Valence. In Figs <xref ref-type="fig" rid="pone.0179289.g002">2</xref> and <xref ref-type="fig" rid="pone.0179289.g003">3</xref> we show the Arousal and Valence predictions for two music pieces (used here as examples; they do not necessarily represent the best or worst instances). The Arousal and Valence values shown were obtained from the best models in each condition (<italic>M</italic>, <italic>S</italic>2<italic>M</italic> and <italic>S</italic>2<italic>M</italic><sub><italic>TL</italic></sub>). We also represent the respective golden standards.</p>
<fig id="pone.0179289.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0179289.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Time-continuous Arousal and Valence predictions by the intra- (<italic>M</italic>) and cross-domain (<italic>S</italic>2<italic>M</italic>, <italic>S</italic>2<italic>M</italic><sub><italic>TL</italic></sub>) models for <italic>M</italic>inority Report, Main Theme (taken from <italic>M</italic><sub><italic>DB</italic><sub>3</sub></sub>, piece 6 in [<xref ref-type="bibr" rid="pone.0179289.ref010">10</xref>]).</title>
<p>The golden standard is also shown (‘Targets’).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0179289.g002" xlink:type="simple"/>
</fig>
<fig id="pone.0179289.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0179289.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Time-continuous Arousal and Valence predictions by the intra- (<italic>M</italic>) and cross-domain (<italic>S</italic>2<italic>M</italic>, <italic>S</italic>2<italic>M</italic><sub><italic>TL</italic></sub>) models for a representative music piece: <italic>T</italic>he Searchers, Suite (taken from<italic>M</italic><sub><italic>DB</italic><sub>3</sub></sub>, piece 8 in [<xref ref-type="bibr" rid="pone.0179289.ref010">10</xref>]).</title>
<p>The golden standard is also shown (‘Targets’).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0179289.g003" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec024">
<title>Speech as the target domain</title>
<p>We turn now to the experiments where Speech was the target domain. Looking first at Arousal, the baseline model yielded higher <italic>ccc</italic> than the cross-domain models (<italic>t</italic>(<italic>S</italic>, <italic>M</italic>2<italic>S</italic>) = 31.2, <italic>p</italic> &lt; 0.001, <italic>t</italic>(<italic>S</italic>, <italic>M</italic>2<italic>S</italic><sub><italic>TL</italic></sub>) = 14.7, <italic>p</italic> &lt; 0.001). This was also the case both in terms of <italic>r</italic> (<italic>t</italic>(<italic>S</italic>, <italic>M</italic>2<italic>S</italic>) = 7.21, <italic>p</italic> &lt; 0.001; <italic>t</italic>(<italic>S</italic>, <italic>M</italic>2<italic>S</italic><sub><italic>TL</italic></sub>) = 9.59, <italic>p</italic> &lt; 0.001), and <italic>rmse</italic> (<italic>t</italic>(<italic>S</italic>, <italic>M</italic>2<italic>S</italic>) = −9.54, <italic>p</italic> &lt; 0.001; <italic>t</italic>(<italic>S</italic>, <italic>M</italic>2<italic>S</italic><sub><italic>TL</italic></sub>) = −10.7, <italic>p</italic> &lt; 0.001). There were no statistically significant differences between both cross-domain models (<italic>p</italic> &gt; 0.05). The results pertaining Valence are identical. The intra-domain model <italic>S</italic> performed globally better than the cross-domain models (<italic>t</italic>(<italic>S</italic>, <italic>M</italic>2<italic>S</italic>) = 24.2, <italic>p</italic> &lt; 0.001, <italic>t</italic>(<italic>S</italic>, <italic>M</italic>2<italic>S</italic><sub><italic>TL</italic></sub>) = 15.9, <italic>p</italic> &lt; 0.001), and there were no significant differences between the cross-domain models <italic>M</italic>2<italic>S</italic> and <italic>M</italic>2<italic>S</italic><sub><italic>TL</italic></sub>. The differences in the global performance are related to a decreased performance in terms of <italic>r</italic> for both cross-domain models (<italic>t</italic>(<italic>S</italic>, <italic>M</italic>2<italic>S</italic>) = 15.1, <italic>p</italic> &lt; 0.001, <italic>t</italic>(<italic>S</italic>, <italic>M</italic>2<italic>S</italic><sub><italic>TL</italic></sub>) = 16.3, <italic>p</italic> &lt; 0.001), and a decrease in terms of precision (<italic>t</italic>(<italic>S</italic>, <italic>M</italic>2<italic>S</italic>) = −2.43, <italic>p</italic> = 0.041; <italic>t</italic>(<italic>S</italic>, <italic>M</italic>2<italic>S</italic><sub><italic>TL</italic></sub>) = −5.28, <italic>p</italic> = 0.001). In Figs <xref ref-type="fig" rid="pone.0179289.g004">4</xref> and <xref ref-type="fig" rid="pone.0179289.g005">5</xref>, we show the predicted Arousal and Valence by the best models in each condition (<italic>S</italic>, <italic>M</italic>2<italic>S</italic> and <italic>M</italic>2<italic>S</italic><sub><italic>TL</italic></sub>)) against the respective golden standards for two speech samples (once more the two instances selected do not necessarily represent the best or worst instances—they are used for illustrative purposes only).</p>
<fig id="pone.0179289.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0179289.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Time-continuous Arousal and Valence predictions by the intra- (<italic>S</italic>) and cross-domain (<italic>M</italic>2<italic>S</italic>, <italic>M</italic>2<italic>S</italic><sub><italic>TL</italic></sub>) models for a RECOLA speech sample (male speaker; instance id: ‘Recola_P26_GRP13_2_HQ’).</title>
<p>The golden standard is also shown (’Targets’).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0179289.g004" xlink:type="simple"/>
</fig>
<fig id="pone.0179289.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0179289.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Time-continuous Arousal and Valence predictions by the intra- (<italic>S</italic>) and cross-domain (<italic>M</italic>2<italic>S</italic>, <italic>M</italic>2<italic>S</italic><sub><italic>TL</italic></sub>) models for a RECOLA speech sample (male speaker; instance id: ‘Recola_P56_GRP28_2_HQ’).</title>
<p>The golden standard is also shown (’Targets’).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0179289.g005" xlink:type="simple"/>
</fig>
</sec>
</sec>
</sec>
<sec id="sec025" sec-type="conclusions">
<title>Discussion and conclusions</title>
<p>In this article, we focused on time-continuous predictions of emotion in music and speech, and investigating the overlap between these two tasks. Indeed, as discussed at the outset, given the tight relationships between the acoustic codes for emotional expression in music and speech, it is plausible to assume that, the communication of emotion in both domains may be achieved by means of shared acoustic patterns. In this context, we proposed to transfer knowledge from one domain to the other with the aims of exploring the extent of the overlap between domains, and testing whether it is possible to enlarge the amount of data available to develop MER and SER systems by combining both domains. In our experiments, we established a comparative framework including intra- (i.e., models trained and tested on the same modality, either music or speech) and cross-domain experiments (i.e., models trained in one modality and tested on the other). The intra-domain models were used as a baseline. In the cross-domain context, we evaluated two strategies—the direct transfer of knowledge between domains, and the contribution of Transfer Learning techniques, namely feature-representation-transfer based on Denoising Auto Encoders.</p>
<p>There are various relevant conclusions and observations deriving from our results and analyses. Starting with overall performances per domain, we found that the speech regression tasks delivered a much better performance compared to the music task for both affective dimensions (Arousal and Valence). Furthermore, as expected from most previous work on Affective Sciences and Affective Computing, Arousal predictions were more accurate than those of Valence for both domains. The fact that we obtained a better performance for the speech compared to music may simply be the consequence of the datasets used, nonetheless there are at least two other possible interpretations. First, predicting emotion in music may be a more challenging task (especially in relation to Valence). This is plausible to assume for various reasons, including the fact that emotion recognition in music is more stereotyped and listener dependant (e.g., [<xref ref-type="bibr" rid="pone.0179289.ref002">2</xref>]), and that valence can be ambivalent in music (e.g., [<xref ref-type="bibr" rid="pone.0179289.ref063">63</xref>]). Furthermore, emotions expressed in the voice have a functional (biological) role, and are meant to communicate specific emotional meanings, something that does not necessarily happen in the case of music [<xref ref-type="bibr" rid="pone.0179289.ref002">2</xref>]. Second, the music model may be under-performing because the feature set used was initially developed for speech-related tasks, and, albeit commonly and effectively used for music tasks, may be lacking important features for music emotion recognition. In point of fact, in [<xref ref-type="bibr" rid="pone.0179289.ref034">34</xref>] it was shown that, the addition of only a few features related to duration, dissonance (roughness), and sharpness to the feature set used in this paper can significantly improve intra-domain music emotion recognition.</p>
<p>We turn now to our considerations concerned with the central focus of this paper—the comparison between intra- and cross-domain experiments. In the of case of Music as the target domain, we found that the Arousal predictions in all three conditions (<italic>M</italic>2<italic>M</italic>, <italic>S</italic>2<italic>M</italic> and <italic>S</italic>2<italic>M</italic><sub><italic>TL</italic></sub>) were similar, although the cross-domain models were slightly worse in terms of precision. The cross-modal models (<italic>S</italic>2<italic>M</italic> and <italic>S</italic>2<italic>M</italic><sub><italic>TL</italic></sub>) performed significantly better than the baseline model (<italic>M</italic>2<italic>M</italic>) when predicting Valence, and there were no significant differences between both. In relation to Speech as the target domain, for both Arousal and Valence predictions, the intra-domain models (<italic>S</italic>2<italic>S</italic>) performed significantly better than the cross-domain models (<italic>M</italic>2<italic>S</italic> and <italic>M</italic>2<italic>S</italic><sub><italic>TL</italic></sub>), which achieved statistically the same performance. These results evidence two important aspects relating to the transfer of knowledge between domains:
<list list-type="order">
<list-item>
<p>Knowledge transfer (with or without domain adaptation) from speech to music was more successful than in the opposite direction. Indeed, emotions in music could be predicted with models trained on speech or music equally, whereas the speech model trained on speech data outperformed those trained on music.</p>
</list-item>
<list-item>
<p>Knowledge transfer approaches with and without domain adaptation lead to the statistically same results for both Music and Speech.</p>
</list-item>
</list></p>
<p>In relation to (1), our results can simply reflect the nature of the datasets, and particularly the distributions of targets for each database. This is evident in <xref ref-type="fig" rid="pone.0179289.g006">Fig 6</xref>, which shows that music and speech emotions ratings in the databases used are only partially overlapping—there are zones in the bi-dimensional space formed by Arousal and Valence that are covered by the music database, but not by the speech instances. The most clear examples are the top-left (high Arousal and negative Valence) and bottom-right (low Arousal and positive Valence) quadrants, but it is generally visible that the emotions perceived in music are more varied. Another possible justification for the knowledge transfer asymmetry is the fact that music makes use of acoustic patterns to convey emotions that are not used in speech, and therefore such knowledge is not transferable. This is plausible considering the much wider range of sounds (compared to speech) that is possible to produce with music instruments (and nowadays computers), and their qualities and organisation in time. In this context, there may be a wider range of emotion conveying mechanisms at work, derived from a wider range of acoustic sources available in music, and possibilities to combine sounds in meaningful ways (including emotional). This is coherent with contemporary theories of music evolution, such as the <italic>musilanguage</italic> model [<xref ref-type="bibr" rid="pone.0179289.ref012">12</xref>], which posits that, music has specialised in the communication of affective meaning (whereas language specialised in the communication of referential meaning). Furthermore, there is evidence of brain specialisations for music, which are related, among other aspects, to the encoding of pitch along musical scales and the ascribing of a regular beat to incoming acoustics signals [<xref ref-type="bibr" rid="pone.0179289.ref064">64</xref>]. Such brain networks may also encode important information about music, and particularly patterns related to the recognition of emotion. In that case, as mentioned above, it is also possible that the standard features used lack relevant information specific to music.</p>
<fig id="pone.0179289.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0179289.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Distribution of the target annotations of the databases used in the regression tasks.</title>
<p>Dots and squares (music and speech, respectively) represent the Arousal and Valence values at each time step for all instances used in the regression experiments (time is not represented).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0179289.g006" xlink:type="simple"/>
</fig>
<p>Regarding (2), our findings reveal two important pieces of evidence: a) the standard cross-domain approach led to excellent cross-domain generalisations; b) knowledge transfer by feature-representation-learning did not lead to significant improvements over the standard cross-domain approach. One the one hand these findings indicate that the cross-domain models perform well even without adaptation of the feature space, reinforcing the ideas of a close coupling between both domain in terms of acoustic codes for emotional communication. On the other hand, considering that we have used deep neural networks in both cross-domain experiments (with the same number of hidden layers), it may indicate that, the pre-training process did not extract additional, relevant statistical information from the features spaces of the source and target domains. In this case, larger, more varied unlabelled databases may be needed to sample a sufficient amount of information from the features in both domains. Also, deeper architectures may favour the extraction of relevant representations at different structural levels (see, for instance, [<xref ref-type="bibr" rid="pone.0179289.ref065">65</xref>] for stacked DAEs with sparse rectifier units approach), and the fine-tuning of the pre-trained layer during the regression task may be also advantageous.</p>
<p>In sum, our results show an evident and excellent cross-domain generalisation of time-continuous estimations of emotional Arousal and Valence in music and speech even without the support of Transfer Learning (in this case feature-representation-transfer). This is a clear demonstration that there is a substantial overlap between the acoustic codes for emotional expression in music and speech, which can help explain the power of music to communicate emotions to its listeners by confirming the existence of shared acoustic codes for emotional communication. These findings are also beneficial for computer scientists interested in automatic estimation of emotion due to the fact that data from either domain (music or speech) can be used to develop models emotion recognition models. This is particularly relevant in contexts where the amount of data in either domain is scarce, which is clearly the case of music, but also in contexts where the whole emotional spectrum is not represented by a particular dataset and can be complement by data from another domain (the emotions typically experienced with music are a different subset from those most commonly experiences in everyday life circumstances; see for instance [<xref ref-type="bibr" rid="pone.0179289.ref028">28</xref>, <xref ref-type="bibr" rid="pone.0179289.ref029">29</xref>]. Finally, we would like to highlight some limitations of this work. The first one pertains to the datasets used. Time continuous labelled databases of music and speech are scarce, and the datasets used here are limited in that they cannot cover the whole emotional spectrum. As mentioned, this may have limited the transfer of knowledge from music to speech. Second, we have only tested a particular Transfer Learning technique and did not perform adaptation on the regression targets. In future work, we plan to explore alternative Transfer Learning techniques (e.g. functional transfer) to further evaluate whether it can benefit the knowledge transfer between features spaces, as well as attempt to deal with the mismatches between the distributions of the regression outputs. Third, we worked with a feature set that was optimised for Speech Emotion Recognition, even though it is commonly (and successfully) used for Music Emotion Recognition tasks. In future work, we intend to conduct more detailed analysis on the characteristics of feature space and enlarge it with new cross- and intra-domain features.</p>
</sec>
</body>
<back>
<ack>
<p>This work was partially funded by European Union’s Horizon 2020 research and innovation programme under grant agreement No. 645378 (ARIA-VALUSPA). We are grateful to Felix Weninger for his suggestions in early stages to this work. We are also grateful to Mark Korhonen and Oliver Grewe for sharing the annotation data from their studies.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0179289.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Juslin</surname> <given-names>PN</given-names></name>. <article-title>From everyday emotions to aesthetic emotions: towards a unified theory of musical emotions</article-title>. <source>Physics of life reviews</source>. <year>2013</year>;<volume>10</volume>(<issue>3</issue>):<fpage>235</fpage>–<lpage>66</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.plrev.2013.05.008" xlink:type="simple">10.1016/j.plrev.2013.05.008</ext-link></comment> <object-id pub-id-type="pmid">23769678</object-id></mixed-citation>
</ref>
<ref id="pone.0179289.ref002">
<label>2</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Coutinho</surname> <given-names>E</given-names></name>. <chapter-title>How music creates emotion: a multifactorial process approach</chapter-title>. In: <name name-style="western"><surname>Cochrane</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Fantini</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>, editors. <source>The Emotional Power of Music: Multidisciplinary perspectives on musical arousal, expression, and social control</source>. <edition>1st ed</edition>. <publisher-loc>Oxford, UK</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>2013</year>. p. <fpage>121</fpage>–<lpage>146</lpage>.</mixed-citation>
</ref>
<ref id="pone.0179289.ref003">
<label>3</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Zentner</surname> <given-names>M</given-names></name>. <chapter-title>Emotional effects of music: production rules</chapter-title>. In: <name name-style="western"><surname>Juslin</surname> <given-names>PN</given-names></name>, <name name-style="western"><surname>Sloboda</surname> <given-names>JA</given-names></name>, editors. <source>Music and emotion: theory and research</source>. <publisher-loc>Oxford/New York</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>2001</year>. p. <fpage>361</fpage>–<lpage>392</lpage>.</mixed-citation>
</ref>
<ref id="pone.0179289.ref004">
<label>4</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Gabrielsson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lindström</surname> <given-names>E</given-names></name>. <chapter-title>The role of structure in the musical expression of emotions</chapter-title>. In: <name name-style="western"><surname>Juslin</surname> <given-names>PN</given-names></name>, <name name-style="western"><surname>Sloboda</surname> <given-names>J</given-names></name>, editors. <source>Handbook of music and emotion: Theory, research, applications</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>2010</year>. p. <fpage>367</fpage>–<lpage>400</lpage>.</mixed-citation>
</ref>
<ref id="pone.0179289.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Balkwill</surname> <given-names>LL</given-names></name>, <name name-style="western"><surname>Thompson</surname> <given-names>WF</given-names></name>. <article-title>A cross-cultural investigation of the perception of emotion in music: psychophysical and cultural cues</article-title>. <source>Music Perception</source>. <year>1999</year>;<volume>17</volume>(<issue>1</issue>):<fpage>43</fpage>–<lpage>64</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2307/40285811" xlink:type="simple">10.2307/40285811</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Balkwill</surname> <given-names>LL</given-names></name>, <name name-style="western"><surname>Thompson</surname> <given-names>WF</given-names></name>, <name name-style="western"><surname>Matsunaga</surname> <given-names>R</given-names></name>. <article-title>Recognition of emotion in Japanese, Western, and Hindustani music by Japanese listeners</article-title>. <source>Japanese Psychological Research</source>. <year>2004</year>;<volume>46</volume>(Special Issue: Cognition and emotion in music):<fpage>337</fpage>–<lpage>349</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1468-5584.2004.00265.x" xlink:type="simple">10.1111/j.1468-5584.2004.00265.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Fritz</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Jentschke</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Gosselin</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Friederici</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Koelsch</surname> <given-names>S</given-names></name>. <article-title>Report Universal Recognition of Three Basic Emotions in Music</article-title>. <source>Current Biology</source>. <year>2009</year>;<volume>19</volume>(<issue>7</issue>):<fpage>573</fpage>–<lpage>576</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2009.02.058" xlink:type="simple">10.1016/j.cub.2009.02.058</ext-link></comment> <object-id pub-id-type="pmid">19303300</object-id></mixed-citation>
</ref>
<ref id="pone.0179289.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Juslin</surname> <given-names>PN</given-names></name>, <name name-style="western"><surname>Laukka</surname> <given-names>P</given-names></name>. <article-title>Communication of emotions in vocal expression and music performance: different channels, same code?</article-title> <source>Psychological bulletin</source>. <year>2003</year>;<volume>129</volume>(<issue>5</issue>):<fpage>770</fpage>–<lpage>814</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0033-2909.129.5.770" xlink:type="simple">10.1037/0033-2909.129.5.770</ext-link></comment> <object-id pub-id-type="pmid">12956543</object-id></mixed-citation>
</ref>
<ref id="pone.0179289.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ilie</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Thompson</surname> <given-names>WF</given-names></name>. <article-title>Experiential and Cognitive Changes Following Seven Minutes Exposure to Music and Speech</article-title>. <source>Music Perception: An Interdisciplinary Journal</source>. <year>2011</year>;<volume>28</volume>(<issue>3</issue>):<fpage>247</fpage>–<lpage>264</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1525/mp.2011.28.3.247" xlink:type="simple">10.1525/mp.2011.28.3.247</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Coutinho</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Dibben</surname> <given-names>NJ</given-names></name>. <article-title>Psychoacoustic cues to emotion in speech prosody and music</article-title>. <source>Cognition &amp; Emotion</source>. <year>2012</year>;<volume>26</volume>:<fpage>658</fpage>–<lpage>684</lpage>.</mixed-citation>
</ref>
<ref id="pone.0179289.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Weninger</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Eyben</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Schuller</surname> <given-names>BW</given-names></name>, <name name-style="western"><surname>Mortillaro</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>. <article-title>On the Acoustics of Emotion in Audio: What Speech, Music, and Sound have in Common</article-title>. <source>Frontiers in Psychology</source>. <year>2013</year>;<volume>4</volume>(<issue>May</issue>):<fpage>1</fpage>–<lpage>12</lpage>.</mixed-citation>
</ref>
<ref id="pone.0179289.ref012">
<label>12</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Brown</surname> <given-names>S</given-names></name>. <chapter-title>The “musilanguage” model of music evolution</chapter-title>. In: <name name-style="western"><surname>Wallin</surname> <given-names>NL</given-names></name>, <name name-style="western"><surname>Merker</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>S</given-names></name>, editors. <source>The Origins of Music</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>; <year>2000</year>. p. <fpage>271</fpage>–<lpage>300</lpage>.</mixed-citation>
</ref>
<ref id="pone.0179289.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kivy</surname> <given-names>P</given-names></name>. <article-title>A failure of aesthetic emotivism</article-title>. <source>Philosophical studies</source>. <year>1980</year>;<volume>38</volume>(<issue>4</issue>):<fpage>351</fpage>–<lpage>365</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00419335" xlink:type="simple">10.1007/BF00419335</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Juslin</surname> <given-names>PN</given-names></name>, <name name-style="western"><surname>Västfjäll</surname> <given-names>D</given-names></name>. <article-title>Emotional responses to music: The need to consider underlying mechanisms</article-title>. <source>Behavioral and brain sciences</source>. <year>2008</year>;<volume>31</volume>(<issue>05</issue>):<fpage>559</fpage>–<lpage>575</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1017/S0140525X08005293" xlink:type="simple">10.1017/S0140525X08005293</ext-link></comment> <object-id pub-id-type="pmid">18826699</object-id></mixed-citation>
</ref>
<ref id="pone.0179289.ref015">
<label>15</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Brown</surname></name>. <chapter-title>The “Musilanguage” Model of Music Evolution</chapter-title>. In: <name name-style="western"><surname>Wallin</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Merker</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>S</given-names></name>, editors. <source>The Origins of Music</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>The MIT Press</publisher-name>; <year>1999</year>. p. <fpage>271</fpage>–<lpage>301</lpage>.</mixed-citation>
</ref>
<ref id="pone.0179289.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>. <article-title>Expression of emotion in voice and music</article-title>. <source>Journal of voice</source>. <year>1995</year>;<volume>9</volume>(<issue>3</issue>):<fpage>235</fpage>–<lpage>248</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0892-1997(05)80231-0" xlink:type="simple">10.1016/S0892-1997(05)80231-0</ext-link></comment> <object-id pub-id-type="pmid">8541967</object-id></mixed-citation>
</ref>
<ref id="pone.0179289.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Coutinho</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Cangelosi</surname> <given-names>A</given-names></name>. <article-title>The Use of Spatio-Temporal Connectionist Models in Psychological Studies of Musical Emotions</article-title>. <source>Music Perception</source>. <year>2009</year>;<volume>27</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1525/mp.2009.27.1.1" xlink:type="simple">10.1525/mp.2009.27.1.1</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Coutinho</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Cangelosi</surname> <given-names>A</given-names></name>. <article-title>Musical Emotions: Predicting Second-by-Second Subjective Feelings of Emotion From Low-Level Psychoacoustic Features and Physiological Measurements</article-title>. <source>Emotion</source>. <year>2011</year>;<volume>11</volume>(<issue>4</issue>):<fpage>921</fpage>–<lpage>937</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0024700" xlink:type="simple">10.1037/a0024700</ext-link></comment> <object-id pub-id-type="pmid">21859207</object-id></mixed-citation>
</ref>
<ref id="pone.0179289.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schubert</surname> <given-names>E</given-names></name>. <article-title>Modeling perceived emotion with continuous musical features</article-title>. <source>Music perception</source>. <year>2004</year>;<volume>21</volume>(<issue>4</issue>):<fpage>561</fpage>–<lpage>585</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1525/mp.2004.21.4.561" xlink:type="simple">10.1525/mp.2004.21.4.561</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref020">
<label>20</label>
<mixed-citation publication-type="other" xlink:type="simple">Soleymani M, Caro MN, Schmidt EM, Sha CY, Yang YH. 1000 Songs for Emotional Analysis of Music. In: Proceedings of the 2Nd ACM International Workshop on Crowdsourcing for Multimedia. CrowdMM’13. New York, NY, USA: ACM; 2013. p. 1–6. <comment>Available from: <ext-link ext-link-type="uri" xlink:href="http://doi.acm.org/10.1145/2506364.2506365" xlink:type="simple">http://doi.acm.org/10.1145/2506364.2506365</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref021">
<label>21</label>
<mixed-citation publication-type="other" xlink:type="simple">Aljanaki A, Yang YH, Soleymani M. Emotion in music task at mediaeval 2014. In: Working Notes Proceedings of the MediaEval 2014 Workshop. vol. 1263. CEUR-WS.org; 2014.</mixed-citation>
</ref>
<ref id="pone.0179289.ref022">
<label>22</label>
<mixed-citation publication-type="other" xlink:type="simple">Wöllmer M, Eyben F, Reiter S, Schuller B, Cox C, Douglas-Cowie E, et al. Abandoning Emotion Classes—Towards Continuous Emotion Recognition with Modelling of Long-Range Dependencies. In: Proceedings INTERSPEECH 2008, 9th Annual Conference of the International Speech Communication Association, incorporating 12th Australasian International Conference on Speech Science and Technology, SST 2008. ISCA/ASSTA. Brisbane, Australia: ISCA; 2008. p. 597–600.</mixed-citation>
</ref>
<ref id="pone.0179289.ref023">
<label>23</label>
<mixed-citation publication-type="other" xlink:type="simple">Douglas-Cowie E, Cowie R, Sneddon I, Cox C, Lowry O, McRorie M, et al. The HUMAINE Database: Addressing the Collection and Annotation of Naturalistic and Induced Emotional Data. In: Paiva ACR, Prada R, Picard RW, editors. Affective Computing and Intelligent Interaction. vol. 4738 of Lecture Notes in Computer Science. Berlin-Heidelberg, Germany: Springer; 2007. p. 488–500. <comment>Available from: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/978-3-540-74889-2_43" xlink:type="simple">http://dx.doi.org/10.1007/978-3-540-74889-2_43</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wöllmer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schuller</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Eyben</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Rigoll</surname> <given-names>G</given-names></name>. <article-title>Combining long short-term memory and dynamic bayesian networks for incremental emotion-sensitive artificial listening</article-title>. <source>Selected Topics in Signal Processing, IEEE Journal of</source>. <year>2010</year>;<volume>4</volume>(<issue>5</issue>):<fpage>867</fpage>–<lpage>881</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/JSTSP.2010.2057200" xlink:type="simple">10.1109/JSTSP.2010.2057200</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref025">
<label>25</label>
<mixed-citation publication-type="other" xlink:type="simple">Ringeval F, Schuller B, Valstar M, Cowie R, Pantic M. AVEC 2015: The 5th international audio/visual emotion challenge and workshop. In: Proceedings of the 23rd ACM international conference on Multimedia. ACM; 2015. p. 1335–1336.</mixed-citation>
</ref>
<ref id="pone.0179289.ref026">
<label>26</label>
<mixed-citation publication-type="other" xlink:type="simple">Coutinho E, Deng J, Schuller B. Transfer Learning Emotion Manifestation Across Music and Speech. In: Proceedings 2014 International Joint Conference on Neural Networks (IJCNN) as part of the IEEE World Congress on Computational Intelligence (IEEE WCCI). IEEE. Beijing, China: IEEE; 2014. p. 3592–3598.</mixed-citation>
</ref>
<ref id="pone.0179289.ref027">
<label>27</label>
<mixed-citation publication-type="other" xlink:type="simple">Feng Y, Zhuang Y, Pan Y. Popular Music Retrieval by Detecting Mood. In: Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval. SIGIR’03. New York, NY, USA: ACM; 2003. p. 375–376. <comment>Available from: <ext-link ext-link-type="uri" xlink:href="http://doi.acm.org/10.1145/860435.860508" xlink:type="simple">http://doi.acm.org/10.1145/860435.860508</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Zentner</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>, <name name-style="western"><surname>Grandjean</surname> <given-names>D</given-names></name>. <article-title>Emotions evoked by the sound of music: characterization, classification, and measurement</article-title>. <source>Emotion</source>. <year>2008</year>;<volume>8</volume>(<issue>4</issue>):<fpage>494</fpage>–<lpage>521</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/1528-3542.8.4.494" xlink:type="simple">10.1037/1528-3542.8.4.494</ext-link></comment> <object-id pub-id-type="pmid">18729581</object-id></mixed-citation>
</ref>
<ref id="pone.0179289.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Coutinho</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Scherer</surname> <given-names>KR</given-names></name>. <article-title>Introducing the GEneva Music-Induced Affect Checklist (GEMIAC): A Brief Instrument for the Rapid Assessment of Musically Induced Emotions</article-title>. <source>Music Perception</source>. <year>2017</year>;<volume>34</volume>:<fpage>371</fpage>–<lpage>386</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1525/mp.2017.34.4.371" xlink:type="simple">10.1525/mp.2017.34.4.371</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref030">
<label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lin</surname> <given-names>YC</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>YH</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>HH</given-names></name>. <article-title>Exploiting Online Music Tags for Music Emotion Classification</article-title>. <source>ACM Trans Multimedia Comput Commun Appl</source>. <year>2011</year>;<volume>7S</volume>(<issue>1</issue>):<fpage>26:1</fpage>–<lpage>26:16</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1145/2037676.2037683" xlink:type="simple">10.1145/2037676.2037683</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref031">
<label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hunter</surname> <given-names>PG</given-names></name>, <name name-style="western"><surname>Schellenberg</surname> <given-names>EG</given-names></name>, <name name-style="western"><surname>Schimmack</surname> <given-names>U</given-names></name>. <article-title>Mixed affective responses to music with conflicting cues</article-title>. <source>Cognition &amp; Emotion</source>. <year>2008</year>;<volume>22</volume>(<issue>2</issue>):<fpage>327</fpage>–<lpage>352</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/02699930701438145" xlink:type="simple">10.1080/02699930701438145</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref032">
<label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Russell</surname> <given-names>JA</given-names></name>. <article-title>A circumplex model of affect</article-title>. <source>Journal of personality and social psychology</source>. <year>1980</year>;<volume>39</volume>(<issue>6</issue>):<fpage>1161</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/h0077714" xlink:type="simple">10.1037/h0077714</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref033">
<label>33</label>
<mixed-citation publication-type="other" xlink:type="simple">Aljanaki A, Yang YH, Soleymani M. Emotion in music task at mediaeval 2015. In: Working Notes Proceedings of the MediaEval 2015 Workshop. vol. 1436. CEUR-WS.org; 2015.</mixed-citation>
</ref>
<ref id="pone.0179289.ref034">
<label>34</label>
<mixed-citation publication-type="other" xlink:type="simple">Coutinho E, Weninger F, Schuller B, Scherer KR. The Munich LSTM-RNN Approach to the MediaEval 2014 “Emotion in Music” Task. In: Working Notes Proceedings of the MediaEval 2014 Workshop. vol. 1263. Barcelona, Spain: CEUR-WS.org; 2014. p. 5–6.</mixed-citation>
</ref>
<ref id="pone.0179289.ref035">
<label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ringeval</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Eyben</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Kroupi</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Yuce</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Thiran</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Ebrahimi</surname> <given-names>T</given-names></name>, <etal>et al</etal>. <article-title>Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data</article-title>. <source>Pattern Recognition Letters</source>. <year>2014</year>;<volume>66</volume>:<fpage>22</fpage>–<lpage>30</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.patrec.2014.11.007" xlink:type="simple">10.1016/j.patrec.2014.11.007</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gers</surname> <given-names>FA</given-names></name>, <name name-style="western"><surname>Schmidhuber</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Cummins</surname> <given-names>F</given-names></name>. <article-title>Learning to forget: Continual prediction with LSTM</article-title>. <source>Neural Computation</source>. <year>2000</year>;<volume>12</volume>(<issue>10</issue>):<fpage>2451</fpage>–<lpage>2471</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976600300015015" xlink:type="simple">10.1162/089976600300015015</ext-link></comment> <object-id pub-id-type="pmid">11032042</object-id></mixed-citation>
</ref>
<ref id="pone.0179289.ref037">
<label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Elman</surname> <given-names>JL</given-names></name>. <article-title>Finding structure in time</article-title>. <source>Cognitive science</source>. <year>1990</year>;<volume>14</volume>(<issue>2</issue>):<fpage>179</fpage>–<lpage>211</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1207/s15516709cog1402_1" xlink:type="simple">10.1207/s15516709cog1402_1</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref038">
<label>38</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Graves</surname> <given-names>A</given-names></name>. <source>Supervised Sequence Labelling with Recurrent Neural Networks. vol. 385 of Studies in Computational Intelligence</source>. <publisher-loc>Berlin-Heidelberg, Germany</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>; <year>2012</year>.</mixed-citation>
</ref>
<ref id="pone.0179289.ref039">
<label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wöllmer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schuller</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Batliner</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Steidl</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Seppi</surname> <given-names>D</given-names></name>. <article-title>Tandem decoding of children’s speech for keyword detection in a child-robot interaction scenario</article-title>. <source>ACM Transactions on Speech and Language Processing (TSLP)</source>. <year>2011</year>;<volume>7</volume>(<issue>4</issue>):<fpage>12</fpage>.</mixed-citation>
</ref>
<ref id="pone.0179289.ref040">
<label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Graves</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Schmidhuber</surname> <given-names>J</given-names></name>. <article-title>Framewise phoneme classification with bidirectional LSTM and other neural network architectures</article-title>. <source>Neural Networks</source>. <year>2005</year>;<volume>18</volume>(<issue>5</issue>):<fpage>602</fpage>–<lpage>610</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neunet.2005.06.042" xlink:type="simple">10.1016/j.neunet.2005.06.042</ext-link></comment> <object-id pub-id-type="pmid">16112549</object-id></mixed-citation>
</ref>
<ref id="pone.0179289.ref041">
<label>41</label>
<mixed-citation publication-type="other" xlink:type="simple">Schuller B, Steidl S, Batliner A, Vinciarelli A, Scherer K, Ringeval F, et al. The INTERSPEECH 2013 Computational Paralinguistics Challenge: Social Signals, Conflict, Emotion, Autism. In: Proceedings INTERSPEECH 2013, 14th Annual Conference of the International Speech Communication Association. ISCA. Lyon, France: ISCA; 2013. p. 148–152.</mixed-citation>
</ref>
<ref id="pone.0179289.ref042">
<label>42</label>
<mixed-citation publication-type="other" xlink:type="simple">Schuller B, Steidl S, Batliner A, Epps J, Eyben F, Ringeval F, et al. The INTERSPEECH 2014 Computational Paralinguistics Challenge: Cognitive &amp; Physical Load. In: Proceedings INTERSPEECH 2014, 15th Annual Conference of the International Speech Communication Association. ISCA. Singapore, Singapore: ISCA; 2014. p. 427–431.</mixed-citation>
</ref>
<ref id="pone.0179289.ref043">
<label>43</label>
<mixed-citation publication-type="other" xlink:type="simple">Eyben F, Weninger F, Groß F, Schuller B. Recent Developments in openSMILE, the Munich Open-Source Multimedia Feature Extractor. In: Proceedings of the 21st ACM International Conference on Multimedia, MM 2013. Barcelona, Spain: ACM; 2013. p. 835–838.</mixed-citation>
</ref>
<ref id="pone.0179289.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schuller</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Vlasenko</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Eyben</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Wollmer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Stuhlsatz</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wendemuth</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Cross-corpus acoustic emotion recognition: Variances and strategies</article-title>. <source>Affective Computing, IEEE Transactions on</source>. <year>2010</year>;<volume>1</volume>(<issue>2</issue>):<fpage>119</fpage>–<lpage>131</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/T-AFFC.2010.8" xlink:type="simple">10.1109/T-AFFC.2010.8</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref045">
<label>45</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Deng</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Marchi</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Schuller</surname> <given-names>B</given-names></name>. <chapter-title>Sparse autoencoder-based feature transfer learning for speech emotion recognition</chapter-title>. In: <source>Proc. ACII. HUMAINE Association</source>. <publisher-loc>Geneva, Switzerland</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2013</year>. p. <fpage>511</fpage>–<lpage>516</lpage>.</mixed-citation>
</ref>
<ref id="pone.0179289.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pan</surname> <given-names>SJ</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>Q</given-names></name>. <article-title>A Survey on Transfer Learning</article-title>. <source>IEEE Transactions on Knowledge and Data Engineering</source>. <year>2010</year>;<volume>22</volume>(<issue>10</issue>):<fpage>1345</fpage>–<lpage>1359</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TKDE.2009.191" xlink:type="simple">10.1109/TKDE.2009.191</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref047">
<label>47</label>
<mixed-citation publication-type="other" xlink:type="simple">Arnold A, Nallapati R, Cohen WW. A Comparative Study of Methods for Transductive Transfer Learning. In: Data Mining Workshops, 2007. ICDM Workshops 2007. Seventh IEEE International Conference on. IEEE; 2007. p. 77–82.</mixed-citation>
</ref>
<ref id="pone.0179289.ref048">
<label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Courville</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Vincent</surname> <given-names>P</given-names></name>. <article-title>Representation learning: A review and new perspectives</article-title>. <source>Pattern Analysis and Machine Intelligence, IEEE Transactions on</source>. <year>2013</year>;<volume>35</volume>(<issue>8</issue>):<fpage>1798</fpage>–<lpage>1828</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TPAMI.2013.50" xlink:type="simple">10.1109/TPAMI.2013.50</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref049">
<label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bourlard</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Kamp</surname> <given-names>Y</given-names></name>. <article-title>Auto-association by multilayer perceptrons and singular value decomposition</article-title>. <source>Biological cybernetics</source>. <year>1988</year>;<volume>59</volume>(<issue>4-5</issue>):<fpage>291</fpage>–<lpage>294</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/BF00332918" xlink:type="simple">10.1007/BF00332918</ext-link></comment> <object-id pub-id-type="pmid">3196773</object-id></mixed-citation>
</ref>
<ref id="pone.0179289.ref050">
<label>50</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Ranzato</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Poultney</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Chopra</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Cun</surname> <given-names>YL</given-names></name>. <chapter-title>Efficient Learning of Sparse Representations with an Energy-Based Model</chapter-title>. In: <name name-style="western"><surname>Schölkopf</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Platt</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Hoffman</surname> <given-names>T</given-names></name>, editors. <source>Advances in Neural Information Processing Systems 19</source>. <publisher-name>MIT Press</publisher-name>; <year>2007</year>. p. <fpage>1137</fpage>–<lpage>1144</lpage>.</mixed-citation>
</ref>
<ref id="pone.0179289.ref051">
<label>51</label>
<mixed-citation publication-type="other" xlink:type="simple">Vincent P, Larochelle H, Bengio Y, Manzagol PA. Extracting and Composing Robust Features with Denoising Autoencoders. In: Proceedings of the 25th International Conference on Machine Learning. ICML’08. New York, NY, USA: ACM; 2008. p. 1096–1103. <comment>Available from: <ext-link ext-link-type="uri" xlink:href="http://doi.acm.org/10.1145/1390156.1390294" xlink:type="simple">http://doi.acm.org/10.1145/1390156.1390294</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vincent</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Larochelle</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Lajoie</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Manzagol</surname> <given-names>PA</given-names></name>. <article-title>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</article-title>. <source>The Journal of Machine Learning Research</source>. <year>2010</year>;<volume>11</volume>:<fpage>3371</fpage>–<lpage>3408</lpage>.</mixed-citation>
</ref>
<ref id="pone.0179289.ref053">
<label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>McKeown</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Valstar</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Cowie</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Pantic</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schroder</surname> <given-names>M</given-names></name>. <article-title>The SEMAINE Database: Annotated Multimodal Records of Emotionally Colored Conversations between a Person and a Limited Agent</article-title>. <source>IEEE Transactions on Affective Computing</source>. <year>2012</year>;<volume>3</volume>:<fpage>5</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/T-AFFC.2011.20" xlink:type="simple">10.1109/T-AFFC.2011.20</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref054">
<label>54</label>
<mixed-citation publication-type="other" xlink:type="simple">Schuller B, Valstar M, Cowie R, Pantic M, editors. Proceedings of the First International Audio/Visual Emotion Challenge and Workshop, AVEC 2011. vol. 6975, Part II of Lecture Notes on Computer Science (LNCS). HUMAINE Association. Memphis, TN: Springer; 2011.</mixed-citation>
</ref>
<ref id="pone.0179289.ref055">
<label>55</label>
<mixed-citation publication-type="other" xlink:type="simple">Ringeval F, Sonderegger A, Sauer J, Lalanne D. Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions. In: Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on. Shanghai, China: IEEE; 2013. p. 1–8.</mixed-citation>
</ref>
<ref id="pone.0179289.ref056">
<label>56</label>
<mixed-citation publication-type="other" xlink:type="simple">Ringeval F, Schuller B, Valstar M, Cowie R, Pantic M. AVEC 2015—The 5th International Audio/Visual Emotion Challenge and Workshop. In: Proceedings of the 23rd ACM International Conference on Multimedia, MM 2015. ACM. Brisbane, Australia: ACM; 2015. p. 1335–1336.</mixed-citation>
</ref>
<ref id="pone.0179289.ref057">
<label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ilie</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Thompson</surname> <given-names>WF</given-names></name>. <article-title>A Comparison of Acoustic Cues in Music and Speech for Three Dimensions of Affect</article-title>. <source>Music Perception</source>. <year>2006</year>;<volume>23</volume>(<issue>4</issue>):<fpage>319</fpage>–<lpage>330</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1525/mp.2006.23.4.319" xlink:type="simple">10.1525/mp.2006.23.4.319</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref058">
<label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Thompson</surname> <given-names>WF</given-names></name>. <article-title>Decoding speech prosody in five languages</article-title>. <source>Semiotica</source>. <year>2006</year>;<volume>158</volume>(<issue>1/4</issue>):<fpage>407</fpage>–<lpage>424</lpage>.</mixed-citation>
</ref>
<ref id="pone.0179289.ref059">
<label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Grewe</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Nagel</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Kopiez</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Altenmüller</surname> <given-names>E</given-names></name>. <article-title>Emotions over time: synchronicity and development of subjective, physiological, and facial affective reactions to music</article-title>. <source>Emotion (Washington, DC)</source>. <year>2007</year>;<volume>7</volume>(<issue>4</issue>):<fpage>774</fpage>–<lpage>88</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/1528-3542.7.4.774" xlink:type="simple">10.1037/1528-3542.7.4.774</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref060">
<label>60</label>
<mixed-citation publication-type="other" xlink:type="simple">Korhonen M. Modeling Continuous Emotional Appraisals of Music Using System Identification; 2004. <comment>Available from: <ext-link ext-link-type="uri" xlink:href="http://hdl.handle.net/10012/879" xlink:type="simple">http://hdl.handle.net/10012/879</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0179289.ref061">
<label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Alain</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Bengio</surname> <given-names>Y</given-names></name>. <article-title>What regularized auto-encoders learn from the data-generating distribution</article-title>. <source>Journal of Machine Learning Research</source>. <year>2014</year>;<volume>15</volume>(<issue>1</issue>):<fpage>3563</fpage>–<lpage>3593</lpage>.</mixed-citation>
</ref>
<ref id="pone.0179289.ref062">
<label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lin</surname> <given-names>L</given-names></name>. <article-title>A concordance correlation coefficient to evaluate reproducibility</article-title>. <source>Biometrics</source>. <year>1989</year>;<volume>45</volume>(<issue>1</issue>):<fpage>255</fpage>–<lpage>268</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2307/2532051" xlink:type="simple">10.2307/2532051</ext-link></comment> <object-id pub-id-type="pmid">2720055</object-id></mixed-citation>
</ref>
<ref id="pone.0179289.ref063">
<label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Kawakami</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Furukawa</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Katahira</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Okanoya</surname> <given-names>K</given-names></name>. <article-title>Sad music induces pleasant emotion</article-title>. <source>Frontiers in Psychology</source>. <year>2013</year>;<volume>4</volume>(<issue>311</issue>):<fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation>
</ref>
<ref id="pone.0179289.ref064">
<label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Peretz</surname> <given-names>I</given-names></name>. <article-title>Brain specialization for music. New evidence from congenital amusia</article-title>. <source>Annals of the New York Academy of Sciences</source>. <year>2001</year>;<volume>930</volume>:<fpage>153</fpage>–<lpage>65</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1749-6632.2001.tb05731.x" xlink:type="simple">10.1111/j.1749-6632.2001.tb05731.x</ext-link></comment> <object-id pub-id-type="pmid">11458826</object-id></mixed-citation>
</ref>
<ref id="pone.0179289.ref065">
<label>65</label>
<mixed-citation publication-type="other" xlink:type="simple">Glorot X, Bordes A, Bengio Y. Domain adaptation for large-scale sentiment classification: A deep learning approach. In: Proceedings of the 28th International Conference on Machine Learning. Bellevue, WA, USA: International Machine Learning Society; 2011. p. 513–520.</mixed-citation>
</ref>
</ref-list>
</back>
</article>