<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">plos</journal-id>
      <journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
      <journal-id journal-id-type="pmc">ploscomp</journal-id>
      <journal-title-group>
        <journal-title>PLoS Computational Biology</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1553-734X</issn>
      <issn pub-type="epub">1553-7358</issn>
      <publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">PCOMPBIOL-D-12-01517</article-id>
      <article-id pub-id-type="doi">10.1371/journal.pcbi.1002943</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Computational biology</subject>
          </subj-group>
          <subj-group>
            <subject>Neuroscience</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Nonlinear dynamics</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Physics</subject>
          <subj-group>
            <subject>Biophysics</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Computational Biology</subject>
          <subject>Neuroscience</subject>
          <subject>Physics</subject>
          <subject>Mathematics</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Embedding Responses in Spontaneous Neural Activity Shaped through Sequential Learning</article-title>
        <alt-title alt-title-type="running-head">Embedding Responses in Spontaneous Activity</alt-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Kurikawa</surname>
            <given-names>Tomoki</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Kaneko</surname>
            <given-names>Kunihiko</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
      </contrib-group>
      <aff id="aff1">
        <label>1</label>
        <addr-line>Graduate School of Arts and Sciences, University of Tokyo, 3-8-1 Komaba, Meguro-ku, Tokyo, Japan</addr-line>
      </aff>
      <aff id="aff2">
        <label>2</label>
        <addr-line>Research Center for Complex Systems Biology, University of Tokyo, 3-8-1 Komaba, Meguro-ku, Tokyo, Japan</addr-line>
      </aff>
      <contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Behrens</surname>
            <given-names>Tim</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group>
      <aff id="edit1">
        <addr-line>University of Oxford, United Kingdom</addr-line>
      </aff>
      <author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">kurikawa@complex.c.u-tokyo.ac.jp</email></corresp>
        <fn fn-type="conflict">
          <p>The authors have declared that no competing interests exist.</p>
        </fn>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: TK. Performed the experiments: TK. Analyzed the data: TK KK. Contributed reagents/materials/analysis tools: TK. Wrote the paper: TK KK.</p>
        </fn>
      </author-notes>
      <pub-date pub-type="collection">
        <month>3</month>
        <year>2013</year>
      </pub-date>
      <pub-date pub-type="epub">
        <day>7</day>
        <month>3</month>
        <year>2013</year>
      </pub-date>
      <volume>9</volume>
      <issue>3</issue>
      <elocation-id>e1002943</elocation-id>
      <history>
        <date date-type="received">
          <day>24</day>
          <month>9</month>
          <year>2012</year>
        </date>
        <date date-type="accepted">
          <day>10</day>
          <month>1</month>
          <year>2013</year>
        </date>
      </history>
      <permissions>
        <copyright-year>2013</copyright-year>
        <copyright-holder>Kurikawa and Kaneko</copyright-holder>
        <license xlink:type="simple">
          <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
        </license>
      </permissions>
      <abstract>
        <p>Recent experimental measurements have demonstrated that spontaneous neural activity in the absence of explicit external stimuli has remarkable spatiotemporal structure. This spontaneous activity has also been shown to play a key role in the response to external stimuli. To better understand this role, we proposed a viewpoint, “memories-as-bifurcations,” that differs from the traditional “memories-as-attractors” viewpoint. Memory recall from the memories-as-bifurcations viewpoint occurs when the spontaneous neural activity is changed to an appropriate output activity upon application of an input, known as a bifurcation in dynamical systems theory, wherein the input modifies the flow structure of the neural dynamics. Learning, then, is a process that helps create neural dynamical systems such that a target output pattern is generated as an attractor upon a given input. Based on this novel viewpoint, we introduce in this paper an associative memory model with a sequential learning process. Using a simple Hebbian-type learning, the model is able to memorize a large number of input/output mappings. The neural dynamics shaped through the learning exhibit different bifurcations to make the requested targets stable upon an increase in the input, and the neural activity in the absence of input shows chaotic dynamics with occasional approaches to the memorized target patterns. These results suggest that these dynamics facilitate the bifurcations to each target attractor upon application of the corresponding input, which thus increases the capacity for learning. This theoretical finding about the behavior of the spontaneous neural activity is consistent with recent experimental observations in which the neural activity without stimuli wanders among patterns evoked by previously applied signals. In addition, the neural networks shaped by learning properly reflect the correlations of input and target-output patterns in a similar manner to those designed in our previous study.</p>
      </abstract>
      <abstract abstract-type="summary">
        <title>Author Summary</title>
        <p>The neural activity without explicit stimuli shows highly structured patterns in space and time, known as spontaneous activity. This spontaneous activity plays a key role in the behavior of the response to external stimuli generated by the interplay between the spontaneous activity and external input. Studying this interplay and how it is shaped by learning is an essential step toward understanding the principles of neural processing. To address this, we proposed a novel viewpoint, memories-as-bifurcations, in which the appropriate changes in the activity upon the input are embedded through learning. Based on this viewpoint, we introduce here an associative memory model with sequential learning by a simple Hebbian-type rule. In spite of its simplicity, the model memorizes the input/output mappings successively, as long as the input is sufficiently large and the synaptic change is slow. The spontaneous neural activity shaped after learning is shown to itinerate over the memorized targets in remarkable agreement with the experimental reports. These dynamics may prepare and facilitate to generate the learned response to the input. Our results suggest that this is the possible functional role of the spontaneous neural activity, while the uncovered network structure inspires a design principle for the memories-as-bifurcations.</p>
      </abstract>
      <funding-group>
        <funding-statement>This work was supported by JSPS KAKENHI Grant Number 233744 and a Grant-in-Aid for Scientific Research on Innovative Areas “Neural creativity for communication (No. 4103)” (No. 21120004) from MEXT, Japan (<ext-link ext-link-type="uri" xlink:href="http://dynamic-brain.jp/en/index.html" xlink:type="simple">http://dynamic-brain.jp/en/index.html</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
      </funding-group>
      <counts>
        <page-count count="15"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>The way in which neural processing of sensory inputs leads to cognitive functions is one of the most important issues in neuroscience. Neural activity in the presence of sensory stimuli <xref ref-type="bibr" rid="pcbi.1002943-Hubel1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1002943-Miyashita1">[4]</xref> and during the execution of cognitive tasks in response to sensory inputs have been measured experimentally <xref ref-type="bibr" rid="pcbi.1002943-Fujiwara1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Friederici1">[6]</xref>, and neural network models that exhibit the requested responses to the inputs have been investigated theoretically <xref ref-type="bibr" rid="pcbi.1002943-BenYishai1">[7]</xref>–<xref ref-type="bibr" rid="pcbi.1002943-Maass1">[12]</xref>. Learning algorithms have also been proposed to memorize several input/output (I/O) mappings <xref ref-type="bibr" rid="pcbi.1002943-Maass1">[12]</xref>–<xref ref-type="bibr" rid="pcbi.1002943-Dayan1">[16]</xref>.</p>
      <p>The response activity has been the main focus both in modeling studies and experiments, while pre-stimulus, i.e., spontaneous, activity has been dismissed simply as background noise. However, spontaneous activity has recently been garnering more attention since experimental measurements have revealed that the spontaneous activity is not random noise and that it shows characteristic spatiotemporal patterns <xref ref-type="bibr" rid="pcbi.1002943-Fox1">[17]</xref>–<xref ref-type="bibr" rid="pcbi.1002943-Destexhe1">[19]</xref>. Furthermore, many observations have revealed that the response activities to external stimuli <xref ref-type="bibr" rid="pcbi.1002943-Arieli1">[20]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Kisley1">[21]</xref> or cognitive tasks depend on the spontaneous activity <xref ref-type="bibr" rid="pcbi.1002943-LinkenkaerHansenKNikulin1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Mathewson1">[23]</xref>. Evoked responses are generated not only by external inputs but also through the interplay of the spontaneous activity and external stimuli. Thus, to establish a neural basis for the cognition and computation in a neural system, it is important to understand the nature of this interplay.</p>
      <p>Spontaneous activity has been analyzed theoretically over the last few decades by using neural network models of rate-coding or spiking neurons with random, designed, or biologically realistic connections <xref ref-type="bibr" rid="pcbi.1002943-Amit2">[24]</xref>–<xref ref-type="bibr" rid="pcbi.1002943-Petermann1">[28]</xref>. However, apart from a few publications <xref ref-type="bibr" rid="pcbi.1002943-Marre1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Rajan1">[30]</xref>, the relationship between the spontaneous activity and response to external input has rarely been investigated. Furthermore, how the learning shapes the spontaneous activity and its response to an input is still an open question, but recent experimental studies suggest that learning and developmental processes modify and shape the spontaneous activity <xref ref-type="bibr" rid="pcbi.1002943-Lewis1">[31]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Berkes1">[32]</xref>. In the present paper, we analyze how the spontaneous activity is formed when I/O mappings are memorized. We do this by introducing a simple learning rule to the neural dynamics in order to study the interplay between the spontaneous activity and input-evoked response.</p>
      <p>To analyze the formation of the spontaneous activity and its response to the memorized input through the learning of I/O mappings, we previously proposed a novel view on memory in <xref ref-type="bibr" rid="pcbi.1002943-Kurikawa1">[33]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Kurikawa2">[34]</xref>, which we called “memories as bifurcations” in contrast to the traditional theoretical viewpoint of “memories as attractors.” According to the memories-as-attractors viewpoint, each memory is embedded in one of the attractors in a unique neural dynamical system <xref ref-type="bibr" rid="pcbi.1002943-Amit1">[11]</xref>. An input specifies an initial condition of the dynamical system, and from that initial state, the neural activity reaches an attractor that matches the target corresponding to the given input. Thus, the initial states are determined by the given inputs, but the neural activity in the absence of inputs is not examined. In contrast, according to the memories-as-bifurcations viewpoint, an input modifies the neural dynamics as a parameter, and the flow structure of the neural activity is also changed from that without an input. In the absence of input, the neural activity evolves and corresponds to spontaneous activity. In the presence of a learned input, the flow structure in the neural dynamics changes and an attractor that matches the requested target corresponding to the applied input emerges. With an increase in the input strength, the flow structure changes via a sequence of bifurcations in terms of dynamical systems theory. Here, the flow structure can be changed substantially by applying different memorized inputs. Thus, for this viewpoint, memories are embedded in the flow structure of the neural dynamics such that they enable appropriate bifurcations to appear upon input application.</p>
      <p>Previously, we designed a neural-network connection matrix through correlations among memorized inputs and targets so that an output that matches a target is generated, as a result of bifurcations from the spontaneous activity, by applying the corresponding input <xref ref-type="bibr" rid="pcbi.1002943-Kurikawa2">[34]</xref>. In the model, similarity between the spontaneous and evoked activities was demonstrated and is consistent with recent observations in experimental studies <xref ref-type="bibr" rid="pcbi.1002943-Berkes1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Kenet1">[35]</xref>–<xref ref-type="bibr" rid="pcbi.1002943-Luczak1">[37]</xref>. Although the simplicity of the model is an advantage for analyzing the relationship between spontaneous and evoked neural activities, it remains unclear whether the simplistic structure in the designed network in <xref ref-type="bibr" rid="pcbi.1002943-Kurikawa2">[34]</xref> is the only way to store associative memories or if there exists a variety of networks that show similar behavior and generate a sufficient memory capacity. Also, how such network structures for memorizing I/O mappings are formed by learning through a widely-accepted synaptic plasticity rule, such as the Hebbian rule, is still open for debate.</p>
      <p>In the present study, we introduce a sequential learning model with a simple Hebbian-type learning rule that changes the synaptic strength according to the activities of the pre- and postsynaptic neurons. From extensive numerical simulations, we have confirmed that through this learning the networks memorize <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e001" xlink:type="simple"/></inline-formula> mappings (where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e002" xlink:type="simple"/></inline-formula> is the number of elements) satisfying the memories-as-bifurcations viewpoint. Here, spontaneous activity shows chaotic behavior with approaches to memorized output patterns. By applying each memorized input, this activity is transformed (after a sequence of bifurcations) into different attractors that generate the target pattern corresponding to the applied input.</p>
      <p>In spite of the sequential learning scheme, the neural network does not lose the memory it learned earlier; it has a capacity of up to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e003" xlink:type="simple"/></inline-formula>. This capacity is not so small, and interestingly it is not possible in conventional sequential learning models in which the learning of a new I/O mapping easily pushes out previous memories. As long as the memorized targets are attractors in the same dynamical system, the formation of a new attraction to a novel attractor will easily destroy the attraction to earlier target patterns. Our model differs in that the different targets are attractors in the presence of the corresponding input, i.e., they are embedded in different neural dynamical systems, so that attractors for earlier targets are not destroyed. Here, the spontaneous activity is flexible; it is possible to apply an input so that a new target is embedded in the network structure without destroying the information of the previous targets.</p>
      <p>Remarkably, the network generated through the learning process to obtain a high memory capacity is found to have a similar structure to the network designed in <xref ref-type="bibr" rid="pcbi.1002943-Deco1">[18]</xref>. Although the learning process can generate a huge variety of networks, which are not similar to the designed network, a common structure is generated by the learning. A simple learning rule for synaptic change is sufficient for generating such a network.</p>
    </sec>
    <sec id="s2">
      <title>Model</title>
      <p>We consider a system composed of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e004" xlink:type="simple"/></inline-formula> continuous rate-coding neurons whose activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e005" xlink:type="simple"/></inline-formula> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e006" xlink:type="simple"/></inline-formula> lies between −1 and 1 and evolves according to<disp-formula id="pcbi.1002943.e007"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002943.e007" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e008" xlink:type="simple"/></inline-formula> denotes a connection from the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e009" xlink:type="simple"/></inline-formula>-th to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e010" xlink:type="simple"/></inline-formula>-th neuron, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e011" xlink:type="simple"/></inline-formula> is an input pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e012" xlink:type="simple"/></inline-formula> with input strength <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e013" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e014" xlink:type="simple"/></inline-formula> is index of learned mappings. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e015" xlink:type="simple"/></inline-formula> can represent the strength of sensory input, for example, the contrast of visual stimulus and the concentration of odorant.</p>
      <p>For each input pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e016" xlink:type="simple"/></inline-formula>, we set a pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e017" xlink:type="simple"/></inline-formula> as the target, and the input and target patterns are generated as random <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e018" xlink:type="simple"/></inline-formula>-bit binary patterns, with probabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e019" xlink:type="simple"/></inline-formula>. We postulate that by applying each input pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e020" xlink:type="simple"/></inline-formula>, the corresponding target pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e021" xlink:type="simple"/></inline-formula> is recalled, i.e., an attractor matching the target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e022" xlink:type="simple"/></inline-formula> is generated. We adopt the following learning procedure to embed the postulated I/O mappings.</p>
      <sec id="s2a">
        <title>Learning procedure</title>
        <p>We first select two random binary patterns, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e023" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e024" xlink:type="simple"/></inline-formula>, as the input and target patterns, respectively. The neural activity evolves in the presence of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e025" xlink:type="simple"/></inline-formula> whose strength <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e026" xlink:type="simple"/></inline-formula> is constant during the learning process for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e027" xlink:type="simple"/></inline-formula>. The synaptic connection <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e028" xlink:type="simple"/></inline-formula> also evolves according to<disp-formula id="pcbi.1002943.e029"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002943.e029" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e030" xlink:type="simple"/></inline-formula> is a learning parameter that is the inverse of the time scale ratio of the synaptic to neural dynamics. The above synaptic dynamics are determined by correlations between the activities of the pre- and postsynaptic neurons. This learning rule takes a similar form as the perceptron learning rule where the synaptic connection is changed by correlations between activities of elements in the input and output layers <xref ref-type="bibr" rid="pcbi.1002943-Dayan1">[16]</xref>.</p>
        <p>Here, although the validity of this learning rule is not mathematically proven in contrast to the perceptron, it is expected by the following argument. According to <xref ref-type="disp-formula" rid="pcbi.1002943.e007">Eq. 1</xref>, the change in the neural activity during <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e031" xlink:type="simple"/></inline-formula> with the connection modified by the learning, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e032" xlink:type="simple"/></inline-formula>, is given by<disp-formula id="pcbi.1002943.e033"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002943.e033" xlink:type="simple"/><label>(3)</label></disp-formula><disp-formula id="pcbi.1002943.e034"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002943.e034" xlink:type="simple"/><label>(4)</label></disp-formula>Following the synaptic dynamics in <xref ref-type="disp-formula" rid="pcbi.1002943.e029">Eq. 2</xref>, the change in the neural activity due to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e035" xlink:type="simple"/></inline-formula> is given by<disp-formula id="pcbi.1002943.e036"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002943.e036" xlink:type="simple"/><label>(5)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e037" xlink:type="simple"/></inline-formula> is a positive value determined by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e038" xlink:type="simple"/></inline-formula> and differential coefficient. Thus, when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e039" xlink:type="simple"/></inline-formula> is larger (smaller) than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e040" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e041" xlink:type="simple"/></inline-formula> increases (decreases), respectively. Hence, the change in the synapses will drive the successive activity toward the target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e042" xlink:type="simple"/></inline-formula>. Note, however, that the distance between the neural activity and the target is not necessarily guaranteed to decrease monotonically through the learning, because the total change in the neural activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e043" xlink:type="simple"/></inline-formula> depends also on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e044" xlink:type="simple"/></inline-formula>.</p>
        <p>The learning process stops automatically when the neural activity matches the target since in this case <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e045" xlink:type="simple"/></inline-formula>, otherwise, the learning process continues. Here we impose several I/O mappings to be successively learned, and after learning the preceding mapping, another input pattern with the same strength as the previous learning is applied while giving a new target pattern. The learning process for each single I/O mapping is called a learning step in what follows. In this learning algorithm, which belongs to a class of palimpsest learning models <xref ref-type="bibr" rid="pcbi.1002943-Nadal1">[38]</xref>–<xref ref-type="bibr" rid="pcbi.1002943-Parisi1">[40]</xref>, each mapping is learned sequentially and previously learned mappings are overwritten by the latest mapping. Thus, it is possible that older mappings are forgotten through the learning process.</p>
        <p>During the learning process, double (neural and synaptic) dynamics run concurrently, and the neural and synaptic states have to be set as initial states: the neural and synaptic states are randomly selected from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e046" xlink:type="simple"/></inline-formula> with a uniform probability and from a binary ensemble of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e047" xlink:type="simple"/></inline-formula> with equal probability, respectively. In this model, fully-connected networks without self-connections are used. Through different learning processes, different sets of mappings are learned so that the generated networks are also different. For a statistical analysis, we take an average over many networks shaped through different learning processes.</p>
        <p>As our purpose in this study is to analyze the relationship between the spontaneous and evoked dynamics, we analyze the neural dynamical system in the absence and presence of input after learning. After the learning is completed, the synaptic connections are fixed and only the neural activities evolve. Note that there is no need for the input strengths for learning and memory recall to be identical: we can set the input strength <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e048" xlink:type="simple"/></inline-formula> used during the recall process after the learning and independently of the input strength used during the learning process. For example, after learning with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e049" xlink:type="simple"/></inline-formula>, we can analyze the evoked dynamics by applying the input with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e050" xlink:type="simple"/></inline-formula>. To distinguish the two clearly, the input strength used in the learning process is denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e051" xlink:type="simple"/></inline-formula> and that used in the analysis of the neural activities after learning is denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e052" xlink:type="simple"/></inline-formula>. The spontaneous and evoked dynamics are given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e053" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e054" xlink:type="simple"/></inline-formula>, respectively.</p>
      </sec>
      <sec id="s2b">
        <title>Definition of memory</title>
        <p>As recall and memory for the memories-as-bifurcations viewpoint are defined differently from those for the memories-as-attractors viewpoint, we outline the definitions of recall and then memory here. A network succeeds in recalling a target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e055" xlink:type="simple"/></inline-formula> for an input of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e056" xlink:type="simple"/></inline-formula>, if, on application of input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e057" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e058" xlink:type="simple"/></inline-formula> = <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e059" xlink:type="simple"/></inline-formula>, the overlap of the evoked activity with the target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e060" xlink:type="simple"/></inline-formula> is higher than the overlap with any other pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e061" xlink:type="simple"/></inline-formula>. Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e062" xlink:type="simple"/></inline-formula> is a transposed vector of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e063" xlink:type="simple"/></inline-formula> and the inner product <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e064" xlink:type="simple"/></inline-formula> is given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e065" xlink:type="simple"/></inline-formula>. By considering a case in which the evoked attractor is not a fixed-point attractor, the temporal average overlap is taken as this criterion. By denoting the temporal average overlap with the target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e066" xlink:type="simple"/></inline-formula> as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e067" xlink:type="simple"/></inline-formula>, the criterion for the successful recall of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e068" xlink:type="simple"/></inline-formula> corresponding to the applied input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e069" xlink:type="simple"/></inline-formula> is given by<disp-formula id="pcbi.1002943.e070"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002943.e070" xlink:type="simple"/><label>(6)</label></disp-formula>where we measure the avaraged overlaps in the presence of the input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e071" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e072" xlink:type="simple"/></inline-formula> is the pattern that has the largest overlap with the activity among other targets and inputs, as well as other random patterns.</p>
        <p>Memory is defined as the ability of a network to recall a target for most initial states. The condition for whether a network memorizes an I/O (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e073" xlink:type="simple"/></inline-formula>/<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e074" xlink:type="simple"/></inline-formula>) mapping is<disp-formula id="pcbi.1002943.e075"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002943.e075" xlink:type="simple"/><label>(7)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e076" xlink:type="simple"/></inline-formula> represents the average over the initial states of this network. By extending this criterion, we adopt a condition for determining whether networks memorize the I/O mapping for a certain parameter as<disp-formula id="pcbi.1002943.e077"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002943.e077" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e078" xlink:type="simple"/></inline-formula> denotes the average over different networks.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Results</title>
      <p>To examine whether a network shaped through the learning process memorizes the I/O mapping(s), we measure the evoked activity. Then we analyze the possible relationship between the spontaneous and evoked activities, and also analyze the characteristic features of the connection matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e079" xlink:type="simple"/></inline-formula> that allows for memory.</p>
      <p>Due to the high dimensionality of neural dynamics, it is difficult to directly analyze the time evolution in the entire phase space. Instead, we mainly use the overlaps of the neural activities with some patterns: that with the target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e080" xlink:type="simple"/></inline-formula>, that with the input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e081" xlink:type="simple"/></inline-formula>, and that with a randomly selected pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e082" xlink:type="simple"/></inline-formula>. The behaviors of these overlaps are characteristics of the neural dynamics. We focus on the dependence of the neural and synaptic dynamics on two parameters: the learning parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e083" xlink:type="simple"/></inline-formula> and the input strength <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e084" xlink:type="simple"/></inline-formula>. We begin by examining the dependences after one learning step of only one mapping and then examine the dependences after multiple learning steps.</p>
      <sec id="s3a">
        <title>Neural dynamics formed through one learning step</title>
        <p><xref ref-type="fig" rid="pcbi-1002943-g001">Fig. 1</xref> exhibits a learning process shown as a raster plot and the time series of the overlap with the target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e085" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e086" xlink:type="simple"/></inline-formula>. After wandering over many neural activity patterns, the neural activity reaches the target pattern and the learning process is completed. The learning process does not stop by becoming trapped in a local minimum, nor does it continue to wander over the neural patterns. We confirmed that in all trials with parameters of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e087" xlink:type="simple"/></inline-formula>, the learning was completed.</p>
        <fig id="pcbi-1002943-g001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002943.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>Learning process for one mapping.</title>
            <p><bold>A.</bold> A raster plot of the activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e088" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e089" xlink:type="simple"/></inline-formula> and for 25 of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e090" xlink:type="simple"/></inline-formula> neurons. <bold>B.</bold> The temporal evolution of the overlap with the target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e091" xlink:type="simple"/></inline-formula> for the learning process in A.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002943.g001" position="float" xlink:type="simple"/>
        </fig>
        <p>During a learning process, the flow structures of the spontaneous and evoked activities change. Hence, the recall process also changes through the learning process. <xref ref-type="fig" rid="pcbi-1002943-g002">Fig. 2</xref> shows a recall process before and after learning for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e092" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e093" xlink:type="simple"/></inline-formula>. Before learning, an attractor matching the applied input pattern is generated when that input is applied (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e094" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pcbi-1002943-g002">Fig. 2A</xref>), but the overlap with the required target is not high and the network thus fails to recall the target. After learning, two types of neural dynamics are generated depending on the parameter values (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e095" xlink:type="simple"/></inline-formula>) (see also <xref ref-type="table" rid="pcbi-1002943-t001">Table 1</xref>):</p>
        <fig id="pcbi-1002943-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002943.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Recall processes before and after the learning.</title>
            <p>Neural activities plotted as a time series of the overlaps with the target (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e096" xlink:type="simple"/></inline-formula>), the input (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e097" xlink:type="simple"/></inline-formula>), and a random pattern (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e098" xlink:type="simple"/></inline-formula>). The random pattern is generated from the same ensemble of targets and inputs. <bold>A.</bold> The recall process before the learning for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e099" xlink:type="simple"/></inline-formula>. <bold>B.</bold> The recall processes after the learning for (i) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e100" xlink:type="simple"/></inline-formula> = (16,0.01) and (ii) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e101" xlink:type="simple"/></inline-formula> = (1,0.5). The activity is spontaneous (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e102" xlink:type="simple"/></inline-formula>) or evoked (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e103" xlink:type="simple"/></inline-formula>) as indicated by the dotted and filled red bars, respectively, above the plots. The evoked activity is introduced by the application of an input of strength <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e104" xlink:type="simple"/></inline-formula>. In (ii), the time series from two initial conditions that lead to the two different attractors are plotted.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002943.g002" position="float" xlink:type="simple"/>
        </fig>
        <table-wrap id="pcbi-1002943-t001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002943.t001</object-id>
          <label>Table 1</label>
          <caption>
            <title>Characteristics of each regime.</title>
          </caption>
          <alternatives>
            <graphic id="pcbi-1002943-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002943.t001" xlink:type="simple"/>
            <table>
              <colgroup span="1">
                <col align="left" span="1"/>
                <col align="center" span="1"/>
                <col align="center" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Regime</td>
                  <td align="left" rowspan="1" colspan="1">Response (R)</td>
                  <td align="left" rowspan="1" colspan="1">Non-response (NR)</td>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Spontaneous activity</td>
                  <td align="left" rowspan="1" colspan="1">Chaotic behavior wandering among targets</td>
                  <td align="left" rowspan="1" colspan="1">Fixed points that match the target and reverse target</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Evoked activity</td>
                  <td align="left" rowspan="1" colspan="1">Target fixed point</td>
                  <td align="left" rowspan="1" colspan="1">No change from the spontaneous activity</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Capacity</td>
                  <td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e105" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e106" xlink:type="simple"/></inline-formula>)</td>
                  <td align="left" rowspan="1" colspan="1">0 or 1</td>
                </tr>
                <tr>
                  <td align="left" rowspan="1" colspan="1">Network structure</td>
                  <td align="left" rowspan="1" colspan="1">Asymmetric based on input/output correlations <xref ref-type="bibr" rid="pcbi.1002943-Kurikawa2">[34]</xref></td>
                  <td align="left" rowspan="1" colspan="1">Mattis type</td>
                </tr>
              </tbody>
            </table>
          </alternatives>
        </table-wrap>
        <list list-type="roman-lower">
          <list-item>
            <p>The spontaneous activity shows chaotic behavior around the origin, while the evoked activity shows stationary activity, which matches the target pattern (shown in <xref ref-type="fig" rid="pcbi-1002943-g002">Fig. 2B(i)</xref>), and the neural activity responds to the applied input. This regime is referred to as the “response” (R) regime.</p>
          </list-item>
          <list-item>
            <p>Only fixed-point attractors that match the target and the “reverse” target patterns exist both in the absence and presence of the input (shown in <xref ref-type="fig" rid="pcbi-1002943-g002">Fig. 2B(ii)</xref>). Here, the reverse target pattern represents a neural pattern in which all the variables take the opposite sign of those of the target pattern. The neural activity in this case does not respond to the input, and the regime is referred to as the “non-response” (NR) regime.</p>
          </list-item>
        </list>
        <p>We now analyze the spontaneous and evoked neural dynamics in these two regimes. First, to reveal the dependence of the evoked dynamics on the parameters, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e107" xlink:type="simple"/></inline-formula> as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e108" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e109" xlink:type="simple"/></inline-formula>, is shown in <xref ref-type="fig" rid="pcbi-1002943-g003">Fig. 3A</xref>. In the R regime, for larger <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e110" xlink:type="simple"/></inline-formula> and smaller <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e111" xlink:type="simple"/></inline-formula> values, only the target attractor exists and the average overlap is equal to one, while in the NR regime, both the target and reverse-target attractors exist and the average overlap is lower than that in the R regime. As <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e112" xlink:type="simple"/></inline-formula> decreases or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e113" xlink:type="simple"/></inline-formula> increases, the volume of the reverse-target attractor basin increases and that of the target attractor decreases so that the average overlap with the target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e114" xlink:type="simple"/></inline-formula> also decreases. The dotted line in <xref ref-type="fig" rid="pcbi-1002943-g003">Fig. 3A</xref> represents the boundary between the R and NR regimes computed using the spontaneous activity, as discussed below.</p>
        <fig id="pcbi-1002943-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002943.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Phase diagram of the evoked and spontaneous dynamics and bifurcation diagram.</title>
            <p><bold>A.</bold> The quenched average of the overlap with the target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e115" xlink:type="simple"/></inline-formula> in the evoked dynamics. <bold>B.</bold> The standard deviation (SD) of the overlap averaged over time and over the networks <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e116" xlink:type="simple"/></inline-formula>. Average values in A and B are computed over 100 networks and over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e117" xlink:type="simple"/></inline-formula>. The dotted curves in A and B, plotted for reference, show the boundary between the R and NR regimes and, which are computed by the ridge of SD in B with smoothing the line. <bold>C.</bold> The local maxima in the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e118" xlink:type="simple"/></inline-formula> time series of the overlap with the target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e119" xlink:type="simple"/></inline-formula> as a function of the input strength in (i) the NR regime for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e120" xlink:type="simple"/></inline-formula> and (ii) the R regime <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e121" xlink:type="simple"/></inline-formula> showing the bifurcations.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002943.g003" position="float" xlink:type="simple"/>
        </fig>
        <p>To analyze the spontaneous dynamics, we note that due to the symmetry, the mean overlap for each target over time is generally zero because the orbit can approach both the target and reverse-target with equal probability. Thus, we measure the standard deviation (SD) of the overlap to quantify the approach to each target. The SD(<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e122" xlink:type="simple"/></inline-formula>) of an overlap with the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e123" xlink:type="simple"/></inline-formula>-th target over time is computed as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e124" xlink:type="simple"/></inline-formula>. If this SD is much larger than that for the overlap with a random pattern, then the spontaneous activity selectively approaches the target (and its reverse). A numerical computation of the SD as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e125" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e126" xlink:type="simple"/></inline-formula> is plotted in <xref ref-type="fig" rid="pcbi-1002943-g003">Fig. 3B</xref>. In the R regime, chaotic behavior appears and the SD takes a finite positive value, while in the NR regime, fixed-point attractors exist and so the SD is zero. Interestingly, a band that has a higher SD, which stretches from (2.6, 0.001) to (16,1), and whose ridge divides the R and NR regimes appears in the figure. In <xref ref-type="fig" rid="pcbi-1002943-g003">Fig. 3B</xref>, the ridge is shown as the dotted line, which is also plotted as a reference in <xref ref-type="fig" rid="pcbi-1002943-g003">Fig. 3A</xref>.</p>
        <p>Around the ridge, the SD of the spontaneous activity is much higher than that in other areas, and the chaotic spontaneous activity shows switching behavior between the target and reverse target. While the target and reverse-target attractors are unstable, their ruins still exist and the neural dynamics intermittently visit them.</p>
        <p>In <xref ref-type="fig" rid="pcbi-1002943-g003">Figs. 3A and B</xref>, the boundary defined by the SD might be slightly ambiguous because of the finite-size effect. However, by extrapolating the result for larger system sizes (to be discussed later), it is expected that, in the absence of inputs, all the networks in the NR regime show fixed-point behavior and those in the R regime show chaotic behavior, in the thermodynamic limit. By increasing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e127" xlink:type="simple"/></inline-formula> or decreasing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e128" xlink:type="simple"/></inline-formula>, the minimum distance between the activity and the target (or the reverse-target) increases in the R regime. Thus, in this limit, the SD in the NR regime is zero. It suddenly increases to nearly one at the transition point, and then gradually decreases in the R regime. The ridge of the SD thus indicates the transition between the NR and R regimes well. The area with the average overlap taking nearly one above the dotted line in <xref ref-type="fig" rid="pcbi-1002943-g003">Fig. 3A</xref> is expected to remain even in the thermodynamic limit. However, this area is included in the NR regime, since according to the analysis of the neural dynamics after multiple learning steps, to be discussed later, no more than a single pattern is recalled, as in the rest of the NR regime.</p>
        <p>We also show how spontaneous activity changes into evoked activity with an increase in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e129" xlink:type="simple"/></inline-formula> in each regime, as shown in <xref ref-type="fig" rid="pcbi-1002943-g003">Fig. 3C</xref>. In the R regime, by increasing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e130" xlink:type="simple"/></inline-formula> from zero, the neural activity shows successive bifurcations such that the overlap with the target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e131" xlink:type="simple"/></inline-formula> increases to approach unity at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e132" xlink:type="simple"/></inline-formula>. The fixed-point attractor matching the target appears at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e133" xlink:type="simple"/></inline-formula>. In the NR regime, the target and reverse target attractors do not change on application of the input, but the basin volumes of the attractors increase.</p>
      </sec>
      <sec id="s3b">
        <title>Connection matrix shaped through the learning process</title>
        <p>We analyze the connection matrix that is shaped through the learning process, in the R and NR regimes, by measuring the element of the matrix C which is projected onto <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e134" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e135" xlink:type="simple"/></inline-formula>, as defined by<disp-formula id="pcbi.1002943.e136"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002943.e136" xlink:type="simple"/><label>(9)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e137" xlink:type="simple"/></inline-formula> = <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e138" xlink:type="simple"/></inline-formula>. Note that for a given binary pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e139" xlink:type="simple"/></inline-formula>, if the system has a large matrix element <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e140" xlink:type="simple"/></inline-formula>, then pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e141" xlink:type="simple"/></inline-formula> is more stable in the absence of inputs for the neural dynamics in <xref ref-type="disp-formula" rid="pcbi.1002943.e007">Eq. (1)</xref>. Similarly, when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e142" xlink:type="simple"/></inline-formula> is larger, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e143" xlink:type="simple"/></inline-formula> is less stable. <xref ref-type="fig" rid="pcbi-1002943-g004">Fig. 4</xref> shows a time series of the elements <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e144" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e145" xlink:type="simple"/></inline-formula> for the NR, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e146" xlink:type="simple"/></inline-formula> and R, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e147" xlink:type="simple"/></inline-formula> regimes. In the NR regime, only the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e148" xlink:type="simple"/></inline-formula> element is much larger than the others after learning, while in the R regime, both <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e149" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e150" xlink:type="simple"/></inline-formula> take salient positive values and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e151" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e152" xlink:type="simple"/></inline-formula> take salient negative values.</p>
        <fig id="pcbi-1002943-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002943.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>The time evolution of the overlap and the matrix elements.</title>
            <p><bold>A.</bold> The overlaps with the target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e153" xlink:type="simple"/></inline-formula> and input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e154" xlink:type="simple"/></inline-formula> during the learning process (i) in the NR regime for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e155" xlink:type="simple"/></inline-formula> and (ii) in the R regime for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e156" xlink:type="simple"/></inline-formula>. <bold>B.</bold> The matrix elements <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e157" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e158" xlink:type="simple"/></inline-formula> in (i) the NR regime and (ii) the R regime with the same parameters as in A.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002943.g004" position="float" xlink:type="simple"/>
        </fig>
        <p>The result that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e159" xlink:type="simple"/></inline-formula> dominates in the NR regime means that the generated connection matrix takes a similar form to that of the Mattis model in a spin system <xref ref-type="bibr" rid="pcbi.1002943-Mattis1">[41]</xref>, which corresponds to the Hopfield network with only one memorized pattern. In the network where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e160" xlink:type="simple"/></inline-formula> is larger and the other elements are much smaller, the target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e161" xlink:type="simple"/></inline-formula> and reverse-target patterns <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e162" xlink:type="simple"/></inline-formula> remain highly stable. This is consistent with the above analysis in the NR regime. In the R regime, in contrast, the connection matrix shows a form distinct from those of the matrices in Mattis and Hopfield-type networks. Remarkably, the matrix takes a similar form to that of the model in <xref ref-type="bibr" rid="pcbi.1002943-Kurikawa2">[34]</xref>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e163" xlink:type="simple"/></inline-formula> was adopted. Indeed, the behaviors of the spontaneous and evoked activities in this regime agree with that observed in that model <xref ref-type="bibr" rid="pcbi.1002943-Kurikawa2">[34]</xref>.</p>
        <p>In general, the behaviors are strongly dependent on the matrix elements. In <xref ref-type="fig" rid="pcbi-1002943-g005">Fig. 5</xref>, the elements as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e164" xlink:type="simple"/></inline-formula> are plotted. For <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e165" xlink:type="simple"/></inline-formula>, all of the elements deviate saliently from zero, and as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e166" xlink:type="simple"/></inline-formula> decreases, the elements, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e167" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e168" xlink:type="simple"/></inline-formula> decrease rapidly, while <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e169" xlink:type="simple"/></inline-formula> does not change. The regime changes from the R to NR regime as this occurs.</p>
        <fig id="pcbi-1002943-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002943.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Dependence of the matrix elements <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e170" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e171" xlink:type="simple"/></inline-formula> on the learning parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e172" xlink:type="simple"/></inline-formula>.</title>
            <p>The matrix elements averaged over 100 networks for a fixed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e173" xlink:type="simple"/></inline-formula> of 0.01 are shown, and the corresponding regimes (NR and R) are indicated above the figure. The error bars represent the standard deviation.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002943.g005" position="float" xlink:type="simple"/>
        </fig>
        <p>We now analyze why such connection matrices are formed through the learning process. The evolution of the matrix element <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e174" xlink:type="simple"/></inline-formula> is also determined by <xref ref-type="disp-formula" rid="pcbi.1002943.e029">Eq. (2)</xref> as follows:<disp-formula id="pcbi.1002943.e175"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002943.e175" xlink:type="simple"/><label>(10)</label></disp-formula><disp-formula id="pcbi.1002943.e176"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002943.e176" xlink:type="simple"/><label>(11)</label></disp-formula>Although <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e177" xlink:type="simple"/></inline-formula> also evolves temporally, we set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e178" xlink:type="simple"/></inline-formula> as a constant value, because relative scale of the elements is relevant for understanding the behavior. In the same way, the evolutions of the other elements are determined by<disp-formula id="pcbi.1002943.e179"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002943.e179" xlink:type="simple"/><label>(12)</label></disp-formula><disp-formula id="pcbi.1002943.e180"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002943.e180" xlink:type="simple"/><label>(13)</label></disp-formula><disp-formula id="pcbi.1002943.e181"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002943.e181" xlink:type="simple"/><label>(14)</label></disp-formula>In both the regimes, the activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e182" xlink:type="simple"/></inline-formula> approaches a target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e183" xlink:type="simple"/></inline-formula> and thus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e184" xlink:type="simple"/></inline-formula> is greater than zero (and smaller than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e185" xlink:type="simple"/></inline-formula>) for most of the learning process. Thus, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e186" xlink:type="simple"/></inline-formula> is positive for most of the learning process and then, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e187" xlink:type="simple"/></inline-formula> takes a large positive value. In contrast, the change in the other elements is distinct between both regimes, which is explained by the initial behavior of the learning process. In the R regime, the overlap with the input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e188" xlink:type="simple"/></inline-formula> increases in the early stage of the learning process as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e189" xlink:type="simple"/></inline-formula> is directed toward <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e190" xlink:type="simple"/></inline-formula> by the input, as shown in <xref ref-type="fig" rid="pcbi-1002943-g004">Fig. 4A(ii)</xref>. It is estimated that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e191" xlink:type="simple"/></inline-formula> is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e192" xlink:type="simple"/></inline-formula> and positive, which is much larger than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e193" xlink:type="simple"/></inline-formula>. Thus, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e194" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e195" xlink:type="simple"/></inline-formula> are negative in the R regime, while <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e196" xlink:type="simple"/></inline-formula> is positive. These estimates of the sign of the elements are consistent with the matrix elements in <xref ref-type="fig" rid="pcbi-1002943-g004">Fig. 4B</xref>. For the NR regime, in which <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e197" xlink:type="simple"/></inline-formula> is smaller and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e198" xlink:type="simple"/></inline-formula> is larger, the increase in the overlap with the input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e199" xlink:type="simple"/></inline-formula> in the early stage is much smaller than that in the R regime; if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e200" xlink:type="simple"/></inline-formula> is small, the neural activity does not respond strongly to the input, whereas if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e201" xlink:type="simple"/></inline-formula> is large, the learning is completed before the overlap with the input increases. Thus, the temporal changes in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e202" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e203" xlink:type="simple"/></inline-formula> are much smaller. Hence, only <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e204" xlink:type="simple"/></inline-formula> takes a large value, and thus the Mattis-type network is generated.</p>
      </sec>
      <sec id="s3c">
        <title>Neural dynamics formed through multiple learning steps</title>
        <p>Neural activities that are shaped through multiple learning steps are analyzed for I/O mappings that are learned sequentially, as shown in <xref ref-type="fig" rid="pcbi-1002943-g006">Fig. 6</xref>. In the presence of each input (as indicated by the colored bars above the plot), the neural activity converges to the target to be memorized in the same way as in the learning process of a single mapping (shown in <xref ref-type="fig" rid="pcbi-1002943-g001">Fig. 1</xref>). Note that although the learning process changes the synaptic connections and flow structure of the neural activity, some of the structure generated in earlier learning steps is preserved because the change in the flow structure in each learning step occurs in the presence of a different input pattern. We mainly present the results after the learning of 40 mappings and analyze the behaviors of spontaneous and evoked activities for later 30 mappings in the following analysis. (We choose 30 mapping because memory capacity is almost 20 as shown later. The number 30 and 40 can be arbitrary, as long as they are chosen to be larger than the many capacity.)</p>
        <fig id="pcbi-1002943-g006" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002943.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>A learning process for five mappings.</title>
            <p>The time evolutions of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e205" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e206" xlink:type="simple"/></inline-formula> = 1, 2, 3, 4, and 5) are indicated by different colors for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e207" xlink:type="simple"/></inline-formula>. In the presence of each input (shown as the colored bar above the plot), the neural activity converges to the target to be learned. After convergence, a new mapping is provided, and in the presence of the new input, the system starts to learn the new target.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002943.g006" position="float" xlink:type="simple"/>
        </fig>
        <p>Corresponding to each phase in the one-step learning, we also found two distinct behaviors in the multiple learning: (i) Neural activity responds to multiple inputs so that an attractor that matches each learned pattern is generated respectively upon each input. Thus, multiple mappings are successfully memorized. (ii) The neural activity does not respond to any input. The two attractors that match the latest learned target and its reverse pattern exist in the absence and presence of the input. Recall in response to an input is not observed either. We call these the R and NR regimes, respectively, in the same manner as the analysis for one-learning step.</p>
        <p>In <xref ref-type="fig" rid="pcbi-1002943-g007">Fig. 7</xref>, we plot the neural dynamics in the presence and absence of inputs after 40 learning steps for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e208" xlink:type="simple"/></inline-formula> in the R regime. The recall processes of 1st, 5th, and 30th targets are shown by the overlaps with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e209" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e210" xlink:type="simple"/></inline-formula> and 30 in the absence and presence of the 1st, 5th, and 30th input, respectively. From here on, the index <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e211" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e212" xlink:type="simple"/></inline-formula> 5, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e213" xlink:type="simple"/></inline-formula> in this case) denotes the order of the I/O mapping beginning with the most recent, i.e., the 1st mapping is the latest learned one, while the 5th is that learned 5 steps earlier, and so forth.</p>
        <fig id="pcbi-1002943-g007" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002943.g007</object-id>
          <label>Figure 7</label>
          <caption>
            <title>The neural dynamics after 40 learning steps in the response (R) regime.</title>
            <p><bold>A.</bold> The time series of the neural activities shown by the overlap with the 1st, 5th, and 30th targets <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e214" xlink:type="simple"/></inline-formula> in the absence and presence of the 1st (red), 5th (green), and 30th (blue) inputs (shown by the colored bars above the plot) for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e215" xlink:type="simple"/></inline-formula>. <bold>B.</bold> The time-averaged overlaps with the learned targets <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e216" xlink:type="simple"/></inline-formula> as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e217" xlink:type="simple"/></inline-formula> (squares). The overlaps with the targets and inputs averaged over the 100 networks are shown as the solid and dashed lines, respectively. <bold>C.</bold> The distributions of the overlaps of the spontaneous activity with the targets. The black line represents the distribution averaged over 10 overlaps with 10 random patterns as a control, and the others are distributions of the overlaps <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e218" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e219" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e220" xlink:type="simple"/></inline-formula> using the same colors as in A. <bold>D.</bold> The SD of the overlap with the target for the temporal evolution (squares), and the SD of the target and random pattern averaged over the 100 networks shown as the right blue and black lines, respectively.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002943.g007" position="float" xlink:type="simple"/>
        </fig>
        <p>In the R regime, by applying an input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e221" xlink:type="simple"/></inline-formula>, the overlap with the required target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e222" xlink:type="simple"/></inline-formula> increases and takes on the highest value of all overlaps. In particular, in the presence of the latest input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e223" xlink:type="simple"/></inline-formula>, the overlap with the latest target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e224" xlink:type="simple"/></inline-formula> takes a much higher value, of nearly one, and an attractor that matches the latest target is generated. Thus, the latest target is successfully recalled by applying the corresponding input. In the presence of earlier inputs, the overlaps with the requested targets take smaller values than that with the latest target, but they are still larger than the overlaps with other patterns (see <xref ref-type="supplementary-material" rid="pcbi.1002943.s001">Fig. S1</xref>), as long as the retrieved mapping is not one that was learned much earlier (as shown below). (The overlaps with the applied inputs also take higher values than the overlaps with other patterns, as well as the overlaps with the required targets. Thus, we compare the overlaps with the targets with those with the inputs in the following part.) For example, the overlap with the 5th target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e225" xlink:type="simple"/></inline-formula> is highest among the overlaps with others, in particular higher than that with the 5th input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e226" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1002943-g007">Fig. 7B</xref>). Thus, the 5th target is also recalled according to <xref ref-type="disp-formula" rid="pcbi.1002943.e034">Eq. 4</xref>. From almost all initial values, the neural activity evolves to an attractor that gives the corresponding target pattern upon application of the appropriate input. Thus, the 1st and 5th targets are always recalled. According to the definition of memory in <xref ref-type="disp-formula" rid="pcbi.1002943.e070">Eq. 6</xref>, the 1st and 5th mappings are memorized in this network. In contrast, the overlap with the 30th target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e227" xlink:type="simple"/></inline-formula>, which is learned much earlier, takes a much smaller value and is lower than the overlap with the 30th input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e228" xlink:type="simple"/></inline-formula>. Thus the network cannot recall the 30th target, i.e., the target has not been memorized. Hence the memory capacity of the present network lies between 5 and 30.</p>
        <p>To examine the memory capacity, we compute the average overlaps with the targets <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e229" xlink:type="simple"/></inline-formula> in the presence of each earlier input, as well as the average overlap with the input itself <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e230" xlink:type="simple"/></inline-formula>, as shown in <xref ref-type="fig" rid="pcbi-1002943-g007">Fig. 7B</xref>. The overlap with an earlier target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e231" xlink:type="simple"/></inline-formula> upon application of the corresponding input gradually decreases with an increase in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e232" xlink:type="simple"/></inline-formula>, while the overlap with the applied input increases. The difference between the average overlaps with the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e233" xlink:type="simple"/></inline-formula>-th target and input under the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e234" xlink:type="simple"/></inline-formula>-th input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e235" xlink:type="simple"/></inline-formula> = <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e236" xlink:type="simple"/></inline-formula> decreases with an increase in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e237" xlink:type="simple"/></inline-formula>. Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e238" xlink:type="simple"/></inline-formula> eventually crosses 0 at around 20. According to definition of memory in <xref ref-type="disp-formula" rid="pcbi.1002943.e077">Eq. 8</xref>, the system in this regime succeeds in recalling the target by applying the corresponding input to 20 I/O mappings. To reduce the artifact from the fluctuations of the overlap on memory capacity due to the finite size effect, we modify the definition of the memory capacity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e239" xlink:type="simple"/></inline-formula> slightly as<disp-formula id="pcbi.1002943.e240"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002943.e240" xlink:type="simple"/><label>(15)</label></disp-formula>Here, we set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e241" xlink:type="simple"/></inline-formula>, however, as long as the value is small, there is no essential change in the memory capacity. According to this modified definition, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e242" xlink:type="simple"/></inline-formula> is computed to be 19.</p>
        <p>We also analyze the spontaneous neural dynamics that underlie the responses to the learned inputs analyzed above in the R regime. The spontaneous neural activity shows noisy behavior, and no fixed pattern is stable, as shown in <xref ref-type="fig" rid="pcbi-1002943-g007">Fig. 7A</xref>. Irrespectively of the noisy behavior, the overlaps with the memorized targets often show high values from time to time. We compute the distributions over time of these overlaps and present them in <xref ref-type="fig" rid="pcbi-1002943-g007">Fig. 7C</xref>. The overlap distribution with the latest target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e243" xlink:type="simple"/></inline-formula> is much broader than that with a random pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e244" xlink:type="simple"/></inline-formula>, and thus, the neural activity gets selectively closer to the latest target from time to time, even in the absence of input. The distributions of the overlaps with earlier targets are also broader than that with a random pattern, even though the magnitude is smaller than that of the overlap with the latest target. Following the analysis introduced in the single-step learning, we measure the SDs of the distributions of the overlaps with all the targets, as represented by dots in <xref ref-type="fig" rid="pcbi-1002943-g007">Fig. 7D</xref>. We also compute the SD by averaging over the networks, as shown in <xref ref-type="fig" rid="pcbi-1002943-g007">Fig. 7D</xref> as the light blue line. As shown, the SDs of the later targets decrease as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e245" xlink:type="simple"/></inline-formula> increases. The major source of decrease in the SD comes from a decrease in the amplitude of the overlap.</p>
        <p>Therefore, the spontaneous activity approaches the learned targets from time to time and the closeness to the target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e246" xlink:type="simple"/></inline-formula> during the spontaneous dynamics decreases with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e247" xlink:type="simple"/></inline-formula>. The SD decreases approximately as a power law as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e248" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e249" xlink:type="simple"/></inline-formula>. This decay rate roughly agrees with that of the evoked activity, which is approximated by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e250" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e251" xlink:type="simple"/></inline-formula>. Both of the exponents are computed from a fit of the overlap and averaged SD to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e252" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e253" xlink:type="simple"/></inline-formula>, respectively, by using the least-squares method. We will analyze the dependence of the decay rates on the parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e254" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e255" xlink:type="simple"/></inline-formula> below.</p>
        <p>In the NR regime, in contrast, the latest target and its reverse pattern exist as attractors in the absence and presence of inputs for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e256" xlink:type="simple"/></inline-formula> (see <xref ref-type="supplementary-material" rid="pcbi.1002943.s002">Fig. S2</xref>). This is identical to the NR-regime behavior after one learning step, for which <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e257" xlink:type="simple"/></inline-formula> was nearly zero. Due to the stability of the latest target attractor, the neural activity does not respond to the earlier input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e258" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e259" xlink:type="simple"/></inline-formula>) either, so that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e260" xlink:type="simple"/></inline-formula> is also nearly zero. According to the definition of memory, <xref ref-type="disp-formula" rid="pcbi.1002943.e240">Eq. 15</xref>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e261" xlink:type="simple"/></inline-formula>. By decreasing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e262" xlink:type="simple"/></inline-formula> or increasing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e263" xlink:type="simple"/></inline-formula>, the reverse target attractor is less stable in the presence of the latest input, and loses stability at some parameter values, while this attractor is still stable in the absence of the input. In this region, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e264" xlink:type="simple"/></inline-formula> is equal to one, while there is still no response to an earlier input, and thus in this region, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e265" xlink:type="simple"/></inline-formula> = 1.</p>
      </sec>
      <sec id="s3d">
        <title>Bifurcation with an increase in the input strength</title>
        <p>So far, we have analyzed the spontaneous neural activity with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e266" xlink:type="simple"/></inline-formula>0 and the evoked activity with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e267" xlink:type="simple"/></inline-formula>. We now examine how the spontaneous activity is transformed into the evoked activity with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e268" xlink:type="simple"/></inline-formula>, as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e269" xlink:type="simple"/></inline-formula> is increased. This change with changing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e270" xlink:type="simple"/></inline-formula> is regarded as a bifurcation or a sequence of bifurcations in terms of the dynamical system theory. The bifurcations of the neural activity, revealed by increasing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e271" xlink:type="simple"/></inline-formula> for the 1st, 5th, and 30th input strengths for the network given in <xref ref-type="fig" rid="pcbi-1002943-g007">Fig. 7</xref>, are shown in <xref ref-type="fig" rid="pcbi-1002943-g008">Fig. 8</xref>.</p>
        <fig id="pcbi-1002943-g008" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002943.g008</object-id>
          <label>Figure 8</label>
          <caption>
            <title>Bifurcation diagram for (</title>
            <p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e272" xlink:type="simple"/></inline-formula><bold>) = (16,0.01) in the R regime.</bold> We use the network shaped after 40 learning steps. <bold>A.</bold> The local maxima in the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e273" xlink:type="simple"/></inline-formula> time series of the overlap with the target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e274" xlink:type="simple"/></inline-formula> in the presence of the corresponding input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e275" xlink:type="simple"/></inline-formula> as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e276" xlink:type="simple"/></inline-formula>. The overlaps with (i) the 1st (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e277" xlink:type="simple"/></inline-formula>), (ii) 5th (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e278" xlink:type="simple"/></inline-formula>), and (iii) 30th (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e279" xlink:type="simple"/></inline-formula>) targets are plotted in red, green, and blue, respectively, while the data in black represent the overlap with each input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e280" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e281" xlink:type="simple"/></inline-formula>). <bold>B.</bold> The number of positive Lyapunov exponents of these evoked dynamics as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e282" xlink:type="simple"/></inline-formula>. Lyapunov exponents are calculated from the time series <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e283" xlink:type="simple"/></inline-formula> according to the algorithm in <xref ref-type="bibr" rid="pcbi.1002943-vonBremen1">[56]</xref>.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002943.g008" position="float" xlink:type="simple"/>
        </fig>
        <p>In the R regime, the overlap with the 1st (i.e., latest) target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e284" xlink:type="simple"/></inline-formula> increases monotonically and continuously by increasing the strength of the 1st input. Finally, the fixed point that matches the 1st target is generated for not only the network used in the figure, but also most of the networks in the R regime. The change to a fixed point is understood as a low-dimensional bifurcation, while the whole sequence of neural activity changes involves higher-dimensional dynamics. For the 5th and 30th inputs, the overlap with the corresponding input is increased continuously with an increase in the input strength, in a similar manner as the bifurcation diagram for the 1st input. In contrast to the latest input, however, the attractor is not a fixed-point attractor even for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e285" xlink:type="simple"/></inline-formula>, where the evoked activity still shows chaotic behavior.</p>
        <p>Apart from the change to a fixed-point attractor, the bifurcation sequences involve a large degree of freedom in a high-dimensional (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e286" xlink:type="simple"/></inline-formula>) space. Hence, plotting a few macroscopic variables, i.e., the overlaps of the neural activity with a few targets, is not sufficient to capture the entire bifurcation sequence. Therefore, to consider the chaotic dynamics, we measured the Lyapunov spectrum for the neural activity dynamics. With an increase in the input strength, the number of positive Lyapunov exponents decreases, implying the existence of successive bifurcations from a high-dimensional attractor to a lower-dimensional attractor (see <xref ref-type="fig" rid="pcbi-1002943-g008">Fig. 8</xref>). Accordingly, the dimension of the neural-activity attractor also decreases. No positive Lyapunov exponents exist once the fixed-point attractor is reached for the input that was just learned, while even for the application of an earlier input, a decrease in the number of positive exponents is observed but the number does not reach zero.</p>
        <p>In the NR regime, the latest target and reverse-target fixed-point attractors exist with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e287" xlink:type="simple"/></inline-formula>. Even by increasing the input strength, these attractors remain stable and no bifurcation occurs.</p>
      </sec>
      <sec id="s3e">
        <title>Dependence of the learned neural activities on the input strength and learning parameters</title>
        <p>The dependence of the spontaneous and evoked activities on the two parameters, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e288" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e289" xlink:type="simple"/></inline-formula>, are analyzed through the capacity and SD. The dependence of the evoked activity is explored by measuring the capacity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e290" xlink:type="simple"/></inline-formula> according to <xref ref-type="disp-formula" rid="pcbi.1002943.e240">Eq. 15</xref>, with the results shown in <xref ref-type="fig" rid="pcbi-1002943-g009">Fig. 9A</xref>. In the R regime with a larger <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e291" xlink:type="simple"/></inline-formula> and smaller <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e292" xlink:type="simple"/></inline-formula>, a high capacity is observed, while in the NR regime with a smaller <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e293" xlink:type="simple"/></inline-formula> and larger <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e294" xlink:type="simple"/></inline-formula>, the capacity is zero or one. Over the entire parameter space, the overlap with the requested target in the presence of an earlier input decreases, i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e295" xlink:type="simple"/></inline-formula> decreases as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e296" xlink:type="simple"/></inline-formula> increases, while that with the corresponding input increases. However, the decay rate of the overlap with the target as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e297" xlink:type="simple"/></inline-formula> and the growth rate of the overlap with the input are dependent on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e298" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e299" xlink:type="simple"/></inline-formula>.</p>
        <fig id="pcbi-1002943-g009" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002943.g009</object-id>
          <label>Figure 9</label>
          <caption>
            <title>Dependence of the evoked and spontaneous activities on </title>
            <p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e300" xlink:type="simple"/></inline-formula><bold> and </bold><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e301" xlink:type="simple"/></inline-formula><bold>.</bold> <bold>A.</bold> The capacity (as defined in the main text). The dotted line denotes the boundary of the R regime, computed by the line where the memory capacity goes beyond one, with smoothing the line. <bold>B.</bold> The average SD of the spontaneous activity. In A and B, we computed the capacity and SD by averaging over 100 network and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e302" xlink:type="simple"/></inline-formula>. <bold>C.</bold> The temporal evolution of the overlap with the latest target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e303" xlink:type="simple"/></inline-formula> in the absence (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e304" xlink:type="simple"/></inline-formula>) and presence (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e305" xlink:type="simple"/></inline-formula>) of the latest input with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e306" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e307" xlink:type="simple"/></inline-formula> in (i) and for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e308" xlink:type="simple"/></inline-formula> in (ii), indicated by (i), and (ii), for in A and B. For (ii), results from two initial conditions that lead to differed attractors are plotted. <bold>D.</bold> The average of the overlap with the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e309" xlink:type="simple"/></inline-formula>-th target in the presence of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e310" xlink:type="simple"/></inline-formula>-th input (magenta line) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e311" xlink:type="simple"/></inline-formula> and the SD of the spontaneous overlap (right blue line) plotted as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e312" xlink:type="simple"/></inline-formula> for the parameter set indicated by (i) and (ii) in A and B. <bold>E.</bold> The exponents <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e313" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e314" xlink:type="simple"/></inline-formula>, computed from a fit of the overlap and averaged SD to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e315" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e316" xlink:type="simple"/></inline-formula>, respectively. Both <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e317" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e318" xlink:type="simple"/></inline-formula> are computed for different <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e319" xlink:type="simple"/></inline-formula> by fixing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e320" xlink:type="simple"/></inline-formula> as represented by the magenta and right blue lines, respectively. <bold>F.</bold> The capacity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e321" xlink:type="simple"/></inline-formula> for different <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e322" xlink:type="simple"/></inline-formula> by fixing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e323" xlink:type="simple"/></inline-formula>.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002943.g009" position="float" xlink:type="simple"/>
        </fig>
        <p>For a large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e324" xlink:type="simple"/></inline-formula> and small <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e325" xlink:type="simple"/></inline-formula>, e.g., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e326" xlink:type="simple"/></inline-formula> as shown in <xref ref-type="fig" rid="pcbi-1002943-g007">Fig. 7B</xref>, the decay rate of the overlap with the target as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e327" xlink:type="simple"/></inline-formula> is small, as well as the growth rate of the overlap with the input. In general, when the capacity is higher, response to an earlier input is higher and the decay rates are lower. As the parameters approach the NR regime and the memory capacity decreases with a decrease in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e328" xlink:type="simple"/></inline-formula> and increase in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e329" xlink:type="simple"/></inline-formula>, these rates become larger (see <xref ref-type="fig" rid="pcbi-1002943-g007">Figs. 7B</xref> and <xref ref-type="fig" rid="pcbi-1002943-g009">9D(i)</xref>). Finally, in the NR regime, the rates reach maximal value, and the network responds only to the most recently learned input and not to any other input, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e330" xlink:type="simple"/></inline-formula> (see <xref ref-type="fig" rid="pcbi-1002943-g009">Fig. 9D(ii)</xref>).</p>
        <p>To explore the dependence of the spontaneous activity, we measure the average SD of the spontaneous activity over the learned mappings,<disp-formula id="pcbi.1002943.e331"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1002943.e331" xlink:type="simple"/><label>(16)</label></disp-formula>as shown in <xref ref-type="fig" rid="pcbi-1002943-g009">Fig. 9B</xref>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e332" xlink:type="simple"/></inline-formula> is set to 30. When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e333" xlink:type="simple"/></inline-formula> is larger, the decay rate of SD(<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e334" xlink:type="simple"/></inline-formula>) is smaller.</p>
        <p>For a large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e335" xlink:type="simple"/></inline-formula> and small <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e336" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e337" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e338" xlink:type="simple"/></inline-formula> takes a higher value, the spontaneous activity approaches not only the latest target, but also an earlier target from time to time, as shown in <xref ref-type="fig" rid="pcbi-1002943-g007">Fig. 7D</xref>. The closeness to the target, as seen by the decrease in the SD of the overlap with an earlier target, decreases for targets memorized earlier. As <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e339" xlink:type="simple"/></inline-formula> decreases and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e340" xlink:type="simple"/></inline-formula> increases, and the system approaches the NR regime, the average SD decreases and this decay rate increases; the spontaneous activity approaches the latest target selectively as shown by the small distance between the spontaneous activity and the latest target (see <xref ref-type="fig" rid="pcbi-1002943-g009">Fig. 9C(i)</xref>). Finally, in the NR regime, the activity in the absence of input falls on the latest target and reverse-target pattern (or the localized fluctuations around these patterns) (see <xref ref-type="fig" rid="pcbi-1002943-g009">Fig. 9C(ii)</xref>).</p>
        <p>The decay rates of the overlap with the evoked activity and the SD of the spontaneous activity in the R regime were seen to obey power laws of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e341" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e342" xlink:type="simple"/></inline-formula>, respectively, and the two exponents <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e343" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e344" xlink:type="simple"/></inline-formula> have a similar value for and dependence on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e345" xlink:type="simple"/></inline-formula>, as shown in <xref ref-type="fig" rid="pcbi-1002943-g009">Fig. 9E</xref>. This suggests that the approach of the spontaneous activity to the target is correlated with the activity evoked in response to the corresponding input.</p>
        <p>Both of the two exponents decrease for a larger <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e346" xlink:type="simple"/></inline-formula> and smaller <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e347" xlink:type="simple"/></inline-formula>. For much larger <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e348" xlink:type="simple"/></inline-formula> and much smaller <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e349" xlink:type="simple"/></inline-formula> values, these decreases become saturated, and the curves of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e350" xlink:type="simple"/></inline-formula> and SD(<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e351" xlink:type="simple"/></inline-formula>) as functions of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e352" xlink:type="simple"/></inline-formula> no longer change with an increase in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e353" xlink:type="simple"/></inline-formula>. Thus, the capacities for different <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e354" xlink:type="simple"/></inline-formula> values become also saturated and take a common value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e355" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1002943-g009">Fig. 9F</xref>). In other words, for a sufficiently large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e356" xlink:type="simple"/></inline-formula> and small <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e357" xlink:type="simple"/></inline-formula>, the capacity in this model with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e358" xlink:type="simple"/></inline-formula> takes a constant value of 20. Further, from results for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e359" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e360" xlink:type="simple"/></inline-formula>, we have confirmed that this capacity is proportional to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e361" xlink:type="simple"/></inline-formula>; the capacity has a universal limit of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e362" xlink:type="simple"/></inline-formula> (see <xref ref-type="supplementary-material" rid="pcbi.1002943.s003">Fig. S3</xref>).</p>
        <p>Note that the R and NR regimes are clearly distinguishable mathematically. Although the boundary between them might slightly ambiguous, as seen in <xref ref-type="fig" rid="pcbi-1002943-g009">Figs. 9A and B</xref> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e363" xlink:type="simple"/></inline-formula> because of the finite-size effect, it is clearer with the increase in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e364" xlink:type="simple"/></inline-formula>, and, in the thermodynamic limit, it is expected that the memory capacity is equal to one (or zero) as is the fixed-point spontaneous activity, i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e365" xlink:type="simple"/></inline-formula>, for all networks in the NR regime. In the R regime, in contrast, spontaneous activity shows chaotic behavior, i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e366" xlink:type="simple"/></inline-formula>, for all networks, and the memory capacity increases linearly with size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e367" xlink:type="simple"/></inline-formula>, as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e368" xlink:type="simple"/></inline-formula>. The proportion coefficient 0.2 may be slightly varied according to the criterion for the memory capacity, but the proportionality to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e369" xlink:type="simple"/></inline-formula> is invariant. Hence, the boundary between R and NR is clearly defined.</p>
      </sec>
      <sec id="s3f">
        <title>Connection matrix shaped through multiple learning steps</title>
        <p>Finally, we analyze the connection matrix by measuring the elements of the matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e370" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e371" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e372" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e373" xlink:type="simple"/></inline-formula> as defined in <xref ref-type="disp-formula" rid="pcbi.1002943.e136">Eq. 9</xref>. In <xref ref-type="fig" rid="pcbi-1002943-g010">Fig. 10</xref>, we show the elements in both the R and NR regimes and also in the border between them. The elements in the R regime take comparable values for each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e374" xlink:type="simple"/></inline-formula>, and decrease with an increase in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e375" xlink:type="simple"/></inline-formula>, but the decay rates are rather small compared with those in the NR regime. Thus, for each mapping, the analysis of the network structure in the R regime after a single learning step is also valid after multiple learning steps. The network structure in which <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e376" xlink:type="simple"/></inline-formula> underlies the chaotic spontaneous activity with high closeness to the learned target patterns and successful recall of the target upon application of the corresponding input. At the border between the R and the NR regimes, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e377" xlink:type="simple"/></inline-formula> is much larger, while <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e378" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e379" xlink:type="simple"/></inline-formula> decreases rapidly with an increase in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e380" xlink:type="simple"/></inline-formula>. This network structure makes the approach of the spontaneous activity to the latest target (and reverse-target pattern) much closer as shown in <xref ref-type="fig" rid="pcbi-1002943-g009">Fig. 9C(i)</xref>.</p>
        <fig id="pcbi-1002943-g010" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1002943.g010</object-id>
          <label>Figure 10</label>
          <caption>
            <title>The matrix elements in the presence of the targets </title>
            <p><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e381" xlink:type="simple"/></inline-formula><bold> and inputs </bold><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e382" xlink:type="simple"/></inline-formula><bold>.</bold> The matrix elements <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e383" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e384" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e385" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e386" xlink:type="simple"/></inline-formula> plotted as functions of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e387" xlink:type="simple"/></inline-formula> for <bold>A</bold>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e388" xlink:type="simple"/></inline-formula> = (16, 0.01) in the R regime, <bold>B</bold>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e389" xlink:type="simple"/></inline-formula> = (2.6, 0.01) in the boundary regime, and <bold>C</bold>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e390" xlink:type="simple"/></inline-formula> = (1,0.5) in the NR regime. The same colors as those used in <xref ref-type="fig" rid="pcbi-1002943-g005">Fig. 5</xref> are used here. The error bars represent the standard deviation.</p>
          </caption>
          <graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1002943.g010" position="float" xlink:type="simple"/>
        </fig>
        <p>In the NR regime (i.e., with a much smaller <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e391" xlink:type="simple"/></inline-formula> and much larger <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e392" xlink:type="simple"/></inline-formula>), the decay rate of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e393" xlink:type="simple"/></inline-formula> is much larger than that in the R regime and, only <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e394" xlink:type="simple"/></inline-formula> takes a significant value. For the latest mapping, the network structure is similar to that in the NR regime after one learning step as analyzed above. This is consistent with the existence of only the latest target and reverse-target attractors in the spontaneous activity and the absence of response to any input.</p>
      </sec>
    </sec>
    <sec id="s4">
      <title>Discussion</title>
      <p>We have proposed an associative memory model with a simple learning rule that realizes the viewpoint of memories-as-bifurcations in which neural activities are transformed appropriately by each input to generate the requested targets. Using this viewpoint, we have analyzed the spontaneous activity and its response to a memorized input. With a Hebbian-type synaptic change based on the correlations between the pre- and postsynaptic neurons, the model succeeds in memorizing I/O mappings by sequential learning without losing earlier memories up to a capacity of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e395" xlink:type="simple"/></inline-formula> for a sufficient large <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e396" xlink:type="simple"/></inline-formula> and small <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e397" xlink:type="simple"/></inline-formula>. In the absence of input, the neural activity typically shows chaotic dynamics, while approaches to memorized target patterns are repeated from time to time. Upon inputs with adequate strength, e.g., the same as that used in the learning, flow structure of the neural activity is changed and the neural activity evolves into an attractor that matches the requested target pattern corresponding to each input. The neural activity dynamics change from spontaneous activity with a large variability in a high-dimensional state space to a lower-dimensional state that loses the variability with an increase in the input strength, which we understand as successive bifurcations. Interestingly, the synaptic connections generated by learning share a common property as those that were previously designed based on correlations in the input and target patterns <xref ref-type="bibr" rid="pcbi.1002943-Kurikawa2">[34]</xref>. We outline here the significance of our viewpoint and the consequences of our results for neuroscience.</p>
      <sec id="s4a">
        <title>Memories as a result of sequential learning</title>
        <p>We introduced a sequential learning rule, to match the memories as bifurcations viewpoint, by adopting a simple rule based on the correlations between the activities of the pre- and postsynaptic neurons; the rule is similar to the perceptron learning rule <xref ref-type="bibr" rid="pcbi.1002943-Rosenblatt1">[13]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Dayan1">[16]</xref>.</p>
        <p>Sequential learning or palimpsest learning have been studied over a few decades <xref ref-type="bibr" rid="pcbi.1002943-Nadal1">[38]</xref>–<xref ref-type="bibr" rid="pcbi.1002943-Parisi1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Fusi1">[42]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Amit4">[43]</xref>, and it has been shown that learning a new I/O mapping can easily destroy traces of previously memorized target patterns to such an extent that the memory capacity is lower than that for non-sequential learning. Methods have been proposed to alleviate the decrease in the capacity by decreasing the degree of synaptic plasticity, for example, by decreasing the number of the synapses that change simultaneously <xref ref-type="bibr" rid="pcbi.1002943-Fusi1">[42]</xref>. However, the destruction of earlier attractors due to the formation of new attractors is still a general trend as long as the memorized targets are attractors in the same dynamical system.</p>
        <p>From our viewpoint, in contrast, the attraction to a new learned target is shaped under a “different” dynamical system because each system exists in the presence of a different input pattern, and as we demonstrated, the neural network does not completely lose the memory learned earlier; the capacity is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e398" xlink:type="simple"/></inline-formula> for an input strength <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e399" xlink:type="simple"/></inline-formula> that is sufficiently large and a rate of synaptic change <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e400" xlink:type="simple"/></inline-formula> that is sufficiently smaller than that of the change in the neural activity. For larger <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e401" xlink:type="simple"/></inline-formula> values, the system under the new input deviates farther from that without input and from that with the previously learned inputs so that the traces of the previously learned memories are not destroyed. For small <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e402" xlink:type="simple"/></inline-formula>, in contrast, the system under the input is close to that without input, so that the traces are easily destroyed. For a larger <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e403" xlink:type="simple"/></inline-formula>, on the other hand, the change in the synaptic connection is larger so that traces of previously learned memories are destroyed, while the synaptic connection is enhanced and selectively stabilizes the new target pattern. Indeed, for a larger <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e404" xlink:type="simple"/></inline-formula> and smaller <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e405" xlink:type="simple"/></inline-formula> area, only a highly stable attractor that matches the latest target is generated by removing earlier memories, and thus multiple mappings are not memorized.</p>
        <p>The simplicity of our learning scheme may have potential applications for the learning algorithm of I/O mappings. A limitation in our model is that the target information is supplied to all neurons because we used all-to-all recurrent connections. This limitation can be overcome by appropriately introducing a layered network structure and reinforcement learning algorithm <xref ref-type="bibr" rid="pcbi.1002943-Kurikawa1">[33]</xref> into the present learning algorithm. In addition, the present scheme is based on Hebbian-type synaptic changes that use only the pre- and postsynaptic neural activities and the target information under the presence of input; this means it may be plausible to expect the existence of such synaptic dynamics in biological neural system.</p>
      </sec>
      <sec id="s4b">
        <title>Spontaneous activity and bifurcation into evoked activity</title>
        <p>There have been extensive experimental studies on the responses of neural activities to external stimuli in the sensory cortex <xref ref-type="bibr" rid="pcbi.1002943-Hubel1">[1]</xref>–<xref ref-type="bibr" rid="pcbi.1002943-Miyashita1">[4]</xref> and higher cortex area <xref ref-type="bibr" rid="pcbi.1002943-Fujiwara1">[5]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Friederici1">[6]</xref>. Pre-stimuli, spontaneous activity had been dismissed as a background noise in these studies, but in recent experimental studies, it has been demonstrated that spontaneous neural activity without sensory input is not simple noise but is in fact highly structured in time and space <xref ref-type="bibr" rid="pcbi.1002943-Fox1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Luczak2">[44]</xref>. In particular, spontaneous activity is often found to exhibit transitory behavior among several activity patterns that are similar to those evoked by external stimuli <xref ref-type="bibr" rid="pcbi.1002943-Berkes1">[32]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Kenet1">[35]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Luczak1">[37]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Tsodyks1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Sakata1">[46]</xref>. In other words, spontaneous activity includes some patterns evoked by external stimuli <xref ref-type="bibr" rid="pcbi.1002943-Deco1">[18]</xref>. Thus, spontaneous activity that is widespread and wanders over many patterns converges to one patterns by applying an input. If one observes a discontinuous change in the neural activity by increasing the input strength, we expect that the change will be interpreted as a bifurcation.</p>
        <p>In the present study, we analyzed the transformation of the spontaneous to evoked activity from the memories-as-bifurcations viewpoint; we found that spontaneous activity that is chaotic but that often approaches the memorized targets is shaped by learning. This is reminiscent of the similarity between the spontaneous and evoked activities noted in the above experimental studies. Interestingly, if the spontaneous activity makes a closer approach to some target patterns, the inputs corresponding to those targets generate a higher neural activity response. This correlation between the responsiveness to a given input and the spontaneous activity may suggest a possible role of the spontaneous activity in preparing the response to the input.</p>
        <p>There have been several studies of neural-network models of the spontaneous activity in neural dynamics in random networks or models of working memory <xref ref-type="bibr" rid="pcbi.1002943-Amit2">[24]</xref>–<xref ref-type="bibr" rid="pcbi.1002943-Brunel2">[27]</xref>. Spontaneous activities that visit several patterns have been investigated as chaotic itinerancy over patterns <xref ref-type="bibr" rid="pcbi.1002943-Tsuda1">[47]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Kaneko1">[48]</xref> or heteroclinic channels <xref ref-type="bibr" rid="pcbi.1002943-Rabinovich1">[49]</xref>. Our focus here lies in understanding whether such structure can be shaped by a simple learning rule and elucidating the characteristic behavior of the shaped spontaneous activity. Thus, our findings can also shed some light on how such transitory neural dynamics are generated.</p>
        <p>We should note that, as an alternative approach contesting the memories-as-attractors viewpoint, the so-called liquid state machine was proposed <xref ref-type="bibr" rid="pcbi.1002943-Maass1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Jaeger1">[50]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Ganguli1">[51]</xref>, where learning I/O mapping was also achieved without multiple attractors. In this machine, there is a “reservoir” that stores the trace of the input and a “read-out unit” that detects this trace and transfers it to the desired output, while learning modifies only the read-out unit to generate the desired output. In our study, in contrast, there is no read-out unit, but the internal neural-activity dynamics (which corresponds to the reservoir) is modified during the learning process. With this approach, we can study spontaneous neural activity dynamics and evoked activity dynamics, which are not considered in the liquid-state machine.</p>
      </sec>
      <sec id="s4c">
        <title>Simple learning rule can shape the spontaneous activity wandering among the memorized targets</title>
        <p>A recent study by Berkes et al., <xref ref-type="bibr" rid="pcbi.1002943-Berkes1">[32]</xref> has demonstrated that the similarity between the spontaneous and evoked neural activities is not an innate property but is shaped through a developmental process; the dynamics of the activities are expected to be modified by the experience-dependent synaptic plasticity, and as a result, the similarity is believed to be shaped. We have shown that such a similarity is shaped through sequential Hebbian learning. In addition, we have found that in the network connection matrix, the characteristic pattern of the matrix elements (<xref ref-type="disp-formula" rid="pcbi.1002943.e075">Eq. 7</xref>) is also shaped, although the learning rule can form another characteristic pattern of network connections. In a parameter regime without any memory capacity, only the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e406" xlink:type="simple"/></inline-formula> element is significant. In striking contrast, in a regime with memory capacity of I<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e407" xlink:type="simple"/></inline-formula>O mappings, the values of the elements of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e408" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e409" xlink:type="simple"/></inline-formula> are of a comparable order, with the former two being positive and the latter two being negative. This network structure (the sign of each element) is found to be in common with the network in <xref ref-type="bibr" rid="pcbi.1002943-Kurikawa2">[34]</xref>, which was designed to achieve appropriate bifurcations upon certain inputs by superposing connections generated by the correlation between each target and input pattern with equal weight. In the present study, such connections, even though the weights are biased to recently memorized patterns, are generated as a result of a simple learning rule. This demonstrates the generality of the memories-as-bifurcation viewpoint and the existence of a variety of connections for its implementation.</p>
      </sec>
      <sec id="s4d">
        <title>Biological plausibility of the synaptic dynamics</title>
        <p>Finally, we briefly discuss the biological plausibility of our learning rule. Indeed, it does not follow the Hebbian unsupervised learning adopted in standard models for the cerebrum cortex with recurrent neural connections. Still, our learning rule also satisfies a minimum requirement for a biological neural system <xref ref-type="bibr" rid="pcbi.1002943-Fusi1">[42]</xref>: a learning rule needs only local information for pre- and postsynaptic cells and does not require any global information, which is difficult for each neuron to obtain. In fact, our learning model given by <xref ref-type="disp-formula" rid="pcbi.1002943.e029">Eq. 2</xref> needs information on only the neural activity of the pre- and postsynaptic cells and the target activity in the postsynaptic cell.</p>
        <p>The learning rule (<xref ref-type="disp-formula" rid="pcbi.1002943.e029">Eq. 2</xref>) consists of two parts: an anti-Hebbian part <xref ref-type="bibr" rid="pcbi.1002943-Falconbridge1">[52]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Destexhe2">[53]</xref>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e410" xlink:type="simple"/></inline-formula>, and the supervised part, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e411" xlink:type="simple"/></inline-formula>. First, a possible interpretation of the anti-Hebbain rule can be provided by introducing an interneuron. It is known that the excitatory neurons (pyramidal neurons) are connected through inhibitory neurons (interneurons) in the sensory cortex. When activations of pre- and post excitatory neurons are correlated and synapses between the presynaptic excitatory neuron and the inhibitory interneuron and those between the interneuron and the postsynaptic neuron are strengthened by the Hebbian rule, the efficacy between the pre- and postsynaptic neurons is effectively weakened. Instead of taking into account these intermediate neurons explicitly, one could eliminate variables for the interneurons and consider effective direct coupling between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e412" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e413" xlink:type="simple"/></inline-formula>, as in our model. In this case, the coupling between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e414" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e415" xlink:type="simple"/></inline-formula> follows anti-Hebbian plasticity of the synapse.</p>
        <p>To discuss the plausibility of the supervised part, let us consider another network whose activity represents target pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e416" xlink:type="simple"/></inline-formula> and which projects onto the network in our model. Here, the target pattern does not represent a signal to error of the output behavior, as often used in supervised learning models in the cerebellum cortex <xref ref-type="bibr" rid="pcbi.1002943-Kawato1">[54]</xref>, <xref ref-type="bibr" rid="pcbi.1002943-Wolpert1">[55]</xref>, but represents only the neural activity pattern to be learned. The term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e417" xlink:type="simple"/></inline-formula> represents a simple Hebbian change between the presynaptic neurons in the network and the other network representing the target. This Hebbian change enables learning the correlation between the activities in the target network and in the presynaptic neurons. This example is only one possible way to implement our model in a biological neural network, and future studies are needed to establish a link between our learning rule and more biological neural-network dynamics.</p>
      </sec>
    </sec>
    <sec id="s5">
      <title>Supporting Information</title>
      <supplementary-material id="pcbi.1002943.s001" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1002943.s001" position="float" xlink:type="simple">
        <label>Figure S1</label>
        <caption>
          <p><bold>Overlap with the target and the input patterns after 40 learning steps in the R regime.</bold> <bold>A.</bold> The average overlap <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e418" xlink:type="simple"/></inline-formula> in the presence of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e419" xlink:type="simple"/></inline-formula>-th input as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e420" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e421" xlink:type="simple"/></inline-formula>. <bold>B.</bold> The average overlap <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e422" xlink:type="simple"/></inline-formula> in the presence of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e423" xlink:type="simple"/></inline-formula>-th input as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e424" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e425" xlink:type="simple"/></inline-formula>. We used the same parameters as those in <xref ref-type="fig" rid="pcbi-1002943-g007">Fig. 7</xref> and computed the overlap by averaging over 100 network and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e426" xlink:type="simple"/></inline-formula>. One can find that, upon application of an input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e427" xlink:type="simple"/></inline-formula>, the overlap with the requested target <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e428" xlink:type="simple"/></inline-formula> is selectively higher than the other overlaps.</p>
          <p>(EPS)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002943.s002" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1002943.s002" position="float" xlink:type="simple">
        <label>Figure S2</label>
        <caption>
          <p><bold>The neural dynamics after 40 learning steps in the non-response (NR) regime.</bold> <bold>A.</bold> The time series of the neural activities shown by the overlap with the 1st, 5th, and 30th targets <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e429" xlink:type="simple"/></inline-formula> in the absence and presence of the 1st (red), 5th (green), and 30th (blue) inputs (shown by the colored bars above the plot) for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e430" xlink:type="simple"/></inline-formula>. <bold>B.</bold> The time-averaged overlaps with the learned targets as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e431" xlink:type="simple"/></inline-formula> (squares). The time- and ensemble-averaged overlaps with the targets and inputs are shown as the solid and dashed lines, respectively. <bold>C.</bold> The average overlap <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e432" xlink:type="simple"/></inline-formula> in the presence of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e433" xlink:type="simple"/></inline-formula>-th input as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e434" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e435" xlink:type="simple"/></inline-formula>. <bold>D.</bold> The average overlap <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e436" xlink:type="simple"/></inline-formula> in the presence of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e437" xlink:type="simple"/></inline-formula>-th input as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e438" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e439" xlink:type="simple"/></inline-formula>. In all figures, we used the time series <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e440" xlink:type="simple"/></inline-formula> as the time-averaged overlap and the ensemble-averaged one.</p>
          <p>(EPS)</p>
        </caption>
      </supplementary-material>
      <supplementary-material id="pcbi.1002943.s003" mimetype="application/postscript" xlink:href="info:doi/10.1371/journal.pcbi.1002943.s003" position="float" xlink:type="simple">
        <label>Figure S3</label>
        <caption>
          <p><bold>Dependence of evoked neural activity on the number of the elements.</bold> The overlap of evoked activity with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e441" xlink:type="simple"/></inline-formula>-th target for different <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e442" xlink:type="simple"/></inline-formula> is shown as a function of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e443" xlink:type="simple"/></inline-formula> divided by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e444" xlink:type="simple"/></inline-formula>. The curves for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e445" xlink:type="simple"/></inline-formula> converge to a unique curve, by scaling the index <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e446" xlink:type="simple"/></inline-formula> of the learned mappings divided by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e447" xlink:type="simple"/></inline-formula>. We computed the overlaps by averaging over time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e448" xlink:type="simple"/></inline-formula> for all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e449" xlink:type="simple"/></inline-formula> and over 100 networks for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e450" xlink:type="simple"/></inline-formula> and 50 networks for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1002943.e451" xlink:type="simple"/></inline-formula>.</p>
          <p>(EPS)</p>
        </caption>
      </supplementary-material>
    </sec>
  </body>
  <back>
    <ack>
      <p>The authors would like to thank D. Colliaux for useful discussions.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pcbi.1002943-Hubel1">
        <label>1</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hubel</surname><given-names>DH</given-names></name>, <name name-style="western"><surname>Wiesel</surname><given-names>TN</given-names></name> (<year>1962</year>) <article-title>Receptive fields, binocular interaction and functional architecture in the cat's visual cortex</article-title>. <source>J physiol</source> <volume>160</volume>: <fpage>106</fpage>–<lpage>54</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Tusa1">
        <label>2</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tusa</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Palmer</surname><given-names>LA</given-names></name>, <name name-style="western"><surname>Rosenquist</surname><given-names>AC</given-names></name> (<year>1978</year>) <article-title>The retinotopic organization of area 17 (striate cortex) in the cat</article-title>. <source>J comp neurol</source> <volume>177</volume>: <fpage>213</fpage>–<lpage>35</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Merzenich1">
        <label>3</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Merzenich</surname><given-names>MM</given-names></name>, <name name-style="western"><surname>Knight</surname><given-names>PL</given-names></name>, <name name-style="western"><surname>Roth</surname><given-names>GL</given-names></name> (<year>1975</year>) <article-title>Representation of cochlea within primary auditory cortex in the cat</article-title>. <source>J neurophysiol</source> <volume>38</volume>: <fpage>231</fpage>–<lpage>49</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Miyashita1">
        <label>4</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miyashita</surname><given-names>Y</given-names></name> (<year>1988</year>) <article-title>Neuronal correlate of visual associative long-term memory in the primate temporal cortex</article-title>. <source>Nature</source> <volume>335</volume>: <fpage>817</fpage>–<lpage>820</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Fujiwara1">
        <label>5</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fujiwara</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Tobler</surname><given-names>PN</given-names></name>, <name name-style="western"><surname>Taira</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Iijima</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Tsutsui</surname><given-names>KI</given-names></name> (<year>2009</year>) <article-title>A parametric relief signal in human ventrolateral prefrontal cortex</article-title>. <source>NeuroImage</source> <volume>44</volume>: <fpage>1163</fpage>–<lpage>70</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Friederici1">
        <label>6</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friederici</surname><given-names>AD</given-names></name> (<year>2003</year>) <article-title>The Role of Left Inferior Frontal and Superior Temporal Cortex in Sentence Comprehension: Localizing Syntactic and Semantic Processes</article-title>. <source>Cereb Cortex</source> <volume>13</volume>: <fpage>170</fpage>–<lpage>177</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-BenYishai1">
        <label>7</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ben-Yishai</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Bar-Or</surname><given-names>RL</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>1995</year>) <article-title>Theory of orientation tuning in visual cortex</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>92</volume>: <fpage>3844</fpage>–<lpage>3848</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Durstewitz1">
        <label>8</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Durstewitz</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Kelc</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Gunturkun</surname><given-names>O</given-names></name> (<year>1999</year>) <article-title>A Neurocomputational Theory of the Dopaminergic Modulation of Working Memory Functions</article-title>. <source>J Neurosci</source> <volume>19</volume>: <fpage>2807</fpage>–<lpage>2822</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Willshaw1">
        <label>9</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Willshaw</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Buneman</surname><given-names>OP</given-names></name>, <name name-style="western"><surname>Longuet-higgins</surname><given-names>HC</given-names></name> (<year>1969</year>) <article-title>Non-Holographic Associative Memory</article-title>. <source>Nature</source> <volume>222</volume>: <fpage>960</fpage>–<lpage>962</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Hopfield1">
        <label>10</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name> (<year>1984</year>) <article-title>Neurons with graded response have collective computational properties like those of two-state neurons</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>81</volume>: <fpage>3088</fpage>–<lpage>3092</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Amit1">
        <label>11</label>
        <mixed-citation publication-type="other" xlink:type="simple">Amit DJ (1992) Modeling Brain Function: The World of Attractor Neural Networks. Cambridge: Cambridge University Press. 504 p.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Maass1">
        <label>12</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maass</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Natschläger</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Markram</surname><given-names>H</given-names></name> (<year>2002</year>) <article-title>Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations</article-title>. <source>Neural Comput</source> <volume>14</volume>: <fpage>2531</fpage>–<lpage>2560</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Rosenblatt1">
        <label>13</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rosenblatt</surname><given-names>F</given-names></name> (<year>1958</year>) <article-title>The perceptron: A probabilistic model for information storage and organization in the brain</article-title>. <source>Psychol Rev</source> <volume>65</volume>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Rumelhart1">
        <label>14</label>
        <mixed-citation publication-type="other" xlink:type="simple">Rumelhart DE, Mcclelland JL (1986). Parallel distributed processing: explorations in the microstructure of cognition. Volume 1. Foundations of Research. Cambridge (Massachusetts): MIT Press.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Gardner1">
        <label>15</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gardner</surname><given-names>E</given-names></name> (<year>1988</year>) <article-title>The space of interactions in neural network models</article-title>. <source>J Phys A: Math Gen</source> <volume>21</volume>: <fpage>257</fpage>–<lpage>270</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Dayan1">
        <label>16</label>
        <mixed-citation publication-type="other" xlink:type="simple">Dayan P, Abbott LF (2001) Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems. Cambridge, MA: MIT Press.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Fox1">
        <label>17</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fox</surname><given-names>MD</given-names></name>, <name name-style="western"><surname>Raichle</surname><given-names>ME</given-names></name> (<year>2007</year>) <article-title>Spontaneous fluctuations in brain activity observed with functional magnetic resonance imaging</article-title>. <source>Nat Rev Neurosci</source> <volume>8</volume>: <fpage>700</fpage>–<lpage>711</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Deco1">
        <label>18</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Deco</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Jirsa</surname><given-names>VK</given-names></name>, <name name-style="western"><surname>McIntosh</surname><given-names>AR</given-names></name> (<year>2011</year>) <article-title>Emerging concepts for the dynamical organization of resting-state activity in the brain</article-title>. <source>Nat Rev Neurosci</source> <volume>12</volume>: <fpage>43</fpage>–<lpage>56</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Destexhe1">
        <label>19</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Destexhe</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Contreras</surname><given-names>D</given-names></name> (<year>2006</year>) <article-title>Neuronal computations with stochastic network states</article-title>. <source>Science (New York, NY)</source> <volume>314</volume>: <fpage>85</fpage>–<lpage>90</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Arieli1">
        <label>20</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Arieli</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Sterkin</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Grinvald</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Aertsen</surname><given-names>A</given-names></name> (<year>1996</year>) <article-title>Dynamics of Ongoing Activity: Explanation of the Large Variability in Evoked Cortical Responses</article-title>. <source>Science</source> <volume>273</volume>: <fpage>1868</fpage>–<lpage>1871</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Kisley1">
        <label>21</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kisley</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Gerstein</surname><given-names>GL</given-names></name> (<year>1999</year>) <article-title>Trial-to-trial variability and state-dependent modulation of auditory-evoked responses in cortex</article-title>. <source>J neurosci</source> <volume>19</volume>: <fpage>10451</fpage>–<lpage>60</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-LinkenkaerHansenKNikulin1">
        <label>22</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Linkenkaer-HansenK, Nikulin</surname><given-names>VV</given-names></name>, <name name-style="western"><surname>Palva</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Ilmoniemi</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Palva</surname><given-names>JM</given-names></name> (<year>2004</year>) <article-title>Prestimulus oscillations enhance psychophysical performance in humans</article-title>. <source>J neurosci</source> <volume>24</volume>: <fpage>10186</fpage>–<lpage>90</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Mathewson1">
        <label>23</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mathewson</surname><given-names>KE</given-names></name>, <name name-style="western"><surname>Gratton</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Fabiani</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Beck</surname><given-names>DM</given-names></name>, <name name-style="western"><surname>Ro</surname><given-names>T</given-names></name> (<year>2009</year>) <article-title>To See or Not to See : Prestimulus Phase Predicts Visual Awareness</article-title>. <source>Neuroscience</source> <volume>29</volume>: <fpage>2725</fpage>–<lpage>2732</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Amit2">
        <label>24</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name> (<year>1997</year>) <article-title>Model of global spontaneous activity and local structured activity during delay periods in the cerebral cortex</article-title>. <source>Cereb Cortex</source> <volume>7</volume>: <fpage>237</fpage>–<lpage>252</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Amit3">
        <label>25</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name> (<year>1997</year>) <article-title>Dynamics of a recurrent network of spiking neurons before and following learning</article-title>. <source>Network: Computation in Neural Systems</source> <volume>8</volume>: <fpage>373</fpage>–<lpage>404</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Brunel1">
        <label>26</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Hakim</surname><given-names>V</given-names></name> (<year>1999</year>) <article-title>Fast Global Oscillations in Networks of Integrate-and-Fire Neurons with Low Firing Rates</article-title>. <source>Neural Comput</source> <volume>11</volume>: <fpage>1621</fpage>–<lpage>1671</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Brunel2">
        <label>27</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name> (<year>2003</year>) <article-title>Dynamics and Plasticity of Stimulus-selective Persistent Activity in Cortical Network Models</article-title>. <source>Cereb Cortex</source> <volume>13</volume>: <fpage>1151</fpage>–<lpage>1161</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Petermann1">
        <label>28</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Petermann</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Thiagarajan</surname><given-names>TC</given-names></name>, <name name-style="western"><surname>Lebedev</surname><given-names>MA</given-names></name>, <name name-style="western"><surname>Nicolelis</surname><given-names>MAL</given-names></name>, <name name-style="western"><surname>Chialvo</surname><given-names>DR</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Spontaneous cortical activity in awake monkeys composed of neuronal avalanches</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>106</volume>: <fpage>15921</fpage>–<lpage>6</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Marre1">
        <label>29</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marre</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Yger</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Davison</surname><given-names>AP</given-names></name>, <name name-style="western"><surname>Fregnac</surname><given-names>Y</given-names></name> (<year>2009</year>) <article-title>Reliable Recall of Spontaneous Activity Patterns in Cortical Networks</article-title>. <source>J neurosci</source> <volume>29</volume>: <fpage>14596</fpage>–<lpage>14606</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Rajan1">
        <label>30</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rajan</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Abbott</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>2010</year>) <article-title>Stimulus-dependent suppression of chaos in recurrent neural networks</article-title>. <source>Phys Rev E</source> <volume>82</volume>: <fpage>1</fpage>–<lpage>5</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Lewis1">
        <label>31</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lewis</surname><given-names>CM</given-names></name>, <name name-style="western"><surname>Baldassarre</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Committeri</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Romani</surname><given-names>GL</given-names></name>, <name name-style="western"><surname>Corbetta</surname><given-names>M</given-names></name> (<year>2009</year>) <article-title>Learning sculpts the spontaneous activity of the resting human brain</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>106</volume>: <fpage>17558</fpage>–<lpage>63</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Berkes1">
        <label>32</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berkes</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Orbán</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Lengyel</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Fiser</surname><given-names>J</given-names></name> (<year>2011</year>) <article-title>Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment</article-title>. <source>Science (New York, NY)</source> <volume>331</volume>: <fpage>83</fpage>–<lpage>7</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Kurikawa1">
        <label>33</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kurikawa</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Kaneko</surname><given-names>K</given-names></name> (<year>2011</year>) <article-title>Learning Shapes Spontaneous Activity Itinerating over Memorized States</article-title>. <source>PLoS ONE</source> <volume>6</volume>: <fpage>e17432</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Kurikawa2">
        <label>34</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kurikawa</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Kaneko</surname><given-names>K</given-names></name> (<year>2012</year>) <article-title>Associative memory model with spontaneous neural activity</article-title>. <source>Europhys Lett</source> <volume>98</volume>: <fpage>48002</fpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Kenet1">
        <label>35</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kenet</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Bibitchkov</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Grinvald</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Arieli</surname><given-names>A</given-names></name> (<year>2003</year>) <article-title>Spontaneously emerging cortical representations of visual attributes</article-title>. <source>Nature</source> <volume>425</volume>: <fpage>954</fpage>–<lpage>956</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-MacLean1">
        <label>36</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>MacLean</surname><given-names>JN</given-names></name>, <name name-style="western"><surname>Watson</surname><given-names>BO</given-names></name>, <name name-style="western"><surname>Aaron</surname><given-names>GB</given-names></name>, <name name-style="western"><surname>Yuste</surname><given-names>R</given-names></name> (<year>2005</year>) <article-title>Internal Dynamics Determine the Cortical Response to Thalamic Stimulation</article-title>. <source>Neuron</source> <volume>48</volume>: <fpage>811</fpage>–<lpage>823</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Luczak1">
        <label>37</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luczak</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Bartho</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>KD</given-names></name> (<year>2009</year>) <article-title>Spontaneous Events Outline the Realm of Possible Sensory Responses in Neocortical Populations</article-title>. <source>Neuron</source> <volume>62</volume>: <fpage>413</fpage>–<lpage>425</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Nadal1">
        <label>38</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nadal</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Toulouse</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Changeux</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>Dehaene</surname><given-names>S</given-names></name> (<year>1986</year>) <article-title>Networks of Formal Neurons and Memory Palimpsests</article-title>. <source>Europhys Lett</source> <volume>1</volume>: <fpage>535</fpage>–<lpage>542</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Mzard1">
        <label>39</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mézard</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Nadal</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Toulouse</surname><given-names>G</given-names></name> (<year>1986</year>) <article-title>Solvable models of working memories</article-title>. <source>J Physique</source> <volume>47</volume>: <fpage>1457</fpage>–<lpage>1462</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Parisi1">
        <label>40</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Parisi</surname><given-names>G</given-names></name> (<year>1986</year>) <article-title>A memory which forgets</article-title>. <source>J Phys A: Math Gen</source> <volume>19</volume>: <fpage>L617</fpage>–<lpage>L620</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Mattis1">
        <label>41</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mattis</surname><given-names>D</given-names></name> (<year>1976</year>) <article-title>Solvable spin systems with random interactions</article-title>. <source>Phys Lett A</source> <volume>56</volume>: <fpage>421</fpage>–<lpage>422</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Fusi1">
        <label>42</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name> (<year>2002</year>) <article-title>Hebbian spike-driven synaptic plasticity for learning patterns of mean firing rates</article-title>. <source>Biol Cybern</source> <volume>87</volume>: <fpage>459</fpage>–<lpage>470</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Amit4">
        <label>43</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Mongillo</surname><given-names>G</given-names></name> (<year>2003</year>) <article-title>Spike-driven synaptic dynamics generating working memory states</article-title>. <source>Neural Comput</source> <volume>15</volume>: <fpage>565</fpage>–<lpage>596</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Luczak2">
        <label>44</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Luczak</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Bartho</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Marguet</surname><given-names>SL</given-names></name>, <name name-style="western"><surname>Buzsaki</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>KD</given-names></name> (<year>2007</year>) <article-title>Sequential structure of neocortical spontaneous activity in vivo</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>104</volume>: <fpage>347</fpage>–<lpage>352</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Tsodyks1">
        <label>45</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Kenet</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Grinvald</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Arieli</surname><given-names>A</given-names></name> (<year>1999</year>) <article-title>Linking Spontaneous Activity of Single Cortical Neurons and the Underlying Functional Architecture</article-title>. <source>Science</source> <volume>286</volume>: <fpage>1943</fpage>–<lpage>1946</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Sakata1">
        <label>46</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sakata</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>KD</given-names></name> (<year>2009</year>) <article-title>Laminar Structure of Spontaneous and Sensory-Evoked Population Activity in Auditory Cortex</article-title>. <source>Neuron</source> <volume>64</volume>: <fpage>404</fpage>–<lpage>418</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Tsuda1">
        <label>47</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsuda</surname><given-names>I</given-names></name> (<year>1992</year>) <article-title>Dynamic link of memoryChaotic memory map in nonequilibrium neural networks</article-title>. <source>Neural Netw</source> <volume>5</volume>: <fpage>313</fpage>–<lpage>326</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Kaneko1">
        <label>48</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kaneko</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Tsuda</surname><given-names>I</given-names></name> (<year>2003</year>) <article-title>Chaotic itinerancy</article-title>. <source>Chaos</source> <volume>13</volume>: <fpage>926</fpage>–<lpage>36</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Rabinovich1">
        <label>49</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rabinovich</surname><given-names>MI</given-names></name>, <name name-style="western"><surname>Huerta</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Varona</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Afraimovich</surname><given-names>VS</given-names></name> (<year>2008</year>) <article-title>Transient Cognitive Dynamics, Metastability, and Decision Making</article-title>. <source>PLoS Comput Biol</source> <volume>4</volume>: <fpage>e1000072</fpage>–<lpage>e1000072</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Jaeger1">
        <label>50</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jaeger</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Haas</surname><given-names>H</given-names></name> (<year>2004</year>) <article-title>Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication</article-title>. <source>Science</source> <volume>304</volume>: <fpage>78</fpage>–<lpage>80</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Ganguli1">
        <label>51</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ganguli</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Huh</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>2008</year>) <article-title>Memory traces in dynamical systems</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>105</volume>: <fpage>18970</fpage>–<lpage>5</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Falconbridge1">
        <label>52</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Falconbridge</surname><given-names>MS</given-names></name>, <name name-style="western"><surname>Stamps</surname><given-names>RL</given-names></name>, <name name-style="western"><surname>Badcock</surname><given-names>DR</given-names></name> (<year>2006</year>) <article-title>A simple Hebbian/anti-Hebbian network learns the sparse, independent components of natural images</article-title>. <source>Neural comput</source> <volume>18</volume>: <fpage>415</fpage>–<lpage>29</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Destexhe2">
        <label>53</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Destexhe</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Marder</surname><given-names>E</given-names></name> (<year>2004</year>) <article-title>Plasticity in single neuron and circuit computations</article-title>. <source>Nature</source> <volume>431</volume>: <fpage>789</fpage>–<lpage>795</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Kawato1">
        <label>54</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kawato</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Gomi</surname><given-names>H</given-names></name> (<year>1992</year>) <article-title>A computational model of four regions of the cerebellum based on feedback-error learning</article-title>. <source>Biol Cybern</source> <volume>68</volume>: <fpage>95</fpage>–<lpage>103</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-Wolpert1">
        <label>55</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wolpert</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Kawato</surname><given-names>M</given-names></name> (<year>1998</year>) <article-title>Multiple paired forward and inverse models for motor control</article-title>. <source>Neural Netw</source> <volume>11</volume>: <fpage>1317</fpage>–<lpage>1329</lpage>.</mixed-citation>
      </ref>
      <ref id="pcbi.1002943-vonBremen1">
        <label>56</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>von Bremen</surname><given-names>HF</given-names></name>, <name name-style="western"><surname>Udwadia</surname><given-names>FE</given-names></name>, <name name-style="western"><surname>Proskurowski</surname><given-names>W</given-names></name> (<year>1997</year>) <article-title>An efficient QR based method for the computation of Lyapunov exponents</article-title>. <source>Physica D</source> <volume>101</volume>: <fpage>1</fpage>–<lpage>16</lpage>.</mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>