<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN"><front><journal-meta><journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher><publisher-name>Public Library of Science</publisher-name><publisher-loc>San Francisco, USA</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">07-PONE-RA-01534R1</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0000943</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline"><subject>Neuroscience</subject><subject>Computational Biology/Computational Neuroscience</subject><subject>Neuroscience/Behavioral Neuroscience</subject><subject>Neuroscience/Cognitive Neuroscience</subject><subject>Neuroscience/Sensory Systems</subject><subject>Neuroscience/Theoretical Neuroscience</subject></subj-group></article-categories><title-group><article-title>Causal Inference in Multisensory Perception</article-title><alt-title alt-title-type="running-head">Causal Inference</alt-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Körding</surname><given-names>Konrad P.</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Beierholm</surname><given-names>Ulrik</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib><contrib contrib-type="author" equal-contrib="yes" xlink:type="simple"><name name-style="western"><surname>Ma</surname><given-names>Wei Ji</given-names></name><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Quartz</surname><given-names>Steven</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Tenenbaum</surname><given-names>Joshua B.</given-names></name><xref ref-type="aff" rid="aff5"><sup>5</sup></xref></contrib><contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Shams</surname><given-names>Ladan</given-names></name><xref ref-type="aff" rid="aff6"><sup>6</sup></xref></contrib></contrib-group><aff id="aff1"><label>1</label><addr-line>Rehabilitation Institute of Chicago, Northwestern University, Chicago, Illinois, United States of America</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Computation and Neural Systems, California Institute of Technology, Pasadena, California, United States of America</addr-line>       </aff><aff id="aff3"><label>3</label><addr-line>Department of Brain and Cognitive Sciences, University of Rochester, Rochester, New York, United States of America</addr-line>       </aff><aff id="aff4"><label>4</label><addr-line>Division of Humanities and Social Sciences, California Institute of Technology, Pasadena, California, United States of America</addr-line>       </aff><aff id="aff5"><label>5</label><addr-line>Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America</addr-line>       </aff><aff id="aff6"><label>6</label><addr-line>Department of Psychology, University of California at Los Angeles, Los Angeles, California, United States of America</addr-line>       </aff><contrib-group><contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Sporns</surname><given-names>Olaf</given-names></name><role>Academic Editor</role><xref ref-type="aff" rid="edit1"/></contrib></contrib-group><aff id="edit1">Indiana University, United States of America</aff><author-notes><corresp id="cor1">* To whom correspondence should be addressed. E-mail: <email xlink:type="simple">beierh@caltech.edu</email></corresp><fn fn-type="con"><p>Conceived and designed the experiments: LS SQ JT. Performed the experiments: UB. Analyzed the data: UB KK WM. Wrote the paper: UB KK LS WM.</p></fn><fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn></author-notes><pub-date pub-type="collection"><year>2007</year></pub-date><pub-date pub-type="epub"><day>26</day><month>9</month><year>2007</year></pub-date><volume>2</volume><issue>9</issue><elocation-id>e943</elocation-id><history><date date-type="received"><day>15</day><month>6</month><year>2007</year></date><date date-type="accepted"><day>3</day><month>9</month><year>2007</year></date></history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2007</copyright-year><copyright-holder>Körding et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract><p>Perceptual events derive their significance to an animal from their meaning about the world, that is from the information they carry about their causes. The brain should thus be able to efficiently infer the causes underlying our sensory events. Here we use multisensory cue combination to study causal inference in perception. We formulate an ideal-observer model that infers whether two sensory cues originate from the same location and that also estimates their location(s). This model accurately predicts the nonlinear integration of cues by human subjects in two auditory-visual localization tasks. The results show that indeed humans can efficiently infer the causal structure as well as the location of causes. By combining insights from the study of causal inference with the ideal-observer approach to sensory cue combination, we show that the capacity to infer causal structure is not limited to conscious, high-level cognition; it is also performed continually and effortlessly in perception.</p></abstract><funding-group><funding-statement>KPK was supported by a DFG Heisenberg Stipend. UB and SQ were supported by the David and Lucille Packard Foundation as well as by the Moore Foundation. JBT was supported by the P. E. Newton Career Development Chair. LS was supported by UCLA Academic senate and Career development grants.</funding-statement></funding-group><counts><page-count count="10"/></counts></article-meta></front><body><sec id="s1"><title>Introduction</title><p>Imagine you are walking in the forest and you see a sudden movement in the bushes. You may infer that this movement was caused by a hidden animal, but you may also consider a gust of wind as an alternative and possibly more probable cause. If you are a hungry predator–or a life-loving prey–this estimation may be critical to your survival. However, you may also hear an animal vocalization coming from a similar direction. Combining both pieces of sensory information, you will be better at judging if there is an animal in the bushes and if so, where exactly it is hiding. Importantly, the way how you will combine pieces of information must depend on the causal relationships you inferred. This example illustrates that perceptual cues are seldom ecologically relevant by themselves, but rather acquire their significance through their meaning about their causes. It also illustrates how cues from multiple sensory modalities can be used to infer underlying causes. The nervous system is constantly engaged in combining uncertain information from different sensory modalities into an integrated understanding of the causes of sensory stimulation.</p><p>The study of multisensory integration has a long and fruitful history in experimental psychology, neurophysiology, and psychophysics. Von Helmholtz, in the late 19<sup>th</sup> century started considering cue combination, formalizing perception as unconscious probabilistic inference of a best guess of the state of the world <xref ref-type="bibr" rid="pone.0000943-Hatfield1">[1]</xref>. Since then, numerous studies have analyzed the way people use and combine cues for perception <xref ref-type="bibr" rid="pone.0000943-Thurlow1">[e.g. 2]</xref>, <xref ref-type="bibr" rid="pone.0000943-Warren1">[3]</xref>, highlighting the rich set of effects that occur in multimodal perception.</p><p>Over the last decade, many scientists have gone back to a probabilistic interpretation of cue combination as had been proposed by von Helmholtz. These probabilistic models formalize the problem of cue combination in an elegant way. It is assumed that there is a single variable in the outside world (e.g., the position of an animal) that is causing the cues (auditory and visual information). Each of the cues is assumed to be a noisy observation of this underlying variable. Due to noise in sensation, there is some uncertainty about the information conveyed by each cue and Bayesian statistics is the systematic way of predicting how subjects could optimally infer the underlying variables from the cues. Several recent studies have demonstrated impressive fits to psychophysical data, starting from the assumption that human performance is close to the ideal defined by probabilistic models <xref ref-type="bibr" rid="pone.0000943-Jacobs1">[4]</xref>–<xref ref-type="bibr" rid="pone.0000943-Alais1">[7]</xref>. In these experiments, cues tend to be close to each other in time, space, and structure, providing strong evidence for there only being a single cause for both cues. In situations where there is only a single underlying cause, these models formalize the central idea of probabilistic inference of a hidden cause.</p><p>A range of experiments have shown effects that are hard to reconcile with the single-cause (i.e., forced-fusion) idea. Auditory-visual integration breaks down when the difference between the presentation of the visual and the auditory stimulus is large <xref ref-type="bibr" rid="pone.0000943-Slutsky1">[8]</xref>–<xref ref-type="bibr" rid="pone.0000943-Jack1">[10]</xref>. Such a distance or inconsistency is called disparity. Increasing disparity, for example by moving an auditory stimulus farther away from the position of a visual stimulus, reduces the influence each stimulus has on the perception of the other <xref ref-type="bibr" rid="pone.0000943-Warren2">[11]</xref>–<xref ref-type="bibr" rid="pone.0000943-Bresciani1">[14]</xref>. Throughout this paper we only consider spatial disparity along the azimuthal axis. When subjects are asked to report their percepts in both modalities <italic>on the same trial</italic>, one can measure the influence that the two senses have on each other <xref ref-type="bibr" rid="pone.0000943-Shams1">[13]</xref>. The data from such a dual-report paradigm show that, although at small disparities there is a tendency to integrate, greater disparity makes it more likely that a subject responds differently in both modalities. Moreover, when people are simply asked whether they perceive a single cue or several cues they give answers that intuitively make a lot of sense: if two events are close to each other in space, time, and structure, subjects tend to perceive a single underlying cause, while if they are far away from one another subjects tend to infer two independent causes <xref ref-type="bibr" rid="pone.0000943-Wallace1">[15]</xref>, <xref ref-type="bibr" rid="pone.0000943-Hairston1">[16]</xref>. If cues are close to one another, they interact and influence the perception of each other, whereas they are processed independently when the discrepancy is large.</p><p>New modeling efforts have made significant progress at formalizing the interactions between two cues. These models assume that there exist <italic>two</italic> relevant variables in the world, for example the position of a visual and the position of an auditory stimulus. The visual and auditory cues that reach the nervous system are noisy versions of the underlying visual and auditory variables. The models further assume an “interaction prior”, a joint prior distribution that defines how likely each combination of visual and auditory stimuli is in the absence of any evidence. This prior formalizes that the probability of both positions being the same (related to a common cause) is high in comparison to the positions being different from one another. This prior in effect determines the way in which two modalities influence each other. Very good fits to human performance have been shown for the combination of two cues <xref ref-type="bibr" rid="pone.0000943-Shams1">[13]</xref>, <xref ref-type="bibr" rid="pone.0000943-Bresciani1">[14]</xref>, <xref ref-type="bibr" rid="pone.0000943-Rowland1">[17]</xref>, <xref ref-type="bibr" rid="pone.0000943-Roach1">[18]</xref>. These studies assume an interaction between processing in each modality and derive predictions of human performance from this idea.</p><p>Von Helmholtz did not only stress the issue of probabilistic inference but also that multiple objects may be the causes of our sensations <xref ref-type="bibr" rid="pone.0000943-McDonald1">[19]</xref>, <xref ref-type="bibr" rid="pone.0000943-Helmholtz1">[20]</xref>. In other words, any two sensory signals may either have a common cause, in which case they should be integrated, or have different, independent causes, in which case they should be processed separately. Further evidence for this idea comes from a study that showed that by providing evidence that two signals are related it is possible to incite subjects to more strongly combine two cues <xref ref-type="bibr" rid="pone.0000943-Ernst2">[21]</xref>. The within-modality binding problem is another example where causal inference is necessary and the nervous system has to determine which set of stimuli correspond to the same object and should be bound together <xref ref-type="bibr" rid="pone.0000943-Treisman1">[22]</xref>–<xref ref-type="bibr" rid="pone.0000943-Knill1">[24]</xref>. We are usually surrounded by many sights, sounds, odors, and tactile stimuli, and our nervous system constantly needs to estimate which signals have a common cause. The nervous system frequently needs to solve problems where it needs to interpret sensory signals in terms of potential causes.</p><p>In this paper we formalize the problem of causal inference as well as integration versus segregation in multisensory perception as an optimal Bayesian observer that not only infers source location from two sensory signals (visual, s<italic><sub>V</sub></italic>, and auditory, s<italic><sub>A</sub></italic>) but also whether the signals have a common cause (<italic>C</italic>). This inference is complicated by the fact that the nervous system does not have access to the source locations of the signals but only to noisy measurements thereof (visual, <italic>x<sub>V</sub></italic>, and auditory, <italic>x<sub>A</sub></italic>). From these noisy observations it needs to infer the best estimates of the source locations (<italic>Ŝ<sub>V</sub></italic> and <italic>Ŝ<sub>A</sub></italic>). All this needs to happen in the presence of uncertainnty about the presence of a common cause (<italic>C</italic>). To take into account multiple possible causal structures, we need a so-called mixture model <xref ref-type="bibr" rid="pone.0000943-Knill1">[e.g. 24]</xref>, but one of a very specific form.</p><p>The model assumes that the underlying variables (azimuthal stimulus positions) cause the sensory inputs. The model considers two hypotheses, either that there is a common cause or that there are independent causes. The optimal observer model defines how the cues might actually be combined (i.e., in a statistically optimal manner). In the model, cues are fused if the cues have one common cause and segregated if they have independent causes. The model typically has uncertainty about the causal interpretation, in which case it will adjust its cue combination continuously depending on the degree of belief about the causal structure.</p><p>This model makes three important predictions: (1) It predicts the circumstances under which subjects should perceive a common cause or independent causes. (2) It predicts if the individual cues should be fused or if they should be processed separately. (3) It predicts how the cues are combined if they are combined. Here we test the predictions of the model and analyze how well it predicts human behavior.</p></sec><sec id="s2"><title>Results</title><sec id="s2a"><title>Causal Bayesian inference</title><p>We model situations in which observers are presented with simultaneous auditory and visual stimuli, and are asked to report their location(s). If the visual and the auditory stimuli have a common cause (<xref ref-type="fig" rid="pone-0000943-g001">Fig. 1</xref>, left), subjects could use the visual cue to improve the auditory estimate, and vice versa. However, in the real world we are usually surrounded by multiple sources of sensory stimulation and hence multiple sights and sounds. Therefore the nervous system cannot simply combine all signals into a joint estimate; it must infer which signals have a common cause and only integrate those. Specifically, for any pair of visual and auditory stimuli, it should also consider the alternative possibility that they are unrelated and co-occurred randomly (<xref ref-type="fig" rid="pone-0000943-g001">Fig. 1</xref>, right).</p><fig id="pone-0000943-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0000943.g001</object-id><label>Figure 1</label><caption><title>The causal inference model.</title><p>Left: One cause can be responsible for both cues. In this case the visually perceived position <italic>x<sub>V</sub></italic> will be the common position <italic>s</italic> perturbed by visual noise with width <italic>σ<sub>V</sub></italic> and the auditory perceived position will be the common position perturbed by auditory noise with width <italic>σ<sub>A</sub></italic>. Right: Alternatively, two distinct causes may be relevant, decoupling the problem into two independent estimation problems. The causal inference model infers the probability of a causal structure with a common cause (left, <italic>C</italic> = 1) versus the causal structure with two independent causes (right, <italic>C</italic> = 2) and then derives optimal predictions from this. We introduce a single variable <italic>C</italic> which determines which sub-model generates the data.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.g001" xlink:type="simple"/></fig><p>Here we developed an ideal observer that estimates the positions of cues and also whether they have a common cause. This <italic>causal inference model</italic> uses two pieces of information. One piece is the likelihood: the sensed visual and auditory positions, which are corrupted by noise. Because perception is corrupted by noise, any sensory stimulus does not reveal the true visual position, but rather induces a distribution of where the stimulus could be, given the stimulus. The other piece of information is the prior: from experience we may know how likely two co-occurring signals are to have a common cause versus two independent causes. The causal inference model combines those pieces of information to estimate if there is a common cause and to estimate the positions of cues (see the <xref ref-type="sec" rid="s4">Methods</xref> section and Supporting Information for details, <xref ref-type="supplementary-material" rid="pone.0000943.s001">Text S1</xref>).</p><p>The causal inference model depends on four parameters characterizing the knowledge about the environment and the observer's sensory systems: the uncertainty of vision (<italic>σ<sub>V</sub></italic>) and audition (<italic>σ<sub>A</sub></italic>); knowledge the observer has about the spatial layout of objects, in particular how much the observer expects that objects are more likely to be located centrally (<italic>σ<sub>P</sub></italic>, introduced to formalize that subjects have a bias to perceive stimuli straight ahead); and the prior probability that there is a single cause versus two causes (<italic>p</italic><sub>common</sub>). These four parameters are fit to human behavior in psychophysical experiments (see <xref ref-type="sec" rid="s4">Methods</xref> for details).</p></sec><sec id="s2b"><title>Experiment 1: Auditory-visual spatial localization</title><p>Experienced ventriloquists move a puppet's mouth in synchrony with their speech patterns, creating a powerful illusion of a talking puppet. This effect is a classical demonstration of auditory-visual integration, where subjects infer that there is only a single cause (the puppet's talking) for both visual (puppet's facial movements) and auditory (speech) stimuli. Numerous experimental studies have analyzed this kind of auditory-visual cue integration and found situations in which the cues are combined and situations in which they are processed separately <xref ref-type="bibr" rid="pone.0000943-Thurlow1">[2]</xref>, <xref ref-type="bibr" rid="pone.0000943-Warren1">[3]</xref>, <xref ref-type="bibr" rid="pone.0000943-Alais1">[7]</xref>, <xref ref-type="bibr" rid="pone.0000943-Slutsky1">[8]</xref>, <xref ref-type="bibr" rid="pone.0000943-Jack1">[10]</xref>, <xref ref-type="bibr" rid="pone.0000943-Wallace1">[15]</xref>, <xref ref-type="bibr" rid="pone.0000943-Recanzone1">[25]</xref>–<xref ref-type="bibr" rid="pone.0000943-Bertelson1">[27]</xref>. To test the causal inference model, we use a laboratory version of the ventriloquist illusion, in which brief auditory and visual stimuli are presented simultaneously with varying amounts of spatial disparity. We use the dual-report paradigm which was introduced recently to study auditory-visual numerosity judgment <xref ref-type="bibr" rid="pone.0000943-Shams1">[13]</xref>, because this provides information about the joint auditory-visual percepts of subjects.</p><p>Nineteen subjects participated in the experiment. On each trial, subjects were presented with either a brief visual stimulus in one of five locations along azimuth, or a brief tone at one of the same five locations, or both simultaneously (see <xref ref-type="sec" rid="s4">Methods</xref> for details). The task was to report the location of the visual stimulus as well as the location of the sound in each trial using two button presses (<xref ref-type="fig" rid="pone-0000943-g002">Fig. 2a</xref>).</p><fig id="pone-0000943-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0000943.g002</object-id><label>Figure 2</label><caption><title>Combination of visual and auditory cues.</title><p>a) The experimental paradigm is shown. In each trial a visual and an auditory stimulus is presented simultaneously and subjects report both the position of the perceived visual and the position of the perceived auditory stimuli by button presses. b) The influence of vision on the perceived position of an auditory stimulus in the center is shown. Different colors correspond to the visual stimulus at different locations (sketched in warm to cold colors from the left to the right). The unimodal auditory case is shown in gray. c) The averaged responses of the subjects (solid lines) are shown along with the predictions of the ideal observer (broken lines) for each of the 35 stimulus conditions. These plot show how often on average which button was pressed in each of the conditions. d) The model responses from c) are plotted with the human responses from c). e) The average auditory bias <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0000943.e001" xlink:type="simple"/></inline-formula>, i.e. the influence of deviations of the visual position on the perceived auditory position is shown as a function of the spatial disparity (solid line) along with the model prediction (dashed line).</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.g002" xlink:type="simple"/></fig><p>We found that subjects show large variability across trials, shown in <xref ref-type="fig" rid="pone-0000943-g002">Fig 2b</xref> for an auditory stimulus at the central location. If subjects would not be affected by noise then we would expect a plot that has 100% of the trials as a press of the button corresponding to the central location. Instead we see a wide distribution, highlighting the presence of uncertainty in auditory perception. All modern theories of cue combination predict that two cues, presented simultaneously, will influence one another and lead to a bimodal precision that is better than the unimodal precision, as the other cue can be used to reduce uncertainty. Indeed, we found that the visual stimulus influences the estimate of the auditory stimulus when the auditory stimulus is held at a fixed location (<xref ref-type="fig" rid="pone-0000943-g002">Fig. 2b</xref>, yellow versus gray). Moreover, we find that vision does have an influence on the perception of the auditory stimulus and a visual stimulus to the left biases perception to the left (<xref ref-type="fig" rid="pone-0000943-g002">Fig 2b</xref>, red versus yellow). Subjects thus base their estimate of the auditory position on both visual and auditory cues. Moreover, subjects' estimates of the auditory position often differ from their estimates of the visual position.</p><p>To examine whether the causal inference model can account for the full range of cue combination observed in human multisensory perception, we make use of both the auditory and the visual response frequencies we measured in our experiment (<xref ref-type="fig" rid="pone-0000943-g002">Fig. 2c</xref>). Four parameters were used to fit 250 data points (25 bisensory conditions, 2 modalities, 5 buttons per modality). The causal inference model accounts for the data very well (<italic>R</italic><sup>2</sup> = 0.97; <italic>R</italic><sup>2 </sup>is calculated as the explained variance divided by the total variance) (<xref ref-type="fig" rid="pone-0000943-g002">Fig. 2c,d</xref>). One interesting finding is that the response distribution generally only has one peak (in <xref ref-type="fig" rid="pone-0000943-g002">Fig 2c</xref>), but its position and skewness is affected by the position of the other stimulus. The model shows this effect because it does not simply decide if there is a common cause or individual causes but considers both possibilities on each trial.</p><p>To facilitate quantitative comparison of the causal inference model with other models, we fitted the parameters individually to each subject's data using a maximum-likelihood procedure: we maximized the probability of the data under the model. For each subject, the best fit from 6 different sets of initial parameter values was used, to reduce the effect of these initial values. We did this for several different models that use previously proposed interaction priors as well as the prior derived from causal inference. We first considered two special cases of the causal inference model: pure integration (causal inference with <italic>p</italic><sub>common</sub> = 1) and pure segregation (causal inference with <italic>p</italic><sub>common</sub> = 0). We then considered two two-dimensional ad hoc priors that have been proposed in other papers. Roach et al. <xref ref-type="bibr" rid="pone.0000943-Roach1">[18]</xref> proposed a two-dimensional (auditory-visual) prior that is defined as the sum of a Gaussian ridge along the diagonal, and a constant. This prior is somewhat similar to the causal inference prior as the constant relates to events that are independent and the Gaussian relates to sensory events that are related and thus have a common cause. Bresciani et al <xref ref-type="bibr" rid="pone.0000943-Bresciani1">[14]</xref> used a special case of the Roach et al. prior ( The Shams et al. model (Shams et al, 2005) was not considered as it involves a prior specified by a large number of parameters (25)). where no constant is added to the Gaussian. According to the Bresciani prior, visual and auditory positions that are very far away from each other are extremely unlikely. According to the Roach prior, such two positions have a fixed, non-vanishing probability.</p><p>In the comparison, we obtain the predicted response distribution by integrating out the internal variables instead of equating it to the posterior distribution. This is the correct way of solving this Bayesian problem and differs from the approach taken in previous papers <xref ref-type="bibr" rid="pone.0000943-Shams1">[13]</xref>, <xref ref-type="bibr" rid="pone.0000943-Bresciani1">[14]</xref>, <xref ref-type="bibr" rid="pone.0000943-Roach1">[18]</xref> (although it only affects predictions in the Roach et al. model). We measure the goodness of fit obtained from these priors relative to that obtained from the causal inference prior, using the log likelihood over the entire data set. The resulting log likelihood ratios are shown in <xref ref-type="table" rid="pone-0000943-t001">Table 1</xref>. The causal inference model fits the data better than the other models. We also compare with an alternative model that instead of minimizing the mean squared error maximizes the probability of being correct and can exclude this model based on the presented evidence.</p><table-wrap id="pone-0000943-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0000943.t001</object-id><label>Table 1</label><caption><title>Maximal log likelihood ratios (base <italic>e</italic>) across subjects of models relative to causal inference model (mean±s.e.m., see <xref ref-type="sec" rid="s4">methods</xref> for details).</title></caption><!--===== Grouping alternate versions of objects =====--><alternatives><graphic id="pone-0000943-t001-1" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.t001" xlink:type="simple"/><table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/></colgroup><thead><tr><td align="left" colspan="1" rowspan="1">Model</td><td align="left" colspan="1" rowspan="1">Relative log likelihood</td></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Causal inference</td><td align="left" colspan="1" rowspan="1">0</td></tr><tr><td align="left" colspan="1" rowspan="1">Causal inference with maximization</td><td align="left" colspan="1" rowspan="1">−11±3</td></tr><tr><td align="left" colspan="1" rowspan="1">Full integration</td><td align="left" colspan="1" rowspan="1">−311±28</td></tr><tr><td align="left" colspan="1" rowspan="1">Full segregation</td><td align="left" colspan="1" rowspan="1">−25±7</td></tr><tr><td align="left" colspan="1" rowspan="1">Roach et al.</td><td align="left" colspan="1" rowspan="1">−18±6</td></tr><tr><td align="left" colspan="1" rowspan="1">Bresciani et al.</td><td align="left" colspan="1" rowspan="1">−22±6</td></tr></tbody></table></alternatives><table-wrap-foot><fn id="nt101"><p>For the last two entries, we used the prior proposed by Roach et al. and Bresciani et al. together with correct inference (see text for more detail). All of the maximal likelihood ratios in the table are considered decisive evidence in favor of the causal inference prior, even when correcting for the number of parameters using the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) <xref ref-type="bibr" rid="pone.0000943-Burnham1">[28]</xref>. These criteria are methods for enabling fair comparison between models. Models with more parameters always fit data better than models with fewer parameters. AIC and BIC are ways of correcting for this bias.</p></fn></table-wrap-foot></table-wrap><p>The parameters found in the likelihood optimization of the causal inference model are as follows. We found the visual system to be relatively precise (<italic>σ<sub>V</sub></italic> = 2.14±0.22°) and the auditory system to be much less precise (<italic>σ<sub>A</sub></italic> = 9.2±1.1°). We found that people have a modest prior estimating stimuli to be more likely to be central (<italic>σ<sub>P</sub></italic> = 12.3±1.1°). Subjects have the tendency of indicating a direction that is straight ahead and the prior allows the model to show such behavior as well. The average probability of perceiving a common cause for visual and auditory stimuli is relatively low (<italic>p</italic><sub>common</sub> = 0.28±0.05°). This explained that the observed biases are small in comparison to the values predicted if subjects were certain that there is a common cause (<xref ref-type="fig" rid="pone-0000943-g002">Fig. 2e</xref>). In summary, the causal inference model provides precise predictions of the way people combine cues in an auditory-visual spatial localization task, and it does so better than earlier models.</p><p>In the cue combination literature, bias is commonly used as an index of crossmodal interactions. In our experiment, auditory localization bias is a measure of the influence of vision on audition and can be plotted as a function of the spatial disparity <xref ref-type="bibr" rid="pone.0000943-Wallace1">[15]</xref>, <xref ref-type="bibr" rid="pone.0000943-Hairston1">[16]</xref>. Like other authors, we find that the bias decreases with increasing spatial disparity (<xref ref-type="fig" rid="pone-0000943-g002">Fig. 2e</xref>). Thus, the larger the distance between visual and auditory stimuli, the smaller is the influence of vision on audition. This result is naturally predicted by the causal inference model: larger discrepancies make the single cause model less likely as it would need to assume large noise values, which are unlikely. A model in which no combination happens at all (<italic>p</italic><sub>common</sub> = 0) cannot explain the observed biases (<xref ref-type="fig" rid="pone-0000943-g002">Fig. 2e</xref>) as it predicts a very small bias (<italic>p</italic>&lt;0.0001 t-test). The traditional forced-fusion model <xref ref-type="bibr" rid="pone.0000943-Jacobs1">[4]</xref>–<xref ref-type="bibr" rid="pone.0000943-Alais1">[7]</xref>, <xref ref-type="bibr" rid="pone.0000943-Adams1">[29]</xref> fails to explain much of the variance in <xref ref-type="fig" rid="pone-0000943-g002">Fig. 2c</xref> (<italic>R</italic><sup>2</sup> = 0.56). Moreover, this model would predict a very high bias—as vision is much more precise than audition in our experiment—and is ruled out by the bias data (<xref ref-type="fig" rid="pone-0000943-g002">Fig. 2e</xref>) (<italic>p</italic>&lt;0.0001 t-test). Neither the traditional nor the no-interaction model can explain the data, whereas the causal inference model can explain the observed patterns of partial combination well (see Supporting Information, <xref ref-type="supplementary-material" rid="pone.0000943.s001">Text S1</xref> and <xref ref-type="supplementary-material" rid="pone.0000943.s003">Fig. S2</xref> for a comparison with some other recent models of cue combination).</p></sec><sec id="s2c"><title>Experiment 2: Auditory-visual spatial localization with measured perception of causality</title><p>While the causal inference model accounts for the cue combination data described above, it makes a prediction that goes beyond the estimates of positions. If people infer the probability of common cause then it should be possible to ask them if they perceive a common cause versus two causes. A recent experiment asked this question <xref ref-type="bibr" rid="pone.0000943-Wallace1">[15]</xref>. We compare the predictions of the causal inference model with the reported data from this experiment. These experiments differed in a number of important respects from our experiment. Subjects were asked to report their perception of unity (i.e., whether the two stimuli have a common cause or two independent causes) on each trial. Only the location of the auditory stimulus was probed. Subjects pointed towards the location of the auditory stimulus instead of choosing a button to indicate the position (see <xref ref-type="sec" rid="s4">Methods</xref>, data analysis for <xref ref-type="fig" rid="pone-0000943-g003">figure 3</xref>, for details).</p><fig id="pone-0000943-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0000943.g003</object-id><label>Figure 3</label><caption><title>Reports of causal inference.</title><p>a) The relative frequency of subjects reporting one cause (black) is shown (reprinted with permission from <xref ref-type="bibr" rid="pone.0000943-Wallace1">[15]</xref>) with the prediction of the causal inference model (red). b) The bias, i.e. the influence of vision on the perceived auditory position is shown (gray and black). The predictions of the model are shown in red. c) A schematic illustration explaining the finding of negative biases. Blue and black dots represent the perceived visual and auditory stimuli, respectively. In the pink area people perceive a common cause.</p></caption><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.g003" xlink:type="simple"/></fig><p>The results of these experiments <xref ref-type="bibr" rid="pone.0000943-Wallace1">[15]</xref> indicate that the closer the visual stimulus is to the auditory stimulus, the more often do people perceive them as having a common cause (<xref ref-type="fig" rid="pone-0000943-g003">Fig. 3a</xref>). However, even if the two stimuli are close to one another, on some trials the noise in the perception of the auditory stimulus will sometimes lead to the perception of distinct causes. For example, a subject may hear the sound at 10° even when both stimuli really are at 0°; on such a trial, the large perceived disparity may lead the subject to report distinct causes. The model also shows the trend that with increasing disparity the probability of the perception of a common cause decreases. It explains 72% of the variance in human performance (<xref ref-type="fig" rid="pone-0000943-g003">Fig. 3a</xref>) and thus well models the human perception of causality.</p><p>We next examined how the perception of a common versus distinct causes affects the estimation of the position of auditory stimuli. The results indicate that when people perceive a common cause they point to a position that is on average very close to the position of the visual stimulus, and therefore the bias is high (<xref ref-type="fig" rid="pone-0000943-g003">Fig. 3b</xref>). If, on the other hand, subjects perceive distinct causes, they seem to not only rely on the auditory stimulus but seem to be pushed away from the visual stimulus and exhibit <italic>negative</italic> bias. This is a counterintuitive finding, as previous models <xref ref-type="bibr" rid="pone.0000943-Jacobs1">[4]</xref>–<xref ref-type="bibr" rid="pone.0000943-Alais1">[7]</xref>, <xref ref-type="bibr" rid="pone.0000943-Adams1">[29]</xref> predict only positive bias. Causal inference shows very similar behavior as it also exhibits negative biases, and explains 87% of the variance of the bias. The causal inference model thus accounts for the counterintuitive negative biases.</p><p>How can an optimal system exhibit negative bias? We argue that this is a selection bias stemming from restricting ourselves to trials in which causes were perceived as distinct. To clarify this, we consider, as an example, the case where the visual stimulus is 5° to the right of the center and the auditory stimulus is in the center. On some trials, the lack of precision of the auditory system will lead to the internal representation of the auditory signal being close to the visual signal and then the system infers a common cause. On those trials, the auditory stimulus will be inferred to be very close to the visual one, and the bias will be high (<xref ref-type="fig" rid="pone-0000943-g003">Fig. 3c</xref> red). On other trials, the internal representation of the auditory signal will, by chance, be far away from the visual signal, resulting in the model inferring distinct causes. The distribution of perceived auditory positions on these trials would be a truncated Gaussian distribution, with the right part of the distribution (corresponding to the perception of a common cause) cut away. This truncated distribution has a negative mean and thus leads to a negative bias. Due to this selection process, when a common cause is inferred, the perceived auditory location must be close to the visual stimulus, resulting in high bias. In contrast, when distinct causes are inferred, the perceived auditory location must be far away from the visual stimulus, and the bias thus becomes negative.</p></sec></sec><sec id="s3"><title>Discussion</title><p>The causal inference model formalizes and addresses the problem that had been phrased by von Helmholtz as “probabilistically inferring the causes of cues”. This refers to the general problem that subjects are faced with: deciding which cues correspond to the same source and which are unrelated to one another. The causal inference model can predict both subjects' unity judgments and their stimulus estimates. Moreover, its interaction prior is similar to the ones that have been proposed in earlier models that did not model causal inference but only the interaction between cues <xref ref-type="bibr" rid="pone.0000943-Shams1">[13]</xref>, <xref ref-type="bibr" rid="pone.0000943-Bresciani1">[14]</xref>, <xref ref-type="bibr" rid="pone.0000943-Rowland1">[17]</xref>, <xref ref-type="bibr" rid="pone.0000943-Roach1">[18]</xref>. It thus provides an explanation for why models utilizing an interaction prior have been successful at modeling human performance.</p><p>In addition to providing a formalism that derives from a strong normative idea, the causal inference model leads to better fits of human performance in the auditory-visual localization task presented here. Moreover, the causal inference model can make direct predictions about the causal structure of sensory input that would have been impossible with the previous models. Inference about causal structure is an important element of the perceptual binding of multisensory stimuli: when and how do sights and sounds get paired into a unified conscious percept <xref ref-type="bibr" rid="pone.0000943-Treisman1">[22]</xref>, <xref ref-type="bibr" rid="pone.0000943-Roskies1">[30]</xref>? The causal inference model presents a partial answer to this question.</p><p>As the causal inference model uses a single inference rule to account for the entire spectrum of sensory cue combination, many previous models are special cases of the model presented here, including those showing statistical optimality of complete integration (<italic>p</italic><sub>common</sub> = 1) when the discrepancy between the signals is small. In that case, the probability of a common cause given the sensory cues will be close to 1.</p><p>It is necessary to discuss in which way we may expect subjects to behave in an optimal way. We have shown that the assumption that subjects optimally solve causal inference problems can well predict many facets of their decision process. However, as has often been argued <xref ref-type="bibr" rid="pone.0000943-Gould1">[31]</xref>, <xref ref-type="bibr" rid="pone.0000943-Gigerenzer1">[32]</xref>, there is no reason why the nervous system should be optimal under all circumstances. For problems of high importance to everyday life, however, we should expect evolution to have found a very good solution. For this range of problems we should expect ideal observer models to make good predictions of human behavior.</p><p>This leaves the question of what neural processes underlie the causal inference computations that emerge at the behavioral level as close to Bayes-optimal. Recent work has shed some light on this issue for the case of complete integration (<italic>p</italic><sub>common</sub> = 1). It is well-known that neural populations naturally encode probability distributions over stimuli through Bayes' rule, a type of coding known as probabilistic population coding <xref ref-type="bibr" rid="pone.0000943-Zemel1">[33]</xref>, <xref ref-type="bibr" rid="pone.0000943-Pouget1">[34]</xref>. Under the assumption of a common cause, optimal cue combination can be implemented in a biologically realistic network using this type of coding. Unexpectedly, this only requires simple linear operations on neural activity <xref ref-type="bibr" rid="pone.0000943-Ma1">[35]</xref>. This implementation makes essential use of the structure of neural variability and leads to physiological predictions for activity in areas that combine multisensory input, such as the superior colliculus. Since complete integration is a special case of causal inference, computational mechanisms for the latter are expected to have a neural substrate that generalizes these linear operations on population activities. A neural implementation of optimal causal inference will be an important step towards a complete neural theory of multisensory perception.</p><p>In the study of higher-level cognition, many experiments have shown that people, starting from infancy, interpret events in terms of the actions of hidden causes <xref ref-type="bibr" rid="pone.0000943-Buehner1">[36]</xref>–<xref ref-type="bibr" rid="pone.0000943-Waldmann1">[40]</xref>. If we see a window shatter, something or someone must have broken it; if a ball flies up into the air, something launched it. It is particularly hard to resist positing invisible common causes to explain surprising conjunctions of events, such as the sudden occurrence of several cases of the same rare cancer in a small town. These causal inferences in higher-level cognition may seem quite different than the causal inferences in sensory integration we have studied here: more deliberate, consciously accessible, and knowledge-dependent, rather than automatic, instantaneous, and universal. Yet an intriguing link is suggested by our finding. The optimal statistical principles that can explain causal inference in sensory integration are very similar to those that have recently been shown to explain more conventional hidden-cause inferences in higher-level cognition <xref ref-type="bibr" rid="pone.0000943-Griffiths1">[39]</xref>, <xref ref-type="bibr" rid="pone.0000943-Tenenbaum1">[41]</xref>, <xref ref-type="bibr" rid="pone.0000943-Sobel1">[42]</xref>. Problems of inferring common causes from observed conjunctions arise everywhere across perception and cognition, and the brain may have evolved similar or even common mechanisms for performing these inferences optimally, in order to build veridical models of the environment's structure.</p></sec><sec id="s4"><title>Materials and Methods</title><sec id="s4a"><title>Paradigm for Experiment 1</title><p>Twenty naive subjects (undergraduate students at the California Institute of Technology, ten male) participated in the experiment. All subjects were informed of the purposes of the study and gave written informed consent as approved by the local ethics committee (Caltech Committee for the Protection of Human Subjects). Subjects were seated at a viewing distance of 54 cm from a 21-inch monitor. In each trial, subjects were asked to report both the perceived visual and auditory positions using keyboard keys 1 through 5, with 1 being the leftmost location and 5 the rightmost. No feedback about the correctness of the response was given. Visual and auditory stimuli were presented independently at one of five positions. The five locations extended from 10° to the left of the fixation point to 10° to the right of the fixation point at 5° intervals, along a horizontal line 5° below the fixation point, Visual stimuli were 35 ms presentations of Gabor wavelets of high contrast extending 2° on a background of visual noise. Auditory stimuli, synchronized with the visual stimuli in the auditory-visual conditions, were presented through a pair of headphones and consisted of 35 ms white noise. Unisensory stimuli were also presented, for which there was no presentation of stimulus for the other modality. However, for the purposes of the modeling in <xref ref-type="fig" rid="pone-0000943-g002">Fig. 2c</xref> we disregarded the unimodal data because of potential attentional differences between bimodal and unimodal data. The sound stimuli were filtered through a Head-Related Transfer Function (HRTF), measured individually for each subject, using methods similar to those described by <ext-link ext-link-type="uri" xlink:href="http://sound.media.mit.edu/KEMAR.html" xlink:type="simple">http://sound.media.mit.edu/KEMAR.html</ext-link>. The HRTFs were created to simulate sounds originating from the five spatial locations in the frontoparallel plane where the visual stimuli were presented. The data from one subject had to be discarded, as the fitted auditory variance of that subject was 98° and the subject was therefore effectively deaf with respect to the objective of our study. Apart from this subject, the HRTF function yielded to good auditory precision in the range of 10°.</p></sec><sec id="s4b"><title>Generative model</title><p>A Bayesian approach based on a generative model requires one to fully specify how the variables of interest are interrelated statistically. This is done as follows. Determine if there is one cause (<italic>C</italic> = 1) versus two causes (<italic>C</italic> = 2) by drawing from a binomial distribution with <italic>p</italic>(<italic>C</italic> = 1) = <italic>p</italic><sub>common</sub>. Outside of the experiment this will not be constant but depend on temporal delays, visual experience, context, and many other factors. In the experiments we consider all these factors are held constant so we can use a fixed <italic>p</italic><sub>common</sub>. If there is one cause (<italic>C</italic> = 1), draw a position <italic>s</italic> from a normal prior distribution <italic>N</italic>(0,<italic>σ<sub>P</sub></italic>), where <italic>N</italic>(<italic>μ</italic>,<italic>σ<sub>P</sub></italic>) stands for a normal distribution with mean <italic>μ</italic> and standard deviation <italic>σ</italic>. It is thus more likely that a stimulus is centrally located than far to the side. We then set <italic>s<sub>V</sub> = s</italic> and <italic>s<sub>A</sub></italic> = <italic>s</italic>. If there are two causes (<italic>C</italic> = 2), draw positions <italic>s<sub>V</sub></italic> and <italic>s<sub>A</sub></italic> each independently from <italic>N</italic>(0,<italic>σ<sub>P</sub></italic>). We assume that the visual and the auditory signal are corrupted by unbiased Gaussian noise of standard deviations <italic>σ<sub>V</sub></italic> and <italic>σ<sub>A,</sub></italic> respectively and draw <italic>x<sub>V</sub></italic> from <italic>N</italic>(<italic>s<sub>V</sub></italic>,<italic>σ<sub>V</sub></italic>) and <italic>x<sub>A</sub></italic> from <italic>N</italic>(<italic>s<sub>A</sub></italic>,<italic>σ<sub>A</sub></italic>). The noise is thus assumed to be independent across modalities.</p></sec><sec id="s4c"><title>Estimating the probability of a common cause</title><p>An ideal observer is faced with the problem of inferring the causal structure, i.e., whether there is one cause or there are two causes. This inference is performed optimally using Bayes' rule: <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.e002" xlink:type="simple"/><label>(1)</label></disp-formula>Here <italic>p</italic>(<italic>x<sub>A</sub></italic>, <italic>x<sub>V</sub></italic>) must be chosen such that <italic>p</italic>(<italic>C</italic> = 1|<italic>x<sub>V</sub></italic>, <italic>x<sub>A</sub></italic>) and <italic>p</italic>(<italic>C</italic> = 2|<italic>x<sub>V</sub></italic>, <italic>x<sub>A</sub></italic>) add to 1, as we are dealing with probabilities. We thus obtain: <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.e003" xlink:type="simple"/><label>(2)</label></disp-formula>For <italic>p</italic>(<italic>x<sub>V</sub></italic>, <italic>x<sub>A</sub></italic>|<italic>C</italic> = 1) we obtain<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.e004" xlink:type="simple"/><label>(3)</label></disp-formula>All three factors in this integral are Gaussians, allowing for an analytic solution: <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.e005" xlink:type="simple"/><label>(4)</label></disp-formula>where <italic>μ<sub>P</sub></italic> = 0is the mean of the prior.</p><p>For <italic>p</italic>(<italic>x<sub>V</sub></italic>, <italic>x<sub>A</sub></italic>|<italic>C</italic> = 2) we note that <italic>x</italic><sub>V</sub> and <italic>x<sub>A</sub></italic> are independent of each other and we thus obtain a product of two factors: <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.e006" xlink:type="simple"/><label>(5)</label></disp-formula>Again, as all these distributions are assumed to be Gaussian, we can write down an analytic solution, <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.e007" xlink:type="simple"/><label>(6)</label></disp-formula>For comparison with experimental data <xref ref-type="bibr" rid="pone.0000943-Wallace1">[15]</xref>, we assume that the model reports a common cause when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0000943.e008" xlink:type="simple"/></inline-formula>. The estimation of a common cause amounts to a Bayesian model selection problem. It is mathematically similar to a mixture model for depth <xref ref-type="bibr" rid="pone.0000943-Knill1">[24]</xref>.</p></sec><sec id="s4d"><title>Optimally estimating the position</title><p>When estimating the position of the visual target, we assume that making an error about the position of <italic>s</italic> in the case of a common cause is just as bad as making an error in the estimate of <italic>s<sub>V</sub></italic>, and likewise for the position of the auditory target. We assume that the cost in this estimation process is the mean squared error <xref ref-type="bibr" rid="pone.0000943-Kording1">[43]</xref>: <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.e009" xlink:type="simple"/><label>(7)</label></disp-formula>An optimal estimate for the subject is the estimate that leads to the lowest expected cost under the subject's posterior belief: <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.e010" xlink:type="simple"/><label>(8)</label></disp-formula>where, <italic>Ŝ<sub>V</sub></italic> is the possible estimate. In general, when the cost function is quadratic, the optimal estimation problem reduces to the problem of finding the mean of the posterior distribution. The estimate that minimizes the mean expected squared error in our case is therefore: <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.e011" xlink:type="simple"/><label>(9)</label></disp-formula>and<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.e012" xlink:type="simple"/><label>(10)</label></disp-formula>where <italic>Ŝ<sub>C</sub></italic><sub> = 1</sub> and <italic>Ŝ<sub>C</sub></italic><sub> = 2</sub> are the best estimates we would obtain if we were certain about there being one or two causes, respectively. These <italic>conditionalized</italic> solutions are obtained by linearly weighing the different cues proportional to their inverse variances <xref ref-type="bibr" rid="pone.0000943-Ghahramani1">[44]</xref>, as follows from the fact that the posterior is a product of Gaussians and thus a Gaussian itself. Therefore, if we know if there is a common or distinct causes we are able to analytically solve for the solution minimizing the mean squared error, which here coincides with the maximum-a-posteriori solution. The optimal visual estimate when visual and auditory sources are different is the same as when there would be only a visual signal, and likewise for the auditory estimate: <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.e013" xlink:type="simple"/><label>(11)</label></disp-formula>When the sensory signals have a common cause, the optimal solution is: <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.e014" xlink:type="simple"/><label>(12)</label></disp-formula>While these are both linear combinations of <italic>x<sub>V</sub></italic> and <italic>x<sub>A</sub></italic>, the prefactors <italic>p</italic>(<italic>C</italic>|<italic>x<sub>V</sub></italic>, <italic>x<sub>A</sub></italic>) appearing in from Eqs. (9) and (10) are nonlinear in these two variables. Therefore, the overall optimal estimates <italic>Ŝ<sub>V</sub></italic> and <italic>Ŝ<sub>A</sub></italic>, when both common and independent causes are possible, are no longer linear combinations. The traditional way of studying the system by only measuring linear weights is not sufficient to fully describe a system that performs causal inference.</p><p>The predicted distribution of visual positions is obtained through marginalization: <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.e015" xlink:type="simple"/><label>(13)</label></disp-formula>These are the distributions which we can compare with experimental data, the response distributions were obtained through simulation (for more details see the Supporting Information, <xref ref-type="supplementary-material" rid="pone.0000943.s001">Text S1</xref> and <xref ref-type="supplementary-material" rid="pone.0000943.s002">Fig. S1</xref>).</p><p>In the simulation, we ran each condition (<italic>s<sub>V</sub></italic>, <italic>s<sub>A</sub></italic>) 10,000 times for each subject. For each trial, we obtained estimates (<italic>Ŝ<sub>V</sub></italic>, <italic>Ŝ<sub>A</sub></italic>) in the way described above. To link these estimates with our data in which we have only five possible responses, we assume that people press the button which is associated with the position closest to <italic>Ŝ</italic>. This is equivalent to choosing the button that leads to the lowest expected mean squared error between the position indicated by the button and the position of the stimulus. This amounts to allowing only estimated values <italic>Ŝ</italic> that are in the set {−10°, −5°, 0°, 5°, 10°}. In this way, we obtained a histogram of responses for each subject and condition. For the model comparison in <xref ref-type="table" rid="pone-0000943-t001">Table 1</xref>, we also considered a maximum-a-posteriori estimator, which does not use a cost function but instead selects the location from the 5-element response set that has the highest probability of being correct. To link the causal inference model with the data on causal inference <xref ref-type="bibr" rid="pone.0000943-Wallace1">[15]</xref> of <xref ref-type="fig" rid="pone-0000943-g003">figure 3</xref> in the main text, we assume additional motor noise with width <italic>σ</italic><sub>motor</sub>, that perturbs the estimated position <italic>Ŝ</italic>.</p><p>A potential problem with any kind of optimization procedure is the danger of over-fitting. To test for this with out model we split the data set in two groups, each with half the data from each subject. When optimizing on the first group, we find an excellent performance of the model (<italic>R</italic><sup>2</sup> = 0.98) and when we transfer the optimized parameters to the second set of data, we still find an excellent performance (<italic>R</italic><sup>2</sup> = 0.96). Overfitting is therefore not a problem with the causal inference model on this data set.</p></sec><sec id="s4e"><title>Data Analysis for <xref ref-type="fig" rid="pone-0000943-g002">Figure 2</xref></title><p>We choose <italic>p</italic><sub>common</sub>, <italic>σ<sub>P</sub></italic>, <italic>σ<sub>V</sub></italic>, <italic>σ<sub>A</sub></italic> to maximize the probability of the experimental data given the model. For the group analysis we obtain these 4 parameters from a dataset obtained by pooling together the data from all subjects. To obtain the histograms plotted in <xref ref-type="fig" rid="pone-0000943-g002">Figure 2C</xref> we simulate the system for each combination of cues for 10,000 times. It is a common mistake <xref ref-type="bibr" rid="pone.0000943-Shams1">[e.g. 13]</xref>, <xref ref-type="bibr" rid="pone.0000943-Roach1">[18]</xref> to directly compare a distribution like <italic>p</italic>(<italic>s<sub>A</sub></italic> | <italic>x<sub>V</sub> ,x<sub>A</sub></italic>) to the data. This is a mistake because <italic>x<sub>V</sub></italic> and <italic>x<sub>A</sub></italic> are internal representations that differ from trial to trial and are not accessible to the experimenter. An inferred distribution like <italic>p</italic>(<italic>s<sub>A</sub></italic> | <italic>x<sub>V</sub> ,x<sub>A</sub></italic>) will give rise to a single estimate <italic>Ŝ<sub>V</sub></italic> or <italic>Ŝ<sub>A</sub></italic> as given by Equation 2. Only a histogram of estimates over many simulated trials can be compared with behavioral responses. This histogram will in many cases be very different in shape from <italic>p</italic>(<italic>s<sub>A</sub></italic> | <italic>x<sub>V </sub>, x<sub>A</sub></italic>). To fit the model to the data we maximize the probability of the data given the model using the multinomial likelihood.</p></sec><sec id="s4f"><title>Calculating the multinomial likelihood</title><p>We need to calculate the likelihood of the data given a model and its parameters. Since the data take the form of counts of discrete outcomes (1 to 5), we use the multinomial distribution. On each trial, a response is drawn from a discrete probability distribution. Each model produces numerical approximations to the probabilities of each response in a given condition (<italic>s<sub>V</sub>, s<sub>A</sub></italic> ). We will denote these <italic>p<sub>i</sub></italic>, with <italic>i</italic> = 1, 2, 3, 4, 5. Suppose the observed response counts in this condition are <italic>n<sub>i</sub></italic>, with <italic>i</italic> = 1, 2, 3, 4, 5. Then the probability of observing counts {<italic>n<sub>i</sub></italic>} under this model is: <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.e016" xlink:type="simple"/><label>(14)</label></disp-formula>where <italic>n</italic> is the total number of trials in this condition, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0000943.e017" xlink:type="simple"/></inline-formula>, and the prefactor <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0000943.e018" xlink:type="simple"/></inline-formula> is the number of ways in which <italic>n</italic> responses can be split up into five counts {<italic>n<sub>i</sub></italic>}. This gives the likelihood of the model given the data: <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.e019" xlink:type="simple"/><label>(15)</label></disp-formula>Maximizing this likelihood is equivalent to maximizing its logarithm, which is given by. <disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.e020" xlink:type="simple"/><label>(16)</label></disp-formula></p><p>So far we have considered a single condition. Since conditions are independent of each other, the overall log likelihood is the sum of the log likelihoods in all conditions. These overall log likelihoods are compared for different models in <xref ref-type="table" rid="pone-0000943-t001">Table 1</xref> in the main text and are used for fitting the parameters to the causal inference model and all other models we are comparing to.</p></sec><sec id="s4g"><title>Data analysis for <xref ref-type="table" rid="pone-0000943-t001">Table 1</xref></title><p>We compare how good the causal inference model is relative to other models. The way this is usually done within Bayesian statistics is by calculating the Bayes factor, the ratio of the likelihood of the data under one model to the likelihood of the data under the other model, <italic>p</italic>(data|model<sub>1</sub>)/<italic>p</italic>(data|model<sub>2</sub>). As the necessary integration of all nuisance parameters is infeasible we instead analyze the ratios of maximal probabilities which can be corrected for the different number of free variables using the BIC <xref ref-type="bibr" rid="pone.0000943-Barnes1">[45]</xref>. This ratio formalizes how much evidence the data provides for one model relative to another model and is thus a very general and mathematically precise way of quantifying the quality of a model. According to the BIC the causal inference model is preferable over the models that have fewer parameters. <xref ref-type="table" rid="pone-0000943-t001">Table 1</xref> reports the log maximal probability ratios for several models relative to the causal inference model.</p></sec><sec id="s4h"><title>Data analysis for <xref ref-type="fig" rid="pone-0000943-g003">Figure 3</xref></title><p>In these experiments, subjects report the perceived position by pointing with a laser pointer. In such cases there may be additional noise due to a misalignment of the cursor relative to the intended position of the cursor. To model this, we introduce one additional variable, motor noise. We assume that motor noise corrupts all reports, is additive and drawn from a Gaussian with width <italic>σ</italic><sub>motor</sub>. We estimate the relevant uncertainties as follows. In both auditory and visual trials the noise will have two sources, motor noise and sensory noise. We assume that visual only trials are dominated by motor noise, stemming from motor errors and memory errors. From data presented in <xref ref-type="fig" rid="pone-0000943-g002">figure 2</xref> of other experiments <xref ref-type="bibr" rid="pone.0000943-Hairston1">[16]</xref> where pointing responses are made in unimodal trials, we obtain <italic>σ</italic><sub>motor</sub> = 2.5°, and from the same graph we obtain <italic>σ<sub>A</sub></italic> = 7.6° (because variances are added linearly). These parameters were not tuned. The other two parameters, <italic>p</italic><sub>common</sub> and <italic>σ<sub>P</sub></italic>, were obtained by minimizing the squared deviation of the model predictions from the data. The choice of <italic>p</italic><sub>common</sub> affects the judgment of unity and thus strongly affects the bias graph as well as the commonality judgments. Large values of <italic>p</italic><sub>common</sub> lead to high bias and small values of <italic>p</italic><sub>common</sub> lead to bias values that are high only very close to zero disparity. The same parameter values are used for both graphs in <xref ref-type="fig" rid="pone-0000943-g003">figure 3</xref>.</p></sec></sec><sec id="s5"><title>Supporting Information</title><supplementary-material id="pone.0000943.s001" mimetype="application/msword" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.s001" xlink:type="simple"><label>Text S1</label><caption><p>Supporting Information for “Causal inference in multisensory perception”</p><p>(0.11 MB DOC)</p></caption></supplementary-material><supplementary-material id="pone.0000943.s002" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.s002" xlink:type="simple"><label>Figure S1</label><caption><p>The interaction priors when fit to our dataset are shown for the causal inference model, the Roach et al. <xref ref-type="bibr" rid="pone.0000943-Hatfield1">[1]</xref> and the Bresciani et al. priors<xref ref-type="bibr" rid="pone.0000943-Warren1">[3]</xref>.</p><p>(1.15 MB EPS)</p></caption></supplementary-material><supplementary-material id="pone.0000943.s003" mimetype="application/postscript" position="float" xlink:href="info:doi/10.1371/journal.pone.0000943.s003" xlink:type="simple"><label>Figure S2</label><caption><p>The average auditory bias, i.e. the relative influence of the visual position on the perceived auditory position, is shown as a function of the absolute spatial disparity (solid line, as in <xref ref-type="fig" rid="pone-0000943-g002">Fig. 2</xref> main text) along with the model predictions (dashed lines). Red: causal inference model. Green: behavior derived from using the Roach et al prior. Purple: behaviour derived from using the Bresciani et al prior.</p><p>(0.94 MB EPS)</p></caption></supplementary-material></sec></body><back><ack><p>We would like to thank David Knill and Philip Sabes for inspiring discussions and Jose L. Pena for help with the setup of the experiment in figure 2. K.P.K., U.B., and W.J.M. contributed equally to this paper. After completion of this work, we became aware of a paper by Yoshiyuki Sato, Taro Toyoizumi and Kazuyuki Aihara, who independently developed a similar model (Neural Computation, in press).</p></ack><ref-list><title>References</title><ref id="pone.0000943-Hatfield1"><label>1</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hatfield</surname><given-names>GC</given-names></name></person-group>             <year>1990</year>             <article-title>The natural and the normative theories of spatial perception from Kant to Helmholtz.</article-title>             <publisher-loc>Cambridge, Mass</publisher-loc>             <publisher-name>MIT Press</publisher-name>          </element-citation></ref><ref id="pone.0000943-Thurlow1"><label>2</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Thurlow</surname><given-names>WR</given-names></name><name name-style="western"><surname>Jack</surname><given-names>CE</given-names></name></person-group>             <year>1973</year>             <article-title>Certain determinants of the “ventriloquism effect”.</article-title>             <source>Percept Mot Skills</source>             <volume>36</volume>             <fpage>1171</fpage>             <lpage>1184</lpage>          </element-citation></ref><ref id="pone.0000943-Warren1"><label>3</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Warren</surname><given-names>DH</given-names></name><name name-style="western"><surname>Welch</surname><given-names>RB</given-names></name><name name-style="western"><surname>McCarthy</surname><given-names>TJ</given-names></name></person-group>             <year>1981</year>             <article-title>The role of visual-auditory “compellingness” in the ventriloquism effect: implications for transitivity among the spatial senses.</article-title>             <source>Percept Psychophys</source>             <volume>30</volume>             <fpage>557</fpage>             <lpage>564</lpage>          </element-citation></ref><ref id="pone.0000943-Jacobs1"><label>4</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jacobs</surname><given-names>RA</given-names></name></person-group>             <year>1999</year>             <article-title>Optimal integration of texture and motion cues to depth.</article-title>             <source>Vision Res</source>             <volume>39</volume>             <fpage>3621</fpage>             <lpage>3629</lpage>          </element-citation></ref><ref id="pone.0000943-vanBeers1"><label>5</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>van Beers</surname><given-names>RJ</given-names></name><name name-style="western"><surname>Sittig</surname><given-names>AC</given-names></name><name name-style="western"><surname>Gon</surname><given-names>JJ</given-names></name></person-group>             <year>1999</year>             <article-title>Integration of proprioceptive and visual position-information: An experimentally supported model.</article-title>             <source>J Neurophysiol</source>             <volume>81</volume>             <fpage>1355</fpage>             <lpage>1364</lpage>          </element-citation></ref><ref id="pone.0000943-Ernst1"><label>6</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ernst</surname><given-names>MO</given-names></name><name name-style="western"><surname>Banks</surname><given-names>MS</given-names></name></person-group>             <year>2002</year>             <article-title>Humans integrate visual and haptic information in a statistically optimal fashion.</article-title>             <source>Nature</source>             <volume>415</volume>             <fpage>429</fpage>             <lpage>433</lpage>          </element-citation></ref><ref id="pone.0000943-Alais1"><label>7</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Alais</surname><given-names>D</given-names></name><name name-style="western"><surname>Burr</surname><given-names>D</given-names></name></person-group>             <year>2004</year>             <article-title>The ventriloquist effect results from near-optimal bimodal integration.</article-title>             <source>Curr Biol</source>             <volume>14</volume>             <fpage>257</fpage>             <lpage>262</lpage>          </element-citation></ref><ref id="pone.0000943-Slutsky1"><label>8</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Slutsky</surname><given-names>DA</given-names></name><name name-style="western"><surname>Recanzone</surname><given-names>GH</given-names></name></person-group>             <year>2001</year>             <article-title>Temporal and spatial dependency of the ventriloquism effect.</article-title>             <source>Neuroreport</source>             <volume>12</volume>             <fpage>7</fpage>             <lpage>10</lpage>          </element-citation></ref><ref id="pone.0000943-Munhall1"><label>9</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Munhall</surname><given-names>KG</given-names></name><name name-style="western"><surname>Gribble</surname><given-names>P</given-names></name><name name-style="western"><surname>Sacco</surname><given-names>L</given-names></name><name name-style="western"><surname>Ward</surname><given-names>M</given-names></name></person-group>             <year>1996</year>             <article-title>Temporal constraints on the McGurk effect.</article-title>             <source>Percept Psychophys</source>             <volume>58</volume>             <fpage>351</fpage>             <lpage>362</lpage>          </element-citation></ref><ref id="pone.0000943-Jack1"><label>10</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Jack</surname><given-names>CE</given-names></name><name name-style="western"><surname>Thurlow</surname><given-names>WR</given-names></name></person-group>             <year>1973</year>             <article-title>Effects of degree of visual association and angle of displacement on the “ventriloquism” effect.</article-title>             <source>Percept Mot Skills</source>             <volume>37</volume>             <fpage>967</fpage>             <lpage>979</lpage>          </element-citation></ref><ref id="pone.0000943-Warren2"><label>11</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Warren</surname><given-names>DH</given-names></name><name name-style="western"><surname>Cleaves</surname><given-names>WT</given-names></name></person-group>             <year>1971</year>             <article-title>Visual-proprioceptive interaction under large amounts of conflict.</article-title>             <source>J Exp Psychol</source>             <volume>90</volume>             <fpage>206</fpage>             <lpage>214</lpage>          </element-citation></ref><ref id="pone.0000943-Gepshtein1"><label>12</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gepshtein</surname><given-names>S</given-names></name><name name-style="western"><surname>Burge</surname><given-names>J</given-names></name><name name-style="western"><surname>Ernst</surname><given-names>MO</given-names></name><name name-style="western"><surname>Banks</surname><given-names>MS</given-names></name></person-group>             <year>2005</year>             <article-title>The combination of vision and touch depends on spatial proximity.</article-title>             <source>J Vis</source>             <volume>5</volume>             <fpage>1013</fpage>             <lpage>1023</lpage>          </element-citation></ref><ref id="pone.0000943-Shams1"><label>13</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Shams</surname><given-names>L</given-names></name><name name-style="western"><surname>Ma</surname><given-names>WJ</given-names></name><name name-style="western"><surname>Beierholm</surname><given-names>U</given-names></name></person-group>             <year>2005</year>             <article-title>Sound-induced flash illusion as an optimal percept.</article-title>             <source>Neuroreport</source>             <volume>16</volume>             <fpage>1923</fpage>             <lpage>1927</lpage>          </element-citation></ref><ref id="pone.0000943-Bresciani1"><label>14</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bresciani</surname><given-names>JP</given-names></name><name name-style="western"><surname>Dammeier</surname><given-names>F</given-names></name><name name-style="western"><surname>Ernst</surname><given-names>MO</given-names></name></person-group>             <year>2006</year>             <article-title>Vision and touch are automatically integrated for the perception of sequences of events.</article-title>             <source>J Vis</source>             <volume>6</volume>             <fpage>554</fpage>             <lpage>564</lpage>          </element-citation></ref><ref id="pone.0000943-Wallace1"><label>15</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Wallace</surname><given-names>MT</given-names></name><name name-style="western"><surname>Roberson</surname><given-names>GE</given-names></name><name name-style="western"><surname>Hairston</surname><given-names>WD</given-names></name><name name-style="western"><surname>Stein</surname><given-names>BE</given-names></name><name name-style="western"><surname>Vaughan</surname><given-names>JW</given-names></name><etal/></person-group>             <year>2004</year>             <article-title>Unifying multisensory signals across time and space.</article-title>             <source>Exp Brain Res</source>             <volume>158</volume>             <fpage>252</fpage>             <lpage>258</lpage>          </element-citation></ref><ref id="pone.0000943-Hairston1"><label>16</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hairston</surname><given-names>WD</given-names></name><name name-style="western"><surname>Wallace</surname><given-names>MT</given-names></name><name name-style="western"><surname>Vaughan</surname><given-names>JW</given-names></name><name name-style="western"><surname>Stein</surname><given-names>BE</given-names></name><name name-style="western"><surname>Norris</surname><given-names>JL</given-names></name><etal/></person-group>             <year>2003</year>             <article-title>Visual localization ability influences cross-modal bias.</article-title>             <source>J Cogn Neurosci</source>             <volume>15</volume>             <fpage>20</fpage>             <lpage>29</lpage>          </element-citation></ref><ref id="pone.0000943-Rowland1"><label>17</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Rowland</surname><given-names>B</given-names></name><name name-style="western"><surname>Stanford</surname><given-names>T</given-names></name><name name-style="western"><surname>Stein</surname><given-names>BE</given-names></name></person-group>             <year>2007</year>             <article-title>A Bayesian model unifies multisensory spatial localization with the physiological properties of the superior colliculus.</article-title>             <comment>Exp Brain Res DOI 10.1007/s00221-006-0847-2</comment>          </element-citation></ref><ref id="pone.0000943-Roach1"><label>18</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Roach</surname><given-names>NW</given-names></name><name name-style="western"><surname>Heron</surname><given-names>J</given-names></name><name name-style="western"><surname>McGraw</surname><given-names>PV</given-names></name></person-group>             <year>2006</year>             <article-title>Resolving multisensory conflict: a strategy for balancing the costs and benefits of audio-visual integration.</article-title>             <source>Proc Biol Sci</source>             <volume>273</volume>             <fpage>2159</fpage>             <lpage>2168</lpage>          </element-citation></ref><ref id="pone.0000943-McDonald1"><label>19</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>McDonald</surname><given-names>P</given-names></name></person-group>             <year>2003</year>             <article-title>Demonstration by Simulation: the Philosophical Significance of Experiment in Helmholtz's Theory of Perception.</article-title>             <source>Perspectives on Science</source>             <volume>11</volume>             <fpage>170</fpage>             <lpage>207</lpage>          </element-citation></ref><ref id="pone.0000943-Helmholtz1"><label>20</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Helmholtz</surname><given-names>HFv</given-names></name></person-group>             <year>1863 (1954)</year>             <article-title>On the sensations of tone as a physiological basis for the theory of music.</article-title>             <publisher-loc>New York</publisher-loc>             <publisher-name>Dover Publics</publisher-name>          </element-citation></ref><ref id="pone.0000943-Ernst2"><label>21</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ernst</surname><given-names>MO</given-names></name></person-group>             <year>2007</year>             <article-title>Learning to integrate arbitrary signals from vision and touch.</article-title>             <source>Journal of Vision in press</source>          </element-citation></ref><ref id="pone.0000943-Treisman1"><label>22</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Treisman</surname><given-names>A</given-names></name></person-group>             <year>1996</year>             <article-title>The binding problem.</article-title>             <source>Curr Opin Neurobiol</source>             <volume>6</volume>             <fpage>171</fpage>             <lpage>178</lpage>          </element-citation></ref><ref id="pone.0000943-Reynolds1"><label>23</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Reynolds</surname><given-names>JH</given-names></name><name name-style="western"><surname>Desimone</surname><given-names>R</given-names></name></person-group>             <year>1999</year>             <article-title>The role of neural mechanisms of attention in solving the binding problem.</article-title>             <source>Neuron</source>             <volume>24</volume>             <fpage>19</fpage>             <lpage>29</lpage>                 </element-citation></ref><ref id="pone.0000943-Knill1"><label>24</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Knill</surname><given-names>DC</given-names></name></person-group>             <year>2003</year>             <article-title>Mixture models and the probabilistic structure of depth cues.</article-title>             <source>Vision Res</source>             <volume>43</volume>             <fpage>831</fpage>             <lpage>854</lpage>          </element-citation></ref><ref id="pone.0000943-Recanzone1"><label>25</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Recanzone</surname><given-names>GH</given-names></name></person-group>             <year>2003</year>             <article-title>Auditory influences on visual temporal rate perception.</article-title>             <source>J Neurophysiol</source>             <volume>89</volume>             <fpage>1078</fpage>             <lpage>1093</lpage>          </element-citation></ref><ref id="pone.0000943-Choe1"><label>26</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Choe</surname><given-names>CS</given-names></name><name name-style="western"><surname>Welch</surname><given-names>RB</given-names></name><name name-style="western"><surname>Gilford</surname><given-names>RM</given-names></name><name name-style="western"><surname>Juola</surname><given-names>JF</given-names></name></person-group>             <year>1975</year>             <article-title>The “ventriloquist effect”: visual dominance or response bias.</article-title>             <source>Perception and Psychophysics</source>             <volume>18</volume>             <fpage>55</fpage>             <lpage>60</lpage>          </element-citation></ref><ref id="pone.0000943-Bertelson1"><label>27</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bertelson</surname><given-names>P</given-names></name><name name-style="western"><surname>Pavani</surname><given-names>F</given-names></name><name name-style="western"><surname>Ladavas</surname><given-names>E</given-names></name><name name-style="western"><surname>Vroomen</surname><given-names>J</given-names></name><name name-style="western"><surname>de Gelder</surname><given-names>B</given-names></name></person-group>             <year>2000</year>             <article-title>Ventriloquism in patients with unilateral visual neglect.</article-title>             <source>Neuropsychologia</source>             <volume>38</volume>             <fpage>1634</fpage>             <lpage>1642</lpage>          </element-citation></ref><ref id="pone.0000943-Burnham1"><label>28</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Burnham</surname><given-names>KP</given-names></name><name name-style="western"><surname>Anderson</surname><given-names>DR</given-names></name></person-group>             <year>2002</year>             <article-title>Model Selection and Multimodel Inference: A Practical-Theoretic Approach.</article-title>             <publisher-loc>New York</publisher-loc>             <publisher-name>Springer</publisher-name>          </element-citation></ref><ref id="pone.0000943-Adams1"><label>29</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Adams</surname><given-names>WJ</given-names></name><name name-style="western"><surname>Graf</surname><given-names>EW</given-names></name><name name-style="western"><surname>Ernst</surname><given-names>MO</given-names></name></person-group>             <year>2004</year>             <article-title>Experience can change the ‘light-from-above’ prior.</article-title>             <source>Nat Neurosci</source>             <volume>7</volume>             <fpage>1057</fpage>             <lpage>1058</lpage>          </element-citation></ref><ref id="pone.0000943-Roskies1"><label>30</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Roskies</surname><given-names>AL</given-names></name></person-group>             <year>1999</year>             <article-title>The binding problem.</article-title>             <source>Neuron</source>             <volume>24</volume>             <fpage>7</fpage>             <lpage>9</lpage>                     </element-citation></ref><ref id="pone.0000943-Gould1"><label>31</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gould</surname><given-names>SJ</given-names></name><name name-style="western"><surname>Lewontin</surname><given-names>R</given-names></name></person-group>             <year>1979</year>             <article-title>The Spandrels of San Marco and the Panglossian Paradigm: A Critique of the Adaptionist Programme.</article-title>             <source>Proceedings of the Royal Society B</source>             <volume>205</volume>             <fpage>581</fpage>             <lpage>598</lpage>          </element-citation></ref><ref id="pone.0000943-Gigerenzer1"><label>32</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gigerenzer</surname><given-names>G</given-names></name></person-group>             <year>2001</year>             <article-title>The adaptive toolbox: toward a darwinian rationality.</article-title>             <source>Nebr Symp Motiv</source>             <volume>47</volume>             <fpage>113</fpage>             <lpage>143</lpage>          </element-citation></ref><ref id="pone.0000943-Zemel1"><label>33</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Zemel</surname><given-names>RS</given-names></name><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name><name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name></person-group>             <year>1998</year>             <article-title>Probabilistic interpretation of population codes.</article-title>             <source>Neural Comput</source>             <volume>10</volume>             <fpage>403</fpage>             <lpage>430</lpage>          </element-citation></ref><ref id="pone.0000943-Pouget1"><label>34</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name><name name-style="western"><surname>Zemel</surname><given-names>RS</given-names></name></person-group>             <year>2003</year>             <article-title>Inference and computation with population codes.</article-title>             <source>Annu Rev Neurosci</source>             <volume>26</volume>             <fpage>381</fpage>             <lpage>410</lpage>          </element-citation></ref><ref id="pone.0000943-Ma1"><label>35</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>WJ</given-names></name><name name-style="western"><surname>Beck</surname><given-names>JM</given-names></name><name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name><name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name></person-group>             <year>2006</year>             <article-title>Bayesian inference with probabilistic population codes.</article-title>             <source>Nat Neurosci</source>             <volume>9</volume>             <fpage>1432</fpage>             <lpage>1438</lpage>          </element-citation></ref><ref id="pone.0000943-Buehner1"><label>36</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Buehner</surname><given-names>MJ</given-names></name><name name-style="western"><surname>Cheng</surname><given-names>PW</given-names></name><name name-style="western"><surname>Clifford</surname><given-names>D</given-names></name></person-group>             <year>2003</year>             <article-title>From covariation to causation: a test of the assumption of causal power.</article-title>             <source>J Exp Psychol Learn Mem Cogn</source>             <volume>29</volume>             <fpage>1119</fpage>             <lpage>1140</lpage>          </element-citation></ref><ref id="pone.0000943-Gopnik1"><label>37</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gopnik</surname><given-names>A</given-names></name><name name-style="western"><surname>Glymour</surname><given-names>C</given-names></name><name name-style="western"><surname>Sobel</surname><given-names>DM</given-names></name><name name-style="western"><surname>Schulz</surname><given-names>LE</given-names></name><name name-style="western"><surname>Kushnir</surname><given-names>T</given-names></name><etal/></person-group>             <year>2004</year>             <article-title>A theory of causal learning in children: causal maps and Bayes nets.</article-title>             <source>Psychol Rev</source>             <volume>111</volume>             <fpage>3</fpage>             <lpage>32</lpage>          </element-citation></ref><ref id="pone.0000943-Saxe1"><label>38</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Saxe</surname><given-names>R</given-names></name><name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name><name name-style="western"><surname>Carey</surname><given-names>S</given-names></name></person-group>             <year>2005</year>             <article-title>Secret agents: inferences about hidden causes by 10- and 12-month-old infants.</article-title>             <source>Psychol Sci</source>             <volume>16</volume>             <fpage>995</fpage>             <lpage>1001</lpage>          </element-citation></ref><ref id="pone.0000943-Griffiths1"><label>39</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Griffiths</surname><given-names>TL</given-names></name><name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name></person-group>             <year>2005</year>             <article-title>Structure and strength in causal induction.</article-title>             <source>Cognit Psychol</source>             <volume>51</volume>             <fpage>334</fpage>             <lpage>384</lpage>          </element-citation></ref><ref id="pone.0000943-Waldmann1"><label>40</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Waldmann</surname><given-names>MR</given-names></name></person-group>             <year>2000</year>             <article-title>Competition among causes but not effects in predictive and diagnostic learning.</article-title>             <source>J Exp Psychol Learn Mem Cogn</source>             <volume>26</volume>             <fpage>53</fpage>             <lpage>76</lpage>          </element-citation></ref><ref id="pone.0000943-Tenenbaum1"><label>41</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name><name name-style="western"><surname>Griffiths</surname><given-names>TL</given-names></name><name name-style="western"><surname>Kemp</surname><given-names>C</given-names></name></person-group>             <year>2006</year>             <article-title>Theory-based Bayesian models of inductive learning and reasoning.</article-title>             <source>Trends Cogn Sci</source>             <volume>10</volume>             <fpage>309</fpage>             <lpage>318</lpage>          </element-citation></ref><ref id="pone.0000943-Sobel1"><label>42</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Sobel</surname><given-names>D</given-names></name><name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name><name name-style="western"><surname>Gopnik</surname><given-names>A</given-names></name></person-group>             <year>2004</year>             <article-title>Children's causal inferences from indirect evidence: Backwards blocking and Bayesian reasoning in preschoolers.</article-title>             <source>Cognitive Science</source>             <volume>28</volume>             <fpage>303</fpage>             <lpage>333</lpage>          </element-citation></ref><ref id="pone.0000943-Kording1"><label>43</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kording</surname><given-names>KP</given-names></name><name name-style="western"><surname>Wolpert</surname><given-names>DM</given-names></name></person-group>             <year>2004</year>             <article-title>The loss function of sensorimotor learning.</article-title>             <source>Proc Natl Acad Sci U S A</source>             <volume>101</volume>             <fpage>9839</fpage>             <lpage>9842</lpage>          </element-citation></ref><ref id="pone.0000943-Ghahramani1"><label>44</label><element-citation publication-type="book" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ghahramani</surname><given-names>Z</given-names></name></person-group>             <year>1995</year>             <article-title>Computational and psychophysics of sensorimotor integration.</article-title>             <publisher-loc>Cambridge</publisher-loc>             <publisher-name>Massachusetts Institute of Technology</publisher-name>          </element-citation></ref><ref id="pone.0000943-Barnes1"><label>45</label><element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Barnes</surname><given-names>CA</given-names></name></person-group>             <year>1979</year>             <article-title>Memory deficits associated with senescence: a neurophysiological and behavioral study in the rat.</article-title>             <source>J Comp Physiol Psychol</source>             <volume>93</volume>             <fpage>74</fpage>             <lpage>104</lpage>          </element-citation></ref></ref-list></back></article>