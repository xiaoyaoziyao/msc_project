<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="3.0" xml:lang="en" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id>
<journal-title-group>
<journal-title>PLOS Computational Biology</journal-title>
</journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-14-01595</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1004039</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Computation in Dynamically Bounded Asymmetric Systems</article-title>
<alt-title alt-title-type="running-head">Computation in Dynamically Bounded Asymmetric Systems</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Rutishauser</surname> <given-names>Ueli</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Slotine</surname> <given-names>Jean-Jacques</given-names></name>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Douglas</surname> <given-names>Rodney</given-names></name>
<xref ref-type="aff" rid="aff005"><sup>5</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Computation and Neural Systems, California Institute of Technology, Pasadena, California, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Division of Biology and Biological Engineering, California Institute of Technology, Pasadena, California, United States of America</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Departments of Neurosurgery, Neurology and Biomedical Sciences, Cedars-Sinai Medical Center, Los Angeles, California, United States of America</addr-line>
</aff>
<aff id="aff004">
<label>4</label>
<addr-line>Nonlinear Systems Laboratory, Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States of America</addr-line>
</aff>
<aff id="aff005">
<label>5</label>
<addr-line>Institute of Neuroinformatics, University and ETH Zurich, Zurich, Switzerland</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Sporns</surname> <given-names>Olaf</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Indiana University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: UR JJS RJD. Performed the experiments: UR. Analyzed the data: UR JJS RJD. Contributed reagents/materials/analysis tools: UR JJS RJD. Wrote the paper: UR JJS RJD.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">urut@caltech.edu</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<month>1</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="epub">
<day>24</day>
<month>1</month>
<year>2015</year>
</pub-date>
<volume>11</volume>
<issue>1</issue>
<elocation-id>e1004039</elocation-id>
<history>
<date date-type="received">
<day>1</day>
<month>9</month>
<year>2014</year>
</date>
<date date-type="accepted">
<day>12</day>
<month>11</month>
<year>2014</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Rutishauser et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pcbi.1004039" xlink:type="simple"/>
<abstract>
<p>Previous explanations of computations performed by recurrent networks have focused on symmetrically connected saturating neurons and their convergence toward attractors. Here we analyze the behavior of asymmetrical connected networks of linear threshold neurons, whose positive response is unbounded. We show that, for a wide range of parameters, this asymmetry brings interesting and computationally useful dynamical properties. When driven by input, the network explores potential solutions through highly unstable ‘expansion’ dynamics. This expansion is steered and constrained by negative divergence of the dynamics, which ensures that the dimensionality of the solution space continues to reduce until an acceptable solution manifold is reached. Then the system contracts stably on this manifold towards its final solution trajectory. The unstable positive feedback and cross inhibition that underlie expansion and divergence are common motifs in molecular and neuronal networks. Therefore we propose that very simple organizational constraints that combine these motifs can lead to spontaneous computation and so to the spontaneous modification of entropy that is characteristic of living systems.</p>
</abstract>
<abstract abstract-type="summary">
<title>Author Summary</title>
<p>Biological systems are obviously able to process abstract information on the states of neuronal and molecular networks. However, the concepts and principles of such biological computation are poorly understood by comparison with technological computing. A key concept in models of biological computation has been the attractor of dynamical systems, and much progress has been made in describing the conditions under which attractors exist, and their stability. Instead, we show here for a broad class of asymmetrically connected networks that it is the unstable dynamics of the system that drive its computation, and we develop new analytical tools to describe the kinds of unstable dynamics that support this computation in our model. In particular we explore the conditions under which networks will exhibit unstable expansion of their dynamics, and how these can be steered and constrained so that the trajectory implements a specific computation. Importantly, the underlying computational elements of the network are not themselves stable. Instead, the overall boundedness of the system is provided by the asymmetrical coupling between excitatory and inhibitory elements commonly observed in neuronal and molecular networks. This inherent boundedness permits the network to operate with the unstably high gain necessary to continually switch its states as it searches for a solution. We propose that very simple organizational constraints can lead to spontaneous computation, and thereby to the spontaneous modification of entropy that is characteristic of living systems.</p>
</abstract>
<funding-group>
<funding-statement>Funded by EU grants DAISY (FP6-2005-015803) and SECO (FP7-2009-216593) awarded to RJD. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="0"/>
<page-count count="22"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>All data in this manuscript is simulated. The MATLAB source code of the underlying models is available on the webpage <ext-link ext-link-type="uri" xlink:href="http://www.ini.uzh.ch/~urut/DFAWTA/" xlink:type="simple">http://www.ini.uzh.ch/~urut/DFAWTA/</ext-link></meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>The principles of biological computation are not well understood. Although the Turing Machine and related concepts [<xref ref-type="bibr" rid="pcbi.1004039.ref001">1</xref>–<xref ref-type="bibr" rid="pcbi.1004039.ref003">3</xref>] have provided powerful models for understanding and developing technological computing, they have provided less insight for biological computation because they generally assume that the machines themselves, as well as their initial program and data are granted as input. In contrast, the organization of states and transitions of the biological process arise out of phylogenetic and ontogenetic configuration processes and execute autonomously without the intervention of an intelligent external programmer and controller being necessary to supply already encoded organizationally relevant information. Our goal here is to make steps towards understanding biological computation [<xref ref-type="bibr" rid="pcbi.1004039.ref004">4</xref>–<xref ref-type="bibr" rid="pcbi.1004039.ref006">6</xref>], by considering the behavior of a simple non-linear dynamical system composed of asymmetrically inter-connected linear-threshold neurons. We suppose that such computations entail a mapping from some input towards a limited (low entropy) region of phase space, which is the solution [<xref ref-type="bibr" rid="pcbi.1004039.ref007">7</xref>]. We do not suppose that the computational goal is known—only that computation must conform to this basic entropy reducing process. Here we describe the organizational constraints that make such spontaneous computation possible.</p>
<p>Previous authors have explained neural network computation in terms of the convergence of special dynamical systems, and emphasized the attractors to which they converge [<xref ref-type="bibr" rid="pcbi.1004039.ref008">8</xref>–<xref ref-type="bibr" rid="pcbi.1004039.ref013">13</xref>]. For example, Hopfield [<xref ref-type="bibr" rid="pcbi.1004039.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004039.ref010">10</xref>] has shown how and why the dynamics of symmetrically connected neurons with saturating outputs converge to attractor states; and others have offered similar insights for symmetrically connected linear threshold neurons [<xref ref-type="bibr" rid="pcbi.1004039.ref014">14</xref>–<xref ref-type="bibr" rid="pcbi.1004039.ref016">16</xref>]. However, interactions between inhibitory and excitatory neurons are clearly asymmetric, making these studies ill suited to study biological computation. To the extent that asymmetrical networks have been considered at all, this has been through assumptions that reduce asymmetrical networks to approximate symmetry. By contrast, we consider here the dynamics of fully asymmetrical networks, and discover that asymmetry contributes strongly to computational behavior.</p>
<p>The scope of our work is restricted to recurrent neural networks with asymmetric coupling that express an important and ubiquitous behavior: soft winner-take-all (sWTA) dynamics. We present a formal account of the response of these networks to exogenous perturbations and use a form of non-linear stability analysis (contraction analysis [<xref ref-type="bibr" rid="pcbi.1004039.ref017">17</xref>]) to characterize the itinerant transients than ensue, and which have useful interpretations in terms of neuronal computation and information theory. Contraction Theory offers a more flexible framework than the conventional Lyapunov approach to non-linear stability (See <xref ref-type="sec" rid="sec004">Methods</xref> for details). This is particularly the case for non-autonomous systems such as our network, in which external inputs can vary with time.</p>
<p>We explore particularly the behavior of network computation during the non-equilibrium phase, when the network is traversing its state-space seeking for a solution. We show that the ability of the network to explore potential solutions depends on highly unstable ’expansion’ dynamics driven by recurrent excitation. This expansion is steered and constrained by negative divergence of the dynamics, which ensures that the dimensionality of the solution space continues to reduce until an acceptable solution manifold is reached. The system then ’contracts’ stably on this manifold [<xref ref-type="bibr" rid="pcbi.1004039.ref017">17</xref>] towards its final solution trajectory, which is not necessarily converging to a fixed point. We argue that the simple principle of unstable expansion constrained by negative divergence provides the central organizing drive for more general autonomous biological systems from molecular networks, through neurons, to society.</p>
<p>Consider a simple network of non-linear neuron-like elements whose task it is to compute the solution to some problem. The states of the computation are encoded in the activations (firing rates) of the neurons, and the computational transitions between these states arise out of their synaptic interactions. The overall trajectory resulting from the successive transitions through its state space express its computation [<xref ref-type="bibr" rid="pcbi.1004039.ref018">18</xref>–<xref ref-type="bibr" rid="pcbi.1004039.ref020">20</xref>]. In current technological systems the hardware states of the system are encoded on binary nodes whose discrete states are imposed by signal restoring [<xref ref-type="bibr" rid="pcbi.1004039.ref021">21</xref>] circuitry [<xref ref-type="bibr" rid="pcbi.1004039.ref019">19</xref>]. This signal restoration is achieved by extremely high gain, so that a small input bias will drive the node into saturation at one of its two voltage limits. Biology rarely commands such sharply demarcated states and transitions. Instead, molecular and electrophysiological activation functions are often approximately sigmoidal (eg Hill functions, voltage dependent conductance, neuronal current-discharge curves, etc). However, neuronal systems do not typically run in saturation. The typical activation of a neuron is thresholded below, and above this threshold it makes use of only the lower part of its dynamic range. It very rarely enters saturation at the upper end of its activation range. Therefore, a suitable model for neuronal activation is a thresholded linear one. That is, their activity is bounded from below, but their positive activity is essentially unbounded (over any practical range of discharge). This is a very well studied model [<xref ref-type="bibr" rid="pcbi.1004039.ref014">14</xref>–<xref ref-type="bibr" rid="pcbi.1004039.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1004039.ref022">22</xref>].</p>
<p>As in our previous work, the neuronal network model is composed of thresholded linear neuron-like units coupled through positive (excitatory) and negative (inhibitory) connections (see <xref ref-type="fig" rid="pcbi.1004039.g001">Fig. 1a</xref>). The unbounded positive range of neuron activation implies that the global stability of networks of these neurons must arise out of their collective interactions rather than from saturation of their individual activation functions as assumed by for example [<xref ref-type="bibr" rid="pcbi.1004039.ref009">9</xref>, <xref ref-type="bibr" rid="pcbi.1004039.ref010">10</xref>]. The key interaction here is the inhibitory feedback, which must at least ensure that not all neurons can simultaneously increase their activation [<xref ref-type="bibr" rid="pcbi.1004039.ref016">16</xref>]. Previous studies of such models have focused on the mathematically more tractable case in which the connections between neurons are symmetrical, and have no transmission delays. Our networks, by contrast, need not be symmetrical and may have transmission delays. Indeed, the asymmetry of connections will be used to computational advantage, not offered by symmetrical networks.</p>
<fig id="pcbi.1004039.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004039.g001</object-id>
<label>Figure 1</label>
<caption>
<title>Circuit motifs and simple circuit composed of motifs.</title>
<p>(A) Circuit motifs are excitatory self-recurrence (top) and shared inhibition (bottom). <italic>I<sub>i</sub></italic> denotes an external input. (B) Connectivity of a simple WTA circuit, consisting of two excitatory units that compete through shared inhibition. (C) More compact notation to denote the circuit shown in (B). Each excitatory element receives an external input.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004039.g001"/>
</fig>
</sec>
<sec id="sec002" sec-type="results">
<title>Results</title>
<sec id="sec002a">
<title>Network model</title>
<p>The network contains two fundamental circuit motifs (<xref ref-type="fig" rid="pcbi.1004039.g001">Fig. 1A</xref>): excitatory neurons that project both onto themselves and others, and inhibitory neurons which receive excitatory input from the same neurons that they inhibit. Several instances of these motifs together compose the general recurrent circuit that we investigate. In its simplest form, this recurrent network has the following form. There are N neurons, N-1 of which are excitatory and one (index N) is inhibitory (<xref ref-type="fig" rid="pcbi.1004039.g001">Fig. 1B,C</xref>). The excitatory neurons <italic>x</italic><sub><italic>i</italic> ≠ <italic>N</italic></sub> receive an optional external input <italic>I</italic><sub><italic>i</italic></sub>, and excitatory feedback <italic>α</italic> from themself and nearby excitatory neurons (<italic>α</italic><sub>1</sub> and <italic>α</italic><sub>2</sub>, respectively). The single inhibitory neuron <italic>x</italic><sub><italic>n</italic></sub> sums the <italic>β</italic><sub>2</sub> weighted input from all the excitatory neurons, and sends a common <italic>β</italic><sub>1</sub> inhibitory signal to each of its excitatory neurons. Each neuron has a resistive constant leak term <italic>G</italic><sub><italic>i</italic></sub>.</p>
<p>The dynamics of this simple network are:
<disp-formula id="pcbi.1004039.e001"><alternatives><graphic id="pcbi.1004039.e001g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e001"/><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mi>τ</mml:mi> <mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mi>N</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives><label>(1)</label></disp-formula> <disp-formula id="pcbi.1004039.e002"><alternatives><graphic id="pcbi.1004039.e002g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e002"/><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:mi>τ</mml:mi> <mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mi>N</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mi>N</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mrow><mml:mi>N</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:munderover> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives><label>(2)</label></disp-formula>
where <italic>f</italic>(<italic>x</italic>) is a non-saturating rectification non-linearity. Here, we take <italic>f</italic>(<italic>x</italic>) = <italic>max</italic>(<italic>x</italic>, 0), making our neurons linear threshold neurons (LTNs).</p>
<p>As long as the key parameters (<italic>α</italic><sub><italic>i</italic></sub>, <italic>β</italic><sub><italic>i</italic></sub>) satisfy rather broad constraints [<xref ref-type="bibr" rid="pcbi.1004039.ref023">23</xref>], this network behaves as a soft winner-take-all by allowing only a small number of active neurons to emerge, and that solution depends on the particular input pattern <italic>I</italic>(<italic>t</italic>). After convergence to the solution, the active neurons (winners) will express an amplified version of the input <italic>I</italic>(<italic>t</italic>), that is they remain sensitive to the input even if it changes.</p>
<p>After convergence to steady state in response to constant input <italic>I</italic><sub><italic>i</italic></sub> &gt; 0, the activation of the winner <italic>i</italic> is:
<disp-formula id="pcbi.1004039.e003"><alternatives><graphic id="pcbi.1004039.e003g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e003"/><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi>I</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>-</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mi>g</mml:mi> <mml:msub><mml:mi>I</mml:mi> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives><label>(3)</label></disp-formula>
where <inline-formula id="pcbi.1004039.e004"><mml:math id="M4" display="inline" overflow="scroll"><mml:mrow><mml:mi>g</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>−</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> is the gain of the network. Importantly, this gain can be <italic>g</italic> &gt; &gt; 1 due to excitatory recurrence, which amplifies the signal in a manner controlled by the feedback loop. While above assumes <italic>α</italic><sub>2</sub> = 0, similar arguments hold without this assumption [<xref ref-type="bibr" rid="pcbi.1004039.ref023">23</xref>].</p>
<p>The dynamics of the non-linear system are <inline-formula id="pcbi.1004039.e005"><mml:math id="M5" display="inline" overflow="scroll">
 <mml:mrow>
 <mml:mi>τ</mml:mi>
 <mml:mover accent="true">
  <mml:mi mathvariant="bold">x</mml:mi>
  <mml:mo>̇</mml:mo>
 </mml:mover>
 <mml:mo>=</mml:mo>
 <mml:mi mathvariant="bold">f</mml:mi>
 <mml:mo>(</mml:mo>
 <mml:mi mathvariant="bold">Wx</mml:mi>
 <mml:mo>+</mml:mo>
 <mml:mi mathvariant="bold">I</mml:mi>
 <mml:mo>(</mml:mo>
 <mml:mi mathvariant="bold">t</mml:mi>
 <mml:mo>)</mml:mo>
 <mml:mo>)</mml:mo>
 <mml:mo>−</mml:mo>
 <mml:mi mathvariant="bold">Gx</mml:mi>
 </mml:mrow>
</mml:math></inline-formula> (where <bold>f</bold> applies the scalar function <italic>f</italic>(<italic>x</italic>) component-wise). For example, a minimal WTA with two excitatory and one inhibitory units, <bold>x</bold> = [<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, <italic>x</italic><sub>3</sub>] (<xref ref-type="fig" rid="pcbi.1004039.g001">Fig. 1B,C</xref>), the weight matrix <bold>W</bold> is
<disp-formula id="pcbi.1004039.e006"><alternatives><graphic id="pcbi.1004039.e006g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e006"/><mml:math id="M6" display="block" overflow="scroll"><mml:mrow> <mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>W</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo> <mml:mrow> <mml:mtable> <mml:mtr> <mml:mtd> <mml:mrow> <mml:msub> <mml:mi>α</mml:mi> <mml:mn>1</mml:mn> </mml:msub> </mml:mrow> </mml:mtd> <mml:mtd> <mml:mn>0</mml:mn> </mml:mtd> <mml:mtd> <mml:mrow> <mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:msub> <mml:mi>β</mml:mi> <mml:mn>1</mml:mn> </mml:msub> </mml:mrow> </mml:mtd> </mml:mtr> <mml:mtr> <mml:mtd> <mml:mn>0</mml:mn> </mml:mtd> <mml:mtd> <mml:mrow> <mml:msub> <mml:mi>α</mml:mi> <mml:mn>1</mml:mn> </mml:msub> </mml:mrow> </mml:mtd> <mml:mtd> <mml:mrow> <mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:msub> <mml:mi>β</mml:mi> <mml:mn>1</mml:mn> </mml:msub> </mml:mrow> </mml:mtd> </mml:mtr> <mml:mtr> <mml:mtd> <mml:mrow> <mml:msub> <mml:mi>β</mml:mi> <mml:mn>2</mml:mn> </mml:msub> </mml:mrow> </mml:mtd> <mml:mtd> <mml:mrow> <mml:msub> <mml:mi>β</mml:mi> <mml:mn>2</mml:mn> </mml:msub> </mml:mrow> </mml:mtd> <mml:mtd> <mml:mn>0</mml:mn> </mml:mtd> </mml:mtr></mml:mtable></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives><label>(4)</label></disp-formula>
where <bold>G</bold> = diag(<italic>G</italic><sub>1</sub>, …, <italic>G<sub>n</sub></italic>) is a diagonal matrix containing the dissipative leak terms for each unit. The time constant of the system is <inline-formula id="pcbi.1004039.e007"><mml:math id="M7" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>τ</mml:mi> <mml:mi>G</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula>. Now we consider the conditions under which the non-linear system is guaranteed to be stable but at the same time powerful, i.e. permitting high gain.</p>
</sec>
<sec id="sec002b">
<title>Instability drives computation</title>
<p>Contraction Theory assesses the stability of non-linear systems <inline-formula id="pcbi.1004039.e037"><mml:math id="M37" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> using virtual displacements of its state at any point with respect to a chosen uniformly positive definite metric <italic>M</italic>(<italic>x</italic>, <italic>t</italic>), obtained by a transformation Θ(<italic>x</italic>, <italic>t</italic>) (see <xref ref-type="sec" rid="sec004">Methods</xref>). The key Theorem 2 of [<xref ref-type="bibr" rid="pcbi.1004039.ref017">17</xref>] asserts that if the temporal evolution of <italic>any</italic> virtual displacement in this metric space tends to zero, then <italic>all</italic> other displacement trajectories in this space will also contract (shrink) to the same (common) trajectory.</p>
<p>For our present case, this theorem implies (see <xref ref-type="sec" rid="sec004">Methods</xref>) that the trajectories of a system of neurons with Jacobian <bold>J</bold> are exponentially contracting if
<disp-formula id="pcbi.1004039.e008"><alternatives><graphic id="pcbi.1004039.e008g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e008"/><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi> <mml:mi mathvariant="bold">J</mml:mi> <mml:msup><mml:mi mathvariant="bold">Θ</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>&lt;</mml:mo> <mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:math></alternatives><label>(5)</label></disp-formula>
The Jacobian <bold>J</bold> has dimension <italic>N</italic> and describes the connection matrix of the network and <bold>Θ</bold> is a transformation matrix that provides a suitable choice of coordinates.</p>
<p>For a WTA with weight matrix <bold>W</bold>, the Jacobian is
<disp-formula id="pcbi.1004039.e009"><alternatives><graphic id="pcbi.1004039.e009g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e009"/><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">J</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>∂</mml:mi> <mml:mi mathvariant="bold">f</mml:mi></mml:mrow> <mml:mrow><mml:mi>∂</mml:mi> <mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mo>Σ</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>−</mml:mo> <mml:mi mathvariant="bold">G</mml:mi></mml:mrow></mml:math></alternatives><label>(6)</label></disp-formula>
where Σ = diag(<italic>σ</italic><sub>1</sub>, …, <italic>σ</italic><sub><italic>n</italic></sub>) is a switching matrix, whose diagonal elements <italic>σ</italic><sub><italic>i</italic></sub> ∈ 0, 1 indicate whether unit <italic>i</italic> is currently active or not.</p>
<p>Following [<xref ref-type="bibr" rid="pcbi.1004039.ref016">16</xref>] we will call the subset of N neurons that are currently above threshold the active set. Those that are inactive cannot contribute to the dynamics of the network. Thus, we may distinguish between the static anatomical connectivity <italic>W</italic> of the network, and the dynamic functional connectivity that involves only the subset of currently active neurons. This active set is described by the switch matrix Σ. The Jacobian expresses which units contribute to the dynamics at any point of time, and is thus a reflection of functional rather than anatomical connectivity. Each possible effective weight matrix has a corresponding effective Jacobian. Thus, Σ<italic>W</italic> − <bold>G</bold> is the effective (functional) Jacobian of the network, in which only active units have an influence on the dynamics.</p>
<p>The active sets may be either stable or unstable. In previous work on the hybrid analog and digital nature of processing by symmetric networks, Hahnloser et al. refer to such sets as active and forbidden, respectively [<xref ref-type="bibr" rid="pcbi.1004039.ref016">16</xref>]. They further show that the eigenvectors associated with positive eigenvalues of forbidden sets are mixed. That is, the eigenvector contains at least one component whose sign is opposite to the remainder of the components. This property ensures that a neuron will finally fall beneath threshold, and that the composition of the active set must change. We now generalize this concept to our asymmetrical networks, and refer to permitted and forbidden subspaces rather than sets, to emphasize the changes in the space in which the computational dynamics play out.</p>
<p>It is the instability (exponential divergence of neighboring trajectories) of the forbidden subspaces rather than stability that drives the computational process. This instability can be approached also through Theorem 2 of [<xref ref-type="bibr" rid="pcbi.1004039.ref017">17</xref>], which notes that if the minimal eigenvalue of the symmetric part of the Jacobian is strictly positive, then it follows that two neighboring trajectories will diverge exponentially. We will use ’expanding’ to refer to this unstable, exponentially diverging behavior of a set of neurons to avoid confusion with Gaussian divergence, which we will need to invoke in a different context, below.</p>
<p>Thus, we will say that the dynamics of a set of active neurons is expanding if
<disp-formula id="pcbi.1004039.e010"><alternatives><graphic id="pcbi.1004039.e010g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e010"/><mml:math id="M10" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">Θ</mml:mi> <mml:mi mathvariant="bold">V</mml:mi> <mml:mi mathvariant="bold">J</mml:mi> <mml:msup><mml:mi mathvariant="bold">V</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:msup><mml:mi mathvariant="bold">Θ</mml:mi> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>&gt;</mml:mo> <mml:mn mathvariant="bold">0</mml:mn></mml:mrow></mml:math></alternatives><label>(7)</label></disp-formula>
where <bold>V</bold> is a projection matrix which describes subspaces that are unstable. For example, for a circuit with two excitatory units that cannot both be simultaneously active, <bold>V</bold> is
<disp-formula id="pcbi.1004039.e011"><alternatives><graphic id="pcbi.1004039.e011g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e011"/><mml:math id="M11" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">V</mml:mi> <mml:mo>=</mml:mo> <mml:mfenced separators="" open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mi>α</mml:mi></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mrow><mml:mo>−</mml:mo> <mml:mspace width="4pt"/><mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mi>α</mml:mi></mml:mtd> <mml:mtd><mml:mrow><mml:mo>−</mml:mo> <mml:mspace width="4pt"/><mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></alternatives><label>(8)</label></disp-formula>
and <bold>Θ</bold> is a metric. The constraint (<xref ref-type="disp-formula" rid="pcbi.1004039.e010">7</xref>) asserts that the system escapes the unstable subspaces where <bold>Vx</bold> is constant. This guarantees that For <bold>V</bold> as defined above <inline-formula id="pcbi.1004039.e012"><mml:math id="M12" display="inline" overflow="scroll"><mml:mrow><mml:mtext mathvariant="bold">Vx</mml:mtext> <mml:mo>=</mml:mo> <mml:mrow><mml:mo stretchy="true">[</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mi>α</mml:mi> <mml:msub><mml:mi>x</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>−</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mn>3</mml:mn></mml:msub></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="center"><mml:mi>α</mml:mi> <mml:msub><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>−</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mn>3</mml:mn></mml:msub></mml:mtd></mml:mtr></mml:mtable> <mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Each row represents one excitatory unit. Guaranteeing that <bold>Vx</bold> cannot remain constant for a particular subset implements the requirement that for a subspace to be forbidden, it cannot be a steady state because if it were <bold>Vx</bold> would remain constant after convergence.</p>
<p>The parameter conditions under which <xref ref-type="disp-formula" rid="pcbi.1004039.e010">Eqn 7</xref> holds, are given in detail in [<xref ref-type="bibr" rid="pcbi.1004039.ref024">24</xref>]. Importantly, these conditions guarantee that when the dynamics of our asymmetric network occupies an unstable subspace, all eigenvectors are mixed (see <xref ref-type="sec" rid="sec004">Methods</xref> for proof). Consequently, as in the symmetric case of [<xref ref-type="bibr" rid="pcbi.1004039.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1004039.ref025">25</xref>], this unstable subspace will be left (it is forbidden ), because one unit will fall beneath its threshold exponentially quickly and so become inactive.</p>
</sec>
<sec id="sec002c">
<title>Divergence quenches computational instability</title>
<p>The dynamics of our asymmetric networks can now be explained in terms of the contraction theory framework outlined above. Consider a simple network consisting of <italic>N</italic> = 5 neurons, one of which is inhibitory and enforces competition through shared inhibition (<xref ref-type="fig" rid="pcbi.1004039.g002">Fig. 2A</xref>). For suitable parameters that satisfy the contraction constraints (see [<xref ref-type="bibr" rid="pcbi.1004039.ref024">24</xref>]), this network will contract towards a steady state for any input. The steady state will be such that the network amplifies one of the inputs while suppressing all others (for example, <xref ref-type="fig" rid="pcbi.1004039.g002">Fig. 2B</xref>). During this process of output selection and amplification, the network passes through a sequence of transformations, at each of which a different subset of units becomes active whereas the remainder are driven beneath threshold and so are inactive. These transformations continue while the network is in its expansion phase and cease when the network contracts. The network is in the expansion phase while the effective Jacobian is positive definite, and contracts when the Jacobian becomes negative definite (<xref ref-type="fig" rid="pcbi.1004039.g002">Fig. 2C</xref>).</p>
<fig id="pcbi.1004039.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004039.g002</object-id>
<label>Figure 2</label>
<caption>
<title>Illustration of key concepts to describe biological computation.</title>
<p>(A) Connectivity of the circuit used in this figure. (B–F) Simulation results, with parameters <italic>α</italic>1 = 1.2, <italic>β</italic><sub>1</sub> = 3, <italic>β</italic><sub>2</sub> = 0.25, <italic>G</italic><sub>5</sub> = 1.5, <italic>G</italic><sub>1..4</sub> = 1.1. (B) Activity levels (top) as a function of time for the units shown in (A) and external inputs provided (bottom). After stimulus onset at <italic>t</italic> = 2000, the network selectively amplifies the unit with the maximal input while suppressing all others. (C) Maximal and minimal eigenvalue of the functional connectivity <bold>F</bold><sub><italic>S</italic></sub> active at every point of time. At <italic>t</italic> = 5000 the system enters a permitted subspace, as indicated by the max eigenvalue becoming negative. (D) Divergence as a function of time. The divergence decreases with every subsapce transition. (E) Illustration of all possible subspaces (sets) of the network. Subspaces are ordered by their divergence. For these parameters, only subspaces with at most one unit above threshold are permitted (green) whereas the others are forbidden (red). (F) Divergence as a function of subspace number (red and green circles) as well as trajectory through set space for the simulation illustrated in (B–D). Note how the divergence decreases with each transition, except when the input changes (set 16 is the off state).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004039.g002"/>
</fig>
<p>The computational process of selecting a solution, conditional on inputs and synaptic constraints, involves testing successive subspaces, expressed as changing patterns of neuronal activation (<xref ref-type="fig" rid="pcbi.1004039.g002">Fig. 2B</xref>). Subspaces do not offer a solution (are forbidden) if for that subspace <bold>VJV</bold><sup><italic>T</italic></sup> is positive definite. In this case its dynamics are expanding, and because all eigenvectors in this subspace are mixed, the subspace will finally switch. Subspaces that offer solutions (are permitted) will have an effective Jacobian that is negative definite in some metric (they are contracting), and so the system will not leave such a subspace provided that the input remains fixed. Note that there can be subspaces whose Jacobian is neither positive nor negative definite, which are then neither permitted or forbidden. However, by definition, the networks we describe assure that each possible subspace is either permitted or forbidden.</p>
<p>Consider the case in which the computational process begins in a forbidden subspace (<xref ref-type="fig" rid="pcbi.1004039.g002">Fig. 2F</xref>). The process then passes successively through several forbidden subspaces before reaching a permitted subspace. Two properties ensure this remarkable orderly progression towards a permitted subspace. Firstly, the process is driven by the instability that ensures that forbidden subspaces are left. However the trajectory is associated with a progressive reduction in the maximum positive eigenvalue of the active set (<xref ref-type="fig" rid="pcbi.1004039.g002">Fig. 2C</xref>).</p>
<p>Secondly, an orderly progression through forbidden subspaces is ensured by systematic reduction of the state space through Gaussian divergence. Gauss’ theorem
<disp-formula id="pcbi.1004039.e013"><alternatives><graphic id="pcbi.1004039.e013g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e013"/><mml:math id="M13" display="block" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>t</mml:mi></mml:mrow></mml:mfrac> <mml:mi>δ</mml:mi> <mml:mi>V</mml:mi> <mml:mo>=</mml:mo> <mml:mi>div</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>dt</mml:mi></mml:mfrac> <mml:mi>δ</mml:mi> <mml:mi mathvariant="normal">z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="3.33333pt"/><mml:mi>δ</mml:mi> <mml:mi mathvariant="normal">V</mml:mi></mml:mrow></mml:math></alternatives><label>(9)</label></disp-formula>
asserts that, in the absence of random fluctuations, any volume element δ<italic>V</italic> shrinks exponentially to zero for uniformly negative definite <inline-formula id="pcbi.1004039.e014"><mml:math id="M14" display="inline" overflow="scroll"><mml:mrow><mml:mtext>div</mml:mtext> <mml:mo stretchy="false">(</mml:mo> <mml:mfrac><mml:mtext>d</mml:mtext> <mml:mrow><mml:mtext>dt</mml:mtext></mml:mrow></mml:mfrac> <mml:mi>δ</mml:mi> <mml:mtext>z</mml:mtext> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. This implies convergence to an (<italic>n</italic>−1) dimensional manifold rather than to a single trajectory. For our system, the Gaussian divergence is the trace of the Jacobian <bold>J</bold> or equivalently the sum of all of its eigenvalues. Note that this is a much weaker requirement than full contraction. In particular, we are concerned about the trace of the effective Jacobian, which is the Jacobian of only the active elements, because the inactive elements (those below threshold) do not contribute to the dynamics.</p>
<p>The divergence quantifies the rate at which the volume shrinks (exponential) in its <italic>n</italic> dimensional manifold towards a (<italic>n</italic>−1) dimensional manifold. The system will enter the (<italic>n</italic>−1) dimensional and continue its evolution in this new active subset, and so on, until the reduced dimensional system finally enters a permitted subspace (which is contracting). Note that here we refer to the dimensionality of the system dynamics. This dimensionality is not necessarily equal to the number of active units (i.e. at steady state or when one of the units is constant).</p>
<p>Note that negative Gaussian divergence does not follow automatically from expansion. In fact most expanding sets will not have negative Gaussian divergence, and so will not be forbidden</p>
<p>For example consider the linear system <inline-formula id="pcbi.1004039.e015"><mml:math id="M15" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mo accent="true">.</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mtext mathvariant="bold">Wx</mml:mtext></mml:mrow></mml:math></inline-formula> with <inline-formula id="pcbi.1004039.e016"><mml:math id="M16" display="inline" overflow="scroll"><mml:mrow><mml:mi>W</mml:mi> <mml:mo>=</mml:mo> <mml:mrow><mml:mo stretchy="true">[</mml:mo> <mml:mtable><mml:mtr><mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd> <mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd> <mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd> <mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd> <mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd> <mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd> <mml:mtd columnalign="center"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable> <mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. This system is expanding but it does not have negative divergence.</p>
<p>For orderly computation to proceed, such sets must not exist. We require that all forbidden subspaces are expanding as defined by contraction theory as well as have negative Gaussian divergence. Indeed, for our LTN networks all forbidden subspaces are guaranteed to satisfy both these properties.</p>
</sec>
<sec id="sec002d">
<title>Computation is steered by the rotational dynamics induced by asymmetric connections</title>
<p>Because their positive output is unbounded, linear thresholded neurons are essentially insensitive to the range of their positive inputs. Networks of these neurons amplify their inputs in a scale-free manner, according to the slope of their activation functions and the network gain induced by their connections. As explained above, this high gain drives the exploration and selection dynamics of the computational process. The network harnesses its high gain to steer the computation through its asymmetric connections. Indeed, the asymmetric nature of the connectivity in our network is central to its operation, and not a minor deviation from symmetry (approximate symmetry, [<xref ref-type="bibr" rid="pcbi.1004039.ref026">26</xref>]).</p>
<p>The interplay between high-gain and steering can be appreciated by considering the behavior of the system within one of the subspaces of its computation.</p>
<p>Consider a linear system of the form <inline-formula id="pcbi.1004039.e017"><mml:math id="M17" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mo accent="true">.</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mtext mathvariant="bold">f</mml:mtext> <mml:mo stretchy="false">(</mml:mo> <mml:mtext mathvariant="bold">x</mml:mtext> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, which can be written as <inline-formula id="pcbi.1004039.e018"><mml:math id="M18" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mo accent="true">.</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mtext mathvariant="bold">M</mml:mtext> <mml:mi>x</mml:mi> <mml:mo>+</mml:mo> <mml:mtext mathvariant="bold">u</mml:mtext></mml:mrow></mml:math></inline-formula> where <bold>M</bold> is a matrix of connection weights and <bold>u</bold> a vector of constant inputs. For the example of a WTA with 2 excitatory units and 1 inhibitory unit,
<disp-formula id="pcbi.1004039.e019"><alternatives><graphic id="pcbi.1004039.e019g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e019"/><mml:math id="M19" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">M</mml:mi> <mml:mo>=</mml:mo> <mml:mfenced separators="" open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>l</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mi>α</mml:mi> <mml:mo>−</mml:mo> <mml:mi>G</mml:mi></mml:mrow></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mrow><mml:mo>−</mml:mo> <mml:mspace width="4pt"/><mml:msub><mml:mi>l</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mrow><mml:msub><mml:mi>l</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mi>α</mml:mi> <mml:mo>−</mml:mo> <mml:mi>G</mml:mi></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>−</mml:mo> <mml:mspace width="4pt"/><mml:msub><mml:mi>l</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>l</mml:mi> <mml:mn>3</mml:mn></mml:msub> <mml:msub><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:msub><mml:mi>l</mml:mi> <mml:mn>3</mml:mn></mml:msub> <mml:msub><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>−</mml:mo> <mml:mi>G</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></alternatives><label>(10)</label></disp-formula>
Setting <bold>M</bold> = <bold>M</bold><sub>1</sub>+<bold>M</bold><sub>2</sub> and defining
<disp-formula id="pcbi.1004039.e020"><alternatives><graphic id="pcbi.1004039.e020g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e020"/><mml:math id="M20" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">M</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:mfenced separators="" open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>l</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mi>α</mml:mi> <mml:mo>−</mml:mo> <mml:mi>G</mml:mi></mml:mrow></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mrow><mml:msub><mml:mi>l</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mi>α</mml:mi> <mml:mo>−</mml:mo> <mml:mi>G</mml:mi></mml:mrow></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mrow><mml:mo>−</mml:mo> <mml:mi>G</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></alternatives><label>(11)</label></disp-formula> <disp-formula id="pcbi.1004039.e021"><alternatives><graphic id="pcbi.1004039.e021g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e021"/><mml:math id="M21" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold">M</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:mfenced separators="" open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mrow><mml:mo>−</mml:mo> <mml:mspace width="4pt"/><mml:msub><mml:mi>l</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mrow><mml:mo>−</mml:mo> <mml:mspace width="4pt"/><mml:msub><mml:mi>l</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>l</mml:mi> <mml:mn>3</mml:mn></mml:msub> <mml:msub><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:msub><mml:mi>l</mml:mi> <mml:mn>3</mml:mn></mml:msub> <mml:msub><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></alternatives><label>(12)</label></disp-formula>
provides a decomposition into a component with negative divergence and zero divergence (see <xref ref-type="sec" rid="sec004">methods</xref> for an example).</p>
<p>Any vector field <italic>f</italic>(<bold>x</bold>) can be written as the sum of a gradient field ∇<italic>V</italic>(<italic>x</italic>) and a rotational vector field <italic>ρ</italic>(<italic>x</italic>), i.e. a vector field whose divergence is zero. In analogy, <inline-formula id="pcbi.1004039.e022"><mml:math id="M22" display="inline" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi> <mml:mo stretchy="false">(</mml:mo> <mml:mtext mathvariant="bold">x</mml:mtext> <mml:mo stretchy="false">)</mml:mo> <mml:mo>=</mml:mo> <mml:mover><mml:mtext mathvariant="bold">x</mml:mtext> <mml:mo accent="true">.</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:msub><mml:mtext mathvariant="bold">M</mml:mtext> <mml:mn>1</mml:mn></mml:msub> <mml:mi>x</mml:mi> <mml:mo>+</mml:mo> <mml:mtext mathvariant="bold">u</mml:mtext> <mml:mo>+</mml:mo> <mml:msub><mml:mtext mathvariant="bold">M</mml:mtext> <mml:mn>2</mml:mn></mml:msub> <mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> where ∇<italic>V</italic>(<italic>x</italic>) = <bold>M</bold><sub>1</sub><italic>x</italic>+<bold>u</bold> and <italic>ρ</italic>(<italic>x</italic>) = <bold>M</bold><sub>2</sub><italic>x</italic>.</p>
<p>For our network <bold>M</bold><sub>1</sub> is the expansion/contraction component and <bold>M</bold><sub>2</sub> is the rotational component. This is an application of the Helmholtz decomposition theorem to our network [<xref ref-type="bibr" rid="pcbi.1004039.ref027">27</xref>, <xref ref-type="bibr" rid="pcbi.1004039.ref028">28</xref>]. These two matrices relate to two functional components in the network architecture: The excitatory recurrence plus input, and the inhibitory recurrence. The first component provides the negative divergence that defines the gradient of computation, while the second steers its direction as follows.</p>
<p>Since <bold>M</bold><sub>2</sub> has a divergence of zero, this component is rotational. If, in addition, <bold>M</bold><sub>2</sub> is skew-symmetric so that <inline-formula id="pcbi.1004039.e023"><mml:math id="M23" display="inline" overflow="scroll"><mml:mrow><mml:mo>−</mml:mo> <mml:msub><mml:mtext mathvariant="bold">M</mml:mtext> <mml:mn>2</mml:mn></mml:msub> <mml:mo>=</mml:mo> <mml:msubsup><mml:mtext mathvariant="bold">M</mml:mtext> <mml:mn>2</mml:mn> <mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, the system is rotating, and the eigenvalues of <bold>M</bold><sub>2</sub> will be imaginary only. In general, <italic>β</italic><sub>2</sub> ≠ −<italic>β</italic><sub>1</sub> and <bold>M</bold><sub>2</sub> is thus not skew-symmetric. However, note that a transform of the form <bold>Φf</bold>(<bold>x</bold>)<bold>Φ</bold><sup>−1</sup> can be found that makes <bold>M</bold><sub>2</sub> skew-symmetric. Because such transform does not change the eigenvalues of <bold>M</bold><sub>1</sub>, it will not change its divergence either. For above example,
<disp-formula id="pcbi.1004039.e024"><alternatives><graphic id="pcbi.1004039.e024g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e024"/><mml:math id="M24" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">Φ</mml:mi> <mml:mo>=</mml:mo> <mml:mfenced separators="" open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:msqrt><mml:mfrac><mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mfrac></mml:msqrt></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></alternatives><label>(13)</label></disp-formula>
which will result in a version of <bold>M</bold><sub>2</sub> that is skew-symmetric
<disp-formula id="pcbi.1004039.e025"><alternatives><graphic id="pcbi.1004039.e025g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e025"/><mml:math id="M25" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">Φ</mml:mi> <mml:msub><mml:mi mathvariant="bold">M</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msup><mml:mi mathvariant="bold">Φ</mml:mi> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>=</mml:mo> <mml:mfenced separators="" open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mrow><mml:mo>−</mml:mo> <mml:msqrt><mml:mrow><mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mrow><mml:mo>−</mml:mo> <mml:msqrt><mml:mrow><mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msqrt><mml:mrow><mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msqrt></mml:mtd> <mml:mtd><mml:msqrt><mml:mrow><mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msqrt></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></alternatives><label>(14)</label></disp-formula></p>
<p>The same transformation to a different metric has to be applied to <bold>M</bold><sub>1</sub> as well, but as both <bold>M</bold><sub>1</sub> and <bold>Φ</bold> are diagonal this will leave <bold>M</bold><sub>1</sub> unchanged, <bold>M</bold><sub>1</sub> = <bold>ΦM</bold><sub>1</sub><bold>Φ</bold><sup>−1</sup>.</p>
<p>The orderly progression through the forbidden subspaces can be understood in this framework: The negative divergence provided by <bold>M</bold><sub>1</sub> enforces an exponential reduction of any volume of the state space while the rotational component <bold>M</bold><sub>2</sub> enforces exploration of the state space by directing (steering) the dynamics.</p>
<p>The stability of the permitted subspaces can also be understood in this framework. Permitted subspaces are contracting, despite the strong self-recurrence of the excitatory elements that results in positive on-diagonal elements. This high gain, which is necessary for computation, is kept under control by a fast enough rotational component <bold>M</bold><sub>2</sub> which ensures stability. This can be seen directly by considering one of the constraints imposed by contraction analysis on valid parameters affecting an individual neuron: keeping <inline-formula id="pcbi.1004039.e026"><mml:math id="M26" display="inline" overflow="scroll"><mml:mrow><mml:mi>α</mml:mi> <mml:mo>&lt;</mml:mo> <mml:mn>2</mml:mn> <mml:msqrt><mml:mrow><mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> guarantees that the system rotates sufficiently fast to remain stable.</p>
<p>Note that both <bold>M</bold><sub>1</sub> and <bold>M</bold><sub>2</sub> change dynamically as a function of the currently active subspace. Thus, the direction and strength of the divergence and rotation change continuously as a function of both the currently active set as well as the input.</p>
</sec>
<sec id="sec002e">
<title>Changes of entropy during computation</title>
<p>While the network is in the expansion phase, the volume of the state space in which the dynamics of the network evolves is shrinking. This process depends on initial conditions and external inputs. Consequently, the sequence by which dimensions are removed from state space is sensitive to initial conditions. To quantify the behavior of a network for arbitrary initial conditions it is useful to compute its information entropy <italic>H</italic>(<italic>t</italic>) as a function of time (see <xref ref-type="sec" rid="sec004">methods</xref>). The smaller <italic>H</italic>(<italic>t</italic>), the smaller the uncertainty about which subspace the network occupies at time <italic>t</italic>. Once <italic>H</italic>(<italic>t</italic>) ceases to change, the network has converged. To reduce state uncertainty, and so to reduce entropy, is fundamentally what it means to compute [<xref ref-type="bibr" rid="pcbi.1004039.ref007">7</xref>].</p>
<p>The entropy remains 0 immediately after the application of external inputs to the network, because the same “all on” subspace is reached regardless of initial conditions. Thereafter the network begins to transition through the hierarchy of forbidden subspaces (<xref ref-type="fig" rid="pcbi.1004039.g002">Fig. 2F</xref>). This process initially increases the entropy as the network explores different subspaces. Eventually, after attaining peak entropy, the network reduces entropy as it converges towards one of its few permitted subspaces. <xref ref-type="fig" rid="pcbi.1004039.g003">Fig. 3</xref> illustrates this process for the network shown in <xref ref-type="fig" rid="pcbi.1004039.g002">Fig. 2A</xref>.</p>
<fig id="pcbi.1004039.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004039.g003</object-id>
<label>Figure 3</label>
<caption>
<title>Spontaneous increase followed by reduction of state entropy during the expansion phase.</title>
<p>Random inputs were provided to the 5-node network as shown in <xref ref-type="fig" rid="pcbi.1004039.g002">Fig. 2</xref>. Each input <italic>I<sub>i</sub></italic> was chosen i.i.d from a normal distribution with <italic>μ</italic> = 6 and <italic>σ</italic> = 0.25. (A) Entropy as a function of time and gain of the network. Higher gains are associated with faster increases and reductions of entropy but converge to the same asymptotic entropy. This indicates that each permitted subspace is reached with equal probability. (B) Adding constraints reduces the peak and asymptotic entropy. The more constraints that are added, the larger is the reduction in entropy. (C) Comparison of time-course of average entropy and divergence (trace). (D) Comparison of time-course of average entropy and eigenvalues (min, max). Notice how both the divergence (C) and the eigenvalues (D) reach their maximal values well before the entropy reaches its maximum.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004039.g003"/>
</fig>
<p>Increasing the gain by increasing the value of the self-recurrence <italic>α</italic> increases the speed by which entropy is changed but not its asymptotic value. This means that all permitted subspaces are equally likely to be the solution but that solutions are found mode quickly with higher gain. Adding additional constraints through excitatory connections makes some permitted subspaces more likely to be the solution, and so the asymptotic entropy is lower (see <xref ref-type="fig" rid="pcbi.1004039.g003">Fig. 3B</xref>, where a connection <italic>α</italic><sub>2</sub> = 0.2 from unit 1 to 2 and <italic>α</italic><sub>3</sub> = 0.2 from unit 4 to 2 was added).</p>
<p>Note how the number of constraints and the gain of the network are systematically related to both the increasing and decreasing components of <italic>H</italic>(<italic>t</italic>). For example, increasing the gain leads to a more rapid increase of entropy, reaching peak earlier and decaying faster towards the asymptotic value (<xref ref-type="fig" rid="pcbi.1004039.g003">Fig. 3A</xref>). Also, adding constraints results in smaller peak entropy, indicating that the additional constraints limited the overall complexity of the computation throughout (<xref ref-type="fig" rid="pcbi.1004039.g003">Fig. 3B</xref>).</p>
<p>The network reaches maximal entropy when there exists the largest number of forbidden subspaces having the same divergence. This occurs always for an intermediate value of divergence, because then there occurs the largest number of subspaces having an equal number of units active and inactive. This can be seen by considering <inline-formula id="pcbi.1004039.e027"><mml:math id="M27" display="inline" overflow="scroll"> <mml:mrow>
  <mml:munder>
   <mml:mrow>
    <mml:mtext>arg max</mml:mtext></mml:mrow>
   <mml:mi>k</mml:mi>
  </mml:munder>
  <mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo>
   <mml:mtable>
    <mml:mtr>
     <mml:mtd>
      <mml:mi>N</mml:mi>
     </mml:mtd>
    </mml:mtr>
    <mml:mtr>
     <mml:mtd>
      <mml:mi>k</mml:mi>
     </mml:mtd>
    </mml:mtr>
   </mml:mtable>
    <mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac>
   <mml:mi>N</mml:mi>
   <mml:mn>2</mml:mn>
  </mml:mfrac>
  </mml:mrow>
</mml:math>
</inline-formula> (if N is even), i.e. the network will have maximal entropy when the number of active units is 50%. Numerically, this can be seen by comparing the time-course of the maximal positive eigenvalue or the divergence with that of the time-course of the entropy (<xref ref-type="fig" rid="pcbi.1004039.g003">Fig. 3C,D</xref>).</p>
<p>Overall, <italic>H</italic>(<italic>t</italic>) demonstrates the dynamics of the computational process, which begins in the same state (at its extreme, all units on), and then proceeds to explore forbidden subspaces in a systematic fashion by first expanding and than contracting towards a permitted subspace.</p>
</sec>
<sec id="sec002f">
<title>Structure and steering of computation</title>
<p>We will now use the concepts introduced in the preceding sections to explain how our circuits compute and how this understanding can be utilized to systematically alter the computation through external inputs and wiring changes in the network.</p>
<p>Provided that the external input remains constant, the network proceeds in an orderly and directed fashion through a sequence of forbidden subspaces. This sequence of steps is guaranteed to not revisit subspaces already explored, because when in a forbidden subspace <italic>S</italic><sub>1</sub> with divergence <italic>d</italic><sub>1</sub>, the next subspace <italic>S</italic><sub>2</sub> that the network enters must have more negative divergence <italic>d</italic><sub>2</sub> &lt; <italic>d</italic><sub>1</sub>. It thus follows that when the system has left a subspace with divergence <italic>d</italic><sub>1</sub> that it can never return to any subspace with divergence ≥ <italic>d</italic><sub>1</sub>. It also follows that the network can only ever enter one of the many subspaces <italic>S</italic><sub><italic>i</italic></sub> with equal divergence <italic>d</italic><sub><italic>i</italic></sub> = <italic>X</italic> (<xref ref-type="fig" rid="pcbi.1004039.g002">Fig. 2F</xref> shows an example). Not all subspaces with lower <italic>d</italic><sub><italic>i</italic></sub> than the current subspace are reachable. This is because once a unit has become inactive by crossing its activation threshold, it will remain inactive. Together, this introduces a hierarchy of forbidden subspaces that the network traverses while in the exploration phase. <xref ref-type="fig" rid="pcbi.1004039.g004">Fig. 4A</xref> shows the hierarchy of the sets imposed by the network used in <xref ref-type="fig" rid="pcbi.1004039.g002">Fig. 2</xref>. This tree-like structure of subspaces constrains computation in such a way that at any point of time, only a limited number of choices can be made. As a consequence, once the network enters a certain forbidden subspace, a subset of other forbidden and permitted subspaces becomes unreachable (<xref ref-type="fig" rid="pcbi.1004039.g004">Fig. 4B</xref>). What those choices are depends on the input whereas the tree-like structure of the subspaces is given by the network connectivity.</p>
<fig id="pcbi.1004039.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004039.g004</object-id>
<label>Figure 4</label>
<caption>
<title>Hierarchy of subspaces and steering of computation.</title>
<p>(A) 5-node network, using notation from <xref ref-type="fig" rid="pcbi.1004039.g002">Fig. 2</xref>. (B) Simulation of an individual run. Top shows the state and bottom the inputs to the network. The unit with the maximal input wins (unit 4). (C) Trajectory through state space for the simulation shown in (B). Each subspace is numbered and plotted as a function of its divergence. Red and green dots indicate forbidden and permitted subspaces respectively. Numbers inside the dots are the subspace (set) numbers (see <xref ref-type="fig" rid="pcbi.1004039.g002">Fig. 2E</xref>). (C–D) Transition probabilities for the same network, simulated with 1000 different random inputs. Connected subspaces are subsets between which the network can transition, in the direction that reduces divergence (gray lines). The size of dots and lines indicates the likelihood that a subset will be visited or a transition executed, respectively. The subset with most negative divergence is the zero set (all units off), which is not shown. (C) All transitions made by the network. Depending on the value of the input, the network will reach one of the permitted subspaces. Notice the strict hierarchy: all transitions were towards subspaces with lower divergence and only a subset of all possible transitions are possible. (D) All transitions the network made, conditional on that subspace 13 was the solution. Note that this subspace cannot be reached from some subspaces (such as nr 7).</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004039.g004"/>
</fig>
<p>Knowledge of how the network transitions through the hierarchy of forbidden subspaces can be used to systematically introduce biases into the computational process. Such additional steering of the computation can be achieved by adding connections in the network. Additional off-diagonal excitatory connections, for example, will make it more likely that a certain configuration is the eventual winner. An example of this effect is shown in <xref ref-type="fig" rid="pcbi.1004039.g005">Fig. 5</xref>, where adding two additional excitatory connections results in the network being more likely to arrive in a given permitted subspace than others. For identical inputs (compare Figs. <xref ref-type="fig" rid="pcbi.1004039.g005">5B</xref> and <xref ref-type="fig" rid="pcbi.1004039.g004">4B</xref>) the resulting permitted subspace can be different through such steering.</p>
<fig id="pcbi.1004039.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004039.g005</object-id>
<label>Figure 5</label>
<caption>
<title>Steering of computation.</title>
<p>(A) The same network as shown in <xref ref-type="fig" rid="pcbi.1004039.g004">Fig. 4A</xref>, with two additional excitatory connections <italic>α</italic><sub>2</sub> = 0.2 and <italic>α</italic><sub>3</sub> = 0.2. (B) Simulation of an individual run. Top shows the state and bottom the inputs to the network. Although the input is identical to <xref ref-type="fig" rid="pcbi.1004039.g004">Fig. 4B</xref>, the computation proceeds differently due to presence of the additional connections. (C) Trajectory through state space for the simulation shown in (C). The state space trajectory is initially identical to the simulation shown in (<xref ref-type="fig" rid="pcbi.1004039.g004">Fig. 4C</xref>), but then diverges. (D) All transitions made by the network. The transition probabilities are now biased and non-equal. Consequently, some permitted subspaces are reached more frequently than others.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004039.g005"/>
</fig>
<p>The network remains continuously sensitive to changes in the external input. This is important and can be used to steer the computation without changing the structure of the network. In the absence of changes in the external input, the network is unable to make transitions other than those which lead to subspaces with lower divergence. When the input changes, on the other hand, the network can make such changes. For example, if the inputs change as a consequence of the network entering a certain forbidden subspace, the network can selectively avoid making certain transitions (<xref ref-type="fig" rid="pcbi.1004039.g006">Fig. 6A</xref>). This will steer the computation such that some permitted subspaces are reached with higher likelihood. Noisy inputs similarly can lead to transitions which make divergence less negative. Neverthless, the large majority of transitions remains negative as long as noise levels are not too large. For example, repeating the same simulation but adding normally distributed noise with <italic>σ</italic> = 1 and <italic>μ</italic> = 0 resulted in 26% of transitions being against the gradient (see <xref ref-type="fig" rid="pcbi.1004039.g006">Fig. 6B</xref> for an illustration).</p>
<fig id="pcbi.1004039.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004039.g006</object-id>
<label>Figure 6</label>
<caption>
<title>Steering of computation by external inputs.</title>
<p>The network remains sensitive to changes in its input throughout the computation. Transitions are bi-directional: towards more negative and positive divergence are gray and green, respectively. (A) Example of a selective change in the input. Here, unit 4 receives additional external input only if the network enters forbidden subspace 6 (see inset). This change forces the network to make a transition to a subspace with less negative divergence (green). As a result, the eventual solution is more likely to be subspace 15. (B) Example of continuously changing inputs (input noise). Notice how noise introduces additional transitions towards less negative divergence at all stages. However, the general gradient towards more negative divergence persists: overall, 26% of transitions made the divergence more positive.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004039.g006"/>
</fig>
<p>So far, we have illustrated how negative divergence and expansion jointly drive the computational process in a simple circuit of multiple excitatory neurons that have common inhibition. While this circuit alone is already capable of performing sophisticated computation,many computations require that several circuits interact with one another [<xref ref-type="bibr" rid="pcbi.1004039.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1004039.ref029">29</xref>] The concepts developed in this paper can also be applied to such compound circuits, because the circuit motifs and parameter bounds we describe guarantee that collections of these circuits will also possess forbidden and permitted subspaces and are thus also computing. The compound network is guaranteed to be dynamically bounded, which means that no neuron’s activity can escape towards infinity. This property of the collective system relies on two key aspects: i) collections of individual circuits with negative divergence also have negative divergence, and ii) collective stability [<xref ref-type="bibr" rid="pcbi.1004039.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1004039.ref030">30</xref>]. Together, these properties guarantee that collections of the motifs will compute automatically.</p>
<p>Consider a network assembled by randomly placing instances of the two circuit motifs on a 2D plane and connecting them to each other probabilistically (<xref ref-type="fig" rid="pcbi.1004039.g007">Fig. 7A,B</xref> and <xref ref-type="sec" rid="sec004">methods</xref>). This random configuration results in some excitatory elements sharing inhibition via only one inhibitory motif, whereas others take part in many inhibitory feedback loops (<xref ref-type="fig" rid="pcbi.1004039.g007">Fig. 7B</xref>). This random circuit will compute spontaneously (<xref ref-type="fig" rid="pcbi.1004039.g007">Fig. 7C,D</xref>). It is not known a priori how many forbidden and permitted subspaces the network has, nor how many possible solutions it can reach. Nevertheless, it is guaranteed that the network will reduce entropy and eventually reach a permitted subspace (<xref ref-type="fig" rid="pcbi.1004039.g007">Fig. 7E</xref>). The more connections (constraints) that are added to the network the smaller the number of permitted subspaces, and generally the harder the computation will become. How long the computation will take to reach a permitted subspace depends on both the network size, and the number of connections (constraints). Generally, the smaller the number of permitted subspaces the harder the computation will be. The important point is that random instances of such circuits will always compute, which means they will always reach a permitted subspace (<xref ref-type="fig" rid="pcbi.1004039.g007">Fig. 7F</xref>).</p>
<fig id="pcbi.1004039.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pcbi.1004039.g007</object-id>
<label>Figure 7</label>
<caption>
<title>Large random networks spontaneously compute.</title>
<p>(A) Each site in a 10×10 grid is either empty or occupied by an excitatory (green) or inhibitory (red) neurons (probabilistically). (B) Connectivity matrix. (C) Example run. (D) Entropy and divergence as a function of time for random initial conditions. (E) The number of different subspaces visited reduces as a function of time similarly to the entropy. (F) Time ±<italic>s.d</italic>. till the network first enters a permitted subspace as a function of network size, specified as grid with. 100 runs of randomly assembled networks were simulated for each size. The larger the network, the longer the network spent going through the forbidden subspaces. Panels A–E show an example of grid size 10. See <xref ref-type="sec" rid="sec004">Methods</xref> for simulation details.</p>
</caption>
<graphic mimetype="image" xlink:type="simple" position="float" xlink:href="info:doi/10.1371/journal.pcbi.1004039.g007"/>
</fig>
</sec>
</sec>
<sec id="sec003" sec-type="conclusions">
<title>Discussion</title>
<p>The contribution of this paper has been to explore the fundamental role of instability in driving computation in networks of linear threshold units. Previous studies of computation in neural networks have focused on networks of sigmoidal units with symmetrical connectivity. Our networks of asymetrically connected LTNs draw attention to important features of computation that were not apparent in these previous models. The conditional selective behavior crucial for computation depends on the threshold nonlinearity of the LTN. However, in order to make use of these non-linearities the network must express substantial gain. Because the activation of LTNs is unbounded for positive inputs, the network can in principle produce very high activations through unstably high gain. In these networks, computation is expressed as passage through a sequence of unstable states. It is this dynamical trajectory by which the network computes [<xref ref-type="bibr" rid="pcbi.1004039.ref001">1</xref>, <xref ref-type="bibr" rid="pcbi.1004039.ref002">2</xref>, <xref ref-type="bibr" rid="pcbi.1004039.ref031">31</xref>]. Despite this essential instability, the system does not escape, but remains bounded in its behavior. In this paper we have analyzed why this is so. We find that the instabilities are self limiting, and that the overall process of computation is systematically quenched by Gaussian divergence. Contraction analysis provides explicit tools to quantify both instantaneous rates of exponential convergence to limiting states or trajectories, and divergence rates from specific subspaces. Here, we use these tools to analyze the unstable phase of the dynamics. This phase is crucial, because computation is inseparable from instability. Here we have made steps towards characterizing and explaining these phenomena.</p>
<p>The type of dynamical system we consider can implement soft-WTA type behavior, amongst others. This makes our framework applicable to the extensive body of literature on this type of network [<xref ref-type="bibr" rid="pcbi.1004039.ref032">32</xref>–<xref ref-type="bibr" rid="pcbi.1004039.ref038">38</xref>]. While simple, the soft-WTA is a powerful computational primitive that offers the same computational power than a multi-layer perceptron [<xref ref-type="bibr" rid="pcbi.1004039.ref032">32</xref>]. Key aspects of what makes WTA-networks powerful are high network gain, which allows computations that require sparsification, and also provides stability. While the computational power of WTAs has long been recognized and exploited to implement simple cognitive behaviors [<xref ref-type="bibr" rid="pcbi.1004039.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1004039.ref029">29</xref>, <xref ref-type="bibr" rid="pcbi.1004039.ref039">39</xref>], it has remained unclear what it means to compute in such networks. Here, we provide such understanding in terms of a dynamical system. This system is physically realizable by realistic neurons and their connections. Other work in this direction has focused on abstract mathematic models [<xref ref-type="bibr" rid="pcbi.1004039.ref036">36</xref>, <xref ref-type="bibr" rid="pcbi.1004039.ref040">40</xref>–<xref ref-type="bibr" rid="pcbi.1004039.ref043">43</xref>], and less on physically realizable dynamical computation. More recently, others [<xref ref-type="bibr" rid="pcbi.1004039.ref044">44</xref>–<xref ref-type="bibr" rid="pcbi.1004039.ref046">46</xref>] have offered useful models for understanding the principles whereby the brain may attain cognition, but these approaches do not offer methods for implementing such algorithms as physical computation in neuronal circuits.</p>
<p>The advances of this paper can be seen in contrast with classical assumptions concerning the form of activation functions, continuous sensitivity to input, and symmetry of connections. For example, the behavior of our LTN networks can be contrasted with networks of the kind originally proposed by Hopfield [<xref ref-type="bibr" rid="pcbi.1004039.ref020">20</xref>] that allow no self-connections (<italic>w</italic><sub><italic>ii</italic></sub> = 0, ∀<italic>i</italic>), have symmetric connectivity (<italic>w</italic><sub><italic>ij</italic></sub> = <italic>w</italic><sub><italic>ji</italic></sub>), and their activation function is bounded on both sides. This guarantees bounded dynamics by construction, allowing such networks to express high gain by virtue of a steep activation function rather than through connections of the network. However, a consequence of this is that when it operates with high gain the network operates in saturation and thus becomes insensitive to input apart from initial conditions. Such networks have neither negative divergence nor rotational dynamics, which together with insensitivity to external input severely restricts their computational abilities as well as systematic design.</p>
<p>Importantly, our networks are continuously sensitive to their inputs. These external inputs are a combination of signal and noise and can transfer the network from one subspace to an other at any point of time and this transfer can be against the gradient imposed by negative divergence. Non-autonomous systems continuously interact with their environment, for which continuous sensitivity to input is crucial. Systems of asymmetrically interacting linear threshold units are well suited for this situation. This is because their non-saturating units make the system adaptive to the input amplitudes and sensitivity to inputs is conditional on the current state, i.e. only the inputs contributing to the dynamics of the currently active state influence the dynamics.</p>
<p>Although there has been a considerable amount of work on symmetric networks, biological neuronal networks are always asymmetric because of inhibitory neurons. Also, the inhibitory inputs to an excitatory neuron can be substantially stronger than the excitatory inputs. This results in particularly strong asymmetry, a property with many implications for computation in such networks [<xref ref-type="bibr" rid="pcbi.1004039.ref047">47</xref>]. The theoretical study of networks with defined cell types (excitatory or inhibitory) thus requires asymmetric connectivity. Previous studies have used infinitely fast all-to-all inhibition to circumvent this problem, which results in symmetric connectivity but lacks defined cell types. Such networks allow dynamically bounded activity for linear threshold units [<xref ref-type="bibr" rid="pcbi.1004039.ref016">16</xref>, <xref ref-type="bibr" rid="pcbi.1004039.ref048">48</xref>]. Apart from being biologically unrealistic, such networks can only express limited gain and are thus computationally very limited [<xref ref-type="bibr" rid="pcbi.1004039.ref047">47</xref>]. By contrast, our networks express high gain and dynamic instabilities during the exploration phase. Their asymmetric connections provide the rotational dynamics that keep their activity bounded despite this high gain. It is worth noting that many powerful algorithms, such as e.g. the Kalman filter [<xref ref-type="bibr" rid="pcbi.1004039.ref049">49</xref>, <xref ref-type="bibr" rid="pcbi.1004039.ref050">50</xref>] also rely on negative feedback and strongly asymmetric connectivity.</p>
<p>The dynamics of the exploration phase are highly structured because the different forbidden subspaces are systematically related to one another. Indeed, the subspaces are ordered in a hierarchy through which the dynamics proceed. At any point in this hierarchy only a limited and known set of subspaces can be entered next (unless the external input changes). The systematic understanding of the unstable dynamics driving exploration can be used to steer and modify the computational trajectory while it is in process, rather than only when a solution has been found. The network can influence its environment continuously as a function of the forbidden subspaces it traverses, for example by executing a specific action whenever a particular subspace is entered. This feature can be used to make the computations of several networks dependent on each other. For example. to enforce dependencies between several ongoing computations such as, “all solutions must be different”.</p>
<p>The connections of the network are the constraints imposed on the computation. The more connections per neuron, the fewer possible solutions exist and the harder (slower) the computation is. From this point of view, the networks we describe perform constraint satisfaction, which is a hard computational problem and which has been proposed as an abstract model of computation [<xref ref-type="bibr" rid="pcbi.1004039.ref051">51</xref>, <xref ref-type="bibr" rid="pcbi.1004039.ref052">52</xref>]. Connections can be inserted systematically to forward program specific algorithms and behaviors [<xref ref-type="bibr" rid="pcbi.1004039.ref023">23</xref>, <xref ref-type="bibr" rid="pcbi.1004039.ref024">24</xref>, <xref ref-type="bibr" rid="pcbi.1004039.ref029">29</xref>], randomly or a combination thereof. Either way, the system will compute [<xref ref-type="bibr" rid="pcbi.1004039.ref053">53</xref>], but in the former case will execute specific algorithms while in the later the algorithm is unknown.</p>
<p>The constraints active at any point of time depend on the state of the network as expressed by the effective connectivity of the network expressed by the switching matrix. Every time the network changes state, the switching matrix changes. Dynamically, the same concept can be applied: the effective Jacobian jointly expresses all the currently activity constraints for a given state. Only if the possible state(s) of a network are known is it possible to determine the effective Jacobian. An important implication is that to understand the underlying algorithm that drives the computation performed by a group of neurons knowledge of the structural connectivity is not sufficient [<xref ref-type="bibr" rid="pcbi.1004039.ref054">54</xref>–<xref ref-type="bibr" rid="pcbi.1004039.ref056">56</xref>]. This is because connectivity alone does not determine the possible states of the network.</p>
<p>The circuit motifs and parameter bounds we describe guarantee that collections of these circuits will also possess forbidden and permitted subspaces and thus are computing. By collections we mean multiple copies of the same motifs that are in addition connected to each other, as for example in the random network discussed in the results. This is important because collections of these motifs will compute automatically, a property we refer to as collective computation. This makes it practical to design large-scale computing systems without having to perform global analysis to guarantee both the type of instability required for computation as well as stability of the solutions.</p>
<p>It is important to note that one need not commit to a particular circuit motif beyond guaranteeing that both forbidden and permitted subspaces exist in the way we define them. While a network composed of such motifs but otherwise connected randomly will always compute, the individual states do not have meaning nor is the algorithm that the network computes known. However, the states that the network proceeds through while it computes are systematically related to each other. Consequently, assigning a meaningful interpretation to a few key states will make all states meaningful. A similar approach is used in reservoir computing, where states are first created and only later assigned with meaning by learning mechanisms [<xref ref-type="bibr" rid="pcbi.1004039.ref057">57</xref>]. A key next step will be to discover how linking specific forbidden subspaces with motor actions that in turn change the input to the system allow a computation to remain continuously sensitive to the environment while it proceeds. An other next step is to discover how several interacting systems can bias each others computations systematically to reach a solution that is agreeable to all while satisfying the local constraints of each computation.</p>
<p>The unstable positive feedback and cross inhibition that underly expansion and divergence are common motifs found in many molecular, cellular and neuronal networks [<xref ref-type="bibr" rid="pcbi.1004039.ref058">58</xref>]. Therefore all such systems follow the very simple organizational constraints that combine these motifs. This will lead such circuits to compute spontaneously and thereby to reduce their state entropy as is characteristic of living systems.</p>
</sec>
<sec id="sec004" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec004a">
<title>Numerical methods</title>
<p>All simulations were performed with Euler integration with δ = 0.01, thus <italic>τ</italic> is equivalent to 100 integration steps. All times in the figures and text refer to numerical integration steps. Unless noted otherwise, the external inputs <italic>I</italic><sub><italic>i</italic></sub>(<italic>t</italic>) were set to 0 at <italic>t</italic> = 0 and then to a constant non-zero value at <italic>t</italic> = 2000. The constant value <italic>I</italic><sub><italic>i</italic></sub> was drawn randomly and i.i.d. from <inline-formula id="pcbi.1004039.e035"><mml:math id="M35" display="inline" overflow="scroll"><mml:mrow>
<mml:mi mathvariant="script">N</mml:mi>
  <mml:mo>(</mml:mo>
  <mml:mn>6</mml:mn>
  <mml:mo>,</mml:mo>
  <mml:mn>1</mml:mn>
  <mml:mi>σ</mml:mi>
  <mml:mo>)</mml:mo>
 </mml:mrow>
</mml:math></inline-formula>. All simulations were implemented in MATLAB.</p>
<p>In cases where noise was added, the noise was supplied as an additional external input term <inline-formula id="pcbi.1004039.e036"><mml:math id="M36" display="inline" overflow="scroll">
 <mml:mrow>
  <mml:mi mathvariant="script">N</mml:mi>
  <mml:mo>(</mml:mo>
  <mml:mn>0</mml:mn>
  <mml:mo>,</mml:mo>
  <mml:mi>σ</mml:mi>
  <mml:mo>)</mml:mo>
 </mml:mrow>
</mml:math></inline-formula> with <italic>σ</italic> = 1.
<disp-formula id="pcbi.1004039.e028"><alternatives><graphic id="pcbi.1004039.e028g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e028"/><mml:math id="M28" display="block" overflow="scroll"><mml:mrow><mml:mi>τ</mml:mi> <mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mi>f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>I</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mi>σ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msub> <mml:mo>−</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mi>N</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives><label>(15)</label></disp-formula>
A new noise sample is drawn from the random variable every <italic>τ</italic> to avoid numerical integration problems.</p>
<sec id="sec004aa">
<title>Large network simulation</title>
<p>A 2D square of dimension 10 by 10 spots is created. Each spot is occupied by a neuron with <italic>P</italic> = 0.4. If a spot is occupied, it is excitatory with <italic>P</italic> = 0.8 and inhibitiory otherwise. Connectivity between units is generated with the following constraints: All excitatory units receive excitation from themselfs with <italic>α</italic><sub>1</sub>. Further, each excitatory unit receives inhibiton from and excites up to 8 randomly chosen inhibitory neurons with probability <italic>P</italic> = 0.4. It is very unlikly that an excitatory unit is not connected to any inhibitory neuron at all (<italic>P</italic> = (0.6)<sup>8</sup>).</p>
</sec>
</sec>
<sec id="sec004b">
<title>Entropy</title>
<p>The information entropy <italic>H</italic>(<italic>t</italic>) is
<disp-formula id="pcbi.1004039.e029"><alternatives><graphic id="pcbi.1004039.e029g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e029"/><mml:math id="M29" display="block" overflow="scroll"><mml:mrow><mml:mi>H</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo>−</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mi>i</mml:mi></mml:munder> <mml:msub><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>l</mml:mi> <mml:mi>o</mml:mi> <mml:msub><mml:mi>g</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>p</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives><label>(16)</label></disp-formula></p>
<p>The sum is over all subspaces <italic>i</italic> and <italic>p</italic><sub><italic>i</italic></sub>(<italic>t</italic>) is the probability of the network being in subspace <italic>i</italic> at time <italic>t</italic>, over random initial conditions (we used 1000 runs). By definition, we take 0<italic>log</italic><sub>2</sub>(0) = 0.</p>
</sec>
<sec id="sec004c">
<title>Analytical methods</title>
<p>The principal analytical tool used is contraction analysis [<xref ref-type="bibr" rid="pcbi.1004039.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1004039.ref059">59</xref>–<xref ref-type="bibr" rid="pcbi.1004039.ref061">61</xref>]. In this section, we briefly summarize the application of contraction analysis to analyzing asymmetric dynamically bounded networks [<xref ref-type="bibr" rid="pcbi.1004039.ref024">24</xref>]. Essentially, a nonlinear time-varying dynamic system will be called <italic>contracting</italic> if arbitrary initial conditions or temporary disturbances are forgotten exponentially fast, i.e., if trajectories of the perturbed system return to their unperturbed behavior with an exponential convergence rate. Relatively simple algebraic conditions can be given for this stability-like property to be verified, and this property is preserved through basic system combinations and aggregations.</p>
<p>A nonlinear contracting system has the following properties [<xref ref-type="bibr" rid="pcbi.1004039.ref017">17</xref>, <xref ref-type="bibr" rid="pcbi.1004039.ref059">59</xref>–<xref ref-type="bibr" rid="pcbi.1004039.ref061">61</xref>]
<list list-type="bullet"><list-item><p>global exponential convergence and stability are guaranteed</p></list-item> <list-item><p>convergence rates can be explicitly computed as eigenvalues of well-defined Hermitian matrices</p></list-item> <list-item><p>combinations and aggregations of contracting systems are also contracting</p></list-item> <list-item><p>robustness to variations in dynamics can be easily quantified</p></list-item></list></p>
<p>Consider now a general dynamical system in ℝ<sup><italic>n</italic></sup>,
<disp-formula id="pcbi.1004039.e030"><alternatives><graphic id="pcbi.1004039.e030g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e030"/><mml:math id="M30" display="block" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">x</mml:mi> <mml:mo>˙</mml:mo></mml:mover> <mml:mo>=</mml:mo> <mml:mi mathvariant="bold">f</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo>,</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives><label>(17)</label></disp-formula>
with <bold>f</bold> a smooth non-linear function. The central result of Contraction Analysis, derived in [<xref ref-type="bibr" rid="pcbi.1004039.ref017">17</xref>] in both real and complex forms, can be stated as:</p>
<p><bold>Theorem</bold> Denote by <inline-formula id="pcbi.1004039.e031"><mml:math id="M31" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo> <mml:mtext mathvariant="bold">f</mml:mtext></mml:mrow> <mml:mrow><mml:mo>∂</mml:mo> <mml:mtext mathvariant="bold">x</mml:mtext></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> the Jacobian matrix of <bold>f</bold> with respect to <bold>x</bold>. Assume that there exists a complex square matrix <bold>Θ</bold>(<italic>x</italic>, <italic>t</italic>) such that the Hermitian matrix <bold>Θ</bold>(<italic>x</italic>, <italic>t</italic>)<sup>*<italic>T</italic></sup> <bold>Θ</bold>(<italic>x</italic>, <italic>t</italic>) is uniformly positive definite, and the Hermitian part <bold>F</bold><sub><italic>H</italic></sub> of the matrix
<disp-formula id="pcbi.1004039.e032"><alternatives><graphic id="pcbi.1004039.e032g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e032"/><mml:math id="M32" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">F</mml:mi> <mml:mo>=</mml:mo> <mml:mrow><mml:mo stretchy="true">(</mml:mo> <mml:mrow><mml:mover accent="true"><mml:mo>Θ</mml:mo> <mml:mo>̇</mml:mo></mml:mover> <mml:mo>+</mml:mo> <mml:mo>Θ</mml:mo> <mml:mfrac><mml:mrow><mml:mo>∂</mml:mo> <mml:mtext>f</mml:mtext></mml:mrow> <mml:mrow><mml:mo>∂</mml:mo> <mml:mtext>x</mml:mtext></mml:mrow></mml:mfrac></mml:mrow> <mml:mo stretchy="true">)</mml:mo></mml:mrow> <mml:msup><mml:mo>Θ</mml:mo> <mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></disp-formula>
is uniformly negative definite. Then, all system trajectories converge exponentially to a single trajectory, with convergence rate |sup<sub><bold>x</bold>, <italic>t</italic></sub>λ<sub>max</sub>(<bold>F</bold><sub><italic>H</italic></sub>)| &gt; 0. The system is said to be <italic>contracting</italic>, <bold>F</bold> is called its <italic>generalized Jacobian</italic>, and <bold>Θ</bold>(<italic>x</italic>, <italic>t</italic>)<sup>*<italic>T</italic></sup> <bold>Θ</bold>(<italic>x</italic>, <italic>t</italic>) its contraction <italic>metric</italic>. The contraction rate is the absolute value of the largest eigenvalue (closest to zero, although still negative) <italic>λ</italic> = ∣<italic>λ</italic><sub><italic>max</italic></sub>(<bold>F</bold><sub><italic>H</italic></sub>)∣.</p>
<p>In the linear time-invariant case, a system is globally contracting if and only if it is strictly stable, and <bold>F</bold> can be chosen as a normal Jordan form of the system, with <bold>Θ</bold> a real matrix defining the coordinate transformation to that form [<xref ref-type="bibr" rid="pcbi.1004039.ref017">17</xref>]. Alternatively, if the system is diagonalizable, <bold>F</bold> can be chosen as the diagonal form of the system, with <bold>Θ</bold> a complex matrix diagonalizing the system. In that case, <bold>F</bold><sub><italic>H</italic></sub> is a diagonal matrix composed of the real parts of the eigenvalues of the original system matrix. Here, we choose Θ = <bold>Q</bold><sup>−1</sup> where <bold>Q</bold> is defined based on the eigendecomposition <bold>J</bold> = <bold>QΛQ</bold><sup>−1</sup>.</p>
<p>The methods of Contraction Analysis were crucial for our study for the following reasons: i) Contraction and divergence rates are exponential guarantees rather than asymptotic (note that the more familiar Lyapunov exponents can be viewed as the average over infinite time of the instantaneous contraction rates in an identity metric). ii) No energy function is required. Instead, the analysis depends on a metric <bold>Θ</bold> that can be identified for a large class of networks using the approach outlined. iii) The analysis is applicable to non-autonomous systems with constantly changing inputs.</p>
<p>The boundary conditions for a WTA-type network to be contracting as well as to move exponentially away from non-permitted configurations were derived in detail in [<xref ref-type="bibr" rid="pcbi.1004039.ref024">24</xref>]. They are:
<disp-formula id="pcbi.1004039.e033"><alternatives><graphic id="pcbi.1004039.e033g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e033"/><mml:math id="M33" display="block" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mn>1</mml:mn> <mml:mo>&lt;</mml:mo> <mml:mi>α</mml:mi> <mml:mo>&lt;</mml:mo> <mml:mn>2</mml:mn> <mml:msqrt><mml:mrow><mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mn>4</mml:mn></mml:mfrac> <mml:mo>&lt;</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>β</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>&lt;</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(18)</label></disp-formula></p>
<sec id="sec004ca">
<title>Forbidden subspaces have mixed eigenvalues</title>
<p>When the system resides in a forbidden subspace, the maximal eigenvalue of the effective Jacobian is positive and its divergence is negative. A forbidden subspace thus has a mixed eigenvector <bold>v</bold> associated with the maximal eigenvalue <italic>λ</italic><sub><italic>max</italic></sub>. A mixed eigenvector is one where at least one entry is strictly negative and at least one strictly positive.</p>
<p>Proof by contradiction: Assume all components <italic>v</italic><sub><italic>i</italic></sub> ≥ 0. Consider a constant external input where the system starts along one of the eigenvectors <bold>v</bold>. In this case, the state of the network is determined entirely by this eigenvector and would grow towards infinity without ever changing dimensionality. However, the conditions for a forbidden subspace do not permit this because <bold>Vx</bold> = 0 ensures that the system escapes this subspace exponentially fast (see Eqs <xref ref-type="disp-formula" rid="pcbi.1004039.e010">7</xref>–<xref ref-type="disp-formula" rid="pcbi.1004039.e011">8</xref>). Thus in a forbidden subspace, by definition, the state cannot grow towards infinity and the eigenvector <bold>v</bold> must be mixed.</p>
</sec>
<sec id="sec004cb">
<title>Rotation versus expansion—Example</title>
<p>This example illustrates the role of rotation.
<disp-formula id="pcbi.1004039.e034"><alternatives><graphic id="pcbi.1004039.e034g" mimetype="image" xlink:type="simple" position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1004039.e034"/><mml:math id="M34" display="block" overflow="scroll"><mml:mrow><mml:mfenced separators="" open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mi>a</mml:mi></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo> <mml:mi>a</mml:mi></mml:mrow></mml:mtd> <mml:mtd><mml:mrow><mml:mo>−</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced> <mml:mo>=</mml:mo> <mml:mfenced separators="" open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mrow><mml:mo>−</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mfenced> <mml:mo>+</mml:mo> <mml:mi>a</mml:mi> <mml:mfenced separators="" open="[" close="]"><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd> <mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mtd> <mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:mrow></mml:math></alternatives><label>(19)</label></disp-formula></p>
<p>In the above, divergence is −1. Whether the system is linearly stable depends on the value of a. If the determinant −2 + <italic>a</italic><sup>2</sup> &gt; 0, this system is stable. Thus the system is stable if <italic>a</italic><sup>2</sup> &gt; 2, i.e. if the rotation is “fast enough”.</p>
</sec>
</sec>
</sec>
</body>
<back>
<ack>
<p>We thank the participants of the Capo Caccia Cognitive Neuromorphic Engineering workshops of 2009–2014 for discussion.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pcbi.1004039.ref001">
<label>1</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Turing</surname> <given-names>AM</given-names></name> (<year>1936</year>) <article-title>On computable numbers, with an application to the entscheidungsproblem</article-title>. <source>J of Math</source> <volume>58</volume>: <fpage>345</fpage>–<lpage>363</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref002">
<label>2</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Minksy</surname> <given-names>L</given-names></name> (<year>1967</year>) <source>Computation: finite and infinite machines</source>. <publisher-loc>Englewood Cliffs</publisher-loc>: <publisher-name>Prentice Hall</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref003">
<label>3</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Gödel</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Kleene</surname> <given-names>SC</given-names></name>, <name name-style="western"><surname>Rosser</surname> <given-names>JB</given-names></name> (<year>1934</year>) <source>On undecidable propositions of formal mathematical systems</source>. <publisher-name>Institute for Advanced Study</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref004">
<label>4</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Mitchell</surname> <given-names>M</given-names></name> (<year>2011</year>) <source>Ubiquity symposium: Biological computation</source>. <publisher-name>Ubiquity</publisher-name> <comment>2011</comment>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref005">
<label>5</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Rabinovich</surname> <given-names>MI</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Varona</surname> <given-names>P</given-names></name> (<year>2012</year>) <source>Principles of brain dynamics: global state interactions</source>. <publisher-name>MIT Press</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref006">
<label>6</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Simon</surname> <given-names>HA</given-names></name> (<year>1991</year>) <source>The architecture of complexity</source>. <publisher-name>Springer</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref007">
<label>7</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Feynman</surname> <given-names>RP</given-names></name>, <name name-style="western"><surname>Hey</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Allen</surname> <given-names>RW</given-names></name> (<year>1998</year>) <source>Feynman lectures on computation</source>. <publisher-name>Addison-Wesley Longman Publishing Co., Inc</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref008">
<label>8</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Cohen</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Grossberg</surname> <given-names>S</given-names></name> (<year>1983</year>) <article-title>Absolute stability of global pattern-formation and parallel memory storage by competitive neural networks</article-title>. <source>IEEE Transactions on Systems Man and Cybernetics</source> <volume>13</volume>: <fpage>815</fpage>–<lpage>826</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TSMC.1983.6313075" xlink:type="simple">10.1109/TSMC.1983.6313075</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref009">
<label>9</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hopfield</surname> <given-names>JJ</given-names></name> (<year>1984</year>) <article-title>Neurons with graded response have collective computational properties like those of two-state neurons</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>81</volume>: <fpage>3088</fpage>–<lpage>3092</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.81.10.3088" xlink:type="simple">10.1073/pnas.81.10.3088</ext-link></comment> <object-id pub-id-type="pmid">6587342</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref010">
<label>10</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hopfield</surname> <given-names>JJ</given-names></name> (<year>1982</year>) <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>79</volume>: <fpage>2554</fpage>–<lpage>2558</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.79.8.2554" xlink:type="simple">10.1073/pnas.79.8.2554</ext-link></comment> <object-id pub-id-type="pmid">6953413</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref011">
<label>11</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Hertz</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Krogh</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Palmer</surname> <given-names>R</given-names></name> (<year>1991</year>) <source>Introduction to the Theory of Neural Computation</source>. <publisher-loc>Redwood City,CA</publisher-loc>: <publisher-name>Addison-Wesley</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref012">
<label>12</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Seung</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Richardson</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Lagarias</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Hopfield</surname> <given-names>JJ</given-names></name> (<year>1998</year>) <article-title>Minimax and hamiltonian dynamics of excitatory-inhibitory networks</article-title>. In: <source>Advances in Neural Information Processing Systems 10</source>. <publisher-name>MIT Press</publisher-name>, pp. <fpage>329</fpage>–<lpage>335</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref013">
<label>13</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Wang</surname> <given-names>X</given-names></name> (<year>2008</year>) <article-title>Attractor network models</article-title>. In: <name name-style="western"><surname>Squire</surname> <given-names>L</given-names></name>, editor, <source>Encyclopedia of Neuroscience</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Academic Press</publisher-name>, <volume>volume 1</volume>, pp. <fpage>667</fpage>–<lpage>679</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref014">
<label>14</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ben-Yishai</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Bar-Or</surname> <given-names>RL</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname> <given-names>H</given-names></name> (<year>1995</year>) <article-title>Theory of orientation tuning in visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>92</volume>: <fpage>3844</fpage>–<lpage>3848</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.92.9.3844" xlink:type="simple">10.1073/pnas.92.9.3844</ext-link></comment> <object-id pub-id-type="pmid">7731993</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref015">
<label>15</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Abbott</surname> <given-names>L</given-names></name> (<year>2001</year>) <source>Theoretical Neuroscience</source>. <publisher-loc>Cambridge MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref016">
<label>16</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hahnloser</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Sarpeshkar</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Mahowald</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Douglas</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Seung</surname> <given-names>HS</given-names></name> (<year>2000</year>) <article-title>Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit</article-title>. <source>Nature</source> <volume>405</volume>: <fpage>947</fpage>–<lpage>51</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/35016072" xlink:type="simple">10.1038/35016072</ext-link></comment> <object-id pub-id-type="pmid">10879535</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref017">
<label>17</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Lohmiller</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Slotine</surname> <given-names>J</given-names></name> (<year>1998</year>) <article-title>On contraction analysis for nonlinear systems</article-title>. <source>Automatica</source> <volume>34</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0005-1098(98)00019-3" xlink:type="simple">10.1016/S0005-1098(98)00019-3</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref018">
<label>18</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>McCulloch</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Pitts</surname> <given-names>W</given-names></name> (<year>1943</year>) <article-title>A logical calculus of the ideas immanent in nervous activity</article-title>. <source>Bulletin of Mathematical Biology</source> <volume>5</volume>: <fpage>115</fpage>–<lpage>133</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref019">
<label>19</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Mead</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Conway</surname> <given-names>L</given-names></name> (<year>1980</year>) <source>Introduction to VLSI systems</source>, <volume>volume 1080</volume>. <publisher-name>Addison-Wesley</publisher-name> <publisher-loc>Reading, MA</publisher-loc>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref020">
<label>20</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hopfield</surname> <given-names>J</given-names></name> (<year>1982</year>) <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>79</volume>: <fpage>2554</fpage>–<lpage>2558</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.79.8.2554" xlink:type="simple">10.1073/pnas.79.8.2554</ext-link></comment> <object-id pub-id-type="pmid">6953413</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref021">
<label>21</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>von Neumann</surname> <given-names>J</given-names></name> (<year>1958</year>) <source>The Computer and the Brain</source>. <publisher-loc>New Haven</publisher-loc>: <publisher-name>Yale University Press</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref022">
<label>22</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Amari</surname> <given-names>SI</given-names></name> (<year>1972</year>) <article-title>Characteristics of random nets of analog neuron-like elements</article-title>. <source>IEEE Transactions on Systems, Man and Cybernetics SMC-2</source>: <fpage>643</fpage>–<lpage>657</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref023">
<label>23</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rutishauser</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Douglas</surname> <given-names>R</given-names></name> (<year>2009</year>) <article-title>State-dependent computation using coupled recurrent networks</article-title>. <source>Neural Computation</source> <volume>21</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/neco.2008.03-08-734" xlink:type="simple">10.1162/neco.2008.03-08-734</ext-link></comment> <object-id pub-id-type="pmid">19431267</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref024">
<label>24</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rutishauser</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Douglas</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Slotine</surname> <given-names>J</given-names></name> (<year>2011</year>) <article-title>Collective stability of networks of winner-take-all circuits</article-title>. <source>Neural computation</source> <volume>23</volume>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00091" xlink:type="simple">10.1162/NECO_a_00091</ext-link></comment> <object-id pub-id-type="pmid">21162667</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref025">
<label>25</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hahnloser</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Seung</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Slotine</surname> <given-names>J</given-names></name> (<year>2003</year>) <article-title>Permitted and forbidden sets in symmetric threshold-linear networks</article-title>. <source>Neural Comput</source> <volume>15</volume>: <fpage>621</fpage>–<lpage>638</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976603321192103" xlink:type="simple">10.1162/089976603321192103</ext-link></comment> <object-id pub-id-type="pmid">12620160</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref026">
<label>26</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hopfield</surname> <given-names>JJ</given-names></name> (<year>1984</year>) <article-title>Neurons with graded response have collective computational properties like those of 2-state neurons</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America-Biological Sciences</source> <volume>81</volume>: <fpage>3088</fpage>–<lpage>3092</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.81.10.3088" xlink:type="simple">10.1073/pnas.81.10.3088</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref027">
<label>27</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ao</surname> <given-names>P</given-names></name> (<year>2004</year>) <article-title>Potential in stochastic differential equations: novel construction</article-title>. <source>Journal of physics A: mathematical and general</source> <volume>37</volume>: <fpage>L25</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1088/0305-4470/37/3/L01" xlink:type="simple">10.1088/0305-4470/37/3/L01</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref028">
<label>28</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Joseph</surname> <given-names>DD</given-names></name> (<year>2006</year>) <article-title>Helmholtz decomposition coupling rotational to irrotational ow of a viscous uid</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>103</volume>: <fpage>14272</fpage>–<lpage>14277</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.0605792103" xlink:type="simple">10.1073/pnas.0605792103</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref029">
<label>29</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Neftci</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Binas</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Rutishauser</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Chicca</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Indiveri</surname> <given-names>G</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>Synthesizing cognition in neuromorphic electronic systems</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>110</volume>: <fpage>E3468</fpage>–<lpage>E3476</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1073/pnas.1212083110" xlink:type="simple">10.1073/pnas.1212083110</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref030">
<label>30</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Slotine</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Lohmiller</surname> <given-names>W</given-names></name> (<year>2001</year>) <article-title>Modularity, evolution, and the binding problem: a view from stability theory</article-title>. <source>Neural Networks</source> <volume>14</volume>: <fpage>137</fpage>–<lpage>145</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0893-6080(00)00089-7" xlink:type="simple">10.1016/S0893-6080(00)00089-7</ext-link></comment> <object-id pub-id-type="pmid">11316230</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref031">
<label>31</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Dijkstra</surname> <given-names>EW</given-names></name>, <name name-style="western"><surname>Dijkstra</surname> <given-names>EW</given-names></name>, <name name-style="western"><surname>Dijkstra</surname> <given-names>EW</given-names></name>, <name name-style="western"><surname>Dijkstra</surname> <given-names>EW</given-names></name> (<year>1976</year>) <source>A discipline of programming</source>, <volume>volume 4</volume>. <publisher-name>prentice-hall Englewood Cliffs</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref032">
<label>32</label>
<mixed-citation xlink:type="simple" publication-type="journal"><name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name> (<year>2000</year>) <article-title>On the computational power of winner-take-all</article-title>. <source>Neural Computation</source> <volume>12</volume>: <fpage>2519</fpage>–<lpage>2536</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976600300014827" xlink:type="simple">10.1162/089976600300014827</ext-link></comment> <object-id pub-id-type="pmid">11110125</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref033">
<label>33</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hahnloser</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Douglas</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Mahowald</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hepp</surname> <given-names>K</given-names></name> (<year>1999</year>) <article-title>Feedback interactions between neuronal pointers and maps for attentional processing</article-title>. <source>Nat Neurosci</source> <volume>2</volume>: <fpage>746</fpage>–<lpage>52</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/11219" xlink:type="simple">10.1038/11219</ext-link></comment> <object-id pub-id-type="pmid">10412065</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref034">
<label>34</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Douglas</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Martin</surname> <given-names>K</given-names></name> (<year>2007</year>) <article-title>Recurrent neuronal circuits in the neocortex</article-title>. <source>Curr Biol</source> <volume>17</volume>: <fpage>R496</fpage>–<lpage>R500</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.cub.2007.04.024" xlink:type="simple">10.1016/j.cub.2007.04.024</ext-link></comment> <object-id pub-id-type="pmid">17610826</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref035">
<label>35</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Yuille</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Geiger</surname> <given-names>D</given-names></name> (<year>2003</year>) <article-title>Winner-take-all networks</article-title>. In: <name name-style="western"><surname>Arbib</surname> <given-names>M</given-names></name>, editor, <source>The Handbook of Brain Theory and Neural Networks</source>, <publisher-name>MIT Press</publisher-name>. pp. <fpage>1228</fpage>–<lpage>1231</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref036">
<label>36</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ermentrout</surname> <given-names>B</given-names></name> (<year>1992</year>) <article-title>Complex dynamics in winner-take-all neural nets with slow inhibition</article-title>. <source>Neural Networks</source> <volume>5</volume>: <fpage>415</fpage>–<lpage>431</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0893-6080(92)90004-3" xlink:type="simple">10.1016/0893-6080(92)90004-3</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref037">
<label>37</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wersing</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Steil</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Ritter</surname> <given-names>H</given-names></name> (<year>2001</year>) <article-title>A competitive-layer model for feature binding and sensory segmentation</article-title>. <source>Neural Computation</source> <volume>13</volume>: <fpage>357</fpage>–<lpage>387</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976601300014574" xlink:type="simple">10.1162/089976601300014574</ext-link></comment> <object-id pub-id-type="pmid">11177439</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref038">
<label>38</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Abbott</surname> <given-names>L</given-names></name> (<year>1994</year>) <article-title>Decoding neuronal firing and modelling neural networks</article-title>. <source>Q Rev Biophys</source> <volume>27</volume>: <fpage>291</fpage>–<lpage>331</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1017/S0033583500003024" xlink:type="simple">10.1017/S0033583500003024</ext-link></comment> <object-id pub-id-type="pmid">7899551</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref039">
<label>39</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Rutishauser</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Slotine</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Douglas</surname> <given-names>RJ</given-names></name> (<year>2012</year>) <article-title>Competition through selective inhibitory synchrony</article-title>. <source>Neural computation</source> <volume>24</volume>: <fpage>2033</fpage>–<lpage>2052</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/NECO_a_00304" xlink:type="simple">10.1162/NECO_a_00304</ext-link></comment> <object-id pub-id-type="pmid">22509969</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref040">
<label>40</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Blum</surname> <given-names>L</given-names></name> (<year>1998</year>) <source>Complexity and real computation</source>. <publisher-name>Springer</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref041">
<label>41</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Blum</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Shub</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Smale</surname> <given-names>S</given-names></name> (<year>1989</year>) <article-title>On a theory of computation and complexity over the real numbers: <italic>np</italic>-completeness, recursive functions and universal machines</article-title>. <source>Bulletin (New Series) of the American Mathematical Society</source> <volume>21</volume>: <fpage>1</fpage>–<lpage>46</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1090/S0273-0979-1989-15750-9" xlink:type="simple">10.1090/S0273-0979-1989-15750-9</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref042">
<label>42</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Siegelmann</surname> <given-names>HT</given-names></name>, <name name-style="western"><surname>Fishman</surname> <given-names>S</given-names></name> (<year>1998</year>) <article-title>Analog computation with dynamical systems</article-title>. <source>Physica D: Nonlinear Phenomena</source> <volume>120</volume>: <fpage>214</fpage>–<lpage>235</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S0167-2789(98)00057-8" xlink:type="simple">10.1016/S0167-2789(98)00057-8</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref043">
<label>43</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Siegelmann</surname> <given-names>HT</given-names></name>, <name name-style="western"><surname>Sontag</surname> <given-names>ED</given-names></name> (<year>1995</year>) <article-title>On the computational power of neural nets</article-title>. <source>Journal of computer and system sciences</source> <volume>50</volume>: <fpage>132</fpage>–<lpage>150</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1006/jcss.1995.1013" xlink:type="simple">10.1006/jcss.1995.1013</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref044">
<label>44</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Deco</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Jirsa</surname> <given-names>VK</given-names></name>, <name name-style="western"><surname>Robinson</surname> <given-names>PA</given-names></name>, <name name-style="western"><surname>Breakspear</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name> (<year>2008</year>) <article-title>The dynamic brain: from spiking neurons to neural masses and cortical fields</article-title>. <source>PLoS computational biology</source> <volume>4</volume>: <fpage>e1000092</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pcbi.1000092" xlink:type="simple">10.1371/journal.pcbi.1000092</ext-link></comment> <object-id pub-id-type="pmid">18769680</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref045">
<label>45</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name> (<year>2008</year>) <article-title>Simple substrates for complex cognition</article-title>. <source>Frontiers in neuroscience</source> <volume>2</volume>: <fpage>255</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.3389/neuro.01.031.2008" xlink:type="simple">10.3389/neuro.01.031.2008</ext-link></comment> <object-id pub-id-type="pmid">19225599</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref046">
<label>46</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Ma</surname> <given-names>WJ</given-names></name>, <name name-style="western"><surname>Beck</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Latham</surname> <given-names>PE</given-names></name>, <name name-style="western"><surname>Pouget</surname> <given-names>A</given-names></name> (<year>2006</year>) <article-title>Bayesian inference with probabilistic population codes</article-title>. <source>Nature neuroscience</source> <volume>9</volume>: <fpage>1432</fpage>–<lpage>1438</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nn1790" xlink:type="simple">10.1038/nn1790</ext-link></comment> <object-id pub-id-type="pmid">17057707</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref047">
<label>47</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Li</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Dayan</surname> <given-names>P</given-names></name> (<year>1999</year>) <article-title>Computational differences between asymmetrical and symmetrical networks</article-title>. In: <source>Advances in Neural Information Processing Systems</source>. pp. <fpage>274</fpage>–<lpage>280</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref048">
<label>48</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Hahnloser</surname> <given-names>RH</given-names></name>, <name name-style="western"><surname>Seung</surname> <given-names>HS</given-names></name>, <name name-style="western"><surname>Slotine</surname> <given-names>JJ</given-names></name> (<year>2003</year>) <article-title>Permitted and forbidden sets in symmetric thresholdlinear networks</article-title>. <source>Neural Computation</source> <volume>15</volume>: <fpage>621</fpage>–<lpage>638</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1162/089976603321192103" xlink:type="simple">10.1162/089976603321192103</ext-link></comment> <object-id pub-id-type="pmid">12620160</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref049">
<label>49</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Haykin</surname> <given-names>SS</given-names></name>, <name name-style="western"><surname>Haykin</surname> <given-names>SS</given-names></name>, <name name-style="western"><surname>Haykin</surname> <given-names>SS</given-names></name> (<year>2001</year>) <source>Kalman filtering and neural networks</source>. <publisher-name>Wiley Online Library</publisher-name>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref050">
<label>50</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Kalman</surname> <given-names>RE</given-names></name>, <name name-style="western"><surname>Bucy</surname> <given-names>RS</given-names></name> (<year>1961</year>) <article-title>New results in linear filtering and prediction theory</article-title>. <source>Journal of Fluids Engineering</source> <volume>83</volume>: <fpage>95</fpage>–<lpage>108</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref051">
<label>51</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Sussman</surname> <given-names>GJ</given-names></name>, <name name-style="western"><surname>Steele Jr</surname> <given-names>GL</given-names></name> (<year>1980</year>) <article-title>Constraintsa language for expressing almost-hierarchical descriptions</article-title>. <source>Artificial intelligence</source> <volume>14</volume>: <fpage>1</fpage>–<lpage>39</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0004-3702(80)90032-6" xlink:type="simple">10.1016/0004-3702(80)90032-6</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref052">
<label>52</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Radul</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Sussman</surname> <given-names>GJ</given-names></name> (<year>2009</year>) <article-title>The art of the propagator</article-title>. In: <source>Proceedings of the 2009 international lisp conference</source>. pp. <fpage>1</fpage>–<lpage>10</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref053">
<label>53</label>
<mixed-citation xlink:type="simple" publication-type="book">
<name name-style="western"><surname>Stepney</surname> <given-names>S</given-names></name> (<year>2012</year>) <article-title>Nonclassical computation: a dynamical systems perspective</article-title>. In: <source>Handbook of Natural Computing</source>. <publisher-name>Springer</publisher-name>, pp. <fpage>1979</fpage>–<lpage>2025</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1004039.ref054">
<label>54</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Marder</surname> <given-names>E</given-names></name> (<year>1998</year>) <article-title>From biophysics to models of network function</article-title>. <source>Annual Review of Neuroscience</source> <volume>21</volume>: <fpage>25</fpage>–<lpage>45</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.neuro.21.1.25" xlink:type="simple">10.1146/annurev.neuro.21.1.25</ext-link></comment> <object-id pub-id-type="pmid">9530490</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref055">
<label>55</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Getting</surname> <given-names>PA</given-names></name> (<year>1989</year>) <article-title>Emerging principles governing the operation of neural networks</article-title>. <source>Annual Review of Neuroscience</source> <volume>12</volume>: <fpage>185</fpage>–<lpage>204</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1146/annurev.ne.12.030189.001153" xlink:type="simple">10.1146/annurev.ne.12.030189.001153</ext-link></comment> <object-id pub-id-type="pmid">2648949</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref056">
<label>56</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Koch</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Laurent</surname> <given-names>G</given-names></name> (<year>1999</year>) <article-title>Complexity and the Nervous System</article-title>. <source>Science</source> <volume>284</volume>: <fpage>96</fpage>–<lpage>98</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.284.5411.96" xlink:type="simple">10.1126/science.284.5411.96</ext-link></comment> <object-id pub-id-type="pmid">10102826</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref057">
<label>57</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Jaeger</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Maass</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Principe</surname> <given-names>J</given-names></name> (<year>2007</year>) <article-title>Special issue on echo state networks and liquid state machines</article-title>. <source>Neural Networks</source> <volume>20</volume>: <fpage>287</fpage>–<lpage>289</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.neunet.2007.04.001" xlink:type="simple">10.1016/j.neunet.2007.04.001</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref058">
<label>58</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Genot</surname> <given-names>AJ</given-names></name>, <name name-style="western"><surname>Fujii</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Rondelez</surname> <given-names>Y</given-names></name> (<year>2012</year>) <article-title>Computing with competition in biochemical networks</article-title>. <source>Physical review letters</source> <volume>109</volume>: <fpage>208102</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1103/PhysRevLett.109.208102" xlink:type="simple">10.1103/PhysRevLett.109.208102</ext-link></comment> <object-id pub-id-type="pmid">23215526</object-id></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref059">
<label>59</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Lohmiller</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Slotine</surname> <given-names>J</given-names></name> (<year>2000</year>) <article-title>Nonlinear process control using contraction theory</article-title>. <source>AIChE Journal</source> <volume>46</volume>: <fpage>588</fpage>–<lpage>596</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/aic.690460317" xlink:type="simple">10.1002/aic.690460317</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref060">
<label>60</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Slotine</surname> <given-names>J</given-names></name> (<year>2003</year>) <article-title>Modular stability tools for distributed computation and control</article-title>. <source>International Journal of Adaptive Control and Signal Processing</source> <volume>17</volume>: <fpage>397</fpage>–<lpage>416</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/acs.754" xlink:type="simple">10.1002/acs.754</ext-link></comment></mixed-citation>
</ref>
<ref id="pcbi.1004039.ref061">
<label>61</label>
<mixed-citation xlink:type="simple" publication-type="journal">
<name name-style="western"><surname>Wang</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Slotine</surname> <given-names>J</given-names></name> (<year>2005</year>) <article-title>On partial contraction analysis for coupled nonlinear oscillators</article-title>. <source>Biological Cybernetics</source> <volume>91</volume>.</mixed-citation>
</ref>
</ref-list>
</back>
</article>