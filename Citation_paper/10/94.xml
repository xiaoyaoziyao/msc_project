<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group>
<journal-title>PLoS ONE</journal-title></journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-13-32579</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0087619</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Evolutionary modeling</subject></subj-group></subj-group><subj-group><subject>Evolutionary biology</subject><subj-group><subject>Evolutionary processes</subject><subj-group><subject>Adaptation</subject></subj-group></subj-group><subj-group><subject>Organismal evolution</subject><subj-group><subject>Human evolution</subject></subj-group></subj-group><subj-group><subject>Evolutionary theory</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Game theory</subject></subj-group></subj-group><subj-group><subject>Probability theory</subject><subj-group><subject>Bayes theorem</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Social and behavioral sciences</subject><subj-group><subject>Information science</subject><subj-group><subject>Information theory</subject></subj-group></subj-group><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Human intelligence</subject></subj-group></subj-group><subj-group><subject>Social psychology</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Theory of Mind: Did Evolution Fool Us?</article-title>
<alt-title alt-title-type="running-head">Evolution of Theory of Mind</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Devaine</surname><given-names>Marie</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Hollard</surname><given-names>Guillaume</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Daunizeau</surname><given-names>Jean</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="aff" rid="aff4"><sup>4</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Motivation, Brain, and Behavior Laboratory, Brain and Spine Institute, Hôpital de la Pitié Salpêtrière Paris, France</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Maison des Sciences Economiques, Paris, France</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>CNRS UMR 7225, INSERM U 975, UPMC Paris, France</addr-line></aff>
<aff id="aff4"><label>4</label><addr-line>Wellcome Trust Centre for Neuroimaging, University College London, London, United Kingdom</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Zalla</surname><given-names>Tiziana</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Ecole Normale Supérieure, France</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">jean.daunizeau@gmail.com</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: JD MD. Performed the experiments: JD MD. Analyzed the data: JD MD. Contributed reagents/materials/analysis tools: JD MD. Wrote the paper: JD MD GH.</p></fn>
</author-notes>
<pub-date pub-type="collection"><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>5</day><month>2</month><year>2014</year></pub-date>
<volume>9</volume>
<issue>2</issue>
<elocation-id>e87619</elocation-id>
<history>
<date date-type="received"><day>30</day><month>7</month><year>2013</year></date>
<date date-type="accepted"><day>26</day><month>12</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Devaine et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Theory of Mind (ToM) is the ability to attribute mental states (e.g., beliefs and desires) to other people in order to understand and predict their behaviour. If others are rewarded to compete or cooperate with you, then what they will do depends upon what they believe about you. This is the reason why social interaction induces recursive ToM, of the sort “I think that you think that I think, etc.”. Critically, recursion is the common notion behind the definition of sophistication of human language, strategic thinking in games, and, arguably, ToM. Although sophisticated ToM is believed to have high adaptive fitness, broad experimental evidence from behavioural economics, experimental psychology and linguistics point towards limited recursivity in representing other’s beliefs. In this work, we test whether such apparent limitation may not in fact be proven to be adaptive, i.e. optimal in an evolutionary sense. First, we propose a meta-Bayesian approach that can predict the behaviour of ToM sophistication phenotypes who engage in social interactions. Second, we measure their adaptive fitness using evolutionary game theory. Our main contribution is to show that one does not have to appeal to biological costs to explain our limited ToM sophistication. In fact, the evolutionary cost/benefit ratio of ToM sophistication is non trivial. This is partly because an informational cost prevents highly sophisticated ToM phenotypes to fully exploit less sophisticated ones (in a competitive context). In addition, cooperation surprisingly favours <italic>lower</italic> levels of ToM sophistication. Taken together, these quantitative corollaries of the “social Bayesian brain” hypothesis provide an evolutionary account for both the limitation of ToM sophistication in humans as well as the persistence of low ToM sophistication levels.</p>
</abstract>
<funding-group><funding-statement>This work was supported by the European Research Council (JD) and the French Ministère de l’Education Nationale (MD). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="12"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Theory of Mind (ToM) is the ability to attribute mental states (e.g., beliefs and desires) to other people in order to understand and predict their behaviour <xref ref-type="bibr" rid="pone.0087619-Premack1">[1]</xref>. This ability lies at the core of human social cognition: it develops early in life <xref ref-type="bibr" rid="pone.0087619-Kovcs1">[2]</xref>, and its impairment is associated with severe neuropsychiatric disorders <xref ref-type="bibr" rid="pone.0087619-Frith1">[3]</xref>–<xref ref-type="bibr" rid="pone.0087619-Brne1">[4]</xref>. ToM endows us with highly adaptive social skills, such as teaching, persuading or deceiving <xref ref-type="bibr" rid="pone.0087619-BaronCohen1">[5]</xref>. Thus, natural selection should have promoted phenotypes that exhibit highly sophisticated forms of ToM <xref ref-type="bibr" rid="pone.0087619-Emery1">[6]</xref>–<xref ref-type="bibr" rid="pone.0087619-Moll1">[10]</xref>.</p>
<p>In fact, behavioural economics has provided undisputable experimental evidence of people’s <italic>bounded rationality</italic> in strategic interactions <xref ref-type="bibr" rid="pone.0087619-Simon1">[11]</xref>. In particular, we seem to be very limited in our ability to correctly guess the behaviour of others in games <xref ref-type="bibr" rid="pone.0087619-Camerer1">[12]</xref>–<xref ref-type="bibr" rid="pone.0087619-Stahl1">[14]</xref>. These results corroborate experimental psychology studies <xref ref-type="bibr" rid="pone.0087619-Hedden1">[15]</xref>–<xref ref-type="bibr" rid="pone.0087619-Kinderman1">[16]</xref>, as well as linguistic and even literary evidence <xref ref-type="bibr" rid="pone.0087619-Dunbar2">[17]</xref>–<xref ref-type="bibr" rid="pone.0087619-Zunshine1">[18]</xref> that all point towards a heterogeneous and limited ToM sophistication in humans. We may thus wonder why evolution has not made all of us smarter. In particular, what made it possible for low ToM sophistication phenotypes to persist in socially demanding environments? In this work, we test whether such apparent limitations may not in fact be proven to be adaptive, i.e. optimal in an evolutionary sense. In turn, this raises two challenging issues: (i) how do we formally define ToM sophistication phenotypes?, and (ii) how do we measure their adaptive fitness?</p>
<p>We start with the premise that if others are rewarded to compete or cooperate with you, what they believe you will do is relevant for you to predict their behaviour. This is the reason why social interaction induces recursive thinking, of the sort “I think that you think that I think, etc.”. Critically, recursion is the common notion behind the definition of sophistication of human language <xref ref-type="bibr" rid="pone.0087619-Hauser1">[19]</xref>–<xref ref-type="bibr" rid="pone.0087619-Corballis1">[20]</xref> and strategic thinking in games <xref ref-type="bibr" rid="pone.0087619-Stahl1">[14]</xref>, <xref ref-type="bibr" rid="pone.0087619-Camerer2">[21]</xref>. In line with Yoshida et al. <xref ref-type="bibr" rid="pone.0087619-Yoshida1">[22]</xref>, we define ToM sophistication as the depth of recursive thinking. Here, a 0-ToM agent learns (over the course of repeated interactions) how likely her opponent’s choices are. In contrast, a 1-ToM agent adopts the “intentional stance” <xref ref-type="bibr" rid="pone.0087619-Dennett1">[23]</xref>, i.e. she tries to understand how 0-ToM updates his belief, from observing his behaviour. Hence, 1-ToM is defined in terms of her recursive belief, i.e. her belief about 0-ToM’s belief. A 2-ToM observer assumes she faces either a 1-ToM or a 0-ToM agent. This means she has to both recognize the sophistication of her opponent and understand how he learns. More generally, a k-ToM agent tries to understand how her opponent learns, under the assumption that he is less sophisticated than herself. In so doing, k-ToM forms high-order recursive beliefs, which may be highly uncertain. Thus, we model the impact of subjective uncertainty onto the mechanism of belief update using information theory (cf. the <italic>Bayesian brain</italic> hypothesis <xref ref-type="bibr" rid="pone.0087619-Friston1">[24]</xref>–<xref ref-type="bibr" rid="pone.0087619-Daunizeau1">[26]</xref>).</p>
<p>In the context of social interaction, we are left with the question of what prior information agents use to learn about how others learn. Here, we simply assume that the brain’s model of other brains presumes they are optimal too. By this we mean that people believe other conspecifics behave according to common sense (e.g., they make decisions that reveal their preferences and beliefs, which change as learning unfolds). The key idea here is to consider how such common sense notion impacts on the (Bayes-optimal) learning rules of agents interacting with each other. In this context, Bayes-optimality simply means that information processing suffers no distortion aside from potential prior biases. Agnostic priors on peoples’ choices (i.e. priors that do not involve the intentional stance) would yield Bayesian agents that track the descriptive statistics of others’ choices. This is essentially what 0-ToM learners do. Eventually, they arrive at uncertain estimates (beliefs) of, e.g., others’ choice frequency. However, Bayes-optimal forecasts of 0-ToM’s behaviour rely on the (ambiguous) identification of the covert beliefs and preferences that determine her overt decisions. This is the essence of 1-ToM’s learning rule, which relies on an informative prior assumption, namely: others are (agnostic) Bayes-optimal agents. Under this “social Bayesian brain” hypothesis, one can derive the learning rule of k-ToM agents recursively, starting with 0-ToM (see <bold>Models</bold>).</p>
<p>Although k-ToM learners are all Bayes-optimal, they differ in terms of the depth of recursion of their beliefs. This difference in ToM sophistication changes the way k-ToM agents react to a given sequence of their opponent’s action. For example, 0-ToM will tend to act as if her opponent was more likely to pick the action that she had chosen most frequently in the past. In turn, 1-ToM will anticipate this and act accordingly. Since their respective behavioural response pattern will be different, 2-ToM is in a position to discriminate between 0-ToM and 1-ToM (and act accordingly). In brief, k-ToM will best-respond to her opponent’s past choices, under the constraint of limited sophistication. Thus, ToM sophistication phenotypes are characterized in terms of (formal) belief update rules that (i) are specific to the depth of their recursion, and (ii) shape their behavioural strategy over the course of repeated social interactions.</p>
<p>We address the second challenge from the perspective of evolutionary game theory (EGT). In brief, EGT states that the reproductive and survival successes of any behavioural phenotype is determined by how well it performs when interacting with other alternative phenotypes <xref ref-type="bibr" rid="pone.0087619-MaynardSmith1">[27]</xref>. Here, we extend this idea to evaluate the adaptive fitness of ToM sophistication. Current ethological debates highlight the importance of competitive versus cooperative types of reciprocal social interactions in the evolution of ToM <xref ref-type="bibr" rid="pone.0087619-Moll1">[10]</xref>. We thus focused on a pair of two-players games that capture these two canonical forms of social interaction. In “hide and seek”, the gain of the winner is exactly balanced by the loss of the looser, which is the essence of competition. In contradistinction, agents playing “battle of the sexes” are most rewarded for coordinating their behaviour (see <bold><xref ref-type="sec" rid="s4">Models and Methods</xref></bold>). Note that both games’ payoffs are contingent on players’ ability in predicting their opponent’s behaviour (there is no prior good decision).</p>
</sec><sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>On the Relative Performance of ToM Phenotypes Engaged in Iterated Games</title>
<p>To assess the relative performance of ToM sophistication phenotypes engaged in either cooperative or competitive social interactions, we performed the following series of Monte-Carlo simulations. We let all 5×5 = 25 combinations of pairs of ToM agents (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e001" xlink:type="simple"/></inline-formula>) play repeatedly “hide and seek” and “battle of the sexes” (cf. game outcomes in <xref ref-type="table" rid="pone-0087619-t001">Table 1</xref> below) against each other. One simulation thus consisted of the history of beliefs, choices and outcomes, for both agents, across trials (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e002" xlink:type="simple"/></inline-formula>).We measured the accumulated payoff each ToM phenotype receives as a function of trial τ, when interacting with any other ToM phenotype. We repeated each type of simulation 500 times, in order to average out variability arising from behavioural noise (see Methods section below). <xref ref-type="fig" rid="pone-0087619-g001">Figure 1</xref> depicts these payoff matrices at trial τ = 512. Since τ controls the amount of available information, those can be understood in terms of the relative success of ToM phenotypes after learning has occurred.</p>
<fig id="pone-0087619-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0087619.g001</object-id><label>Figure 1</label><caption>
<title>MCMC average payoffs of all pairs of ToM agents.</title>
<p>This figure depicts the MCMC average of the payoff matrices for both “hide and seek” (left) and “battle of the sexes” (right) after learning has occurred. The i<sup>th</sup> line gives the accumulated payoff of the i<sup>th</sup> type of agent, when playing against each and every other ToM phenotype. Note that the absolute payoff levels of both types of games cannot be compared.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0087619.g001" position="float" xlink:type="simple"/></fig><table-wrap id="pone-0087619-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0087619.t001</object-id><label>Table 1</label><caption>
<title>Payoffs for each player in the “hide and seek” game (left) and “battle of the sexes” (right).</title>
</caption><alternatives><graphic id="pone-0087619-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0087619.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">P2: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e003" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">P2: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e004" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">P2: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e005" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">P2: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e006" xlink:type="simple"/></inline-formula></td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">P1: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e007" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e008" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e009" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">P1: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e010" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e011" xlink:type="simple"/></inline-formula></td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">P1: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e012" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e013" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e014" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1">P1: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e015" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e016" xlink:type="simple"/></inline-formula></td>
<td align="left" rowspan="1" colspan="1"/>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt101"><label/><p>Numbers inside brackets indicate the payoffs; the number on the left (resp. on the right) indicates the payoff player 1 (resp. player 2) gets when making decision <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e017" xlink:type="simple"/></inline-formula> while player 2 chooses <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e018" xlink:type="simple"/></inline-formula>.</p></fn></table-wrap-foot></table-wrap>
<p>In the competitive game, the expected payoff matrix is anti-symmetrical (this is because “hide and seek” is a zero-sum game). Overall, increasing ToM sophistication improves performance: for any ToM level, gains are systematically positive (respectively, negative) against less (respectively, more) sophisticated ToM agents. Interestingly, there is a systematic cost to sophistication: the relative gains decrease as the difference in ToM levels increases. This informational cost to sophistication essentially limits the way one can exploit less sophisticated ToM agents. Results in the context of the cooperative game are entirely different. Here, pairs of agents with different ToM levels perform much better than pairs of “twin” 0-ToM and 1-ToM agents, who fail to coordinate their behaviour. Note that the best performance level is observed for 1-ToM agents, when playing against <italic>more</italic> sophisticated agents. In addition, behavioural performance of pairs of k-ToM agents with k≥2 neither depends upon whether agents have similar sophistication levels (“twin” pairs versus non “twin” pairs), nor on the sophistication level <italic>per se</italic>. This is surprising, since it suggests that there is no advantage in being more sophisticated than a 2-ToM agent when engaging in a cooperative interaction. This means that being less sophisticated than the other player is only detrimental (in the sense of yielding inaccurate behavioural predictions) in a competitive setting.</p>
<p>The nature of the beliefs, which ToM agents develop as learning unfolds during the iterated games, sheds some light on these intriguing results.Recall that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e019" xlink:type="simple"/></inline-formula>-ToM selects the appropriate action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e020" xlink:type="simple"/></inline-formula> on the basis of her prediction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e021" xlink:type="simple"/></inline-formula>about her opponent’s next move. <xref ref-type="fig" rid="pone-0087619-g002">Figure 2</xref> compares this prediction against the real behavioural tendency experienced by her opponent, in the case of 0-ToM playing against 1-ToM (for both games).</p>
<fig id="pone-0087619-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0087619.g002</object-id><label>Figure 2</label><caption>
<title>Accuracy of behavioural predictions in competitive and cooperative contexts: example of 0-ToM playing against 1-ToM.</title>
<p>The behavioural prediction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e022" xlink:type="simple"/></inline-formula> of ToM players (y-axis) is plotted against her opponent’s true behavioural tendency <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e023" xlink:type="simple"/></inline-formula> (x-axis) for each trial of a simulated repeated game with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e024" xlink:type="simple"/></inline-formula> trials. The grey line indicates the best-fitting straight line in the data. Upper half: “Hide and Seek”. Lower half: “Battle of the Sexes”. Left: accuracy of 1-ToM predictions when playing against 0-ToM. Right: accuracy of 0-ToM predictions when playing against 1-ToM.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0087619.g002" position="float" xlink:type="simple"/></fig>
<p>One can see that when playing “hide and seek”, 1-ToM predicts very well the behaviour of 0-ToM, but that 0-ToM is almost always entirely wrong about 1-ToM next move. In other words, 0-ToM agents are fooled by 1-ToM agents in a competitive setting.</p>
<p>However, this is not the case when ToM agents play “battle of the sexes”: both players are able to correctly predict the behaviour of their partner. In other words, 0-ToM is not confused by 1-ToM in a cooperative setting. We will now check whether this difference between the prediction accuracy of less sophisticated ToM agents in a competitive/cooperative context generalizes to any ToM sophistication level. <xref ref-type="fig" rid="pone-0087619-g003">Figure 3</xref> summarizes the quality of this behavioural prediction for all pairs of ToM players.</p>
<fig id="pone-0087619-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0087619.g003</object-id><label>Figure 3</label><caption>
<title>MCMC average prediction accuracy of all pairs of ToM agents.</title>
<p>This figure depicts the MCMC average of the linear trend between the behavioural prediction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e025" xlink:type="simple"/></inline-formula> of ToM players and their opponent’s true behavioural tendency <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e026" xlink:type="simple"/></inline-formula>. In other words, this corresponds to the slope of the best-fitting straight line in <xref ref-type="fig" rid="pone-0087619-g002">Figure 2</xref>. The figure uses the same format as <xref ref-type="fig" rid="pone-0087619-g001">Figure 1</xref>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0087619.g003" position="float" xlink:type="simple"/></fig>
<p>One can see that the overall pattern is quite similar to the behavioural performances depicted on <xref ref-type="fig" rid="pone-0087619-g001">Fig. 1</xref>. This is intuitive, since this means that the accuracy of the prediction determines a significant amount of the variability in behavioural performance.</p>
<p>This is particularly salient when ToM agents play “hide and “seek”, which induces an almost perfect anti-symmetric pattern in the prediction accuracy. This means that, on average, ToM agents are fooled by more sophisticated opponent in a competitive setting. Note that “twin” pairs (pairs of ToM agents with identical sophistication levels) form behavioural predictions that are, on average, uncorrelated with the real behavioural tendency of their opponent. In addition, the prediction accuracy decreases with the ToM sophistication level. This consequence of statistical complexity induces the cost to sophistication that was observed on behavioural performance or accumulated reward (cf. <xref ref-type="fig" rid="pone-0087619-g001">Fig. 1</xref>).</p>
<p>These results are somewhat at odds with the pattern of prediction accuracy of ToM agents playing “battle of the sexes”. In brief, except for “twin” pairs of 0-ToM and 1-ToM agents, behavioural predictions are quite accurate. Interestingly also, behavioural predictions slightly improve with overall ToM sophistication level. This means that, on average, ToM agents are not confused by more sophisticated partners in a cooperative setting. In fact, ToM agents even benefit from the sophistication of their partner. This holds as well for “twin” pairs of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e027" xlink:type="simple"/></inline-formula>-ToM agents, provided <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e028" xlink:type="simple"/></inline-formula>. This is important, since this means that being less sophisticated than the other player is only inappropriate (in the sense of yielding inaccurate behavioural predictions) in a competitive setting.</p>
<p>The case of “twin” pairs is interesting because it reveals a fundamental difference between the nature of beliefs in competitive and cooperative contexts. In brief, for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e029" xlink:type="simple"/></inline-formula>, “twin” pairs form poor behavioural predictions about their opponent, whether they are in a competitive or in a cooperative context. More precisely, their behavioural predictions are effectively non-informative (they are right half of the time). However, for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e030" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e031" xlink:type="simple"/></inline-formula>-ToM agents that engage in a cooperative context can form very accurate behavioural predictions. Recall that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e032" xlink:type="simple"/></inline-formula> is a critical ToM sophistication level, in that any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e033" xlink:type="simple"/></inline-formula>-ToM agent with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e034" xlink:type="simple"/></inline-formula> has to learn the sophistication level of the other player. It turns out this is quite important to understand the difference in the prediction accuracy of “twin” pairs of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e035" xlink:type="simple"/></inline-formula>-ToM agents (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e036" xlink:type="simple"/></inline-formula>) in a competitive or a cooperative context, respectively. We will now summarize the beliefs of “twin pairs” of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e037" xlink:type="simple"/></inline-formula>-ToM agents about their opponent’s sophistication, and highlight its impact on behavioural performance in both games. First note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e038" xlink:type="simple"/></inline-formula>-ToM agents in a “twin pair” cannot infer the correct level of their opponent. This is because, by construction, they assume their opponent is less sophisticated than themselves. However, we will see that the type of game is highly predictive of the nature of their (erroneous) inference. <xref ref-type="fig" rid="pone-0087619-g004">Figure 4</xref> depicts the MCMC empirical histograms of ToM sophistication levels (see <bold><xref ref-type="sec" rid="s4">Models and Methods</xref></bold>) attributed by “twin” pairs of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e039" xlink:type="simple"/></inline-formula>-ToM agents with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e040" xlink:type="simple"/></inline-formula> to each other, for both types of games.</p>
<fig id="pone-0087619-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0087619.g004</object-id><label>Figure 4</label><caption>
<title>MCMC empirical distribution of learned opponent’s sophistication level for “twin” pairs of ToM agents.</title>
<p>Each bar gives the number of MCMC simulations (z-axis) that led to each particular combination of belief <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e041" xlink:type="simple"/></inline-formula>, both agents had on each other’s ToM sophistication level (x/y-plane). Histograms are truncated to the upper-left triangle for visualization purposes (they are symmetrical by construction). Upper half: “Hide and Seek”. Lower half: “battle of the Sexes”. Left: “twin” pairs of 2-ToM agents, Middle: “twin” pairs of 3-ToM agents, right: “twin” pairs of 4-ToM agents.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0087619.g004" position="float" xlink:type="simple"/></fig>
<p>One can see that when playing “hide and seek”, each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e042" xlink:type="simple"/></inline-formula>-ToM agent in the “twin” pairs almost always believes that her opponent is a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e043" xlink:type="simple"/></inline-formula>-ToM agent (cf. peak at the upper-right corner of the histograms). In other words, the competitive setting induces a bias in the attribution of the opponent’s ToM level towards maximal sophistication.</p>
<p>Results are entirely different when ToM agents play “battle of sexes”. In this context, a pair of “twin” 2-ToM agents eventually arrives at different beliefs: one agent believes her opponent is 0-ToM, whereas the other systematically thinks hers is 1-ToM (cf. peak at the upper-left corner of the histogram). This makes the “twin” 2-ToM behave as a pair of 1-ToM and 2-ToM agents, and yields good coordination performance (cf. <xref ref-type="fig" rid="pone-0087619-g001">Fig. 1</xref>). This pattern tends to be confirmed for “twin” pairs of 3-ToM and 4-ToM: the agents almost never have the same belief about their opponent sophistication (cf. empty main diagonal in the histograms). In fact, agents have heterogeneous beliefs most of the time, which makes them behave as a heterogeneous pair. In other words, the cooperative setting induces a bias towards heterogeneous reciprocal beliefs about each other ToM sophistication. This means that coordination is successful when there is heterogeneity in the reciprocal beliefs about ToM sophistication levels. Ironically speaking, successful cooperation arises when one agent is more dismissive about her partner than her partner is about her.</p>
<p>To sum up, in contrast to competitive interactions, ToM agents are not confused by more sophisticated partners in a cooperative setting. In fact, ToM agents even benefit from the sophistication of their partner.</p>
</sec><sec id="s2b">
<title>Evolution of ToM: Influence of Learning and Cooperative Interactions</title>
<p>We then used EGT to simulate the evolution of societies populated with heterogeneous ToM sophistication phenotypes. In brief, we inserted the average payoff matrices into EGT replicator dynamics, which describe the dynamics of the frequency of competing phenotypes over evolutionary time (see <bold><xref ref-type="sec" rid="s4">Models and Methods</xref></bold>). These eventually converge to the evolutionary stable states, which are a repartition of phenotypes that is restored by selection after a disturbance <xref ref-type="bibr" rid="pone.0087619-MaynardSmith1">[27]</xref>. <xref ref-type="fig" rid="pone-0087619-g005">Figure 5</xref> shows examples of replicator dynamics, with five ToM phenotypes (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e044" xlink:type="simple"/></inline-formula>), after <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e045" xlink:type="simple"/></inline-formula> game iterations, and for both pure cooperative (“battle of the sexes”) and pure competitive (“hide and seek”) social interactions.</p>
<fig id="pone-0087619-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0087619.g005</object-id><label>Figure 5</label><caption>
<title>Replicator dynamics for purely cooperative and competitive social interactions.</title>
<p>The frequency of each ToM phenotype (y-axis) is plotted against evolutionary time (x-axis), for 128 different simulations with different initial conditions. Different ToM traits correspond to different colours (see legend). Pie charts depict the evolutionary stale states, i.e. the equilibrium or fixed point, replicator dynamics converge to (the colour coding is the same). Upper half: “Hide and Seek”. Lower half: “battle of the Sexes”.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0087619.g005" position="float" xlink:type="simple"/></fig>
<p>Different paths correspond to different initial phenotypes frequency distributions. First, one can see that the equilibrium points are stable, with basins of attraction spanning all sampled initial conditions (this was always the case). Second, these evolutionary stable states depend upon the game type (i.e. cooperative or competitive). This is because replicator dynamics unfold from relative performance of ToM phenotypes captured by payoff matrices depicted on <xref ref-type="fig" rid="pone-0087619-g001">Figure 1</xref>. In a purely competitive context, evolutionary dynamics follow a very reproducible sequence of ToM phenotypes extinction. In brief, expected extinction time increases with ToM sophistication levels, i.e. 0-ToM traits disappear first, then 1-ToM, 2-ToM, etc… This winner-take-all Darwinian competition eventually selects the most sophisticated ToM phenotype, whose evolutionary stable frequency reaches unity.</p>
<p>As one would expect from behavioural performance results (cf. <xref ref-type="fig" rid="pone-0087619-g001">Figure 1</xref>), replicator dynamics in the context of purely cooperative interactions are qualitatively different. In brief, two time scales seem to be at play: first, very quick selection pressure make 0-ToM disappear and the frequency of 1-ToM phenotypes converge towards <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e046" xlink:type="simple"/></inline-formula>. Second, slower winner-take-all competition between higher ToM sophistication phenotypes (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e047" xlink:type="simple"/></inline-formula>) eventually selects 2-ToM phenotypes, whose evolutionary stable frequency approaches <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e048" xlink:type="simple"/></inline-formula>.</p>
<p>Let us now inspect in a more systematic manner the effect of cooperation and learning onto evolutionary stable states. In brief, we varied the proportion <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e049" xlink:type="simple"/></inline-formula> of cooperative social interactions as well as the number of game iterations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e050" xlink:type="simple"/></inline-formula> (see <bold><xref ref-type="sec" rid="s4">Models and Methods</xref></bold>). Note that no oscillation or cycle in the evolutionary dynamics was observed throughout the entire range of phase parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e051" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e052" xlink:type="simple"/></inline-formula>. This means that selective pressure always eventually converges toward an evolutionary stable state. Additionally, this evolutionary stable state was always unique (no multistability). Taken together, this means evolutionary stable states are a faithful summary of replicator dynamics. <xref ref-type="fig" rid="pone-0087619-g006">Figure 6</xref> summarizes the dependency of evolutionary stable states w.r.t. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e053" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e054" xlink:type="simple"/></inline-formula>.</p>
<fig id="pone-0087619-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0087619.g006</object-id><label>Figure 6</label><caption>
<title>Phase diagram of ToM evolution.</title>
<p>Each pie chart depict the evolutionary stable state that is induced by a particular combination of amount of learning τ (x-axis) and proportion ω of cooperative interactions (y-axis).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0087619.g006" position="float" xlink:type="simple"/></fig>
<p>First, irrespective of the proportion <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e055" xlink:type="simple"/></inline-formula> of cooperative interactions and the number of game iterations <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e056" xlink:type="simple"/></inline-formula> (except for one-shot games, i.e.: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e057" xlink:type="simple"/></inline-formula>), the 0-ToM phenotype is not evolutionary stable. This means that selective pressure favours phenotypes that are capable of taking an “intentional stance”. In other words, natural selection induces a lower bound on ToM phenotypes. Second, evolutionary stable states are either dominated by the most sophisticated ToM phenotypes (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e058" xlink:type="simple"/></inline-formula>) or consist of mixed populations, most particularly when cooperative social interactions become more likely. More precisely, when the proportion ω of cooperative social interactions reaches a critical threshold, the population mostly consists of ToM phenotypes smaller than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e059" xlink:type="simple"/></inline-formula>. This means that cooperative social interactions effectively induce an upper bound on ToM sophistication. Note that the critical threshold depends upon the amount <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e060" xlink:type="simple"/></inline-formula> of learning: the longer the games, the smaller the proportion of cooperative social interactions is required for inducing the upper bound on ToM sophistication. Essentially however, with enough learning experience, cooperation would in most cases yield the same evolutionary stable state, namely a mixture of 1-ToM and 2-ToM phenotypes. Effectively, one can thus think of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e061" xlink:type="simple"/></inline-formula> as the most likely upper bound on ToM sophistication.</p>
<p>One may wonder whether our main conclusion still holds if other types of players invade the population. In fact, it may be argued that behavioural responses in the context competitive or cooperative games may be driven by mechanisms that are qualitatively different from ToM. We have thus augmented the pool of possible phenotypes within our population of agents with objectively optimal strategies (i.e. Nash players) and adaptive heuristic behavioural traits (i.e. reinforcement learners). The former phenotype is motivated from game theoretic considerations: playing Nash is typically understood as the average best response (across all types of opponent’s strategies <xref ref-type="bibr" rid="pone.0087619-Rasmusen1">[28]</xref>. The latter phenotype is derived from behaviouristic accounts of decision making: in brief, animal act on the basis of learned action-outcome contingencies. Reinforcement learning (RL) is a celebrated model of such automatic behavioural processes <xref ref-type="bibr" rid="pone.0087619-Sutton1">[29]</xref>. Note that RL generalizes “tit-for-tat” or “win-stay, loose-switch” heuristic strategies, which have been suggested to be of particular importance for explaining the emergence of altruism and cooperation in evolving human societies <xref ref-type="bibr" rid="pone.0087619-Axelrod1">[30]</xref>. <xref ref-type="fig" rid="pone-0087619-g007">Figure 7</xref> depicts the ensuing replicator dynamics phase diagram, having included Nash and RL agents within the set of competing phenotypes.</p>
<fig id="pone-0087619-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0087619.g007</object-id><label>Figure 7</label><caption>
<title>Phase diagram of ToM evolution: Impact of RL and Nash phenotypes.</title>
<p>This figure uses the same format as <xref ref-type="fig" rid="pone-0087619-g006">Fig. 6</xref>.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0087619.g007" position="float" xlink:type="simple"/></fig>
<p>One can see that including Nash and RL agents does not fundamentally change the overall picture. Interestingly, there is no combination of cooperation and learning that make the Nash phenotype evolutionary stable. This is because, even though no other phenotype performs better than Nash on average, ToM phenotypes achieve higher performance when facing each other. Second, only in the context of very short games can the RL phenotype be considered evolutionary stable: RL agents effectively disappear for game durations longer than <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e062" xlink:type="simple"/></inline-formula>. This is actually the only noticeable difference with <xref ref-type="fig" rid="pone-0087619-g006">Figure 6:</xref> short game durations, which were previously dominated by the highest ToM sophistication level (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e063" xlink:type="simple"/></inline-formula>), now yield mixed populations that include RL phenotypes. However, the critical threshold on the amount of cooperation (above which less sophisticated ToM phenotypes dominate) is unchanged. In addition, the nature of evolutionary stable states above this critical threshold seems to be invariant to the presence or absence of non-ToM phenotypes. This includes the induced lower and upper bounds on ToM sophistication levels.</p>
</sec></sec><sec id="s3">
<title>Discussion</title>
<p>In this work, we have proposed a quantitative evolutionary account of ToM sophistication in humans. This relies upon a meta-Bayesian formalism <xref ref-type="bibr" rid="pone.0087619-Daunizeau1">[26]</xref> for recursive ToM inferences that arise in the context of reciprocal social interactions. The key idea here is that meta-bayesian agents learn or recognize the subjective (potentially high-order) beliefs of other agents in a Bayes-optimal fashion. Here, ToM sophistication is defined as the level of recursion of such meta-bayesian agents. We have assessed the relative performance of ToM agents playing competitive or coordinative games with each other. Finally, we have identified what evolutionary forces could have led to the observed variability of ToM sophistication in humans. More precisely, we have shown that: (i) a non-trivial informational cost to sophistication limits the way one can exploit less sophisticated ToM agents, and (ii) one may benefit from engaging in a cooperative interaction with more sophisticated ToM agents. Eventually, these properties yield an evolutionary stable mixture of ToM phenotypes with a lower bound at k = 1 (agents without ToM get extinct) and an upper bound at k = 2.</p>
<p>Our model was largely inspired by previous work from behavioural economics and experimental psychology on bounded rationality. More precisely, <italic>k</italic>-ToM shares with models such as “level k” <xref ref-type="bibr" rid="pone.0087619-Nagel1">[13]</xref> and the “cognitive hierarchy” <xref ref-type="bibr" rid="pone.0087619-Camerer1">[12]</xref> the notion of recursive thinking. These models have been typically used to explain people’s behaviour in non-repeated games such as the “beauty contest” (but see <xref ref-type="bibr" rid="pone.0087619-Stahl1">[14]</xref>, <xref ref-type="bibr" rid="pone.0087619-Camerer3">[31]</xref>–<xref ref-type="bibr" rid="pone.0087619-Hampton1">[32]</xref> for nice extensions to repeated games). They prove useful in capturing inter-individual variability in peoples’ behaviour, in terms of the sophistication of their strategic thinking. For example, Camerer and colleagues <xref ref-type="bibr" rid="pone.0087619-Camerer2">[21]</xref> have reported the following distribution of levels: around 20% of level 0 players, 33% of level 1, 25% of level 2 and then a decreasing proportion of higher levels. Although not identical, such results are consistent with our EGT prediction (cf. the distribution peaks around level 1 and 2). Observed discrepancies may have three distinct causes. First, peoples’ behaviour is not unambiguously mapped onto levels of strategic thinking (cf. issues with levels’ stability across games, etc…). Second, we may not have included all the relevant evolutionary constraints on ToM sophistication (see comment below on comparing ToM across species). Third, there are conceptual differences between <italic>k</italic>-ToM (which deals with the sophistication of learning rules) and the cognitive hierarchy (which cares about the sophistication of behavioural policies). This theoretical difference is not trivial. On the one hand, one could argue that the basic cognitive resource that underlies both processes is the same, namely: the ability to form recursive beliefs. On the other hand, theory of mind is essentially inferential (cf. the intentional stance). That is, ToM is engaged when we identify mental states (beliefs, intentions, emotions, etc…) from social signals (decisions, facial expressions, etc…). In this perspective, ToM may have more to do with the way we adapt to others (through learning) than with the evaluation of the consequences of our actions (decision making).</p>
<p>We will now discuss the limitations of our model.</p>
<p>First, we did not account for social preferences or norms, such as fairness or inequity aversion. These are thought to explain people’s altruistic behaviour despite strong incentive for betrayal, as in the “prisoner’s dilemma” game <xref ref-type="bibr" rid="pone.0087619-Rapoport1">[33]</xref>–<xref ref-type="bibr" rid="pone.0087619-Fehr1">[34]</xref>. However, it turns out that, in these games, meta-Bayesian agents choose the egoistic (dominant) strategy, irrespective of their ToM sophistication level. This means that ToM alone cannot explain people’s altruistic behaviour. Interestingly, a recent study <xref ref-type="bibr" rid="pone.0087619-McNally1">[35]</xref> has used EGT with the iterated “prisoner’s dilemma” to explain the emergence of fairness through evolution. The captivating question of whether ToM’s adaptive fitness depends upon social preferences (and reciprocally) is beyond the scope of the present work. Addressing this would require modelling, e.g. inference on others’ fairness preferences.</p>
<p>Second, our approach shares with similar hierarchical models (such as the “cognitive hierarchy” <xref ref-type="bibr" rid="pone.0087619-Nagel1">[13]</xref>, <xref ref-type="bibr" rid="pone.0087619-Camerer2">[21]</xref>) the relative arbitrariness of the first level. This is critical, because the behavioural response of all subsequent levels in the hierarchy (recursively) rely on the definition of the first level <xref ref-type="bibr" rid="pone.0087619-Colman1">[36]</xref>. Our definition of 0-ToM agents follows from the “Bayesian brain” hypothesis: there is no reason to consider 0-ToM agents that would not learn optimally, aside from their inability to take the “intentional stance”. We believe this is mandatory for evaluating ToM’s adaptive fitness. This is because we do not want the effect of ToM sophistication on behavioural performance to be confounded by differences in, e.g., the principles underlying the way agents learn and decide. Taken together, these considerations constrain the definition of 0-ToM agents. This deserves further comments. It seems to us that it would not make sense to define 0-ToM agents that would be insensitive to feedback (e.g., payoff). This is because there will always exist a broad class of social interactions, in the context of which any such feed-forward system would perform very poorly. In other terms, feed-forward 0-ToM agents would have no evolutionary adaptive fitness. Critically, the feedback’s source is twofold: context (i.e. nature of the interaction -cf. game payoff table-) and opponent (i.e. behavioural tendencies). This is important, because there are not many types of agents that would differ qualitatively in their response to such information. An example of an agent sensitive to the context but not to her opponent is the Nash policy. By construction however, the ensuing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e064" xlink:type="simple"/></inline-formula>-ToM agents would be Nash players as well, and thus ToM sophistication would have no adaptive fitness. In contradistinction, imitative learners are sensitive to their opponent, but not to context. However, the adaptive fitness of such agents is similar to feed-forward agents. Yet another possibility is to consider agents that would respond to an aggregate context-opponent feedback, namely: reward. This is the essence of genuine reinforcement learning (RL) agents. Note that, in terms of behavioural performance, RL agents are comparatively closer to 0-ToM than to any other agent type we have considered (including Nash players; cf. <xref ref-type="fig" rid="pone-0087619-g007">Figure 7</xref>). In fact, this was expected, since there is a linear one-to-one mapping between the value of each option and the opponent’s choice probability. Additionally, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e065" xlink:type="simple"/></inline-formula>-ToM agents (with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e066" xlink:type="simple"/></inline-formula>) have a clear tendency to identify RL players as 0-ToM, at least in a competitive context. This means that we expect our results to be robust to re-defining 0-ToM agents as RL agents. Note that any agent that would be differentially sensitive to context and opponent feedbacks would be formally very similar to our 0-ToM. Taken together, we believe our results would be very robust to admissible changes in the definition of 0-ToM agents.</p>
<p>Third, one may invoke another line of work, which consists in considering that biological costs (such as brain size) induce additional evolutionary forces that eventually limited our cognitive skills <xref ref-type="bibr" rid="pone.0087619-Gavrilets1">[37]</xref>. The weakness of such studies is the lack of specificity: how global features such as brain size relate to different cognitive functions is unclear. In any case, what we have shown is that one does not have to appeal to biological costs to explain our limited ToM sophistication. More generally, one could challenge the very idea that natural selection acted upon ToM sophistication. For example, a radical non-adaptationist scenario would consider that such cognitive phenotypes evolved from random genetic drift. Alternatively, one could argue that ToM sophistication is a by-product of constraints imposed by other cognitive traits (such attention or working memory) that were under selective pressure. Debates about whether or not a given phenotype has been shaped by natural selection are not uncommon in evolutionary biology (cf. e.g., <xref ref-type="bibr" rid="pone.0087619-Gould1">[38]</xref>). In our context, we would appeal to the importance of social cognitive skills in shaping humans’ adaptive fitness <xref ref-type="bibr" rid="pone.0087619-Byrne1">[7]</xref>, <xref ref-type="bibr" rid="pone.0087619-Dunbar1">[8]</xref>. However, we believe that, if properly extended, our work could provide a more satisfactory answer to this question. This is because EGT can be used to predict a specific relationship between features of the ecological niche (here, we considered the proportion of cooperative interactions and the typical amount of learning) and the distribution of ToM sophistication. The key point is that such features can vary across different species. Thus, provided one appropriately captures the critical differences between ecological niches, one could then test the induced variability in ToM sophistication (across species) against the null. We will pursue this in subsequent publications.</p>
<p>Last, one could challenge the fact that we have neglected developmental (and, to a lesser extent maybe, pathological) aspects of ToM <xref ref-type="bibr" rid="pone.0087619-BaronCohen2">[39]</xref>. This is related to the notion of “proximal constraints” of evolution, which relate to the ability of individuals to gradually adopt behavioural strategies that have local adaptive fitness, and are thus positively reinforced by their environment <xref ref-type="bibr" rid="pone.0087619-Montague1">[40]</xref>. Applying the principles of such reinforcement learning theories of motivation <xref ref-type="bibr" rid="pone.0087619-Singh1">[41]</xref> would advocate for considering agents that could change their ToM sophistication level at will. Here, we have rather assumed that ToM sophistication is a phenotype that can hardly be changed or learned over the course of the agent’s life time. However, another way of looking at ToM phenotypes is in terms of an informative prior belief on the population profile of ToM sophistication. Effectively, k-ToM phenotypes can be thought of as agents with unbounded ToM sophistication, who <italic>a priori</italic> believe that their conspecifics’ level of ToM sophistication cannot exceed k-1. This has two implications: (i) one could relax this prior and effectively allow agents to adapt their effective ToM sophistication level, and (ii) one could think of evolution as selecting a very specific form of prior that defines classes of meta-Bayesian agents <xref ref-type="bibr" rid="pone.0087619-Friston2">[42]</xref>.</p>
<p>To conclude, our meta-Bayesian approach unravelled non-trivial properties of inferential aspects of ToM. In particular, the informational cost to sophistication is a key determinant of ToM’s adaptive fitness. Note that this cost might in fact induce strong evolutionary forces for most cognitive processes that can be viewed as inferential in nature, as is the case for, e.g., learning or perception <xref ref-type="bibr" rid="pone.0087619-Friston1">[24]</xref>, <xref ref-type="bibr" rid="pone.0087619-Poggio1">[43]</xref>. This is because, as any ill-posed problem, inference heavily relies upon some form of prior information or belief <xref ref-type="bibr" rid="pone.0087619-Hadamard1">[44]</xref>. Critically, we speculate that the sophistication of such prior eventually matches the complexity of the agent’s ecological niche, because of its inevitable evolutionary cost/benefit ratio.</p>
</sec><sec id="s4">
<title>Models and Methods</title>
<p>In this section, we describe our model of theory of mind in human observers/agents. This model attempts to capture how agents infer on others beliefs and preferences, given a series of observed choices. In <xref ref-type="bibr" rid="pone.0087619-Daunizeau1">[26]</xref>, we exposed a Bayesian solution to the inverse BDT problem (where BDT stands for “Bayesian Decision Theory”). The inverse BDT problem relates to inferring prior beliefs and subjective utility from observed decisions. This meta-bayesian approach enables us to place ToM processes on a solid quantitative footing, which obeys optimality principles. In brief, learning rules unfold from information theory. Here, we extend this approach to account for the fact that agents can differ in terms of the depth of recursivity of their beliefs (cf. “cognitive hierarchy” <xref ref-type="bibr" rid="pone.0087619-Camerer2">[21]</xref>).</p>
<sec id="s4a">
<title>Cooperative and Competitive (Reciprocal) Social Interaction</title>
<p>Note that it is the reciprocal nature of social interaction that induces the potentially infinite recursion of ToM. This is because if my actions cannot influence your environment, what I believe or feel is irrelevant to you, i.e. you do not have to go beyond 0-ToM. Thus, ToM sophistication levels can only be assessed in the context of reciprocal interaction, which is why we adopt a game theoretic formulation of ToM. In its simplest form, a game is defined in terms of a utility table <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e067" xlink:type="simple"/></inline-formula>, which yields the payoff one gets when making decision <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e068" xlink:type="simple"/></inline-formula> while the other player chooses <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e069" xlink:type="simple"/></inline-formula>. Incentives can be arbitrarily chosen to capture different forms of social exchanges or transactions, which makes game theory a very general and powerful tool to analyze the behaviour of people engaged in social interactions <xref ref-type="bibr" rid="pone.0087619-Rasmusen1">[28]</xref>.</p>
<p>We aim at understanding the respective impact of cooperative and competitive (reciprocal) social interactions onto the adaptive fitness of ToM sophistication levels. We thus have to choose appropriate game-theoretic scenarios that capture these types of interaction. Critically however, we chose games whose computational challenge is similar, in the sense that payoff is contingent on how well players predict their opponent’s behaviour.</p>
<p>An ecologically valid proxy for a competition for resources is the game “hide and seek” (also named “matching pennies”), which has already been extensively used in experimental assessments of animal ToM, e.g. food-caching in birds <xref ref-type="bibr" rid="pone.0087619-Clayton1">[45]</xref>. In evolutionary terms, the average payoff of phenotypes playing “hide and seek” can be thought of as a proxy for survival success in the context of competitive social interactions. The version of “hide and seek” we use is a symmetric zero-sum game, whose outcome table is given in <xref ref-type="table" rid="pone-0087619-t001">Table 1</xref> of the main text. For any decision pair <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e070" xlink:type="simple"/></inline-formula>, the gain of the winner is exactly balanced by the loss of the loser, which makes “hide and seek” the simplest of all conflict games. Here, the “hider” wins when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e071" xlink:type="simple"/></inline-formula> and the “seeker” wins when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e072" xlink:type="simple"/></inline-formula>. Its Nash equilibrium is a mixed strategy with probabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e073" xlink:type="simple"/></inline-formula> for both players. This completely random policy is the best strategy against itself, but yields an average payoff of zero. In contradistinction, bilateral deviation from Nash can induce strong bias in the expected outcomes, whereby a given strategy can be exploited by the other one.</p>
<p>“Battle of sexes” is a cooperation game that emulates a dilemma, whereby coordination is only achieved at the cost of one’s subjective preferences <xref ref-type="bibr" rid="pone.0087619-Fudenberg1">[46]</xref>. Interestingly, it is known in the animal literature as “intralocus sexual conflict”: it arises when a trait which is good for the breeding success of one sex is bad for the other <xref ref-type="bibr" rid="pone.0087619-Harano1">[47]</xref>. More generally, the average payoff of phenotypes playing “battle of sexes” can be thought of as a proxy for mating success through (costly) cooperation. We will use a symmetric version of it, whose outcome table is also given in <xref ref-type="table" rid="pone-0087619-t001">Table 1</xref> of the main text. Here, players are most rewarded for coordinated behaviour (i.e., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e074" xlink:type="simple"/></inline-formula>), whereas they are punished when choose different options (i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e075" xlink:type="simple"/></inline-formula>). Note that, in contradistinction with “hide and seek”, payoffs are unbalanced (chance: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e076" xlink:type="simple"/></inline-formula>). Essentially, the game payoffs are such that: (i) if one knew what the other player would do, one would choose to cooperate, and (ii) if one had no idea what the other player would do, one would choose the option that maximizes one’s own preferences. There are two pure Nash equilibria, i.e. either both players choose <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e077" xlink:type="simple"/></inline-formula>, or both players choose <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e078" xlink:type="simple"/></inline-formula>. However, in both situations, one player does better than the other one (unfair outcomes). In addition, there is one Nash mixed strategy, with probabilities <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e079" xlink:type="simple"/></inline-formula> for player 1 and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e080" xlink:type="simple"/></inline-formula> for player 2.</p>
</sec><sec id="s4b">
<title>Meta-Bayesian Agents</title>
<p>According to Bayesian decision theory, agents aim at maximising expected payoff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e081" xlink:type="simple"/></inline-formula>, where the expectation is defined in relation to the agent’s uncertain predictions about his opponent’s next move (see below). Importantly, this implies that the form of the decision policy is the same for all agents, irrespective of their ToM level. In this work, we consider that agent’s choices may exhibit small deviations from the optimal decision rule, i.e. we assume agents employ the so-called “softmax” probabilistic policy:<disp-formula id="pone.0087619.e082"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0087619.e082" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e083" xlink:type="simple"/></inline-formula> is the probability that the agent chooses the action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e084" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e085" xlink:type="simple"/></inline-formula> is the standard sigmoid function and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e086" xlink:type="simple"/></inline-formula> is the exploration temperature that controls the magnitude of behavioural noise. <xref ref-type="disp-formula" rid="pone.0087619.e082">Equation 1</xref> simply says that the probability of choosing the action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e087" xlink:type="simple"/></inline-formula> increases with its expected payoff <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e088" xlink:type="simple"/></inline-formula>. Here, the critical variable is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e089" xlink:type="simple"/></inline-formula>: the probability that the opponent will choose the action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e090" xlink:type="simple"/></inline-formula>.</p>
<p>The repeated observation of his opponent’s behaviour <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e091" xlink:type="simple"/></inline-formula> gives the agent the opportunity to learn this prediction. Theory of Mind comes into play when agents consider that the opponent’s behavioural tendency <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e092" xlink:type="simple"/></inline-formula> is motivated by his hidden beliefs and desires. More precisely, our “social Bayesian brain” hypothesis implies that ToM agents consider that the opponent is himself a Bayesian agent, whose decision policy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e093" xlink:type="simple"/></inline-formula> is formally similar to <xref ref-type="disp-formula" rid="pone.0087619.e082">Equation 1</xref>. In this situation, one has to track one’s opponent’s prediction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e094" xlink:type="simple"/></inline-formula> about one’s own actions. This makes ToM agents <italic>meta</italic>-Bayesian agents <xref ref-type="bibr" rid="pone.0087619-Daunizeau1">[26]</xref>, i.e. Bayesian observers of Bayesian agents. In line with the notion of cognitive hierarchy <xref ref-type="bibr" rid="pone.0087619-Camerer2">[21]</xref>, this meta-Bayesian inference is recursive (“I think that you think that I think…”). The recursion depth induces different ToM levels, which differ in how they update their subjective prediction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e095" xlink:type="simple"/></inline-formula>.</p>
<p>In analogy to Yoshida et al. <xref ref-type="bibr" rid="pone.0087619-Yoshida1">[22]</xref>, we thus define ToM levels (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e096" xlink:type="simple"/></inline-formula>-ToM agents) in terms of the way they learn from their opponent’s behaviour, starting with 0-ToM. By convention, a 0-ToM agent does not attribute mental states to his opponent. More precisely, 0-ToM agents assume that their opponents choose the action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e097" xlink:type="simple"/></inline-formula> with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e098" xlink:type="simple"/></inline-formula>, where the log-odds <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e099" xlink:type="simple"/></inline-formula> varies across trials <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e100" xlink:type="simple"/></inline-formula> with a certain volatility <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e101" xlink:type="simple"/></inline-formula> (and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e102" xlink:type="simple"/></inline-formula> is the sigmoid function). Observing his opponent’s choices gives 0-ToM information about the hidden state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e103" xlink:type="simple"/></inline-formula>, whose estimate is updated trial after trial. Under these premises, one can derive 0-ToM’s Bayesian learning rule, in terms of the change in his prediction about his opponent’s next move (see <xref ref-type="supplementary-material" rid="pone.0087619.s001">Text S1</xref>):<disp-formula id="pone.0087619.e104"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0087619.e104" xlink:type="simple"/><label>(2)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e105" xlink:type="simple"/></inline-formula> (resp. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e106" xlink:type="simple"/></inline-formula>) is the mean (resp. the variance) of 0-ToM’s posterior distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e107" xlink:type="simple"/></inline-formula> on the log-odds <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e108" xlink:type="simple"/></inline-formula>, having observed his opponent’s behaviour up to trial <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e109" xlink:type="simple"/></inline-formula>. In other words, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e110" xlink:type="simple"/></inline-formula> is 0-ToM’s estimate of the log-odds at trial <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e111" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e112" xlink:type="simple"/></inline-formula> is his subjective uncertainty about it. Inserting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e113" xlink:type="simple"/></inline-formula> into <xref ref-type="disp-formula" rid="pone.0087619.e082">Equation 1</xref> now yields 0-ToM’s decision rule. Note that the term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e114" xlink:type="simple"/></inline-formula> can be thought of as a prediction error, whose impact on learning accounts for changes in the subjective uncertainty <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e115" xlink:type="simple"/></inline-formula>. Here, the effective learning rate is controlled by the volatility <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e116" xlink:type="simple"/></inline-formula>, which captures 0-ToM’s priors (see <xref ref-type="bibr" rid="pone.0087619-Mathys1">[48]</xref> for a hierarchical generalization, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e117" xlink:type="simple"/></inline-formula> is learned as well). At the limit <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e118" xlink:type="simple"/></inline-formula>, <xref ref-type="disp-formula" rid="pone.0087619.e104">Equation 2</xref> converges towards the opponent’s choice frequency and <xref ref-type="disp-formula" rid="pone.0087619.e082">Equations 1</xref>–<xref ref-type="disp-formula" rid="pone.0087619.e104">2</xref> essentially reproduce “fictitious play” strategies <xref ref-type="bibr" rid="pone.0087619-Berger1">[49]</xref>.</p>
<p>Taken together, <xref ref-type="disp-formula" rid="pone.0087619.e082">Equations 1</xref>–<xref ref-type="disp-formula" rid="pone.0087619.e104">2</xref> describe how 0-ToM agents learn and decide, trial by trial. This is the starting point for a 1-ToM agent, who considers that she is facing a 0-ToM agent. This means that 1-ToM has to predict 0-ToM’s next move, given his beliefs and the choices’ payoffs. The issue here is that 0-ToM’s priors (as well as his exploration temperature) are unknown to 1-ToM and have to be learned, through their non-trivial effect on 0-ToM’s choices. More precisely, 1-ToM agents assume that 0-ToM chooses the action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e119" xlink:type="simple"/></inline-formula> with probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e120" xlink:type="simple"/></inline-formula>, where the hidden states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e121" xlink:type="simple"/></inline-formula> lumps <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e122" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e123" xlink:type="simple"/></inline-formula> together and the mapping <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e124" xlink:type="simple"/></inline-formula> is derived from inserting <xref ref-type="disp-formula" rid="pone.0087619.e104">Equation 2</xref> into <xref ref-type="disp-formula" rid="pone.0087619.e082">Equation 1</xref>. Similarly to 0-ToM agents, 1-ToM assume that the hidden states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e125" xlink:type="simple"/></inline-formula> vary across trials with a certain volatility <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e126" xlink:type="simple"/></inline-formula>, which yields a meta-Bayesian learning rule similar in form to <xref ref-type="disp-formula" rid="pone.0087619.e104">Equation 2</xref> (see below).</p>
<p>More generally, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e127" xlink:type="simple"/></inline-formula>-ToM agents (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e128" xlink:type="simple"/></inline-formula>) consider that their opponent is a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e129" xlink:type="simple"/></inline-formula>-ToM agent with a lower ToM sophistication level (i.e.: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e130" xlink:type="simple"/></inline-formula>). Importantly, the sophistication level <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e131" xlink:type="simple"/></inline-formula> of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e132" xlink:type="simple"/></inline-formula>-ToM’s opponent has to be learned, in addition to the hidden states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e133" xlink:type="simple"/></inline-formula> that control the opponent’s learning and decision making. The difficulty for a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e134" xlink:type="simple"/></inline-formula>-ToM agent is that she needs to consider different scenarios: each of her opponent’s possible sophistication level <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e135" xlink:type="simple"/></inline-formula> yields a specific probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e136" xlink:type="simple"/></inline-formula> that she will choose action <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e137" xlink:type="simple"/></inline-formula>.</p>
<p>We will now show how to derive the learning rule of a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e138" xlink:type="simple"/></inline-formula>-ToM (meta-Bayesian) observer, who takes the intentional stance when interpreting the behaviour of his <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e139" xlink:type="simple"/></inline-formula>-ToM opponent. Below, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e140" xlink:type="simple"/></inline-formula> (resp. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e141" xlink:type="simple"/></inline-formula>) denotes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e142" xlink:type="simple"/></inline-formula>-ToM’s action (resp. his <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e143" xlink:type="simple"/></inline-formula>-ToM opponent’s action). Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e144" xlink:type="simple"/></inline-formula> be the set of sufficient statistics that parameterize the (probabilistic) belief of a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e145" xlink:type="simple"/></inline-formula>-ToM observer at trial <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e146" xlink:type="simple"/></inline-formula> of the repeated game. Typically, the states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e147" xlink:type="simple"/></inline-formula> include the first- and second-order moments of the conditional probability density on the uncertain (hidden) state of his opponent (e.g., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e148" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pone.0087619.e104">Equation 2</xref> above). As <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e149" xlink:type="simple"/></inline-formula>-ToM learns, her belief evolves from trial to trial according to Bayes’ rule, which can be summarized as the change in the states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e150" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pone.0087619-Daunizeau1">[26]</xref>:<disp-formula id="pone.0087619.e151"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0087619.e151" xlink:type="simple"/><label>(3)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e152" xlink:type="simple"/></inline-formula> are the players’ action at trial <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e153" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e154" xlink:type="simple"/></inline-formula> is a set of parameters that encode <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e155" xlink:type="simple"/></inline-formula>-ToM’s priors. For example, the belief evolution function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e156" xlink:type="simple"/></inline-formula> of 0-ToM is given in <xref ref-type="disp-formula" rid="pone.0087619.e104">Equation 2</xref>. Both the derivation and the explicit form of the belief evolution function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e157" xlink:type="simple"/></inline-formula> will become clearer below. At this point, suffices to say that the dynamics of belief sufficient statistics <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e158" xlink:type="simple"/></inline-formula> is controlled by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e159" xlink:type="simple"/></inline-formula>-ToM’s priors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e160" xlink:type="simple"/></inline-formula>. Recall that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e161" xlink:type="simple"/></inline-formula>-ToM’s belief serves to make a prediction <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e162" xlink:type="simple"/></inline-formula> about her own move <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e163" xlink:type="simple"/></inline-formula> at the next trial. This then enters <xref ref-type="disp-formula" rid="pone.0087619.e082">Equation 1</xref> to yield <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e164" xlink:type="simple"/></inline-formula>-ToM’s softmax decision policy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e165" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e166" xlink:type="simple"/></inline-formula> depends upon <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e167" xlink:type="simple"/></inline-formula>-ToM’s priors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e168" xlink:type="simple"/></inline-formula> through her posterior belief’s sufficient statistics <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e169" xlink:type="simple"/></inline-formula>. Let us first assume that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e170" xlink:type="simple"/></inline-formula>-ToM knows his opponent’s sophistication level <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e171" xlink:type="simple"/></inline-formula>, e.g. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e172" xlink:type="simple"/></inline-formula>. Taken together, <xref ref-type="disp-formula" rid="pone.0087619.e082">Equations 1</xref> and <xref ref-type="disp-formula" rid="pone.0087619.e151">3</xref> then induce the following (Bernouilli) likelihood function for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e173" xlink:type="simple"/></inline-formula>-ToM’s actions sequence, <italic>from the perspective of </italic><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e174" xlink:type="simple"/></inline-formula><italic>-ToM</italic>:<disp-formula id="pone.0087619.e175"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0087619.e175" xlink:type="simple"/><label>(4)</label></disp-formula>where where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e176" xlink:type="simple"/></inline-formula> stands for all actions up to trial <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e177" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e178" xlink:type="simple"/></inline-formula> is derives from inserting <xref ref-type="disp-formula" rid="pone.0087619.e151">Equation 3</xref> into <xref ref-type="disp-formula" rid="pone.0087619.e082">Equation 1</xref>. <xref ref-type="disp-formula" rid="pone.0087619.e175">Equation 4</xref> measures how likely is any particular history of choices up to trial <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e179" xlink:type="simple"/></inline-formula>, given <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e180" xlink:type="simple"/></inline-formula>-ToM’s (unknown) properties <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e181" xlink:type="simple"/></inline-formula>, having fixed her sophistication level to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e182" xlink:type="simple"/></inline-formula>. In fact, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e183" xlink:type="simple"/></inline-formula>-ToM does not know the level <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e184" xlink:type="simple"/></inline-formula> of her opponent. Without loss of generality, the complete likelihood of the actions sequence of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e185" xlink:type="simple"/></inline-formula>-ToM’s opponent can thus be written as the following mixture:<disp-formula id="pone.0087619.e186"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0087619.e186" xlink:type="simple"/><label>(5)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e187" xlink:type="simple"/></inline-formula> lumps the volatility and temperature of all possible sophistication levels of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e188" xlink:type="simple"/></inline-formula>-ToM’s opponent, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e189" xlink:type="simple"/></inline-formula> is the indicator vector of the opponent’s ToM level <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e190" xlink:type="simple"/></inline-formula> (i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e191" xlink:type="simple"/></inline-formula> is a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e192" xlink:type="simple"/></inline-formula> null vector, except <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e193" xlink:type="simple"/></inline-formula>). Note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e194" xlink:type="simple"/></inline-formula>-ToM’s generative model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e195" xlink:type="simple"/></inline-formula> includes the above likelihood function, as well as priors <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e196" xlink:type="simple"/></inline-formula> on his opponent’s ToM sophistication level <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e197" xlink:type="simple"/></inline-formula> and the observation/evolution parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e198" xlink:type="simple"/></inline-formula> for all levels <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e199" xlink:type="simple"/></inline-formula>. At each trial <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e200" xlink:type="simple"/></inline-formula>, these likelihood and priors induce a free energy bound <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e201" xlink:type="simple"/></inline-formula> on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e202" xlink:type="simple"/></inline-formula>-ToM’s (log-) evidence <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e203" xlink:type="simple">
</inline-graphic></inline-formula> of his opponent’s behaviour:<disp-formula id="pone.0087619.e204"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0087619.e204" xlink:type="simple"/><label>(6)</label></disp-formula>where the expectation is taken under <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e205" xlink:type="simple"/></inline-formula>, the conditional density on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e206" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e207" xlink:type="simple"/></inline-formula> at trial <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e208" xlink:type="simple"/></inline-formula>, which captures <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e209" xlink:type="simple"/></inline-formula>-ToM’s posterior belief on her opponent’s properties. The second line of <xref ref-type="disp-formula" rid="pone.0087619.e204">Equation 6</xref> derives from the factorization of the likelihood across time or trials (cf. <xref ref-type="disp-formula" rid="pone.0087619.e175">Equation 4</xref>). Variational Bayesian update rules follow from optimizing the free energy with respect to the conditional density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e210" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pone.0087619-Beal1">[50]</xref>. Without any additional constraint, this yields Bayes rule, i.e.:</p>
<p><disp-formula id="pone.0087619.e211"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0087619.e211" xlink:type="simple"/><label>(7)</label></disp-formula><xref ref-type="disp-formula" rid="pone.0087619.e211">Equation 7</xref> describes Bayesian (on line) recognition or learning, i.e. how the previous belief <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e212" xlink:type="simple"/></inline-formula> at trial <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e213" xlink:type="simple"/></inline-formula> is updated to yield <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e214" xlink:type="simple"/></inline-formula>, after having observed the opponent’s choice <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e215" xlink:type="simple"/></inline-formula> at trial <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e216" xlink:type="simple"/></inline-formula>. <xref ref-type="disp-formula" rid="pone.0087619.e211">Equation 7</xref> obtains because maximizing the free energy with respect to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e217" xlink:type="simple"/></inline-formula> indirectly minimizes the Kullback-Leibler divergence between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e218" xlink:type="simple"/></inline-formula> and the posterior density. This means that, without loss of generality, we can rewrite Bayes’ rule in terms of the trial-to-trial evolution of the sufficient statistics <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e219" xlink:type="simple"/></inline-formula> of the time-dependent conditional density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e220" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pone.0087619-Daunizeau1">[26]</xref>:<disp-formula id="pone.0087619.e221"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0087619.e221" xlink:type="simple"/><label>(8)</label></disp-formula>where we have introduced <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e222" xlink:type="simple"/></inline-formula>, the set of variables that parameterize e.g., <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e223" xlink:type="simple"/></inline-formula><italic>-</italic>ToM’s prior belief <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e224" xlink:type="simple"/></inline-formula> on his opponent (see Appendix 1). In <xref ref-type="disp-formula" rid="pone.0087619.e221">Equation 8</xref>, the form of the evolution function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e225" xlink:type="simple"/></inline-formula> is determined by the Free Energy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e226" xlink:type="simple"/></inline-formula>, which derives from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e227" xlink:type="simple"/></inline-formula><italic>-</italic>ToM’s generative model <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e228" xlink:type="simple"/></inline-formula>. The appeal of this variational formulation is that, under some simplifying assumption about the form of the approximate posterior <xref ref-type="bibr" rid="pone.0087619-Friston3">[51]</xref>, Bayesian learning becomes analytic. In brief, we have shown how to derive the learning rule of any ToM sophistication level recursively, i.e. from that of the level above. Except for 0-ToM agents, the belief evolution function has the following form (<xref ref-type="supplementary-material" rid="pone.0087619.s001">Text S1</xref> for derivation):<disp-formula id="pone.0087619.e229"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0087619.e229" xlink:type="simple"/><label>(9)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e230" xlink:type="simple"/></inline-formula> are the sufficient statistics of the time-dependent conditional density <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e231" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e232" xlink:type="simple"/></inline-formula> is a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e233" xlink:type="simple"/></inline-formula> vector composed of the sigmoid observation mappings for each potential ToM sophistication level of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e234" xlink:type="simple"/></inline-formula>-ToM’s opponent (cf. <xref ref-type="disp-formula" rid="pone.0087619.e151">Equation 3</xref>), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e235" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e236" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e237" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e238" xlink:type="simple"/></inline-formula> are analytic (matrix and vector) functions of the two moments <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e239" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e240" xlink:type="simple"/></inline-formula>. More precisely, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e241" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e242" xlink:type="simple"/></inline-formula> are the first- and second-order moments of the probabilistic belief on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e243" xlink:type="simple"/></inline-formula>, whereas <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e244" xlink:type="simple"/></inline-formula> is the first-order moment of the probabilistic belief on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e245" xlink:type="simple"/></inline-formula> (i.e.: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e246" xlink:type="simple"/></inline-formula>). Although <xref ref-type="disp-formula" rid="pone.0087619.e229">Equation 9</xref> is slightly more complex than <xref ref-type="disp-formula" rid="pone.0087619.e104">Equation 2</xref>, note that learning is still driven by a simple prediction error term. More precisely, one can see that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e247" xlink:type="simple"/></inline-formula>-ToM’s prediction error <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e248" xlink:type="simple"/></inline-formula> drives the change in her belief sufficient statistics <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e249" xlink:type="simple"/></inline-formula>. Critically however, this prediction error is weighted by her current belief about her opponent’s sophistication level. <xref ref-type="disp-formula" rid="pone.0087619.e229">Equation 9</xref> is but a compact formulation of how the summary statistics (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e250" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e251" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e252" xlink:type="simple"/></inline-formula>) of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e253" xlink:type="simple"/></inline-formula>-ToM’s posterior distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e254" xlink:type="simple"/></inline-formula> evolve from trial to trial. Both <xref ref-type="disp-formula" rid="pone.0087619.e104">Equations 2</xref> and <xref ref-type="disp-formula" rid="pone.0087619.e229">9</xref> have been derived using a variational approach to approximate Bayesian inference <xref ref-type="bibr" rid="pone.0087619-Friston3">[51]</xref>–<xref ref-type="bibr" rid="pone.0087619-Daunizeau2">[52]</xref>. We refer the interested reader to the <xref ref-type="supplementary-material" rid="pone.0087619.s001">Text S1</xref>.</p>
<p><xref ref-type="disp-formula" rid="pone.0087619.e151">Equation 3</xref> concludes the exposition of our meta-bayesian model of ToM agents. In brief, we have defined ToM sophistication levels recursively, in terms of their respective (social) learning rule. A critical feature of this meta-Bayesian model is that the complexity of the scenarios that a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e255" xlink:type="simple"/></inline-formula>-ToM agent uses to learn increases with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e256" xlink:type="simple"/></inline-formula>. This means that the relative performance of different ToM sophistication levels playing against each other is non-trivial and cannot be evaluated without resorting to computational simulations.</p>
</sec><sec id="s4c">
<title>The Adaptive Fitness of ToM Sophistication Levels</title>
<p>Recall that the adaptive fitness results from the relative behavioural performance of competing phenotypes, which proxies their ability to survive and reproduce <xref ref-type="bibr" rid="pone.0087619-Day1">[53]</xref>. Critically, we view ToM levels as social learning phenotypes that compete with each other (in a Darwinian sense). This differs from standard EGT models, in which phenotypes are defined in terms of their decision policy or strategy (e.g. playing “tit for tat” in the prisoner’s dilemma, <xref ref-type="bibr" rid="pone.0087619-Axelrod1">[30]</xref>, <xref ref-type="bibr" rid="pone.0087619-Nowak1">[54]</xref>. However, this does not invalidate the use of standard EGT replicator dynamics. These describe the evolution of the frequency distribution of competing phenotypes over evolutionary time, given how well they perform when interacting with each other <xref ref-type="bibr" rid="pone.0087619-Hofbauer1">[55]</xref>. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e257" xlink:type="simple"/></inline-formula> be the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e258" xlink:type="simple"/></inline-formula> game-dependent expected payoff matrix after <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e259" xlink:type="simple"/></inline-formula> repetitions, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e260" xlink:type="simple"/></inline-formula> is the maximum ToM sophistication level within the (human) population. The matrix element <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e261" xlink:type="simple"/></inline-formula> is the expected payoff of a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e262" xlink:type="simple"/></inline-formula>-ToM agent playing against a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e263" xlink:type="simple"/></inline-formula>-ToM agent. It is obtained by first integrating the system of coupled ToM agents, i.e. iterating forward in time the learning (<xref ref-type="disp-formula" rid="pone.0087619.e104">Equation 2</xref> or 3) and decision (<xref ref-type="disp-formula" rid="pone.0087619.e082">Equation 1</xref>) rules up to trial <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e264" xlink:type="simple"/></inline-formula>, and then measuring the accumulated payoff for each player. The expected payoff is then defined as the Monte-Carlo average of the accumulated payoff over multiple repetitions of the iterated game, where games may yield different outcomes due to the probabilistic nature of the decision policy. On average (across games), the payoff matrix that summarizes the pairwise interaction of individuals is: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e265" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e266" xlink:type="simple"/></inline-formula> is the number of game repetitions and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0087619.e267" xlink:type="simple"/></inline-formula> is the probability, for any pair of agents, to engage in a cooperative social interaction. We inserted this average payoff matrix in replicator dynamics to derive the ToM evolutionary stable states. We refer the interested reader to <xref ref-type="supplementary-material" rid="pone.0087619.s001">Text S1</xref> for details regarding our implementation of EGT.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pone.0087619.s001" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" xlink:href="info:doi/10.1371/journal.pone.0087619.s001" position="float" xlink:type="simple"><label>Text S1</label><caption>
<p>This note provides technical details about the derivation of the learning rule of ToM agents and our application of Evolutionary Game Theory (EGT) to ToM sophistication phenotypes.</p>
<p>(DOCX)</p>
</caption></supplementary-material></sec></body>
<back><ref-list>
<title>References</title>
<ref id="pone.0087619-Premack1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Premack</surname><given-names>DG</given-names></name>, <name name-style="western"><surname>Woodruff</surname><given-names>G</given-names></name> (<year>1978</year>) <article-title>Does the chimpanzee have a theory of mind? Behav. Brain Sci</article-title>. <volume>1(4)</volume>: <fpage>515</fpage>–<lpage>526</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Kovcs1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kovács</surname><given-names>ÁM</given-names></name>, <name name-style="western"><surname>Téglás</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Endress</surname><given-names>AD</given-names></name> (<year>2010</year>) <article-title>The Social Sense: Susceptibility to Others’ Beliefs in Human Infants and Adults</article-title>. <source>Science</source> <volume>330</volume>: <fpage>1830</fpage>–<lpage>1834</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Frith1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Frith</surname><given-names>U</given-names></name>, <name name-style="western"><surname>Happé</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Siddons</surname><given-names>F</given-names></name> (<year>1994</year>) <article-title>Autism and theory of mind in everyday life</article-title>. <source>Social Development</source> <volume>2</volume>: <fpage>108</fpage>–<lpage>124</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Brne1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brüne</surname><given-names>M</given-names></name> (<year>2005</year>) <article-title>“Theory of Mind” in Schizophrenia: A Review of the Literature</article-title>. <source>Schizophr Bull</source> <volume>31(1)</volume>: <fpage>21</fpage>–<lpage>42</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-BaronCohen1"><label>5</label>
<mixed-citation publication-type="other" xlink:type="simple">Baron-Cohen S (1999) The evolution of a theory of mind. In M. C. Corballis &amp; S. E. G. Lea (Eds.), <italic>The descent of mind: Psychological perspectives on hominid evolution</italic> (pp. 261–277). New York: Oxford University Press.</mixed-citation>
</ref>
<ref id="pone.0087619-Emery1"><label>6</label>
<mixed-citation publication-type="other" xlink:type="simple">Emery NJ (2005) The evolution of social cognition. <italic>Cognitive Neuroscience of Social Behaviour</italic>, eds Easton A, Emery NJ (Psychology Press, Hove, UK).</mixed-citation>
</ref>
<ref id="pone.0087619-Byrne1"><label>7</label>
<mixed-citation publication-type="other" xlink:type="simple">Byrne C, Whiten A (1988) <italic>Machiavellian intelligence</italic>. (Oxford Univ Press, Oxford).</mixed-citation>
</ref>
<ref id="pone.0087619-Dunbar1"><label>8</label>
<mixed-citation publication-type="other" xlink:type="simple">Dunbar RI (1998) The social brain hypothesis. Evol Anthropol 6(5) 178–190.</mixed-citation>
</ref>
<ref id="pone.0087619-Malle1"><label>9</label>
<mixed-citation publication-type="other" xlink:type="simple">Malle BF (2002) The relation between language and theory of mind in development and evolution. <italic>The evolution of language out of pre-language,</italic> eds Givon? T? and Malle BF (Benjamins, Amsterdam). 265–284.</mixed-citation>
</ref>
<ref id="pone.0087619-Moll1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moll</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Tomasello</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>Cooperation and human cognition: The Vygotskian intelligence hypothesis. Phil. Trans. Roy. Soc</article-title>. <source>B</source> <volume>362</volume>: <fpage>639</fpage>–<lpage>648</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Simon1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Simon</surname><given-names>H</given-names></name> (<year>1991</year>) <article-title>Bounded Rationality and Organizational Learning. Organizat. Sci</article-title>. <volume>2(1)</volume>: <fpage>125</fpage>–<lpage>134</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Camerer1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Camerer</surname><given-names>C</given-names></name> (<year>2003b</year>) <article-title>Behavioural studies of strategic thinking in games</article-title>. <source>Trends Cog Sci</source> <volume>7</volume>: <fpage>225</fpage>–<lpage>231</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Nagel1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nagel</surname><given-names>R</given-names></name> (<year>1995</year>) <article-title>Unraveling in Guessing Games: An Experimental Study. The American Econ. Rev</article-title>. <volume>85(5)</volume>: <fpage>1313</fpage>–<lpage>1326</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Stahl1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stahl</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Wilson</surname><given-names>P</given-names></name> (<year>1995</year>) <article-title>On Players’ Models of Other Players: Theory and Experimental Evidence</article-title>. <source>Games and Econ Behavior</source> <volume>10</volume>: <fpage>218</fpage>–<lpage>254</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Hedden1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hedden</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>J</given-names></name> (<year>2002</year>) <article-title>What do you think I think you think?: Strategic reasoning in matrix games</article-title>. <source>Cognition</source> <volume>85(1)</volume>: <fpage>1</fpage>–<lpage>36</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Kinderman1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kinderman</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Dunbar</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Bentall</surname><given-names>R</given-names></name> (<year>1998</year>) <article-title>Theory-of-mind deficits and causal attributions</article-title>. <source>Brit J Psy</source> <volume>89(2)</volume>: <fpage>191</fpage>–<lpage>204</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Dunbar2"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dunbar</surname><given-names>RI</given-names></name>, <name name-style="western"><surname>Duncan</surname><given-names>NDC</given-names></name>, <name name-style="western"><surname>Nettle</surname><given-names>D</given-names></name> (<year>1995</year>) <article-title>Size and structure of freely forming conversational groups. Human Nat</article-title>. <source>6(1)</source> <volume>67</volume>: <fpage>78</fpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Zunshine1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zunshine</surname><given-names>L</given-names></name> (<year>2007</year>) <article-title>Why Jane Austen was different, and why we need cognitive science to see it</article-title>. <source>Style</source> <volume>41(3)</volume>: <fpage>275</fpage>–<lpage>299</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Hauser1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hauser</surname><given-names>MD</given-names></name>, <name name-style="western"><surname>Chomsky</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Fitch</surname><given-names>WT</given-names></name> (<year>2002</year>) <article-title>The faculty of language: what it is, who has it, and how did it evolve?</article-title> <source>Science</source> <volume>298</volume>: <fpage>1569</fpage>–<lpage>1579</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Corballis1"><label>20</label>
<mixed-citation publication-type="other" xlink:type="simple">Corballis MC (2007) The evolution of language: from hand to mouth. Platek, S. M., Keenan, J. P., &amp; Shackelford, T. K. (Eds). <italic>Evolutionary Cognitive Neuroscience</italic>. MIT Press: Cambridge MA.</mixed-citation>
</ref>
<ref id="pone.0087619-Camerer2"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Camerer</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Ho</surname><given-names>TH</given-names></name>, <name name-style="western"><surname>Chong</surname><given-names>JK</given-names></name> (<year>2004</year>) <article-title>A cognitive hierarchy model of games</article-title>. <source>Q J Econ</source> <volume>119(3)</volume>: <fpage>861</fpage>–<lpage>898</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Yoshida1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yoshida</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Dolan</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name> (<year>2008</year>) <article-title>Game theory of mind</article-title>. <source>PLoS Comp Biol</source> <volume>4(12)</volume>: <fpage>e1000254</fpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Dennett1"><label>23</label>
<mixed-citation publication-type="other" xlink:type="simple">Dennett DC (1996) <italic>The Intentional Stance</italic> (6th printing), (MIT Press, Cambridge, MA), ISBN 0-262-54053-3.</mixed-citation>
</ref>
<ref id="pone.0087619-Friston1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name> (<year>2010</year>) <article-title>The free-energy principle: a unified brain theory? Nat. Rev. Neurosci</article-title>. <volume>11</volume>: <fpage>127</fpage>–<lpage>138</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Tenenbaum1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Kemp</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Griffiths</surname><given-names>TL</given-names></name>, <name name-style="western"><surname>Goodman</surname><given-names>ND</given-names></name> (<year>2011</year>) <article-title>How to grow a mind: statistics, structure and abstraction</article-title>. <source>Science</source> <volume>331</volume>: <fpage>1279</fpage>–<lpage>1285</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Daunizeau1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Den Ouden</surname><given-names>HEM</given-names></name>, <name name-style="western"><surname>Pessiglione</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Stephan</surname><given-names>KE</given-names></name>, <name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name>, <etal>et al</etal>. (<year>2010a</year>) <article-title>Observing the observer (I): meta-Bayesian models of learning and decision-making</article-title>. <source>PLoS ONE</source> <volume>5(12)</volume>: <fpage>e15554</fpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-MaynardSmith1"><label>27</label>
<mixed-citation publication-type="other" xlink:type="simple">Maynard-Smith J (1982) <italic>Evolution and the theory of games</italic>, (Cambridge Univ Press, Cambridge, UK).</mixed-citation>
</ref>
<ref id="pone.0087619-Rasmusen1"><label>28</label>
<mixed-citation publication-type="other" xlink:type="simple">Rasmusen E (2006) <italic>Games and Information: An Introduction to Game Theory (4th ed.)</italic>, Wiley-Blackwell, ISBN 978-1-4051-3666-2.</mixed-citation>
</ref>
<ref id="pone.0087619-Sutton1"><label>29</label>
<mixed-citation publication-type="other" xlink:type="simple">Sutton R, Barto A (1998) <italic>Reinforcement Learning</italic>. MIT Press. ISBN 0-585-02445-6.</mixed-citation>
</ref>
<ref id="pone.0087619-Axelrod1"><label>30</label>
<mixed-citation publication-type="other" xlink:type="simple">Axelrod R (1984). <italic>The Evolution of Cooperation</italic>. Basic Books. ISBN 0-465-02121-2.</mixed-citation>
</ref>
<ref id="pone.0087619-Camerer3"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Camerer</surname><given-names>CF</given-names></name>, <name name-style="western"><surname>Ho</surname><given-names>TH</given-names></name>, <name name-style="western"><surname>Chong</surname><given-names>JK</given-names></name> (<year>2002</year>) <article-title>Sophisticated Experience-Weighted Attraction Learning and Strategic Teaching in Repeated Games. J. Econ</article-title>. <source>Theory</source> <volume>104(1)</volume>: <fpage>137</fpage>–<lpage>188</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Hampton1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hampton</surname><given-names>AN</given-names></name>, <name name-style="western"><surname>Bossaerts</surname><given-names>P</given-names></name>, <name name-style="western"><surname>O’Doherty</surname><given-names>JP</given-names></name> (<year>2008</year>) <article-title>Neural correlates of mentalizing-related computations during strategic interactions in humans</article-title>. <source>PNAS</source> <volume>105(18)</volume>: <fpage>6741</fpage>–<lpage>6</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Rapoport1"><label>33</label>
<mixed-citation publication-type="other" xlink:type="simple">Rapoport A, Chammah AM (1965) <italic>Prisoner’s Dilemma.</italic> (Univ. Michigan Press).</mixed-citation>
</ref>
<ref id="pone.0087619-Fehr1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fehr</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Schmidt</surname><given-names>KM</given-names></name> (<year>1999</year>) <article-title>A theory of fairness, competition and cooperation. Q. J. Econ</article-title>. <volume>114</volume>: <fpage>817</fpage>–<lpage>868</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-McNally1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McNally</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Brown</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Jackson</surname><given-names>A</given-names></name> (<year>2012</year>) <article-title>Cooperation and the evolution of intelligence</article-title>. <source>Proc R Soc B</source> <volume>279(1740)</volume>: <fpage>3027</fpage>–<lpage>34</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Colman1"><label>36</label>
<mixed-citation publication-type="other" xlink:type="simple">Colman AM (2003). Depth of strategic reasoning in games. Trends Cog Sci 7(1).</mixed-citation>
</ref>
<ref id="pone.0087619-Gavrilets1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gavrilets</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Vose</surname><given-names>A</given-names></name> (<year>2003</year>) <article-title>The dynamics of Machiavellian intelligence, Proc Nat Acad Sci USA</article-title>. <volume>103(45)</volume>: <fpage>16823</fpage>–<lpage>16828</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Gould1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gould</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Lewontin</surname><given-names>RC</given-names></name> (<year>1979</year>) <article-title>The Spandrels of San Marco and the Panglossian Paradigm: A Critique of the Adaptationist Programme. Proc. Roy. Soc</article-title>. <source>B</source> <volume>205(1161)</volume>: <fpage>581</fpage>–<lpage>598</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-BaronCohen2"><label>39</label>
<mixed-citation publication-type="other" xlink:type="simple">Baron-Cohen S (1991) Precursors to a theory of mind: Understanding attention in others. <italic>Natural theories of mind: Evolution, development and simulation of everyday mindreading,</italic> ed Whiten A (Basil Blackwell, Oxford), 233–251.</mixed-citation>
</ref>
<ref id="pone.0087619-Montague1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Montague</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Dolan</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name> (<year>2012</year>) <article-title>Computational psychiatry</article-title>. <source>Trends in Cog Sci</source> <volume>16(1)</volume>: <fpage>72</fpage>–<lpage>80</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Singh1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Singh</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Lewis</surname><given-names>RL</given-names></name>, <name name-style="western"><surname>Barto</surname><given-names>AG</given-names></name>, <name name-style="western"><surname>Sorg</surname><given-names>J</given-names></name> (<year>2010</year>) <article-title>Intrinsically motivated reinforcement learning: an evolutionary perspective</article-title>. <source>IEEE Trans Autonom Ment Dev</source> <volume>2</volume>: <fpage>70</fpage>–<lpage>82</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Friston2"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Kilner</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name> (<year>2010</year>) <article-title>Action and behaviour: a free energy formulation. Bio. Cybern</article-title>. <volume>102</volume>: <fpage>227</fpage>–<lpage>260</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Poggio1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Koch</surname><given-names>C</given-names></name> (<year>1985</year>) <article-title>Ill posed problems in early vision : from computational theory to analogue networks</article-title>. <source>Proc R Soc London B</source> <volume>226</volume>: <fpage>303</fpage>–<lpage>323</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Hadamard1"><label>44</label>
<mixed-citation publication-type="other" xlink:type="simple">Hadamard J (1902) Sur les problèmes aux derivees partielles et leur signification physique. Princeton Univ Bull 49–52.</mixed-citation>
</ref>
<ref id="pone.0087619-Clayton1"><label>45</label>
<mixed-citation publication-type="other" xlink:type="simple">Clayton NS, Dally JM, Emery N (2007) <italic>Social cognition by food-caching corvids. The western scrub-jay as a natural psychologist.</italic> Phil. Trans. Roy. Soc. B: 505–627.</mixed-citation>
</ref>
<ref id="pone.0087619-Fudenberg1"><label>46</label>
<mixed-citation publication-type="other" xlink:type="simple">Fudenberg D, Tirole J (1991) Game theory, MIT Press, 1991.</mixed-citation>
</ref>
<ref id="pone.0087619-Harano1"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harano</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Okada</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Nakayama</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Miyatake</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Hosken</surname><given-names>DJ</given-names></name> (<year>2010</year>) <article-title><italic>Intralocus sexual conflict unresolved by sex-limited trait expression.</italic> Curr. Biol</article-title>. <volume>20(22)</volume>: <fpage>2036</fpage>–<lpage>2039</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Mathys1"><label>48</label>
<mixed-citation publication-type="other" xlink:type="simple">Mathys C, Daunizeau J, Friston K, Stephan K (2011) A bayesian foundation for individual learning under uncertainty. Frontiers Hum. Neurosci. 5 (39).</mixed-citation>
</ref>
<ref id="pone.0087619-Berger1"><label>49</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berger</surname><given-names>U</given-names></name> (<year>2007</year>) <article-title>Brown’s original fictitious play</article-title>. <source>J Econ Theory</source> <volume>135</volume>: <fpage>572</fpage>–<lpage>578</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Beal1"><label>50</label>
<mixed-citation publication-type="other" xlink:type="simple">Beal M (2003) <italic>Variational algorithms for approximate Bayesian inference</italic>. PhD thesis, Gatsby Computational Unit, University College London, UK.</mixed-citation>
</ref>
<ref id="pone.0087619-Friston3"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name>, <name name-style="western"><surname>Mattout</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Trujillo-Barreto</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Ashburner</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Penny</surname><given-names>W</given-names></name> (<year>2007</year>) <article-title><italic>Variational free energy and the Laplace approximation.</italic></article-title>. <source>Neuroimage</source> <volume>34</volume>: <fpage>220</fpage>–<lpage>234</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Daunizeau2"><label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name> (<year>2009</year>) <article-title>Variational Bayesian identification and prediction of stochasticdynamic causal models</article-title>. <source>Physica D</source> <volume>289</volume>: <fpage>2089</fpage>–<lpage>2118</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Day1"><label>53</label>
<mixed-citation publication-type="other" xlink:type="simple">Day T, Otto SP (2001) Fitness, <italic>Encyclopedia of Life Sciences.</italic></mixed-citation>
</ref>
<ref id="pone.0087619-Nowak1"><label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nowak</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Sigmund</surname><given-names>K</given-names></name> (<year>1993</year>) <article-title>A strategy of win-stay, loose-shift that outperforms tit-for-tat in the Prisonner’s Dilemma game</article-title>. <source>Nature</source> <volume>364</volume>: <fpage>56</fpage>–<lpage>58</lpage>.</mixed-citation>
</ref>
<ref id="pone.0087619-Hofbauer1"><label>55</label>
<mixed-citation publication-type="other" xlink:type="simple">Hofbauer J, Sigmund K (1998) <italic>Evolutionary games and population dynamics</italic>. (Cambridge Univ Press, Cambridge, UK).</mixed-citation>
</ref>
</ref-list></back>
</article>