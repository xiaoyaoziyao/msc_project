<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="research-article" dtd-version="3.0" xml:lang="en">
  <front>
    <journal-meta><journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><issn pub-type="epub">1932-6203</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PONE-D-11-25312</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0042058</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Anatomy and physiology</subject>
            <subj-group>
              <subject>Neurological system</subject>
              <subj-group>
                <subject>Sensory physiology</subject>
              </subj-group>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Computational biology</subject>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subj-group>
                <subject>Sensory systems</subject>
              </subj-group>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Sensory systems</subject>
              <subj-group>
                <subject>Visual system</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Computational neuroscience</subject>
              <subject>Neural networks</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Computer science</subject>
          <subj-group>
            <subject>Computer modeling</subject>
          </subj-group>
          <subj-group>
            <subject>Synthetic vision systems</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Physiology</subject>
          <subject>Computational Biology</subject>
          <subject>Neuroscience</subject>
          <subject>Computer Science</subject>
        </subj-group>
      </article-categories><title-group><article-title>The Roles of Endstopped and Curvature Tuned Computations in a Hierarchical Representation of 2D Shape</article-title><alt-title alt-title-type="running-head">Endstopped and Curvature Computations for Shape</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Rodríguez-Sánchez</surname>
            <given-names>Antonio J.</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Tsotsos</surname>
            <given-names>John K.</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1">
        <label>1</label>
        <addr-line>Intelligent and Interactive Systems, University of Innsbruck, Innsbruck, Austria</addr-line>
      </aff><aff id="aff2">
        <label>2</label>
        <addr-line>Centre for Vision Research and Dept. of Computer Science and Engineering, York University, Toronto, Ontario, Canada</addr-line>
      </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Bondarenko</surname>
            <given-names>Vladimir E.</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">
        <addr-line>Georgia State University, United States of America</addr-line>
      </aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">Antonio.Rodriguez-Sanchez@uibk.ac.at</email></corresp>
        <fn fn-type="conflict">
          <p>The authors have the following competing interest to declare. This study was partly funded by the Teledyne Scientific Company, Durham, North Carolina. There are no patents, products in development or marketed products to declare. This does not alter the authors' adherence to all the PLoS ONE policies on sharing data and materials, as detailed online in the guide for authors.</p>
        </fn>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: ARS JKT. Performed the experiments: ARS. Analyzed the data: ARS. Contributed reagents/materials/analysis tools: ARS JKT. Wrote the paper: ARS JKT.</p>
        </fn>
      </author-notes><pub-date pub-type="collection">
        <year>2012</year>
      </pub-date><pub-date pub-type="epub">
        <day>9</day>
        <month>8</month>
        <year>2012</year>
      </pub-date><volume>7</volume><issue>8</issue><elocation-id>e42058</elocation-id><history>
        <date date-type="received">
          <day>7</day>
          <month>12</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>2</day>
          <month>7</month>
          <year>2012</year>
        </date>
      </history><permissions>
        
        <copyright-holder>Rodríguez-Sánchez and Tsotsos</copyright-holder>
        <license xlink:type="simple">
          <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
        </license>
      </permissions><abstract>
        <p>That shape is important for perception has been known for almost a thousand years (thanks to Alhazen in 1083) and has been a subject of study ever since by scientists and phylosophers (such as Descartes, Helmholtz or the Gestalt psychologists). Shapes are important object descriptors. If there was any remote doubt regarding the importance of shape, recent experiments have shown that intermediate areas of primate visual cortex such as V2, V4 and TEO are involved in analyzing shape features such as corners and curvatures. The primate brain appears to perform a wide variety of complex tasks by means of simple operations. These operations are applied across several layers of neurons, representing increasingly complex, abstract intermediate processing stages. Recently, new models have attempted to emulate the human visual system. However, the role of intermediate representations in the visual cortex and their importance have not been adequately studied in computational modeling.</p>
        <p>This paper proposes a model of shape-selective neurons whose shape-selectivity is achieved through intermediate layers of visual representation not previously fully explored. We hypothesize that hypercomplex - also known as endstopped - neurons play a critical role to achieve shape selectivity and show how shape-selective neurons may be modeled by integrating endstopping and curvature computations. This model - a representational and computational system for the detection of 2-dimensional object silhouettes that we term 2DSIL - provides a highly accurate fit with neural data and replicates responses from neurons in area V4 with an average of 83% accuracy. We successfully test a biologically plausible hypothesis on how to connect early representations based on Gabor or Difference of Gaussian filters and later representations closer to object categories without the need of a learning phase as in most recent models.</p>
      </abstract><funding-group>
        <funding-statement>The authors are grateful for research support from the Natural Sciences and Engineering Research Council of Canada through a grant to JKT (4557-2009) and the Teledyne Scientific Company, Durham, North Carolina, through a contract to JKT (BOU546385). JKT holds the Canada Research Chair in Computational Vision which also supported this work. JKT is not an employee of Teledyne - he was PI on a contract to York University from Teledyne. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
      </funding-group><counts>
        <page-count count="13"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Since the foundation of modern neuroanatomy by Ramón y Cajal, who gave a detailed description of the nerve cell organization in the central and peripheral nervous system <xref ref-type="bibr" rid="pone.0042058-RamnyCajal1">[1]</xref>–<xref ref-type="bibr" rid="pone.0042058-Jones1">[4]</xref>, great progress has been achieved in understanding the human brain. At the same time, computing power and technology have provided more sophisticated tools to study the brain and its great complexity. Computational neuroscience has appeared as an important methodology for formalizing and testing new hypotheses on how that complex system may perform certain operations.</p>
      <p>Over the last decades, many models inspired by advances in the anatomy of the visual cortex have been presented, the earliest from the late 60 s and early 70 s <xref ref-type="bibr" rid="pone.0042058-Grossberg1">[5]</xref>–<xref ref-type="bibr" rid="pone.0042058-Grossberg4">[8]</xref>. A subsequent and very influential model is Fukushima's Neocognitron <xref ref-type="bibr" rid="pone.0042058-Fukushima1">[9]</xref>. The Neocognitron is a self-organizing neural network model that achieves position invariance and later demonstrated to perform well on digit recognition <xref ref-type="bibr" rid="pone.0042058-Fukushima2">[10]</xref>. The network contains an input layer followed by a cascade of <italic>S-cells</italic> (for simple cells) and <italic>C-cells</italic> (complex cells). After unsupervised training thanks to a self-organization process, one of the C-cells in the last layer will respond selectively to the input pattern used in training. Later models, based on Fukushima's foundation, that included backpropagation <xref ref-type="bibr" rid="pone.0042058-Rumelhart1">[11]</xref> were also successful at the task of handwriting digit recognition <xref ref-type="bibr" rid="pone.0042058-Lecun1">[12]</xref>, <xref ref-type="bibr" rid="pone.0042058-Lecun2">[13]</xref>.</p>
      <p>Since then, there have been several relevant works. Visnet <xref ref-type="bibr" rid="pone.0042058-Wallis1">[14]</xref> consists of a four layer network that achieves invariant object recognition. The most crucial part of such a method is a trace learning rule that is Hebbian based. To achieve translation invariance, the network is trained with inputs at different positions. Riesenhuber and Poggio's <xref ref-type="bibr" rid="pone.0042058-Riesenhuber1">[15]</xref>–<xref ref-type="bibr" rid="pone.0042058-Serre2">[19]</xref> model consists of five hierarchical levels of <italic>S</italic> and <italic>C</italic> neurons (following Fukushima's Neocognitron <xref ref-type="bibr" rid="pone.0042058-Fukushima1">[9]</xref>) that are connected through linear operations in one layer and non-linear (MAX) in the next (the strongest units determine the response of the system). The first level receives input from the retina and is composed of simple neuron receptive fields that analyze orientations. The next levels account for more complex features (e.g. junctions). The last level is composed of view-tuned neurons that achieve position and scale invariance.</p>
      <p>Amit <xref ref-type="bibr" rid="pone.0042058-Amit1">[20]</xref>, <xref ref-type="bibr" rid="pone.0042058-Amit2">[21]</xref> presents a parallel neural network for visual selection. This network is trained to detect candidate locations for object recognition. Objects are represented as composed of features localized at different locations with respect to an object centre. Simple features (edges and conjunctions) are detected in lower levels, while higher levels carry out disjunctions over regions. Suzuki and colleagues <xref ref-type="bibr" rid="pone.0042058-Suzuki1">[22]</xref> construct a model of the form pathway based on predictive coding <xref ref-type="bibr" rid="pone.0042058-Rao1">[23]</xref>, <xref ref-type="bibr" rid="pone.0042058-Rao2">[24]</xref>. Predictive coding hypothesizes that feedback connections from high to lower-order cortical areas carry predictions of lower-level neural activities. Feedforward connections carry residual errors between predictions and the actual lower-level activities. In the model, a fast coarse processing precedes and contrains more detailed processing.</p>
      <p>None of the models presented until now fully explore the possible contributions of intermediate representations as they are known in the brain. Common to most models is a first step that performs some sort of edge-detection in a similar way to some V1 neurons in the brain. Even though some of the proposals may include hierarchies with intermediate representations (e.g. <xref ref-type="bibr" rid="pone.0042058-Serre2">[19]</xref>, <xref ref-type="bibr" rid="pone.0042058-Cadieu1">[25]</xref>), these representations do not include much of the complexity now known to exist in the intermediate layers of the visual cortex. The usual modeling of intermediate layers to date is a simple composition of earlier features to approximate shape without computing curvature or shape directly. Here, we propose a more direct approach, one that provides models of units that compute shape properties directly using several novel neurally-based computations. Distinct from the best of the previous approaches, we do not use simple hierarchical composition of a common neural type but rather, define new neural selectivities for each of several intermediate visual computation layers.</p>
      <p>Models up to now have been stagnant on the representation of contours following Marr's <xref ref-type="bibr" rid="pone.0042058-Marr1">[26]</xref>, <xref ref-type="bibr" rid="pone.0042058-Marr2">[27]</xref> primal sketch, that is, edge combinations are used to represent shapes and objects. Models have added layers of <italic>S</italic> and <italic>C</italic> cells following early systems <xref ref-type="bibr" rid="pone.0042058-Fukushima1">[9]</xref> into higher levels of the hierarchy, not considering that cells in those higher levels perform quite different, more complex, operations. There has been some progress on how hypercomplex cells, also known as endstopped, may be defined <xref ref-type="bibr" rid="pone.0042058-Dobbins1">[28]</xref>–<xref ref-type="bibr" rid="pone.0042058-Dobbins2">[30]</xref>, but except for the work of <xref ref-type="bibr" rid="pone.0042058-Heitger2">[31]</xref>–<xref ref-type="bibr" rid="pone.0042058-Craft1">[33]</xref> on figure-ground segregation, the role of endstopping has been neglected. Here, following this past work, we hypothesize that endstopped neurons play an important role in encoding curvature and shape.</p>
      <p>We present a biologically plausible model for shape representation, 2DSIL, where the focus is on 2D silhouettes. In the following section we describe in detail each layer in the model. Next we show the strongly positive results of testing the model with stimuli used in previous single-cell recording studies followed by a discussion regarding the characteristics of 2DSIL. In a previous paper <xref ref-type="bibr" rid="pone.0042058-RodrguezSnchez1">[34]</xref> we showed that even when this representation is used within a recognition system, it outperforms the leading competing models. Material and methods are presented at the end.</p>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <p>In this section we explain how shape selectivity may be achieved with a model that incorporates intermediate layers inspired by the primate visual system. We demonstrate the performance of our model by comparing computed responses with neurons from area V4.</p>
      <sec id="s2a">
        <title>Incorporating endstopping and curvature in a model of shape representation</title>
        <p><xref ref-type="fig" rid="pone-0042058-g001">Figure 1</xref> presents a depiction of the proposed architecture, which comprises simple, complex, endstopped, local curvature and shape-selective cells that are described next in detail. In what follows whenever a neuron is referred to as model neuron/cell it is one developed for our theory. A neuron or cell referred to without the model adjective is a biological one.</p>
        <fig id="pone-0042058-g001" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0042058.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>Architecture of the representational and computational system for the detection of 2-dimensional object silhouettes (2DSIL).</title>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0042058.g001" xlink:type="simple"/>
        </fig>
        <sec id="s2a1">
          <title>Model simple cells</title>
          <p>Simple neurons of visual area V1 are sensitive to bar and edge orientations as previous models also stipulate. Common spatial response profiles to model simple neurons in area V1 include Gabor filters <xref ref-type="bibr" rid="pone.0042058-Marcelja1">[35]</xref> and Difference of Gaussians. The latter provides a better fit to neuronal responses <xref ref-type="bibr" rid="pone.0042058-Hawken1">[36]</xref> and accordingly gave better results in our case than the Gabor filter formulation:</p>
          <disp-formula id="pone.0042058.e001"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0042058.e001" xlink:type="simple"/></disp-formula>
          <p>where <inline-formula id="pone.0042058.e011"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e011" xlink:type="simple"/></inline-formula> is the height and <inline-formula id="pone.0042058.e012"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e012" xlink:type="simple"/></inline-formula> and <inline-formula id="pone.0042058.e013"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e013" xlink:type="simple"/></inline-formula> are the width of each Gaussian function. <inline-formula id="pone.0042058.e014"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e014" xlink:type="simple"/></inline-formula> is their orientation. The relation between these parameters may be referred to as the aspect ratio <inline-formula id="pone.0042058.e015"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e015" xlink:type="simple"/></inline-formula> and the width ratio <inline-formula id="pone.0042058.e016"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e016" xlink:type="simple"/></inline-formula>. Size of filters were 4<inline-formula id="pone.0042058.e017"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e017" xlink:type="simple"/></inline-formula>. As with all the model neurons within 2DSIL, these are defined at multiple scales, each scale being band-pass for a range of receptive field sizes, with the number of scales represented appropriate for the modelling task. Values assigned to these parameters are exposed in the methods section.</p>
          <p>Cells in area V1 are heterogeneous, i.e. they are not all uniform. In the realization of the model, four different groups of simple cells were designed, varying sizes and values of width and length. Model simple cells are organized into hypercolumns. Within a hypercolumn, cells are organized at the same orientation but are spatially displaced and combined into model complex cells as described next (<xref ref-type="fig" rid="pone-0042058-g001">Figure 1</xref>), however there is no input from left and right eye since binocular responses are not considered in this study. Model simple cells are at different orientations and scales.</p>
        </sec>
        <sec id="s2a2">
          <title>Model complex cells</title>
          <p>Complex cells have a sensitivity for bars and orientations as well, but their receptive fields are larger than the ones of simple neurons. Hubel and Wiesel <xref ref-type="bibr" rid="pone.0042058-Hubel1">[37]</xref>–<xref ref-type="bibr" rid="pone.0042058-Hubel3">[39]</xref> suggested that complex cells may integrate the responses of simple cells. In addition to this, <xref ref-type="bibr" rid="pone.0042058-Spitzer1">[40]</xref> showed that complex cells may be the result of the addition of simple cells along the axis perpendicular to their orientation. Following these studies, in our model, a complex cell is the weighted sum of 5 laterally displaced model simple cells within a column. The model complex cell response is given by <xref ref-type="bibr" rid="pone.0042058-Dobbins2">[30]</xref>:</p>
          <disp-formula id="pone.0042058.e002"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0042058.e002" xlink:type="simple"/></disp-formula>
          <p><inline-formula id="pone.0042058.e018"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e018" xlink:type="simple"/></inline-formula> is the response of the ith cell and <inline-formula id="pone.0042058.e019"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e019" xlink:type="simple"/></inline-formula> is its weight. Model cells are Gaussian weighted by position, with weight inversely proportional to distance to the center. <inline-formula id="pone.0042058.e020"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e020" xlink:type="simple"/></inline-formula> is a rectification function, where any value less than 0 is set to 0. Model simple cells combining into a model complex cell are laterally displaced, their displacement being proportional to the cell's <italic>size</italic> as well as the height (<inline-formula id="pone.0042058.e021"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e021" xlink:type="simple"/></inline-formula>) and width (<inline-formula id="pone.0042058.e022"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e022" xlink:type="simple"/></inline-formula>) of the Gaussian function. Displacement is in the direction of the orientation perpendicular to the preferred one (<inline-formula id="pone.0042058.e023"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e023" xlink:type="simple"/></inline-formula>, using the <italic>modulo</italic> function to keep values in the range <inline-formula id="pone.0042058.e024"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e024" xlink:type="simple"/></inline-formula>) and are given by <inline-formula id="pone.0042058.e025"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e025" xlink:type="simple"/></inline-formula> (displacement in <italic>x</italic> axis) and <inline-formula id="pone.0042058.e026"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e026" xlink:type="simple"/></inline-formula> (displacement in <italic>y</italic> axis) in the following equation:</p>
          <disp-formula id="pone.0042058.e003"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0042058.e003" xlink:type="simple"/></disp-formula>
          <p>The construction of a model complex neuron is depicted in <xref ref-type="fig" rid="pone-0042058-g002">Figure 2A</xref>. The orientation of its model simple neuronal components in this case is for 90<inline-formula id="pone.0042058.e027"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e027" xlink:type="simple"/></inline-formula> (vertical), while the 5 model simple cells are organized perpendicularly (spatially displaced but overlapping) to this preferred orientation, that is, 0<inline-formula id="pone.0042058.e028"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e028" xlink:type="simple"/></inline-formula>. This results in slightly less sensitivity for orientations since each model complex cell integrates five model simple cells. A model complex neuron yields a positive response for stimuli at more locations inside its receptive field and their receptive fields are larger as well. These characteristics follow <xref ref-type="bibr" rid="pone.0042058-Hubel1">[37]</xref>–<xref ref-type="bibr" rid="pone.0042058-Hubel3">[39]</xref> and up to this point our model simple and complex cells follow <xref ref-type="bibr" rid="pone.0042058-Fukushima1">[9]</xref> and share some similarities with its followers as well <xref ref-type="bibr" rid="pone.0042058-Riesenhuber1">[15]</xref>, <xref ref-type="bibr" rid="pone.0042058-Amit2">[21]</xref>, <xref ref-type="bibr" rid="pone.0042058-Geman1">[41]</xref>.</p>
          <fig id="pone-0042058-g002" orientation="portrait" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pone.0042058.g002</object-id>
            <label>Figure 2</label>
            <caption>
              <title>Endstopping.</title>
              <p>(A) Model complex cell. (B) Structure of model endstopped cell. (C) Response of the model endstopped cells to different radius of curvatures. Simple cell sizes were 40 (blue color), 80 (red color), 100 (green color) and 120 pixels (black color). <inline-formula id="pone.0042058.e029"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e029" xlink:type="simple"/></inline-formula> = (10,20,25,30). <italic>AR</italic> (aspect ratio) = (1.15,2,3,4). <italic>WR</italic> (width ratio) = 2.5 for all cells. Gain <italic>c</italic> = (0.7,0.8,1,2). Responses were normalized for the range [0,1].</p>
            </caption>
            <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0042058.g002" xlink:type="simple"/>
          </fig>
        </sec>
        <sec id="s2a3">
          <title>Model endstopped cells</title>
          <p>Endstopped - also known as hypercomplex - neurons respond to contours, both real and illusory <xref ref-type="bibr" rid="pone.0042058-vonderHeydt1">[42]</xref>. A more recent study <xref ref-type="bibr" rid="pone.0042058-Ito1">[43]</xref> has found that although V2 neurons are mainly selective for angles and corners, these neurons also showed submaximal responses for bars. Model endstopped cells result from the difference between a simple cell and two displaced complex cells <xref ref-type="bibr" rid="pone.0042058-Kato1">[44]</xref>. At this point, our model diverges strongly away from formulations in the previous works cited above. When simple and complex cells are combined at the same orientation we can distinguish between degrees of curvature. Through the use of model complex cells at different orientations with respect to the simple cell, we can obtain the sign of the curvature. These two model neuron types are explained next.</p>
        </sec>
        <sec id="s2a4">
          <title>Model cells discriminant to the degree of curvature</title>
          <p>This model endstopped cell is the neural convergence of a model simple neuron and two displaced model complex neurons selective for the same orientation as follows (<xref ref-type="fig" rid="pone-0042058-g002">Figure 2B</xref>):</p>
          <disp-formula id="pone.0042058.e004"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0042058.e004" xlink:type="simple"/></disp-formula>
          <p><inline-formula id="pone.0042058.e030"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e030" xlink:type="simple"/></inline-formula>, <inline-formula id="pone.0042058.e031"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e031" xlink:type="simple"/></inline-formula> and <inline-formula id="pone.0042058.e032"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e032" xlink:type="simple"/></inline-formula> are the gains for the center and displaced cells. <inline-formula id="pone.0042058.e033"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e033" xlink:type="simple"/></inline-formula>, <inline-formula id="pone.0042058.e034"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e034" xlink:type="simple"/></inline-formula> and <inline-formula id="pone.0042058.e035"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e035" xlink:type="simple"/></inline-formula> are the responses of the center and the two displaced cells. <inline-formula id="pone.0042058.e036"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e036" xlink:type="simple"/></inline-formula> is a rectification function, where any value less than 0 is set to 0. <inline-formula id="pone.0042058.e037"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e037" xlink:type="simple"/></inline-formula> is:</p>
          <disp-formula id="pone.0042058.e005"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0042058.e005" xlink:type="simple"/></disp-formula>
          <p>This sigmoidal function - whose parameter values are given in the methods section - scales responses to highly intense stimuli. Displaced cells are shifted 1/2 of their receptive field size in the direction of their prefered orientation. The center simple cell has an excitatory effect while the two complex cells (at the top and bottom in <xref ref-type="fig" rid="pone-0042058-g002">Figure 2B</xref>) have an inhibitory effect, which are wider than the center cell, following <xref ref-type="bibr" rid="pone.0042058-Orban1">[45]</xref>, <xref ref-type="bibr" rid="pone.0042058-Willmore1">[46]</xref>. This design follows the work of <xref ref-type="bibr" rid="pone.0042058-Dobbins1">[28]</xref>, <xref ref-type="bibr" rid="pone.0042058-Dobbins2">[30]</xref>, <xref ref-type="bibr" rid="pone.0042058-Dobbins3">[47]</xref> and <xref ref-type="bibr" rid="pone.0042058-Kato1">[44]</xref>, <xref ref-type="bibr" rid="pone.0042058-Orban1">[45]</xref>, <xref ref-type="bibr" rid="pone.0042058-Orban2">[48]</xref>, <xref ref-type="bibr" rid="pone.0042058-Bishop1">[49]</xref>.</p>
          <p>Thanks to this configuration of simple and complex cells, we obtain a coarse estimation of curvature such that different curvatures can be discriminated into classes. <xref ref-type="fig" rid="pone-0042058-g002">Figure 2C</xref> shows how this type of cell can discriminate among different degrees of curvature. The plot shows how arcs of different radius provide different responses from this type of cell depending on the size of the component simple and complex cells. The scales of the simple and complex neurons that are combined in the configuration of endstopped cells play an important role in this curvature discrimination as it is shown in <xref ref-type="fig" rid="pone-0042058-g002">Figure 2C</xref>. Different neuronal sizes provide a different response to different degress of curvature. The model endstopped smallest neuron (<xref ref-type="fig" rid="pone-0042058-g002">Figure 2C</xref> blue plot, simple cell size 40 pixels) is selective for very high curvatures, while the largest model enstopped neuron (<xref ref-type="fig" rid="pone-0042058-g002">Figure 2C</xref> black plot, simple cell size 120 pixels) was selective to very broad curvatures, in-between scales (sizes of 80 and 100 pixels) provide preferred responses to intermediate curvatures (red and green plots). Note that this configuration also has maximal responses to bars of a specified length (that of the simple cell at the center) as it is the case of real endstopped cells as well. Also note that the choice of these sizes, and even the number of sizes or scales in the model overall, are at the discretion of the modeler so that the space of visual contours addressed by the model are best fit by the scales represented.</p>
        </sec>
        <sec id="s2a5">
          <title>Model cells selective to the sign of curvature</title>
          <p>Apart from the degree of curvature, an additional contour characteristic that V2 cells seem to encode is the sign of curvature <xref ref-type="bibr" rid="pone.0042058-Dobbins1">[28]</xref>, <xref ref-type="bibr" rid="pone.0042058-Hegde1">[50]</xref>. Through the local information available to endstopping we may compute the sign of curvature. Here, in contrast to the curvature model cells, each displaced complex cell has a different orientation to the simple cell, and the two model complex cells are oriented at opposite signs (e.g. 45<inline-formula id="pone.0042058.e038"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e038" xlink:type="simple"/></inline-formula> and 135<inline-formula id="pone.0042058.e039"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e039" xlink:type="simple"/></inline-formula> for the 0<inline-formula id="pone.0042058.e040"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e040" xlink:type="simple"/></inline-formula> model endstopped neurons) (<xref ref-type="fig" rid="pone-0042058-g003">Figure 3</xref>). A hint regarding this concept was first proposed by <xref ref-type="bibr" rid="pone.0042058-Dobbins2">[30]</xref>, which is extended here to all orientations and used on curvatures.</p>
          <fig id="pone-0042058-g003" orientation="portrait" position="float">
            <object-id pub-id-type="doi">10.1371/journal.pone.0042058.g003</object-id>
            <label>Figure 3</label>
            <caption>
              <title>Model endstopped cell selective for curvature sign.</title>
            </caption>
            <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0042058.g003" xlink:type="simple"/>
          </fig>
          <p>For one sign of curvature, a curve excites the model simple excitatory cell at the center but curves falling into the region of the model complex inhibitory cells reduces the response of the model endstopped cell. A similar curve of the opposite sign passes only through the excitatory region (model simple cell), the curve having no inhibition effect (or a very low inhibition) on the overall response of the model endstopped cell since it is not, or is barely, falling on the model complex cell receptive fields (<xref ref-type="fig" rid="pone-0042058-g003">Figure 3</xref>).</p>
          <p>Two types of model sign cells are used. These different signs are obtained by changing the order of the displaced subtracted neurons.</p>
          <disp-formula id="pone.0042058.e006"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0042058.e006" xlink:type="simple"/></disp-formula>
          <p>where <inline-formula id="pone.0042058.e041"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e041" xlink:type="simple"/></inline-formula>, <inline-formula id="pone.0042058.e042"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e042" xlink:type="simple"/></inline-formula> and <inline-formula id="pone.0042058.e043"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e043" xlink:type="simple"/></inline-formula> are the gains for the center and displaced cells as before. <inline-formula id="pone.0042058.e044"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e044" xlink:type="simple"/></inline-formula>, <inline-formula id="pone.0042058.e045"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e045" xlink:type="simple"/></inline-formula> and <inline-formula id="pone.0042058.e046"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e046" xlink:type="simple"/></inline-formula> are the responses of center and displaced cells. The difference here is that the displaced cells are at different orientations of the preferred center simple cell, for the positive sign model endstopped neuron, the displaced model complex neuron <italic>d1</italic> is at 45<inline-formula id="pone.0042058.e047"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e047" xlink:type="simple"/></inline-formula>, while the model complex component <italic>d2</italic> is at 135<inline-formula id="pone.0042058.e048"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e048" xlink:type="simple"/></inline-formula>. For the negative sign model endstopped cell, the order is the opposite. For best results, these model cells required larger receptive field overlap than their degree of curvature endstopped model cells counterpart (see methods).</p>
        </sec>
        <sec id="s2a6">
          <title>Model local curvature cells</title>
          <p>This type of cell is the result of the combination of the responses from the two types of model endstoped cells (degree and sign of curvature), e.g. a model curvature cell that is selective for broad curvatures whose sign is positive as opposed to a model cell also selective for broad curvatures whose sign is negative. Through this neural convergence of model endstopped cells discriminative to the degree of curvature and the ones to the sign of curvature, we obtain twice the number of curvature classes. For example, if we have four types of model endstopped cells, through the use of the sign of curvature of those cells we obtain eight curvature classes.</p>
          <disp-formula id="pone.0042058.e007"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0042058.e007" xlink:type="simple"/></disp-formula>
          <p>where <inline-formula id="pone.0042058.e049"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e049" xlink:type="simple"/></inline-formula> denotes the response of a neuron tuned to angle <inline-formula id="pone.0042058.e050"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e050" xlink:type="simple"/></inline-formula>, curvature <italic>r</italic> and sign <italic>s</italic>. <italic>n</italic> is the number of model endstopped cell types, <inline-formula id="pone.0042058.e051"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e051" xlink:type="simple"/></inline-formula> is the response of the model endstopped cell <italic>i</italic> and <inline-formula id="pone.0042058.e052"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e052" xlink:type="simple"/></inline-formula> <inline-formula id="pone.0042058.e053"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e053" xlink:type="simple"/></inline-formula> are the responses of the model sign selective endstopped neurons. In the realization of our model <italic>i</italic> = {1, 2, 3, 4} and <italic>n</italic> = 4 (see Material and Methods). This equation is read like: If the value of <inline-formula id="pone.0042058.e054"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e054" xlink:type="simple"/></inline-formula> is greater than <inline-formula id="pone.0042058.e055"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e055" xlink:type="simple"/></inline-formula>, <inline-formula id="pone.0042058.e056"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e056" xlink:type="simple"/></inline-formula> has the same value as the model curvature endstopped cell, otherwise, <inline-formula id="pone.0042058.e057"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e057" xlink:type="simple"/></inline-formula> contains that value and <inline-formula id="pone.0042058.e058"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e058" xlink:type="simple"/></inline-formula> is 0. For the case where the response from endstopped cells is small, a high response from a model orientation simple cell means the contour is a straight line, so its curvature is set to 0. <inline-formula id="pone.0042058.e059"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e059" xlink:type="simple"/></inline-formula> is computed at each location.</p>
        </sec>
        <sec id="s2a7">
          <title>Model shape cells</title>
          <p>V4 cells are quite sensitive to shape and less sensitive to spatial position <xref ref-type="bibr" rid="pone.0042058-Gallant1">[51]</xref>. Experiments in area V4 <xref ref-type="bibr" rid="pone.0042058-Pasupathy1">[52]</xref> and TEO <xref ref-type="bibr" rid="pone.0042058-Tanaka1">[53]</xref>, <xref ref-type="bibr" rid="pone.0042058-Brincat1">[54]</xref> of the macaque monkey seem to point to a strategy of recognition of objects by parts. In the case of V4 and TEO, those parts would be local curvatures <xref ref-type="bibr" rid="pone.0042058-Pasupathy1">[52]</xref>, <xref ref-type="bibr" rid="pone.0042058-Brincat1">[54]</xref>–<xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[56]</xref>. The response to a shape could correspond to the response of the local curvatures of the object. In TEO, some components of local curvatures excite the neuron, and others inhibit its response <xref ref-type="bibr" rid="pone.0042058-Brincat1">[54]</xref>.</p>
          <p>Neurons in areas V4 and TEO share similar characteristics regarding shape analysis <xref ref-type="bibr" rid="pone.0042058-Brincat1">[54]</xref>, <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[56]</xref> and selectivity <xref ref-type="bibr" rid="pone.0042058-Boussaoud1">[57]</xref>. Although similar, TEO neurons show a higher degree of complexity than V4 neurons <xref ref-type="bibr" rid="pone.0042058-Brincat1">[54]</xref>. Our model shape neurons mimic that curvature by parts representation of shapes and silhouettes but are slightly more complex than just the curvature<inline-formula id="pone.0042058.e060"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e060" xlink:type="simple"/></inline-formula>angular position coding proposed by <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[56]</xref> for V4 neurons since they are not only selective to curvatures at angular positions but also to the distance of the curvature element to the center of the shape. This conveys more information regarding the contour element. A shape would be different if the curvature is far away from the shape center or near the shape center even though its angular position is the same. We thus make use of both components to better describe the position of the curvature element than just one of them (angular position) as proposed in <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[56]</xref>.</p>
          <p>Our model shape cells integrate the responses from a population of model local curvature neurons to encode a shape. The proposed response of a model's shape neuron at location <italic>x</italic> is:</p>
          <disp-formula id="pone.0042058.e008"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0042058.e008" xlink:type="simple"/></disp-formula>
          <p>where <inline-formula id="pone.0042058.e061"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e061" xlink:type="simple"/></inline-formula> denotes the response of a model local curvature cell tuned to angle <inline-formula id="pone.0042058.e062"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e062" xlink:type="simple"/></inline-formula>, curvature <italic>r</italic> and sign <italic>s</italic> at location <italic>x</italic>, and <inline-formula id="pone.0042058.e063"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e063" xlink:type="simple"/></inline-formula> is a gaussian weight centered at <inline-formula id="pone.0042058.e064"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e064" xlink:type="simple"/></inline-formula> (<italic>x</italic> and <inline-formula id="pone.0042058.e065"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e065" xlink:type="simple"/></inline-formula> are in polar coordinates). <italic>max</italic> selects the maximum reponse from the local curvature over all angles, since the importance is on the responses to curvatures from curvature neurons, not their orientation at this level of the architecture. A model shape neuron will respond to a shape, and depending on how close the stimulus is to its selectivity (controlled through <inline-formula id="pone.0042058.e066"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e066" xlink:type="simple"/></inline-formula> - see <xref ref-type="sec" rid="s4">Materials and Methods</xref>), its response will be stronger or weaker. Total response of a shape neuron is the summation over all <italic>p</italic> locations:</p>
          <disp-formula id="pone.0042058.e009"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0042058.e009" xlink:type="simple"/></disp-formula>
        </sec>
      </sec>
      <sec id="s2b">
        <title>Response of a model shape neuron in curvature space</title>
        <p>The model shape neuron of <xref ref-type="fig" rid="pone-0042058-g004">Figure 4A</xref> has a response depending on how close the stimulus is to its curvature-by-parts selectivity (<xref ref-type="fig" rid="pone-0042058-g004">Figure 4A</xref>). In the figure, the model neuron is selective to a sharp curvature at the top left. This neuron would respond maximally when that feature is present at that specific location, but it would respond also to a broader curvature at that location with a lower value and would have a small response to a very broad curvature or a straight line.</p>
        <fig id="pone-0042058-g004" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0042058.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>Shape-selective neuron.</title>
            <p>(A) Shape-selective neurons respond to different curvatures at different positions. The response is maximal when those curvatures are present at their selective positions (red). If they are in nearby positions the neuron provides some response as well (orange and yellow). (B) Shape-selective neuron tuning profile for location and curvature. (C) Shape neuron response to different stimuli, maximum response is to the stimulus at the top (value 1).</p>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0042058.g004" xlink:type="simple"/>
        </fig>
        <p>Model shape neurons exhibit band-pass tuning for curvature information. Their responses achieve a peak at a specific curvature, then decay providing a decreasing response for curvature values of increasing distance. No response is provided for curvatures very far from the optimal. The model shape neuron in this example is then selective for those model endstopped neurons that respond strongly to sharp curvatures at that position. Since a model endstopped neuron with a high response to a sharp curvature has also some response to a slightly broader type of curvature, model shape neurons will not provide a binary response but a range or responses depending on the distance between curvatures in curvature space (<xref ref-type="fig" rid="pone-0042058-g004">Figure 4B,C</xref>).</p>
      </sec>
      <sec id="s2c">
        <title>Response of a model shape neuron based on curvature locations</title>
        <p>Features (curvatures) comprising the model shape neuron are weighted with respect to a factor <inline-formula id="pone.0042058.e067"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e067" xlink:type="simple"/></inline-formula> (Equation 8) depending on how close the desired curvature is to the desired position (<xref ref-type="fig" rid="pone-0042058-g004">Figure 4A</xref>). Continuing with the example of a neuron selective for a sharp curvature at the top left, this model neuron will have a high response to any stimuli that contain such sharp curvature at that position, but some response will still be elicited in a nearby position, e.g. a sharp curvature at the top mid-left, but no response will be obtained for a sharp curvature present at far away positions (e.g. the sharp curvature is at the bottom) (<xref ref-type="fig" rid="pone-0042058-g004">Figure 4B</xref>).</p>
        <p>The curvatures that fall into the preferred cell's positions are considered in their full value (red in <xref ref-type="fig" rid="pone-0042058-g004">Figure 4A</xref>), but if they fall close, they are weighted in a Gaussian manner depending on how far from the preferred position they are (orange and yellow in <xref ref-type="fig" rid="pone-0042058-g004">Figure 4A</xref>).This is encoded using polar coordinates <xref ref-type="bibr" rid="pone.0042058-Pasupathy1">[52]</xref>, that is, the radial distance to the center of the model shape neuron and its angular position.</p>
        <sec id="s2c1">
          <title>Representational adequacy</title>
          <p>In the words of Pasupathy and Connor <xref ref-type="bibr" rid="pone.0042058-Pasupathy1">[52]</xref>: <italic>The population code for shape has to accomodate the virtual infinity of possible objects as well as the variability of a given object's retinal image</italic>. Our model shape neuron has the capability of representing that virtual infinity of objects: If we consider that our stimuli are within 400<inline-formula id="pone.0042058.e068"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e068" xlink:type="simple"/></inline-formula>400 pixel images, for the bin size selection used in the experiments below (see <xref ref-type="sec" rid="s4">Material and Methods</xref>) this gives a total of 1,800 possible curvature parts inside a model shape neuron receptive field. In the case of only 8 curvature classes, when we consider any possible combination of curvature/location, our model can represent a maximum of 14,400 (approximately 10 to the power of 86400) possible configurations of stimuli. In practice, one might take into account Gestalt properties such as continuity, proximity and others, and that number can be reduced to reflect only realizable configurations. The point here is that this representation is sufficiently rich to enable coding of a wide variety of shapes and task knowledge or learning through developmental experience will help determine the relevant subset for a given task domain.</p>
        </sec>
      </sec>
      <sec id="s2d">
        <title>Comparison with biological neurons from area V4</title>
        <p>Here we compare the performance of the model shape neurons with neurons in area V4 of the macaque's visual cortex from the same study on which our shape cells are based. For most cells in area V4 of the macaque, shapes evoking strongest responses are characterized by a consistent type of boundary configuration at a specific position within the stimulus <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[56]</xref>. We show that this behavior is compatible with the model shape-selective neurons constructed as explained previously.</p>
        <p>Pasupathy and Connor <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[56]</xref> recorded the responses of 109 neurons to 366 different shapes. Each cell in the sample responded to a variety of very different shapes. No cell displayed a response pattern that could be characterized in terms of a single type of global shape. However, for most cells the effective stimuli showed some degree of shape consistency at one position. In other words, these cells were tuned for boundary configuration in one part of the shape.</p>
        <p>In order to demonstrate the plausibility of our shape neurons and the hypothesis that curvature and shape may be encoded through endstopping, we study the behavior of the model shape neurons by comparing their responses against real neuron responses. We compared the responses from 75 - those cells where the shape consistency was more clear (see <xref ref-type="sec" rid="s4">Material and Methods</xref>) - out of the 109 neurons recorded by Pasupathy and Connor's group. Data from real neurons to achieve this set of experiments was kindly provided by Dr. Anitha Pasupathy.</p>
        <p>We first compared the responses from our shape-selective neurons with the four examples from <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[56]</xref>. We start with <xref ref-type="fig" rid="pone-0042058-g002">Figure 2</xref> from <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[56]</xref> (our <xref ref-type="fig" rid="pone-0042058-g005">Figure 5</xref>). Real V4 neuron responses are on the left (stimuli within circles), our model shape neuron equivalent responses are on the right (stimuli within squares). Each row on both cases contains stimuli consisting of 2 shapes (one after the other) rotated in steps of 45<inline-formula id="pone.0042058.e069"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e069" xlink:type="simple"/></inline-formula>. This is the stimulus set used by <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[56]</xref>. Each stimulus is represented by a white icon drawn within a circle (Pasupathy and Connor's results) or within a square (model shape neuron responses) representing the unit receptive field. The darker the background behind the icon, the higher the response of the neuron is to that shape, this applies both to Pasupathy and Connor's neuron recording and our model shape neuron.</p>
        <fig id="pone-0042058-g005" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0042058.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>Comparison to <xref ref-type="fig" rid="pone-0042058-g002"><bold>Figure 2</bold></xref> of <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[<bold>56</bold>]</xref>.</title>
            <p>Cells responses are on the left (<inline-formula id="pone.0042058.e070"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e070" xlink:type="simple"/></inline-formula>© 2001 The American Physiological Society, reproduced with permission) and their respective model responses are on right.</p>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0042058.g005" xlink:type="simple"/>
        </fig>
        <p>For the cell in <xref ref-type="fig" rid="pone-0042058-g005">Figure 5</xref>, stimuli with a sharp convex angle at the bottom left were particularly effective (e.g. stimuli 1 and 2 in the middle column, bottom block; these stimuli are labeled with superscript numbers). Stimuli with a medium convex curve evoked moderate responses (e.g., stimuli 3 and 4). Thus this cell appears to encode information about the bottom left boundary region, responding well to sharp convexity at this location and poorly to broad convexity or concavity. Based on the response of this cell to the stimuli, this neuron was selective to a sharp convexity at the bottom left and a concavity adjacent to it (at the bottom). A first examination shows that the responses of the model's shape neurons are very similar to those of real cells. Our shape-selective neurons respond strongly to a sharp convexity at the botton left and a concavity at the bottom as well. If the curvature adjacent to the sharp convexity at the bottom left is convex, real cell responses are much weaker, our shape-selective neurons show also weaker responses as well but not as weak as for real cells. This additional weakness might be due to local inhibitory mechanisms (local competition) which are not presently included in the model.</p>
        <p>Another example provided by Pasupathy and Connor is on <xref ref-type="fig" rid="pone-0042058-g004">Figure 4</xref> of their article (Replicated in <xref ref-type="fig" rid="pone-0042058-g006">Figure 6</xref>, right). This cell was sensitive to boundary configuration on the right side of the object, responding best to concave curvature at that position. This is exemplified by stimuli 1 and 2; stimulus 1, with a concavity at the right, evoked a stronger response. Stimulus 2 is almost identical, but with a convexity at the right, and it evoked no response. The cell also appears to be tuned for sharper convexities at the counter-clockwise-adjacent position and medium convexities at the clockwise-adjacent position. Pasupathy and Connor note that this is shown by stimulus 3 providing a strong response, while for stimulus 4, its response is weak (opposite combination: sharp curvature clockwise and medium curvature counter-clockwise). The results for the model in this case are almost equal for these stimuli as well as the other cases mentioned in <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[56]</xref>: compare shapes 5 and 6, and 7 and 8. As previously, there are some small differences, the model providing stronger responses than the real cells for a few stimuli.</p>
        <fig id="pone-0042058-g006" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0042058.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>Comparison to <xref ref-type="fig" rid="pone-0042058-g004"><bold>Figure 4</bold></xref> of <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[<bold>56</bold>]</xref>.</title>
            <p>Cells responses are on the left (<inline-formula id="pone.0042058.e071"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e071" xlink:type="simple"/></inline-formula>© 2001 The American Physiological Society, reproduced with permission) and their respective model responses are on right.</p>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0042058.g006" xlink:type="simple"/>
        </fig>
        <p><xref ref-type="fig" rid="pone-0042058-g007">Figure 7</xref> shows the comparison between one of our model shape neurons with the neuron corresponding to <xref ref-type="fig" rid="pone-0042058-g005">Figure 5</xref> from <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[56]</xref>. This neuron was sensitive to a sharp convexity at the top right flanked by a concavity on one side or the other. A first examination shows that the responses of the model's shape neurons are very similar to those of real cells. As it is the case for <xref ref-type="fig" rid="pone-0042058-g008">Figure 8</xref> of that same article, that cell was selective for broad convex curvature at the top. Their results are replicated here in <xref ref-type="fig" rid="pone-0042058-g008">Figure 8</xref>.</p>
        <fig id="pone-0042058-g007" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0042058.g007</object-id>
          <label>Figure 7</label>
          <caption>
            <title>Comparison to <xref ref-type="fig" rid="pone-0042058-g005"><bold>Figure 5</bold></xref> of <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[<bold>56</bold>]</xref>.</title>
            <p>Cells responses are on the left (<inline-formula id="pone.0042058.e072"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e072" xlink:type="simple"/></inline-formula>© 2001 The American Physiological Society, reproduced with permission) and their respective model responses are on right.</p>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0042058.g007" xlink:type="simple"/>
        </fig>
        <fig id="pone-0042058-g008" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0042058.g008</object-id>
          <label>Figure 8</label>
          <caption>
            <title>Comparison to <xref ref-type="fig" rid="pone-0042058-g008"><bold>Figure 8</bold></xref> of <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[<bold>56</bold>]</xref>.</title>
            <p>Cells responses are on the left (<inline-formula id="pone.0042058.e073"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e073" xlink:type="simple"/></inline-formula>© 2001 The American Physiological Society, reproduced with permission) and their respective model responses are on right.</p>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0042058.g008" xlink:type="simple"/>
        </fig>
        <p>We compared the responses of 75 of our model shape neurons with 75 V4 cells. The comparison consisted in computing the absolute difference between the normalized responses of each model shape neuron and that of a real V4 neuron averaged over the 366 stimuli:</p>
        <disp-formula id="pone.0042058.e010"><graphic orientation="portrait" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0042058.e010" xlink:type="simple"/></disp-formula>
        <p><inline-formula id="pone.0042058.e074"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e074" xlink:type="simple"/></inline-formula> is the absolute difference between each model shape neuron's response and the response from the real neuron. <inline-formula id="pone.0042058.e075"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e075" xlink:type="simple"/></inline-formula> corresponds to the response of the <italic>i-th</italic> model shape neuron to the <italic>j-th stimulus</italic> and <inline-formula id="pone.0042058.e076"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e076" xlink:type="simple"/></inline-formula> is the response of its real neuron counterpart to the same stimulus. For each cell, mean and standard deviation were computed and results will be provided next as error percentages, that is, mean difference between our model shape neurons and real cells.</p>
        <p>The results for all the 75 cells considered in this study are shown in <xref ref-type="fig" rid="pone-0042058-g009">Figure 9</xref> for two conditions: model neuron responses using the curvature parts with respect to the center of the neuron (blue bars) and model shape neuron responses with respect the centroid of the shape (green bars). Note that the stimuli from <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[56]</xref> are not always at the receptive field center. We did not find a significant difference between using curvature parts with respect to the center of the model neurons or the centroid of the object.</p>
        <fig id="pone-0042058-g009" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0042058.g009</object-id>
          <label>Figure 9</label>
          <caption>
            <title>Difference between the model's Shape-selective neurons and 75 real cells responses from area V4.</title>
          </caption>
          <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0042058.g009" xlink:type="simple"/>
        </fig>
        <p>For both cases we can see that there are only a few model shape neurons with over 20% error, most of the differences between the model and that of real cells fall in the range 10–20%. Average error for all model shape neurons was 16.95% for the center of the model neuron (<italic>stdev</italic> = 12.61) and almost the same when using the centroid of the shape (<italic>error</italic> = 16.98%, <italic>stdev</italic> = 12.25). This shows that even for such a large number of neurons the model performs well and the difference between the response of the model shape-selective neurons and that of real cells is small. In direct comparison with the only other work to compare performance to this dataset of neural responses, our method significantly outperforms <xref ref-type="bibr" rid="pone.0042058-Cadieu1">[25]</xref>.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>We have presented a model of 2D shape representation - 2DSIL - that follows the structure and behavior of the visual cortex. Building on past conjectures that one of the functional roles of endstopped cells may be to aid in shape analysis <xref ref-type="bibr" rid="pone.0042058-Dobbins1">[28]</xref>, <xref ref-type="bibr" rid="pone.0042058-Dobbins3">[47]</xref>, <xref ref-type="bibr" rid="pone.0042058-Pasupathy2">[55]</xref>, we set out to define a biologically plausible computational model of shape representation. Here, we tested this hypothesis and have shown how a hierarchy starting from basic simple neurons, that combine into complex neurons and further endstopped neurons provide local curvature neurons that are selective for shape stimuli.</p>
      <p>The main element in this architecture is that of the model shape-selective neuron, that represents curvature parts in a curvature<inline-formula id="pone.0042058.e077"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e077" xlink:type="simple"/></inline-formula>position (radial and angular) domain. The possible number of shapes that may be represented by our model shape neurons is very large, considering the limited types of neurons at each level of the architecture. Even though the primate visual system and our model have the capability to represent a virtual infinity of shapes, the way to handle the large but finite number of shapes in our world may be achieved through learning, selecting those configurations of curvatures relevant to recognize the shapes around us based on our visual experiences. Since the representation has the capability to represent any shape, a new shape can be easily incorporated into the system. The model supports a recognition by parts strategy, in which the parts are curvature values at different positions, as suggested also by Connor's group <xref ref-type="bibr" rid="pone.0042058-Brincat1">[54]</xref>. We have compared the response of our model shape neurons with 75 real neurons from <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[56]</xref>. The results obtained by the model are very similar to those of the neurons, and accomplished without any learning or classifier method.</p>
      <p>Our model local curvature neurons do not provide an exact value of curvature but can discriminate among degrees of curvature (e.g. 4 in <xref ref-type="fig" rid="pone-0042058-g002">Figure 2C</xref>). This was done using a starting point where V1 is composed of neurons of different sizes. Through the use of different neuronal sizes and the integration of model simple neurons into model complex neurons we obtained model endstopped neurons able to discriminate between degrees of curvature, from very sharp to very broad (<xref ref-type="fig" rid="pone-0042058-g002">Figure 2C</xref>). It is important to note as well that these neurons do not provide a binary response for a given curve; model local curvature neurons provide a band-pass curvature filtering, with the highest response to the selective curvature and a decaying response that is inversely proportional to the curvature distances in curvature space. The response of model endstopped and curvature neurons over a range of curvatures have a Gaussian shape (<xref ref-type="fig" rid="pone-0042058-g002">Figure 2C</xref>), as well as a model shape neuron (<xref ref-type="fig" rid="pone-0042058-g004">Figure 4B</xref>). There is no maximum selection from the responses from early areas, so, no information is lost when ascending the hierarchy in a feedforward direction. However, there is a max selection computation at the last stage of the hierarchy, the shape cells, where it no longer affects further decisions, in keeping with Marr's Principle of Least Commitment <xref ref-type="bibr" rid="pone.0042058-Marr2">[27]</xref>. We consider that any attentive selection, filtering or bias <xref ref-type="bibr" rid="pone.0042058-Tsotsos1">[58]</xref>–<xref ref-type="bibr" rid="pone.0042058-Tsotsos2">[61]</xref> in such a hierarchy would occur top-down and leave that for future work. Interestingly, our model of sign endstopped neurons could provide a foundation to deal with the border-ownership problem. Sign endstopped neurons could represent opponent channels <xref ref-type="bibr" rid="pone.0042058-Zhou1">[62]</xref>, and this combined with feedback modulation through a model of attention (e.g. <xref ref-type="bibr" rid="pone.0042058-Tsotsos1">[58]</xref>) would further support a model such as the one presented by <xref ref-type="bibr" rid="pone.0042058-Craft1">[33]</xref> on border ownership.</p>
      <p>Our model may be considered as a major extension of the works <xref ref-type="bibr" rid="pone.0042058-Fukushima1">[9]</xref> and <xref ref-type="bibr" rid="pone.0042058-Dobbins1">[28]</xref>, <xref ref-type="bibr" rid="pone.0042058-Dobbins2">[30]</xref>. In a similar work, Serre, Cadieu and colleagues construct a hierarchical representation with a first layer computing oriented edge responses. This is followed by a maximum response selection layer that feeds a pooling stage that groups spatial piece-wise linear elements. This strategy - borrowed from Fukushima's NeoCognitron <xref ref-type="bibr" rid="pone.0042058-Fukushima1">[9]</xref> - is repeated for each layer of the hierarchy. Curved lines are thus approximated by linear pieces and there is no direct computation of curvature of any form. Another related model, based on excitatory connections is the one proposed by Amit <xref ref-type="bibr" rid="pone.0042058-Amit1">[20]</xref>. One important difference (among others) between our model and these types of models is that we use inhibition for curvature representation through endstopping instead of purely excitatory components. Inhibitory flankers as proposed in our model have been strongly supported by neurophysiological studies <xref ref-type="bibr" rid="pone.0042058-Hubel3">[39]</xref>, <xref ref-type="bibr" rid="pone.0042058-Kato1">[44]</xref>–<xref ref-type="bibr" rid="pone.0042058-Willmore1">[46]</xref>, <xref ref-type="bibr" rid="pone.0042058-Orban2">[48]</xref>, <xref ref-type="bibr" rid="pone.0042058-Bishop1">[49]</xref> and since our goal is to test the computational embodiment of these neurophysiological results, this necessarily figures prominently in our model. It is an aspect that is considered of great importance by neuroscientists <xref ref-type="bibr" rid="pone.0042058-Willmore1">[46]</xref>, and surprisingly has been neglected in models to date.</p>
      <p>Given that it seems accepted that the visual system computes increasingly abstract quantities as a signal ascends the visual processing hierarchy, are those quantities computed by applying the same computation and thus neural convergence alone suffices to achieve abstraction, or, is it truly necessary to include more sophisticated computations layer by layer? This is not easy to answer in the general case. However, we can point to one important instance that supports the latter position. In our previous work where we look at motion processing <xref ref-type="bibr" rid="pone.0042058-Tsotsos3">[63]</xref>, we found that simple neural convergence did not suffice. We needed to include a layer of neurons selective to the spatial derivative of velocity, a much more complex construct. This is supported by neurophysiology in monkey <xref ref-type="bibr" rid="pone.0042058-Treue1">[64]</xref>, <xref ref-type="bibr" rid="pone.0042058-Meese1">[65]</xref> and by our own fMRI human studies <xref ref-type="bibr" rid="pone.0042058-MartnezTrujillo1">[66]</xref>. Similarly, for shape representation, although our approach is also based on a hierarchical set of computations, we deploy different processes at each layer, not simply repetitions of the same process. Those different processes are intended to reflect the reality of the different neural computations in the visual cortex. Our approach is distinct in that we perform a direct computation of curvature and the sign of curvature. We develop that computation using well documented neural computation types that include not only oriented simple cells and complex cells (as the pooling layer of others is intended to capture) but also endstopped cells, curvature cells, and curvature sign cells. These naturally provide a sufficient basis for the definition of shape cells, a basis that not only mirrors neurophysiological reality of the visual cortex better, but also provides a richer substrate for shape definition than piecewise linear components. This is the first model of shape representation (to the best of our knowledge) to include aforementioned cells in intermediate layers departing from the near universal previous use of Fukushima's <italic>S</italic> and <italic>C</italic> types of cells.</p>
      <p>The role of learning from examples also differs between our work and those mentioned. Although a statistical learning approach such as that employed by Serre, Cadieu and colleagues for all of the layers of their processing hierarchy except for the first, is valuable when there is no other option, we show that in the case of the successive representations, namely those computed by endstopped and curvature cells, there is now sufficient knowledge to directly model these cells and to do so with a significantly high degree of fidelity. Learning is not required if the appropriate representations are selected in the first place.</p>
      <p>Although this paper does not address object recognition directly, it may provide important contributions to elements that may advance the state-of-the-art. In a previous paper <xref ref-type="bibr" rid="pone.0042058-RodrguezSnchez1">[34]</xref>, we connected the 2DSIL representation to a recognition system and compared its performance in object recognition tasks with several other systems including benchmark systems. Our system performed well beating other systems in several categories while maintaining comparable performance in others. Following previous authors such as Zucker and Marr, we advocate that deeper understanding of visual processes in humans and non-human primates can lead to important advancements in perceptual theories and computational systems.</p>
      <p>With the model introduced in this paper we follow the steps of early theories of vision <xref ref-type="bibr" rid="pone.0042058-Fukushima1">[9]</xref>, <xref ref-type="bibr" rid="pone.0042058-Marr1">[26]</xref>, <xref ref-type="bibr" rid="pone.0042058-Zucker1">[67]</xref> and propose how to – following the philosophy of those influential works – take modeling to a next stage by incorporating new intermediate layer computations hoping future works will continue building on these hierarchies aimed at modeling the visual cortex.</p>
    </sec>
    <sec id="s4" sec-type="materials|methods">
      <title>Materials and Methods</title>
      <p>We used the same stimuli created for <xref ref-type="bibr" rid="pone.0042058-Pasupathy1">[52]</xref>, <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[56]</xref>. In order to construct the stimuli, a Matlab program was provided by Dr. Anitha Pasupathy. The stimuli were constructed combining convex and concave boundary elements to form closed shapes. Boundary elements include sharp convex angles, and medium and high convex and concave curvatures. The combination of these boundary elements gave rise to 49 different stimuli. Stimuli were composed of white edges against a black background, the inside was black as well but it is shown in our figures (<xref ref-type="fig" rid="pone-0042058-g005">Figures 5</xref>, <xref ref-type="fig" rid="pone-0042058-g006">6</xref>, <xref ref-type="fig" rid="pone-0042058-g007">7</xref>, and <xref ref-type="fig" rid="pone-0042058-g008">8</xref>) as white-filled for illustration purposes. For the experiment, stimuli were those 49 shapes but rotated to 8 orientations (some only 2 or 4 due to redundancies) in 45<inline-formula id="pone.0042058.e078"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e078" xlink:type="simple"/></inline-formula> increments to give a total of 366 different shapes. Stimuli are shown in <xref ref-type="fig" rid="pone-0042058-g005">Figures 5</xref>, <xref ref-type="fig" rid="pone-0042058-g006">6</xref>, <xref ref-type="fig" rid="pone-0042058-g007">7</xref>, and <xref ref-type="fig" rid="pone-0042058-g008">8</xref>.</p>
      <p>Experiments were run on Matlab in a Mac G5 PowerPC. The input to the model is a gray-value image. Images used are 400<inline-formula id="pone.0042058.e079"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e079" xlink:type="simple"/></inline-formula>400 pixels, a shape would span 300<inline-formula id="pone.0042058.e080"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e080" xlink:type="simple"/></inline-formula>300 pixels and correspond to the stimuli used in the aforementioned study. For our experiments, we used 12 orientations (0<inline-formula id="pone.0042058.e081"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e081" xlink:type="simple"/></inline-formula>, 15<inline-formula id="pone.0042058.e082"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e082" xlink:type="simple"/></inline-formula>, 30<inline-formula id="pone.0042058.e083"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e083" xlink:type="simple"/></inline-formula>, 45<inline-formula id="pone.0042058.e084"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e084" xlink:type="simple"/></inline-formula>, 60<inline-formula id="pone.0042058.e085"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e085" xlink:type="simple"/></inline-formula>, 75<inline-formula id="pone.0042058.e086"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e086" xlink:type="simple"/></inline-formula>, 90<inline-formula id="pone.0042058.e087"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e087" xlink:type="simple"/></inline-formula>, 105<inline-formula id="pone.0042058.e088"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e088" xlink:type="simple"/></inline-formula>, 120<inline-formula id="pone.0042058.e089"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e089" xlink:type="simple"/></inline-formula>, 135<inline-formula id="pone.0042058.e090"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e090" xlink:type="simple"/></inline-formula>, 150<inline-formula id="pone.0042058.e091"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e091" xlink:type="simple"/></inline-formula>, 165<inline-formula id="pone.0042058.e092"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e092" xlink:type="simple"/></inline-formula>) and 4 different sizes for model simple cells, this gives a total of 48 types. Size of V1 model simple neurons are 40, 60, 88 and 120 pixels, their corresponding values for AR are 0.7, 1.4, 2.15 and 3 respectively, WR is 2.5 for all model neurons.</p>
      <p>For the integration into model endstopped neurons, the values of gain <italic>c</italic> (Equation 4) for displaced neurons were from the smaller to the larger cell: <inline-formula id="pone.0042058.e093"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e093" xlink:type="simple"/></inline-formula> = <inline-formula id="pone.0042058.e094"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e094" xlink:type="simple"/></inline-formula> = {1.5, 1.25, 1, 3}, <inline-formula id="pone.0042058.e095"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e095" xlink:type="simple"/></inline-formula> = 1 for all centre cells. For the chosen parameters, cells respond (90% of their maximum value) to the following ranges of curvature radius: 6 to 11, 25 to 52, 48 to 77 and 140 to 301 pixels. Refer also to <xref ref-type="fig" rid="pone-0042058-g002">Figure 2C</xref> for an example on how the selection of these parameters (size, AR, WR and gain) affect neuronal curvature selectivity. The parameters for the rectification function (Equation 5) were <inline-formula id="pone.0042058.e096"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e096" xlink:type="simple"/></inline-formula> = 0.01 and <inline-formula id="pone.0042058.e097"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e097" xlink:type="simple"/></inline-formula> is the maximum response of the set of neurons for a given scale divided by 8.5, a factor that provided a good normalization approximation for this sigmoidal saturation function. The displacement values for model endstopped neurons selective to degrees of curvature was 1/2 the size of the simple neuron component along its preferred orientation. Displacements for the model sign endstopped neurons were from smaller to larger: 1/5 the size, 1/4 the size, 1/4 the size and 2/5 the size along the orientations stated in Equation 6. The 4 types of model endstopped neurons and the curvature direction selective neurons lead to eight curvatures. In order to obtain the aforementioned parameter values, a program designed to evaluate different parameter values was created. The target of this program was to obtain values that would provide neurons able to separate different degrees of curvature, providing a graph such as the one shown in <xref ref-type="fig" rid="pone-0042058-g002">Figure 2C</xref>.</p>
      <p>Neuron responses were provided by Dr. Anitha Pasupathy for the comparison with model shape neuron responses. In their influential study <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[56]</xref>, the results from 109 neurons are reported for 366 different stimuli. We compared with 75 out of those 109 neurons, the reason for this as well as the detailed process are explained next. Due to the enormous range of shape representation of the model, we needed to select (or <italic>isolate</italic> in neurophysiological terms) a subset of model shape neurons that would correspond to their 109 subset of V4 biological counterparts recorded in <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[56]</xref>. In order to do this, we created new stimulus images and stored their model shape representation. The way these stimuli were created was by superimposing the stimuli for which the biological neuronal responses were on the 70% maximum percentile (e.g. <xref ref-type="fig" rid="pone-0042058-g010">Figure 10A</xref>). This simple process would give us an insight on the selectivity of the 109 biological neurons and is similar to the way <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[56]</xref> analyzes the selectivity of 4 neurons (<xref ref-type="fig" rid="pone-0042058-g002">Figures 2</xref>, <xref ref-type="fig" rid="pone-0042058-g004">4</xref>, <xref ref-type="fig" rid="pone-0042058-g005">5</xref> and <xref ref-type="fig" rid="pone-0042058-g008">8</xref> on that work). That is, we consider the stimuli that maximize the neuron responses to reach the conclusion that a neuron is selective to some type of curvature at a specified position, e.g. in <xref ref-type="fig" rid="pone-0042058-g010">Figure 10A</xref> it is clear that this biological neuron is selective for a sharp curvature at the top-right, flanked by a broad concavity that ends in a medium convexity on the left side of the stimulus. Then, this image would be modified such as to only keep the relevant curvatures. This is the stimulus used to isolate our model shape neurons. This would also be the stimulus for which the model shape neuron response is maximum.</p>
      <fig id="pone-0042058-g010" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0042058.g010</object-id>
        <label>Figure 10</label>
        <caption>
          <title>How the features for <italic>isolating</italic> a Shape neuron are obtained.</title>
          <p>See text.</p>
        </caption>
        <graphic mimetype="image" orientation="portrait" position="float" xlink:href="info:doi/10.1371/journal.pone.0042058.g010" xlink:type="simple"/>
      </fig>
      <p>We repeated this process for the 109 biological neurons, but 34 of them failed to provide any clear insight on their selectivity using the present process (e.g. <xref ref-type="fig" rid="pone-0042058-g010">Figure 10D</xref>). On the other hand, the other 75 provided a very clear picture on their selectivities (<xref ref-type="fig" rid="pone-0042058-g010">Figure 10A–C</xref>). We then stored the representation (<xref ref-type="fig" rid="pone-0042058-g004">Figure 4A</xref>) of each shape model neuron for the stimuli created the way explained above. The weights <inline-formula id="pone.0042058.e098"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e098" xlink:type="simple"/></inline-formula> (Equation 8) are derived from the responses from the eight curvature classes model neurons at their different positions. Model shape neuron's receptive fields were organized into angular-radial bins (<xref ref-type="fig" rid="pone-0042058-g004">Figure 4A</xref>) of 10 pixels for radial values and <inline-formula id="pone.0042058.e099"><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0042058.e099" xlink:type="simple"/></inline-formula>/45 for angular values. A smaller bin size did not provide significantly better results while having a much higher computational load.</p>
      <p>For each one of the model shape neurons isolated this way, we recorded responses for each of the 366 stimuli in <xref ref-type="bibr" rid="pone.0042058-Pasupathy3">[56]</xref>. Each response is normalized in the 0–1 range using the maximum response for the created stimulus as explained before as the normalization factor. These normalized responses were compared to their biological counterparts (responses already normalized) and the absolute value of the difference was computed for each one of the 366 stimuli. <xref ref-type="fig" rid="pone-0042058-g009">Figure 9</xref> shows the results of these averaged values with their corresponding standard deviations for each neuron.</p>
    </sec>
  </body>
  <back>
    <ack>
      <p>The authors would like to thank Prof. Anitha Pauspathy for providing all the stimuli and data we needed to compare our model neuron responses with that of real neurons. We appreciate as well the helpful comments of Prof. Allan C. Dobbins.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pone.0042058-RamnyCajal1">
        <label>1</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ramón y Cajal</surname><given-names>S</given-names></name> (<year>1888</year>) <article-title>Sobre las fibras nerviosas de la capa molecular del cerebelo</article-title>. <source>Revista Trimestral de Histología Normal y Patologica</source> <volume>1</volume>: <fpage>33</fpage>–<lpage>49</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-RamnyCajal2">
        <label>2</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ramón y Cajal</surname><given-names>S</given-names></name> (<year>1894</year>) <article-title>The croonian lecture: La fine structure des centres nerveux</article-title>. <source>Royal Society of London Proceedings Series I</source> <volume>55</volume>: <fpage>444</fpage>–<lpage>468</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-RamnyCajal3">
        <label>3</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ramón y Cajal</surname><given-names>S</given-names></name> (<year>1904</year>) <article-title>Variaciones morfólogicas, normales y patológicas del retículo neurofibrilar</article-title>. <source>Trabajos del Laboratorio de Investigaciones Biólogicas Madrid</source> <volume>3</volume>: <fpage>9</fpage>–<lpage>15</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Jones1">
        <label>4</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jones</surname><given-names>E</given-names></name> (<year>2007</year>) <article-title>Neuroanatomy: Cajal and after cajal</article-title>. <source>Brain Research Reviews</source> <volume>55</volume>: <fpage>248</fpage>–<lpage>255</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Grossberg1">
        <label>5</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grossberg</surname><given-names>S</given-names></name> (<year>1968</year>) <article-title>Some nonlinear networks capable of learning a spatial pattern of arbitrary complexity</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>59</volume>: <fpage>368</fpage>–<lpage>72</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Grossberg2">
        <label>6</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grossberg</surname><given-names>S</given-names></name> (<year>1970</year>) <article-title>Neural pattern discrimination</article-title>. <source>Journal of Theoretical Biology</source> <volume>27</volume>: <fpage>291</fpage>–<lpage>337</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Grossberg3">
        <label>7</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grossberg</surname><given-names>S</given-names></name> (<year>1971</year>) <article-title>Pavlovian pattern learning by nonlinear neural networks</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source> <volume>68</volume>: <fpage>828</fpage>–<lpage>31</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Grossberg4">
        <label>8</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grossberg</surname><given-names>S</given-names></name> (<year>1975</year>) <article-title>A neural model of attention, reinforcement and discrimination learning</article-title>. <source>International Review of Neurobiology</source> <volume>18</volume>: <fpage>263</fpage>–<lpage>327</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Fukushima1">
        <label>9</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fukushima</surname><given-names>K</given-names></name> (<year>1980</year>) <article-title>Neocognitron: a self organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</article-title>. <source>Biological Cybernetics</source> <volume>36</volume>: <fpage>193</fpage>–<lpage>202</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Fukushima2">
        <label>10</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fukushima</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Miyake</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Ito</surname><given-names>T</given-names></name> (<year>1983</year>) <article-title>Neocognitron: A neural network model for a mechanism of visual patter recognition</article-title>. <source>IEEE Transactions on Systems, Man and Cybernetics</source> <volume>13</volume>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Rumelhart1">
        <label>11</label>
        <mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Rumelhart</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Mcclelland</surname><given-names>J</given-names></name> (<year>1986</year>) <source>Parallel Distributed Processing: Explorations in the Microstructure of Cognition</source>. <publisher-name>MIT Press</publisher-name>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Lecun1">
        <label>12</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lecun</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Boser</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Denker</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Henderson</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Howard</surname><given-names>R</given-names></name>, <etal>et al</etal>. (<year>1989</year>) <article-title>Backpropagation applied to handwritten zip code recognition</article-title>. <source>Neural Computation</source> <volume>1</volume>: <fpage>541</fpage>–<lpage>551</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Lecun2">
        <label>13</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lecun</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Bottou</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Bengio</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Haffner</surname><given-names>P</given-names></name> (<year>1998</year>) <article-title>Gradient-based learning applied to document recognition</article-title>. <source>Proceedings of the IEEE</source> <volume>86</volume>: <fpage>2278</fpage>–<lpage>2324</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Wallis1">
        <label>14</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wallis</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Rolls</surname><given-names>E</given-names></name> (<year>1997</year>) <article-title>Invariant face and object recognition in the visual system</article-title>. <source>Progress in Neurobiology</source> <volume>51</volume>: <fpage>167</fpage>–<lpage>194</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Riesenhuber1">
        <label>15</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Riesenhuber</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name> (<year>1999</year>) <article-title>Hierarchical models of object recognition in cortex</article-title>. <source>Nature Neuroscience</source> <volume>2</volume>: <fpage>1019</fpage>–<lpage>1025</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Riesenhuber2">
        <label>16</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Riesenhuber</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name> (<year>2000</year>) <article-title>Models of object recognition</article-title>. <source>Nature Neuroscience</source> <volume>3 Suppl</volume>: <fpage>1199</fpage>–<lpage>1204</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Riesenhuber3">
        <label>17</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Riesenhuber</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name> (<year>2002</year>) <article-title>Neural mechanisms of object recognition</article-title>. <source>Current Opinion in Neurobiology</source> <volume>12</volume>: <fpage>162</fpage>–<lpage>168</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Serre1">
        <label>18</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Serre</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Wolf</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name> (<year>2005</year>) <article-title>Object recognition with features inspired by visual cortex</article-title>. <source>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source> <fpage>994</fpage>–<lpage>1000</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Serre2">
        <label>19</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Serre</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Wolf</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Bileschi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Riesenhuber</surname><given-names>M</given-names></name> (<year>2007</year>) <article-title>Robust object recognition with cortex-like mechanisms</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source> <volume>29</volume>: <fpage>411</fpage>–<lpage>426</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Amit1">
        <label>20</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>Y</given-names></name> (<year>2000</year>) <article-title>A neural network architecture for visual selection</article-title>. <source>Neural Computation</source> <volume>12</volume>: <fpage>1141</fpage>–<lpage>1164</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Amit2">
        <label>21</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Mascaro</surname><given-names>M</given-names></name> (<year>2003</year>) <article-title>An integrated network for invariant visual detection and recognition</article-title>. <source>Vision Research</source> <volume>93</volume>: <fpage>2073</fpage>–<lpage>2088</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Suzuki1">
        <label>22</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Suzuki</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Hashimoto</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Kashimori</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Zheng</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Kambara</surname><given-names>T</given-names></name> (<year>2004</year>) <article-title>A neural model of predictive recognition in form patway of visual cortex</article-title>. <source>BioSystems</source> <volume>76</volume>: <fpage>33</fpage>–<lpage>42</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Rao1">
        <label>23</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rao</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Ballard</surname><given-names>D</given-names></name> (<year>1997</year>) <article-title>Dynamic model of visual recognition predicts neural response properties in the visual cortex</article-title>. <source>Neural Computation</source> <volume>9</volume>: <fpage>721</fpage>–<lpage>763</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Rao2">
        <label>24</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rao</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Ballard</surname><given-names>D</given-names></name> (<year>1999</year>) <article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title>. <source>Nature Neuroscience</source> <volume>2</volume>: <fpage>79</fpage>–<lpage>87</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Cadieu1">
        <label>25</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cadieu</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Kouch</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Connor</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Riesenhuber</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Poggio</surname><given-names>T</given-names></name> (<year>2007</year>) <article-title>A model of v4 shape selectivity and invariance</article-title>. <source>Journal of Neurlophysiology</source> <volume>3</volume>: <fpage>1733</fpage>–<lpage>1750</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Marr1">
        <label>26</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marr</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Hildreth</surname><given-names>E</given-names></name> (<year>1980</year>) <article-title>Theory of edge detection</article-title>. <source>Proceedings of the Royal Society of London, series B, Biological Sciences</source> <volume>207</volume>: <fpage>187</fpage>–<lpage>217</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Marr2">
        <label>27</label>
        <mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Marr</surname><given-names>D</given-names></name> (<year>1982</year>) <source>Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</source>. <publisher-name>W.H. Freeman</publisher-name>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Dobbins1">
        <label>28</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dobbins</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Zucker</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Cynader</surname><given-names>M</given-names></name> (<year>1987</year>) <article-title>Endstopped neurons in the visual cortex as a substrate for calculating curvature</article-title>. <source>Nature</source> <volume>329</volume>: <fpage>438</fpage>–<lpage>441</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Heitger1">
        <label>29</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Heitger</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Rosenthaler</surname><given-names>L</given-names></name>, <name name-style="western"><surname>von der Heydt</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Peterhans</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Kubler</surname><given-names>O</given-names></name> (<year>1992</year>) <article-title>Simulation of neural contour mechanisms: from simple to end-stopped cells</article-title>. <source>Vision Research</source> <volume>32</volume>: <fpage>963</fpage>–<lpage>81</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Dobbins2">
        <label>30</label>
        <mixed-citation publication-type="other" xlink:type="simple">Dobbins A (1992) Difference models of Visual Cortical neurons. Ph.D. thesis, Department of Electrical Engineering. McGill University.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Heitger2">
        <label>31</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Heitger</surname><given-names>F</given-names></name>, <name name-style="western"><surname>von der Heydt</surname><given-names>R</given-names></name> (<year>1993</year>) <article-title>A computational model of neural contour processing: figureground segregation and illusory contours</article-title>. <source>Proceedings of the IEEE International Conference on Computer Vision</source> <fpage>32</fpage>–<lpage>40</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Heitger3">
        <label>32</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Heitger</surname><given-names>F</given-names></name>, <name name-style="western"><surname>von der Heydt</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Peterhans</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Rosenthaler</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Kübler</surname><given-names>O</given-names></name> (<year>1998</year>) <article-title>Simulation of neural contour mechanisms: representing anomalous contours</article-title>. <source>Image and Vision Computing</source> <volume>16</volume>: <fpage>409</fpage>–<lpage>423</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Craft1">
        <label>33</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Craft</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Schuetze</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Niebur</surname><given-names>E</given-names></name>, <name name-style="western"><surname>von der Heydt</surname><given-names>R</given-names></name> (<year>2007</year>) <article-title>A neural model of figure-ground organization</article-title>. <source>Journal of Neurophysiology</source> <volume>97</volume>: <fpage>4310</fpage>–<lpage>4326</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-RodrguezSnchez1">
        <label>34</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rodríguez-Sánchez</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Tsotsos</surname><given-names>J</given-names></name> (<year>2011</year>) <article-title>The importance of intermediate representations for the modeling of 2 d shape detection: Endstopping and curvature tuned computations</article-title>. <source>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source> <fpage>4321</fpage>–<lpage>4326</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Marcelja1">
        <label>35</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marcelja</surname><given-names>S</given-names></name> (<year>1980</year>) <article-title>Mathematical description of the responses of simple cortical cells</article-title>. <source>Journal of Optical Society of America</source> <volume>70</volume>: <fpage>1297</fpage>–<lpage>1300</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Hawken1">
        <label>36</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hawken</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Parker</surname><given-names>A</given-names></name> (<year>1987</year>) <article-title>Spatial properties of neurons in the monkey striate cortex</article-title>. <source>Proceedings of the Royal Society of London, series B, Biological Sciences</source> <volume>231</volume>: <fpage>251</fpage>–<lpage>288</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Hubel1">
        <label>37</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hubel</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Wiesel</surname><given-names>T</given-names></name> (<year>1959</year>) <article-title>Receptive fields of single neurones in the cat's striate cortex</article-title>. <source>The Journal of Physiology</source> <volume>148</volume>: <fpage>574</fpage>–<lpage>591</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Hubel2">
        <label>38</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hubel</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Wiesel</surname><given-names>T</given-names></name> (<year>1962</year>) <article-title>Receptive fields, binocular interaction and functional architecture in the cat's visual cortex</article-title>. <source>Journal of Physiology</source> <volume>160</volume>: <fpage>106</fpage>–<lpage>154</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Hubel3">
        <label>39</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hubel</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Wiesel</surname><given-names>T</given-names></name> (<year>1968</year>) <article-title>Receptive fields and functional architecture of monkey striate cortex</article-title>. <source>Journal of Physiology</source> <volume>195</volume>: <fpage>215</fpage>–<lpage>243</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Spitzer1">
        <label>40</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Spitzer</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Hochstein</surname><given-names>S</given-names></name> (<year>1985</year>) <article-title>A complex-cell receptive-field model</article-title>. <source>Journal of Neurophysiology</source> <volume>53</volume>: <fpage>1266</fpage>–<lpage>1286</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Geman1">
        <label>41</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Geman</surname><given-names>S</given-names></name> (<year>2006</year>) <article-title>Invariance and selectivity in the ventral visual pathway</article-title>. <source>Journal of Physiology Paris</source> <volume>100</volume>: <fpage>212</fpage>–<lpage>224</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-vonderHeydt1">
        <label>42</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>von der Heydt</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Peterhans</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Baumgartner</surname><given-names>G</given-names></name> (<year>1984</year>) <article-title>Illusory contours and cortical neuron responses</article-title>. <source>Science</source> <volume>224</volume>: <fpage>1260</fpage>–<lpage>1262</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Ito1">
        <label>43</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ito</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Komatsu</surname><given-names>H</given-names></name> (<year>2004</year>) <article-title>Representation of angles embedded within contour stimuli in area v2 of macaque monkeys</article-title>. <source>Journal of Neuroscience</source> <volume>24</volume>: <fpage>3313</fpage>–<lpage>3324</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Kato1">
        <label>44</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kato</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Bishop</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Orban</surname><given-names>G</given-names></name> (<year>1978</year>) <article-title>Hypercomplex and simple/complex cells classifications in cat striate cortex</article-title>. <source>Journal of Neurophysiology</source> <fpage>1071</fpage>–<lpage>1095</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Orban1">
        <label>45</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Orban</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Kato</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Bishop</surname><given-names>P</given-names></name> (<year>1979</year>) <article-title>Dimensions and properties of end-zone inhibitory areas of hypercomplex cells in cat striate cortex</article-title>. <source>Journal of Neurophysiology</source> <volume>42</volume>: <fpage>833</fpage>–<lpage>849</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Willmore1">
        <label>46</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Willmore</surname><given-names>BD</given-names></name>, <name name-style="western"><surname>Prenger</surname><given-names>RJ</given-names></name>, <name name-style="western"><surname>Gallant</surname><given-names>JL</given-names></name> (<year>2010</year>) <article-title>Neural representation of natural images in visual area v2</article-title>. <source>The Journal of Neuroscience</source> <volume>30</volume>: <fpage>2102</fpage>–<lpage>14</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Dobbins3">
        <label>47</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dobbins</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Zucker</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Cynader</surname><given-names>M</given-names></name> (<year>1989</year>) <article-title>Endstopping and curvature</article-title>. <source>Vision Research</source> <volume>29</volume>: <fpage>1371</fpage>–<lpage>1387</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Orban2">
        <label>48</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Orban</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Kato</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Bishop</surname><given-names>P</given-names></name> (<year>1979</year>) <article-title>End-zone region in receptive fields of hypercomplex and other striate neurons in the cat</article-title>. <source>Journal of Neurophysiology</source> <volume>42</volume>: <fpage>818</fpage>–<lpage>832</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Bishop1">
        <label>49</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bishop</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Kato</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Orban</surname><given-names>G</given-names></name> (<year>1980</year>) <article-title>Direction-selective cells in complex family in cat striate cortex</article-title>. <source>Journal of Neurophysiology</source> <volume>43</volume>: <fpage>1266</fpage>–<lpage>1283</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Hegde1">
        <label>50</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hegde</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Van Essen</surname><given-names>DC</given-names></name> (<year>2000</year>) <article-title>Selectivity for complex shapes in primate visual area v2</article-title>. <source>The Journal of Neuroscience</source> <volume>20</volume>: <fpage>61</fpage>–<lpage>66</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Gallant1">
        <label>51</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gallant</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Braun</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Van Essen</surname><given-names>D</given-names></name> (<year>1993</year>) <article-title>Selectivity for polar, hyperbolic, and cartesian gratings in macaque visual cortex</article-title>. <source>Science</source> <volume>259</volume>: <fpage>100</fpage>–<lpage>103</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Pasupathy1">
        <label>52</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pasupathy</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Connor</surname><given-names>C</given-names></name> (<year>2002</year>) <article-title>Population coding of shape in area v4</article-title>. <source>Nature Neuroscience</source> <volume>5</volume>: <fpage>1332</fpage>–<lpage>1338</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Tanaka1">
        <label>53</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tanaka</surname><given-names>K</given-names></name> (<year>1996</year>) <article-title>Inferotemporal cortex and object vision</article-title>. <source>Annual Review on Neuroscience</source> <volume>19</volume>: <fpage>109</fpage>–<lpage>139</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Brincat1">
        <label>54</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brincat</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Connor</surname><given-names>C</given-names></name> (<year>2004</year>) <article-title>Underlying principles of visual shape selectivity in posterior inferotemporal cortex</article-title>. <source>Nature Neuroscience</source> <volume>7</volume>: <fpage>880</fpage>–<lpage>886</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Pasupathy2">
        <label>55</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pasupathy</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Connor</surname><given-names>C</given-names></name> (<year>1999</year>) <article-title>Responses to contour features in macaque area v4</article-title>. <source>Journal of Neurophysiology</source> <volume>82</volume>: <fpage>2490</fpage>–<lpage>2502</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Pasupathy3">
        <label>56</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pasupathy</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Connor</surname><given-names>C</given-names></name> (<year>2001</year>) <article-title>Shape representation in area v4: Position-specific tuning for boundary conformation</article-title>. <source>Journal of Neurophysiology</source> <volume>86</volume>: <fpage>2505</fpage>–<lpage>2519</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Boussaoud1">
        <label>57</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Boussaoud</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Desimone</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Ungerleider</surname><given-names>L</given-names></name> (<year>1991</year>) <article-title>Visual topography of area teo in the macaque</article-title>. <source>The Journal of Comparative Neurology</source> <volume>306</volume>: <fpage>554</fpage>–<lpage>575</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Tsotsos1">
        <label>58</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsotsos</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Culhane</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Winky</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Lai</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Davis</surname><given-names>N</given-names></name>, <etal>et al</etal>. (<year>1995</year>) <article-title>Modeling visual attention via selective tuning</article-title>. <source>Artificial Intelligence</source> <volume>78</volume>: <fpage>507</fpage>–<lpage>545</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Desimone1">
        <label>59</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Desimone</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Duncan</surname><given-names>J</given-names></name> (<year>1995</year>) <article-title>Neural mechanisms of selective visual attention</article-title>. <source>Annual Review on Neuroscience</source> <volume>18</volume>: <fpage>193</fpage>–<lpage>222</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-RodrguezSnchez2">
        <label>60</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rodríguez-Sánchez</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Simine</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Tsotsos</surname><given-names>J</given-names></name> (<year>2007</year>) <article-title>Attention and visual search</article-title>. <source>International Journal of Neural Systems</source> <volume>17</volume>: <fpage>275</fpage>–<lpage>288</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Tsotsos2">
        <label>61</label>
        <mixed-citation publication-type="other" xlink:type="simple">Tsotsos JK (2011) A Computational Perspective on Visual Attention. The MIT Press.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Zhou1">
        <label>62</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhou</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Friedman</surname><given-names>HS</given-names></name>, <name name-style="western"><surname>von der Heydt</surname><given-names>R</given-names></name> (<year>2000</year>) <article-title>Coding of border ownership in monkey visual cortex</article-title>. <source>The Journal of Neuroscience</source> <volume>20</volume>: <fpage>6594</fpage>–<lpage>6611</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Tsotsos3">
        <label>63</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsotsos</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Martíinez-Trujillo</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Pomplun</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Simine</surname><given-names>E</given-names></name>, <etal>et al</etal>. (<year>2005</year>) <article-title>Attending to visual motion</article-title>. <source>Computer Vision and Image Understanding</source> <volume>100</volume>: <fpage>3</fpage>–<lpage>43</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Treue1">
        <label>64</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Treue</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Andersen</surname><given-names>R</given-names></name> (<year>1996</year>) <article-title>Neural responses to velocity gradients in macaque cortical area mt</article-title>. <source>Visual Neuroscience</source> <volume>13</volume>: <fpage>797</fpage>–<lpage>804</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Meese1">
        <label>65</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meese</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Anderson</surname><given-names>S</given-names></name> (<year>2002</year>) <article-title>Spiral mechanisms are required to account for summation of complex motion components</article-title>. <source>Vision Research</source> <volume>42</volume>: <fpage>1073</fpage>–<lpage>1080</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-MartnezTrujillo1">
        <label>66</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Martínez-Trujillo</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Tsotsos</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Simine</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Pomplun</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Wildes</surname><given-names>R</given-names></name>, <etal>et al</etal>. (<year>2005</year>) <article-title>Selectivity for speed gradients in human area mt/v5</article-title>. <source>Neuroreport</source> <volume>16</volume>: <fpage>435</fpage>–<lpage>438</lpage>.</mixed-citation>
      </ref>
      <ref id="pone.0042058-Zucker1">
        <label>67</label>
        <mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zucker</surname><given-names>SW</given-names></name> (<year>1981</year>) <article-title>Computer vision and human perception: An essay on the discovery of constraints</article-title>. <source>Proceedings of the International Conference on Artificial Intelligence</source> <fpage>1102</fpage>–<lpage>1116</lpage>.</mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>