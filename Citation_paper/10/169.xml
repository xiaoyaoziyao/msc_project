<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-16-29871</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0173392</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Machine learning algorithms</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject><subj-group><subject>Machine learning algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Artificial intelligence</subject><subj-group><subject>Machine learning</subject><subj-group><subject>Machine learning algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject><subj-group><subject>Bioacoustics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Bioacoustics</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Acoustics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Music cognition</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Music cognition</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Music cognition</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Music perception</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Mathematical and statistical techniques</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Regression analysis</subject><subj-group><subject>Linear regression analysis</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Statistics (mathematics)</subject><subj-group><subject>Statistical methods</subject><subj-group><subject>Regression analysis</subject><subj-group><subject>Linear regression analysis</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Developing a benchmark for emotional analysis of music</article-title>
<alt-title alt-title-type="running-head">Developing a benchmark for emotional analysis of music</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7119-8312</contrib-id>
<name name-style="western">
<surname>Aljanaki</surname> <given-names>Anna</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="fn" rid="currentaff001"><sup>¤</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Yang</surname> <given-names>Yi-Hsuan</given-names></name>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Soleymani</surname> <given-names>Mohammad</given-names></name>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Utrecht University, Utrecht, the Netherlands</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Swiss Center for Affective Sciences, University of Geneva, Geneva, Switzerland</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Academia Sinica, Taipei, Taiwan</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Papadelis</surname> <given-names>Christos</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Boston Children’s Hospital / Harvard Medical School, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p><list list-type="simple">
<list-item><p><bold>Conceptualization:</bold> MS AA YY.</p></list-item>
<list-item><p><bold>Data curation:</bold> AA MS YY.</p></list-item>
<list-item><p><bold>Formal analysis:</bold> AA MS YY.</p></list-item>
<list-item><p><bold>Funding acquisition:</bold> MS.</p></list-item>
<list-item><p><bold>Investigation:</bold> AA MS YY.</p></list-item>
<list-item><p><bold>Methodology:</bold> MS AA YY.</p></list-item>
<list-item><p><bold>Resources:</bold> MS YY AA.</p></list-item>
<list-item><p><bold>Software:</bold> AA MS.</p></list-item>
<list-item><p><bold>Visualization:</bold> AA.</p></list-item>
<list-item><p><bold>Writing – original draft:</bold> AA MS.</p></list-item>
<list-item><p><bold>Writing – review &amp; editing:</bold> AA MS YY.</p></list-item>
</list></p>
</fn>
<fn fn-type="current-aff" id="currentaff001">
<label>¤</label>
<p>Current address: Swiss Center for Affective Sciences, University of Geneva, Geneva, Switzerland</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">aljanaki@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<year>2017</year>
</pub-date>
<pub-date pub-type="epub">
<day>10</day>
<month>3</month>
<year>2017</year>
</pub-date>
<volume>12</volume>
<issue>3</issue>
<elocation-id>e0173392</elocation-id>
<history>
<date date-type="received">
<day>26</day>
<month>7</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>20</day>
<month>2</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Aljanaki et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0173392"/>
<abstract>
<p>Music emotion recognition (MER) field rapidly expanded in the last decade. Many new methods and new audio features are developed to improve the performance of MER algorithms. However, it is very difficult to compare the performance of the new methods because of the data representation diversity and scarcity of publicly available data. In this paper, we address these problems by creating a data set and a benchmark for MER. The data set that we release, a MediaEval Database for Emotional Analysis in Music (DEAM), is the largest available data set of dynamic annotations (valence and arousal annotations for 1,802 songs and song excerpts licensed under Creative Commons with 2Hz time resolution). Using DEAM, we organized the ‘Emotion in Music’ task at MediaEval Multimedia Evaluation Campaign from 2013 to 2015. The benchmark attracted, in total, 21 active teams to participate in the challenge. We analyze the results of the benchmark: the winning algorithms and feature-sets. We also describe the design of the benchmark, the evaluation procedures and the data cleaning and transformations that we suggest. The results from the benchmark suggest that the recurrent neural network based approaches combined with large feature-sets work best for dynamic MER.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution>Netherlands, COMMIT/</institution>
</funding-source>
<award-id>BI.000119</award-id>
<principal-award-recipient>
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-7119-8312</contrib-id>
<name name-style="western">
<surname>Aljanaki</surname> <given-names>Anna</given-names></name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100001711</institution-id>
<institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung</institution>
</institution-wrap>
</funding-source>
<award-id>PZ00P2_154981</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Soleymani</surname> <given-names>Mohammad</given-names></name>
</principal-award-recipient>
</award-group>
<funding-statement>AA’s work was supported by COMMIT/ project (commit-nl.nl). MS was supported by Ambizione program of the Swiss National Science Foundation (Grant number PZ00P2_154981).</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="8"/>
<page-count count="22"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data and materials are available at <ext-link ext-link-type="uri" xlink:href="http://cvml.unige.ch/databases/DEAM/" xlink:type="simple">http://cvml.unige.ch/databases/DEAM/</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Music emotion recognition (MER) is a young, but fast expanding field, stimulated by the interest from music industry to improve automatic music categorization methods for large-scale online music collections. In [<xref ref-type="bibr" rid="pone.0173392.ref001">1</xref>], an analysis of written music queries from creative professionals showed that 80% of the queries for production music contain emotional terms, making them one of the most salient and important components of exploratory music search. In the last decade, many new MER methods have been proposed (see [<xref ref-type="bibr" rid="pone.0173392.ref002">2</xref>, <xref ref-type="bibr" rid="pone.0173392.ref003">3</xref>] for reviews). However, methodological differences in data representation result in a choice of different evaluation metrics, which makes the accuracy of the algorithms impossible to compare. <xref ref-type="fig" rid="pone.0173392.g001">Fig 1</xref> shows 14 different data annotation and representation choices in a form of a labyrinth. In addition to these choices, a wide variety of <italic>categorical</italic> and <italic>dimensional</italic> emotional models are used, such as basic emotions [<xref ref-type="bibr" rid="pone.0173392.ref004">4</xref>], valence and arousal model [<xref ref-type="bibr" rid="pone.0173392.ref005">5</xref>–<xref ref-type="bibr" rid="pone.0173392.ref008">8</xref>], Geneva Emotional Music Scales (GEMS) [<xref ref-type="bibr" rid="pone.0173392.ref009">9</xref>, <xref ref-type="bibr" rid="pone.0173392.ref010">10</xref>], or custom mood clusters [<xref ref-type="bibr" rid="pone.0173392.ref011">11</xref>–<xref ref-type="bibr" rid="pone.0173392.ref013">13</xref>]. Despite differences in data representation, most of the methods are essentially solving the same problem of mapping acoustic features (or lyrics and meta-data based features) to the emotional annotations. A specific learning algorithm can not always be adapted to other representations (though many algorithms, such as SVM or different types of neural networks, are versatile), but audio features are more often transferable. A benchmark can therefore enable a comparison of different methods and feature sets.</p>
<fig id="pone.0173392.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0173392.g001</object-id>
<label>Fig 1</label>
<caption>
<title>A labyrinth of data representation choices for a MER algorithm.</title>
<p>The choices that we made for the benchmark are highlighted in red.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0173392.g001" xlink:type="simple"/>
</fig>
<p>Another problem of MER is that due to audio copyright restrictions, the data sets used in various studies are seldom made public and reused in other studies. Annotations are often obtained by crawling the tags from social music websites, such as last.fm or allmusic.com. In this case, the audio is usually copyrighted and can not be redistributed by the researchers. The music that is distributed for free under a license such as Creative Commons, usually is less well-known and has less tags, and therefore needs to be annotated. Annotating with emotional labels is burdensome, because with such a subjective task many annotations are needed for every item.</p>
<p>A fundamental property of music is that it unfolds over time. An emotion expressed in the song may also change over time, though it is always possible to reduce this variety to a single average value. The online music websites, such as moodfuse.com, musicovery.com, allmusic.com, usually represent songs in a mood space by a single label, which is always an approximation of the emotional content of the song. In design of the benchmark, we recognized the time-dependent nature of music by setting out to predict the emotion of the music dynamically (per-second), i.e., the main purpose of the benchmark is to compare <italic>dynamic MER</italic> algorithms, also known as music emotion variation detection (MEVD) algorithms in the literature [<xref ref-type="bibr" rid="pone.0173392.ref002">2</xref>].</p>
<p>In this paper, we describe the design, evaluation metrics and data that we used to benchmark dynamic MER algorithms. MediaEval Database for Emotional Analysis in Music (DEAM) is the combination of the data sets developed in three years (with data transformation and cleaning procedures applied to them), in addition to the manual annotations we received on Amazon Mechanical Turk (MTurk). DEAM contains 1,802 songs (58 full-length songs and 1,744 excerpts of 45 seconds) from a variety of Western popular music genres (rock, pop, electronic, country, Jazz etc). Part of the data was annotated in the lab and part using MTurk platform (<ext-link ext-link-type="uri" xlink:href="http://www.mturk.com" xlink:type="simple">http://www.mturk.com</ext-link>). Since the benchmark started in 2013, we have opted for characterizing the emotion of music as numerical values in two dimensions—valence (positive or negative emotions expressed in music) and arousal (energy of the music) (VA) [<xref ref-type="bibr" rid="pone.0173392.ref014">14</xref>, <xref ref-type="bibr" rid="pone.0173392.ref015">15</xref>], to make it easier to depict the temporal dynamics of emotion variation. We release the full dataset including the averaged and the raw annotations for the benefit of the community (available online at <ext-link ext-link-type="uri" xlink:href="http://cvml.unige.ch/databases/DEAM" xlink:type="simple">http://cvml.unige.ch/databases/DEAM</ext-link>). Over three years of activity 21 teams participated in the task. We will also systematically evaluate the feature-sets and the algorithms in this paper.</p>
<sec id="sec002">
<title>Background</title>
<p>MER is a young field which has been developing in the last decade. In this section, we will review available benchmarks for MER algorithms.</p>
<sec id="sec003">
<title>MIREX benchmark and static MER</title>
<p>The only other benchmark that exists for MER methods is the audio mood classification (AMC) task, organized by annual Music Information Retrieval Evaluation eXchange (<ext-link ext-link-type="uri" xlink:href="http://www.music-ir.org/mirex/wiki/" xlink:type="simple">http://www.music-ir.org/mirex/wiki/</ext-link>) (MIREX) [<xref ref-type="bibr" rid="pone.0173392.ref011">11</xref>]. In this task, 600 audio files are provided to the participants of the task, who have agreed not to distribute the files for commercial purposes. Since 2013, another set of 1,438 segments of 30 seconds clipped from Korean pop songs has been added to MIREX. The benchmark uses five discrete emotion clusters, derived from cluster analysis of online tags, instead of more widely accepted dimensional or categorical models of emotion. Emotional model used in AMC has been the topic of debate since it is not based on psychological research. There is also semantic or acoustic overlap between clusters [<xref ref-type="bibr" rid="pone.0173392.ref016">16</xref>]. Furthermore, the dataset only applies a singular static rating per audio clip (i.e. it deals with the <italic>static MER</italic> problem), which does not take into account the temporally dynamic nature of music.</p>
</sec>
<sec id="sec004">
<title>Dynamic MER methods</title>
<p>Since the late 1980s, time-varying responses to music were measured using Continuous Response Digital Interface [<xref ref-type="bibr" rid="pone.0173392.ref017">17</xref>]. Usually, only one dimension (such as tension, musical intensity or emotionality) was measured. Schubert proposed to use two-dimensional interface (valence–arousal plane) to annotate music with emotion continuously [<xref ref-type="bibr" rid="pone.0173392.ref018">18</xref>]. This approach was adopted by MER researchers as well.</p>
<p>The first study that models musical emotion unfolding over time with musical features (loudness, tempo, melodic contour, texture, and spectral centroid) was conducted by Schubert in 2004 [<xref ref-type="bibr" rid="pone.0173392.ref019">19</xref>]. The model, using a linear regression, could explain from 33% to 73% of variation in emotion. In 2006, Korhonen <italic>et al.</italic> [<xref ref-type="bibr" rid="pone.0173392.ref020">20</xref>] suggested a method to model musical emotion as a function of musical features using system identification techniques. Korhonen <italic>et al.</italic> used the low-level spectral features extracted using Marsyas software (<ext-link ext-link-type="uri" xlink:href="http://marsyas.info" xlink:type="simple">http://marsyas.info</ext-link>), and perceptual features extracted with PsySound software [<xref ref-type="bibr" rid="pone.0173392.ref021">21</xref>]. The system reached a performance of 0.22 for valence and 0.78 for arousal in terms of the coefficient of determination (<italic>R</italic><sup>2</sup>). In 2010, Schmidt <italic>et al.</italic> [<xref ref-type="bibr" rid="pone.0173392.ref022">22</xref>] used Kalman filtering to predict per-second changes in the distribution of emotion over time on 15 second music excerpts. In 2011, Schmidt and Kim suggested using a new method—Conditional Random Fields—to model continuous emotion with a resolution of 11 × 11 in valence–arousal space [<xref ref-type="bibr" rid="pone.0173392.ref023">23</xref>]. A very small feature-set was used—MFCCs, spectral contrast and timbre—and the system reached performance of 0.173 in terms of Earth Mover’s Distance (between the true 11 × 11 2D histogram of valence–arousal values and predicted one). Panda <italic>et al.</italic> [<xref ref-type="bibr" rid="pone.0173392.ref024">24</xref>] used Support Vector Machines and features extracted with Marsyas and MIRToolbox to track music over quadrants of valence–arousal space. Imbrasaite <italic>et al.</italic> [<xref ref-type="bibr" rid="pone.0173392.ref025">25</xref>] combined Continuous Conditional Random Fields with a relative representation of features. Later, Imbrasaite <italic>et al.</italic> [<xref ref-type="bibr" rid="pone.0173392.ref026">26</xref>] showed that using Continuous Conditional Neural Fields offers improvement over the previous approach. Wang <italic>et al.</italic> [<xref ref-type="bibr" rid="pone.0173392.ref027">27</xref>] represented the ambiguity of emotion through a Gaussian distribution and tracked the emotion variation over time using a mapping between music emotion space and low-level acoustic feature space through a set of latent feature classes. Markov <italic>et al.</italic> [<xref ref-type="bibr" rid="pone.0173392.ref028">28</xref>] used Gaussian Processes for dynamic MER. The bidirectional Long Short-Term Memory Recurrent Neural Networks were first applied to continuous emotion recognition not in the domain of music, but in the domain of multimodal human emotion detection from speech, facial expression and shoulder gesture [<xref ref-type="bibr" rid="pone.0173392.ref029">29</xref>].</p>
<p>Most of the algorithms mentioned in this section were employed in the benchmark: Support Vector Regression, linear regression, Kalman filtering, Gaussian Processes, Conditional Random Fields, Continuous Conditional Neural Fields and Long Short-Term Memory Recurrent Neural Networks, giving us an opportunity to qualitatively compare their performance in the benchmark.</p>
</sec>
<sec id="sec005">
<title>Datasets for dynamic MER</title>
<p>Most of the studies reviewed above did not release public data. The only exception is the MoodSwings dataset [<xref ref-type="bibr" rid="pone.0173392.ref030">30</xref>], developed by Schmidt <italic>et al.</italic>, which comprises 240 segments of US pop songs (each 15-second long) with per-second VA annotations, collected through MTurk. After an automatic verification step that removed unreliable annotations, each clip in this dataset was annotated by 7 to 23 subjects.</p>
<p>A similar task from a different domain is continuous emotion recognition from human behavior. Audiovisual emotion challenge (AVEC) [<xref ref-type="bibr" rid="pone.0173392.ref031">31</xref>–<xref ref-type="bibr" rid="pone.0173392.ref035">35</xref>] is a challenge that has been running since 2011 and is addressing the problem of continuous emotion recognition. Since 2011, they used SEMAINE [<xref ref-type="bibr" rid="pone.0173392.ref036">36</xref>] and RECOLA [<xref ref-type="bibr" rid="pone.0173392.ref037">37</xref>] databases which include human behavior with continuous emotion labels. There are also public datasets with static per song music emotion annotations. The DEAP dataset [<xref ref-type="bibr" rid="pone.0173392.ref038">38</xref>] has the ratings on valence, arousal and dominance for 120 clips of one-minute music video clips of Western pop music. Each clip was annotated by 14–16 listeners (50% female), who were asked to rate the felt valence, arousal and dominance on a 9-point scale for each clip. The AMG1608 dataset [<xref ref-type="bibr" rid="pone.0173392.ref039">39</xref>] contains the VA ratings for 1,608 Western music in different genres, also annotated through MTurk.</p>
</sec>
</sec>
</sec>
<sec id="sec006">
<title>Music database</title>
<p>Our data set consists of royalty-free (Creative Commons license enables us to redistribute the content) music from several sources: <monospace>freemusicarchive.org</monospace> (FMA), <monospace>jamendo.com</monospace>, and the medleyDB dataset [<xref ref-type="bibr" rid="pone.0173392.ref040">40</xref>]. There are 1,744 clips of 45 seconds from FMA and 58 full length songs, half of which come from medleyDB and another half from Jamendo.</p>
<p>The music from the FMA was in <italic>rock, pop, soul, blues, electronic, classical, hip-hop, international, experimental, folk, jazz, country</italic> and <italic>pop</italic> genres. The music from the MedleyDB dataset in addition had music in <italic>world</italic> and <italic>rap</italic> genres, and the music from Jamendo also had <italic>reggae</italic> music. For 2014 and 2015 data set, we manually checked the music and excluded the files with bad recording quality or those containing speech or noise instead of music. For each artist, we selected no more than 5 songs to be included in the dataset. For medleyDB and Jamendo full-length songs, we selected songs which had emotional variation in them, using an existing dynamic MER algorithm for filtering and manual final selection [<xref ref-type="bibr" rid="pone.0173392.ref041">41</xref>].</p>
</sec>
<sec id="sec007">
<title>Annotations</title>
<p>Getting high quality data is a crucial step for a highly subjective task. To collect annotations, we have turned to crowdsourcing using MTurk, which was successfully used by others to label large libraries [<xref ref-type="bibr" rid="pone.0173392.ref030">30</xref>, <xref ref-type="bibr" rid="pone.0173392.ref039">39</xref>]. We developed a procedure to filter out poor quality workers, following current state-of-the-art crowdsourcing approaches [<xref ref-type="bibr" rid="pone.0173392.ref042">42</xref>]. The workers passed a test to demonstrate a thorough understanding of the task, and an ability to produce good quality work. The test contained several automatically scored multiple choice questions, and several free-form questions and assignments, which were evaluated manually if the automatically scored part was passed correctly. In years 2013 and 2014, each excerpt was annotated by a minimum of 10 workers. In 2015, each song was annotated by five workers, three of which were recruited among the most successful workers from previous years, and two were working in the lab. To ensure a high-quality outcome, we first discussed and set a fair compensation for such a demanding task (about $8 per hour) on a MTurk workers’ forum (<ext-link ext-link-type="uri" xlink:href="http://www.mturkgrind.com/" xlink:type="simple">http://www.mturkgrind.com/</ext-link>). We then double-checked the agreement between the annotators in each batch before assigning an increasing qualification score which permitted workers to work on the next batches. The dynamic annotations were collected using a web-interface on a scale from −10 to 10, where the Mechanical Turk workers could dynamically annotate the songs on valence and arousal dimensions separately while the song was being played. The static annotations were made on nine-point scale on valence and arousal for the whole 45 seconds excerpts after the dynamic annotations. <xref ref-type="fig" rid="pone.0173392.g002">Fig 2</xref> shows the interface used for annotation.</p>
<fig id="pone.0173392.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0173392.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Annotation interface for both continuous (upper-left corner) and static per song (middle; using the self-assessment manikins [<xref ref-type="bibr" rid="pone.0173392.ref043">43</xref>]) ratings of arousal.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0173392.g002" xlink:type="simple"/>
</fig>
<p>As summarized in <xref ref-type="table" rid="pone.0173392.t001">Table 1</xref>, in addition to the audio features, we also provide meta-data covering the genre labels obtained from FMA, medleyDB and Jamendo, folksonomy tags crawled from last.fm, and meta-data about the annotators.</p>
<table-wrap id="pone.0173392.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0173392.t001</object-id>
<label>Table 1</label>
<caption>
<title>The data overview of DEAM.</title>
</caption>
<alternatives>
<graphic id="pone.0173392.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0173392.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Year</th>
<th align="left">Number of songs</th>
<th align="left">Source</th>
<th align="left">Meta-data about music</th>
<th align="left">Meta-data about annotators</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">2013</td>
<td align="left">1000 (744 unique)</td>
<td align="left">MTurk</td>
<td align="left">Genre</td>
<td align="left">Time of the day, mood</td>
</tr>
<tr>
<td align="left">2014</td>
<td align="left">1000</td>
<td align="left">MTurk</td>
<td align="left">Genre</td>
<td align="left">Confidence in rating, familiarity of music, liking of music, free emotion label, Big Five personality, preferred genre</td>
</tr>
<tr>
<td align="left">2015</td>
<td align="left">58</td>
<td align="left">MTurk/Lab</td>
<td align="left">Genre</td>
<td align="left">Liking, free emotion label</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<sec id="sec008">
<title>Annotation consistency</title>
<p>We will evaluate annotation consistency using two measures: Cronbach’s <italic>α</italic> on the sequences of annotations for each of the songs, and coefficient of determination of a Generalized Additive Model that generalizes song’s annotations across annotators.</p>
<p>We resample the annotations to 2Hz, and normalize the annotations for each song by
<disp-formula id="pone.0173392.e001"><alternatives><graphic id="pone.0173392.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0173392.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>a</mml:mi> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mi>i</mml:mi></mml:mrow></mml:msub> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mover><mml:msub><mml:mi>A</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>-</mml:mo> <mml:mover><mml:mi>A</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where <italic>a</italic><sub><italic>j</italic>,<italic>i</italic></sub> is an annotation by annotator <italic>j</italic> at timestamp <italic>i</italic>, <inline-formula id="pone.0173392.e002"><alternatives><graphic id="pone.0173392.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0173392.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mover><mml:msub><mml:mi>A</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is the mean of the annotations by annotator <italic>j</italic>, and <inline-formula id="pone.0173392.e003"><alternatives><graphic id="pone.0173392.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0173392.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mover><mml:mi>A</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is a mean of all annotations for this song by all annotators (global mean).</p>
<p>Cronbach’s <italic>α</italic> is used to estimate the degree to which a set of items measures a single unidimensional latent construct. This measure should theoretically range between 0 and 1, but in practice can be negative when inter-item correlations are negative. There is no lower bound on negative values of this measure. Only positive values are informative and accurately report degree of agreement. Therefore, we clip the negative tail by assigning the value of 0. <xref ref-type="table" rid="pone.0173392.t002">Table 2</xref> shows the averaged Cronbach’s <italic>α</italic> for each year’s annotations. To test whether annotation consistency improved with a change of experimental design, we will compare the three groups. Groups’ sample sizes and variances are different, therefore we will use a non-parametric test based on ranks. Kruskal-Wallis test (one way ANOVA on ranks) shows that there are significant differences between groups for arousal (<italic>χ</italic><sup>2</sup>(2) = 81.24, <italic>p</italic>-value = 2.2 × 10<sup>−16</sup>) and Dunnett-Tukey-Kramer test shows that the differences are significant between all three years on a 1% significance level. For valence, the differences exist (<italic>χ</italic><sup>2</sup>(2) = 57.91, <italic>p</italic>-value = 2.6 × 10<sup>−13</sup>), but only annotations from 2015 are significantly different from other groups.</p>
<table-wrap id="pone.0173392.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0173392.t002</object-id>
<label>Table 2</label>
<caption>
<title>Annotation consistency.</title>
<p>Cronbach’s <italic>α</italic> and generalized additive mixed models (GAM)’s coefficient of determination (mean and standard deviation) per year.</p>
</caption>
<alternatives>
<graphic id="pone.0173392.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0173392.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Year</th>
<th align="center">2013</th>
<th align="center">2014</th>
<th align="center">2015</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Total length</td>
<td align="center">9h 18min</td>
<td align="center">12h 30min</td>
<td align="center">3h 46min</td>
</tr>
<tr>
<td align="left">Cronbach’s <italic>α</italic> for arousal</td>
<td align="center">.28 ± 0.28</td>
<td align="center">.31 ± 0.30</td>
<td align="center">.66 ± 0.26</td>
</tr>
<tr>
<td align="left">GAM’s <italic>R</italic><sup>2</sup> for arousal</td>
<td align="center">.13 ± 0.10</td>
<td align="center">.14 ± 0.11</td>
<td align="center">.44 ± 0.19</td>
</tr>
<tr>
<td align="left">Cronbach’s <italic>α</italic> for valence</td>
<td align="center">.28 ± 0.29</td>
<td align="center">.20 ± 0.24</td>
<td align="center">.51 ± 0.35</td>
</tr>
<tr>
<td align="left">GAM’s <italic>R</italic><sup>2</sup> for valence</td>
<td align="center">.13 ± 0.10</td>
<td align="center">.10 ± 0.08</td>
<td align="center">.37 ± 0.21</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>Cronbach’s <italic>α</italic> test has some deficiencies, such as being sensitive to the number of items on the test (greater number of items in the test can artificially inflate the value of alpha). Therefore, we conduct an additional consistency test with generalized additive mixed models (GAMs) [<xref ref-type="bibr" rid="pone.0173392.ref044">44</xref>]. A GAM is a generalized (i.e., allowing non-normal error distributions of the response variable) linear model with a linear predictor involving a sum of smooth functions of covariates. The model is defined as follows:
<disp-formula id="pone.0173392.e004"><alternatives><graphic id="pone.0173392.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0173392.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>g</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mn>0</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>+</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>+</mml:mo> <mml:msub><mml:mi>f</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
where <italic>g</italic> is a link function (a function defining a relationship between the linear predictor and the mean of the dependent variable); <italic>μ</italic> = <italic>E</italic>(<italic>Y</italic>), where <italic>Y</italic> is a dependent variable; and <italic>f</italic><sub><italic>i</italic></sub>(<italic>x</italic><sub><italic>i</italic></sub>) are non-parametric smooth functions, estimated, e.g. via scatterplot smoothing techniques, or can also be parametric functions or factors.</p>
<p>GAMs are suitable for modeling continuous annotations of emotion, because these annotations are usually non-linear in nature and do not have abrupt changes, making it possible to model them using smooth functions. KcKeown and Sneddon [<xref ref-type="bibr" rid="pone.0173392.ref044">44</xref>] described how GAMs and their mixed model extension can be used to model continuous emotion annotations and make inferences concerning linear differences between groups. In this paper, we only use GAMs to assess the effect size of shared perceived emotion. This is done by building a model for each song and calculating the <italic>R</italic><sup>2</sup> of the model.</p>
<p>There is only one smooth component in the model—time. We use penalized cubic regression splines with basis dimension of 20 and identity link function. The results are shown in <xref ref-type="table" rid="pone.0173392.t002">Table 2</xref>. <xref ref-type="fig" rid="pone.0173392.g003">Fig 3</xref> shows scatter-plots of the manual annotations and the fitted GAMS for 2 songs picked from 2015 dataset.</p>
<fig id="pone.0173392.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0173392.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Fitted GAMs for the arousal and valence annotations of two songs.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0173392.g003" xlink:type="simple"/>
</fig>
<p>There are significant differences between groups for arousal according to Kruskal-Wallis test (<italic>χ</italic><sup>2</sup>(2) = 121.03, <italic>p</italic>-value = 2.2 × 10<sup>−16</sup>) and Dunnett-Tukey-Kramer test shows that the differences are significant between 2015 and other groups on a 1% significance level. For valence, the outcome is the same: differences exist (<italic>χ</italic><sup>2</sup>(2) = 134.37, <italic>p</italic>-value = 2.2 × 10<sup>−16</sup>), and only year’s 2015 annotations are significantly different from other groups.</p>
<p>According to both consistency measures, in 2015 we could achieve better consistency, which can be attributed to employing lab workers, choosing complete songs over excerpts and introducing preliminary listening.</p>
<sec id="sec009">
<title>Influence of music familiarity, liking and other factors on annotations</title>
<p>The Creative Commons music that we selected was largely unfamiliar to participants (only in 1% of the listening sessions the participant reported having heard the piece before). Hence, there was not enough data to derive any patterns regarding the familiarity of the music.</p>
<p>We found that liking influenced self evaluation of confidence in rating. <xref ref-type="fig" rid="pone.0173392.g004">Fig 4</xref> shows the 2D histogram for self-reported confidence in rating and liking the music. The confidence in rating is on average very high (the workers never reported being very uncertain), which is, probably, caused by the fact that the data was collected from paid workers who did not want to be suspected of incompetence. Liking the music influenced perceived self-reported confidence. A similar effect was found in [<xref ref-type="bibr" rid="pone.0173392.ref045">45</xref>], when there was a positive dependency between liking the music and annotation consistency. We could not find any effect of averaged music liking on actual rating consistency as measured by correlation of a rating with other workers, or Cronbach’s <italic>α</italic> of a song.</p>
<fig id="pone.0173392.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0173392.g004</object-id>
<label>Fig 4</label>
<caption>
<title/>
<p>Liking of the music and confidence in rating for a) valence, Spearman’s <italic>ρ</italic> = 0.37, <italic>p</italic>-value = 2.2 × 10<sup>−16</sup> b) arousal, Spearman’s <italic>ρ</italic> = 0.29, <italic>p</italic>-value = 2.2 × 10<sup>−16</sup>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0173392.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec010">
<title>Convergence of annotations</title>
<p>It is a known issue that the annotators need some initial orientation time (IOT), before their continuous annotations become meaningful and reliable. In [<xref ref-type="bibr" rid="pone.0173392.ref046">46</xref>], median IOT was found to be 8 seconds for valence and 12 seconds for arousal. Also, afterglow effects—large outliers in spread of scores just after the end of a piece—were identified. In [<xref ref-type="bibr" rid="pone.0173392.ref047">47</xref>], participants required on average 8.31 seconds to initiate giving emotional judgements on music on a two-dimensional plane. The length of delay was influenced by familiarity, genre and tempo of music.</p>
<p>To measure the IOT of the annotators in the beginning of the song, we calculate the average Krippendorff’s <italic>α</italic> for every sample of the corresponding second for the whole dataset of 2015. The songs in the dataset had different length. <xref ref-type="fig" rid="pone.0173392.g005">Fig 5</xref> shows that the annotations start to converge around the 13<sup>th</sup> second. A similar result was obtained in 2013<sup>th</sup> annotations [<xref ref-type="bibr" rid="pone.0173392.ref048">48</xref>]. So, despite preliminary listening stage, the reaction time did not diminish.</p>
<fig id="pone.0173392.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0173392.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Krippendorff’s <italic>α</italic> of dynamic annotations in 2015, averaged over all dynamic samples.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0173392.g005" xlink:type="simple"/>
</fig>
<p>We remove the first 15 seconds of the annotation from the benchmark data.</p>
</sec>
</sec>
</sec>
<sec id="sec011">
<title>Benchmark history and design</title>
<p>The benchmark for music emotion recognition algorithms, described in this article, was organized in years 2013-2015 inside the MediaEval Benchmarking Initiative for Multimedia Evaluation (<ext-link ext-link-type="uri" xlink:href="http://www.multimediaeval.org" xlink:type="simple">http://www.multimediaeval.org</ext-link>). MediaEval is a community-driven benchmark dedicated to evaluating algorithms for multimedia access and retrieval, that is organized annually since year 2008 (as VideoCLEF, in years 2008 and 2009). The list of tasks offered at the benchmark is renewed every year based on interest and feedback from the multimedia retrieval community. Alongside Emotion in Music task, 10-11 other tasks related to speech, music, image and video processing were held at MediaEval in years 2013-2015. We followed MediaEval benchmarking tradition, by developing a separate development and evaluation-set for each year.</p>
<sec id="sec012">
<title>Task definition 2013</title>
<p>In year 2013, the task was first proposed and organized inside MediaEval framework by Mohammad Soleymani, Yi-Hsuan Yang and Erik Schmidt [<xref ref-type="bibr" rid="pone.0173392.ref049">49</xref>]. The task consisted of two subtasks: <italic>dynamic</italic> and <italic>static</italic> emotion characterization. In dynamic emotion characterization, the participating algorithms predicted emotion (valence and arousal) of the music dynamically per-second. In the static task, the valence and arousal of the complete music clip (45 seconds) were predicted. The training data set consisted of 700 excerpts of 45 seconds, which were labelled both with dynamic annotation (1Hz) and static annotation, where static was not derived from dynamic, but was given separately. 300 clips were left out for the evaluation-set. The music came from Free Music Archive. Later, duplicates (excerpts sampled from the same song) were discovered and removed from this data, leaving 744 clips out of 1000.</p>
</sec>
<sec id="sec013">
<title>Task definition 2014</title>
<p>In 2014, the static emotion characterization task was removed and a new subtask—<italic>feature design</italic>—was added instead [<xref ref-type="bibr" rid="pone.0173392.ref050">50</xref>]. In the feature design task, new features, which have not been developed before, were proposed and applied to valence and arousal prediction task. The feature design task was not popular and only one team submitted to that task [<xref ref-type="bibr" rid="pone.0173392.ref051">51</xref>]. The training set consisted of 744 clips from previous year and 1000 new clips, all from Free Music Archive, served as the evaluation-set. The time resolution for the dynamic task was changed to 2Hz.</p>
</sec>
<sec id="sec014">
<title>Task definition 2015</title>
<p>In 2015, the feature design subtask was removed, leaving only dynamic emotion characterization task. The training set consisted of 431 clips, which were selected out of 1,744 clips from previous years based on consistency metrics. The data cleaning procedure is described in [<xref ref-type="bibr" rid="pone.0173392.ref041">41</xref>]. The evaluation-set consisted of 58 full length songs, one half from the medleyDB dataset [<xref ref-type="bibr" rid="pone.0173392.ref040">40</xref>] of royalty-free multitrack recordings and another half from the <monospace>jamendo.com</monospace> music website, which provides music under Creative Commons license. The songs were ≈4 minutes (234 ± 107 s) long on average. The time resolution for the annotations was 2Hz. The participants had to submit:
<list list-type="bullet"><list-item><p>Features that the participants used in their approach. The features were used to train a baseline regression method (linear regression) to estimate dynamic affect. Any features automatically extracted from the audio or the meta-data provided by the organizers were allowed.</p></list-item> <list-item><p>Predictions using baseline features.</p></list-item> <list-item><p>Predictions using any combination of the features and machine learning methods of their choice.</p></list-item></list></p>
</sec>
<sec id="sec015">
<title>Evaluation metrics</title>
<p>We used two evaluation metrics to compare the performance of different methods: Pearson’s correlation coefficient between the ground truth and predicted values for each song, averaged across songs, and root mean square error (RMSE), averaged the same way. In years 2013 and 2014, we used correlation coefficient as the main metric and RMSE as an auxiliary metric to break the ties. The tie is a situation, when the difference between two methods adjacent in the ranking is not significant based on the one sided Wilcoxon test (<italic>p</italic> &lt; 0.05). In 2015, we used RMSE as our primary metric. RMSE metric measures how far is the prediction of the emotion from the true emotion of the song, and correlation measures whether the direction of change is guessed correctly. However, in case of dynamic emotions, the trend shape of the traces are also important. In this paper, we will also report concordance correlation coefficient (CCC) <italic>ρ</italic><sub><italic>c</italic></sub> as an evaluation metric. This metric was suggested by Lin [<xref ref-type="bibr" rid="pone.0173392.ref052">52</xref>] in 1989 and is defined as follows:
<disp-formula id="pone.0173392.e005"><alternatives><graphic id="pone.0173392.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0173392.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mn>2</mml:mn> <mml:msub><mml:mi>s</mml:mi> <mml:mrow><mml:mi>x</mml:mi> <mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow> <mml:mrow><mml:msubsup><mml:mi>s</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi>s</mml:mi> <mml:mi>y</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>+</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mover><mml:mi>x</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>+</mml:mo> <mml:mover><mml:mi>y</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula>
where <italic>x</italic> and <italic>y</italic> are the vectors of numbers to compare, <inline-formula id="pone.0173392.e006"><alternatives><graphic id="pone.0173392.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0173392.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:msubsup><mml:mi>s</mml:mi> <mml:mi>x</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> is the variance of x, <italic>s</italic><sub><italic>xy</italic></sub> is the covariance of <italic>x</italic> and <italic>y</italic>, and <inline-formula id="pone.0173392.e007"><alternatives><graphic id="pone.0173392.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0173392.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:mover><mml:mi>x</mml:mi> <mml:mo>¯</mml:mo></mml:mover></mml:math></alternatives></inline-formula> is the mean of vector <italic>x</italic>. CCC has recently been promoted as the metric of choice for continuous emotion recognition [<xref ref-type="bibr" rid="pone.0173392.ref035">35</xref>].</p>
</sec>
<sec id="sec016">
<title>Baseline features</title>
<p>In every year, baseline features extracted from the audio were offered to the participants along with the audio files. In the majority of cases, these features were used by the participants in their submissions. In year 2013, the features were MFCCs, octave- based spectral contrast, spectral features (centroid, flux, rolloff, flatness), chromagram, and timbre, pitch, and loudness features from the Echonest 7 API. In year 2014, we released the features extracted with openSMILE toolbox [<xref ref-type="bibr" rid="pone.0173392.ref053">53</xref>] as described in [<xref ref-type="bibr" rid="pone.0173392.ref054">54</xref>]. In year 2015, we extracted a smaller set of features with openSMILE. We obtained 260 low-level features (mean and standard deviation of 65 low-level acoustic descriptors, and their first-order derivatives) from non-overlapping segments of 500ms, with the frame size of 60ms with a 10ms step.</p>
<p>No feature selection was applied when building baseline linear regression models from baseline features. In year 2014, as an exception, different features were used to build a baseline model (spectral flux, harmonic change detection function, loudness, roughness and zero crossing rate).</p>
</sec>
</sec>
<sec id="sec017">
<title>Analysis of proposed methods</title>
<p>In this section, we will analyze the best systems suggested over the three years of benchmark. In the last edition of the benchmark (2015), we asked the participants to provide their feature-sets, and to run their algorithms on the baseline feature-set. In this way, we can conduct a systematic evaluation of the algorithms and feature-sets separately.</p>
<sec id="sec018">
<title>Task participation</title>
<p>Three teams participated in the task in year 2014 and the results were analyzed in [<xref ref-type="bibr" rid="pone.0173392.ref048">48</xref>]. In 2014, there were six teams and in 2015, twelve teams. Every team wrote a working notes paper which is available in the proceedings on the corresponding year MediaEval workshop. The last edition of the benchmark had most participating teams, and most of the algorithms from the previous years featured in the last edition. In this paper, we will mostly analyze the results of the benchmark held in 2015.</p>
</sec>
<sec id="sec019">
<title>Performance in a challenge over years</title>
<p>Tables <xref ref-type="table" rid="pone.0173392.t003">3</xref>–<xref ref-type="table" rid="pone.0173392.t005">5</xref> show the results of the benchmark in year 2013, 2014 and 2015. The results are sorted by RMSE of arousal ascending (best solutions on top). Column “Method” shows the abbreviation of the machine learning algorithm used by a particular team, and a working notes paper that was published in the proceedings, where the details of the approach are explained. All the methods beat the baseline, shown on the bottom row. The baseline method is a multi-linear regression with openSMILE features.</p>
<table-wrap id="pone.0173392.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0173392.t003</object-id>
<label>Table 3</label>
<caption>
<title>Performance of the algorithms for arousal and valence in year 2013.</title>
<p><bold>BLSTM-RNN</bold>—Bi-directional Long-Short Term Memory Recurrent Neural Networks. <bold>GPR</bold>—Gaussian Processes Regression. <bold>SVR</bold>—Support Vector Regression.</p>
</caption>
<alternatives>
<graphic id="pone.0173392.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0173392.t003" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="2">Method</th>
<th align="center" colspan="2">Arousal</th>
<th align="center" colspan="2">Valence</th>
</tr>
<tr>
<th align="center">RMSE</th>
<th align="center"><italic>ρ</italic></th>
<th align="center">RMSE</th>
<th align="center"><italic>ρ</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><bold>BLSTM-RNN</bold> [<xref ref-type="bibr" rid="pone.0173392.ref054">54</xref>]</td>
<td align="center">.08 ± .05</td>
<td align="center">.31 ± .37</td>
<td align="center">.08 ± .04</td>
<td align="center">.19 ± .43</td>
</tr>
<tr>
<td align="left"><bold>GPR</bold> [<xref ref-type="bibr" rid="pone.0173392.ref028">28</xref>]</td>
<td align="center">.10 ± .05</td>
<td align="center">.11 ± .36</td>
<td align="center">.09 ± .05</td>
<td align="center">.06 ± .28</td>
</tr>
<tr>
<td align="left"><bold>SVR</bold> [<xref ref-type="bibr" rid="pone.0173392.ref057">57</xref>]</td>
<td align="center">.10 ± .06</td>
<td align="center">.14 ± .28</td>
<td align="center">.12 ± .07</td>
<td align="center">−.01 ± .27</td>
</tr>
<tr>
<td align="left"><bold>Baseline</bold></td>
<td align="center">.25 ± .11</td>
<td align="center">.16 ± .36</td>
<td align="center">.23 ± .10</td>
<td align="center">.06 ± .30</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="pone.0173392.t004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0173392.t004</object-id>
<label>Table 4</label>
<caption>
<title>Performance of the algorithms for arousal and valence in year 2014.</title>
<p><bold>KF</bold>—Kalman Filter. <bold>LSTM</bold>—Long-Short Term Memory Recurrent Neural Network. <bold>CCRF</bold>—Continuous Conditional Random Fields. <bold>CCNF</bold>—Continuous Conditional Neural Fields. <bold>MR</bold>—Multi-level regression. <bold>PLSR</bold>—Partial Least Squares Regression.</p>
</caption>
<alternatives>
<graphic id="pone.0173392.t004g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0173392.t004" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="2">Method</th>
<th align="center" colspan="2">Arousal</th>
<th align="center" colspan="2">Valence</th>
</tr>
<tr>
<th align="center">RMSE</th>
<th align="center"><italic>ρ</italic></th>
<th align="center">RMSE</th>
<th align="center"><italic>ρ</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><bold>KF</bold> [<xref ref-type="bibr" rid="pone.0173392.ref056">56</xref>]</td>
<td align="center">.08 ± .05</td>
<td align="center">.21 ± .57</td>
<td align="center">.14 ± .07</td>
<td align="center">.17 ± .5</td>
</tr>
<tr>
<td align="left"><bold>LSTM</bold> [<xref ref-type="bibr" rid="pone.0173392.ref055">55</xref>]</td>
<td align="center">.10 ± .05</td>
<td align="center">.35 ± .45</td>
<td align="center">.08 ± .05</td>
<td align="center">.20 ± .49</td>
</tr>
<tr>
<td align="left"><bold>CCRF</bold> [<xref ref-type="bibr" rid="pone.0173392.ref058">58</xref>]</td>
<td align="center">.12 ± .05</td>
<td align="center">.23 ± .56</td>
<td align="center">.09 ± .05</td>
<td align="center">.12 ± .55</td>
</tr>
<tr>
<td align="left"><bold>CCNF</bold> [<xref ref-type="bibr" rid="pone.0173392.ref026">26</xref>]</td>
<td align="center">.12 ± .07</td>
<td align="center">.18 ± .60</td>
<td align="center">.10 ± .06</td>
<td align="center">.07 ± .29</td>
</tr>
<tr>
<td align="left"><bold>MR</bold> [<xref ref-type="bibr" rid="pone.0173392.ref059">59</xref>]</td>
<td align="center">.12 ± .05</td>
<td align="center">.17 ± .41</td>
<td align="center">.09 ± .05</td>
<td align="center">.10 ± .37</td>
</tr>
<tr>
<td align="left"><bold>PLSR</bold> [<xref ref-type="bibr" rid="pone.0173392.ref051">51</xref>]</td>
<td align="center">.13 ± .07</td>
<td align="center">.28 ± .50</td>
<td align="center">.10 ± .06</td>
<td align="center">.15 ± .5</td>
</tr>
<tr>
<td align="left"><bold>Baseline</bold></td>
<td align="center">.14 ± .06</td>
<td align="center">.18 ± .36</td>
<td align="center">.10 ± .06</td>
<td align="center">.11 ± .34</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<table-wrap id="pone.0173392.t005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0173392.t005</object-id>
<label>Table 5</label>
<caption>
<title>Performance of the algorithms for arousal and valence in 2015.</title>
<p><bold>BLSTM-ELM</bold>—BLSTM-based multi-scale regression fusion with Extreme Learning Machine. <bold>AE-HE-BLSTM</bold>—BLSTM + features created through deep learning. <bold>LS</bold>—Linear regression + Smoothing. <bold>LSB</bold>—Least Squares Boosting + Smoothing. <bold>SVR + CCRF</bold>—SVR + Continuous Conditional Random Fields.</p>
</caption>
<alternatives>
<graphic id="pone.0173392.t005g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0173392.t005" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="2">Method</th>
<th align="center" colspan="2">Arousal</th>
<th align="center" colspan="2">Valence</th>
</tr>
<tr>
<th align="center">RMSE</th>
<th align="center"><italic>ρ</italic></th>
<th align="center">RMSE</th>
<th align="center"><italic>ρ</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><bold>BLSTM-RNN</bold> [<xref ref-type="bibr" rid="pone.0173392.ref060">60</xref>]</td>
<td align="center">.12 ± 0.06</td>
<td align="center">.66 ± 0.25</td>
<td align="center">.17 ± 0.09</td>
<td align="center">.12 ± 0.54</td>
</tr>
<tr>
<td align="left"><bold>BLSTM-ELM</bold> [<xref ref-type="bibr" rid="pone.0173392.ref060">60</xref>]</td>
<td align="center">.12 ± 0.05</td>
<td align="center">.63 ± 0.27</td>
<td align="center">.15 ± 0.08</td>
<td align="center">.15 ± 0.47</td>
</tr>
<tr>
<td align="left"><bold>LSTM-RNN</bold> [<xref ref-type="bibr" rid="pone.0173392.ref061">61</xref>]</td>
<td align="center">.12 ± 0.06</td>
<td align="center">.61 ± 0.28</td>
<td align="center">.19 ± 0.10</td>
<td align="center">.03 ± 0.50</td>
</tr>
<tr>
<td align="left"><bold>LSTM-RNN</bold> [<xref ref-type="bibr" rid="pone.0173392.ref061">61</xref>]</td>
<td align="center">.12 ± 0.06</td>
<td align="center">.60 ± 0.29</td>
<td align="center">.19 ± 0.10</td>
<td align="center">.02 ± 0.49</td>
</tr>
<tr>
<td align="left"><bold>LS</bold> [<xref ref-type="bibr" rid="pone.0173392.ref062">62</xref>]</td>
<td align="center">.12 ± 0.05</td>
<td align="center">.65 ± 0.22</td>
<td align="center">.17 ± 0.09</td>
<td align="center">.01 ± 0.50</td>
</tr>
<tr>
<td align="left"><bold>LSB</bold> [<xref ref-type="bibr" rid="pone.0173392.ref062">62</xref>]</td>
<td align="center">.12 ± 0.05</td>
<td align="center">.59 ± 0.23</td>
<td align="center">.17 ± 0.09</td>
<td align="center">.05 ± 0.43</td>
</tr>
<tr>
<td align="left"><bold>SVR</bold> [<xref ref-type="bibr" rid="pone.0173392.ref063">63</xref>]</td>
<td align="center">.12 ± 0.05</td>
<td align="center">.56 ± 0.27</td>
<td align="center">.19 ± 0.10</td>
<td align="center">−0.02 ± 0.45</td>
</tr>
<tr>
<td align="left"><bold>SVR + CCRF</bold> [<xref ref-type="bibr" rid="pone.0173392.ref064">64</xref>]</td>
<td align="center">.12 ± 0.05</td>
<td align="center">.54 ± 0.27</td>
<td align="center">.17 ± 0.09</td>
<td align="center">.02 ± 0.43</td>
</tr>
<tr>
<td align="left"><bold>AE-HE-BLSTM</bold> [<xref ref-type="bibr" rid="pone.0173392.ref060">60</xref>]</td>
<td align="center">.12 ± 0.06</td>
<td align="center">.52 ± 0.37</td>
<td align="center">.17 ± 0.09</td>
<td align="center">.02 ± 0.51</td>
</tr>
<tr>
<td align="left"><bold>LSTM-RNN</bold> [<xref ref-type="bibr" rid="pone.0173392.ref061">61</xref>]</td>
<td align="center">.12 ± 0.06</td>
<td align="center">.61 ± 0.25</td>
<td align="center">.19 ± 0.10</td>
<td align="center">.00 ± 0.50</td>
</tr>
<tr>
<td align="left"><bold>Baseline</bold></td>
<td align="center">.14 ± 0.06</td>
<td align="center">.37 ± 0.26</td>
<td align="center">.18 ± 0.09</td>
<td align="center">−0.01 ± 0.38</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>In year 2013 and 2015, LSTM-RNN based solutions were the best both for arousal and valence, in year 2014 LSTM-based solution was second best for arousal, but best for valence.</p>
<p>In year 2013, all the teams used different feature-sets. The results are analyzed in detail in [<xref ref-type="bibr" rid="pone.0173392.ref048">48</xref>].</p>
<p>In year 2014, solutions [<xref ref-type="bibr" rid="pone.0173392.ref026">26</xref>] and [<xref ref-type="bibr" rid="pone.0173392.ref055">55</xref>] used openSMILE feature-sets. The rest of the teams used other features. The combination that produced the best result for arousal (but worse than baseline result for valence) [<xref ref-type="bibr" rid="pone.0173392.ref056">56</xref>], was a combination of a Kalman filter and low-level features: MFCCs, zero-crossing rate, spectral flux, centroid, rolloff, and spectral crest factor.</p>
<p>
<xref ref-type="table" rid="pone.0173392.t005">Table 5</xref> shows only 10 best solutions for 2015. Each of the 12 teams submitted 3 runs, which creates more than 30 different solutions, some of which were on par with the baseline. All of the solutions listed use the baseline openSMILE feature-set, but it is usually transformed, or new features are added.</p>
</sec>
<sec id="sec020">
<title>Evaluation of the machine learning algorithms</title>
<p>In this section, we describe an evaluation of the algorithms on the same feature-set (the baseline features of year 2015).</p>
<p>
<xref ref-type="table" rid="pone.0173392.t006">Table 6</xref> shows the evaluation of the algorithms participating in 2015 challenge on this feature-set. 10 best approaches are reported. The performance in terms of RMSE for arousal is the same for all the solutions (though correlation coefficient is different), indicating that the algorithms might have reached some sort of ceiling in performance with this combination of annotations and features.</p>
<table-wrap id="pone.0173392.t006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0173392.t006</object-id>
<label>Table 6</label>
<caption>
<title>Performance of the different algorithms for arousal and valence, using the baseline feature-set.</title>
<p><bold>Combo</bold>—An unweighted combination of LS, LSB and Boosted ensemble of single feature filters.</p>
</caption>
<alternatives>
<graphic id="pone.0173392.t006g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0173392.t006" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="2">Method</th>
<th align="center" colspan="3">Arousal</th>
<th align="center" colspan="3">Valence</th>
</tr>
<tr>
<th align="center">RMSE</th>
<th align="center"><italic>ρ</italic></th>
<th align="center"><italic>ρ</italic><sub><italic>c</italic></sub></th>
<th align="center">RMSE</th>
<th align="center"><italic>ρ</italic></th>
<th align="center"><italic>ρ</italic><sub><italic>c</italic></sub></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><bold>BLSTM-RNN</bold> [<xref ref-type="bibr" rid="pone.0173392.ref060">60</xref>]</td>
<td align="center">.12 ± .06</td>
<td align="center">.66 ± .25</td>
<td align="center">.30 ± .24</td>
<td align="center">.15 ± .08</td>
<td align="center">.15 ± .47</td>
<td align="center">.06 ± .17</td>
</tr>
<tr>
<td align="left"><bold>BLSTM-ELM</bold> [<xref ref-type="bibr" rid="pone.0173392.ref060">60</xref>]</td>
<td align="center">.12 ± .05</td>
<td align="center">.63 ± .27</td>
<td align="center">.25 ± .22</td>
<td align="center">.15 ± .08</td>
<td align="center">.15 ± .47</td>
<td align="center">.06 ± .17</td>
</tr>
<tr>
<td align="left"><bold>LR + S</bold> [<xref ref-type="bibr" rid="pone.0173392.ref062">62</xref>]</td>
<td align="center">.12 ± .05</td>
<td align="center">.65 ± .22</td>
<td align="center">.32 ± .23</td>
<td align="center">.17 ± .09</td>
<td align="center">.01 ± .50</td>
<td align="center">.01 ± .19</td>
</tr>
<tr>
<td align="left"><bold>LSB</bold> [<xref ref-type="bibr" rid="pone.0173392.ref062">62</xref>]</td>
<td align="center">.12 ± .05</td>
<td align="center">.59 ± .23</td>
<td align="center">.30 ± .24</td>
<td align="center">.17 ± .09</td>
<td align="center">.05 ± .43</td>
<td align="center">.01 ± .18</td>
</tr>
<tr>
<td align="left"><bold>LSTM-RNN</bold> [<xref ref-type="bibr" rid="pone.0173392.ref061">61</xref>]</td>
<td align="center">.12 ± .06</td>
<td align="center">.61 ± .25</td>
<td align="center">.31 ± .26</td>
<td align="center">.19 ± .10</td>
<td align="center">.00 ± .50</td>
<td align="center">.01 ± .20</td>
</tr>
<tr>
<td align="left"><bold>Combo</bold> [<xref ref-type="bibr" rid="pone.0173392.ref062">62</xref>]</td>
<td align="center">.12 ± .05</td>
<td align="center">.64 ± .23</td>
<td align="center">.28 ± .22</td>
<td align="center">.17 ± .09</td>
<td align="center">.00 ± .48</td>
<td align="center">.01 ± .19</td>
</tr>
<tr>
<td align="left"><bold>SVR</bold> [<xref ref-type="bibr" rid="pone.0173392.ref063">63</xref>]</td>
<td align="center">.12 ± .05</td>
<td align="center">.56 ± .27</td>
<td align="center">.31 ± .25</td>
<td align="center">.19 ± .10</td>
<td align="center">−0.02 ± .45</td>
<td align="center">.00 ± .18</td>
</tr>
<tr>
<td align="left"><bold>SVR + CCRF</bold> [<xref ref-type="bibr" rid="pone.0173392.ref064">64</xref>]</td>
<td align="center">.12 ± .05</td>
<td align="center">.52 ± .30</td>
<td align="center">.22 ± .22</td>
<td align="center">.17 ± .10</td>
<td align="center">.00 ± .43</td>
<td align="center">.00 ± .13</td>
</tr>
<tr>
<td align="left"><bold>LSTM-RNN</bold> [<xref ref-type="bibr" rid="pone.0173392.ref065">65</xref>]</td>
<td align="center">.12 ± .06</td>
<td align="center">.59 ± .24</td>
<td align="center">.30 ± .23</td>
<td align="center">.18 ± .09</td>
<td align="center">.03 ± .48</td>
<td align="center">.00 ± .20</td>
</tr>
<tr>
<td align="left"><bold>SVR</bold> [<xref ref-type="bibr" rid="pone.0173392.ref060">60</xref>]</td>
<td align="center">.12 ± .07</td>
<td align="center">.56 ± .24</td>
<td align="center">.08 ± .08</td>
<td align="center">.15 ± .09</td>
<td align="center">.01 ± .40</td>
<td align="center">.00 ± .04</td>
</tr>
<tr>
<td align="left"><bold>Baseline - FNN</bold></td>
<td align="center">.12 ± .08</td>
<td align="center">.45 ± .22</td>
<td align="center">.25 ± .20</td>
<td align="center">.14 ± .13</td>
<td align="center">.00 ± .41</td>
<td align="center">.04 ± .16</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>The algorithms are sorted by their performance according to RMSE on arousal ascending (RMSE increases and performance decreases). The algorithms show very good performance on arousal and completely unsatisfactory performance on valence. It is a known issue, that valence is much more difficult to model than arousal, but not to the extent that we observe.</p>
<p>In 2013 and 2014, valence and arousal annotations were highly correlated whereas in 2015, they were not. We hypothesize that due to the high correlation the algorithms did not train to recognize valence-specific cues and could not perform well on the evaluation-set. <xref ref-type="fig" rid="pone.0173392.g006">Fig 6</xref> shows the scatter plots of the annotations along with regression lines.</p>
<fig id="pone.0173392.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0173392.g006</object-id>
<label>Fig 6</label>
<caption>
<title/>
<p>Distribution of the labels on arousal-valence plane for a) development-set b) evaluation-set.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0173392.g006" xlink:type="simple"/>
</fig>
<p>Almost all the solutions listed in <xref ref-type="table" rid="pone.0173392.t006">Table 6</xref> are either based on LSTM-RNN networks or SVR. Exception are solutions suggested by team SAILUSC [<xref ref-type="bibr" rid="pone.0173392.ref062">62</xref>], which are based on linear regression with smoothing, or least squares boosting. LSTM-RNN networks are capable of incorporating local context in their predictions. A smoothing step also incorporates the context, though it can not learn the dependencies in time-series. We also provide a baseline Feed-Forward Neural Netword (FNN)—dropout-regularized neural net with three hidden layers. Without any smoothing step, the feed-forward neural net demonstrates worse performance in terms of Pearson’s correlation coefficient. With median-filter smoothing applied to results, the correlation coefficient for arousal is similar to the rest of the approaches (0.57 ± 0.24).</p>
</sec>
<sec id="sec021">
<title>Evaluation of the feature-sets</title>
<p>In this section, we will analyze the features proposed by the teams in 2015 through building a system using the same machine learning algorithm, but different feature-sets. We chose the best performing algorithm of previous years—LSTM-RNN.</p>
<p>We constructed a network with three hidden layers with 250, 150 and 50 nodes, similar to the architecture used by ICL team. We used the number of memory blocks in each hidden layer, the learning rate (LR), and the standard deviation of the Gaussian noise applied to the input activations, which were optimized for our data by the ICL team [<xref ref-type="bibr" rid="pone.0173392.ref061">61</xref>]. Every layer was pretrained (in a supervised way) before the next layer was added and the network was trained again. We used 20-fold cross-validation for evaluating results.</p>
<sec id="sec022">
<title>Proposed features</title>
<p>A variety of software for audio signal processing and feature extraction was used by participants: Marsyas, MIRToolbox for Matlab, PsySound, openSMILE, Essentia, jAudio. Mostly, participants used the features that are known to be important for emotion recognition, such as MFCCs, tempo, loudness, low level spectral features related to timbre. Few novel features were proposed. Kumar <italic>et al.</italic> [<xref ref-type="bibr" rid="pone.0173392.ref051">51</xref>] proposed three new types of features: compressibility features, which describe how much the audio can be compressed, median spectral band energy, which describes the spectral bandwidth of the audio. The compressibility of audio was strongly positively correlated with static arousal ratings (Pearson’s <italic>r</italic> = 0.656). Cai <italic>et al.</italic> [<xref ref-type="bibr" rid="pone.0173392.ref066">66</xref>] proposed edge orientation histograms on mel-frequency spectrogram.</p>
</sec>
<sec id="sec023">
<title>Results on development and evaluation-set cross-validation</title>
<p>
<xref ref-type="table" rid="pone.0173392.t007">Table 7</xref> shows the evaluation of the feature-sets on valence, ordered by Concordance Correlation Coefficient of the results on evaluation-set, descending. The best performing feature-set for valence (by JUNLP team) is a baseline feature-set with feature selection applied to it to find the features optimized for valence recognition. The second best feature-set, suggested by PKUAIPL team, consisted of the baseline feature-set with an addition of three types of features: MFCCs and Δ MFCCs, edge-orientation histograms and standard low-level spectral features. In addition, team PKUAIPL applied auto-regressive and moving average filters to the features to account for the temporal changes in music, and added the output as new features to the feature vector. Team HKPOLYU suggested a supervised transformation on the baseline feature-set (valence–arousal similarity preserving embedding). This transformation maps high-dimensional feature vectors to a lower-dimensional space so that for similar songs (in terms of valence or arousal) the feature vectors are also closer in this low-dimensional space.</p>
<table-wrap id="pone.0173392.t007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0173392.t007</object-id>
<label>Table 7</label>
<caption>
<title>Performance of the different feature-sets on valence, development and evaluation-sets of 2015, 20 fold cross-validation.</title>
</caption>
<alternatives>
<graphic id="pone.0173392.t007g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0173392.t007" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="2">Method</th>
<th align="center" colspan="3">Development-set</th>
<th align="center" colspan="3">Evaluation-set</th>
</tr>
<tr>
<th align="center">RMSE</th>
<th align="center"><italic>ρ</italic></th>
<th align="center"><italic>ρ</italic><sub><italic>c</italic></sub></th>
<th align="center">RMSE</th>
<th align="center"><italic>ρ</italic></th>
<th align="center"><italic>ρ</italic><sub><italic>c</italic></sub></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><bold>JUNLP (2)</bold> [<xref ref-type="bibr" rid="pone.0173392.ref067">67</xref>]</td>
<td align="center">.26 ± .15</td>
<td align="center">.22 ± .51</td>
<td align="center">.09 ± .24</td>
<td align="center">.27 ± .13</td>
<td align="center">.19 ± .35</td>
<td align="center">.08 ± .15</td>
</tr>
<tr>
<td align="left"><bold>PKUAIPL</bold> [<xref ref-type="bibr" rid="pone.0173392.ref064">64</xref>]</td>
<td align="center">.22 ± .13</td>
<td align="center">.33 ± .50</td>
<td align="center">.16 ± .27</td>
<td align="center">.27 ± .14</td>
<td align="center">.16 ± .35</td>
<td align="center">.07 ± .20</td>
</tr>
<tr>
<td align="left"><bold>HKPOLYU</bold> [<xref ref-type="bibr" rid="pone.0173392.ref063">63</xref>]</td>
<td align="center">.21 ± .13</td>
<td align="center">.41 ± .53</td>
<td align="center">.20 ± .28</td>
<td align="center">.28 ± .14</td>
<td align="center">.19 ± .36</td>
<td align="center">.06 ± .17</td>
</tr>
<tr>
<td align="left"><bold>JUNLP (3)</bold> [<xref ref-type="bibr" rid="pone.0173392.ref067">67</xref>]</td>
<td align="center">.26 ± .15</td>
<td align="center">.23 ± .53</td>
<td align="center">.09 ± .24</td>
<td align="center">.28 ± .13</td>
<td align="center">.17 ± .33</td>
<td align="center">.06 ± .14</td>
</tr>
<tr>
<td align="left"><bold>UNIZA (1)</bold> [<xref ref-type="bibr" rid="pone.0173392.ref068">68</xref>]</td>
<td align="center">.22 ± .14</td>
<td align="center">.32 ± .50</td>
<td align="center">.16 ± .27</td>
<td align="center">.29 ± .14</td>
<td align="center">.14 ± .37</td>
<td align="center">.06 ± .14</td>
</tr>
<tr>
<td align="left"><bold>ICL</bold> [<xref ref-type="bibr" rid="pone.0173392.ref061">61</xref>]</td>
<td align="center">.22 ± .13</td>
<td align="center">.30 ± .50</td>
<td align="center">.15 ± .27</td>
<td align="center">.30 ± .14</td>
<td align="center">.12 ± .40</td>
<td align="center">.06 ± .16</td>
</tr>
<tr>
<td align="left"><bold>JUNLP (1)</bold> [<xref ref-type="bibr" rid="pone.0173392.ref067">67</xref>]</td>
<td align="center">.22 ± .14</td>
<td align="center">.32 ± .50</td>
<td align="center">.15 ± .27</td>
<td align="center">.28 ± .13</td>
<td align="center">.12 ± .39</td>
<td align="center">.05 ± .15</td>
</tr>
<tr>
<td align="left"><bold>UNIZA (2)</bold> [<xref ref-type="bibr" rid="pone.0173392.ref068">68</xref>]</td>
<td align="center">.23 ± .14</td>
<td align="center">.31 ± .49</td>
<td align="center">.15 ± .26</td>
<td align="center">.29 ± .16</td>
<td align="center">.09 ± .40</td>
<td align="center">.05 ± .17</td>
</tr>
<tr>
<td align="left"><bold>IRIT-SAMOVA</bold> [<xref ref-type="bibr" rid="pone.0173392.ref065">65</xref>]</td>
<td align="center">.23 ± .14</td>
<td align="center">.33 ± .50</td>
<td align="center">.16 ± .27</td>
<td align="center">.29 ± .15</td>
<td align="center">.08 ± .41</td>
<td align="center">.05 ± .16</td>
</tr>
<tr>
<td align="left"><bold>MIRUtrecht</bold> [<xref ref-type="bibr" rid="pone.0173392.ref069">69</xref>]</td>
<td align="center">.24 ± .15</td>
<td align="center">.30 ± .49</td>
<td align="center">.13 ± .23</td>
<td align="center">.29 ± .14</td>
<td align="center">.11 ± .43</td>
<td align="center">.04 ± .15</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>
<xref ref-type="table" rid="pone.0173392.t008">Table 8</xref> shows the evaluation of the feature-sets on arousal, ordered by Concordance Correlation Coefficient of the results of development-set, descending. Teams HKPOLYU, THU-HCSIL and IRIT-SAMOVA suggested the best features for arousal. The features by the team HKPOLYU were already described above. Team THU-HCSIL applied Deep Belief Networks to a set of features extracted with openSMILE and MIRToolbox, in order to learn the higher representation for each group features independently, which were then fused by a special autoencoder with a modified cost function considering sparse and heterogeneous entropy, to produce the final features at a rate of 2Hz for the succeeding regression. Team IRIT-SAMOVA could achieve a very good performance with a very simple feature-set consisting of 6 measurements on bands of a Bark scale for spectral valley, and spectral flatness on ERB and Bark scale, for a total of only 8 features. Spectral flatness provides a way to quantify how noise-like a sound is. Spectral valley is a feature derived from the so-called spectral contrast feature, which represents the relative spectral distribution.</p>
<table-wrap id="pone.0173392.t008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0173392.t008</object-id>
<label>Table 8</label>
<caption>
<title>Performance of the different feature-sets on arousal, development and evaluation-sets of 2015, 20 fold cross-validation.</title>
</caption>
<alternatives>
<graphic id="pone.0173392.t008g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0173392.t008" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left" rowspan="2">Method</th>
<th align="center" colspan="3">Development-set</th>
<th align="center" colspan="3">Evaluation-set</th>
</tr>
<tr>
<th align="center">RMSE</th>
<th align="center"><italic>ρ</italic></th>
<th align="center"><italic>ρ</italic><sub><italic>c</italic></sub></th>
<th align="center">RMSE</th>
<th align="center"><italic>ρ</italic></th>
<th align="center"><italic>ρ</italic><sub><italic>c</italic></sub></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><bold>HKPOLYU</bold> [<xref ref-type="bibr" rid="pone.0173392.ref063">63</xref>]</td>
<td align="center">.20 ± .12</td>
<td align="center">.48 ± .47</td>
<td align="center">.23 ± .28</td>
<td align="center">.22 ± .12</td>
<td align="center">.39 ± .41</td>
<td align="center">.24 ± .26</td>
</tr>
<tr>
<td align="left"><bold>THU-HCSIL</bold> [<xref ref-type="bibr" rid="pone.0173392.ref060">60</xref>]</td>
<td align="center">.21 ± .13</td>
<td align="center">.46 ± .42</td>
<td align="center">.22 ± .26</td>
<td align="center">.27 ± .12</td>
<td align="center">.33 ± .40</td>
<td align="center">.16 ± .22</td>
</tr>
<tr>
<td align="left"><bold>IRIT-SAMOVA (3)</bold> [<xref ref-type="bibr" rid="pone.0173392.ref065">65</xref>]</td>
<td align="center">.21 ± .13</td>
<td align="center">.49 ± .43</td>
<td align="center">.21 ± .27</td>
<td align="center">.24 ± .13</td>
<td align="center">.52 ± .37</td>
<td align="center">.25 ± .25</td>
</tr>
<tr>
<td align="left"><bold>IRIT-SAMOVA</bold> [<xref ref-type="bibr" rid="pone.0173392.ref065">65</xref>]</td>
<td align="center">.21 ± .12</td>
<td align="center">.45 ± .44</td>
<td align="center">.21 ± .27</td>
<td align="center">.24 ± .12</td>
<td align="center">.43 ± .30</td>
<td align="center">.22 ± .22</td>
</tr>
<tr>
<td align="left"><bold>JUNLP (1)</bold> [<xref ref-type="bibr" rid="pone.0173392.ref067">67</xref>]</td>
<td align="center">.21 ± .12</td>
<td align="center">.45 ± .43</td>
<td align="center">.19 ± .26</td>
<td align="center">.24 ± .12</td>
<td align="center">.52 ± .31</td>
<td align="center">.26 ± .24</td>
</tr>
<tr>
<td align="left"><bold>UNIZA (2)</bold> [<xref ref-type="bibr" rid="pone.0173392.ref068">68</xref>]</td>
<td align="center">.22 ± .12</td>
<td align="center">.41 ± .44</td>
<td align="center">.19 ± .25</td>
<td align="center">.24 ± .12</td>
<td align="center">.48 ± .32</td>
<td align="center">.26 ± .24</td>
</tr>
<tr>
<td align="left"><bold>PKU-AIPL</bold> [<xref ref-type="bibr" rid="pone.0173392.ref064">64</xref>]</td>
<td align="center">.21 ± .12</td>
<td align="center">.40 ± .44</td>
<td align="center">.19 ± .26</td>
<td align="center">.23 ± .10</td>
<td align="center">.52 ± .30</td>
<td align="center">.32 ± .27</td>
</tr>
<tr>
<td align="left"><bold>UNIZA (1)</bold> [<xref ref-type="bibr" rid="pone.0173392.ref068">68</xref>]</td>
<td align="center">.22 ± .12</td>
<td align="center">.40 ± .44</td>
<td align="center">.19 ± .25</td>
<td align="center">.25 ± .13</td>
<td align="center">.49 ± .30</td>
<td align="center">.27 ± .23</td>
</tr>
<tr>
<td align="left"><bold>JKU-Tinnitus (2)</bold> [<xref ref-type="bibr" rid="pone.0173392.ref070">70</xref>]</td>
<td align="center">.22 ± .13</td>
<td align="center">.39 ± .45</td>
<td align="center">.19 ± .26</td>
<td align="center">.30 ± .14</td>
<td align="center">.06 ± .38</td>
<td align="center">.04 ± .17</td>
</tr>
<tr>
<td align="left"><bold>JKU-Tinnitus (1)</bold> [<xref ref-type="bibr" rid="pone.0173392.ref070">70</xref>]</td>
<td align="center">.22 ± .12</td>
<td align="center">.38 ± .43</td>
<td align="center">.19 ± .26</td>
<td align="center">.29 ± .14</td>
<td align="center">.09 ± .39</td>
<td align="center">.05 ± .15</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
</sec>
</sec>
<sec id="sec024">
<title>Discussions and perspectives</title>
<p>During the three years of organizing the ‘Emotion in Music’ task at MediaEval, changes were introduced to the data collection routine, which led to the improvement of the quality of the annotations. In the first two years of the benchmark, the size of the segment was chosen in such a way that both static and dynamic ratings were possible. This resulted in a compromise, which led to selecting the window of 45 seconds, which appears to be too short to capture a lot of emotional variation, and too long to make estimating the static emotion unambiguous. In 2015, we opted for full-length songs. In combination with preliminary listening and more careful selection of workers, the quality of the annotations was improved. However, full-length songs might also not be the optimal solution because the annotation procedure is very demanding and requires a lot of concentration, and there is a danger that full-length song annotation stretches the limits of what human annotators are capable of. This question requires more investigation. Also, in 2015 we employed a dynamic MER method and manual filtering to select songs with more emotional variety, in particular songs in the upper left and lower right quadrants of the VA space. This led to a different distribution of labels, which allowed to identify problems with valence recognition.</p>
<p>Estimating the absolute value of an emotion in real time could be difficult for the annotators, and often the direction of change is indicated correctly whereas the magnitude is not. We proposed to alleviate this problem by offsetting the annotations into the same bias using the overall emotion of the song (as annotated separately).</p>
<p>The valence–arousal model considered in the benchmark has been widely adopted in research on affective computing [<xref ref-type="bibr" rid="pone.0173392.ref002">2</xref>, <xref ref-type="bibr" rid="pone.0173392.ref038">38</xref>, <xref ref-type="bibr" rid="pone.0173392.ref071">71</xref>–<xref ref-type="bibr" rid="pone.0173392.ref073">73</xref>]. However, the model is not free of criticisms and some other alternatives may be considered in the future. For example, the VA model has been criticized for being too reductionist and that other dimensions such as dominance should be added [<xref ref-type="bibr" rid="pone.0173392.ref074">74</xref>]. Moreover, the terms ‘valence’ and ‘arousal’ may be sometimes too abstract for people to have a common understanding of its meaning. Such drawbacks of the VA model can further harm the inter-annotator agreement of the annotations for an annotation task which is already inherently fairly subjective.</p>
<p>In the benchmark, we resampled the annotations to either 1Hz or 2Hz. This led to benchmark participants using 1 or 0.5 second windows as the main unit of emotion prediction. As far as musical emotion is usually created on bigger time scales, the best algorithms for dynamic MER were those that could incorporate the bigger context, through either algorithm design (LSTM-RNN) or smoothing step applied at a later stage. Another way of performing dynamic MER is to first segment the emotional segments or use different units, such as scores, for emotion recognition [<xref ref-type="bibr" rid="pone.0173392.ref075">75</xref>].</p>
<p>The best feature-sets that were suggested for the task treated predicting valence and arousal separately, and suggested separate feature selection or dimensionality reduction steps for each emotional dimension. Again, it was shown that though arousal can be successfully modeled just with simple timbral features (spectral valley and spectral flatness), modeling valence is much more complex, and satisfactory performance was not achieved by any of the algorithms.</p>
<p>It is known that emotion perception is highly subjective, especially for valence [<xref ref-type="bibr" rid="pone.0173392.ref002">2</xref>]. Therefore, instead of taking the average values of the emotional annotations as the ground truth and training a generalized model for predicting them, we might want to have a look at the raw annotations and investigate the difference across the annotators. For example, it is possible that two songs with similar average ratings would have different variances in the raw annotation, and that it is better to explicitly model the variance computationally [<xref ref-type="bibr" rid="pone.0173392.ref027">27</xref>, <xref ref-type="bibr" rid="pone.0173392.ref072">72</xref>]. It is also possible to build personalization techniques for customized MER predictions [<xref ref-type="bibr" rid="pone.0173392.ref076">76</xref>, <xref ref-type="bibr" rid="pone.0173392.ref077">77</xref>], though to our best knowledge little has been done to personalize a dynamic MER model.</p>
<p>As shown in <xref ref-type="table" rid="pone.0173392.t001">Table 1</xref>, the DEAM dataset contains rich extra meta-data about the songs and the annotators, such as the genre labels of the songs, the familiarity and liking of each of the annotated songs for each annotators, and even the personality traits of the annotators (in 2014). Such information can be studied and exploited in future work.</p>
<p>Although the benchmark is mainly designed for dynamic MER, the annotations, after being summarized over time, can also be useful for static MER. We also expect the dataset can facilitate the application and development of other mid- to high- level audio and non-audio features (e.g. [<xref ref-type="bibr" rid="pone.0173392.ref078">78</xref>, <xref ref-type="bibr" rid="pone.0173392.ref079">79</xref>]), and other machine learning algorithms (e.g. that better account for temporal dynamics or personal differences) in the context of MER.</p>
<p>Emotion recognition from audiovisual signals is a task that is related to recognizing the spontaneous emotional expressions. Coutinho <italic>et al.</italic> [<xref ref-type="bibr" rid="pone.0173392.ref080">80</xref>] demonstrated that emotion recognition models can be transferred from speech to music and vice versa. As a result, there are parallels between the winning models in AVEC challenges that are addressing emotion recognition from human behavior and the ones addressing Emotion in Music task. In both cases, fine-tuned LSTM recurrent neural networks are the best performing models [<xref ref-type="bibr" rid="pone.0173392.ref060">60</xref>, <xref ref-type="bibr" rid="pone.0173392.ref081">81</xref>].</p>
</sec>
<sec id="sec025" sec-type="conclusions">
<title>Conclusions</title>
<p>In this paper, we analyzed and summarized our findings in developing a new benchmark for emotional analysis in music. Analyzing three years of annotations on dynamic emotion recognition, we found them to be demanding in need of very conscientious and well trained annotators. We only succeeded in acquiring high quality labels on a crowdsourcing platform after directly engaging with workers and providing feedback in addition to a fair and mutually agreed compensation. We found that the results are less sensitive to the type of acoustic features, if we take enough of them into account. Recurrent neural networks and particularly LSTM is very effective in capturing the dynamic changes in emotion in music from acoustic features.</p>
<p>We release the data under Non Commercial Creative Commons (BY-NC) and we hope that this benchmark including its dataset and evaluation metrics helps accelerating research in MER.</p>
</sec>
</body>
<back>
<ack>
<p>We would like to thank Erik M. Schmidt, Mike N. Caro, Cheng-Ya Sha, Alexander Lansky, Sung-Yen Liu and Eduardo Countinho for their contributions in the development of this task. We also thank the task participants and the anonymous Turkers, without whom the benchmark would not happen.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0173392.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Inskip</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Macfarlane</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rafferty</surname> <given-names>P</given-names></name>. <article-title>Towards the disintermediation of creative music search: analysing queries to determine important facets</article-title>. <source>International Journal on Digital Libraries</source>. <year>2012</year>;<volume>12</volume>(<issue>2</issue>):<fpage>137</fpage>–<lpage>147</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/s00799-012-0084-1" xlink:type="simple">10.1007/s00799-012-0084-1</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0173392.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yang</surname> <given-names>YH</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>HH</given-names></name>. <article-title>Machine recognition of music emotion: A review</article-title>. <source>ACM Transactions on Intelligent Systems and Technology</source>. <year>2012</year>;<volume>3</volume>(<issue>3</issue>). <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1145/2168752.2168754" xlink:type="simple">10.1145/2168752.2168754</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0173392.ref003">
<label>3</label>
<mixed-citation publication-type="other" xlink:type="simple">Kim YE, Schmidt EM, Migneco R, Morton BG, Richardson P, Scott J, et al. Music emotion recognition: A state of the art review. In: Proceedings of International Society for Music Information Retrieval Conference; 2010. p. 255–266.</mixed-citation>
</ref>
<ref id="pone.0173392.ref004">
<label>4</label>
<mixed-citation publication-type="other" xlink:type="simple">Laurier C, Lartillot O, Eerola T, Toiviainen P. Exploring relationships between audio features and emotion in music. In: Proceedings of Triennal Conference of European Society for Cognitive Sciences of Music; 2009. p. 260–264.</mixed-citation>
</ref>
<ref id="pone.0173392.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Yang</surname> <given-names>YH</given-names></name>, <name name-style="western"><surname>Lin</surname> <given-names>YC</given-names></name>, <name name-style="western"><surname>Su</surname> <given-names>YF</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>HH</given-names></name>. <article-title>A regression approach to music emotion recognition</article-title>. <source>IEEE Transactions on Audio, Speech, and Language Processing</source>. <year>2008</year>;<volume>16</volume>(<issue>2</issue>):<fpage>448</fpage>–<lpage>457</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TASL.2007.911513" xlink:type="simple">10.1109/TASL.2007.911513</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0173392.ref006">
<label>6</label>
<mixed-citation publication-type="other" xlink:type="simple">Eerola T. Modelling emotions in music: Advances in conceptual, contextual and validity issues. In: Proceedings of AES International Confernece; 2014.</mixed-citation>
</ref>
<ref id="pone.0173392.ref007">
<label>7</label>
<mixed-citation publication-type="other" xlink:type="simple">Barthet M, Fazekas G, Sandler M. Multidisciplinary perspectives on music emotion recognition: Implications for content and context-based models. In: Proceedings of International Symposium on Computer Music Modelling &amp; Retrieval; 2012. p. 492–507.</mixed-citation>
</ref>
<ref id="pone.0173392.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hu</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>YH</given-names></name>. <article-title>Cross-dataset and cross-cultural music mood prediction: A case on Western and Chinese Pop songs</article-title>. <source>IEEE Transactions on Affective Computing</source>. <year>2016</year>;PP(<issue>99</issue>):<fpage>1</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TAFFC.2016.2523503" xlink:type="simple">10.1109/TAFFC.2016.2523503</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0173392.ref009">
<label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Vuoskoski</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Eerola</surname> <given-names>T</given-names></name>. <article-title>Measuring music-induced emotion: A comparison of emotion models, personality biases, and intensity of experiences</article-title>. <source>Musicae Scientiae</source>. <year>2011</year>;<volume>15</volume>(<issue>2</issue>):<fpage>159</fpage>–<lpage>173</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1177/1029864911403367" xlink:type="simple">10.1177/1029864911403367</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0173392.ref010">
<label>10</label>
<mixed-citation publication-type="other" xlink:type="simple">Aljanaki A, Wiering F, Veltkamp RC. Computational modeling of induced emotion using GEMS. In: Proceedings of International Society for Music Information Retrieval Conference; 2014. p. 373–378.</mixed-citation>
</ref>
<ref id="pone.0173392.ref011">
<label>11</label>
<mixed-citation publication-type="other" xlink:type="simple">Hu X, Downie JS, Laurier C, Bay M, Ehmann AF. The 2007 MIREX Audio Mood Classification Task: Lessons learned. In: Proceedings of International Society for Music Information Retrieval Conference; 2008. p. 462–467.</mixed-citation>
</ref>
<ref id="pone.0173392.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schubert</surname> <given-names>E</given-names></name>. <article-title>Update of the Hevner adjective checklist</article-title>. <source>Perceptual Motor Skills</source>. <year>2003</year>;<volume>96</volume>:<fpage>1117</fpage>–<lpage>1122</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2466/pms.2003.96.3c.1117" xlink:type="simple">10.2466/pms.2003.96.3c.1117</ext-link></comment> <object-id pub-id-type="pmid">12929763</object-id></mixed-citation>
</ref>
<ref id="pone.0173392.ref013">
<label>13</label>
<mixed-citation publication-type="other" xlink:type="simple">Laurier C, Sordo M, Serrà J, Herrera P. Music mood representations from social tags. In: Proceedings of International Society for Music Information Retrieval Conference; 2009. p. 381–386.</mixed-citation>
</ref>
<ref id="pone.0173392.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Russell</surname> <given-names>J</given-names></name>. <article-title>A circumplex model of affect</article-title>. <source>Journal of Personality and Social Psychology</source>. <year>1980</year>;<volume>39</volume>:<fpage>1161</fpage>–<lpage>1178</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/h0077714" xlink:type="simple">10.1037/h0077714</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0173392.ref015">
<label>15</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Thayer</surname> <given-names>RE</given-names></name>. <source>The Biopsychology of Mood and Arousal</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>1989</year>.</mixed-citation>
</ref>
<ref id="pone.0173392.ref016">
<label>16</label>
<mixed-citation publication-type="other" xlink:type="simple">Laurier C, Herrera P. Audio Music Mood Classification using support vector machine. In: MIREX task on Audio Mood Classification; 2007.</mixed-citation>
</ref>
<ref id="pone.0173392.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Geringer</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Madsen</surname> <given-names>CK</given-names></name>, <name name-style="western"><surname>Gregory</surname> <given-names>D</given-names></name>. <article-title>A fifteen-year history of the Continuous Response Digital Interface: Issues relating to validity and reliability</article-title>. <source>Bulletin of the Council for Research in Music Education</source>. <year>2004</year>;<volume>160</volume>:<fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation>
</ref>
<ref id="pone.0173392.ref018">
<label>18</label>
<mixed-citation publication-type="other" xlink:type="simple">Schubert E. Continuous response to music using a two dimensional emotion space. In: Proceedings of International Conference of Music Perception and Cognition; 1996. p. 263–268.</mixed-citation>
</ref>
<ref id="pone.0173392.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schubert</surname> <given-names>E</given-names></name>. <article-title>Modeling perceived emotion with continuous musical features</article-title>. <source>Music Perception: An Interdisciplinary Journal</source>. <year>2004</year>;<volume>21</volume>(<issue>4</issue>):<fpage>561</fpage>–<lpage>585</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1525/mp.2004.21.4.561" xlink:type="simple">10.1525/mp.2004.21.4.561</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0173392.ref020">
<label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Korhonen</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Clausi</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Jernigan</surname> <given-names>ME</given-names></name>. <article-title>Modeling emotional content of music using system identification</article-title>. <source>IEEE Transactions on Systems, Man, and Cybernetics</source>. <year>2006</year>;<volume>36</volume>(<issue>3</issue>):<fpage>588</fpage>–<lpage>599</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TSMCB.2005.862491" xlink:type="simple">10.1109/TSMCB.2005.862491</ext-link></comment> <object-id pub-id-type="pmid">16761812</object-id></mixed-citation>
</ref>
<ref id="pone.0173392.ref021">
<label>21</label>
<mixed-citation publication-type="other" xlink:type="simple">Cabrera D. PsySound: A computer program for psychoacoustical analysis. Proceedings of Australian Acoustical Society Conference. 1999;p. 47–54.</mixed-citation>
</ref>
<ref id="pone.0173392.ref022">
<label>22</label>
<mixed-citation publication-type="other" xlink:type="simple">Schmidt EM, Kim YE. Prediction of time-varying musical mood distributions using Kalman filtering. In: Proceedings of IEEE International Conference on Machine Learning and Applications; 2010. p. 655–660.</mixed-citation>
</ref>
<ref id="pone.0173392.ref023">
<label>23</label>
<mixed-citation publication-type="other" xlink:type="simple">Schmidt EM, Kim YE. Modeling musical emotion dynamics with conditional random fields. In: Proceedings of International Society for Music Information Retrieval Conference; 2011.</mixed-citation>
</ref>
<ref id="pone.0173392.ref024">
<label>24</label>
<mixed-citation publication-type="other" xlink:type="simple">Panda R, Paiva RP. Using support vector machines for automatic mood tracking in audio music. In: Proceedings of 130th Audio Engineering Society Convention; 2011.</mixed-citation>
</ref>
<ref id="pone.0173392.ref025">
<label>25</label>
<mixed-citation publication-type="other" xlink:type="simple">Imbrasaite V, Baltrušaitis T, Robinson P. Emotion tracking in music using continuous conditional random fields and relative feature representation. In: Proceedings of IEEE International Conference on Multimedia and Expo Workshops; 2013. p. 1–6.</mixed-citation>
</ref>
<ref id="pone.0173392.ref026">
<label>26</label>
<mixed-citation publication-type="other" xlink:type="simple">Imbrasaite V, Baltrušaitis T, Robinson P. CCNF for continuous emotion tracking in music: Comparison with CCRF and relative feature representation. In: Proceedings of IEEE International Conference on Multimedia and Expo.; 2014.</mixed-citation>
</ref>
<ref id="pone.0173392.ref027">
<label>27</label>
<mixed-citation publication-type="other" xlink:type="simple">Wang JC, Yang YH, Wang HM, Jeng SK. The Acoustic Emotion Gaussians model for emotion-based music annotation and retrieval. In: Proceedings of ACM International Conference on Multimedia; 2012. p. 89–98.</mixed-citation>
</ref>
<ref id="pone.0173392.ref028">
<label>28</label>
<mixed-citation publication-type="other" xlink:type="simple">Markov K, Iwata M, Matsui T. Music emotion recognition using Gaussian processes. In: Working Notes Proceedings of the MediaEval 2013 Workshop; 2013.</mixed-citation>
</ref>
<ref id="pone.0173392.ref029">
<label>29</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Nicolaou</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Gunes</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Pantic</surname> <given-names>M</given-names></name>. <article-title>Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space</article-title>. <source>IEEE Transactions on Affective Computing</source>. <year>2011</year>;<volume>2</volume>(<issue>2</issue>):<fpage>92</fpage>–<lpage>105</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/T-AFFC.2011.9" xlink:type="simple">10.1109/T-AFFC.2011.9</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0173392.ref030">
<label>30</label>
<mixed-citation publication-type="other" xlink:type="simple">Speck JA, Schmidt EM, Morton BG, Kim YE. A comparative study of collaborative vs. traditional musical mood annotation. In: Proceedings of International Society for Music Information Retrieval Conference; 2011.</mixed-citation>
</ref>
<ref id="pone.0173392.ref031">
<label>31</label>
<mixed-citation publication-type="other" xlink:type="simple">Schuller B, Valstar M, Eyben F, McKeown G, Cowie R, Pantic M. Avec 2011–the first international audio/visual emotion challenge. In: International Conference on Affective Computing and Intelligent Interaction. Springer; 2011. p. 415–424.</mixed-citation>
</ref>
<ref id="pone.0173392.ref032">
<label>32</label>
<mixed-citation publication-type="other" xlink:type="simple">Schuller B, Valster M, Eyben F, Cowie R, Pantic M. AVEC 2012: the continuous audio/visual emotion challenge. In: Proceedings of the 14th ACM international conference on Multimodal interaction. ACM; 2012. p. 449–456.</mixed-citation>
</ref>
<ref id="pone.0173392.ref033">
<label>33</label>
<mixed-citation publication-type="other" xlink:type="simple">Valstar M, Schuller B, Smith K, Eyben F, Jiang B, Bilakhia S, et al. AVEC 2013: the continuous audio/visual emotion and depression recognition challenge. In: Proceedings of the 3rd ACM international workshop on Audio/visual emotion challenge. ACM; 2013. p. 3–10.</mixed-citation>
</ref>
<ref id="pone.0173392.ref034">
<label>34</label>
<mixed-citation publication-type="other" xlink:type="simple">Valstar M, Schuller B, Smith K, Almaev T, Eyben F, Krajewski J, et al. Avec 2014: 3d dimensional affect and depression recognition challenge. In: Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge. ACM; 2014. p. 3–10.</mixed-citation>
</ref>
<ref id="pone.0173392.ref035">
<label>35</label>
<mixed-citation publication-type="other" xlink:type="simple">Ringeval F, Schuller B, Valstar M, Cowie R, Pantic M. AVEC 2015: The 5th International Audio/Visual Emotion Challenge and Workshop. In: Proceedings of ACM International Conference on Multimedia; 2015. p. 1335–1336.</mixed-citation>
</ref>
<ref id="pone.0173392.ref036">
<label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>McKeown</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Valstar</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Cowie</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Pantic</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schroder</surname> <given-names>M</given-names></name>. <article-title>The SEMAINE Database: Annotated Multimodal Records of Emotionally Colored Conversations between a Person and a Limited Agent</article-title>. <source>IEEE Transactions on Affective Computing</source>. <year>2012</year> <month>Jan</month>;<volume>3</volume>(<issue>1</issue>):<fpage>5</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/T-AFFC.2011.20" xlink:type="simple">10.1109/T-AFFC.2011.20</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0173392.ref037">
<label>37</label>
<mixed-citation publication-type="other" xlink:type="simple">Ringeval F, Sonderegger A, Sauer J, Lalanne D. Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions. In: Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on; 2013. p. 1–8.</mixed-citation>
</ref>
<ref id="pone.0173392.ref038">
<label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Koelstra</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Mühl</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Soleymani</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Yazdani</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ebrahimi</surname> <given-names>T</given-names></name>, <etal>et al</etal>. <article-title>DEAP: A database for emotion analysis using physiological signals</article-title>. <source>IEEE Transactions on Affective Computing</source>. <year>2012</year>;<volume>3</volume>(<issue>1</issue>):<fpage>18</fpage>–<lpage>31</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/T-AFFC.2011.15" xlink:type="simple">10.1109/T-AFFC.2011.15</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0173392.ref039">
<label>39</label>
<mixed-citation publication-type="other" xlink:type="simple">Chen YA, Wang JC, Yang YH, Chen HH. The AMG1608 dataset for music emotion recognition. In: Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing; 2015. p. 693–697.</mixed-citation>
</ref>
<ref id="pone.0173392.ref040">
<label>40</label>
<mixed-citation publication-type="other" xlink:type="simple">Bittner R, Salamon J, Tierney M, Mauch M, Cannam C, Bello JP. MedleyDB: A multitrack dataset for annotation-intensive MIR research. In: Proceedings of the International Society for Music Information Retrieval Conference; 2014.</mixed-citation>
</ref>
<ref id="pone.0173392.ref041">
<label>41</label>
<mixed-citation publication-type="other" xlink:type="simple">Aljanaki A, Yang YH, Soleymani M. Emotion in Music task at MediaEval 2015. In: Working Notes Proceedings of the MediaEval 2015 Workshop; 2015.</mixed-citation>
</ref>
<ref id="pone.0173392.ref042">
<label>42</label>
<mixed-citation publication-type="other" xlink:type="simple">Soleymani M, Larson M. Crowdsourcing for affective annotation of video: Development of a viewer-reported boredom corpus. In: Workshop on Crowdsourcing for Search Evaluation; 2010.</mixed-citation>
</ref>
<ref id="pone.0173392.ref043">
<label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Morris</surname> <given-names>JD</given-names></name>. <article-title>SAM: The Self-Assessment Manikin an efficient cross-cultural measurement of emotional response</article-title>. <source>J Advertising Research</source>. <year>1995</year>;<volume>35</volume>(<issue>8</issue>):<fpage>63</fpage>–<lpage>68</lpage>.</mixed-citation>
</ref>
<ref id="pone.0173392.ref044">
<label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>McKeown</surname> <given-names>GJ</given-names></name>, <name name-style="western"><surname>Sneddon</surname> <given-names>I</given-names></name>. <article-title>Modeling continuous self-report measures of perceived emotion using generalized additive mixed models</article-title>. <source>Psychological Methods</source>. <year>2014</year>;<volume>19</volume>(<issue>1</issue>):<fpage>155</fpage>–<lpage>174</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0034282" xlink:type="simple">10.1037/a0034282</ext-link></comment> <object-id pub-id-type="pmid">24219542</object-id></mixed-citation>
</ref>
<ref id="pone.0173392.ref045">
<label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Aljanaki</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Wiering</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Veltkamp</surname> <given-names>RC</given-names></name>. <article-title>Studying emotion induced by music through a crowdsourcing game</article-title>. <source>Information Processing and Management</source>. <year>2016</year>;<volume>52</volume>(<issue>1</issue>):<fpage>115</fpage>–<lpage>128</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.ipm.2015.03.004" xlink:type="simple">10.1016/j.ipm.2015.03.004</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0173392.ref046">
<label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schubert</surname> <given-names>E</given-names></name>. <article-title>Reliability issues regarding the beginning, middle and end of continuous emotion ratings to music</article-title>. <source>Psychology of Music</source>. <year>2013</year>;<volume>41</volume>(<issue>3</issue>):<fpage>350</fpage>–<lpage>371</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1177/0305735611430079" xlink:type="simple">10.1177/0305735611430079</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0173392.ref047">
<label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bachorik</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Bangert</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Loui</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Larke</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Berger</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Rowe</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>Emotion in motion: Investigating the time-course of emotional judgments of musical stimuli</article-title>. <source>Music Perception</source>. <year>2009</year>;<volume>26</volume>(<issue>4</issue>):<fpage>355</fpage>–<lpage>364</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1525/mp.2009.26.4.355" xlink:type="simple">10.1525/mp.2009.26.4.355</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0173392.ref048">
<label>48</label>
<mixed-citation publication-type="other" xlink:type="simple">Soleymani M, Aljanaki A, Yang YH. Emotional Analysis of Music: A comparison of methods. In: Proceedings of ACM International Conference on Multimedia; 2014.</mixed-citation>
</ref>
<ref id="pone.0173392.ref049">
<label>49</label>
<mixed-citation publication-type="other" xlink:type="simple">Soleymani M, Caro MN, Schmidt EM, Yang YH. The MediaEval 2013 Brave New Task: Emotion in Music. In: Working Notes Proceedings of the MediaEval 2013 Workshop; 2013.</mixed-citation>
</ref>
<ref id="pone.0173392.ref050">
<label>50</label>
<mixed-citation publication-type="other" xlink:type="simple">Aljanaki A, Yang YH, Soleymani M. Emotion in Music task at MediaEval 2014. In: Working Notes Proceedings of the MediaEval 2014 Workshop; 2014.</mixed-citation>
</ref>
<ref id="pone.0173392.ref051">
<label>51</label>
<mixed-citation publication-type="other" xlink:type="simple">Kumar N, Gupta R, Guha T, Vaz C, van Segbroeck M, Kim J, et al. Affective feature design and predicting continuous affective dimensions from music. In: Working Notes Proceedings of the MediaEval 2014 Workshop; 2014.</mixed-citation>
</ref>
<ref id="pone.0173392.ref052">
<label>52</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Lin</surname> <given-names>LIK</given-names></name>. <article-title>A concordance correlation coefficient to evaluate reproducibility</article-title>. <source>Biometrics (International Biometric Society)</source>. <year>1989</year>;<volume>45</volume>(<issue>1</issue>):<fpage>255</fpage>–<lpage>268</lpage>. <object-id pub-id-type="pmid">2720055</object-id></mixed-citation>
</ref>
<ref id="pone.0173392.ref053">
<label>53</label>
<mixed-citation publication-type="other" xlink:type="simple">Eyben F, Weninger F, Gross F, Schuller B. Recent developments in openSMILE, the Munich Open-source Multimedia Feature Extractor. In: Proceedings of ACM International Conference on Multimedia; 2013. p. 835–838.</mixed-citation>
</ref>
<ref id="pone.0173392.ref054">
<label>54</label>
<mixed-citation publication-type="other" xlink:type="simple">Weninger F, Eyben F, Schuller B. The TUM Approach to the MediaEval Music Emotion Task Using Generic Affective Audio Features. In: Working Notes Proceedings of the MediaEval 2013 Workshop; 2013.</mixed-citation>
</ref>
<ref id="pone.0173392.ref055">
<label>55</label>
<mixed-citation publication-type="other" xlink:type="simple">Coutinho E, Weninger F, Schuller B, Scherer KR. The Munich LSTM-RNN approach to the MediaEval 2014 Emotion in Music Task. In: Working Notes Proceedings of the MediaEval 2014 Workshop; 2014.</mixed-citation>
</ref>
<ref id="pone.0173392.ref056">
<label>56</label>
<mixed-citation publication-type="other" xlink:type="simple">Markov K, Matsui T. Dynamic music emotion recognition using state-space models. In: Working Notes Proceedings of the MediaEval 2014 Workshop; 2014.</mixed-citation>
</ref>
<ref id="pone.0173392.ref057">
<label>57</label>
<mixed-citation publication-type="other" xlink:type="simple">Aljanaki A, Wiering F, Veltkamp RC. MIRUtrecht participation in MediaEval 2013: Emotion in Music Task. In: Working Notes Proceedings of the MediaEval 2013 Workshop; 2013.</mixed-citation>
</ref>
<ref id="pone.0173392.ref058">
<label>58</label>
<mixed-citation publication-type="other" xlink:type="simple">Yang W, Cai K, Wu B, Wang Y, Chen X, Yang D, et al. Beatsens solution for MediaEval 2014 Emotion in Music Task. In: Working Notes Proceedings of the MediaEval 2014 Workshop; 2014.</mixed-citation>
</ref>
<ref id="pone.0173392.ref059">
<label>59</label>
<mixed-citation publication-type="other" xlink:type="simple">Fan Y, Xu M. MediaEval 2014: THU-HCSIL approach to emotion in music task using multi-level regression. In: Working Notes Proceedings of the MediaEval 2014 Workshop; 2014.</mixed-citation>
</ref>
<ref id="pone.0173392.ref060">
<label>60</label>
<mixed-citation publication-type="other" xlink:type="simple">Xu M, Li X, Xianyu H, Tian J, Meng F, Chen W. Multi-scale approaches to the MediaEval 2015 “Emotion in Music” Task. In: Working Notes Proceedings of the MediaEval 2015 Workshop; 2015.</mixed-citation>
</ref>
<ref id="pone.0173392.ref061">
<label>61</label>
<mixed-citation publication-type="other" xlink:type="simple">Coutinho E, Trigeorgis G, Zafeiriou S, Schuller B. Automatically estimating emotion in music with deep long-short term memory recurrent neural networks. In: Working Notes Proceedings of the MediaEval 2015 Workshop; 2015.</mixed-citation>
</ref>
<ref id="pone.0173392.ref062">
<label>62</label>
<mixed-citation publication-type="other" xlink:type="simple">Gupta R, Narayanan S. Predicting affect in music using regression methods on low level features. In: Working Notes Proceedings of the MediaEval 2015 Workshop; 2015.</mixed-citation>
</ref>
<ref id="pone.0173392.ref063">
<label>63</label>
<mixed-citation publication-type="other" xlink:type="simple">Liu Y, Liu Y, Gu Z. Affective feature extraction for music emotion prediction. In: Working Notes Proceedings of the MediaEval 2015 Workshop; 2015.</mixed-citation>
</ref>
<ref id="pone.0173392.ref064">
<label>64</label>
<mixed-citation publication-type="other" xlink:type="simple">Cai K, Yang W, Cheng Y, Yang D, Chen X. PKU-AIPL solution for MediaEval 2015 Emotion in Music Task. In: Working Notes Proceedings of the MediaEval 2015 Workshop; 2015.</mixed-citation>
</ref>
<ref id="pone.0173392.ref065">
<label>65</label>
<mixed-citation publication-type="other" xlink:type="simple">Pellegrini T, Barriere V. Time-continuous estimation of emotion in music with recurrent neural networks. In: Working Notes Proceedings of the MediaEval 2015 Workshop; 2015.</mixed-citation>
</ref>
<ref id="pone.0173392.ref066">
<label>66</label>
<mixed-citation publication-type="other" xlink:type="simple">Cai K, Yang W, Cheng Y, Yang D, Chen X. PKU-AIPL’ Solution for MediaEval 2015 Emotion in Music Task. In: Working Notes Proceedings of the MediaEval 2015 Workshop; 2015.</mixed-citation>
</ref>
<ref id="pone.0173392.ref067">
<label>67</label>
<mixed-citation publication-type="other" xlink:type="simple">Patra BG, Maitra P, Das D, Bandyopadhyay S. MediaEval 2015: Music Emotion Recognition based on feed-forward neural network. In: Working Notes Proceedings of the MediaEval 2015 Workshop; 2015.</mixed-citation>
</ref>
<ref id="pone.0173392.ref068">
<label>68</label>
<mixed-citation publication-type="other" xlink:type="simple">Chmulik M, Guoth I, Malik M, Jarina R. UNIZA system for the “Emotion in Music” task at MediaEval 2015. In: Working Notes Proceedings of the MediaEval 2015 Workshop; 2015.</mixed-citation>
</ref>
<ref id="pone.0173392.ref069">
<label>69</label>
<mixed-citation publication-type="other" xlink:type="simple">Aljanaki A, Wiering F, Veltkamp R. MediaEval 2015: A segmentation-based approach to continuous emotion tracking. In: Working Notes Proceedings of the MediaEval 2015 Workshop; 2015.</mixed-citation>
</ref>
<ref id="pone.0173392.ref070">
<label>70</label>
<mixed-citation publication-type="other" xlink:type="simple">Weber M, Krismayer T, Wöß J, Aigmüller L, Birnzain P. MediaEval 2015: JKU-Tinnitus approach to Emotion in Music Task. In: Working Notes Proceedings of the MediaEval 2015 Workshop; 2015.</mixed-citation>
</ref>
<ref id="pone.0173392.ref071">
<label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Huq</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bello</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Rowe</surname> <given-names>R</given-names></name>. <article-title>Automated music emotion recognition: A systematic evaluation</article-title>. <source>Journal of New Music Research</source>. <year>2010</year>;<volume>39</volume>(<issue>3</issue>):<fpage>227</fpage>–<lpage>244</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/09298215.2010.513733" xlink:type="simple">10.1080/09298215.2010.513733</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0173392.ref072">
<label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Wang</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Yang</surname> <given-names>YH</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>HM</given-names></name>, <name name-style="western"><surname>Jeng</surname> <given-names>SK</given-names></name>. <article-title>Modeling the affective content of music with a Gaussian mixture model</article-title>. <source>IEEE Transactions on Affective Computing</source>. <year>2015</year>;<volume>6</volume>(<issue>1</issue>):<fpage>56</fpage>–<lpage>68</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TAFFC.2015.2397457" xlink:type="simple">10.1109/TAFFC.2015.2397457</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0173392.ref073">
<label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Soleymani</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Larson</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Pun</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Hanjalic</surname> <given-names>A</given-names></name>. <article-title>Corpus development for affective video indexing</article-title>. <source>IEEE Transactions on Multimedia</source>. <year>2014</year>;<volume>16</volume>(<issue>4</issue>):<fpage>1075</fpage>–<lpage>1089</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TMM.2014.2305573" xlink:type="simple">10.1109/TMM.2014.2305573</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0173392.ref074">
<label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Collier</surname> <given-names>G</given-names></name>. <article-title>Beyond valence and activity in the emotional connotations of music</article-title>. <source>Psychology of Music</source>. <year>2007</year>;<volume>35</volume>(<issue>1</issue>):<fpage>110</fpage>–<lpage>131</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1177/0305735607068890" xlink:type="simple">10.1177/0305735607068890</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0173392.ref075">
<label>75</label>
<mixed-citation publication-type="other" xlink:type="simple">Aljanaki A, Wiering F, Veltkamp RC. Emotion based segmentation of musical audio. In: Proceedings of International Society for Music Information Retrieval Conference; 2015.</mixed-citation>
</ref>
<ref id="pone.0173392.ref076">
<label>76</label>
<mixed-citation publication-type="other" xlink:type="simple">Su D, Fung P. Personalized music emotion classification via active learning. In: Proceedings of ACM International Workshop on Music Information Retrieval with User-Centered and Multimodal Strategies; 2012. p. 51–62.</mixed-citation>
</ref>
<ref id="pone.0173392.ref077">
<label>77</label>
<mixed-citation publication-type="other" xlink:type="simple">Chen YA, Wang JC, Yang YH, Chen HH. Linear regression-based adaptation of music emotion recognition models for personalization. In: Proceedings of IEEE International Conference Acoustics, Speech &amp; Signal Processing; 2014. p. 2149–2153.</mixed-citation>
</ref>
<ref id="pone.0173392.ref078">
<label>78</label>
<mixed-citation publication-type="other" xlink:type="simple">Schmidt EM, Kim YE. Learning rhythm and melody features with deep belief networks. In: Proceedings of International Society for Music Information Retrieval Conference; 2013.</mixed-citation>
</ref>
<ref id="pone.0173392.ref079">
<label>79</label>
<mixed-citation publication-type="other" xlink:type="simple">Panda R, Rocha B, Paiva R. Dimensional music emotion recognition: Combining standard and melodic audio features. In: Proceedings of International Symposium on Computer Music Modelling &amp; Retrieval; 2013.</mixed-citation>
</ref>
<ref id="pone.0173392.ref080">
<label>80</label>
<mixed-citation publication-type="other" xlink:type="simple">Coutinho E, Deng J, Schuller B. Transfer learning emotion manifestation across music and speech. In: 2014 International Joint Conference on Neural Networks (IJCNN); 2014. p. 3592–3598.</mixed-citation>
</ref>
<ref id="pone.0173392.ref081">
<label>81</label>
<mixed-citation publication-type="other" xlink:type="simple">He L, Jiang D, Yang L, Pei E, Wu P, Sahli H. Multimodal Affective Dimension Prediction Using Deep Bidirectional Long Short-Term Memory Recurrent Neural Networks. In: Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge. AVEC’15. New York, NY, USA: ACM; 2015. p. 73–80.</mixed-citation>
</ref>
</ref-list>
</back>
</article>