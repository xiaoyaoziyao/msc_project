<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pone.0196846</article-id>
<article-id pub-id-type="publisher-id">PONE-D-17-40619</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Image analysis</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Specimen preparation and treatment</subject><subj-group><subject>Staining</subject><subj-group><subject>Nuclear staining</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Histochemistry and cytochemistry techniques</subject><subj-group><subject>Immunohistochemistry techniques</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Immunologic techniques</subject><subj-group><subject>Immunohistochemistry techniques</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Computing methods</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>People and places</subject><subj-group><subject>Population groupings</subject><subj-group><subject>Professions</subject><subj-group><subject>Analysts</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Oncology</subject><subj-group><subject>Cancers and neoplasms</subject><subj-group><subject>Breast tumors</subject><subj-group><subject>Breast cancer</subject></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Optimized generation of high-resolution phantom images using cGAN: Application to quantification of Ki67 breast cancer images</article-title>
<alt-title alt-title-type="running-head">Optimized generation of high-resolution phantom images using cGAN</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-4211-4367</contrib-id>
<name name-style="western">
<surname>Senaras</surname>
<given-names>Caglar</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Niazi</surname>
<given-names>Muhammad Khalid Khan</given-names>
</name>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Sahiner</surname>
<given-names>Berkman</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Pennell</surname>
<given-names>Michael P.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Tozbikian</surname>
<given-names>Gary</given-names>
</name>
<role content-type="http://credit.casrai.org/">Validation</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Lozanski</surname>
<given-names>Gerard</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Funding acquisition</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
<xref ref-type="fn" rid="econtrib001"><sup>‡</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Gurcan</surname>
<given-names>Metin N.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="fn" rid="econtrib001"><sup>‡</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Center for Biomedical Informatics, Wake Forest School of Medicine, Winston-Salem, North Carolina, United States of America</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Office of Science and Engineering Laboratories, Center for Devices and Radiological Health, Food and Drug Administration, Silver Spring, Maryland, United States of America</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Division of Biostatistics, College of Public Health, The Ohio State University, Columbus, Ohio, United States of America</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Department of Pathology, The Ohio State University, Columbus, Ohio, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Kumar-Sinha</surname>
<given-names>Chandan</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Michigan, UNITED STATES</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="other" id="econtrib001">
<p>‡ GL and MNG are joint senior authors on this work.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">csenaras@wakehealth.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>9</day>
<month>5</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<year>2018</year>
</pub-date>
<volume>13</volume>
<issue>5</issue>
<elocation-id>e0196846</elocation-id>
<history>
<date date-type="received">
<day>16</day>
<month>11</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>20</day>
<month>4</month>
<year>2018</year>
</date>
</history>
<permissions>
<license xlink:href="https://creativecommons.org/publicdomain/zero/1.0/" xlink:type="simple">
<license-p>This is an open access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/" xlink:type="simple">Creative Commons CC0</ext-link> public domain dedication.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0196846"/>
<abstract>
<p>In pathology, Immunohistochemical staining (IHC) of tissue sections is regularly used to diagnose and grade malignant tumors. Typically, IHC stain interpretation is rendered by a trained pathologist using a manual method, which consists of counting each positively- and negatively-stained cell under a microscope. The manual enumeration suffers from poor reproducibility even in the hands of expert pathologists. To facilitate this process, we propose a novel method to create artificial datasets with the known ground truth which allows us to analyze the recall, precision, accuracy, and intra- and inter-observer variability in a systematic manner, enabling us to compare different computer analysis approaches. Our method employs a conditional Generative Adversarial Network that uses a database of Ki67 stained tissues of breast cancer patients to generate synthetic digital slides. Our experiments show that synthetic images are indistinguishable from real images. Six readers (three pathologists and three image analysts) tried to differentiate 15 real from 15 synthetic images and the probability that the average reader would be able to correctly classify an image as synthetic or real more than 50% of the time was only 44.7%.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000054</institution-id>
<institution>National Cancer Institute</institution>
</institution-wrap>
</funding-source>
<award-id>R01CA134451</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Gurcan</surname>
<given-names>Metin N.</given-names>
</name>
</principal-award-recipient>
</award-group>
<award-group id="award002">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000054</institution-id>
<institution>National Cancer Institute</institution>
</institution-wrap>
</funding-source>
<award-id>U24CA199374</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Gurcan</surname>
<given-names>Metin N.</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>The project described was supported in part by Awards Number R01CA134451 (PIs: Gurcan, Lozanski), U24CA199374 (PI: Gurcan) from the National Cancer Institute. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Cancer Institute, the National Institute of Allergy and Infectious Diseases, or the National Institutes of Health.</funding-statement>
</funding-group>
<counts>
<fig-count count="7"/>
<table-count count="1"/>
<page-count count="12"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All training dataset are available at: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1184621" xlink:type="simple">https://doi.org/10.5281/zenodo.1184621</ext-link>. All source codes are available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/cialab/DeepSlides" xlink:type="simple">https://github.com/cialab/DeepSlides</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>In clinical practice, Immunohistochemistry (IHC) is widely used to localize specific epitopes of molecules in cells and tissues that aid in diagnosis and prognosis of cancer [<xref ref-type="bibr" rid="pone.0196846.ref001">1</xref>–<xref ref-type="bibr" rid="pone.0196846.ref003">3</xref>]. IHC also plays a critical role in selecting a proper systemic therapy for cancer patients [<xref ref-type="bibr" rid="pone.0196846.ref002">2</xref>]. Generally, IHC markers are used according to specific guidelines where the intensity of stains and the number of positively stained cells are expressed as a percentage of all malignant cells. In clinical practice, IHC stain interpretation is often carried out manually. The prediction consists of counting each positively- and negatively-stained malignant cell under a microscope [<xref ref-type="bibr" rid="pone.0196846.ref004">4</xref>] and calculating the ratio of positive to total malignant cells. Faced with this daunting task and shortage of time, some pathologists revert to visual estimation of this ratio [<xref ref-type="bibr" rid="pone.0196846.ref003">3</xref>]. As expected, the manual ratio estimation suffers from poor reproducibility even in the hands of expert pathologists [<xref ref-type="bibr" rid="pone.0196846.ref002">2</xref>, <xref ref-type="bibr" rid="pone.0196846.ref005">5</xref>, <xref ref-type="bibr" rid="pone.0196846.ref006">6</xref>].</p>
<p>A traditional approach for the evaluation of computerized quantitative image analysis methods includes having an expert diligently generate a reference standard (e.g., by segmenting structures or by counting cells), and then comparing the computer results to the reference standard. However, due to inter- and intra-observer variability in performing a quantitative task on digital pathology images [<xref ref-type="bibr" rid="pone.0196846.ref007">7</xref>], a reference standard generated by one expert is often considered inadequate, and multiple experts’ interpretation is sought. Involving multiple experts results in a resource-intensive evaluation process and limits the sample size for the evaluation. If the ground truth were known, as in the case of synthetically generated images, the effort for the evaluation would be immensely reduced, and much larger evaluation data sets could be used, reducing the uncertainty inherent due to limited sample sizes.</p>
<p>There have been some efforts to develop synthetic histopathological images. Cheikh et al. recently developed a synthetic histological image generation algorithm by modeling tumor architecture and spatial interactions in breast cancer [<xref ref-type="bibr" rid="pone.0196846.ref008">8</xref>]. Although the statistical properties of the generated synthetic images (i.e., the number of tumor patterns, their shape and their area) were similar to those of real images, the models created ‘unrealistic’ details in the synthetic images. In a recent study by our group [<xref ref-type="bibr" rid="pone.0196846.ref009">9</xref>], we manually generated a collection by extracting a group of Ki-67 positive and negative nuclei from images of Ki-67 stained follicular lymphoma biopsies. Our algorithm generated synthetic tissue sections with known percentages of positive and negative nuclei by using this collection. Although the statistical characteristics of the nuclei and their appearance mimicked real cases, the visual variance of the nuclei was dependent on the richness of the created collection, and the tissue background appeared unrealistic. As a result, neither of these approaches could create realistic images to match pathologists’ expectations or to validate analytical methods.</p>
<p>In this study, we developed a novel approach for creating synthetic digital histopathological slides by artificial neural networks. In recent years, the convolutional neural networks (CNN) have become a critical workhorse for many different image processing problems [<xref ref-type="bibr" rid="pone.0196846.ref010">10</xref>–<xref ref-type="bibr" rid="pone.0196846.ref012">12</xref>]. A novel application of the CNN is in Generative Adversarial Networks (GAN) with a goal to “make the output indistinguishable from reality” [<xref ref-type="bibr" rid="pone.0196846.ref013">13</xref>]. Our method is a variation of a GAN, termed conditional GAN (cGAN), which allows generating very realistic histopathological images with fully controlled ground truth [<xref ref-type="bibr" rid="pone.0196846.ref014">14</xref>]. We believe that an important application area for our synthetic IHC image generation method is the evaluation of quantitative image analysis methods for IHC slides. By using our method, we can generate realistic looking positive and negative nuclei with different shape, size, and spatial distributions.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Method</title>
<sec id="sec003">
<title>Dataset collection</title>
<p>In this study, we collected Ki67-stained whole slide images from 32 different breast cancer patients. This study is IRB approved by the OSU Cancer Institutional Review Board (OSU-15136), Office of Responsible Research Practices, with Waiver of Consent Process, and Full of Waiver of HIPAA Research Authorization. For this particular application, we scanned these slides using an Aperio ScanScope (Leica Biosystems Inc., Buffalo Grove, IL) at a 40x magnification where the pixel size is 0.2461 x 0.2461 μm<sup>2</sup>. An experienced breast pathologist carefully annotated these slides for tumor and non-tumor regions. We randomly selected a total of 84 region-of-interest (ROI) images within the tumor region. Each ROI has a size of 2300x1200 pixels which is equivalent to one high-power-field. We intentionally selected this size to provide the pathologists with the similar environment when they analyze a slide at 40x magnification under a microscope. We experimented with two different input data types to train our system: 1) user annotations mask and 2) segmentation output mask.</p>
<p>After the input data generation, all of the ROIs were divided into tiles of size 256x256 pixels. Any tile that doesn’t contain a positive or a negative nucleus was excluded from the dataset. There were a total of 694 tiles, 572 of which were used for training, and the remaining 122 for visual validation. We followed two different approaches to train our system.</p>
<sec id="sec004">
<title>User annotation mask</title>
<p>To create the training dataset, all of the stain-positive and stain-negative nuclei in the ROIs were marked manually. A stain-positive (or negative) nucleus means that a cell within a tissue is stained positively (or negatively). To ensure the quality of the annotations, we worked with four trained operators. Each operator first annotated the entire positive and negative nuclei with colored dots in 21 ROIs, analyzed the annotations of another operator, and corrected any annotation errors. No area, orientation or shape information was recorded because the nuclei were represented by only coordinate information represented by dots.</p>
</sec>
<sec id="sec005">
<title>Computer segmentation mask</title>
<p>As a second approach, we trained our system with the output of a nuclei segmentation technique that we developed in a prior study [<xref ref-type="bibr" rid="pone.0196846.ref005">5</xref>]. To illustrate the process, in <xref ref-type="fig" rid="pone.0196846.g001">Fig 1</xref>, the colors green and red represent the regions that are segmented as Ki67 positive and Ki67 negative, respectively by our technique. The yellow color was used for lightly stained positive regions, which may occur as staining artifacts or background staining. For each tile, we generated the nuclei segmentation and used it as an input for our cGAN neural network similar to the implementation in [<xref ref-type="bibr" rid="pone.0196846.ref013">13</xref>].</p>
<fig id="pone.0196846.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0196846.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Training of discriminator network.</title>
<p><italic>For real examples</italic>, <italic>we used the real images and their segmentation/annotation masks (M</italic><sup><italic>i</italic></sup>, <inline-formula id="pone.0196846.e001"><alternatives><graphic id="pone.0196846.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196846.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula><italic>) as an input. The</italic> green and red colored annotations correspond to Ki67 positive and Ki67 negative nuclei, respectively. <italic>For fake examples</italic>, <italic>we applied a two-step procedure</italic>. <italic>In Step 1</italic>, <italic>we used generator (U-net) algorithm to create a synthetic image by using the segmentation/annotation</italic>. <italic>In Step 2</italic>, <italic>the output of the generator and initial segmentation (M</italic><sup><italic>i</italic></sup>, <inline-formula id="pone.0196846.e002"><alternatives><graphic id="pone.0196846.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196846.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula><italic>) are used as an input for D</italic>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0196846.g001" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec006">
<title>Phantom image generation</title>
<p>As mentioned earlier, a cGAN is computational model to generate realistic looking synthetic images. It consists of two main components: a generator (<bold>G</bold>) and a discriminator (<bold>D</bold>). During the training, the generator learns to produce realistic looking images without a prior knowledge of underlying probability distribution. Simultaneously, the discriminator learns to distinguish between real images and the images produced by the generator. The main idea is to devise a system where synthetic images produced by the generator become indistinguishable from real images. The technical details necessary to implement our method are described below.</p>
<p>For a given real image, <inline-formula id="pone.0196846.e003"><alternatives><graphic id="pone.0196846.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196846.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, let <italic>M</italic><sup><italic>i</italic></sup> represent its corresponding user annotations or segmentation output mask. The generator <bold>G</bold>, tries to create output images, <inline-formula id="pone.0196846.e004"><alternatives><graphic id="pone.0196846.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196846.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, that cannot be distinguished by <bold>D</bold> from real images. The final objective function, <italic>L</italic><sub><italic>final</italic></sub> is defined as:
<disp-formula id="pone.0196846.e005">
<alternatives>
<graphic id="pone.0196846.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196846.e005" xlink:type="simple"/>
<mml:math display="block" id="M5">
<mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mtext>arg</mml:mtext></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mspace width="1pt"/><mml:mtext>min</mml:mtext></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:mspace width="1pt"/><mml:mtext>max</mml:mtext></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>G</mml:mi><mml:mi>A</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
where L<sub><italic>cGAN</italic></sub>(<italic>G</italic>, <italic>D</italic>) is part of the objective function which <bold>D</bold> tries to maximize while learning on how to distinguish real pairs (<italic>M</italic><sup><italic>i</italic></sup>, <inline-formula id="pone.0196846.e006"><alternatives><graphic id="pone.0196846.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196846.e006" xlink:type="simple"/><mml:math display="inline" id="M6"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) from fake pairs (<italic>M</italic><sup><italic>i</italic></sup>, <inline-formula id="pone.0196846.e007"><alternatives><graphic id="pone.0196846.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196846.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>). Simultaneously, <bold>G</bold> tries to minimize L<sub><italic>cGAN</italic></sub>(<italic>G</italic>, <italic>D</italic>) and synthesize fake images that would deceive D. Here, L<sub>l1</sub>(G) is the difference of output <inline-formula id="pone.0196846.e008"><alternatives><graphic id="pone.0196846.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196846.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, and the ground truth, <inline-formula id="pone.0196846.e009"><alternatives><graphic id="pone.0196846.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196846.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, as L1 distance [<xref ref-type="bibr" rid="pone.0196846.ref013">13</xref>].</p>
<p>In the study, as a generator, we used a modified version of the “U-net” [<xref ref-type="bibr" rid="pone.0196846.ref015">15</xref>], whose architectural overview is shown in <xref ref-type="fig" rid="pone.0196846.g002">Fig 2</xref>. All CNN blocks described in <xref ref-type="fig" rid="pone.0196846.g002">Fig 2</xref> includes 3x3 CNNs with 2x2 strides, Batch Normalization [<xref ref-type="bibr" rid="pone.0196846.ref016">16</xref>], and leak Relu layers [<xref ref-type="bibr" rid="pone.0196846.ref017">17</xref>]. The generator includes 16 CNN blocks, eight of those are used for encoding and the remaining eight are used for decoding. For larger images, the number of blocks may be increased. The number of the filters at i<sup>th</sup> CNN block, n<sub>i</sub>, is defined as:
<disp-formula id="pone.0196846.e010">
<alternatives>
<graphic id="pone.0196846.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196846.e010" xlink:type="simple"/>
<mml:math display="block" id="M10">
<mml:msub><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>64</mml:mn><mml:mo>*</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mtext>min</mml:mtext><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mspace width="1pt"/><mml:mi mathvariant="normal">L</mml:mi><mml:mo>-</mml:mo><mml:mn>0.5</mml:mn><mml:mo>-</mml:mo><mml:mo>|</mml:mo><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mo>-</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>|</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:msup>
</mml:math>
</alternatives>
</disp-formula>
where L is the number of layers in the encoder and decoder, and is equal to eight in the current setup.</p>
<fig id="pone.0196846.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0196846.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Used neural network framework for generator, G.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0196846.g002" xlink:type="simple"/>
</fig>
<p>As discriminator, <bold>D</bold>, we used a CNN based classifier “patchGAN” [<xref ref-type="bibr" rid="pone.0196846.ref013">13</xref>]. The classifier includes four CNN blocks and a convolution layer with a 1-dimensional output. The image is divided into small tiles and for each patch; patchGAN tries to identify the input as real or fake. The final output is the average of all responses.</p>
<p>During the training of the proposed method, we followed the standard approach [<xref ref-type="bibr" rid="pone.0196846.ref013">13</xref>, <xref ref-type="bibr" rid="pone.0196846.ref018">18</xref>], such that one gradient descent step on <bold>D</bold> is followed by one gradient descent step on <bold>G</bold> for optimization. The training procedure for <bold>D</bold> is given in <xref ref-type="fig" rid="pone.0196846.g001">Fig 1</xref>. The network is optimized with Adam [<xref ref-type="bibr" rid="pone.0196846.ref019">19</xref>] for 200 epochs with the batch size of four.</p>
<p>For inference, the method provides the freedom to manually create a scenario which allows: 1) defining different spatial overlap of nuclei, 2) placement of different sized nuclei at certain locations, and 3) control over spatial frequency of nuclei during synthetic data generation (<xref ref-type="fig" rid="pone.0196846.g003">Fig 3</xref>). We fed the input data to G and skipped the D to create an inferred synthetic image, i.e. we do not use <bold>D</bold> during the inference. Besides since the G does not include fully connected layers, it is possible to generate larger images during the inference.</p>
<fig id="pone.0196846.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0196846.g003</object-id>
<label>Fig 3</label>
<caption>
<title/>
<p>Fully synthetic images (e-g). We created several toy data to generate synthetic images with different characteristics by using annotation based input (a) and segmentation based input (b and c).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0196846.g003" xlink:type="simple"/>
</fig>
<sec id="sec007">
<title>Experiments</title>
<p>We trained our method for both of the datasets (i.e. “user annotated”, and “segmentation output masks”) separately. We trained two systems with different input types, and the comparison of their results is presented in the results section. We tested our algorithm on an independent set of 122 randomly selected validation images, none of which were used during the training. We used each <inline-formula id="pone.0196846.e011"><alternatives><graphic id="pone.0196846.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196846.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>, and it’s corresponding <italic>M</italic><sup><italic>i</italic></sup> to create a synthetic image with the same characteristics as <inline-formula id="pone.0196846.e012"><alternatives><graphic id="pone.0196846.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0196846.e012" xlink:type="simple"/><mml:math display="inline" id="M12"><mml:msubsup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>.</p>
<p>In our first experiment, we worked with three image analysts and three pathologists for their visual evaluations. To maintain the attention of the observers, we divided the experiment into three parts. In each part, we showed a dataset of 10 images and asked the observers to identify synthetic images. To make the parts unbiased, the distributions of the synthetic images in the three datasets were kept confidential. The dataset for the first part included 10 synthetic images. The second dataset included 10 real images, and final dataset included five real and five synthetic images.</p>
<p>Reader accuracy in identifying the correct image type (real versus synthetic) was analyzed using a hierarchical Bayesian logistic regression model containing random effects for images and readers. The random reader effects accounted for heterogeneity in reader accuracy while the random image effects accounted for heterogeneity in the difficulty of images. Diffuse or non-informative priors were assigned to all parameters, and the posterior inference was obtained using Metropolis-Hastings sampling run for 500,000 iterations following a 5,000-iteration burn-in. Sampled values were used to calculate the posterior probability that the average reader would be able to identify the correct image type more than 50% of the time if presented with an image of average difficulty. Two readers (Image Analysts 1 and 2) were excluded from this analysis since we did not record their decisions on individual images; we just tabulated the number correct and incorrect for each data set. As a secondary analysis, the hierarchical model was extended to included fixed effects of data set to determine if performance differed by ratio of real to artificial cases. Modeling was performed using PROC MCMC in SAS Version 9.4 (SAS Inc, Cary, NC).</p>
<p>As a prerequisite to the claim that images generated by our technique can produce images that can be used for evaluation of computerized quantitative methods, we demonstrated that quantitative methods perform similarly for real and synthetic images. To test our method in a situation similar to the example in <xref ref-type="fig" rid="pone.0196846.g003">Fig 3</xref>, we used a real data set of 122 Ki-67 stained images that was completely independent from the cGAN training data set. We call this data set as the <italic>quantitative comparison dataset</italic>. For each image patch in our quantitative comparison data set, we aimed at generating a synthetic image that is different from the real image in terms of its overall appearance (i.e., location and spatial arrangement of the cells) but is similar to the real image in terms of Ki-67 quantitation. To achieve this, we used a segmentation algorithm that was previously developed in our laboratory [<xref ref-type="bibr" rid="pone.0196846.ref005">5</xref>] to generate a segmentation mask, and applied the segmentation mask and the real image as the input to the cGAN. We used the output of the cGAN as the synthetic image. An example of the real and synthetic images used in this experiment is shown in <xref ref-type="fig" rid="pone.0196846.g004">Fig 4</xref>.</p>
<fig id="pone.0196846.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0196846.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Example (a) real image, (b) segmentation result based on [<xref ref-type="bibr" rid="pone.0196846.ref005">5</xref>], (c) synthetic image used for evaluation of computerized quantitative method, (d) visual ImmunoRatio output for the real image, visual ImmunoRatio output for synthetic image.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0196846.g004" xlink:type="simple"/>
</fig>
<p>If the cGAN output is suitable for the evaluation of computerized quantitative methods, then a quantitative method applied to the real and cGAN-generated images should provide similar results, as discussed above. To test this, we applied a quantification method that uses a fundamentally different segmentation algorithm from our segmentation algorithm to both real and synthetic images. The quantification method, ImmunoRatio, calculates the percentage of positively stained nuclear area by using a color deconvolution algorithm for separating the staining components and adaptive thresholding for nuclear area segmentation [<xref ref-type="bibr" rid="pone.0196846.ref020">20</xref>]. Agreement between ImmunoRatio values measured on real images and their artificial replicas was quantified using Lin’s concordance correlation [<xref ref-type="bibr" rid="pone.0196846.ref021">21</xref>] and visualized using a Bland-Altman plot [<xref ref-type="bibr" rid="pone.0196846.ref022">22</xref>].</p>
</sec>
</sec>
</sec>
<sec id="sec008" sec-type="conclusions">
<title>Results and discussion</title>
<p><xref ref-type="fig" rid="pone.0196846.g005">Fig 5</xref> shows some example input images, associated masks (either user annotations or computer segmentations) and generated synthetic images. <xref ref-type="fig" rid="pone.0196846.g005">Fig 5b</xref> shows the manual nuclei annotations for an example image in <xref ref-type="fig" rid="pone.0196846.g005">Fig 5a</xref>. The output of the generator is given in <xref ref-type="fig" rid="pone.0196846.g005">Fig 5c</xref>. Similarly, the output of the generator by using the segmentation algorithm’s output (<xref ref-type="fig" rid="pone.0196846.g005">Fig 5e</xref>) is given in <xref ref-type="fig" rid="pone.0196846.g005">Fig 5f</xref>. The numbers of correctly identified real and synthetic image by readers are given in <xref ref-type="table" rid="pone.0196846.t001">Table 1</xref>.</p>
<table-wrap id="pone.0196846.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0196846.t001</object-id>
<label>Table 1</label> <caption><title>Experts’ discrimination performance on synthetic/real images.</title> <p>TP represents the number of correctly identified synthetic images and TN represents the number of correctly identified real images.</p></caption>
<alternatives>
<graphic id="pone.0196846.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0196846.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center" colspan="3">Pathologist 1</th>
<th align="center" colspan="3">Pathologist 2</th>
<th align="center" colspan="3">Pathologist 3</th>
<th align="center" colspan="3">Image Analyst 1</th>
<th align="center" colspan="3">Image Analyst 2</th>
<th align="center" colspan="3">Image Analyst 3</th>
</tr>
<tr>
<th align="left"/>
<th align="left">TP</th>
<th align="left">TN</th>
<th align="left">TP+ TN</th>
<th align="left">TP</th>
<th align="left">TN</th>
<th align="left">TP+ TN</th>
<th align="left">TP</th>
<th align="left">TN</th>
<th align="left">TP+ TN</th>
<th align="left">TP</th>
<th align="left">TN</th>
<th align="left">TN+ TP</th>
<th align="left">TP</th>
<th align="left">TN</th>
<th align="left">TN+ TP</th>
<th align="left">TP</th>
<th align="left">TN</th>
<th align="left">TN+ TP</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Dataset1 (10 synthetic)</td>
<td align="left">2</td>
<td align="left">0</td>
<td align="left">2</td>
<td align="left">6</td>
<td align="left">0</td>
<td align="left">6</td>
<td align="left">6</td>
<td align="left">0</td>
<td align="left">6</td>
<td align="left">4</td>
<td align="left">0</td>
<td align="left">4</td>
<td align="left">10</td>
<td align="left">0</td>
<td align="left">10</td>
<td align="left">4</td>
<td align="left">0</td>
<td align="left">4</td>
</tr>
<tr>
<td align="left">Dataset2 (10 real)</td>
<td align="left">0</td>
<td align="left">6</td>
<td align="left">6</td>
<td align="left">0</td>
<td align="left">6</td>
<td align="left">6</td>
<td align="left">0</td>
<td align="left">4</td>
<td align="left">4</td>
<td align="left">0</td>
<td align="left">5</td>
<td align="left">5</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">6</td>
<td align="left">6</td>
</tr>
<tr>
<td align="left">Dataset3 (5 synthetic, 5 real)</td>
<td align="left">0</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">4</td>
<td align="left">2</td>
<td align="left">6</td>
<td align="left">2</td>
<td align="left">3</td>
<td align="left">5</td>
<td align="left">2</td>
<td align="left">3</td>
<td align="left">5</td>
<td align="left">3</td>
<td align="left">1</td>
<td align="left">4</td>
<td align="left">3</td>
<td align="left">3</td>
<td align="left">6</td>
</tr>
<tr>
<td align="left">Accuracy</td>
<td align="center" colspan="3">33.3%</td>
<td align="center" colspan="3">60.0%</td>
<td align="center" colspan="3">50.0%</td>
<td align="center" colspan="3">46.7%</td>
<td align="center" colspan="3">46.7%</td>
<td align="center" colspan="3">53.3%</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<fig id="pone.0196846.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0196846.g005</object-id>
<label>Fig 5</label>
<caption>
<title/>
<p>Example images (a) original image used for annotation (b) a dot based annotation, (c) cGAN generated synthetic image from (b). (d) Original image used for segmentation (e) segmentation result using [<xref ref-type="bibr" rid="pone.0196846.ref023">23</xref>], (f) cGAN generated image from (e).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0196846.g005" xlink:type="simple"/>
</fig>
<p>According to our hierarchical logistic regression model, the probability that the average reader would be able to correctly classify an image as synthetic or real more than 50% of the time was only 44.7%. These results suggest that, overall; readers are incapable of distinguishing synthetic images from real ones. However, the results differed by data set: when presented with a data set comprised entirely of real images, the posterior probability of correctly classifying an image more than 50% of the time was 70.4% compared to only a 30.5% probability for data set 1 (100% synthetic images) and a 40.1% probability for data set 3 (50% synthetic, 50% real). The improved classification performance in dataset 1 could be due to a tendency of readers to label images as “real” slightly more often than “synthetic” (54% of the time compared to 46% of the time based on the data for the four readers used in the modeling).</p>
<p>When analyzing the real and synthetic images using computerized quantitative methods, we examined the ImmunoRatio differences between real and synthetic images. <xref ref-type="fig" rid="pone.0196846.g006">Fig 6</xref> displays the Bland-Altman analysis of the ImmunoRatio data. The average difference in ImmunoRatio values was 0.53 and the difference did not appear to depend on value of the ratio. Considering that ImmunoRatio is a percentage (i.e., its value ranges between 0% and 100%), this average difference (estimated bias) is very small. The limits of agreement between the two images were (-6.1, 7.1). Furthermore, the concordance correlation coefficient was 0.99 (95% CI: 0.98–0.99) which indicates almost perfect agreement between the ImmunoRatio values of the real and artificial images. <xref ref-type="fig" rid="pone.0196846.g006">Fig 6</xref> displays the Bland-Altman analysis of the ImmunoRatio data. There were two main reasons for the observed ImmunoRatio differences between the real and synthetic images. The first reason was the initial segmentation algorithm may result in some false positives and false negatives. This error was propagated to the synthetically generated image (<xref ref-type="fig" rid="pone.0196846.g007">Fig 7</xref>). Similarly, the ImmunoRatio algorithm may generate some false alarms, and false negatives for both of the images and the amount of error may change depending on the difference of their color characteristics.</p>
<fig id="pone.0196846.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0196846.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Bland-Altman plot comparing ImmunoRatio values of real and artificial images.</title>
<p>Shaded region corresponds to the limits of agreement.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0196846.g006" xlink:type="simple"/>
</fig>
<fig id="pone.0196846.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0196846.g007</object-id>
<label>Fig 7</label>
<caption>
<title>An example case where the immunoRatio values are different in the real and synthetic images.</title>
<p>The upper left nucleus in the real image (a) was missed by the segmentation result (b) based on [<xref ref-type="bibr" rid="pone.0196846.ref005">5</xref>]. Therefore the synthetic image (c) was not including that nucleus.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0196846.g007" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec009" sec-type="conclusions">
<title>Conclusions</title>
<p>In this study, we proposed a novel method to create realistic synthetic histopathological breast cancer images with Ki67 staining by using conditional Generative Adversarial Networks. The proposed method is different from the prior synthetic tissue generation approaches by producing realistic synthetic images that are hard to distinguish from their real counterparts even by experienced pathologists. For training, two different input methods are evaluated: manual nuclei location annotations and segmentation masks generated by a computer algorithm. We observed that using the segmentation masks provides several advantages over manual annotations. First, it allows defining size, orientation and shape information for each nucleus. Second, unequivocal staining conditions (i.e. a nuclei that cannot be easily labeled as negative or positive) can be simulated with this approach (e.g. yellow color regions in <xref ref-type="fig" rid="pone.0196846.g003">Fig 3b</xref>). Finally, using an existing segmentation algorithm suppress the need of manual annotation during the training.</p>
<p>This study has several practical implications. The artificially created datasets with known ground truth can allow researchers to analyze the accuracy, recall, precision, and intra- and inter-observer variability in a systematic manner and compare the human readers with a computer analysis. The algorithm has the potential to generalize to different types of carcinomas (e.g. neuroendocrine, bladder cancers, etc.) and produce an unlimited number of teaching cases for pathology residents. For instance, we can modify the proposed algorithm to produce different levels of invasion in bladder cancer to train pathology residents in staging bladder cancer pathology. In addition, this approach may help algorithm developers for not only evaluating their methods but also for generating unlimited training and testing samples for algorithm development.</p>
<p>This study also has several practical applications. For example, currently, each laboratory within United States uses locally devised tissue slide preparation and scanning protocols. The study is significant as it has the potential to assist in careful selection of technical parameters that directly affect the tissue slide preparation and its display and also assist in regular checking of scanner performance with measurement of physical image parameters. Both, the technical parameters and the physical parameters have the potential to bring standardization to digital slide preparation process. Moreover, the study can assist in devising new standards to compare the quality of different scanners. Finally, it is worth mentioning that the proposed method can be easily generalized to other stains (such as CD3, CD4, CD8, CD21 etc.) and diseases (e.g., lung, colon, prostate cancer, kidney disease, etc.)</p>
</sec>
</body>
<back>
<ack>
<p>The project described was supported in part by Awards Number R01CA134451 (PIs: Gurcan, Lozanski), U24CA199374 (PI: Gurcan) from the National Cancer Institute. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Cancer Institute, the National Institute of Allergy and Infectious Diseases, or the National Institutes of Health.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0196846.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zaha</surname> <given-names>DC</given-names></name>. <article-title>Significance of immunohistochemistry in breast cancer</article-title>. <source>World journal of clinical oncology</source>. <year>2014</year>;<volume>5</volume>(<issue>3</issue>):<fpage>382</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5306/wjco.v5.i3.382" xlink:type="simple">10.5306/wjco.v5.i3.382</ext-link></comment> <object-id pub-id-type="pmid">25114853</object-id></mixed-citation></ref>
<ref id="pone.0196846.ref002"><label>2</label><mixed-citation publication-type="other" xlink:type="simple">Niazi MKK, Downs-Kelly E, Gurcan MN, editors. Hot spot detection for breast cancer in Ki-67 stained slides: image dependent filtering approach. SPIE Medical Imaging; 2014: International Society for Optics and Photonics.</mixed-citation></ref>
<ref id="pone.0196846.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Di Cataldo</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ficarra</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Acquaviva</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Macii</surname> <given-names>E</given-names></name>. <article-title>Automated segmentation of tissue images for computerized IHC analysis</article-title>. <source>Computer methods and programs in biomedicine</source>. <year>2010</year>;<volume>100</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>15</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cmpb.2010.02.002" xlink:type="simple">10.1016/j.cmpb.2010.02.002</ext-link></comment> <object-id pub-id-type="pmid">20359767</object-id></mixed-citation></ref>
<ref id="pone.0196846.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Taylor</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Levenson</surname> <given-names>RM</given-names></name>. <article-title>Quantification of immunohistochemistry—issues concerning methods, utility and semiquantitative assessment II</article-title>. <source>Histopathology</source>. <year>2006</year>;<volume>49</volume>(<issue>4</issue>):<fpage>411</fpage>–<lpage>24</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1365-2559.2006.02513.x" xlink:type="simple">10.1111/j.1365-2559.2006.02513.x</ext-link></comment> <object-id pub-id-type="pmid">16978205</object-id></mixed-citation></ref>
<ref id="pone.0196846.ref005"><label>5</label><mixed-citation publication-type="other" xlink:type="simple">Niazi MKK, Pennell M, Elkins C, Hemminger J, Jin M, Kirby S, et al., editors. Entropy based quantification of Ki-67 positive cell images and its evaluation by a reader study. SPIE Medical Imaging; 2013: International Society for Optics and Photonics.</mixed-citation></ref>
<ref id="pone.0196846.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Reid</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Bagci</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Ohike</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Saka</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Seven</surname> <given-names>IE</given-names></name>, <name name-style="western"><surname>Dursun</surname> <given-names>N</given-names></name>, <etal>et al</etal>. <article-title>Calculation of the Ki67 index in pancreatic neuroendocrine tumors: a comparative analysis of four counting methodologies</article-title>. <source>Modern pathology: an official journal of the United States and Canadian Academy of Pathology, Inc</source>. <year>2015</year>;<volume>28</volume>(<issue>5</issue>):<fpage>686</fpage>.</mixed-citation></ref>
<ref id="pone.0196846.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fauzi</surname> <given-names>MFA</given-names></name>, <name name-style="western"><surname>Pennell</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sahiner</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Shana’ah</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hemminger</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Classification of follicular lymphoma: the effect of computer aid on pathologists grading</article-title>. <source>BMC medical informatics and decision making</source>. <year>2015</year>;<volume>15</volume>(<issue>1</issue>):<fpage>1</fpage>.</mixed-citation></ref>
<ref id="pone.0196846.ref008"><label>8</label><mixed-citation publication-type="other" xlink:type="simple">Cheikh BB, Bor-Angelier C, Racoceanu D, editors. A model of tumor architecture and spatial interactions with tumor microenvironment in breast carcinoma. SPIE Medical Imaging; 2017: International Society for Optics and Photonics.</mixed-citation></ref>
<ref id="pone.0196846.ref009"><label>9</label><mixed-citation publication-type="other" xlink:type="simple">Niazi K, Abas F, Senaras C, Pennell M, Sahiner B, Chen W, et al. Nuclear IHC enumeration: A digital phantom to evaluate the performance of automated algorithms in digital pathology. (submitted).</mixed-citation></ref>
<ref id="pone.0196846.ref010"><label>10</label><mixed-citation publication-type="other" xlink:type="simple">Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z, editors. Rethinking the inception architecture for computer vision. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition; 2016.</mixed-citation></ref>
<ref id="pone.0196846.ref011"><label>11</label><mixed-citation publication-type="other" xlink:type="simple">Bojarski M, Del Testa D, Dworakowski D, Firner B, Flepp B, Goyal P, et al. End to end learning for self-driving cars. arXiv preprint arXiv:160407316. 2016.</mixed-citation></ref>
<ref id="pone.0196846.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shen</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Suk</surname> <given-names>H-I</given-names></name>. <article-title>Deep Learning in Medical Image Analysis</article-title>. <source>Annual Review of Biomedical Engineering</source>. <year>2017</year>;(<issue>0</issue>).</mixed-citation></ref>
<ref id="pone.0196846.ref013"><label>13</label><mixed-citation publication-type="other" xlink:type="simple">Isola P, Zhu J-Y, Zhou T, Efros AA. Image-to-image translation with conditional adversarial networks. arXiv preprint arXiv:161107004. 2016.</mixed-citation></ref>
<ref id="pone.0196846.ref014"><label>14</label><mixed-citation publication-type="other" xlink:type="simple">Wang X, Gupta A, editors. Generative image modeling using style and structure adversarial networks. European Conference on Computer Vision; 2016: Springer.</mixed-citation></ref>
<ref id="pone.0196846.ref015"><label>15</label><mixed-citation publication-type="other" xlink:type="simple">Ronneberger O, Fischer P, Brox T, editors. U-net: Convolutional networks for biomedical image segmentation. International Conference on Medical Image Computing and Computer-Assisted Intervention; 2015: Springer.</mixed-citation></ref>
<ref id="pone.0196846.ref016"><label>16</label><mixed-citation publication-type="other" xlink:type="simple">Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:150203167. 2015.</mixed-citation></ref>
<ref id="pone.0196846.ref017"><label>17</label><mixed-citation publication-type="other" xlink:type="simple">Maas AL, Hannun AY, Ng AY, editors. Rectifier nonlinearities improve neural network acoustic models. Proc ICML; 2013.</mixed-citation></ref>
<ref id="pone.0196846.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goodfellow</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Pouget-Abadie</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mirza</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Xu</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Warde-Farley</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Ozair</surname> <given-names>S</given-names></name>, <etal>et al</etal>., editors. <article-title>Generative adversarial nets</article-title>. <source>Advances in neural information processing systems</source>; <year>2014</year>.</mixed-citation></ref>
<ref id="pone.0196846.ref019"><label>19</label><mixed-citation publication-type="other" xlink:type="simple">Kingma D, Ba J. Adam: A method for stochastic optimization. arXiv preprint arXiv:14126980. 2014.</mixed-citation></ref>
<ref id="pone.0196846.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tuominen</surname> <given-names>VJ</given-names></name>, <name name-style="western"><surname>Ruotoistenmäki</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Viitanen</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Jumppanen</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Isola</surname> <given-names>J</given-names></name>. <article-title>ImmunoRatio: a publicly available web application for quantitative image analysis of estrogen receptor (ER), progesterone receptor (PR), and Ki-67</article-title>. <source>Breast cancer research</source>. <year>2010</year>;<volume>12</volume>(<issue>4</issue>):<fpage>R56</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/bcr2615" xlink:type="simple">10.1186/bcr2615</ext-link></comment> <object-id pub-id-type="pmid">20663194</object-id></mixed-citation></ref>
<ref id="pone.0196846.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lawrence</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Lin</surname> <given-names>K</given-names></name>. <article-title>A concordance correlation coefficient to evaluate reproducibility</article-title>. <source>Biometrics</source>. <year>1989</year>:<fpage>255</fpage>–<lpage>68</lpage>. <object-id pub-id-type="pmid">2720055</object-id></mixed-citation></ref>
<ref id="pone.0196846.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bland</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Altman</surname> <given-names>D</given-names></name>. <article-title>Statistical methods for assessing agreement between two methods of clinical measurement</article-title>. <source>The lancet</source>. <year>1986</year>;<volume>327</volume>(<issue>8476</issue>):<fpage>307</fpage>–<lpage>10</lpage>.</mixed-citation></ref>
<ref id="pone.0196846.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Niazi</surname> <given-names>MKK</given-names></name>, <name name-style="western"><surname>Lin</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Ashoka</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Marcellin</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Tozbikian</surname> <given-names>G</given-names></name>, <etal>et al</etal>. <source>Interactive Image Compression for Big Data Image Analysis: Application to Hotspot Detection in Breast Cancer. Submitted for Journal publication</source>. <year>2017</year>.</mixed-citation></ref>
</ref-list>
</back>
</article>