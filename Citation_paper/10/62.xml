<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group>
<journal-title>PLoS ONE</journal-title></journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-13-09582</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0068189</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Circuit models</subject></subj-group></subj-group></subj-group><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject><subj-group><subject>Circuit models</subject></subj-group></subj-group><subj-group><subject>Cognitive neuroscience</subject><subject>Learning and memory</subject><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Computer science</subject><subj-group><subject>Computer modeling</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Medicine</subject><subj-group><subject>Mental health</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Modeling Reconsolidation in Kernel Associative Memory</article-title>
<alt-title alt-title-type="running-head">Reconsolidation in Kernel Associative Memory</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Nowicki</surname><given-names>Dimitri</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Verga</surname><given-names>Patrick</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Siegelmann</surname><given-names>Hava</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="aff" rid="aff3"><sup>3</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>The Biologically Inspired Neural and Dynamic Systems (BINDS) Lab, Department of Computer Science, University of Massachusetts Amherst, Amherst, Massachusetts, United States of America</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Institute of Mathematical Machines &amp; Systems Problems of NASU, Center for Cybernetics, Kiev, Ukraine</addr-line></aff>
<aff id="aff3"><label>3</label><addr-line>Program of Neuroscience and Behavior, University of Massachusetts Amherst, Amherst, Massachusetts, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Chacron</surname><given-names>Maurice J.</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>McGill University, Canada</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">nowicki@cs.umass.edu</email> (DN); <email xlink:type="simple">hava@cs.umass.edu</email> (HS)</corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: DN HS PV. Performed the experiments: DN. Analyzed the data: DN PV. Contributed reagents/materials/analysis tools: DN PV HS. Wrote the paper: DN PV HS. Proved the theorems and derived the algorithms: DN.</p></fn>
</author-notes>
<pub-date pub-type="collection"><year>2013</year></pub-date>
<pub-date pub-type="epub"><day>2</day><month>8</month><year>2013</year></pub-date>
<volume>8</volume>
<issue>8</issue>
<elocation-id>e68189</elocation-id>
<history>
<date date-type="received"><day>6</day><month>3</month><year>2013</year></date>
<date date-type="accepted"><day>27</day><month>5</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2013</copyright-year>
<copyright-holder>Nowicki et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Memory reconsolidation is a central process enabling adaptive memory and the perception of a constantly changing reality. It causes memories to be strengthened, weakened or changed following their recall. A computational model of memory reconsolidation is presented. Unlike Hopfield-type memory models, our model introduces an unbounded number of attractors that are updatable and can process real-valued, large, realistic stimuli. Our model replicates three characteristic effects of the reconsolidation process on human memory: increased association, extinction of fear memories, and the ability to track and follow gradually changing objects. In addition to this behavioral validation, a continuous time version of the reconsolidation model is introduced. This version extends average rate dynamic models of brain circuits exhibiting persistent activity to include adaptivity and an unbounded number of attractors.</p>
</abstract>
<funding-group><funding-statement>This research was supported by Office of Naval Research (ONR) grant AWARD # N00014-09-1-0069. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="10"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>Memory reconsolidation (ReC) is a recently proposed process explaining the update of long-term memories in the brain. Upon activation, the memory trace enters a state of lability rendering it subject to alteration and permitting integration of new information before being restabalized, or reconsolidated. “Reconsolidation” coined by Sara in 2000 <xref ref-type="bibr" rid="pone.0068189-Sara1">[1]</xref> has become a widely studied topic in neuroscience. Recent animal and human experiments <xref ref-type="bibr" rid="pone.0068189-Dudai1">[2]</xref>–<xref ref-type="bibr" rid="pone.0068189-Medina1">[7]</xref> have presented overwhelming evidence supporting the existence of ReC and identified boundary conditions that characterize and limit this phenomenon <xref ref-type="bibr" rid="pone.0068189-Antoine1">[8]</xref>. ReC is postulated to strengthen, weaken or extinct memories and update them with new, relevant information. Reconsolidation draws a striking new way of understanding memory and its roles: from a computer-like reliable log, to an adaptive and active part of perception.</p>
<p>Recent experiments have also identified reconsolidation as a possible avenue of treatment for phobias and PTSD by effectively allowing the erasure of fear memories. These memories come about through classical conditioning mechanisms that pair aversive stimuli (unconditioned stimuli – US) with co-occurring, once neutral stimuli (conditioned stimuli – CS). This coupling is the basis for anxiety disorders and PTSD. The most common treatment for fear related disorders is exposure therapy. Exposure therapy leverages extinction learning mechanisms to create a second safety memory that competes with and suppresses the fear response <xref ref-type="bibr" rid="pone.0068189-Golkar1">[9]</xref>, <xref ref-type="bibr" rid="pone.0068189-Barlow1">[10]</xref>. This technique, however, does not fully erase the fear memory, allowing it to spontaneously reappear <xref ref-type="bibr" rid="pone.0068189-Rescorla1">[11]</xref>. Reconsolidation has been demonstrated as a possible method of completely erasing fear associations. In several experiments, fear memories in previously conditioned rats were reactivated, returning the memory traces to labile states. Protein synthesis inhibitors or beta-adrenergic receptor antagonists were then injected into the amygdala, blocking the reconsolidation process. This process resulted in extinction of fear and was not subject to spontaneous recovery <xref ref-type="bibr" rid="pone.0068189-Dudai2">[4]</xref>, <xref ref-type="bibr" rid="pone.0068189-Nader1">[5]</xref>, <xref ref-type="bibr" rid="pone.0068189-Nader2">[12]</xref>, <xref ref-type="bibr" rid="pone.0068189-Alberini1">[13]</xref>. Cases of reconsolidation of fear memories have also been demonstrated in humans. In these experiments, subjects were exposed to stimuli, which reactivated the fear memory trace rendering it labile. Rather than pharmacological intervention, the normal reconsolidation process was disrupted with competing information which resulted in the memory being updated <xref ref-type="bibr" rid="pone.0068189-Schiller1">[14]</xref>, <xref ref-type="bibr" rid="pone.0068189-Agren1">[15]</xref>.</p>
<p>We propose an adaptive memory model that is consistent with recent findings in ReC. The framework introduces efficient ways to add, remove, and update attractors. Additionally, memories can be strengthened, weakened, or extinguished by controlling the attractor radius.</p>
<p>Our memory model builds on an earlier Kernel Associative Memory (KAM) model <xref ref-type="bibr" rid="pone.0068189-Nowicki1">[16]</xref>, <xref ref-type="bibr" rid="pone.0068189-Nowicki2">[17]</xref> that uses a kernel structure to efficiently compute attractor dynamics. The KAM model is an extension of the attractor based Hopfield network. It has been shown that attractor mechanisms are employed by the brain, notably in the CA3 region of the hippocampus <xref ref-type="bibr" rid="pone.0068189-Wills1">[18]</xref>. The KAM has several advantages over previous Hopfield models including the number of attractors unbounded and independent of the input dimension, dynamic rewiring of neurons, and the ability to accommodate large real-valued inputs and attractors. This paper derives a ReC algorithm that allows KAM to hold an unbounded number of now flexible attractors, which we call ReKAM. Our approach to the modeling of reconsolidation is based on the principle of robust global update, analogous to psychological findings such as the gang effect where the update of one attractor affects neighboring attractors <xref ref-type="bibr" rid="pone.0068189-McClelland1">[19]</xref>. We also introduce an approximate ReC algorithm which changes the global updates to local ones, gaining time efficiency at the cost of precision.</p>
<p>The relevance of our ReKAM model is demonstrated by replicating three recently found characteristics of ReC seen in human behavioral experiments. First, ReKAM imitates a recent list-learning experiment in which human participants merged new objects into a previously learned list during retrieval. ReKAM also demonstrates fear extinction via the controllable attractor radius. The third experiment follows gradually changing objects resulting in an evolved representation. Finally, a continuous time version of ReKAM is introduced which relates the model to neurobiological studies. This version extends the capabilities of the continuous-time Hopfield network <xref ref-type="bibr" rid="pone.0068189-Hopfield1">[20]</xref> commonly used to model average firing rate dynamics <xref ref-type="bibr" rid="pone.0068189-Hopfield2">[21]</xref>, <xref ref-type="bibr" rid="pone.0068189-Maass1">[22]</xref> of adaptive persistent activity.</p>
<sec id="s1a">
<title>Previous Reconsolidation Models</title>
<p>Reconsolidation's significance in explaining the dynamic properties of healthy memory has led to several mathematical models proposing to explain the process. The first ReC model <xref ref-type="bibr" rid="pone.0068189-Blumenfeld1">[23]</xref> extended the Hopfield model to allow attractors to evolve through weight decay and Hamming-distance terms. Our ReKAM also allows attractors to evolve, but since our attractors lie in high dimensional space, the number of memories is unbounded and inputs are realistic, thus modeling reconsolidation in a more relevant and technologically practical way.</p>
<p>The second ReC model to be introduced, called Reconsolidation Attractor Network (RAN) <xref ref-type="bibr" rid="pone.0068189-Siegelmann1">[24]</xref>, takes the approach that attractors do not have to lie in input space and hence an unbounded number of memories are possible. The architecture of the RAN is layered. Attractors appear in the upper level separate from the neural flow and input space. Our ReKAM builds on the same concept of attractors not lying in input space, but it also draws from Hopfield-like networks for mathematical completeness of attractor dynamics.</p>
<p>The third model presented in <xref ref-type="bibr" rid="pone.0068189-Osan1">[25]</xref> is designed to reproduce extinction of fear memories. Like the first model, it is also based on the classical Hopfield network. Attractors can be extinct when an additional binary variable which represents the anisomycin (consolidation-inhibiting) drug is set to 0. Our ReKAM is the only memory model demonstrating all known ReC properties as opposed to a particular architecture demonstrating only one facet of the ReC process; it is also the only one that describes reconsolidation of large memories with real world stimuli.</p>
</sec><sec id="s1b">
<title>Modeling with Kernels</title>
<p>Our ReKAM model is based on our KAM architecture <xref ref-type="bibr" rid="pone.0068189-Nowicki2">[17]</xref>. Kernel representations were introduced by Vladimir Vapnik to the field of Machine Learning when he showed how to transfer input data to a high-dimensional data space called <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e001" xlink:type="simple"/></inline-formula>-space (phi-space). The data is classified in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e002" xlink:type="simple"/></inline-formula>-space and then projected back to the original space resulting in the most efficient, optimal, non-linear separation. This is achieved by using the kernel property: a scalar kernel function applied to two inputs is equal to their product in the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e003" xlink:type="simple"/></inline-formula>-space: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e004" xlink:type="simple"/></inline-formula>. This kernel property is the basis of Support Vector Machines (SVM), regarded as the most efficient supervised classifiers <xref ref-type="bibr" rid="pone.0068189-Vapnik1">[26]</xref>.</p>
<p>Support Vector Clustering (SVC) was introduced in a joint work by the third author's research group and Vapnik. SVC is an unsupervised extension of SVM (for the case when labels are not available) that groups data into clusters through kernel functions that mimic high-dimensional organization and projections <xref ref-type="bibr" rid="pone.0068189-BenHur1">[27]</xref>.</p>
<p>In Kernel Associative Memory, we follow similar mathematics. However, here the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e005" xlink:type="simple"/></inline-formula>-space is not abstract. Instead, it is based on the output of multiple neurons. Mathematically, Mercer kernels are no longer sufficient. We define the strong Mercer kernels that provide the condition needed to load an unbounded number of attractors (See Materials and Methods 4.4). The use of both low-level and high-level spaces is an effective mathematical way to describe both the synaptic changes of neurobiological memories as well as the behavioral effects of cognitive memories.</p>
</sec><sec id="s1c">
<title>Model for Reconsolidation based on KAM</title>
<p>The practical advantages of our ReKAM model include an input space that can be composed of continuous valued vectors rather than binary ones, a number of attractors that is independent of the input dimension, and a variable input length where longer and shorter input vectors are learned with no a priori bound. Furthermore, attractors are efficiently loaded, deleted, and updated.</p>
<p>We briefly describe the KAM which is the basis of our ReKAM model (a complete description is given in <xref ref-type="bibr" rid="pone.0068189-Nowicki2">[17]</xref>. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e006" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e007" xlink:type="simple"/></inline-formula> be matrices whose columns represent the input and output space of the memories. Memories are defined by the transformation on these columns through the projective operator. We transfer the input to the higher <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e008" xlink:type="simple"/></inline-formula> space (as explained in previous sectionf), so that the current transformation is now : <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e009" xlink:type="simple"/></inline-formula>. A connection matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e010" xlink:type="simple"/></inline-formula> is defined as:<disp-formula id="pone.0068189.e011"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e011" xlink:type="simple"/><label>(1)</label></disp-formula></p>
<p>Memory loading is defined by<disp-formula id="pone.0068189.e012"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e012" xlink:type="simple"/><label>(2)</label></disp-formula>and recall of input <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e013" xlink:type="simple"/></inline-formula> by the iterations:<disp-formula id="pone.0068189.e014"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e014" xlink:type="simple"/><label>(3)</label></disp-formula>where the first iteration is initialized with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e015" xlink:type="simple"/></inline-formula>, each iteration ends with applying any sigmoid-like activation (bounded monotonically increasing) function coordinate-wise to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e016" xlink:type="simple"/></inline-formula>: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e017" xlink:type="simple"/></inline-formula>, and the iterations stop when update is under a chosen threshold. The KAM can be depicted as a neural network, as explained in <xref ref-type="bibr" rid="pone.0068189-Nowicki2">[17]</xref>.</p>
</sec></sec><sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Model for Reconsolidation and Extinction: ReKAM</title>
<p>Unlike the traditional Hopfield networks, where attractors lie in input space, our ReKAM's attractors (stemming from the KAM architecture, see last subsection in Previous Work) lie in a high dimensional manifold. While a Hebbian networks' (e.g., <xref ref-type="bibr" rid="pone.0068189-Blumenfeld1">[23]</xref>) synaptic matrices compose a linear space, our use of the efficient pseudo inverse learning method gives rise to Riemannian manifolds in the attractor space. An unbounded number of attractors can exist in the higher dimensional space. Between every two points in a Riemannian manifold there exists at least one geodesic that has a minimal length of all curves joining the two points. The geodesic is analogous to the shortest straight line between two points but in a nonlinear space. Updating an attractor toward a new input is calculated along a geodesic between the new input and the given attractor it recalled. Our ReC algorithm with this manifold makes the memory update global, and capable of representing psychological properties such as the gang effect. This global update is more expensive, although more accurate, and we provide another local algorithm which is faster and just a bit less general. Comparisons between the architectures are provided both for time analysis (in this section below) and in the result “Updating Memories Incrementally”.</p>
<sec id="s2a1">
<title>The global geodesic ReC Algorithm</title>
<p>We propose a memory update algorithm that assumes that every ReC update has a global effect. Mathematically it is based on geodesic computation in the Reimannian manifold representing the memory attractors. The metric structure of this manifold and a comparison with the special case of the Grassmann manifold are derived in Materials and Methods (4.1–4.3).</p>
<p>Suppose that we have an initial memory <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e018" xlink:type="simple"/></inline-formula> that contains <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e019" xlink:type="simple"/></inline-formula> patterns (concepts) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e020" xlink:type="simple"/></inline-formula>. We then obtain <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e021" xlink:type="simple"/></inline-formula> by replacing one of the attractor patterns <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e022" xlink:type="simple"/></inline-formula> with a new stimulus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e023" xlink:type="simple"/></inline-formula> that recalls it. The distance between <bold>X<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e024" xlink:type="simple"/></inline-formula></bold> and <bold>X<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e025" xlink:type="simple"/></inline-formula></bold> can be interpreted as a measure of the amount of “surprise” that the memory experiences when it meets a new stimuli. To track these changes, we build a geodesic <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e026" xlink:type="simple"/></inline-formula> joining <bold>X<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e027" xlink:type="simple"/></inline-formula></bold> and <bold>X<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e028" xlink:type="simple"/></inline-formula></bold> on the manifold and take a new point <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e029" xlink:type="simple"/></inline-formula>. Here <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e030" xlink:type="simple"/></inline-formula> is a step parameter related to the size of a shift during each update. When <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e031" xlink:type="simple"/></inline-formula>, the memory remains at <bold>X<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e032" xlink:type="simple"/></inline-formula></bold>, when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e033" xlink:type="simple"/></inline-formula> = 1, the memory is changed to <bold>X<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e034" xlink:type="simple"/></inline-formula></bold>.</p>
<p>Using the same process, when a stimulus <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e035" xlink:type="simple"/></inline-formula> appears we can track the change from <bold>X<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e036" xlink:type="simple"/></inline-formula></bold> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e037" xlink:type="simple"/></inline-formula>. The process the continues for future stimuli. The algorithm of memory update using geodesics is shown in <xref ref-type="fig" rid="pone-0068189-g001">Fig. 1</xref>. The exact geodesic calculation is described in the Materials and Methods. Its complexity depends on the optimization algorithm used. The dimension of our manifold is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e038" xlink:type="simple"/></inline-formula>. A “typical” gradient calculation would require <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e039" xlink:type="simple"/></inline-formula> operations. The gradient-like minimum search calculation has complexity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e040" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e041" xlink:type="simple"/></inline-formula> is the required tolerance <xref ref-type="bibr" rid="pone.0068189-Nocedal1">[28]</xref>. This leads to complexity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e042" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pone.0068189-Udriste1">[29]</xref>. However, with derivation-free optimization techniques which do not require explicit gradient calculations, we can reduce this complexity estimation to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e043" xlink:type="simple"/></inline-formula>.</p>
<fig id="pone-0068189-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0068189.g001</object-id><label>Figure 1</label><caption>
<title>Algorithm of Geodesic Update.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0068189.g001" position="float" xlink:type="simple"/></fig></sec><sec id="s2a2">
<title>The Local Approximate ReC Algorithm</title>
<p>The exact computation of geodesics may be resource consuming especially for high dimensional data. Here we develop a simplified ReC algorithm with local rather than global updates to attractors. In this linear approximation, we simply replace the geodesic with a straight line in the coordinate space.</p>
<p>This leads to the Approximate-Update algorithm in <xref ref-type="fig" rid="pone-0068189-g002">Fig. 2</xref>. The approximation algorithm's complexity is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e044" xlink:type="simple"/></inline-formula>, equivalent to the derivative-free version of the geodesic algorithm. However, the approximation algorithm is much easier and faster to implement. Because it requires only a few operations per element, the complexity does not depend on the tolerance.</p>
<fig id="pone-0068189-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0068189.g002</object-id><label>Figure 2</label><caption>
<title>Algorithm of Approximate Update.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0068189.g002" position="float" xlink:type="simple"/></fig>
<p>While the approximate algorithm of <xref ref-type="fig" rid="pone-0068189-g002">Fig. 2</xref> shows only one update per reconsolidation, we can easily construct a version of this algorithm that updates any desired number of attractors. For this, we repeat step 3 for the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e045" xlink:type="simple"/></inline-formula> most relevant attractors with the largest values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e046" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e047" xlink:type="simple"/></inline-formula>, and use the value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e048" xlink:type="simple"/></inline-formula> for the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e049" xlink:type="simple"/></inline-formula>-th attractor. This version of the algorithm demonstrates gang effect properties by updating neighboring attractors.</p>
<p>The approximation error is bounded by the following theorem:</p>
<p><bold>Theorem 1</bold> <italic>Let</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e050" xlink:type="simple"/></inline-formula>. Denote <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e051" xlink:type="simple"/></inline-formula> <italic>as the solution given by the geodesic algorithm and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e052" xlink:type="simple"/></inline-formula> as the solution given by the approximate algorithm. There then exists a constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e053" xlink:type="simple"/></inline-formula> such that</italic><disp-formula id="pone.0068189.e054"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e054" xlink:type="simple"/></disp-formula></p>
<p><bold>Proof:</bold> Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e055" xlink:type="simple"/></inline-formula> be a metric tensor on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e056" xlink:type="simple"/></inline-formula> dependent on coordinates and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e057" xlink:type="simple"/></inline-formula>. The straight line <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e058" xlink:type="simple"/></inline-formula> between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e059" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e060" xlink:type="simple"/></inline-formula> is a geodesic in the flat space with a constant metric form <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e061" xlink:type="simple"/></inline-formula>. Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e062" xlink:type="simple"/></inline-formula> is a twice differentiable manifold, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e063" xlink:type="simple"/></inline-formula> along the geodesic <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e064" xlink:type="simple"/></inline-formula> that lies between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e065" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e066" xlink:type="simple"/></inline-formula>. Denote <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e067" xlink:type="simple"/></inline-formula> as the distance between the starting point <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e068" xlink:type="simple"/></inline-formula> and a given point <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e069" xlink:type="simple"/></inline-formula> along the (geodesic) curve. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e070" xlink:type="simple"/></inline-formula> is called the arc length (see remark 1 below, <xref ref-type="bibr" rid="pone.0068189-doCarmo1">[30]</xref>, or other textbook on Riemannian and differential geometry). Because <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e071" xlink:type="simple"/></inline-formula> is a secant of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e072" xlink:type="simple"/></inline-formula> curve <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e073" xlink:type="simple"/></inline-formula> in the coordinate space, when <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e074" xlink:type="simple"/></inline-formula>, there exists a constant <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e075" xlink:type="simple"/></inline-formula> such that for the given arc length <italic>s</italic>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e076" xlink:type="simple"/></inline-formula></p>
<p><bold>Remark 1</bold> <italic>Arc length could also be defined as a parameterization of a curve <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e077" xlink:type="simple"/></inline-formula> such that</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e078" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s2a3">
<title>Controllable Attraction Radius</title>
<p>As part of the ReKAM architecture we include a mechanism for altering the size of an attractor's basin of attraction. This affects the probability of recalling an attractor. As the attraction radius goes to zero, the attractor will never be recalled. This is analogous to extinction.</p>
<p><bold>Definition 1</bold> <italic>A Kernel</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e079" xlink:type="simple"/></inline-formula> <italic>is called </italic><bold><italic>uniform</italic></bold><italic> if it depends only on the difference</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e080" xlink:type="simple"/></inline-formula></p>
<p>If the kernel network has a uniform kernel, Then the attraction radius can be controlled. Assign the <italic>scaling factor</italic> <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e081" xlink:type="simple"/></inline-formula> to the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e082" xlink:type="simple"/></inline-formula>-th attractor. We can then divide the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e083" xlink:type="simple"/></inline-formula>-th entry of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e084" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e085" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e086" xlink:type="simple"/></inline-formula> is the temporary vector used in the recall algorithm of ReKAM.<disp-formula id="pone.0068189.e087"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e087" xlink:type="simple"/><label>(4)</label></disp-formula></p>
<p>This causes the attraction basin to be scaled by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e088" xlink:type="simple"/></inline-formula>.</p>
</sec></sec><sec id="s2b">
<title>Model Verification with Human Experiments</title>
<p>We verified our model's ability to describe reconsolidation by comparing the dynamics of our model to those observed in humans. The first experiment simulates the effect of reconsolidation on episodic memories. The second demonstrates the model's capability to replicate extinction. The third follows memory changes created by the gradual altering of the associated input.</p>
<sec id="s2b1">
<title>List Learning</title>
<p>We first replicate a human experiment investigating reconsolidation of episodic memories <xref ref-type="bibr" rid="pone.0068189-Hupbach1">[31]</xref>. In the original experiment, participants were split into two groups (A and B). On Day 1, both groups learned a list of 20 objects (List 1) that were associated with a blue basket. On Day 2, Both groups learned a second list of 20 items (List 2). Before learning, group A received a reminder of List 1 in the form of the blue basket; group B did not receive any reminder. On Day 3 both groups were tested on their ability to retrieve List 1. Group A made more errors confusing List 2 items into List 1 than Group B did (<xref ref-type="fig" rid="pone-0068189-g003">Fig. 3</xref>). When the experiment was repeated to test recall of List 2, both groups performed equally well.</p>
<fig id="pone-0068189-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0068189.g003</object-id><label>Figure 3</label><caption>
<title>Results of the original list learning human experiment <xref ref-type="bibr" rid="pone.0068189-Hupbach1">[31]</xref>.</title>
<p>Group A received a reminder cue before learning List 2. This resulted in the List 1 memory becoming labile and updated by integrating some of the new items from List 2. Group B did not receive this reminder and these intrusions were not seen. For our analysis, results were normalized for each group by dividing the number of items recalled per list by the total number of items recalled in both lists together.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0068189.g003" position="float" xlink:type="simple"/></fig>
<p>In our simulation, all objects were shown as images, rescaled to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e089" xlink:type="simple"/></inline-formula> pixels. Note that the ability of the model to handle large colored images is already beyond the standard Hopfield model used in previous work. Images were represented as real-valued vectors with components <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e090" xlink:type="simple"/></inline-formula>. We added an indicator variable <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e091" xlink:type="simple"/></inline-formula> to each item, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e092" xlink:type="simple"/></inline-formula> where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e093" xlink:type="simple"/></inline-formula> denotes that the object is unrelated to the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e094" xlink:type="simple"/></inline-formula>-th list, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e095" xlink:type="simple"/></inline-formula> means that the object belongs to this list with 100% certainty.</p>
<p>For computational efficiency, we took the variant of the Gaussian kernel:<disp-formula id="pone.0068189.e096"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e096" xlink:type="simple"/><label>(5)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e097" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e098" xlink:type="simple"/></inline-formula> are tuned to balance between the data vector and the list indicator components.</p>
<p>In our simulation we modeled the two groups. For each group we created 40 initial attractors corresponding to the items in both lists. In group A we gradually shifted the value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e099" xlink:type="simple"/></inline-formula> of each item towards 1 when this item was recalled with the blue basket reminder in the background to simulate the effects of reconsolidation. In group B, these updates were not performed. For both groups we tested the memory in recall mode inputting 1000 new vectors per list by taking the attractor and adding uncorrelated white noise (intensity equaled 10% of data STD). For all query vectors, we set the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e100" xlink:type="simple"/></inline-formula> as the initial value.</p>
<p>Using our model, we found an exact correspondence between our simulation and the human experiment for values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e101" xlink:type="simple"/></inline-formula> for Group A and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e102" xlink:type="simple"/></inline-formula> for Group B (see <xref ref-type="table" rid="pone-0068189-t001">Table 1</xref>).</p>
<table-wrap id="pone-0068189-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0068189.t001</object-id><label>Table 1</label><caption>
<title>Comparing the results of actual human experiment to our model's simulation.</title>
</caption><alternatives><graphic id="pone-0068189-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0068189.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td colspan="3" align="left" rowspan="1">Human Results</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Group</td>
<td align="left" rowspan="1" colspan="1">A</td>
<td align="left" rowspan="1" colspan="1">B</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">List 1 %</td>
<td align="left" rowspan="1" colspan="1">60.399</td>
<td align="left" rowspan="1" colspan="1">90.180</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">List 2 %</td>
<td align="left" rowspan="1" colspan="1">39.600</td>
<td align="left" rowspan="1" colspan="1">9.819</td>
</tr>
</tbody>
</table>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td colspan="3" align="left" rowspan="1">Simulation Results</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>x</italic><sub>01</sub></td>
<td align="left" rowspan="1" colspan="1">0.75</td>
<td align="left" rowspan="1" colspan="1">0.25</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">List 1 %</td>
<td align="left" rowspan="1" colspan="1">60.287</td>
<td align="left" rowspan="1" colspan="1">90.105</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">List 2 %</td>
<td align="left" rowspan="1" colspan="1">39.713</td>
<td align="left" rowspan="1" colspan="1">9.895</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt101"><label/><p>By normalizing the data by the total number of items recalled over both lists, our simulation matched exactly with Group A and Group B of the human experiment.</p></fn></table-wrap-foot></table-wrap>
<p>We next simulated more values of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e103" xlink:type="simple"/></inline-formula> which could arise for varying levels of reconsolidation due to differing experimental procedures, memory type, etc. (<xref ref-type="fig" rid="pone-0068189-g004">Fig. 4</xref>).</p>
<fig id="pone-0068189-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0068189.g004</object-id><label>Figure 4</label><caption>
<title>Results of our models simulation of reconsolidation on the list learning experiment for different values of the reconsolidated parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e104" xlink:type="simple"/></inline-formula>.</title>
<p><bold><inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e105" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e106" xlink:type="simple"/></inline-formula> exactly matched the results of Group A and Group B of the human experiment.</bold></p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0068189.g004" position="float" xlink:type="simple"/></fig></sec><sec id="s2b2">
<title>Extinction</title>
<p>Many recent experiments have demonstrated the effects of fear extinction in both humans and animals – e.g. <xref ref-type="bibr" rid="pone.0068189-Monfils1">[32]</xref>, <xref ref-type="bibr" rid="pone.0068189-Suzuki1">[33]</xref>, <xref ref-type="bibr" rid="pone.0068189-Pedreira1">[34]</xref>. Numerical simulations with Hopfield memory and Hebbian-like learning were presented in <xref ref-type="bibr" rid="pone.0068189-Osan1">[25]</xref>. Our model has a far larger number of far more detailed memories than previously modeled.</p>
<p>We propose to model extinction as a reduction in the attractor's radius. To demonstrate, we created a kernel network that memorized 10 images. All images were scaled to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e107" xlink:type="simple"/></inline-formula> pixels. One of the images was randomly chosen to be a “fear” (shock) memory. In our procedure, the scaling factor for the “shock” attractor was gradually decreased. This process is analogous to the weakening of the memory occurring through reconsolidation during extinction training. For each scaling factor value we measured the frequency of retrieval (recall) of the shock memory on 1000 random inputs (<xref ref-type="fig" rid="pone-0068189-g005">Fig. 5</xref>). The decreasing attraction basin radius effectively extinguishes the fear memory trace as its probability of recall goes virtually to 0.</p>
<fig id="pone-0068189-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0068189.g005</object-id><label>Figure 5</label><caption>
<title>Results of extinction experiment showing the probability of shock memory retrieval as the attraction radius multiplier is varied.</title>
<p>Each point represents the average of 1000 random input trials as well as their standard deviation. Note that the scale of the x-axis varies.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0068189.g005" position="float" xlink:type="simple"/></fig></sec><sec id="s2b3">
<title>Updating Memories Incrementally</title>
<p>In an experiment testing the incremental changes of gradually morphing memories <xref ref-type="bibr" rid="pone.0068189-Preminger1">[35]</xref>, participants learned to recognize four faces as “friends.” One face was morphed incrementally over a period of days. When the face morphed slowly, participants continually recognized the morphed face as their original friend. By the end of the process, the morphed face was recognized as a friend while the original face was not. The results demonstrated merging of the source and the new face. However, this effect was only observed when the faces were changed gradually, demonstrating that the order in which morphing took place was crucial. A gradual, subtle change was needed to allow for reconsolidation to occur.</p>
<p>In our previous work <xref ref-type="bibr" rid="pone.0068189-Nowicki2">[17]</xref> we published a numerical experiment with morphing face images that replicated the previous result described above. Attractors in the KAM were gradually morphed following the slowly changing face inputs.</p>
<p>Here we present a similar experiment aimed at examining the network's ability to track images varying gradually over time. Additionally, we compare the performance of the exact and approximate ReC algorithms for this manipulation. We created a training set consisting of 9,000 rotated digits. The rotated digits were created from 100 original MNIST handwritten digits (10 per class from ‘0’ to ‘9’). Digits were <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e108" xlink:type="simple"/></inline-formula> pixel grayscale images which we rotated counterclockwise from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e109" xlink:type="simple"/></inline-formula> to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e110" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pone-0068189-g006">Fig. 6</xref>).</p>
<fig id="pone-0068189-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0068189.g006</object-id><label>Figure 6</label><caption>
<title>Example of rotated digit inputs (A) and corresponding attractors (B) during reconsolidation.</title>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0068189.g006" position="float" xlink:type="simple"/></fig>
<p>We applied principal-component (PC) preprocessing without considering any specific handwritten digit optimized feature extraction techniques. We took the first <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e111" xlink:type="simple"/></inline-formula> PCs which contain 96.77% of the variance. For computational efficiency, the kernel we chose was:<disp-formula id="pone.0068189.e112"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e112" xlink:type="simple"/><label>(6)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e113" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e114" xlink:type="simple"/></inline-formula> is a bias parameter. This is a Gaussian kernel dependent on a weighting metric. The weights were chosen as:<disp-formula id="pone.0068189.e115"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e115" xlink:type="simple"/><label>(7)</label></disp-formula>We also tried:<disp-formula id="pone.0068189.e116"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e116" xlink:type="simple"/><label>(8)</label></disp-formula>Where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e117" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e118" xlink:type="simple"/></inline-formula> are the expectation and standard deviation over the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e119" xlink:type="simple"/></inline-formula>-th class, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e120" xlink:type="simple"/></inline-formula> is the number of classes. Formula (7) yielded better results.</p>
<p>Evolution of the classification rate over time for the digit rotation experiment is shown in <xref ref-type="fig" rid="pone-0068189-g007">Fig. 7</xref> with confusion matrices in <xref ref-type="fig" rid="pone-0068189-g007">Fig. 7</xref>. The exact reconsolidation algorithm achieved a recognition accuracy of 96.4+/−0.43%. Results for the local approximate algorithm were 96.32+/−0.26%. The algorithm without reconsolidation performed significantly worse (see <xref ref-type="fig" rid="pone-0068189-g007">Fig. 7</xref>). The CPU time was 142 sec for the approximate algorithm and 54.7 min for the exact geodesic reconsolidation on Intel Centrino Duo 1.4 GHz CPU with no parallelism, in the Matlab environment. The average relative error in attractors was <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e121" xlink:type="simple"/></inline-formula>.</p>
<fig id="pone-0068189-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0068189.g007</object-id><label>Figure 7</label><caption>
<title>Results of the digit rotation experiment</title>
<p>. The solid lines show the average accuracy over all classes for rotations between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e122" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e123" xlink:type="simple"/></inline-formula>. The blue line shows the accuracy while using reconsolidation. The red line shows the corresponding accuracy while not reconsolidating. The dotted lines surrounding the solid lines refer to the standard deviations.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0068189.g007" position="float" xlink:type="simple"/></fig>
<p>When inputs were shuffled randomly, gradual reconsolidation was unable to occur. We note that because we are testing on a handwritten digit dataset, there are variations between each test digit: while an ideal number 6 rotated <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e124" xlink:type="simple"/></inline-formula> would be equal to an ideal number 9, this will very rarely be the case with the variable hand written digits. Results of no reconsolidation for digits rotated at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e125" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pone-0068189-g008">Fig. 8</xref>–C) shows that digits such as 0, 1, and 8 remain mislabeled while, to a human eye, these would seem the same. It is possible that using a preprocessing technique specifically designed for hand written digit recognition may allow the system to generalize these specific cases to a greater degree. However, even under these conditions, the reconsolidation algorithm works effectively and allows for accurate classification under constantly changing inputs.</p>
<fig id="pone-0068189-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0068189.g008</object-id><label>Figure 8</label><caption>
<title>Confusion matrices for rotating digit experiment showing predicted labels vs true labels where a correct classification appears along the diagonal.</title>
<p>A, B, and C show reconsolidation with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e143" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e144" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e145" xlink:type="simple"/></inline-formula> rotations. D, E, and F show results without reconsolidation for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e146" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e147" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e148" xlink:type="simple"/></inline-formula> rotations. Warmer (more red) colors refer to higher number values. The reconsolidation algorithm provides stable clustering even under changing conditions (presented as rotation in this experiment). Without reconsolidation, the memories are unable to track these changes.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0068189.g008" position="float" xlink:type="simple"/></fig></sec></sec><sec id="s2c">
<title>Continuous-time ReKAM Models Firing-Rate Dynamics</title>
<p>Up to here, we described the discrete-time form of associative recall. We next relate the ReKAM to biology by introducing a continuous-time version of the kernel memory and comparing it to other firing-rate models. It is important to note that the nature of the time (discrete or continuous) is involved only in dynamical systems of recall, not in the reconsolidation phase. Any step of the reconsolidation (both exact and approximate) depends only on the input and the attractors, not on the time. For this reason both the exact and approximate algorithms of ReC work in continuous time.</p>
<p>The Hopfield equation for the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e126" xlink:type="simple"/></inline-formula>-th neuron is:<disp-formula id="pone.0068189.e127"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e127" xlink:type="simple"/><label>(9)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e128" xlink:type="simple"/></inline-formula> is the output of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e129" xlink:type="simple"/></inline-formula>-th neuron, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e130" xlink:type="simple"/></inline-formula> are the elements of the symmetric synaptic matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e131" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e132" xlink:type="simple"/></inline-formula> are direct external inputs, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e133" xlink:type="simple"/></inline-formula> is the activation function, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e134" xlink:type="simple"/></inline-formula> is the “relaxation rate” of the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e135" xlink:type="simple"/></inline-formula>-th neuron. The Hopfield <xref ref-type="disp-formula" rid="pone.0068189.e127">equation (9</xref>) imposes linear and symmetric neuron-to-neuron interactions in the network which can be described by the synaptic matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e136" xlink:type="simple"/></inline-formula>. Escalating the model from discrete neurons to neural field (mean field) gives rise to the Wilson-Cowan partial integro-differential equation <xref ref-type="bibr" rid="pone.0068189-Wilson1">[36]</xref>:</p>
<p><disp-formula id="pone.0068189.e137"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e137" xlink:type="simple"/><label>(10)</label></disp-formula>If the activation function is simply the Heaviside step function, <xref ref-type="disp-formula" rid="pone.0068189.e137">equation (10</xref>) becomes the Amari field equation <xref ref-type="bibr" rid="pone.0068189-Amari1">[37]</xref>.</p>
<p>If the network's activity is a Markov stochastic process (with a vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e138" xlink:type="simple"/></inline-formula>), then the first-order approximation of the average firing rate dynamics is (see <xref ref-type="bibr" rid="pone.0068189-Buice1">[38]</xref>):<disp-formula id="pone.0068189.e139"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e139" xlink:type="simple"/><label>(11)</label></disp-formula></p>
<p>This equation can have arbitrary dynamics in contrast to (9) that has a Lyapunov function and converges to attractors.</p>
<p>We propose a continuous-time version of the Kernel Associative Memory that updates the recall similar to (11):<disp-formula id="pone.0068189.e140"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e140" xlink:type="simple"/><label>(12)</label></disp-formula>where the components of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e141" xlink:type="simple"/></inline-formula> are:</p>
<p><disp-formula id="pone.0068189.e142"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e142" xlink:type="simple"/><label>(13)</label></disp-formula>The continuous ReKAM described by <xref ref-type="disp-formula" rid="pone.0068189.e140">Equations (12</xref>) – (13) with a scalar-product kernel is isomorphic to (9) except for having the synaptic matrix W calculated by the pseudoinverse rule (not the Hebbian rule), or, equivalently, orthogonal Hopfield learning. This continuous memory inherits the Hopfield-like attractor dynamics but is more biologically relevant: the number of attractors is independent of the input dimension and rewiring of the neurons is dynamic. We propose the continuous ReKAM as a model for firing rate adaptive dynamics in the course of persistent activity in various networks in the brain.</p>
</sec></sec><sec id="s3">
<title>Discussion</title>
<p>While the existence of reconsolidation in human memory was once a topic of debate, the accumulation of human experimental results has led to the mechanism becoming widely accepted in the field of neuroscience. Reconsolidation has been dissociated from extinction learning, the latter of which results in a second memory trace rather than the removal of the old one <xref ref-type="bibr" rid="pone.0068189-Besnard1">[39]</xref>, <xref ref-type="bibr" rid="pone.0068189-Besnard2">[40]</xref>. However, it is not yet entirely clear when or to what extent reconsolidation mechanisms will occur in a given situation. Experimental results have identified numerous boundary conditions involved in determining whether or not a memory will undergo reconsolidation <xref ref-type="bibr" rid="pone.0068189-Besnard1">[39]</xref>, <xref ref-type="bibr" rid="pone.0068189-Besnard2">[40]</xref>.</p>
<p>One such boundary condition is the amount of time between a memory's retrieval and the encountering of relevant stimuli. This time window varies depending on the animal tested <xref ref-type="bibr" rid="pone.0068189-Tronson1">[41]</xref> and in humans begins about 10 minutes after retrieval and lasts for several hours. During this time, the memory is labile and susceptible to new information or experimental interference. If the stimulus is encountered outside of this time window, reconsolidation will not occur. A second boundary condition is the age and strength of the memory trace, affecting the ease in which the memory will undergo reconsolidation. A stronger or older memory may require longer and more frequent reactivation sessions for reconsolidation to occur. A third condition, the predictability of reactivation stimulus, also plays a role in whether or not reconsolidation will occur. If a subject does not correctly predict a novel response to a stimulus, reconsolidation is more likely to occur in order to update an incorrect prediction model <xref ref-type="bibr" rid="pone.0068189-Sevenster1">[42]</xref>. Another boundary condition is the “trace dominance” – when a memory stabilizes and becomes resistant to reconsolidation and certain amnesic agents.</p>
<p>It would be possible to extend our model in the future to include these observed boundary conditions. The addition of variables that account for time elapsed since retrieval, age of memory, and strength of memory could be implemented to allow for an accurate simulation of the boundary conditions that accompany reconsolidation. Additionally, a mechanism to account for prediction error would allow for a representation of the novelty prediction that has been shown to influence whether or not reconsolidation will occur. These additions could allow for a more accurate simulation of reconsolidation as well as a more biological learning model.</p>
<p>We have proposed a mathematical framework of memory reconsolidation, which demonstrates properties as seen in human studies: incremental updates, associations, and extinction. Our ReKAM memory model is far more technologically relevant than previous ones in that it is able to include real-valued inputs as well as massively long inputs; the number of memories is independent of input dimension and hence is practically unbounded. This results in a model providing both a better functional understanding of reconsolidation and the basis for a powerful technology for following changes in real world environments.</p>
<p>The mathematical structure has its own beauty: The kernel associative memory has an underlying structure of a Grassman-like manifold in the (feature) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e149" xlink:type="simple"/></inline-formula>-space. Since it is a curved Riemannian manifold, reconsolidation is no longer a linear update, but the creation of geodesics is required. We provided both an exact Reconsolidation algorithm as well as a more efficient one, which is local in update and does not require the exact computation of geodesics. A continuous time version of the memory is introduced with further biological relevance.</p>
<p>The kernel method opens the door to reconsolidation of multimodal and dynamical (temporal) memories; this is a subject of our future research.</p>
</sec><sec id="s4" sec-type="materials|methods">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Defining a Riemannian Structure for the ReC algorithm</title>
<p>We formulate the distance between two kernel associative networks where both networks have the same kernel and number of memories. Each network contains a different sets of memory attractors. In the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e150" xlink:type="simple"/></inline-formula>-space each kernel memory is a symmetric network whose synaptic matrix is a projective operator <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e151" xlink:type="simple"/></inline-formula>. We measure the distance between two projective operators, <bold>X</bold> and <bold>Y</bold> (both of finite rank <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e152" xlink:type="simple"/></inline-formula>), as a Frobenius norm <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e153" xlink:type="simple"/></inline-formula>. Taking into account protectiveness and self-conjugatedness of <bold>X</bold> and <bold>Y</bold>, we have:<disp-formula id="pone.0068189.e154"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e154" xlink:type="simple"/><label>(14)</label></disp-formula></p>
<p>For each projective operator <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e155" xlink:type="simple"/></inline-formula>, the singular value decomposition (SVD) leads to the following:<disp-formula id="pone.0068189.e156"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e156" xlink:type="simple"/></disp-formula></p>
<p>For any <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e157" xlink:type="simple"/></inline-formula> defined as above, a matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e158" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e159" xlink:type="simple"/></inline-formula>) is defined as having elements <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e160" xlink:type="simple"/></inline-formula>, the pairwise scalar products of the memorized vectors. In matrix notation this is represented as:<disp-formula id="pone.0068189.e161"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e161" xlink:type="simple"/></disp-formula>Since <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e162" xlink:type="simple"/></inline-formula> is an orthogonal operator, then</p>
<p><disp-formula id="pone.0068189.e163"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e163" xlink:type="simple"/></disp-formula></p>
<p>Using this template we represent <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e164" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e165" xlink:type="simple"/></inline-formula> as follows: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e166" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e167" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e169" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e170" xlink:type="simple"/></inline-formula>. So,<disp-formula id="pone.0068189.e171"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e171" xlink:type="simple"/></disp-formula>Here <bold>Q</bold> is an <bold><italic>m<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e172" xlink:type="simple"/></inline-formula>m</italic></bold> matrix such that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e173" xlink:type="simple"/></inline-formula>. Having a singular decomposition for <bold>XY</bold>, we can now compute the distance as</p>
<p><disp-formula id="pone.0068189.e174"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e174" xlink:type="simple"/><label>(15)</label></disp-formula>The above defines a Riemannian structure for the KAM manifold.</p>
</sec><sec id="s4b">
<title>Pseudoinverse Memories and the Grassmann Manifold</title>
<p>We next relate the manifold defined by the ReKAM model to the more well-known and less complex Grassmann manifold. An associative memory with a pseudoinverse learning rule is described in <xref ref-type="bibr" rid="pone.0068189-Personnaz1">[43]</xref>. This is a Hopfield-type auto-associative memory defined originally for bipolar vectors: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e175" xlink:type="simple"/></inline-formula>. Suppose these vectors are columns of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e176" xlink:type="simple"/></inline-formula> matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e177" xlink:type="simple"/></inline-formula>. Then a synaptic matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e178" xlink:type="simple"/></inline-formula> of the memory is given by:<disp-formula id="pone.0068189.e179"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e179" xlink:type="simple"/><label>(16)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e180" xlink:type="simple"/></inline-formula> is a Moore-Penrose pseudoinverse of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e181" xlink:type="simple"/></inline-formula>. For linearly independent columns of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e182" xlink:type="simple"/></inline-formula>, the pseudoinverse can be computed by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e183" xlink:type="simple"/></inline-formula> or by using the Greville formulae (see, e.g., <xref ref-type="bibr" rid="pone.0068189-Albert1">[44]</xref>). The resulting weight matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e184" xlink:type="simple"/></inline-formula> is projective, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e185" xlink:type="simple"/></inline-formula> with rank <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e186" xlink:type="simple"/></inline-formula>.</p>
<p>The Grassmann manifold is a particular type of Riemannian. The Real Grassman manifold is the manifold of all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e187" xlink:type="simple"/></inline-formula>-dimensional subspaces in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e188" xlink:type="simple"/></inline-formula> and is denoted as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e189" xlink:type="simple"/></inline-formula>. To define the Grassman manifold, we first introduce the Stifiel manifold – a set of orthogonal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e190" xlink:type="simple"/></inline-formula>–matrices <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e191" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e192" xlink:type="simple"/></inline-formula> endowed with the Riemannian metric which is induced by the Euclidean norm in the space of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e193" xlink:type="simple"/></inline-formula>-matrices. Next, we say that two matrices are equivalent if their columns span the same <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e194" xlink:type="simple"/></inline-formula>-dimensional subspace. This means that two matrices <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e195" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e196" xlink:type="simple"/></inline-formula> are equivalent if they are related by right multiplication of an orthogonal <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e197" xlink:type="simple"/></inline-formula> matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e198" xlink:type="simple"/></inline-formula>. The quotient of the Stiefel manifold with respect to this equivalence relation is called <italic>Grassmann Manifold</italic> <xref ref-type="bibr" rid="pone.0068189-Absil1">[45]</xref>.</p>
<p>For each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e199" xlink:type="simple"/></inline-formula>-dimensional subspace in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e200" xlink:type="simple"/></inline-formula> there exists a unique projective matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e201" xlink:type="simple"/></inline-formula> of rank <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e202" xlink:type="simple"/></inline-formula>, and vice versa (see <xref ref-type="bibr" rid="pone.0068189-Golub1">[46]</xref>). Therefore a space of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e203" xlink:type="simple"/></inline-formula>-ranked projective matrices is a Grassmann manifold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e204" xlink:type="simple"/></inline-formula>. Moreover, the Frobenius norm of the difference of two projective matrices <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e205" xlink:type="simple"/></inline-formula> gives one possible Riemannian metric over this manifold.</p>
<p>The Grassman manifold emerges in our model in the special case of a scalar-product kernel. Other kernels used in our ReKAM model result in manifolds that can be considered generalizations of the Grassmann.</p>
</sec><sec id="s4c">
<title>Computing Geodesics for the ReC Algorithm</title>
<p>To implement the geodesic update algorithm, we have to efficiently compute geodesics on the kernel memory manifolds. Given the metric in explicit form (15) this can be solved as an optimization problem. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e206" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e207" xlink:type="simple"/></inline-formula> be points on manifold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e208" xlink:type="simple"/></inline-formula> with metric <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e209" xlink:type="simple"/></inline-formula>. Let a point <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e210" xlink:type="simple"/></inline-formula> lie on the (minimizing) geodesic segment joining <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e211" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e212" xlink:type="simple"/></inline-formula>. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e213" xlink:type="simple"/></inline-formula> divides the segment into two parts with proportions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e214" xlink:type="simple"/></inline-formula>. Let <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e215" xlink:type="simple"/></inline-formula> be a point which lies on the manifold but not in the geodesics. The process of finding <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e216" xlink:type="simple"/></inline-formula> is stated as follows:<disp-formula id="pone.0068189.e217"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e217" xlink:type="simple"/><label>(17)</label></disp-formula></p>
<p>The geodesic minimizes the sum of two distances (first line). For the point <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e218" xlink:type="simple"/></inline-formula> on the (minimizing) geodesic, the following inequality holds:<disp-formula id="pone.0068189.e219"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e219" xlink:type="simple"/><label>(18)</label></disp-formula></p>
<p>The Process (17) can be solved numerically using a Gradient Descent Method (or other first-order unconstrained optimization method). Its complexity is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e220" xlink:type="simple"/></inline-formula> for a tolerance <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e221" xlink:type="simple"/></inline-formula>. The constant here is typically large due to the hardness of gradient computation.</p>
</sec><sec id="s4d">
<title>Functions with Mercer Condition</title>
<p>The classical Kernels <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e222" xlink:type="simple"/></inline-formula> introduced to the field of Machine Learning by Vapnik <xref ref-type="bibr" rid="pone.0068189-Vapnik1">[26]</xref> had the Mercer condition. That is, for all square integrable functions <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e223" xlink:type="simple"/></inline-formula> the kernel satisfied:<disp-formula id="pone.0068189.e224"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e224" xlink:type="simple"/><label>(19)</label></disp-formula></p>
<p>The Mercer theorem states that if <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e225" xlink:type="simple"/></inline-formula> satisfies the Mercer condition there exists a Hilbert space <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e226" xlink:type="simple"/></inline-formula> with a basis <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e227" xlink:type="simple"/></inline-formula> and a function <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e228" xlink:type="simple"/></inline-formula>, where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e229" xlink:type="simple"/></inline-formula>, such that<disp-formula id="pone.0068189.e230"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pone.0068189.e230" xlink:type="simple"/><label>(20)</label></disp-formula>and all <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e231" xlink:type="simple"/></inline-formula>0. That is, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e232" xlink:type="simple"/></inline-formula> is a scalar product of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e233" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e234" xlink:type="simple"/></inline-formula></p>
<p>General Mercer kernels are not sufficient for creating the associative memory since our kernel memories require that all attractors are linearly independent in the feature space. Some Mercer kernels, such as the basic scalar-product kernel <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pone.0068189.e235" xlink:type="simple"/></inline-formula>, do not assure this property. The strong Mercer kernels defined for our kernel memory <xref ref-type="bibr" rid="pone.0068189-Nowicki2">[17]</xref> provide linear independence of the attractors in the feature space which enables correct association.</p>
</sec></sec></body>
<back>
<ack>
<p>We express our appreciation to Keen Sung for finding and reviewing the relevant literature as well as for helpful discussions and editing advise.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0068189-Sara1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sara</surname><given-names>S</given-names></name> (<year>2000</year>) <article-title>Retrieval and reconsolidation: Toward a neurobiology of remembering</article-title>. <source>Learning and Memory</source> <volume>7</volume>: <fpage>73</fpage>–<lpage>84</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Dudai1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dudai</surname><given-names>Y</given-names></name> (<year>1997</year>) <article-title>Time to remember</article-title>. <source>Neuron</source> <volume>18</volume>: <fpage>179</fpage>–<lpage>182</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Eichenbaum1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Eichenbaum</surname><given-names>H</given-names></name> (<year>2006</year>) <article-title>The secret life of memories</article-title>. <source>Neuron</source> <volume>50</volume>: <fpage>350</fpage>–<lpage>352</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Dudai2"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dudai</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Eisenberg</surname><given-names>M</given-names></name> (<year>2004</year>) <article-title>Rite of passage of the engram: Reconsolidation and the lingering consolidation hypothesis</article-title>. <source>Neuron</source> <volume>44</volume>: <fpage>93</fpage>–<lpage>100</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Nader1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nader</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Schafe</surname><given-names>G</given-names></name>, <name name-style="western"><surname>LeDoux</surname><given-names>J</given-names></name> (<year>2000</year>) <article-title>Fear memories require protein synthesis in the amygdala for reconsolidation after retrieval</article-title>. <source>Nature</source> <volume>406</volume>: <fpage>722</fpage>–<lpage>726</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Lee1"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lee</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Everitt</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Thomas</surname><given-names>K</given-names></name> (<year>2004</year>) <article-title>Independent cellular processes for hippocampal memory consolidation and reconsolidation</article-title>. <source>Science</source> <volume>304</volume>: <fpage>839</fpage>–<lpage>843</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Medina1"><label>7</label>
<mixed-citation publication-type="other" xlink:type="simple">Medina J, Bekinschtein P, Cammarota M, Izquierdo I (2008) Do memories consolidate to persist or do they persist to consolidate? Behavioural Brain Research : 61–69.</mixed-citation>
</ref>
<ref id="pone.0068189-Antoine1"><label>8</label>
<mixed-citation publication-type="other" xlink:type="simple">Antoine B, Jocelyne C, Serge L (2012) Reconsolidation of memory: a decade of debate. Progress in Neurobiology.</mixed-citation>
</ref>
<ref id="pone.0068189-Golkar1"><label>9</label>
<mixed-citation publication-type="other" xlink:type="simple">Golkar A, Bellander M, Olsson A, Ohman A (2012) Are fear memories erasable?–reconsolidation of learned fear with fear-relevant and fear-irrelevant stimuli. Frontiers in Behavioral Neuroscience 6.</mixed-citation>
</ref>
<ref id="pone.0068189-Barlow1"><label>10</label>
<mixed-citation publication-type="other" xlink:type="simple">Barlow D (2004) Anxiety and its disorders: The nature and treatment of anxiety and panic. Guilford Press.</mixed-citation>
</ref>
<ref id="pone.0068189-Rescorla1"><label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rescorla</surname><given-names>R</given-names></name> (<year>2004</year>) <article-title>Spontaneous recovery</article-title>. <source>Learning and Memory</source> <volume>11</volume>: <fpage>501</fpage>–<lpage>509</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Nader2"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nader</surname><given-names>K</given-names></name> (<year>2003</year>) <article-title>Memory traces unbound</article-title>. <source>Trends in Neurosciences</source> <volume>26</volume>: <fpage>65</fpage>–<lpage>72</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Alberini1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alberini</surname><given-names>C</given-names></name> (<year>2005</year>) <article-title>Mechanisms of memory stabilization: are consolidation and reconsolidation similar or distinct processes?</article-title> <source>Trends in neurosciences</source> <volume>28</volume>: <fpage>51</fpage>–<lpage>56</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Schiller1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schiller</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Monfils</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Raio</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Johnson</surname><given-names>D</given-names></name>, <name name-style="western"><surname>LeDoux</surname><given-names>J</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Preventing the return of fear in humans using reconsolidation update mechanisms</article-title>. <source>Nature</source> <volume>463</volume>: <fpage>49</fpage>–<lpage>53</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Agren1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Agren</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Engman</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Frick</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Bjorkstrand</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Larsson</surname><given-names>E</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Disruption of reconsolidation erases a fear memory trace in the human amygdala</article-title>. <source>Science</source> <volume>337</volume>: <fpage>1550</fpage>–<lpage>1552</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Nowicki1"><label>16</label>
<mixed-citation publication-type="other" xlink:type="simple">Nowicki D, Siegelmann H (2009) The secret life of kernels: Reconsolidation in exible memories. In: Frontiers in Systems Neuroscience. Conference Abstract: Computational and systems neuroscience. doi: 10.3389/conf.neuro.06.2009.03.271.</mixed-citation>
</ref>
<ref id="pone.0068189-Nowicki2"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nowicki</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Siegelmann</surname><given-names>H</given-names></name> (<year>2010</year>) <article-title>Flexible kernel memory</article-title>. <source>PLoS ONE</source> <volume>5</volume>: <fpage>e10955</fpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Wills1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wills</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Lever</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Cacucci</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Burgess</surname><given-names>N</given-names></name>, <name name-style="western"><surname>O'Keefe</surname><given-names>J</given-names></name> (<year>2005</year>) <article-title>Attractor dynamics in the hippocampal representation of the local environment</article-title>. <source>Science</source> <volume>308</volume>: <fpage>873</fpage>–<lpage>876</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-McClelland1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McClelland</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Rumelhart</surname><given-names>D</given-names></name> (<year>1981</year>) <article-title>An interactive activation model of context effects in letter perception: Part i. an account of basic findings</article-title>. <source>Psychological Review</source> <volume>88</volume>: <fpage>375</fpage>–<lpage>407</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Hopfield1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>J</given-names></name> (<year>1984</year>) <article-title>Neurons with graded response have collective computational properties like those of two-state neurons</article-title>. <source>PNAS</source> <volume>81</volume>: <fpage>3088</fpage>–<lpage>3092</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Hopfield2"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Tank</surname><given-names>D</given-names></name> (<year>1986</year>) <article-title>Computing with neural circuits: A model</article-title>. <source>Science</source> <volume>233</volume>: <fpage>625</fpage>–<lpage>633</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Maass1"><label>22</label>
<mixed-citation publication-type="other" xlink:type="simple">Maass W, Joshi P, Sontag E (2007) Computational aspects of feedback in neural circuits. PLOS Computational Biology 3.</mixed-citation>
</ref>
<ref id="pone.0068189-Blumenfeld1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Blumenfeld</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Preminger</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Sagi</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Tsodyks</surname><given-names>M</given-names></name> (<year>2006</year>) <article-title>Dynamics of memory representations in networks with novelty-facilitated synaptic plasticity</article-title>. <source>Neuron</source> <volume>52</volume>: <fpage>383</fpage>–<lpage>394</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Siegelmann1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Siegelmann</surname><given-names>H</given-names></name> (<year>2008</year>) <article-title>Analog-symbolic memory that tracks via reconsolidation</article-title>. <source>Physica D</source> <volume>237</volume>: <fpage>1207</fpage>–<lpage>1214</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Osan1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Osan</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Tort</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Amaral</surname><given-names>O</given-names></name> (<year>2011</year>) <article-title>A mismatch-based model for memory reconsolidation and extinction in attractor networks</article-title>. <source>PLoS ONE</source> <volume>6</volume>: <fpage>e23113</fpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Vapnik1"><label>26</label>
<mixed-citation publication-type="other" xlink:type="simple">Vapnik V (1998) Statistical Learning Theory. John Wiley &amp; Sons, NY.</mixed-citation>
</ref>
<ref id="pone.0068189-BenHur1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ben-Hur</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Horn</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Siegelmann</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Vapnik</surname><given-names>V</given-names></name> (<year>2001</year>) <article-title>Support vector clustering</article-title>. <source>Journal of Machine Learning Research</source> <volume>2</volume>: <fpage>125</fpage>–<lpage>137</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Nocedal1"><label>28</label>
<mixed-citation publication-type="other" xlink:type="simple">Nocedal J, Wright S (2006) Numerical optimization. Springer Science+ Business Media.</mixed-citation>
</ref>
<ref id="pone.0068189-Udriste1"><label>29</label>
<mixed-citation publication-type="other" xlink:type="simple">Udriste C (1994) Convex functions and optimization methods on Riemannian manifolds, volume 297. Springer.</mixed-citation>
</ref>
<ref id="pone.0068189-doCarmo1"><label>30</label>
<mixed-citation publication-type="other" xlink:type="simple">do Carmo M (1992) Riemannian Geometry. Birkhauser Verlag AG.</mixed-citation>
</ref>
<ref id="pone.0068189-Hupbach1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hupbach</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Gomez</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Hardt</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Nadel</surname><given-names>L</given-names></name> (<year>2007</year>) <article-title>Reconsolidation of episodic memories: A subtle reminder triggers integration of new information</article-title>. <source>Learning &amp; Memory</source> <volume>14</volume>: <fpage>47</fpage>–<lpage>53</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Monfils1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Monfils</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Cowansage</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Klann</surname><given-names>E</given-names></name>, <name name-style="western"><surname>LeDoux</surname><given-names>J</given-names></name> (<year>2009</year>) <article-title>Extinction-reconsolidation boundaries: key to persistent attenuation of fear memories</article-title>. <source>Science</source> <volume>324</volume>: <fpage>951</fpage>–<lpage>955</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Suzuki1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Suzuki</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Josselyn</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Frankland, Paul</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Masushige</surname><given-names>S</given-names></name>, <etal>et al</etal>. (<year>2004</year>) <article-title>Memory reconsolidation and extinction have distinct temporal and biochemical signatures</article-title>. <source>The Journal of neuroscience</source> <volume>24</volume>: <fpage>4787</fpage>–<lpage>4795</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Pedreira1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pedreira</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Pérez-Cuesta</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Maldonado</surname><given-names>H</given-names></name> (<year>2004</year>) <article-title>Mismatch between what is expected and what actually occurs triggers memory reconsolidation or extinction</article-title>. <source>Learning &amp; Memory</source> <volume>11</volume>: <fpage>579</fpage>–<lpage>585</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Preminger1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Preminger</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Blumenfeld</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Tsodyks</surname><given-names>DSM</given-names></name> (<year>2009</year>) <article-title>Mapping dynamic memories of gradually changing objects</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>106</volume>: <fpage>5371</fpage>–<lpage>5376</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Wilson1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Cowan</surname><given-names>J</given-names></name> (<year>1973</year>) <article-title>A mathematical theory of the functional dynamics of cortical and thalamic nervous tissue</article-title>. <source>Kybernetik</source> <volume>13</volume>: <fpage>55</fpage>–<lpage>80</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Amari1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amari</surname><given-names>S</given-names></name> (<year>1977</year>) <article-title>Dynamics of pattern formation in lateral-inhibition type neural fields</article-title>. <source>Biol Cybernet</source> <volume>27</volume>: <fpage>77</fpage>–<lpage>87</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Buice1"><label>38</label>
<mixed-citation publication-type="other" xlink:type="simple">Buice M, Cowan JD, Chow C (2009) Systematic uctuation expansion for neural network activity equations. arxivorg arXiv:0902.3925v2.</mixed-citation>
</ref>
<ref id="pone.0068189-Besnard1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Besnard</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Machado</surname><given-names>ML</given-names></name>, <name name-style="western"><surname>Vignaux</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Boulouard</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Coquerel</surname><given-names>A</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Influence of vestibular input on spatial and nonspatial memory and on hippocampal nmda receptors</article-title>. <source>Hippocampus</source> <volume>22</volume>: <fpage>814</fpage>–<lpage>826</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Besnard2"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Besnard</surname><given-names>A</given-names></name> (<year>2012</year>) <article-title>A model of hippocampal competition between new learning and memory updating</article-title>. <source>The Journal of Neuroscience</source> <volume>32</volume>: <fpage>3281</fpage>–<lpage>3283</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Tronson1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tronson</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Taylor</surname><given-names>J</given-names></name> (<year>2007</year>) <article-title>Molecular mechanisms of memory reconsolidation</article-title>. <source>Nature Reviews Neuroscience</source> <volume>8</volume>: <fpage>262</fpage>–<lpage>275</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Sevenster1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sevenster</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Beckers</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Kindt</surname><given-names>M</given-names></name> (<year>2012</year>) <article-title>Retrieval per se is not sufficient to trigger reconsolidation of human fear memory</article-title>. <source>Neurobiology of learning and memory</source> <volume>97</volume>: <fpage>338</fpage>–<lpage>345</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Personnaz1"><label>43</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Personnaz</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Dreyfus</surname><given-names>I</given-names></name> (<year>1986</year>) <article-title>Collective computational properties of neural networks: New learning mechanisms</article-title>. <source>Phys Rev A</source> <volume>34</volume>: <fpage>4217</fpage>–<lpage>4228</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Albert1"><label>44</label>
<mixed-citation publication-type="other" xlink:type="simple">Albert A (1972) Regression and the Moore-Penrose pseudoinverse. Academic Press, NY-London.</mixed-citation>
</ref>
<ref id="pone.0068189-Absil1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Absil</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Mahony</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Sepulchre</surname><given-names>R</given-names></name> (<year>2004</year>) <article-title>Riemannian geometry of grassmann manifolds with a view on algorithmic computation</article-title>. <source>Acta Applicandae Mathematicae</source> <volume>80</volume>: <fpage>199</fpage>–<lpage>220</lpage>.</mixed-citation>
</ref>
<ref id="pone.0068189-Golub1"><label>46</label>
<mixed-citation publication-type="other" xlink:type="simple">Golub G, Loan CV (1996) Matrix computations (3rd ed.). Baltimore, MD, USA: Johns Hopkins University Press.</mixed-citation>
</ref>
</ref-list></back>
</article>