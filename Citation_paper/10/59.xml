<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
<journal-id journal-id-type="pmc">ploscomp</journal-id><journal-title-group>
<journal-title>PLoS Computational Biology</journal-title></journal-title-group>
<issn pub-type="ppub">1553-734X</issn>
<issn pub-type="epub">1553-7358</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PCOMPBIOL-D-13-01206</article-id>
<article-id pub-id-type="doi">10.1371/journal.pcbi.1003489</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Computational neuroscience</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Optimal Recall from Bounded Metaplastic Synapses: Predicting Functional Adaptations in Hippocampal Area CA3</article-title>
<alt-title alt-title-type="running-head">Optimal Recall from Bounded Metaplastic Synapses</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Savin</surname><given-names>Cristina</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Dayan</surname><given-names>Peter</given-names></name><xref ref-type="aff" rid="aff2"><sup>2</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Lengyel</surname><given-names>Máté</given-names></name><xref ref-type="aff" rid="aff1"><sup>1</sup></xref></contrib>
</contrib-group>
<aff id="aff1"><label>1</label><addr-line>Computational &amp; Biological Learning Lab, Department of Engineering, University of Cambridge, Cambridge, United Kingdom</addr-line></aff>
<aff id="aff2"><label>2</label><addr-line>Gatsby Computational Neuroscience Unit, University College London, London, United Kingdom</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Sporns</surname><given-names>Olaf</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Indiana University, United States of America</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">cs664@cam.ac.uk</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: CS PD ML. Performed the experiments: CS. Analyzed the data: CS. Wrote the paper: CS PD ML.</p></fn>
</author-notes>
<pub-date pub-type="collection"><month>2</month><year>2014</year></pub-date>
<pub-date pub-type="epub"><day>27</day><month>2</month><year>2014</year></pub-date>
<volume>10</volume>
<issue>2</issue>
<elocation-id>e1003489</elocation-id>
<history>
<date date-type="received"><day>8</day><month>7</month><year>2013</year></date>
<date date-type="accepted"><day>23</day><month>12</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2014</copyright-year>
<copyright-holder>Savin et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>A venerable history of classical work on autoassociative memory has significantly shaped our understanding of several features of the hippocampus, and most prominently of its CA3 area, in relation to memory storage and retrieval. However, existing theories of hippocampal memory processing ignore a key biological constraint affecting memory storage in neural circuits: the bounded dynamical range of synapses. Recent treatments based on the notion of metaplasticity provide a powerful model for individual bounded synapses; however, their implications for the ability of the hippocampus to retrieve memories well and the dynamics of neurons associated with that retrieval are both unknown. Here, we develop a theoretical framework for memory storage and recall with bounded synapses. We formulate the recall of a previously stored pattern from a noisy recall cue and limited-capacity (and therefore lossy) synapses as a probabilistic inference problem, and derive neural dynamics that implement approximate inference algorithms to solve this problem efficiently. In particular, for binary synapses with metaplastic states, we demonstrate for the first time that memories can be efficiently read out with biologically plausible network dynamics that are completely constrained by the synaptic plasticity rule, and the statistics of the stored patterns and of the recall cue. Our theory organises into a coherent framework a wide range of existing data about the regulation of excitability, feedback inhibition, and network oscillations in area CA3, and makes novel and directly testable predictions that can guide future experiments.</p>
</abstract>
<abstract abstract-type="summary"><title>Author Summary</title>
<p>Memory is central to nervous system function and has been a particular focus for studies of the hippocampus. However, despite many clues, we understand little about how memory storage and retrieval is implemented in neural circuits. In particular, while many previous studies considered the amount of information that can be stored in synaptic connections under biological constraints on the dynamic range of synapses, how much of this information can be successfully recovered by neural dynamics during memory retrieval remains unclear. Here, we use a top-down approach to address this question: we assume memories are laid down in bounded synapses by biologically relevant plasticity rules and then derive from first principles how the neural circuit should behave during recall in order to retrieve these memories most efficiently. We show that the resulting recall dynamics are consistent with a wide variety of properties of hippocampal area CA3, across a range of biophysical levels – from synapses, through neurons, to circuits. Furthermore, our approach allows us to make novel and experimentally testable predictions about the link between the structure, dynamics, and function of CA3 circuitry.</p>
</abstract>
<funding-group><funding-statement>This work was supported by the Wellcome Trust (CS and ML), and the Gatsby Charitable Foundation (PD), and was written under partial support by project #FP7-269921 (BrainScaleS) of the European Union (ML). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="22"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>The hippocampus, together with associated medial temporal-lobe structures, plays a critical role in memory storage and retrieval. A venerable line of classical theoretical work has shaped our understanding of how different hippocampal subfields subserve this function <xref ref-type="bibr" rid="pcbi.1003489-Treves1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Treves2">[2]</xref>. At the core of this body of work is the notion that area CA3 of the hippocampus operates as an autoassociator, retrieving previously stored memory traces from noisy or partial cues by the process of pattern completion. Furthermore, the theoretical framework of autoassociative memory networks helped elucidate how recurrently-coupled neural circuits, such as CA3 <xref ref-type="bibr" rid="pcbi.1003489-Amaral1">[3]</xref>, are capable of such pattern completion <xref ref-type="bibr" rid="pcbi.1003489-Hopfield1">[4]</xref>–<xref ref-type="bibr" rid="pcbi.1003489-Amit3">[10]</xref>. In this framework, synaptic plasticity stores memory traces in the efficacies (or weights) of the recurrent synapses of the neural circuit, and the recall of memories is achieved by the dynamical evolution of network activity through the synapses that were previously altered <xref ref-type="bibr" rid="pcbi.1003489-Hebb1">[11]</xref>. This framework has paved the way for a thorough analysis of the memory capacity of recurrent neural circuits <xref ref-type="bibr" rid="pcbi.1003489-Hopfield1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Hertz1">[8]</xref>–<xref ref-type="bibr" rid="pcbi.1003489-Amit3">[10]</xref>, and ensuing experimental results have confirmed many of its qualitative predictions <xref ref-type="bibr" rid="pcbi.1003489-Nakazawa1">[12]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Wills1">[13]</xref>. However, despite much progress, existing models of auto-associative memories make drastic simplifying assumptions, as we describe below, concerning both the synaptic plasticity rules storing information in the circuit and the dynamics of the network at recall.</p>
<p>First, at the level of memory storage, one powerful, yet biologically untenable, simplification made by most existing models <xref ref-type="bibr" rid="pcbi.1003489-Hopfield1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Hertz1">[8]</xref>–<xref ref-type="bibr" rid="pcbi.1003489-Amit3">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Lengyel1">[14]</xref> is the use of additive learning rules, whereby the cumulative effect of storing multiple memory traces is obtained as the linear sum of the contributions made by storing each individual trace. This simplification makes the analysis of the circuit tractable and suggests high memory capacity, but it also implies that synaptic weights can grow arbitrarily large or even switch sign, thereby violating Dale's principle. The shortcomings of assuming additive learning rules can be partially alleviated by introducing additional mechanisms such as synaptic scaling or metaplasticity, that ensure synapses are maintained in the relevant biological range <xref ref-type="bibr" rid="pcbi.1003489-Abbott1">[15]</xref>. Metaplasticity is loosely defined as any mechanism that manipulates or modulates synaptic plasticity; it comes in many forms, from the sliding threshold in BCM-like models <xref ref-type="bibr" rid="pcbi.1003489-Bienenstock1">[16]</xref> to sophisticated cascade models <xref ref-type="bibr" rid="pcbi.1003489-Fusi1">[17]</xref>. It is ubiquitous in the neocortex <xref ref-type="bibr" rid="pcbi.1003489-Abraham1">[18]</xref>–<xref ref-type="bibr" rid="pcbi.1003489-Hulme1">[20]</xref> and the hippocampus <xref ref-type="bibr" rid="pcbi.1003489-Debanne1">[21]</xref>, and has long been implicated in endowing synapses with powerful computational properties <xref ref-type="bibr" rid="pcbi.1003489-Bienenstock1">[16]</xref>. Importantly for memory storage, it was shown that one particular form of metaplasticity, the cascade model <xref ref-type="bibr" rid="pcbi.1003489-Fusi1">[17]</xref>, enables information to be stored in bounded synapses almost as efficiently as additive learning rules, whereas synapses with the same range of efficacies but without metaplasticity are hopelessly poor <xref ref-type="bibr" rid="pcbi.1003489-Fusi1">[17]</xref>. Unfortunately, despite their advantage at storing information, metaplastic synapses were found to perform equally poorly when the amount of recalled information was measured instead <xref ref-type="bibr" rid="pcbi.1003489-Huang1">[22]</xref>, indicating that much of the information laid down in the synapses remained inaccessible for the standard attractor dynamics used at retrieval. Thus, perhaps surprisingly, we still do not know how competent memory recall is possible from more realistic synapses that suffer from a bounded dynamical range.</p>
<p>Second, at the level of retrieval, there are also several aspects of hippocampal circuit dynamics of which we lack a theoretical account. For example, experimental work has long shown that synaptic plasticity is accompanied by changes in the excitability of CA3 neurons <xref ref-type="bibr" rid="pcbi.1003489-Thompson1">[23]</xref>–<xref ref-type="bibr" rid="pcbi.1003489-Zhang1">[25]</xref>, that the activity of pyramidal cells is modulated by several classes of inhibitory neurons <xref ref-type="bibr" rid="pcbi.1003489-Klausberger1">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Lapray1">[27]</xref>, and that the interaction of excitation and inhibition induces prominent oscillations in multiple frequency bands <xref ref-type="bibr" rid="pcbi.1003489-Klausberger1">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Kullmann1">[28]</xref>. Yet, it is largely unclear whether and how these dynamical motifs contribute to efficient memory recall.</p>
<p>Here, we develop a theory that specifically addresses the problem of memory recall from synapses with a limited dynamic range, and thus consider how various neuronal and synaptic biophysical properties of area CA3 contribute to this process. We start by assuming that synaptic efficacies are limited and adopt one particular, oft-studied model of metaplasticity, the cascade model, where synapses make transitions between different states which have the same overt efficacy but differ in their propensity to exhibit further plasticity <xref ref-type="bibr" rid="pcbi.1003489-Fusi1">[17]</xref>. In order to understand how memories can be recalled efficiently from such synapses, we derive recurrent network dynamics that are optimal for this purpose. Our approach is based on treating memory recall as a probabilistic inference problem, in which the memory pattern to be recalled needs to be inferred from partial and noisy information in the recall cue and the synaptic weights of the network, and network dynamics act to produce activity patterns that are representative of the resulting posterior distribution. Given the statistical properties of the prior distribution of patterns, the recall cues, and the learning rule, the network dynamics that we derive to be optimal for retrieval are fully specified without free parameters to tune (except, as we show later, for some parameters affecting the speed of recall). The essence of our approach is that there is a tight coupling between the specifics of the learning rule governing memory storage and the dynamics of the circuit during recall. This approach has already helped reveal some basic principles of efficient memory recall in neural circuits <xref ref-type="bibr" rid="pcbi.1003489-Lengyel1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-MacKay1">[29]</xref>–<xref ref-type="bibr" rid="pcbi.1003489-Lengyel2">[31]</xref>, but has not yet been applied to bounded metaplastic synapses.</p>
<p>While we derived optimal recall dynamics with only minimal <italic>a priori</italic> regard to biological constraints, we found that approximately optimal retrieval can be achieved in a neural circuit whose basic functional structure resembles the standard, biophysically motivated dynamics used for additive learning rules <xref ref-type="bibr" rid="pcbi.1003489-Hopfield1">[4]</xref>. Importantly, the solution involves several critical motifs that are not predicted by standard approaches, and yet map onto known features of the dynamical organisation of hippocampal area CA3. First, precisely balanced feed-back inhibition <xref ref-type="bibr" rid="pcbi.1003489-Freund1">[32]</xref> and pre- and postsynaptic forms of intrinsic plasticity (IP) <xref ref-type="bibr" rid="pcbi.1003489-Zhang1">[25]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Desai1">[33]</xref> matched to the form of synaptic plasticity that stores the memory traces, are necessary for ensuring stability during retrieval. Second, oscillations that periodically change the relative contributions of afferent and recurrent synapses to circuit dynamics <xref ref-type="bibr" rid="pcbi.1003489-Wyble1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Klausberger2">[35]</xref> can further improve recall performance by helping the network explore representative activity patterns more effectively.</p>
<p>In sum, addressing the computational challenges associated with effective retrieval of information from bounded synapses provides novel insights into the dynamics of the hippocampal circuitry implementing this function. Thus, our work extends previous approaches that sought to understand the basic anatomical and physiological organisation of the hippocampus <xref ref-type="bibr" rid="pcbi.1003489-Treves2">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Marr1">[36]</xref> as functional adaptions towards memory recall by providing a similar functional account of further crucial aspects of hippocampal organisation, involving plasticity and circuit dynamics.</p>
</sec><sec id="s2">
<title>Results</title>
<p>We start by providing a formal description of autoassociative memory recall as a probabilistic inference task. We then derive recall dynamics that solve this task (approximately) optimally and investigate their computational and biological implications. First, we show that efficient recall is possible from metaplastic synapses with such dynamics. Second, as several details of the derived dynamics are unrealistic, we investigate biologically plausible approximations for them which enable us to identify the circuit motifs that are critical for effective memory retrieval in hippocampal circuits. Finally, we consider one particular improvement of the original solution which makes the recall dynamics more efficient and suggests a novel computational role for network oscillations.</p>
<sec id="s2a">
<title>A probabilistic framework for autoassociative memory recall</title>
<p>We consider an auto-associative memory task in which a sequence of patterns, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e001" xlink:type="simple"/></inline-formula>, is stored by one-shot learning in the synaptic efficacies (or weights), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e002" xlink:type="simple"/></inline-formula>, of the recurrent collaterals of a neural network. This models the network of pyramidal neurons in hippocampal area CA3. (We do not model other cell types or hippocampal subfields explicitly, but do consider their effects on CA3 pyramids, see also below). Here, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e003" xlink:type="simple"/></inline-formula> is the activity of neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e004" xlink:type="simple"/></inline-formula> in the pattern that was stored <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e005" xlink:type="simple"/></inline-formula> time steps prior to recall (in other words, the age of this pattern is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e006" xlink:type="simple"/></inline-formula>), and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e007" xlink:type="simple"/></inline-formula> is the (overt) efficacy of the synapse between presynaptic cell <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e008" xlink:type="simple"/></inline-formula> and postsynaptic cell <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e009" xlink:type="simple"/></inline-formula> at the time of recall (<xref ref-type="fig" rid="pcbi-1003489-g001">Fig. 1A–B</xref>). For tractability, we assume that neural activities are binary; although extensions of the theory to analogue activities are also possible <xref ref-type="bibr" rid="pcbi.1003489-Lengyel1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Lengyel2">[31]</xref>.</p>
<fig id="pcbi-1003489-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003489.g001</object-id><label>Figure 1</label><caption>
<title>Autoassociative memory with bounded synapses.</title>
<p><bold>A.</bold> Memories are stored in the recurrent collaterals of a neural network. Five example synapses are shown, each in a different state (colors from panel C). <bold>B.</bold> During storage, a sequence of items, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e010" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e011" xlink:type="simple"/></inline-formula> indexes time backwards from the time of recall), induces changes to the internal states, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e012" xlink:type="simple"/></inline-formula>, and thus to the overt efficacies, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e013" xlink:type="simple"/></inline-formula>, of recurrent synapses in the network. During retrieval, the dynamics of the network should identify the pattern to be recalled given a cue and information in the synaptic efficacies. <bold>C.</bold> The cascade model of synaptic metaplasticity <xref ref-type="bibr" rid="pcbi.1003489-Fusi1">[17]</xref>. Colored circles are latent states, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e014" xlink:type="simple"/></inline-formula>, that correspond to two different synaptic efficacies, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e015" xlink:type="simple"/></inline-formula>; arrows are state transitions (blue: depression, red: potentiation). Tables show different variants of mapping pre- and post-synaptic activations to depression (D) and potentiation (P) under the pre- and postsynaptically-gated learning rules. <bold>D.</bold> Left: the evolution of the expected distribution over synaptic states (thickness of stripes is proportional to the probability of the corresponding state, see panel C for color code) after a potentiation event at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e016" xlink:type="simple"/></inline-formula> (marked by the vertical arrow) and the storage of random patterns in subsequent steps, and the distribution of times at which this memory may need to be recalled (white curve). Middle: the time-averaged expected distribution of hidden synaptic states at the unknown time of recall of this memory. Right: the corresponding distribution over overt synaptic efficacies.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003489.g001" position="float" xlink:type="simple"/></fig><sec id="s2a1">
<title>Memory storage in cascade-type metaplastic synapses</title>
<p>Although we assume that the efficacy of a synapse, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e017" xlink:type="simple"/></inline-formula>, is binary, underlying these two ‘overt’ states there is a larger number of ‘hidden’ states, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e018" xlink:type="simple"/></inline-formula>, between which the synapse can transition, engendering a form of metaplasticity <xref ref-type="bibr" rid="pcbi.1003489-Debanne1">[21]</xref> (<xref ref-type="fig" rid="pcbi-1003489-g001">Fig. 1B–C</xref>). More specifically, we use a model in which synaptic plasticity is stochastic and local, with (meta)plasticity events inducing changes in the hidden state of each synapse, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e019" xlink:type="simple"/></inline-formula>, as a function of the activity of the pre- and postsynaptic neuron, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e020" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e021" xlink:type="simple"/></inline-formula> respectively <xref ref-type="bibr" rid="pcbi.1003489-Fusi1">[17]</xref> (see <xref ref-type="sec" rid="s4">Methods</xref> for details). Each of these hidden synaptic states is mapped into one of the two overt binary synaptic efficacies, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e022" xlink:type="simple"/></inline-formula>, which can be used to influence network dynamics at recall.</p>
<p>We considered two possible rules for mapping the activity of the pre- and postsynaptic neuron into plasticity events: a postsynaptically-gated learning rule, with plasticity occurring whenever the postsynaptic neuron is active, leading to either potentiation when the presynaptic neuron is also active or to depression otherwise; and a presynaptically-gated learning rule, in which synaptic change occurs only if the presynaptic neuron is active (<xref ref-type="fig" rid="pcbi-1003489-g001">Fig. 1C</xref>). The first form seems more biologically relevant, as plasticity in hippocampal area CA3 is NMDA-receptor dependent (and hence requires postsynaptic depolarization for induction) <xref ref-type="bibr" rid="pcbi.1003489-Tsien1">[37]</xref>, while the presynaptically-gated form has been traditionally assumed in past analyses of recall performance for autoassociative memory tasks <xref ref-type="bibr" rid="pcbi.1003489-Huang1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-BenDayanRubin1">[38]</xref>. We followed common practice in setting the parameters of the model that determine the particular transition probabilities (see <xref ref-type="sec" rid="s4">Methods</xref>), but did not otherwise attempt to set them explicitly to maximize information storage <xref ref-type="bibr" rid="pcbi.1003489-Lahiri1">[39]</xref>.</p>
</sec><sec id="s2a2">
<title>Memory recall as probabilistic inference</title>
<p>At the time of retrieval, the network is presented with a cue, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e023" xlink:type="simple"/></inline-formula>, which is a noisy or partial version of one of the originally stored patterns, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e024" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003489-g001">Fig. 1B</xref>). Network dynamics should lead to a recalled pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e025" xlink:type="simple"/></inline-formula> by combining the information in the cue, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e026" xlink:type="simple"/></inline-formula>, and the weights, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e027" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003489-g001">Fig. 1B</xref>). Note that each of these sources of information alone is unreliable: the cue is is imperfect by definition (otherwise there would be no computational task to solve, as the cue would already be identical to the pattern that needs to be recalled), and the weights provide only partial information, because the synaptic plasticity rule is stochastic and the information about any particular memory pattern interferes with the effects of storing other patterns in the same set of synaptic weights (<xref ref-type="fig" rid="pcbi-1003489-g001">Fig. 1D</xref>).</p>
<p>Combining information from multiple unreliable sources, such as the recall cue and the synaptic efficacies, is inherently a probabilistic inference problem <xref ref-type="bibr" rid="pcbi.1003489-MacKay1">[29]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Sommer1">[30]</xref>. In order to understand better what this problem implies, and to start our investigation of potential solutions to it, we first focus on the posterior distribution over patterns, which expresses the probability that pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e028" xlink:type="simple"/></inline-formula> is the correct pattern to be recalled given the information in the recall cue and the weights:<disp-formula id="pcbi.1003489.e029"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e029" xlink:type="simple"/><label>(1)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e030" xlink:type="simple"/></inline-formula> is the prior distribution from which patterns are sampled at the time of storage, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e031" xlink:type="simple"/></inline-formula> is the distribution describing noise corrupting the recall cue, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e032" xlink:type="simple"/></inline-formula> is the probability that the synaptic weight matrix is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e033" xlink:type="simple"/></inline-formula> at the time of recall given that pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e034" xlink:type="simple"/></inline-formula> was stored some time in the past with a known synaptic plasticity rule (such as the one described above). Thus, in general, there are several patterns that may constitute the correct answer to a recall query, each with a different probability given by the posterior distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e035" xlink:type="simple"/></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1003489.e029">Eq. 1</xref>). We consider a neural circuit to perform well in autoassociative recall if its dynamics are such that the resulting activity patterns are somehow representative of this distribution. However, before we spell out in detail the link between the posterior and actual neural dynamics (see next section), we first need to understand more thoroughly some key properties of the posterior, and in particular how it is affected – through the likelihood term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e036" xlink:type="simple"/></inline-formula> – by the synaptic plasticity rule used to store memories.</p>
<p>Previous analyses of optimal recall considered forms of synaptic plasticity that are mathematically unstable and biologically unrealistic. In these, unlike actual neural circuits <xref ref-type="bibr" rid="pcbi.1003489-Barbour1">[40]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Ikegaya1">[41]</xref>, synaptic weights do not have a proper stationary distribution. The most common case involves additive learning rules <xref ref-type="bibr" rid="pcbi.1003489-Hopfield1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Lengyel1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Lengyel2">[31]</xref>. These make synaptic weights grow without bound and imply that the information available about a pattern is independent of pattern age. They thus do not correctly capture behavioural forgetting <xref ref-type="bibr" rid="pcbi.1003489-Anderson1">[42]</xref>. Conversely, storage in binary synapses with a logical OR-like rule <xref ref-type="bibr" rid="pcbi.1003489-Sommer1">[30]</xref> creates a degenerate stationary distribution for synaptic weights, because all synapses eventually become potentiated. It also makes forgetting catastrophically fast <xref ref-type="bibr" rid="pcbi.1003489-Fusi1">[17]</xref>. The cascade learning rule we investigate here covers the biologically relevant scenario in which synaptic weights have a well-defined, non-singular, stationary distribution (<xref ref-type="fig" rid="pcbi-1003489-g001">Fig. 1D</xref>). In this case, old memories are overwritten by the storage of new ones, but metaplasticity helps to maintain memories efficiently over long retention intervals <xref ref-type="bibr" rid="pcbi.1003489-Fusi1">[17]</xref>.</p>
<p><xref ref-type="fig" rid="pcbi-1003489-g001">Fig. 1D</xref> provides intuition for the four steps involved in computing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e037" xlink:type="simple"/></inline-formula> when synapses evolve according to the cascade model (for formal details, see <xref ref-type="sec" rid="s4">Methods</xref>).</p>
<list list-type="order"><list-item>
<p>The evolution of the hidden synaptic states, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e038" xlink:type="simple"/></inline-formula>, after storing pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e039" xlink:type="simple"/></inline-formula>, can be described by a stochastic process (formally a Markov chain), characterizing the probability of the synapse being in any possible state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e040" xlink:type="simple"/></inline-formula> after storing a specific pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e041" xlink:type="simple"/></inline-formula> (the example shown in <xref ref-type="fig" rid="pcbi-1003489-g001">Fig. 1D</xref> is for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e042" xlink:type="simple"/></inline-formula>), and then a set of subsequent patterns.</p>
<p>There are three key stages in the evolution of the synaptic state. First, before storing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e043" xlink:type="simple"/></inline-formula>, the state of the synapse reflects the large number of patterns that preceded it and were drawn from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e044" xlink:type="simple"/></inline-formula>. These leave the synapse in a stationary distribution which, in our case, is uniform. Thus in <xref ref-type="fig" rid="pcbi-1003489-g001">Fig. 1D</xref> (left) the thickness of the stripes showing the probability before storage, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e045" xlink:type="simple"/></inline-formula> (with slightly informal notation), is the same for all possible synaptic states. (Note that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e046" xlink:type="simple"/></inline-formula> can equivalently denote the age of the pattern that needs to be recalled at any particular time, or the time elapsed since the storage of a particular pattern, starting with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e047" xlink:type="simple"/></inline-formula> when the pattern is the last pattern that has been stored.)</p>
<p>Second, at the time of storage, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e048" xlink:type="simple"/></inline-formula>, pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e049" xlink:type="simple"/></inline-formula> is stored in the synapse. In the particular example shown in <xref ref-type="fig" rid="pcbi-1003489-g001">Fig. 1D</xref>, both the post- and pre-synaptic cells are active in this pattern, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e050" xlink:type="simple"/></inline-formula>. This triggers a potentiation event in the form of a stochastic transition between the latent synaptic states (following the red arrows in <xref ref-type="fig" rid="pcbi-1003489-g001">Fig. 1C</xref>). In this case, this increases the probability of the synapse being in states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e051" xlink:type="simple"/></inline-formula>, corresponding to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e052" xlink:type="simple"/></inline-formula>.</p>
<p>Finally, subsequent patterns stored after <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e053" xlink:type="simple"/></inline-formula>, again drawn from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e054" xlink:type="simple"/></inline-formula>, lead to similar stochastic transitions, ultimately determining the state of the synapses at the time of recall, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e055" xlink:type="simple"/></inline-formula>. Formally, the effect of these other patterns can be described by repeatedly applying a single transition operator that averages over the possible identities of the other patterns (see <xref ref-type="sec" rid="s4">Methods</xref> for details). From the perspective of the original pattern we aim to retrieve, all these subsequent patterns act as a source of noise, because they reduce the amount of information available in the synapses about the original pattern (the distribution becomes increasingly similar to that before storing the pattern, <xref ref-type="fig" rid="pcbi-1003489-g001">Figure 1D</xref>, left).</p>
</list-item><list-item>
<p>As the distribution over synaptic states at the time of recall depends on the (unknown) pattern age (i.e. the number of times the average transition operator has been applied since storing the original pattern), we need to integrate over the distribution of possible pattern ages <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e056" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e057" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003489-g001">Fig. 1D</xref>, left, white curve). Thus, we compute <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e058" xlink:type="simple"/></inline-formula>, yielding the time-averaged expected synaptic state distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e059" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003489-g001">Fig. 1D</xref>, middle).</p>
</list-item><list-item>
<p>As it is only the overt synaptic efficacies, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e060" xlink:type="simple"/></inline-formula>, and not the hidden states, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e061" xlink:type="simple"/></inline-formula>, that can influence the interactions between neurons during recall, we apply the deterministic mapping between hidden synaptic states and overt synaptic efficacies to determine the probability distribution over the latter, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e062" xlink:type="simple"/></inline-formula>, obtained by summing together the probabilities of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e063" xlink:type="simple"/></inline-formula> values that correspond to the same <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e064" xlink:type="simple"/></inline-formula> value (<xref ref-type="fig" rid="pcbi-1003489-g001">Fig. 1D</xref>, right).</p>
</list-item><list-item>
<p>Finally, in order to simplify our analysis, we assume that for local synaptic plasticity rules, for which the change in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e065" xlink:type="simple"/></inline-formula> only depends on <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e066" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e067" xlink:type="simple"/></inline-formula>, the evidence from the synaptic efficacies can be factorized as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e068" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003489-Lengyel1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Sommer1">[30]</xref> (but see <xref ref-type="bibr" rid="pcbi.1003489-Savin1">[43]</xref>).</p>
</list-item></list>
</sec><sec id="s2a3">
<title>Dynamics for approximately optimal recall</title>
<p>As we saw above, the answer to a recall query lies in the posterior distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e069" xlink:type="simple"/></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1003489.e029">Eq. 1</xref>). How can neural dynamics compute and represent it, even if approximately? While there exist several proposals for representing probability distributions in neural populations <xref ref-type="bibr" rid="pcbi.1003489-Pouget1">[44]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Fiser1">[45]</xref>, sampling-based methods offer a particularly suitable representational scheme. In this, each neuron corresponds to one random variable (one element of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e070" xlink:type="simple"/></inline-formula>), and thus the pattern of activities in the whole population at any particular time (the momentary ‘population vector’ <xref ref-type="bibr" rid="pcbi.1003489-Jezek1">[46]</xref>) represents one possible setting of the whole vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e071" xlink:type="simple"/></inline-formula>. The key step is to show that biologically plausible interactions between neurons can lead to stochastic network dynamics (also known as Markov chain Monte Carlo <xref ref-type="bibr" rid="pcbi.1003489-Hastings1">[47]</xref>) which, over time, visit any particular state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e072" xlink:type="simple"/></inline-formula> with just the right frequency, i.e., proportional to its probability under the posterior <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e073" xlink:type="simple"/></inline-formula> (for an illustration, see the stochastic trajectory trace in <xref ref-type="supplementary-material" rid="pcbi.1003489.s001">Fig. S1</xref>). Thus, the resulting sequence of activity patterns can be interpreted as successive samples taken from this high-dimensional posterior.</p>
<p>This representational scheme has the advantage that it is naturally suited to work when the number of variables over which a probability distribution needs to be represented is the same as the number of neurons in the system, as is conventional for associative memories. Furthermore, a sampling-based representation is also computationally appealing, as it allows the ‘best’ estimate (in the squared error sense) to be read off by simple temporal averaging, and also allows the uncertainty associated with this estimate to be characterized naturally by the variability of responses (<xref ref-type="supplementary-material" rid="pcbi.1003489.s002">Fig. S2</xref>). This uncertainty can then feed into higher order processes monitoring and modulating memory retrieval <xref ref-type="bibr" rid="pcbi.1003489-Simons1">[48]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Savin2">[49]</xref>.</p>
<p>While we discuss later some direct evidence for sampling-based representations of the posterior in the hippocampus <xref ref-type="bibr" rid="pcbi.1003489-Jezek1">[46]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Jackson1">[50]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Dupret1">[51]</xref>, we have also considered several alternative neural representations of the posterior. These include representing the most probable pattern (maximum a posteriori, or MAP, estimate) <xref ref-type="bibr" rid="pcbi.1003489-Lengyel1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Sommer1">[30]</xref>, or representing explicitly the (real-valued) probability of each neuron in the (binary) stored pattern being active (mean-field solution) <xref ref-type="bibr" rid="pcbi.1003489-Sommer1">[30]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Lengyel2">[31]</xref>. These approximations can be achieved by deterministic, attractor-like, dynamics, rather than the stochastic dynamics required by a sampling-based representation; nevertheless, we showed that the same circuit motifs arise in these cases as for the sampling case discussed below (see <xref ref-type="supplementary-material" rid="pcbi.1003489.s006">Text S1</xref> and <xref ref-type="supplementary-material" rid="pcbi.1003489.s001">Fig. S1</xref>).</p>
<p>The particular form of sampling dynamics we consider is called Gibbs sampling <xref ref-type="bibr" rid="pcbi.1003489-Dayan1">[52]</xref>, which, in general, requires the activity of each neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e074" xlink:type="simple"/></inline-formula> to be updated asynchronously by computing the probability of it being active conditioned on the current states of all other neurons (a vector denoted by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e075" xlink:type="simple"/></inline-formula>). Formally, in each update step the activity of a randomly selected neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e076" xlink:type="simple"/></inline-formula> is computed by sampling the probability <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e077" xlink:type="simple"/></inline-formula>. In our case, this is equivalent to the firing of a neuron being driven by a sigmoid transfer function (<xref ref-type="fig" rid="pcbi-1003489-g002">Fig. 2A</xref>):<disp-formula id="pcbi.1003489.e078"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e078" xlink:type="simple"/><label>(2)</label></disp-formula>with the total somatic current to the neuron, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e079" xlink:type="simple"/></inline-formula>, given as the log-odds ratio:<disp-formula id="pcbi.1003489.e080"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e080" xlink:type="simple"/><label>(3)</label></disp-formula>and the contribution of the recurrent weights themselves given by:<disp-formula id="pcbi.1003489.e081"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e081" xlink:type="simple"/><label>(4)</label></disp-formula><disp-formula id="pcbi.1003489.e082"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e082" xlink:type="simple"/><label>(5)</label></disp-formula>The expressions for the current in <xref ref-type="disp-formula" rid="pcbi.1003489.e080">equations 3</xref>–<xref ref-type="disp-formula" rid="pcbi.1003489.e082">5</xref> show the important result that the optimal way for a neuron to integrate inputs in its total somatic current is via a simple sum of neuron-specific terms (<xref ref-type="disp-formula" rid="pcbi.1003489.e080">Eq. 3</xref>): a constant bias, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e083" xlink:type="simple"/></inline-formula>, an input current from the recall cue, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e084" xlink:type="simple"/></inline-formula>, and terms that account for recurrent interactions within the network, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e085" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e086" xlink:type="simple"/></inline-formula>, which themselves can also be expressed in a simple linear form, as a sum over the synaptic partners of the neuron (<xref ref-type="disp-formula" rid="pcbi.1003489.e081">Eqs. 4</xref>–<xref ref-type="disp-formula" rid="pcbi.1003489.e082">5</xref>).</p>
<fig id="pcbi-1003489-g002" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003489.g002</object-id><label>Figure 2</label><caption>
<title>Optimal recall.</title>
<p><bold>A.</bold> Optimal neural transfer function: the total somatic current combines the recurrent contribution and a persistent external input corresponding to the recall cue. <bold>B.</bold> An example retrieval trial, from left to right: the pattern to be retrieved; the recall cue; activity of a subset of neurons during retrieval; final answer to the retrieval query obtained by temporally averaging the activity of the population; evolution of r.m.s. retrieval error over time in a trial. <bold>C.</bold> Recall performance as a function of pattern age (blue). As a reference, performance when the age of the pattern is known to the network is also shown (black, see <xref ref-type="supplementary-material" rid="pcbi.1003489.s008">Text S3</xref>). Gray filled curve shows distribution of retrieval times. <bold>D.</bold> Average performance as a function of the number of synapses per neuron in fully connected networks of different sizes (blue), or a sparsely connected network of fixed size <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e087" xlink:type="simple"/></inline-formula> varying the number of connections (red). <bold>E.</bold> Average performance as a function of average pattern age in fully-connected networks of different sizes for balanced patterns (coding level = <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e088" xlink:type="simple"/></inline-formula>, black, gray), and sparse patterns (coding level = <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e089" xlink:type="simple"/></inline-formula>, green). Dashed lines in panels C–E show, as a control, the performance of an optimised feed-forward network without synaptic plasticity (see main text for why this is a relevant upper bound on average recall error).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003489.g002" position="float" xlink:type="simple"/></fig>
<p>Note that the general functional form of our dynamics resembles a stochastic version of a canonical model of recurrent network dynamics for autoassociative recall: the Hopfield network <xref ref-type="bibr" rid="pcbi.1003489-Hopfield2">[53]</xref>. However, along with their broad conceptual similarity, there are also several distinctive features of our model that set it apart from Hopfield-like network models. First, the exact expression of the optimal current includes several terms that are not included in standard network models, but which will prove to be critical for efficient recall (<xref ref-type="supplementary-material" rid="pcbi.1003489.s007">Text S2</xref>). In turn, the same terms also correspond to biological processes not accounted for by previous models of autoassociative memory. Moreover, while making Hopfield-like neural dynamics work for the kind of realistic learning rules we are studying is very difficult, and at the very least requires considerable fine tuning of parameters <xref ref-type="bibr" rid="pcbi.1003489-Huang1">[22]</xref>, the parameters of our recall dynamics, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e090" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e091" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e092" xlink:type="simple"/></inline-formula>, are all uniquely determined by the parameters defining the input noise and the storage process (i.e. the learning rule, the statistics of the stored patterns, and the pattern age distribution, see <xref ref-type="sec" rid="s4">Methods</xref>).</p>
<p>By construction, the dynamics defined by <xref ref-type="disp-formula" rid="pcbi.1003489.e078">equations 2</xref>–<xref ref-type="disp-formula" rid="pcbi.1003489.e082">5</xref> are optimal, in the sense that they will (asymptotically) produce samples from the correct posterior distribution. But are these dynamics neurally plausible? While our dynamical equations may seem somewhat abstract, previous work has shown that a network of simple stochastic integrate-and-fire-like spiking neurons, in which each neuron receives a total somatic current that is determined by the corresponding log-odds ratio (i.e. just as in our case, see <xref ref-type="disp-formula" rid="pcbi.1003489.e080">Eq. 3</xref>), naturally implements precisely the same sampling procedure as our simpler Gibbs dynamics <xref ref-type="bibr" rid="pcbi.1003489-Buesing1">[54]</xref>.</p>
<p>Hence, as long as the total somatic current has a realistic form, the complete form of the network dynamics can also be rendered realistic. For example, although the expression for the total current is semi-local, in that it depends only on the activity of the neuron's pre- and post-synaptic partners, it assumes an unrealistic symmetry in a neuron's ability to process information through its incoming and outgoing synapses (<xref ref-type="disp-formula" rid="pcbi.1003489.e081">Eq. 4</xref>–<xref ref-type="disp-formula" rid="pcbi.1003489.e082">5</xref>). Therefore, to address this issue, along with the biological implications of other features of our model, in the following we will focus on analysing properties of the total somatic current.</p>
</sec></sec><sec id="s2b">
<title>Computational efficiency of approximately optimal recall</title>
<p>Having derived the approximately optimal dynamics for memory recall, we first study its efficiency by numerical simulations, using the simpler Gibbs dynamics (in light of their formal equivalence to a network of stochastic spiking neurons, see above). Specifically, our network dynamics proceed in discrete iterations corresponding to a full network update. In each such iteration, we first sample a random permutation to determine the order in which the neurons are to be updated, and then we update each neuron by applying <xref ref-type="disp-formula" rid="pcbi.1003489.e078">Eqs. 2</xref>–<xref ref-type="disp-formula" rid="pcbi.1003489.e082">5</xref>.</p>
<p>We first consider an example in which we store a specific pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e093" xlink:type="simple"/></inline-formula>, followed by a sequence of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e094" xlink:type="simple"/></inline-formula> random other patterns (<xref ref-type="fig" rid="pcbi-1003489-g002">Fig. 2B</xref>, left). The retrieval cue, a noisy version of the original pattern, is used both as an initial condition at the beginning of recall and, as required by <xref ref-type="disp-formula" rid="pcbi.1003489.e080">Eq. 3</xref>, also as a source of external input biasing the network throughout the retrieval process. The activity of the network is stochastic, asymptotically sampling the corresponding posterior distribution, and the output of the network, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e095" xlink:type="simple"/></inline-formula>, is taken to be the running temporal average of the network activity (<xref ref-type="fig" rid="pcbi-1003489-g002">Fig. 2B</xref>, middle). We measure retrieval performance by root-mean-squared (r.m.s.) error, which implies that the optimal response is exactly the posterior mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e096" xlink:type="simple"/></inline-formula>. Even though sampling-based dynamics may, in general, suffer from slow convergence and mixing, as we will also show below, the particular dynamics here attains its asymptotic performance in only a few time steps (<xref ref-type="fig" rid="pcbi-1003489-g002">Fig. 2B</xref>, right). A useful corollary of these dynamics is that the variability of the responses during recall also represents a computationally relevant quantity: the confidence in the correctness of the output (the average activity). Indeed, as expected from a system with a well-calibrated representation of confidence, variability correlates strongly with the actual errors made by the network (see <xref ref-type="supplementary-material" rid="pcbi.1003489.s002">Fig. S2</xref>).</p>
<p>To evaluate the overall retrieval performance of the network more systematically, we repeat the storage and retrieval procedure described above <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e097" xlink:type="simple"/></inline-formula> times. The patterns are drawn randomly from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e098" xlink:type="simple"/></inline-formula>, a uniform distribution over binary vectors, and the age of each pattern to be recalled is drawn from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e099" xlink:type="simple"/></inline-formula>, the prior over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e100" xlink:type="simple"/></inline-formula> (<xref ref-type="fig" rid="pcbi-1003489-g001">Fig. 1D</xref>, left, white curve). For each pattern, we simulate the effects of storing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e101" xlink:type="simple"/></inline-formula> other random patterns on the synaptic weights, and then run our network dynamics, by starting it from the recall cue. At the end of each recall trial, lasting 100 time steps, we measure the error (normalised Euclidean distance) between the originally stored and the recalled pattern and average the errors across all trials.</p>
<p>We compare the average performance of the optimal network to that of a ‘control’ network which is a feed-forward network that retains no information about the particular patterns that have been stored, but does perform optimal inference given the general distribution of patterns and the recall cue (first two terms in <xref ref-type="disp-formula" rid="pcbi.1003489.e029">Eq. 1</xref>). This should provide an upper bound on recall errors because it simply ignores the information in the recurrent collaterals. While this control may seem trivial, several classical recurrent autoassociative memory networks are, in fact, unable to outperform it <xref ref-type="bibr" rid="pcbi.1003489-Lengyel1">[14]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Savin1">[43]</xref> (see also <xref ref-type="supplementary-material" rid="pcbi.1003489.s003">Fig. S3</xref>).</p>
<p>The performance of our recurrent network deteriorates as a function of pattern age (<xref ref-type="fig" rid="pcbi-1003489-g002">Fig. 2C</xref>, see also <xref ref-type="supplementary-material" rid="pcbi.1003489.s008">Text S3</xref>), as expected, but the average error across pattern ages reveals that the network performs significantly better than the control (<xref ref-type="fig" rid="pcbi-1003489-g002">Fig. 2D</xref>). In line with previous work that assumed additive synaptic plasticity <xref ref-type="bibr" rid="pcbi.1003489-Hopfield1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Lengyel1">[14]</xref>, retrieval performance is ultimately determined by the number of synapses per neuron (<xref ref-type="fig" rid="pcbi-1003489-g002">Fig. 2D</xref>). Due to the limited dynamic range of synapses, recall performance is also influenced by the average pattern age, such that a larger network (with more synapses per neuron) can recall older patterns more proficiently (<xref ref-type="fig" rid="pcbi-1003489-g002">Fig. 2E</xref>). A similar rescaling of errors is observed when using more biologically plausible sparse patterns <xref ref-type="bibr" rid="pcbi.1003489-Treves1">[1]</xref> instead of the dense patterns we used in other simulations (<xref ref-type="fig" rid="pcbi-1003489-g002">Fig. 2E</xref>). In this case, the amount of information per pattern is reduced and so more patterns can be remembered. The quality of recall of the control (green dashed line) also improves, because the prior over patterns also becomes more informative (specifying <italic>a priori</italic> that most neurons should be inactive).</p>
<p>Despite the well-known advantage of the cascade model over simple two-state synapses in storing information <xref ref-type="bibr" rid="pcbi.1003489-Fusi1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Leibold1">[55]</xref>, previous work using heuristically constructed recall dynamics was unable to demonstrate a similar advantage in recall performance <xref ref-type="bibr" rid="pcbi.1003489-Huang1">[22]</xref>. Optimal dynamics confers substantial improvement in recall performance when synapses have multiple metaplastic states (<xref ref-type="fig" rid="pcbi-1003489-g003">Fig. 3A</xref>). Importantly, one of the hallmark benefits of metaplastic synapses is that the time for which they retain information after encoding scales as a power-law of the number of synapses per neuron, instead of the catastrophically poor logarithmic scaling exhibited by deterministic two-state synapses <xref ref-type="bibr" rid="pcbi.1003489-Fusi1">[17]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Fusi2">[56]</xref>. The quality of information recall in our network shows the same scaling relationships, thus retaining this crucial advantage of metaplastic synapses (<xref ref-type="fig" rid="pcbi-1003489-g003">Fig. 3B</xref>).</p>
<fig id="pcbi-1003489-g003" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003489.g003</object-id><label>Figure 3</label><caption>
<title>The advantage of metaplastic synapses.</title>
<p><bold>A.</bold> Recall performance in simple two-state (cascade depth <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e102" xlink:type="simple"/></inline-formula>) versus metaplastic (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e103" xlink:type="simple"/></inline-formula>) synapses. <bold>B.</bold> Scaling of memory span, defined as the maximum age for which patterns can be recalled reliably within the allowable error, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e104" xlink:type="simple"/></inline-formula>, for two-state (left) and metaplastic (right) synapses.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003489.g003" position="float" xlink:type="simple"/></fig>
<p>Motivated by these findings, we now turn to the question of how these approximately optimal dynamics can be implemented, and further approximated, by neural circuit dynamics. For computational convenience, we will consider moderately sized all-to-all connected networks, dense patterns, and small average pattern ages, but these results generalise to the more realistic case of large sparsely-connected networks recalling sparse memories after longer retention intervals (as suggested by <xref ref-type="fig" rid="pcbi-1003489-g002">Fig. 2D–E</xref>). We will take special care to assess the effects of sparse connectivity in those cases in which the detailed structure of the connectivity matrix can be expected to matter for recall performance.</p>
</sec><sec id="s2c">
<title>Neural implementation of approximately optimal recall</title>
<p>The dynamics defined by <xref ref-type="disp-formula" rid="pcbi.1003489.e078">equations 2</xref>–<xref ref-type="disp-formula" rid="pcbi.1003489.e082">5</xref> have two appealing properties. First, by construction, they represent an approximately optimal solution to autoassociative recall, with all the parameters of the recall dynamics being derived from those characterising memory storage and the input noise. Second, at the same time, they are in a form that is in a loose agreement with standard reduced models of single neuron dynamics (thresholded, linear summation of inputs). However, several details of the dynamics are unrealistic, and it is therefore necessary to show whether and how these details can be approximated by a neural circuit without severely compromising recall performance. Conversely, neuronal dynamics in cortical areas such as CA3, that may be involved in the recall of associative memories, exhibit features that are mysterious from the perspective of recall based on conventional, additive, plasticity rules. We consider the possibility that these features might play a role in the approximations.</p>
<p>A common theme in the approximations we are to consider is to replace an original, implausible term of the total somatic current by its statistical average. Averages can readily be taken over the activity of the population (as we will see when we consider the role of the balance between excitation and inhibition), or over the statistics of previous patterns (as we will see when we consider pre- and/or post-synaptic forms of intrinsic plasticity). In general we ask two questions about each approximation:</p>
<list list-type="bullet"><list-item>
<p>Is it efficient, i.e. is recall performance close to that seen with the exact dynamics?</p>
</list-item><list-item>
<p>Is it necessary, i.e. is it possible to achieve the same performance by an even simpler approximation?</p>
</list-item></list>
<p>The conclusion of the following sections will be that, in fact, several aspects of neural circuit organisation characterising hippocampal area CA3 can be understood as such necessary and efficient approximations.</p>
<p>It is important to note that we only explicitly model pyramidal neurons (the principal cells) in CA3, and that all other mechanisms involved in implementing approximately optimal memory recall will be described phenomenologically, in terms of their effects on the total somatic current of pyramidal neurons – which is the only computationally-relevant quantity in our model. Nevertheless, for each of these mechanisms, we will point out ways in which they may be dynamically implemented in the neural substrate and also quantify their effects in a way that allows direct comparisons with experimentally measurable quantities (see also <xref ref-type="sec" rid="s3">Discussion</xref>).</p>
<sec id="s2c1">
<title>Intrinsic plasticity</title>
<p>The most obviously unrealistic feature of the optimal recall dynamics derived above is that incoming and outgoing synapses to a neuron should both contribute directly to the total somatic current (<xref ref-type="disp-formula" rid="pcbi.1003489.e080">Eq. 3</xref>). As synaptic transmission is unidirectional, the term corresponding to the outgoing synapses, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e105" xlink:type="simple"/></inline-formula>, needs to be approximated. We investigated three different approximations of increasing complexity, all of which are based on (conditional) expectations of this quantity:<disp-formula id="pcbi.1003489.e106"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e106" xlink:type="simple"/><label>(6)</label></disp-formula><disp-formula id="pcbi.1003489.e107"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e107" xlink:type="simple"/><label>(7)</label></disp-formula><disp-formula id="pcbi.1003489.e108"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e108" xlink:type="simple"/><label>(8)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e109" xlink:type="simple"/></inline-formula> are constants defined by the statistics of the stored patterns and the learning rule (see <xref ref-type="sec" rid="s4">Methods</xref>).</p>
<p>The simplest approximation, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e110" xlink:type="simple"/></inline-formula>, replaces <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e111" xlink:type="simple"/></inline-formula> by its unconditional expectation <xref ref-type="bibr" rid="pcbi.1003489-Buesing1">[54]</xref>, which in our case is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e112" xlink:type="simple"/></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1003489.e106">Eq. 6</xref>). As it predicts no influence from outgoing synapses, we will refer to this approximation as ‘none’. The remaining two approximations are based on the conditional expectation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e113" xlink:type="simple"/></inline-formula>, conditioning on sources of information that may be available to the neuron (see below). <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e114" xlink:type="simple"/></inline-formula> is conditioned on the summed activity of the neuron's postsynaptic partners, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e115" xlink:type="simple"/></inline-formula>, and as such it is still independent of the synaptic weights of the particular neuron (<xref ref-type="disp-formula" rid="pcbi.1003489.e107">Eq. 7</xref>). The last and most sophisticated approximation, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e116" xlink:type="simple"/></inline-formula>, conditions on both <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e117" xlink:type="simple"/></inline-formula> and on the sum of the outgoing synaptic weights of the specific neuron, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e118" xlink:type="simple"/></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1003489.e108">Eq. 8</xref>). To minimise the complexity of the two more sophisticated conditional expectations, we further approximated them as sums of terms (plus a constant) that each depend linearly on one of the quantities on which they are conditioned (<xref ref-type="disp-formula" rid="pcbi.1003489.e107">Eqs. 7</xref>–<xref ref-type="disp-formula" rid="pcbi.1003489.e108">8</xref>).</p>
<p>There may thus be two quantities that need to be available to a neuron so that it can implement these approximations: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e119" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e120" xlink:type="simple"/></inline-formula>. The magnitude of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e121" xlink:type="simple"/></inline-formula> should vary over time within a recall trial, and we consider how it can be furnished by feedback inhibition in the next section. The magnitude of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e122" xlink:type="simple"/></inline-formula> is constant on the time-scale of a recall trial, and acts as a bias term shifting the transfer function of the cell in the same way as the recall cue does in <xref ref-type="fig" rid="pcbi-1003489-g002">Fig. 2A</xref>. As we will show below, such a bias can be provided by a process which corresponds to a form of intrinsic plasticity that adjusts the excitability of the cell as a function of the strength of <italic>outgoing</italic> synapses, and to which we thus refer as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e123" xlink:type="simple"/></inline-formula>. In order to assess the computational importance of these terms, and thus of the corresponding biological processes, we used numerical simulations as described above to compare the recall performance of three networks, each using one of the approximations in <xref ref-type="disp-formula" rid="pcbi.1003489.e106">equations 6</xref>–<xref ref-type="disp-formula" rid="pcbi.1003489.e108">8</xref>.</p>
<p>Comparing the three approximations we have introduced above reveals an interesting dissociation between pre- and postsynaptically-gated synaptic plasticity rules (explained in <xref ref-type="fig" rid="pcbi-1003489-g001">Fig. 1C</xref>). The two rules behave identically when using the exact recall dynamics (as expected, because the synaptic weight matrix produced by them is identical up to a transpose operation). However, the effectiveness of the different approximations depends critically on the specifics of the synaptic plasticity used for encoding (<xref ref-type="fig" rid="pcbi-1003489-g004">Fig. 4A</xref>, top).</p>
<fig id="pcbi-1003489-g004" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003489.g004</object-id><label>Figure 4</label><caption>
<title>Intrinsic plasticity.</title>
<p><bold>A.</bold> Effects of different forms of IP (rows) for different forms of synaptic plasticity (columns). Recall performance is shown for different variants of each form of IP (bars), entailing different approximations of the exact (optimal) dynamics. Dashed lines show control performance of an optimized feedforward network, as in <xref ref-type="fig" rid="pcbi-1003489-g002">Fig. 2C–E</xref>; solid lines show performance of exact dynamics, asterisks mark neural dynamics that are formally equivalent to the exact case. <bold>B.</bold> Recall performance as a function of pattern age with neuron-independent (black) and -specific (blue) variants of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e124" xlink:type="simple"/></inline-formula> for the postsynaptically-gated learning rule. Gray filled curve shows distribution of pattern age. <bold>C.</bold> Recall performance for an online implementation of the two forms of IP. <bold>D.</bold> Net change in excitability induced by the two forms of IP together as a function of time since memory storage for neurons that were either active (gray) or inactive (black) in the originally stored pattern. Lines correspond to different random sequences of consecutively stored patterns.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003489.g004" position="float" xlink:type="simple"/></fig>
<p>While the two simple solutions, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e125" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e126" xlink:type="simple"/></inline-formula>, work well for the presynaptically-gated learning rule (<xref ref-type="fig" rid="pcbi-1003489-g004">Fig. 4A</xref>, top left), they fare considerably worse for postsynaptically-gated learning (<xref ref-type="fig" rid="pcbi-1003489-g004">Fig. 4A</xref>, top right). In the presence of intrinsic plasticity, network performance becomes very close to that achieved by the exact dynamics for the postsynaptically-gated rule (<xref ref-type="fig" rid="pcbi-1003489-g004">Fig. 4A</xref>, top right), but not for the presynaptically-gated learning rule (<xref ref-type="fig" rid="pcbi-1003489-g004">Fig. 4A</xref>, top left). This latter, slightly counterintuitive, effect is due to the linear approximation of the conditional expectation (<xref ref-type="disp-formula" rid="pcbi.1003489.e108">Eq. 8</xref>), which ignores correlations between the synaptic efficacies of outgoing synapses and the activity of the postsynaptic neurons. The same linear approximation has no detrimental effect for the postsynaptically-gated rule. Moreover, the benefit of the weight-specific approximation for the postsynaptically-gated rule is particularly significant for recent patterns that could potentially be recalled well (<xref ref-type="fig" rid="pcbi-1003489-g004">Fig. 4B</xref>).</p>
<p>In sum, our theory predicts that the storage of memories by a hippocampal form of synaptic plasticity, which is postsynaptically-gated, should be accompanied by the appropriate form of IP for maintaining near-optimal performance. This IP is predicted to have a non-trivial form: it affects the presynaptic neuron (because it depends on the outgoing synapses), and it is ‘anti-homeostatic’ in that the potentiation or depression of the synapses between two cells should be accompanied by a respective increase or decrease in the excitability of the presynaptic cell. Interestingly, recent reports demonstrated a form of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e127" xlink:type="simple"/></inline-formula> that followed precisely this pattern <xref ref-type="bibr" rid="pcbi.1003489-Ganguly1">[57]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Li1">[58]</xref>.</p>
<p>The expression for the other main component of the total somatic current dictated by the optimal dynamics, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e128" xlink:type="simple"/></inline-formula> (<xref ref-type="disp-formula" rid="pcbi.1003489.e081">Eq. 4</xref>), is also problematic biologically. Although computing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e129" xlink:type="simple"/></inline-formula> only requires information about the strength of incoming synaptic weights, there is a term in it that depends on the sum of these weights directly rather than the sum of currents (weights multiplied by presynaptic activities) through the incoming synapses. As we saw above in the case of outgoing synapses, sums over synaptic weights can be approximated by adjusting neural excitability, and hence this suggests that, beside <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e130" xlink:type="simple"/></inline-formula>, there should also be a postsynaptic form of IP that regulates a neuron's excitability depending on the sum of <italic>incoming</italic> synaptic weights, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e131" xlink:type="simple"/></inline-formula>, and to which we refer as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e132" xlink:type="simple"/></inline-formula>.</p>
<p>Therefore, we again constructed three approximations to this term that were analogous to those used for outgoing synapses (<xref ref-type="disp-formula" rid="pcbi.1003489.e106">Eqs. 6</xref>–<xref ref-type="disp-formula" rid="pcbi.1003489.e108">8</xref>), but replaced <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e133" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e134" xlink:type="simple"/></inline-formula>. Interestingly, the need for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e135" xlink:type="simple"/></inline-formula> is specific to the postsynaptically-gated learning rule, as the constant factor multiplying <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e136" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e137" xlink:type="simple"/></inline-formula> vanishes for the presynaptically-gated rule, see <xref ref-type="sec" rid="s4">Methods</xref>. Unlike in the case of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e138" xlink:type="simple"/></inline-formula>, this term is homeostatic in nature (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e139" xlink:type="simple"/></inline-formula> for the postsynaptically-gated rule), with neurons becoming less excitable when many of their incoming synapses are strong. Such homeostatic regulation of the postsynaptic neuron's excitability is well documented experimentally, and is believed to play an important role in modulating neuronal activity during learning to ensure network stability <xref ref-type="bibr" rid="pcbi.1003489-Zhang1">[25]</xref>. The same principle applies in our model, as removing this regulation has catastrophic consequences for retrieval, and even replacing this term with a neuron-independent form of homeostatic regulation still impairs network performance (<xref ref-type="fig" rid="pcbi-1003489-g004">Fig. 4A</xref>, bottom right). Moreover, this impairment becomes dramatically worse in sparsely connected network (not shown). Conversely, introducing a homeostatic regulation term in the case of the presynaptically-gated learning rule has equally detrimental effects (<xref ref-type="fig" rid="pcbi-1003489-g004">Fig. 4A</xref>, bottom left). This reinforces the notion that a tight match is needed between the form of the synaptic plasticity rule storing memories and the presence and form of mechanisms regulating neural excitability.</p>
<p>Although a direct dependence of neuronal excitability on the strength of net incoming and outgoing connections, as proposed above, may seem difficult to achieve biologically, it can be well approximated by a temporal average of the incoming (or outgoing) excitatory drive to the neuron, as it is commonly formalised in standard models of IP <xref ref-type="bibr" rid="pcbi.1003489-Fldik1">[59]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Triesch1">[60]</xref>. Essentially, this requires estimating the average current into the neuron when the incoming inputs are distributed according to the prior over stored patterns (by averaging over responses during other retrieval trials, see <xref ref-type="sec" rid="s4">Methods</xref> for details). The effectiveness of this approximation depends on the time scale for integrating past activity, and needs to be at least an order of magnitude slower than the time scale on which individual memories are stored and retrieved (<xref ref-type="fig" rid="pcbi-1003489-g004">Fig. 4C</xref>). Thus, this process makes the neural threshold neuron-specific and keeps it fixed on the time scale of individual recall trials, while slowly updating it to reflect the history of patterns stored in the network. This is consistent with experimental evidence suggesting intrinsic plasticity be a slow process relative to the induction of synaptic plasticity <xref ref-type="bibr" rid="pcbi.1003489-Zhang1">[25]</xref>.</p>
<p>While postsynaptically-gated plasticity consistently predicts the need for mechanisms regulating neuronal excitability, it is not immediately clear what the net effects of the two distinct forms of IP, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e140" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e141" xlink:type="simple"/></inline-formula>, should be. In fact, at first glance, they seem to have opposite effects on neural excitability, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e142" xlink:type="simple"/></inline-formula> acting in a positive feedback loop, with neurons having a strong contribution to the drive of their postsynaptic partners becoming more excitable, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e143" xlink:type="simple"/></inline-formula> acting homeostatically, reducing neural excitability for neurons receiving many strong inputs. Predicting the net effect of the two processes is further complicated by the asymmetry in the learning rule itself, which makes it nontrivial to determine the changes in net synaptic strength into and out of a neuron. To investigate this question directly in a way that allows experimentally testable predictions, we monitored the changes in excitability in individual neurons triggered by storing a specific pattern, and the evolution of these changes with pattern age (<xref ref-type="fig" rid="pcbi-1003489-g004">Fig. 4D</xref>). We found that neuronal changes in excitability are ultimately dominated by the positive feedback process, with neurons activated or deactivated in the original storage event displaying an increase or decrease in excitability, respectively. This effect is general, and does not depend on the details of the synaptic plasticity rule as long as it is postsynaptically-gated. Furthermore, this effect is predicted to decrease with pattern age, following the time constant of synaptic forgetting.</p>
</sec><sec id="s2c2">
<title>Dynamic feedback inhibition</title>
<p>Another important consequence of the optimal retrieval dynamics derived above is that the total current to a neuron should include a negative contribution proportional to the population activity of its pre- and possibly postsynaptic partners. While earlier theoretical work already considered the importance of inhibition during retrieval <xref ref-type="bibr" rid="pcbi.1003489-Marr1">[36]</xref>, and several standard models of spike-based recurrent circuits exhibit a linear dependence of inhibition on the level of excitation <xref ref-type="bibr" rid="pcbi.1003489-vanVreeswijk1">[61]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Bernacchia1">[62]</xref>, our model advances these findings by predicting a specific form of feed-back inhibition that is both temporally and spatially specific. Temporal specificity requires that inhibition be dynamically regulated to match the level of excitation in the network. Spatial specificity requires that the level of inhibition received by each neuron should be determined by just the right pool of excitatory neurons (i.e., those ones with which it is connected). We investigated the importance of both forms of inhibitory specificity.</p>
<p>Temporal specificity in the model leads to inhibition closely tracking excitation in single neurons, with the difference in magnitude between the two reflecting the evidence in favour of the neuron having been active in the pattern to be retrieved (<xref ref-type="fig" rid="pcbi-1003489-g005">Fig. 5A</xref>). In fact, the stabilisation of neural dynamics during retrieval relies heavily on such dynamically balanced feedback inhibition. Replacing the corresponding term in the total current term by its average value, which corresponds to replacing feedback by tonic inhibition, has catastrophic consequences for retrieval performance (<xref ref-type="fig" rid="pcbi-1003489-g005">Fig. 5B</xref>), as network activity becomes unstable, and – depending on pattern age and initial conditions – either explodes or dies out altogether (<xref ref-type="fig" rid="pcbi-1003489-g005">Fig. 5C</xref>).</p>
<fig id="pcbi-1003489-g005" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003489.g005</object-id><label>Figure 5</label><caption>
<title>Dynamic feedback inhibition.</title>
<p><bold>A.</bold> Example statistics of inhibitory vs. excitatory currents to three example neurons during a recall trial. Blue: neuron correctly recalling a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e144" xlink:type="simple"/></inline-formula> bit in the originally stored pattern, correctly recalled; red: neuron correctly recalling a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e145" xlink:type="simple"/></inline-formula> bit in the originally stored pattern; gray: neuron with high variability during the trial, corresponding to an incorrectly recalled bit. Individual dots correspond to different time steps within the same recall trial. <bold>B.</bold> Effect of replacing feedback inhibition by tonic inhibition with the same average level. <bold>C.</bold> Evolution of the mean population activity during retrieval when network dynamics involve feedback (red) versus tonic inhibition (blue). Lines correspond to different trials. <bold>D.</bold> Schematic view of inhibitory connectivity in the network. Pyramidal neurons sending or receiving monosynaptic excitation (disynaptic inhibition) to example neuron 2 (black) are colored red (blue). Blue circle: local interneuron (not explicitly modeled) mediating disynaptic lateral inhibition received by neuron 2. E/I overlap is measured as the ratio of presynaptic pyramidal neurons colored both blue and red, 0% is chance. <bold>E.</bold> Total somatic current (through recurrents) to an example cell in a sparsely connected network (20% connectivity) with full (x-axis) or partial E/I overlap (y-axis, colors); different points correspond to different time steps. <bold>F.</bold> Recall performance as a function of E/I overlap. Asterisks in B and F indicate network configurations that are formally equivalent to the exact dynamics.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003489.g005" position="float" xlink:type="simple"/></fig>
<p>Spatial specificity in the model requires a precise overlap for each neuron (for example, black neuron in <xref ref-type="fig" rid="pcbi-1003489-g005">Fig. 5D</xref>) between the population of those excitatory cells that are pre- or postsynaptic to the neuron (<xref ref-type="fig" rid="pcbi-1003489-g005">Fig. 5D</xref>, neurons with red fill) and the population that provides disynaptic inhibition to it (<xref ref-type="fig" rid="pcbi-1003489-g005">Fig. 5D</xref>, neurons with blue fill). While this can be trivially guaranteed in fully connected networks, it requires considerable fine tuning in realistic, sparsely connected networks. Although inhibitory plasticity has been suggested to tune inhibitory inputs to match excitation <xref ref-type="bibr" rid="pcbi.1003489-Vogels1">[63]</xref>, it remains an open question how precisely biologically realistic synaptic plasticity of inhibitory circuits can realise such a match. Thus, we investigated the robustness of the recall dynamics to perturbations of the optimal inhibitory connectivity by systematically varying the probability that an existing source or target of monosynaptic excitation is also a source of disynaptic inhibition while keeping the total inhibitory input to each neuron constant (see <xref ref-type="sec" rid="s4">Methods</xref>).</p>
<p>Perturbing the precise pattern of inhibition needed for optimal recall acts as a source of noise in the total current to the neuron (<xref ref-type="fig" rid="pcbi-1003489-g005">Fig. 5E</xref>). This depends on the excitatory/inhibitory (E/I) overlap, which is 0% for random connectivity, 100% for a precise match. This noise translates into an impairment in retrieval performance which also varies with E/I overlap, with the network continuing to perform significantly better than control even at small degrees of overlap (<xref ref-type="fig" rid="pcbi-1003489-g005">Fig. 5F</xref>). (In these simulations, the lower bound on achievable error, given by the exact recall dynamics, is relatively high due to the reduction in the number of synapses per neuron in the sparsely connected network.) Importantly, although some information is lost due to this approximation, the dynamics remain stable to perturbations in inhibitory connectivity, suggesting that approximately optimal dynamics could be realistically implemented in sparsely connected neural circuits without an exquisitly fine tuning of inhibitory connections.</p>
</sec><sec id="s2c3">
<title>The cumulative effects of biological approximations</title>
<p>Although we have shown that individual terms of the optimal recall dynamics can be approximated via biologically plausible mechanisms with relatively small detriments in recall performance, it is unclear whether the network can still work appropriately with all these approximations in place. To test this, we constructed retrieval dynamics combining the online form of both pre- and postsynaptic IP, and assumed a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e146" xlink:type="simple"/></inline-formula> overlap between the excitatory and inhibitory input sources to neurons in the network (with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e147" xlink:type="simple"/></inline-formula> sparse network connectivity). We considered two sets of comparisons: to exact sampling dynamics, which provides a lower bound for the error rate achievable by our approximation, and to networks involving the various components individually (<xref ref-type="fig" rid="pcbi-1003489-g006">Fig. 6</xref>). We found that the performance of the biologically realistic network involving all approximations remained close to that of the optimal network. Indeed, the relative error of the whole set of approximations (compared to that of the exact sampling dynamics) was less than the sum of relative errors of the individual approximations.</p>
<fig id="pcbi-1003489-g006" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003489.g006</object-id><label>Figure 6</label><caption>
<title>Combining different circuit motifs for approximately optimal retrieval.</title>
<p>Retrieval performance with individual approximations (left), and all approximations combined (right), compared with a hypothetical scenario cumulating errors additively (middle). All networks are 50% sparsely connected. Dashed and solid lines show performance of exact dynamics and control network. Approximations used: online neuron-specific pre- (red) and postsynaptic IP (pink) with an online integration window of 10 patterns, 50% E/I overlap (yellow), all combined (blue), with additional population oscillations (green, see also <xref ref-type="fig" rid="pcbi-1003489-g007">Fig. 7</xref>).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003489.g006" position="float" xlink:type="simple"/></fig></sec></sec><sec id="s2d">
<title>Population oscillations</title>
<p>Although Gibbs sampling was an attractive starting point for deriving dynamics that both work well in practice and can be related to biologically plausible neural network dynamics <xref ref-type="bibr" rid="pcbi.1003489-Buesing1">[54]</xref>, it suffers from a major computational shortcoming: sloth. This means that a very large number of iterations may be necessary before the samples are appropriately distributed (i.e., a long burn-in time). Further, the alacrity with which Gibbs sampling explores this distribution may be limited (slow mixing). This would mean that consecutive samples are highly correlated, implying that very many of them would be needed to compute reliable expectations under the distribution <xref ref-type="bibr" rid="pcbi.1003489-Neal1">[64]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Iba1">[65]</xref> thus compounding the error in the output of our network – which is computed as just such an expectation (<xref ref-type="fig" rid="pcbi-1003489-g002">Fig. 2B</xref>). These problems become particularly acute when the posterior distribution that needs to be sampled is multimodal (when modelling hippocampal flickering, see below) or itself exhibits strong correlations (e.g. corresponding to strong coupling in frustrated Ising models). In fact, similar problems affect the alternative, deterministic mean-field or MAP dynamics (discussed in <xref ref-type="supplementary-material" rid="pcbi.1003489.s006">Text S1</xref>) which suffer from local optima and regions of the objective function that gradient-based methods find hard to traverse.</p>
<sec id="s2d1">
<title>Recalling old memories</title>
<p><xref ref-type="fig" rid="pcbi-1003489-g007">Fig. 7A</xref> (gray vs. blue) presents evidence for the infelicity of Gibbs sampling. It shows that a large fraction of the errors suffered by our network is solely due to slow convergence speed: an artificial sampler which samples the exact same posterior distribution but more efficiently <xref ref-type="bibr" rid="pcbi.1003489-Savin2">[49]</xref> (see <xref ref-type="sec" rid="s4">Methods</xref>) performs substantially better. The difference between our network and the artificial sampler is particularly striking for old memories: this is because for these, the entropy of the posterior distribution is large, and so a great number of different states needs to be visited for it to be represented fairly.</p>
<fig id="pcbi-1003489-g007" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003489.g007</object-id><label>Figure 7</label><caption>
<title>Population oscillations.</title>
<p><bold>A.</bold> Recall performance as a function of pattern age with optimal network dynamics without oscillations (blue, cf. <xref ref-type="fig" rid="pcbi-1003489-g002">Fig. 2C</xref>), with medium- (purple) or large-amplitude (red) oscillations, and with an artificial sampling algorithm (gray). <bold>B.</bold> Average recall performance with the artificial sampling algorithm (gray) and with different levels of amplitude modulation for network oscillations (amplitude 0.75 corresponds to the ‘medium oscillation’ in panel A). <bold>C.</bold> Average normalised population activity and response entropy at different phases during a cycle of a large-amplitude oscillation.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003489.g007" position="float" xlink:type="simple"/></fig>
<p>How can we improve retrieval dynamics in our neural dynamics to reduce the errors due to inefficient sampling? One potential solution, inspired by work in optimisation and sampling is to use annealing <xref ref-type="bibr" rid="pcbi.1003489-Neal1">[64]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Iba1">[65]</xref>. In this procedure, the function that needs to be navigated – in our case the (log) posterior – is gradually ‘morphed’ between an approximate form, that is easy to handle, and its original form, that is hard, with the degree of morphing being controlled by a ‘temperature’ parameter. In the context of sampling in particular, an annealing-based procedure termed tempered transitions (TT) has been proposed as a way to ensure a more efficient exploration of the state space <xref ref-type="bibr" rid="pcbi.1003489-Neal1">[64]</xref> (<xref ref-type="supplementary-material" rid="pcbi.1003489.s004">Fig. S4</xref>). This sampling procedure involves a form of oscillatory dynamics which periodically increases and decreases the ‘temperature’ parameter in way that could potentially be implemented by appropriate population oscillations in a neural circuit.</p>
<p>To construct TT-based dynamics for our problem, a naïve choice for the approximate distribution at the highest temperature would be a uniform distribution. However, while the uniform distribution is trivially easy to sample from, it is too unspecific as it retains no information about the original posterior and thus runs the risk of leading to inefficient sampling. Fortunately, there is a better option for the maximum temperature approximation: the combination of the prior over patterns and the likelihood for the recall cue. This approximation still retains important aspects of the posterior (the first two terms comprising it, see <xref ref-type="disp-formula" rid="pcbi.1003489.e029">Eq. 1</xref>) while avoiding all the correlations in the posterior of which the sole source is the likelihood of the weights (the last term in <xref ref-type="disp-formula" rid="pcbi.1003489.e029">Eq. 1</xref>). Therefore, it is a more efficient approximation than a uniform distribution, but it is equally easy to sample from (because it is fully factorised), and – as it is exactly the distribution sampled by the feed-forward network that we have used as a control – it is also readily implemented in the same recurrent circuit that represents the full posterior by simply suppressing the effects the recurrent connections.</p>
<p>As a result of using this more efficient approximation at high temperatures, the TT-based sampler results in network dynamics very similar to those corresponding to our original Gibbs sampler (<xref ref-type="disp-formula" rid="pcbi.1003489.e080">Eqs. 3</xref>–<xref ref-type="disp-formula" rid="pcbi.1003489.e082">5</xref>), with only two alterations. First, the relative contribution of recurrent inputs compared to that of external feed-forward inputs (corresponding to the recall cue) needs to be modulated in an oscillatory fashion (<xref ref-type="sec" rid="s4">Methods</xref>). Such periodic modulation could potentially be implemented by a form of shunting inhibition differentially affecting distal and proximal synapses, corresponding to feed-forward and recurrent inputs, respectively <xref ref-type="bibr" rid="pcbi.1003489-Wyble1">[34]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Klausberger2">[35]</xref>. (As above, we do not explicitly model the inhibitory population which would provide this oscillatory input, only its effects on the total somatic current of the principal cells; there exist several spiking neuron models that could generate the required signal, see e.g. <xref ref-type="bibr" rid="pcbi.1003489-Wang1">[66]</xref>). Second, the readout of population activity needs to occur at the phase of the oscillation corresponding to the original posterior (temperature = <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e148" xlink:type="simple"/></inline-formula>). This could also be achieved by an appropriate oscillatory modulation of the Schäffer collaterals (the efferent fibers of CA3, see also <xref ref-type="sec" rid="s3">Discussion</xref>).</p>
<p>One further approximation is required. In order to sample from the exact distribution, TT normally requires a step in which a possible sample taken at the lowest temperature after the whole oscillation, could be rejected as a whole. The state of all the neurons should then be returned to their original activities before the sample was created. This is a highly non-local operation in space and time, and so we made the approximation of omitting it.</p>
<p>Importantly, although the oscillatory dynamics we have introduced are only approximate, they are still helpful in speeding up convergence, allowing old memories to be retrieved substantially more competently (<xref ref-type="fig" rid="pcbi-1003489-g007">Fig. 7A</xref>, red vs. gray). Unfortunately, the same oscillatory dynamics prove to be detrimental when recalling recent memories. This is because the synaptic weights retain substantial information about these <xref ref-type="bibr" rid="pcbi.1003489-Fusi1">[17]</xref> implying that the posterior distribution is very concentrated, which is inconsistent with the over-exuberant changes in state that happen at the higher temperatures in TT. In the exact forms of this procedure, such moves are penalised by the highly concentrated posterior, leading to high rejection rates and slow dynamics. The approximate sampler, which lacks rejection, becomes less accurate. Therefore, there is an inherent trade-off in the utility of oscillations: the more useful they are for recalling remote memories, the more damaging they are for the recall of recent memories. Parametrically varying the amplitude of the oscillations reveals that an intermediate oscillatory strength, where the dynamics take into account recurrent inputs throughout the cycle (see <xref ref-type="sec" rid="s4">Methods</xref>), best resolves this tradeoff (<xref ref-type="fig" rid="pcbi-1003489-g007">Fig. 7A</xref>, purple and <xref ref-type="fig" rid="pcbi-1003489-g007">Fig. 7B</xref>).</p>
<p>Generating network oscillations is almost unavoidable in a network combining excitatory and inhibitory neurons. However, the kind of oscillations we employ here have several characteristic signatures that can be used for experimentally validating our predictions. In particular, since the oscillation phase controls the temperature used to anneal the posterior distribution, the activity at the trough of the oscillation (phase <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e149" xlink:type="simple"/></inline-formula>, highest temperature) should correspond to samples from a broader distribution (<xref ref-type="supplementary-material" rid="pcbi.1003489.s004">Fig. S4</xref>). Hence, neural responses at the trough of the oscillation should be more variable than those at the peak. Indeed, if we measure the average entropy of the responses in the network as a function of the phase of the oscillation, response variability is predicted to be modulated with the period of the underlying oscillation, with most variability at the trough (<xref ref-type="fig" rid="pcbi-1003489-g007">Fig. 7C</xref>, bottom). An intriguing prediction that the overall level of population activity should be modulated much more weakly by the same oscillation (<xref ref-type="fig" rid="pcbi-1003489-g007">Fig. 7C</xref>, top). This is because, in the model, oscillations improve convergence speed by periodically modulating the spread of the distribution from which the network needs to sample (<xref ref-type="supplementary-material" rid="pcbi.1003489.s004">Fig. S4A</xref>), rather than by biasing it in any particular way, e.g. towards higher firing rates.</p>
</sec><sec id="s2d2">
<title>Representing spatial ambiguity</title>
<p>We have argued that oscillations help the network explore a broad posterior resulting from limited information in the synapses. Another particularly revealing regime involves multimodal posteriors. Such distributions might arise when animals receive conflicting cues, for instance, after an instantaneous change in spatial context. In this scenario, current sensory inputs suggest that the animal is in a new context, while the generally correct assumption that spatial contexts are contiguous in time suggests that the animal is still in the previous context, thereby creating substantial spatial ambiguity.</p>
<p>The effects of spatial ambiguity have recently been examined in experiments recording place cells in rats experiencing just such rapid and abrupt changes between different spatial contexts <xref ref-type="bibr" rid="pcbi.1003489-Jezek1">[46]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Jackson1">[50]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Dupret1">[51]</xref>. Immediately following a switch in spatial context, and before hippocampal activity settled to representing the new context, transient flickering was observed, in which there was rapid switching back and forth between the recall states representing the previous and the new context (<xref ref-type="fig" rid="pcbi-1003489-g008">Fig. 8A</xref>; top), in a manner that was paced by oscillations in the theta range <xref ref-type="bibr" rid="pcbi.1003489-Jezek1">[46]</xref> (or, in a different experiment, the gamma range <xref ref-type="bibr" rid="pcbi.1003489-Dupret1">[51]</xref>).</p>
<fig id="pcbi-1003489-g008" position="float"><object-id pub-id-type="doi">10.1371/journal.pcbi.1003489.g008</object-id><label>Figure 8</label><caption>
<title>Network flickering.</title>
<p><bold>A.</bold> Hippocampal population dynamics during a single retrieval trial, reproduced from Ref. <xref ref-type="bibr" rid="pcbi.1003489-Jezek1">[46]</xref>. Correlation of the instantaneous population vector to the stereotypical responses of the network in the two contexts are shown (red vs. blue) Top: flickering (box) following the switching of visual cues at time 0 (green vertical line), bottom: spontaneous flickering (box) without external cue switching. <bold>B.</bold> Dynamics of population responses in the model showing flickering (boxes) after cue switching (top), and spontaneously, without cue switching (bottom).</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pcbi.1003489.g008" position="float" xlink:type="simple"/></fig>
<p>When the effective recall cue is the recent history of sensory inputs (which is statistically appropriate since spatial location should only change slowly and smoothly under normal circumstances, see <xref ref-type="sec" rid="s4">Methods</xref>), our network also generates transient flickering (<xref ref-type="fig" rid="pcbi-1003489-g008">Fig. 8B</xref>, top). In fact, as the net information available in the cue always remains limited, it never perfectly excludes other contexts, such that transient flickering can also be observed without switching, albeit much less frequently (<xref ref-type="fig" rid="pcbi-1003489-g008">Fig. 8B</xref>, bottom). Such spontaneous flickers were also observed in the original experiment (<xref ref-type="fig" rid="pcbi-1003489-g008">Fig. 8A</xref>, bottom; <xref ref-type="bibr" rid="pcbi.1003489-Jezek1">[46]</xref>).</p>
</sec></sec></sec><sec id="s3">
<title>Discussion</title>
<p>A venerable history of classical theoretical work on hippocampal area CA3 accounted for many of the architectural and anatomical features of this area in terms of how they support its function as an autoassociative memory system <xref ref-type="bibr" rid="pcbi.1003489-Treves1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Treves2">[2]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Marr1">[36]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-McNaughton1">[67]</xref>. However, the <italic>dynamical</italic> behaviour of CA3 has so far escaped such a theoretical treatment. Indeed, unlike the simple dynamics of theoretical models for autoassociative memory recall <xref ref-type="bibr" rid="pcbi.1003489-Hopfield1">[4]</xref>, the dynamics of hippocampal networks implementing this function are dauntingly complex. Individual neurons change their integration properties on multiple time scales <xref ref-type="bibr" rid="pcbi.1003489-Moyer1">[24]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Zhang1">[25]</xref>, the activity of pyramidal cells is modulated by a plethora of functionally specialised inhibitory neurons <xref ref-type="bibr" rid="pcbi.1003489-Klausberger1">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Lapray1">[27]</xref>, each with its own intrinsic dynamics and connectivity properties<xref ref-type="bibr" rid="pcbi.1003489-Freund1">[32]</xref>, innervating distinct domains of pyramidal cells <xref ref-type="bibr" rid="pcbi.1003489-Kullmann1">[28]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Freund1">[32]</xref> and inducing task-specific oscillations in several frequency bands <xref ref-type="bibr" rid="pcbi.1003489-Klausberger1">[26]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Kullmann1">[28]</xref>. Here, we have shown that it is possible to dissect some of this complexity in light of the circuit approximating optimal auto-associative memory recall. Nevertheless, it should be noted that there is still more to be said about the contribution of different gross neuroanatomical features of the hippocampus (and CA3) to associative memory, as well as the roles that various cell types may play in it. Addressing these questions were outside the scope of the present study and thus remain the subject of future work.</p>
<sec id="s3a">
<title>Distinctive features</title>
<p>The recall dynamics in our theory share some of the basic features of standard autoassociative memory networks (recurrent excitation, linear inhibition <xref ref-type="bibr" rid="pcbi.1003489-Hopfield1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Hertz1">[8]</xref>), but refine them in several critical ways. First, traditional approaches require considerable fine tuning of parameters for scenarios different from the standard Hopfield network (storing binary patterns with the additive ‘covariance’ learning rule). In our approach, the basic form of the network dynamics during recall is fully specified by the statistical properties of the recall cue and the storage process, with no free parameter left to be tuned. (Note, though, that tuning the amplitude of population oscillations in the more sophisticated, tempered transition dynamics, might be useful to improve the speed of convergence.) This allowed us to include the effects of a markedly non-uniform prior over the delay after which a memory needs to be recalled, motivated by human forgetting data <xref ref-type="bibr" rid="pcbi.1003489-Anderson1">[42]</xref>, in contrast to traditional autoassociative memory models that assume, mostly implicitly, an (improper) uniform distribution over a finite range of recall delays (see also <xref ref-type="supplementary-material" rid="pcbi.1003489.s007">Text S2</xref>).</p>
<p>Second, our theory provides an explicit prescription for how the excitability of neurons should be regulated depending on the efficacies of both their incoming and outgoing synapses. Akin to standard approaches, this means that excitability should depend on all previously stored patterns. However, while previous proposals for adjusting neuronal excitability require a somewhat heuristic offline procedure where the full list of stored patterns needs to be known <xref ref-type="bibr" rid="pcbi.1003489-BenDayanRubin1">[38]</xref>, we were able to show that the appropriate regulation of neural excitability in our system can be well approximated by commonly-assumed online forms of IP <xref ref-type="bibr" rid="pcbi.1003489-Triesch1">[60]</xref> resulting in competent recall performance.</p>
<p>Third, while standard approaches only consider deterministic dynamics, our dynamics are stochastic. This makes it straightforward to optimise network performance for a squared-error loss, and additionally allows for a simple representation of uncertainty, thereby also naturally accounting for hippocampal flickering phenomena (see also below).</p>
<p>Finally, in keeping with other statistical treatments of autoassociative memory, which, however, number only few <xref ref-type="bibr" rid="pcbi.1003489-Sommer1">[30]</xref>, the recall cue modulates the network dynamics throughout retrieval (as an external field) rather than just being an initial condition. Its relative contribution to the dynamics reflects its quality (or noisiness).</p>
</sec><sec id="s3b">
<title>Storage versus recall performance</title>
<p>One powerful, yet biologically-untenable, simplification made by previous autoassociative memory models <xref ref-type="bibr" rid="pcbi.1003489-Hopfield1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Hertz1">[8]</xref>–<xref ref-type="bibr" rid="pcbi.1003489-Amit3">[10]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Lengyel1">[14]</xref> is the use of additive learning rules. The memory capacity of such networks is linear in the number of synapses per neuron <xref ref-type="bibr" rid="pcbi.1003489-Treves1">[1]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Hopfield1">[4]</xref>, but at the cost of unrealistic synaptic and neural dynamics. At the other extreme, the importance of bounded synaptic plasticity has been investigated using synapses with extremely limited dynamic ranges, including synapses with only two states <xref ref-type="bibr" rid="pcbi.1003489-Willshaw1">[7]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Sommer1">[30]</xref>. While the capacity of such networks was shown to be disappointingly poor, recent work has shown that metaplastic synapses can store memories far more efficiently than synapses with the same range of efficacies but without metaplasticity <xref ref-type="bibr" rid="pcbi.1003489-Fusi1">[17]</xref>. As theoretical work investigating the role of metaplastic synapses in memory has so far concentrated on the benefits for <italic>storing</italic> information, it has been unclear how these benefits can be translated into <italic>recall</italic> performance – which is what ultimately matters for the organism (after all, there is not much point in storing memories if they cannot be recalled). Surprisingly, almost no work has considered the quality of memory recall from metaplastic synapses, with the notable exception of Ref. <xref ref-type="bibr" rid="pcbi.1003489-Huang1">[22]</xref> who found only very modest improvements compared with the recall performance of simple two-state synapses. Thus, it had remained unclear if the benefits of metaplasticity in terms of information stored can be translated into recall performance.</p>
<p>We have shown that with appropriate recall dynamics, recall performance can in fact be substantially improved using metaplastic synapses (without explicit optimisation of the synaptic plasticity rule used for storage), avoiding the characteristic of simple two-state synapses that they exhibit catastrophically poor logarithmic scaling of memory life time with the number of synapses (<xref ref-type="fig" rid="pcbi-1003489-g003">Fig. 3</xref>). While our measures of performance are currently based on numerical simulations, it may be possible to apply and extend the analytical approaches originally developed for computing the recall capacity of simpler network dynamics <xref ref-type="bibr" rid="pcbi.1003489-Huang1">[22]</xref> to provide a systematic analysis of the performance of our optimal network dynamics.</p>
<p>The performance of our recall dynamics follows the qualitative trends predicted by earlier analyses of metaplastic synapses <xref ref-type="bibr" rid="pcbi.1003489-Fusi1">[17]</xref>. However, there remain some quantitative discrepancies: for example, the cascade depth at which stored information is maximised is not the same as that at which recall error is minimised (<xref ref-type="supplementary-material" rid="pcbi.1003489.s005">Fig. S5</xref>). There may be several sources of these discrepancies. First, the degree to which our approximately optimal recall dynamics is able to make use of the information that is stored in the synapses may depend on the parameters of the system. Second, analyses of stored information typically quantify information in single synapses (using measures such as the signal-to-noise ratio, SNR), while recall error (e.g. fraction of correctly recalled bits for whole patterns) is a result of the information stored jointly in all synapses. These metrics may themselves only be related to each other in a complex and nonlinear manner. For example, when synaptic weights are correlated, these two information measures will differ in general. In this work, we have side-stepped this issue by using an approximation which treats synapses in the network as independent given a particular pattern has been stored. This is formally incorrect in statistical terms, as we expect dependencies between synapses sharing a pre- and post- synaptic partners. Indeed, weak but significant correlations are observed between such synapses in the cortex <xref ref-type="bibr" rid="pcbi.1003489-Song1">[68]</xref>. It will be an important next step to explicitly consider these statistical dependencies and their significance for memory retrieval <xref ref-type="bibr" rid="pcbi.1003489-Savin1">[43]</xref>. More importantly, however, these measures implicitly quantify performance on fundamentally different tasks: while SNR is appropriate for measuring recognition performance, i.e. the error in making the relatively simple binary judgement on whether a particular (and noiseless) pattern has been stored in the past <xref ref-type="bibr" rid="pcbi.1003489-Fusi1">[17]</xref>, our central interest has been recollection performance, i.e. the error on the much more demanding task of recalling the details of a high-dimensional pattern from noisy input <xref ref-type="bibr" rid="pcbi.1003489-Hopfield1">[4]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Huang1">[22]</xref>.</p>
</sec><sec id="s3c">
<title>Intrinsic plasticity</title>
<p>Our model predicts changes in neuronal excitability that can be traced back to the specifics of CA3 synaptic plasticity (i.e. the NMDA-receptor dependence of learning). In particular, we expect that the excitability of individual neurons should constantly change as a function of the state of the incoming and outgoing connections to and from the neuron. A range of experiments has long confirmed the homeostatic regulation of a neuron's responsiveness to injected current after chronic manipulations of network activity, corresponding to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e150" xlink:type="simple"/></inline-formula> in the model <xref ref-type="bibr" rid="pcbi.1003489-Zhang1">[25]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Desai1">[33]</xref>. More remarkably, recent evidence confirmed that neuronal excitability is also modulated by the strength of a neuron's outgoing connections <xref ref-type="bibr" rid="pcbi.1003489-Ganguly1">[57]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Li1">[58]</xref>, closely matching the predictions of our model for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e151" xlink:type="simple"/></inline-formula>: not only do the shifts in presynaptic neuron excitability follow the apparently anti-homeostatic direction predicted (increases after LTP, reduction after LTD) <xref ref-type="bibr" rid="pcbi.1003489-Ganguly1">[57]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Li1">[58]</xref>, but this form of plasticity was also shown to be specific to excitatory-to-excitatory connections <xref ref-type="bibr" rid="pcbi.1003489-Li1">[58]</xref>, as required by the theory. To our knowledge, we are the first to ascribe a functional role to such presynaptic IP. Furthermore, while homeostatic plasticity has been introduced in some models as a heuristic addition to the network dynamics, meant to enhance stability during learning <xref ref-type="bibr" rid="pcbi.1003489-Fldik1">[59]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Triesch1">[60]</xref>, here it is derived from first principles, as a necessity for optimal recall.</p>
<p>Our model also offers insights into some of the paradoxical findings surrounding IP. Namely, while homeostatic IP can be robustly expressed <italic>in vitro</italic> by pharmacological manipulations <xref ref-type="bibr" rid="pcbi.1003489-Zhang1">[25]</xref>, as we noted, the changes in excitability reported <italic>in vivo</italic> after more naturalistic manipulations (e.g. after learning) are typically anti-homeostatic <xref ref-type="bibr" rid="pcbi.1003489-Thompson1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Moyer1">[24]</xref> (see also <xref ref-type="bibr" rid="pcbi.1003489-Zhang1">[25]</xref>). Our results suggest that, although different experimental manipulations may preferentially expose one or the other mechanism (see <xref ref-type="bibr" rid="pcbi.1003489-Zhang1">[25]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Ganguly1">[57]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Li1">[58]</xref>), both are necessary for circuit function, and that the presynaptic anti-homeostatic component dominates overall (<xref ref-type="fig" rid="pcbi-1003489-g004">Fig. 4D</xref>). This would explain non-homeostatic increases in neural excitability in the hippocampus after hippocampus-dependent learning <xref ref-type="bibr" rid="pcbi.1003489-Thompson1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Moyer1">[24]</xref>.</p>
<p>Our model offers an equilibrium theory – we expect constancy of neural excitability over the long-run, irrespective of the details of synaptic plasticity, at least as long as there exists a stationary distribution for the weights. Such a balance is consistent with experimental findings about spatial learning preserving the global firing rate of the network <xref ref-type="bibr" rid="pcbi.1003489-Dragoi1">[69]</xref>. However, it is not obviously consonant with the observations of net shifts in excitability that have been measured across a population of CA3 neurons following learning <xref ref-type="bibr" rid="pcbi.1003489-Thompson1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Moyer1">[24]</xref>. One possibility is that this comes from a detection bias given sparse population patterns, such as those observed in CA3 <xref ref-type="bibr" rid="pcbi.1003489-Moser1">[70]</xref>. That is, for such populations, we predict that the overall balance in excitability is achieved by large increases in excitability in the small subset of neurons that are active in the pattern, accompanied by small decrements of excitability in the inactive population of neurons. If the larger changes are preferentially detected (e.g. simply due to signal-to-noise constraints in recordings), the changes in excitability that will be evident will be positive but not negative. Indeed, the pattern of experimental reports follows this trend: not all neurons recorded during the course of an experiment show detectable changes in excitability, but when they do, those changes are positive <xref ref-type="bibr" rid="pcbi.1003489-Thompson1">[23]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Zhang1">[25]</xref>. The model also makes the novel prediction that differential shifts in excitability should be recorded after separating neurons based on their activity in the pattern being stored. Using indicators of immediate early expression gene (c-Fos) expression to generate a lasting tag for the neurons that are active during the encoding of a particular memory (when the animal is exposed to a novel environment <xref ref-type="bibr" rid="pcbi.1003489-Liu1">[71]</xref>) should make it possible to probe the excitability of these neurons at various retrieval delays, thus directly testing our prediction for the temporal evolution of excitability following memory storage (<xref ref-type="fig" rid="pcbi-1003489-g004">Fig. 4D</xref>).</p>
</sec><sec id="s3d">
<title>Feedback inhibition</title>
<p>Another key prediction of our model concerns the structure of the inhibitory circuitry that provides feedback inhibition in CA3 (most likely by fast-spiking basket cells <xref ref-type="bibr" rid="pcbi.1003489-Mori1">[72]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Freund2">[73]</xref>). In particular, optimal recall dynamics require a form of feedback inhibition that dynamically tracks excitation, but without the need for tonic levels of excitation and inhibition to be tightly balanced. This mode of operation is fundamentally different from previously proposed theories of E/I balance in cortical circuits requiring tonic excitation and inhibition to match <xref ref-type="bibr" rid="pcbi.1003489-vanVreeswijk1">[61]</xref>, because it is the very difference between tonic excitation and inhibition levels (on the time scale of a recall trial) that carries the information about the identity of the pattern that needs to be recalled (and about the confidence in this pattern). The network is therefore operating in a rather different regime from other work on associative memory in balanced spiking networks which has considered additive synaptic plasticity <xref ref-type="bibr" rid="pcbi.1003489-Roudi1">[74]</xref>. In our model, when the stored patterns are sparse, this translates into inhibition dominating neural responses, as reported for sensory responses in awake (but not anesthesized) mice, at least in V1 <xref ref-type="bibr" rid="pcbi.1003489-Haider1">[75]</xref>. It also predicts a net shift between excitation and inhibition within the same neuron depending on the memory being retrieved, consistent with a shift in the average membrane potential of hippocampal place cells depending on whether the animal is inside or outside their place field <xref ref-type="bibr" rid="pcbi.1003489-Harvey1">[76]</xref>. At a finer temporal resolution, we also expect that fluctuations in excitation and inhibition are closely correlated. There is evidence for this in neocortical recordings <xref ref-type="bibr" rid="pcbi.1003489-Haider1">[75]</xref> but a test of this prediction in the hippocampus has yet to be performed.</p>
<p>At the level of the underlying hippocampal circuitry, the model predicts a high degree of overlap between a neuron's monosynaptic excitatory and disynaptic inhibitory partners, which could, in principle, be detected anatomically <xref ref-type="bibr" rid="pcbi.1003489-Wittner1">[77]</xref> or functionally <xref ref-type="bibr" rid="pcbi.1003489-Miles1">[78]</xref>. Indeed, recordings in behaving rats confirm a close functional coupling between excitatory and inhibitory cell populations <xref ref-type="bibr" rid="pcbi.1003489-Maurer1">[79]</xref>. Moreover, as the underlying recurrent connectivity is modified, e.g. during learning, the inhibitory circuitry should be plastic as well, on a time course similar to that of learning at excitatory synapses. One recent experiment demonstrates that, at least in CA1, such structural plasticity of inhibitory connections does accompany the induction of (synaptic and structural) plasticity at the excitatory synapses <xref ref-type="bibr" rid="pcbi.1003489-Bourne1">[80]</xref>. At the level of synaptic plasticity, theoretical models of excitatory-inhibitory networks have already predicted the dynamical matching of excitatory and inhibitory inputs in individual excitatory cells <xref ref-type="bibr" rid="pcbi.1003489-Vogels1">[63]</xref>. A recent experiment found evidence for this by measuring the profile of inhibition during learning of a new spatial representation <xref ref-type="bibr" rid="pcbi.1003489-Dupret1">[51]</xref>. This experiment revealed a reconfiguration of inhibitory activity that mirrored the reorganization of excitatory activity during place map formation, as we would expect from a process actively matching excitation with inhibition.</p>
</sec><sec id="s3e">
<title>Oscillations</title>
<p>We have shown that a periodic modulation of the relative contribution of external versus recurrent inputs facilitates the exploration of the state space of the network, and hence improves performance when there is limited time to answer a recall query. Such periodic modulation of extrinsic vs. recurrent inputs has been anticipated to be useful in the rather specific context of sequence disambiguation <xref ref-type="bibr" rid="pcbi.1003489-Sohal1">[81]</xref> but its general utility for memory recall under time-pressure is a novel aspect of our model.</p>
<p>The computational role we ascribe to oscillations leads to a number of predictions that are unique to our theory. First, the main effect of oscillations in CA3 should be on the variability rather than the rates of pyramidal cell responses (<xref ref-type="fig" rid="pcbi-1003489-g007">Fig. 7C</xref>). This points to gamma oscillations as potential biological substrates because they have only weak effects on the firing rates of CA3 pyramidal cells <xref ref-type="bibr" rid="pcbi.1003489-Csicsvari1">[82]</xref>. Second, the transmission of information to read-out areas of CA3, most prominently to pyramidal cells in hippocampal area CA1, should also be strongly modulated by the oscillation, because only samples from the target distribution at the peak of the underlying oscillation (corresponding to temperature = <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e152" xlink:type="simple"/></inline-formula>) are correctly representing the pattern that needs to be recalled. This means that CA3 input into CA1 should be periodically gated such that it impacts CA1 preferentially at this phase of the oscillation, which is consistent with gamma modulation of population rates being stronger in CA1 than in CA3 pyramidal cells <xref ref-type="bibr" rid="pcbi.1003489-Csicsvari1">[82]</xref>. Further evidence for this oscillatory coordination between CA3 and CA1 is that their in-phase synchronization in the lower gamma band is a signature of coordinated memory reactivation across the hippocampal network <xref ref-type="bibr" rid="pcbi.1003489-Colgin1">[83]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Carr1">[84]</xref>, and in particular of the transfer of information between them <xref ref-type="bibr" rid="pcbi.1003489-Carr1">[84]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Montgomery1">[85]</xref>. This analysis does not delimit a role for theta oscillations.</p>
<p>Another novel prediction of the theory is that response variability across the CA3 pyramidal cell population (measured, for instance, by the entropy of their responses across trials) should depend on the phase of gamma oscillations. This can be directly tested using multielectrode hippocampal recordings in awake behaving animals, pooling data across trials in which the same item is being recalled (e.g. the same spatial position is being traversed), and measuring the variability across such trials as a function of the phase of the simultaneously recorded gamma oscillation.</p>
<p>Mechanistically, our model of oscillations requires a rhythmic modulation of the different excitatory inputs to pyramidal cells in CA3, affecting the relative contribution of recurrent versus perforant path inputs. While the specific mechanisms achieving this effect remain unclear, recent evidence suggests that at least two classes of inhibitory neurons – bistratified <xref ref-type="bibr" rid="pcbi.1003489-Klausberger1">[26]</xref> and oriens-lacunosum moleculare (OLM) cells <xref ref-type="bibr" rid="pcbi.1003489-Leo1">[86]</xref> – can rhythmically modulate external versus recurrent inputs to pyramidal cells, as would be required in our model. As OLM cells show strong modulation by gamma <xref ref-type="bibr" rid="pcbi.1003489-Varga1">[87]</xref>, they seem to be ideally placed to play this role.</p>
<p>Lastly, our analysis of retrieval performance revealed an inherent tradeoff between the utility of oscillations when exploring complex posterior distributions (very wide for old patterns or multimodal as in the case of the flickering experiment) and their detrimental effects when the correct answer is very clear (the posterior is sharp and unimodal, as for recent patterns). It is tempting to speculate that the amplitude of gamma oscillations could be modulated with task difficulty (estimated by some measure of response confidence, which is readily provided in a sampling based representation) to optimise retrieval performance. Indirect evidence for this comes from chronic recordings in the human hippocampus showing increased gamma (and theta) power for retrieving remote versus recent autobiographical memories <xref ref-type="bibr" rid="pcbi.1003489-Steinvorth1">[88]</xref>.</p>
</sec><sec id="s3f">
<title>Representation of uncertainty</title>
<p>One key aspect of our theory is that the uncertainty about the patterns that are being recalled is represented along with the patterns themselves. This facilitates recall within the network, and it is also essential for downstream functions such as decision-making, for which evidence from recalled memories has to be combined with other, e.g. perceptual, sources of information – weighting each source of information with their respective certainties <xref ref-type="bibr" rid="pcbi.1003489-Fiser1">[45]</xref>. The behavioural ability to assess confidence in a retrieved memory trace has been demonstrated in various species, including humans <xref ref-type="bibr" rid="pcbi.1003489-Smith1">[89]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Smith2">[90]</xref>. We proposed that this is underpinned by a sampling-based neural code for uncertainty in the hippocampus <xref ref-type="bibr" rid="pcbi.1003489-Fiser1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Berkes1">[91]</xref>. Although the neural dynamics considered here are highly simplified, recent theoretical work has shown that the dynamics of more realistic leaky integrate-and-fire neurons can closely approximate those required by Gibbs sampling used here <xref ref-type="bibr" rid="pcbi.1003489-Buesing1">[54]</xref>.</p>
<p>We showed that a sampling-based representation can explain some puzzling experimental observations revealing transient flickering in population responses following an instantaneous transformation of the spatial context <xref ref-type="bibr" rid="pcbi.1003489-Jezek1">[46]</xref>. In order to capture this flickering in traditional attractor dynamics, high levels of input noise would need to be assumed. However, in the actual experiments, special care was taken to make the cues for the individual environments as reliable as possible, so that the animals faced a problem of ambiguity rather than noise. According to our theory, hippocampal flickering is a variant of bistable ‘spatial perception’, and as such can be viewed as a signature of the dynamics exploring different modes of the posterior, each corresponding to one of the stored memories. Bistability poses a particular challenge to attractor dynamics, which actively eliminate ambiguity by a winner-take-all mechanism. Conversely, sampling-based representations have been used to account for a host of perceptual and neural phenomena surrounding bistable perception <xref ref-type="bibr" rid="pcbi.1003489-Fiser1">[45]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-Sundareswara1">[92]</xref>–<xref ref-type="bibr" rid="pcbi.1003489-Reichert1">[95]</xref>. If our sampling-based interpretation of flickering is correct, then it should be possible to modulate the degree of flickering, and the distribution of dwell-times for the individual representations, by experimentally manipulating sources of uncertainty (the reliability of sensory cues, or the prior probabilities of the animal finding itself in any one of the possible environments).</p>
</sec><sec id="s3g">
<title>Conclusions</title>
<p>In sum, our work makes two important contributions. First, it shows for the first time that high-quality recall from metaplastic synapses is at all possible with neurally plausible dynamics. Second, the resulting recall dynamics involve several critical motifs that had not been predicted by standard approaches, and yet map onto known features of hippocampal dynamics. Thus the model provides insights into the computational role of several aspects of hippocampal activity and allows us to make a range of novel, experimentally testable, predictions.</p>
</sec></sec><sec id="s4" sec-type="methods">
<title>Methods</title>
<sec id="s4a">
<title>Pattern and input statistics</title>
<p>We model a network of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e153" xlink:type="simple"/></inline-formula> neurons, with connectivity defined by matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e154" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e155" xlink:type="simple"/></inline-formula> if there is a synapse from neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e156" xlink:type="simple"/></inline-formula> to neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e157" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e158" xlink:type="simple"/></inline-formula>, otherwise. To control connectivity (<xref ref-type="fig" rid="pcbi-1003489-g002">Figs. 2E</xref>, <xref ref-type="fig" rid="pcbi-1003489-g005">5E–F</xref>, and <xref ref-type="fig" rid="pcbi-1003489-g006">6</xref>), a randomly selected <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e159" xlink:type="simple"/></inline-formula> fraction of elements in <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e160" xlink:type="simple"/></inline-formula> was set to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e161" xlink:type="simple"/></inline-formula> and the rest to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e162" xlink:type="simple"/></inline-formula>. The corresponding synaptic efficacies are binary and defined by matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e163" xlink:type="simple"/></inline-formula>, which is obtained as the result of storing a sequence of patterns <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e164" xlink:type="simple"/></inline-formula> by the cascade learning rule (see below). The patterns are also binary and, consistent with data suggesting that the inputs to the CA3 network are decorrelated by the dentate gyrus <xref ref-type="bibr" rid="pcbi.1003489-Deng1">[96]</xref>, we assume individual bits in a pattern to be independent, such that the distribution of the stored patterns factorizes over neurons (and also, implicitly, over patterns):<disp-formula id="pcbi.1003489.e165"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e165" xlink:type="simple"/><label>(9)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e166" xlink:type="simple"/></inline-formula> is the pattern density, or coding level.</p>
<p>Finally, the recall cue is a noisy version of the original pattern, corrupted by independent noise modelled as a binary symmetric channel:<disp-formula id="pcbi.1003489.e167"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e167" xlink:type="simple"/><label>(10)</label></disp-formula><disp-formula id="pcbi.1003489.e168"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e168" xlink:type="simple"/><label>(11)</label></disp-formula>with parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e169" xlink:type="simple"/></inline-formula> describing the probability of a bit in the original pattern being flipped in the recall cue.</p>
<p>Pattern age <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e170" xlink:type="simple"/></inline-formula> is assumed to be distributed geometrically with mean <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e171" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003489.e172"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e172" xlink:type="simple"/><label>(12)</label></disp-formula></p>
</sec><sec id="s4b">
<title>Cascade rule</title>
<p>Learning is stochastic and local, with changes in the state of a synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e173" xlink:type="simple"/></inline-formula> being determined only by the activation of the pre- and postsynaptic neurons, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e174" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e175" xlink:type="simple"/></inline-formula> and the current value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e176" xlink:type="simple"/></inline-formula>. Following the presentation of a pattern with activation <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e177" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e178" xlink:type="simple"/></inline-formula>, the synaptic state transitions from the current state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e179" xlink:type="simple"/></inline-formula> to to the new state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e180" xlink:type="simple"/></inline-formula>. In the most general form, the probability of a synapse changing between any two states can be defined through a set of transition matrices <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e181" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e182" xlink:type="simple"/></inline-formula>, which leads to a large number of model parameters. A natural way to reduce this number is to define a transition matrix for potentiating, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e183" xlink:type="simple"/></inline-formula>, and depressing, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e184" xlink:type="simple"/></inline-formula>, events and separately map different neuron activation pairs into such events, possibly with some pairs leading to no change. Here, we assume a postsynaptically-gated rule, where the co-activation of pre- and post- neuron leads to potentiation, while an active postsynaptic neuron causes depression if the presynaptic neuron is silent, i.e. <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e185" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e186" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e187" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e188" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e189" xlink:type="simple"/></inline-formula> denoting the identity matrix. For comparison, we also use the traditionally assumed presynaptically-gated learning rule <xref ref-type="bibr" rid="pcbi.1003489-Huang1">[22]</xref>, <xref ref-type="bibr" rid="pcbi.1003489-BenDayanRubin1">[38]</xref>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e190" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e191" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e192" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e193" xlink:type="simple"/></inline-formula>.</p>
<p>We express the two transition matrices <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e194" xlink:type="simple"/></inline-formula> using a generalization of Fusi et al.'s 2005 cascade model <xref ref-type="bibr" rid="pcbi.1003489-Fusi1">[17]</xref>, parametrized by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e195" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e196" xlink:type="simple"/></inline-formula> and the cascade depth <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e197" xlink:type="simple"/></inline-formula>. We index states corresponding to weak and strong synapses with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e198" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e199" xlink:type="simple"/></inline-formula>, respectively (<xref ref-type="fig" rid="pcbi-1003489-g001">Fig. 1C</xref>). We describe the elements of the transition matrix, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e200" xlink:type="simple"/></inline-formula>, as a sum of two terms: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e201" xlink:type="simple"/></inline-formula> describing the probability that a weak (strong) synapse in state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e202" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e203" xlink:type="simple"/></inline-formula>) will potentiate (depress) to become a strong (weak) synapse, by occupying the ‘shallowest’ corresponding state in the cascade hierarchy, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e204" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e205" xlink:type="simple"/></inline-formula>); and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e206" xlink:type="simple"/></inline-formula> describing the probability that a weak (strong) synapse in state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e207" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e208" xlink:type="simple"/></inline-formula>), will remain weak (strong), but even more so, by changing to a corresponding state that is one step deeper in the cascade hierarchy, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e209" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e210" xlink:type="simple"/></inline-formula>).</p>
<p>The probability of potentiation and depression decays as a geometric progression: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e211" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e212" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e213" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e214" xlink:type="simple"/></inline-formula>), and we set <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e215" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e216" xlink:type="simple"/></inline-formula>) to compensate for boundary effects. The probability of transitions towards deeper metastates is defined as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e217" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e218" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e219" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e220" xlink:type="simple"/></inline-formula>) with the correction parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e221" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e222" xlink:type="simple"/></inline-formula>) ensuring that different metastates are equally occupied for any pattern sparseness value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e223" xlink:type="simple"/></inline-formula>, as done in the original model <xref ref-type="bibr" rid="pcbi.1003489-Fusi1">[17]</xref>. Additionally, the constraint <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e224" xlink:type="simple"/></inline-formula> ensures that we have proper transition probabilities for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e225" xlink:type="simple"/></inline-formula>. The two additional parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e226" xlink:type="simple"/></inline-formula> are inspired by previous work on simple binary synapses, which showed that, for sparse patterns, it is beneficial to have different transitions probabilities for potentiation and depression <xref ref-type="bibr" rid="pcbi.1003489-Amit3">[10]</xref>. The original Fusi model <xref ref-type="bibr" rid="pcbi.1003489-Fusi1">[17]</xref> can be easily recovered by setting <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e227" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e228" xlink:type="simple"/></inline-formula>.</p>
<sec id="s4b1">
<title>Computing the synaptic weight distribution</title>
<p>A key quantity that is determined by the synaptic plasticity rule, and that we will need for deriving the optimal recall dynamics below, is the probability, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e229" xlink:type="simple"/></inline-formula>, that the weight of the synapse between presynaptic neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e230" xlink:type="simple"/></inline-formula> and postsynaptic neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e231" xlink:type="simple"/></inline-formula> takes a particular value after having stored pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e232" xlink:type="simple"/></inline-formula> some time ago, where the delay (time since storage) is drawn from the prior over pattern ages, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e233" xlink:type="simple"/></inline-formula>. For this, we first need to understand how the probability distribution of the underlying synaptic state, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e234" xlink:type="simple"/></inline-formula>, evolves over time.</p>
<p>The presentation of a sequence of patterns (intervening between storage and recall time) drawn independently from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e235" xlink:type="simple"/></inline-formula> defines a Markov process, described by a transition matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e236" xlink:type="simple"/></inline-formula>. This matrix defines the transition probabilities caused by the storage of an individual intervening pattern, obtained by marginalizing over the unknown identity of this intervening pattern:<disp-formula id="pcbi.1003489.e237"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e237" xlink:type="simple"/><label>(13)</label></disp-formula>This transition matrix also defines the stationary distribution of the synaptic states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e238" xlink:type="simple"/></inline-formula> as the eigenvector of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e239" xlink:type="simple"/></inline-formula> corresponding to the eigenvalue <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e240" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e241" xlink:type="simple"/></inline-formula> under the stationary distribution (with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e242" xlink:type="simple"/></inline-formula>).</p>
<p>Using this notation, the evolution of a synaptic state after encoding a pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e243" xlink:type="simple"/></inline-formula>, reduces to a sequence of matrix multiplications, starting from the stationary distribution, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e244" xlink:type="simple"/></inline-formula> (corresponding to having stored an infinite sequence of patterns prior to storing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e245" xlink:type="simple"/></inline-formula>), applying the transition induced by the pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e246" xlink:type="simple"/></inline-formula>, then applying repeatedly the operator <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e247" xlink:type="simple"/></inline-formula> the appropriate number of times. Formally, the distribution over the synaptic states, for pattern age <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e248" xlink:type="simple"/></inline-formula> (i.e. after storing <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e249" xlink:type="simple"/></inline-formula> intervening patterns), can be expressed as:<disp-formula id="pcbi.1003489.e250"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e250" xlink:type="simple"/><label>(14)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e251" xlink:type="simple"/></inline-formula>. An example of the evolution of this distribution under cascade dynamics, when the stored pattern is <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e252" xlink:type="simple"/></inline-formula> is shown in <xref ref-type="fig" rid="pcbi-1003489-g001">Fig. 1D</xref>.</p>
<p>Next, as the pattern age <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e253" xlink:type="simple"/></inline-formula> is unknown at the time of recall, we need to integrate over all possible pattern ages, with probabilities given by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e254" xlink:type="simple"/></inline-formula>:<disp-formula id="pcbi.1003489.e255"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e255" xlink:type="simple"/><label>(15)</label></disp-formula></p>
<p>To make the marginalisation of the unknown pattern age <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e256" xlink:type="simple"/></inline-formula> practical, we use the diagonalized form of the transition matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e257" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e258" xlink:type="simple"/></inline-formula> being a diagonal matrix containing the eigenvalues of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e259" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e260" xlink:type="simple"/></inline-formula> a matrix having the corresponding eigenvectors as columns. As the expression in <xref ref-type="disp-formula" rid="pcbi.1003489.e255">Eq. 15</xref> is linear, we can reorder the operations and compute <xref ref-type="disp-formula" rid="pcbi.1003489.e250">Eqs. 14</xref>–<xref ref-type="disp-formula" rid="pcbi.1003489.e255">15</xref> in a single step:<disp-formula id="pcbi.1003489.e261"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e261" xlink:type="simple"/><label>(16)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e262" xlink:type="simple"/></inline-formula>. It is hard to compute the eigenvalues and corresponding eigenvectors analytically in general; thus, we estimate them numerically. Nonetheless, if the prior over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e263" xlink:type="simple"/></inline-formula> is relatively simple, it is possible to do the marginalization analytically, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e264" xlink:type="simple"/></inline-formula> for the geometric prior we used in our simulations (see above), where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e265" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e266" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e267" xlink:type="simple"/></inline-formula> otherwise.</p>
<p>Finally, we use the deterministic map of synaptic states into synaptic efficacies (formalised as a <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e268" xlink:type="simple"/></inline-formula> matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e269" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e270" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e271" xlink:type="simple"/></inline-formula>; <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e272" xlink:type="simple"/></inline-formula> for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e273" xlink:type="simple"/></inline-formula>; and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e274" xlink:type="simple"/></inline-formula>, otherwise) as:<disp-formula id="pcbi.1003489.e275"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e275" xlink:type="simple"/><label>(17)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e276" xlink:type="simple"/></inline-formula> with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e277" xlink:type="simple"/></inline-formula>.</p>
<p>Note that this approach is general and can be applied to any synaptic plasticity model which involves stochastic transitions between a finite set of states, e.g. the serial model of Ref. <xref ref-type="bibr" rid="pcbi.1003489-Leibold1">[55]</xref>.</p>
</sec></sec><sec id="s4c">
<title>Optimal recall</title>
<p>As is conventional, and plausibly underpinned by neuromodulatory interactions <xref ref-type="bibr" rid="pcbi.1003489-Hasselmo1">[97]</xref>, we assume that network dynamics do not play a role during storage, with stimuli being imposed as static patterns of activity on the neurons; and conversely, that the network does not undergo further plasticity during recall.</p>
<sec id="s4c1">
<title>The posterior distribution over patterns</title>
<p>A recall query implies a posterior distribution over patterns, given the information in the weights and the recall cue:<disp-formula id="pcbi.1003489.e278"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e278" xlink:type="simple"/><label>(18)</label></disp-formula>The first two terms composing the posterior have been defined in the section describing ‘pattern and input statistics’ above. To be able to analyze the last term, we make the approximation of assuming that the evidence from the synaptic efficacies factorizes over individual synapses as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e279" xlink:type="simple"/></inline-formula>, where we have derived the form of the individual terms, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e280" xlink:type="simple"/></inline-formula>, in the preceding section. To simplify notation and since we usually focus on all-to-all connected networks, the dependence on matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e281" xlink:type="simple"/></inline-formula> is not made explicit in the main text.</p>
<p>Note that we do not assume that this posterior is ever computed explicitly by a neural circuit: we use it merely as an intermediate conceptual step to construct network dynamics that produce activity patterns optimizing network performance under this posterior distribution.</p>
</sec><sec id="s4c2">
<title>Gibbs sampling</title>
<p>All procedures that we use for sampling from the posterior distribution in this paper are variations of Gibbs sampling which updates sequentially dimension (neuron) <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e282" xlink:type="simple"/></inline-formula> of the vector <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e283" xlink:type="simple"/></inline-formula>, conditioned on the current state of all other dimensions (neurons), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e284" xlink:type="simple"/></inline-formula>, by sampling from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e285" xlink:type="simple"/></inline-formula>. For binary variables, as in our case, this is equivalent to computing the log-odds ratio:<disp-formula id="pcbi.1003489.e286"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e286" xlink:type="simple"/><label>(19)</label></disp-formula>and then passing it through a logistic sigmoid nonlinearity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e287" xlink:type="simple"/></inline-formula>.</p>
<p>Under the assumptions used for computing the posterior described above, the log-odds ratio can be decomposed into individual contributions from the prior over patterns, from the recall cue, and from individual synapses in the network that link neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e288" xlink:type="simple"/></inline-formula> to its pre- or postsynaptic partners (by applying Bayes rule, and appropriately ordering the factors):<disp-formula id="pcbi.1003489.e289"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e289" xlink:type="simple"/><label>(20)</label></disp-formula></p>
<p>The contributions of individual recurrent weights in the total current is computed using the expression for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e290" xlink:type="simple"/></inline-formula> in <xref ref-type="disp-formula" rid="pcbi.1003489.e275">Eq.17</xref>. This results in a set of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e291" xlink:type="simple"/></inline-formula> possible outcomes depending on the value of the synaptic efficacy <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e292" xlink:type="simple"/></inline-formula> and presynaptic activity <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e293" xlink:type="simple"/></inline-formula>: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e294" xlink:type="simple"/></inline-formula>, which can be further rewritten as a quadratic form in the two variables, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e295" xlink:type="simple"/></inline-formula>, with the parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e296" xlink:type="simple"/></inline-formula> computed as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e297" xlink:type="simple"/></inline-formula>. A very similar functional form can be obtained for the outgoing synapses. Note that these values are thus fully determined by the parameters of the learning rule (here, the cascade rule), the pattern distribution, and the prior for the pattern age, with no free parameters. (These parameters can also be derived for the case when the true pattern age is known, by replacing the prior for the pattern age with a delta function in <xref ref-type="disp-formula" rid="pcbi.1003489.e261">Equation 16</xref>.) As a special case, when using a balanced (for which the average probability of a potentiation or depression event is the same) presynaptically-gated learning rule we find that <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e298" xlink:type="simple"/></inline-formula>; equally, by symmetry, for a balanced postsynaptically-gated learning rule we will have <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e299" xlink:type="simple"/></inline-formula>.</p>
<p>Putting everything together, the total current to a neuron under Gibbs dynamics has the form:<disp-formula id="pcbi.1003489.e300"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e300" xlink:type="simple"/><label>(21)</label></disp-formula><disp-formula id="pcbi.1003489.e301"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e301" xlink:type="simple"/><label>(22)</label></disp-formula><disp-formula id="pcbi.1003489.e302"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e302" xlink:type="simple"/><label>(23)</label></disp-formula>where <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e303" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e304" xlink:type="simple"/></inline-formula>.</p>
<p>Starting from the recall cue, the recall dynamics involve asynchronous updates of each neuron in the network, with samples collected at the end of each full network update, corresponding to one time step in the figures. The permutation determining the order in which neurons are updated is also randomly redrawn at the beginning of each network update.</p>
</sec></sec><sec id="s4d">
<title>Network approximations</title>
<sec id="s4d1">
<title>Intrinsic plasticity</title>
<p>We consider three variants for approximating the term corresponding to the outgoing synapses (presynaptic IP):<disp-formula id="pcbi.1003489.e305"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e305" xlink:type="simple"/><label>(24)</label></disp-formula><disp-formula id="pcbi.1003489.e306"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e306" xlink:type="simple"/><label>(25)</label></disp-formula><disp-formula id="pcbi.1003489.e307"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e307" xlink:type="simple"/><label>(26)</label></disp-formula>Computationally, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e308" xlink:type="simple"/></inline-formula> corresponds to an approximation of the expected value of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e309" xlink:type="simple"/></inline-formula>, conditioned on the net activity of the neuron's postsynaptic partners, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e310" xlink:type="simple"/></inline-formula>. The last, most refined approximation, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e311" xlink:type="simple"/></inline-formula>, represents a similar expectation, further conditioned on the sum of the efficacies of outgoing synapses, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e312" xlink:type="simple"/></inline-formula>. We obtain <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e313" xlink:type="simple"/></inline-formula> by taking an expectation over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e314" xlink:type="simple"/></inline-formula> (implicitly still ignoring correlations between weights and postsynaptic activities).</p>
<p>To investigate the role of the homeostatic regulation of neural excitability depending on the incoming synaptic weights (postsynaptic IP), we replaced the term corresponding to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e315" xlink:type="simple"/></inline-formula> by its expected value, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e316" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e317" xlink:type="simple"/></inline-formula> the expected synaptic efficacy under the stationary distribution, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e318" xlink:type="simple"/></inline-formula> the synaptic connection probability (see above). Furthermore, when introducing or removing the homeostatic regulation of excitability in <xref ref-type="fig" rid="pcbi-1003489-g004">Fig. 4A</xref> (bottom) we replace <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e319" xlink:type="simple"/></inline-formula> by <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e320" xlink:type="simple"/></inline-formula> (alternatively, we could have varied the factor scaling the dependence of the total synaptic efficacy parametrically).</p>
<p>For both forms of regulation of neural excitability, the online version of the recall dynamics assumes the term <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e321" xlink:type="simple"/></inline-formula>, or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e322" xlink:type="simple"/></inline-formula>, respectively, is replaced by a temporal average of the form <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e323" xlink:type="simple"/></inline-formula>, with the presynaptic activity sampled from the prior, which would correspond to network activity while retrieving other patterns (we use a square temporal window and only vary its width, i.e. the number of samples used for the estimation, but a more realistic time decaying kernel would also be possible).</p>
</sec><sec id="s4d2">
<title>Inhibition</title>
<p>To model tonic inhibition, we replace the inhibition terms <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e324" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e325" xlink:type="simple"/></inline-formula> by their expected value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e326" xlink:type="simple"/></inline-formula>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e327" xlink:type="simple"/></inline-formula> (or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e328" xlink:type="simple"/></inline-formula>) the total number of pre- or postsynaptic connections to neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e329" xlink:type="simple"/></inline-formula>. To asses the importance of spatial selectivity of inhibitory connections we use a sparsely connected network (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e330" xlink:type="simple"/></inline-formula>) and vary the degree of overlap of the sources of excitation and inhibition to a neuron, while maintaining the average magnitude of inhibition fixed. In particular, we define a second connectivity matrix <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e331" xlink:type="simple"/></inline-formula> for defining the sources of (disynaptic) inhibition to each neuron and vary the E/I overlap by manipulating the similarity between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e332" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e333" xlink:type="simple"/></inline-formula>. To preserve the average net inhibition to a neuron, we keep the average number of inhibitory connections to a neuron fixed (the two connectivity matrices are equally sparse), and replace a certain percentage of the correctly matched inhibitory sources to a neuron (e.g. in <xref ref-type="fig" rid="pcbi-1003489-g005">Fig. 5D</xref> from neuron 4 to neuron 2) with inhibitory connections from neurons that are not in the set of its (pre- or post-) synaptic partners (e.g. from neuron 5), with the E/I overlap parameter defining the probability of a ‘correct’ inhibitory source actually feeding inhibition to a neuron, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e334" xlink:type="simple"/></inline-formula>. Furthermore, to keep the net inhibitory current to the neurons unchanged, we add random inhibitory connections from neurons that do not share recurrent collaterals with neuron <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e335" xlink:type="simple"/></inline-formula> as to preserve the average number of inhibitory synapses onto the neuron. Using this setup, the feedback inhibition term becomes <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e336" xlink:type="simple"/></inline-formula>.</p>
</sec></sec><sec id="s4e">
<title>Artificial dual sampler</title>
<p>We construct artificial recall dynamics that perform Gibbs sampling in the space of the joint distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e337" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pcbi.1003489-Savin2">[49]</xref>. Introducing the pattern age as an auxiliary variable in the sampling procedure can be related to other auxiliary variable methods for sampling and is expected to improve sampling efficacy in the case of complex distributions <xref ref-type="bibr" rid="pcbi.1003489-Iba1">[65]</xref>.</p>
<p>Formally, the dynamics alternates between sampling an individual neuron's activity, conditioned on everything else (including the current value to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e338" xlink:type="simple"/></inline-formula>), <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e339" xlink:type="simple"/></inline-formula> and sampling the pattern age <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e340" xlink:type="simple"/></inline-formula>, according to the distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e341" xlink:type="simple"/></inline-formula> (which simplifies to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e342" xlink:type="simple"/></inline-formula>, as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e343" xlink:type="simple"/></inline-formula> is independent of the recall cue after conditioning on the stored pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e344" xlink:type="simple"/></inline-formula>). This last step makes this sampling procedure biologically unrealistic, as computing the distribution over pattern ages requires knowledge of the full set of recurrent collaterals of the network.</p>
<p>Practically, the procedure involves stochastic updates of neuron activities that are very similar to those of the simple Gibbs sampler, with the distinction that now the parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e345" xlink:type="simple"/></inline-formula> depend on the pattern age, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e346" xlink:type="simple"/></inline-formula>, and are computed using the distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e347" xlink:type="simple"/></inline-formula>, obtained by projecting the distribution over the synaptic states <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e348" xlink:type="simple"/></inline-formula> directly into synaptic efficacies, without marginalising out <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e349" xlink:type="simple"/></inline-formula>. This means that the expression of the total current <xref ref-type="disp-formula" rid="pcbi.1003489.e080">Eq. 3</xref> now includes age-dependent parameters <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e350" xlink:type="simple"/></inline-formula>. Conceptually, this will result in a modulation of the relative contribution of the recurrent collaterals versus the external input from the cue, such that the recurrent dynamics dominate for recent patterns while the output is driven by the external input when the pattern is deemed to be old, when little or no information about the pattern is available in the weights.</p>
<p>Finally, to be able to sample the pattern age, we limit the maximum possible pattern ages resulting in a finite discrete distribution, from which it is easy to sample (in practice, we assume events with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e351" xlink:type="simple"/></inline-formula> can be treated as equivalent). As there is little signal in the tail of the distribution over pattern ages, this does not affect performance for the network sizes considered here.</p>
</sec><sec id="s4f">
<title>Tempered transitions and network oscillations</title>
<p>Tempered transitions (TT) is a method that can improve sampling efficiency by using annealing, i.e., systematically increasing and then decreasing a temperature parameter to ensure better exploration of the state space <xref ref-type="bibr" rid="pcbi.1003489-Neal1">[64]</xref>. According to TT, in order to sample from a target distribution <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e352" xlink:type="simple"/></inline-formula> we choose a set of intermediate probability distributions, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e353" xlink:type="simple"/></inline-formula>, indexed by the inverse temperature parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e354" xlink:type="simple"/></inline-formula>, that are increasingly dissimilar from, but also easier to sample than, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e355" xlink:type="simple"/></inline-formula> as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e356" xlink:type="simple"/></inline-formula> decreases. The target distribution is represented at inverse temperature <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e357" xlink:type="simple"/></inline-formula>: <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e358" xlink:type="simple"/></inline-formula>. For each intermediate distribution we need a form of stochastic dynamics (formally, defining a Markov transition operator) which samples from (i.e. has as its stationary distribution) the corresponding <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e359" xlink:type="simple"/></inline-formula>. Starting from the current state <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e360" xlink:type="simple"/></inline-formula>, which is a sample at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e361" xlink:type="simple"/></inline-formula>, i.e. from <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e362" xlink:type="simple"/></inline-formula>, a sampling cycle involves first lowering the inverse temperature in a sequence of steps down to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e363" xlink:type="simple"/></inline-formula> and then increasing it back to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e364" xlink:type="simple"/></inline-formula>. At each temperature level, we run the corresponding stochastic dynamics for a few steps starting from the last sample collected at the previous temperature level. This results in a sequence of intermediate samples, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e365" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e366" xlink:type="simple"/></inline-formula>, for the descending and ascending inverse temperature sequences, respectively (<xref ref-type="supplementary-material" rid="pcbi.1003489.s004">Fig. S4</xref>). Finally, all the intermediate samples produced at inverse temperatures <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e367" xlink:type="simple"/></inline-formula> are discarded, and the final sample produced at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e368" xlink:type="simple"/></inline-formula> is accepted or rejected (in which case network activity would need to return to the state it had at the beginning of the cycle) with a probability given by the product of pairwise ratios of probabilities of all the intermediate states <xref ref-type="bibr" rid="pcbi.1003489-Neal1">[64]</xref> (see also Suppl. Info. in Ref. <xref ref-type="bibr" rid="pcbi.1003489-Savin2">[49]</xref>).</p>
<p>For us, the target distribution is the posterior <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e369" xlink:type="simple"/></inline-formula>. Common practice would dictate that we choose the intermediate distributions to be simply exponentiated (with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e370" xlink:type="simple"/></inline-formula> as the exponent) versions of the target distribution, which would result in a completely uniform distribution at <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e371" xlink:type="simple"/></inline-formula>. However, this would not be efficient as the uniform distribution has no information about the original problem and thus results in unnecessarily wide Markov steps and, as a consequence, in a high rejection rate. Instead, we can use an important insight about the structure of our posterior to construct a better sequence of intermediate distributions. This insight is that the only factor that makes the posterior hard to sample from (thus motivating the usage of TT in the first place) is the correlations in it that are solely introduced by the weight-likelihood term, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e372" xlink:type="simple"/></inline-formula>. (Note that although we approximated this term above as factorized over the elements of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e373" xlink:type="simple"/></inline-formula>, this still does not mean that it also factorizes over the elements of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e374" xlink:type="simple"/></inline-formula>, of which the correlations are of issue here.) Therefore, we chose only this term to be modulated by temperature, such that<disp-formula id="pcbi.1003489.e375"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e375" xlink:type="simple"/><label>(27)</label></disp-formula></p>
<p>The are two important features of exact TT dynamics that are problematic in the context of our network dynamics: first, the order in which neurons are updated in the ascending phase should be the exact reverse of that used in the descending phase; second, and more critically, an acceptance step is required at the end of each temperature cycle, as we saw above. As both the final acceptance step and the tight control on the ordering of neural updates are biologically unrealistic, the neural network approximates TT dynamics by ignoring sample rejections and by updating neuron activities in a random order during an oscillation cycle <xref ref-type="bibr" rid="pcbi.1003489-Neal1">[64]</xref>. Under these approximations, the network dynamics are essentially identical to those of simple Gibbs (<xref ref-type="disp-formula" rid="pcbi.1003489.e300">Eqs. 21</xref>–<xref ref-type="disp-formula" rid="pcbi.1003489.e302">23</xref>), with all parameters unchanged, with the only modification that the recurrent currents are multiplicatively modulated by the inverse temperature <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e376" xlink:type="simple"/></inline-formula> (cf. <xref ref-type="disp-formula" rid="pcbi.1003489.e300">Eq. 21</xref>):<disp-formula id="pcbi.1003489.e377"><graphic position="anchor" xlink:href="info:doi/10.1371/journal.pcbi.1003489.e377" xlink:type="simple"/><label>(28)</label></disp-formula>At <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e378" xlink:type="simple"/></inline-formula> this is equivalent to sampling from a purely feed-forward network which uses no information in the recurrent weights and which is the network that we used throughout the paper as our ‘control’.</p>
<p>In general, the inverse temperature parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e379" xlink:type="simple"/></inline-formula> can take values between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e380" xlink:type="simple"/></inline-formula> (corresponding to control) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e381" xlink:type="simple"/></inline-formula> (the target distribution). Here, we took a sequence that linearly interpolated between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e382" xlink:type="simple"/></inline-formula> and a minimum value <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e383" xlink:type="simple"/></inline-formula>, with the amplitude of the oscillation being defined as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e384" xlink:type="simple"/></inline-formula>. In all cases the number of neurons updated at each temperature level was chosen such that the total number of neurons updated over a whole cycle was the number of neurons in the network, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e385" xlink:type="simple"/></inline-formula>.</p>
<p>Although, due to the approximations we introduced above, the resulting network dynamics is no longer guaranteed to generate samples from exactly the correct posterior distribution, simulation results suggest that this approximation does not significantly alter the estimate of the posterior mean or the average response variability provided that the acceptance probability under the exact dynamics remains high, which we ensure by an appropriate modulation of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e386" xlink:type="simple"/></inline-formula>.</p>
</sec><sec id="s4g">
<title>Simulation parameters</title>
<p>We start by defining the general setup and the default parameters used in all simulations, after which we proceed to list the parameter settings specific to each figure, in the order in which they are included in the main text. Unless otherwise specified, we considered a network of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e387" xlink:type="simple"/></inline-formula> fully-connected neurons. The stored patterns were balanced, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e388" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e389" xlink:type="simple"/></inline-formula> when sparse patterns were used, in <xref ref-type="fig" rid="pcbi-1003489-g002">Fig. 2</xref>); the recall cue noise was <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e390" xlink:type="simple"/></inline-formula>, the average pattern age was <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e391" xlink:type="simple"/></inline-formula> and the cascade parameters were <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e392" xlink:type="simple"/></inline-formula>, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e393" xlink:type="simple"/></inline-formula> and depth <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e394" xlink:type="simple"/></inline-formula>.</p>
<p>For measuring retrieval performance, we started from sampling the stationary distribution of the synaptic states, then we sampled from the prior, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e395" xlink:type="simple"/></inline-formula>, one N-dimensional binary pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e396" xlink:type="simple"/></inline-formula> which we stored by modifying synaptic states in the network according to the cascade learning rule described above. To separate the effects of synaptic correlations from the correlations among recalled activities, we simulated the effects of the storage of intervening patterns following the storage of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e397" xlink:type="simple"/></inline-formula> by evolving individual synapses independently for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e398" xlink:type="simple"/></inline-formula> steps according to the transition operator, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e399" xlink:type="simple"/></inline-formula>, corresponding to storing a random pattern from the prior. At recall, we sampled a recall cue, which was a noisy version of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e400" xlink:type="simple"/></inline-formula>, according to the noise model. This cue was provided as input to the network throughout retrieval as well as the starting point for the network dynamics. The network was allowed to evolve for 100 steps according to the dynamics we derived above. We took the temporal average across all these samples to be the recalled pattern <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e401" xlink:type="simple"/></inline-formula>, and computed the root mean square error between the stored and recalled pattern as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e402" xlink:type="simple"/></inline-formula>. The performance for the control feed-forward network could be computed analytically (as both the prior over <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e403" xlink:type="simple"/></inline-formula> and the recall cue distribution are factorized) as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e404" xlink:type="simple"/></inline-formula>, which for the case of balanced patterns (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e405" xlink:type="simple"/></inline-formula>, as in most cases considered here) reduces to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e406" xlink:type="simple"/></inline-formula>. When plotting mean performance as a function of pattern age, we used 10 trials for estimating the error for each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e407" xlink:type="simple"/></inline-formula>; for the average performance plots, we repeated the storage-retrieval procedure described above <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e408" xlink:type="simple"/></inline-formula> times, with pattern ages drawn randomly from the prior distribution. Average performance was measured as the average error over these independent runs, with all error bars representing the standard error of the mean.</p>
<p>For <xref ref-type="fig" rid="pcbi-1003489-g002">Fig. 2B</xref>, we stored the <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e409" xlink:type="simple"/></inline-formula> binary pattern shown and used Gibbs dynamics without further approximations to retrieve it at a pattern age <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e410" xlink:type="simple"/></inline-formula>. To emphasize the stochastic aspects of the dynamics, we chose to show a subset of neurons whose activity evolution happened to be most variable. When investigating the dependence of the networks' performance on the total number of synapses per neuron in the sparse condition, we fixed network size and varied the connection probability, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e411" xlink:type="simple"/></inline-formula>; in particular, we used a network size of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e412" xlink:type="simple"/></inline-formula> with one exception: we took <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e413" xlink:type="simple"/></inline-formula> for the case of 500 synapses/neuron. To investigate the effects of the prior over pattern ages, we reran the same procedure for different settings of the average pattern age (adjusting the parameters of the network accordingly and resampling the pattern ages for which retrieval is performed).</p>
<p>For the memory capacity analysis in <xref ref-type="fig" rid="pcbi-1003489-g003">Fig. 3</xref>, we used the default parameters for the cascade (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e414" xlink:type="simple"/></inline-formula> and depth <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e415" xlink:type="simple"/></inline-formula>) and optimized the parameters of the two-state synapse such that the signal decays exponentially with the same time constant as the prior assumed over patterns (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e416" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e417" xlink:type="simple"/></inline-formula>). The memory capacity was defined, in line with classic SNR-based analyses, as the maximum pattern age for which retrieval error (averaged over 100 trials for each <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e418" xlink:type="simple"/></inline-formula>) was below a predefined threshold <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e419" xlink:type="simple"/></inline-formula>. The network evolved by simple Gibbs dynamics (assuming <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e420" xlink:type="simple"/></inline-formula> unknown). When optimizing cascade depth (<xref ref-type="supplementary-material" rid="pcbi.1003489.s005">Fig. S5</xref>), we assumed <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e421" xlink:type="simple"/></inline-formula>, and estimated for each setting of <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e422" xlink:type="simple"/></inline-formula> the average retrieval error under the prior for <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e423" xlink:type="simple"/></inline-formula> and the mutual information between a synapse <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e424" xlink:type="simple"/></inline-formula> and the activity of its corresponding neurons <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e425" xlink:type="simple"/></inline-formula> (marginalizing over the unknown <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e426" xlink:type="simple"/></inline-formula>; i.e. using <xref ref-type="disp-formula" rid="pcbi.1003489.e275">Eq. 17</xref>).</p>
<p>For <xref ref-type="fig" rid="pcbi-1003489-g005">Fig. 5A</xref>, we monitored the excitatory, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e427" xlink:type="simple"/></inline-formula>, and inhibitory, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e428" xlink:type="simple"/></inline-formula>, input to a neuron in a network with the default parameters settings, evolving by Gibbs dynamics, as described above (the actual values obtained were discrete so we added a small amount of Gaussian jitter to them for visualization purposes). These two quantities are plotted against each other for three example neurons, two with low entropy (red, blue) and one with high response entropy (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e429" xlink:type="simple"/></inline-formula>). All-to-all connectivity was used for panel B, and sparse connectivity (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e430" xlink:type="simple"/></inline-formula>) for panels E and F. Panel E shows the total recurrent current to an example neuron using the exact vs. the approximate expression for computing the inhibitory current, while the dynamics evolve by Gibbs.</p>
<p>For <xref ref-type="fig" rid="pcbi-1003489-g006">Fig. 6</xref>, we used relatively dense connectivity (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e431" xlink:type="simple"/></inline-formula>) in order to preserve a relatively high number of synapses per neuron. The parameters for different approximations were <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e432" xlink:type="simple"/></inline-formula> steps for the online forms of IP, and we used <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e433" xlink:type="simple"/></inline-formula> E/I coherence and oscillation amplitude <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e434" xlink:type="simple"/></inline-formula>.</p>
<p>For the simulations with oscillations (<xref ref-type="fig" rid="pcbi-1003489-g007">Fig. 7</xref>), we used <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e435" xlink:type="simple"/></inline-formula> different temperature levels, linearly spanning the range <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e436" xlink:type="simple"/></inline-formula>, with 5 neurons updated at each temperature step, such that one oscillation cycle corresponded to one full network update (50 descending and 50 ascending inverse temperature steps), as before. In this case, the posterior mean was computed by averaging over the samples obtained at (inverse) temperature <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e437" xlink:type="simple"/></inline-formula>. (We kept the total simulation length constant, which meant that we had a reduced number of samples for estimating the posterior with oscillations, thus slightly favoring simple Gibbs dynamics without oscillations, but we deemed this a fair comparison if the duration of a recall trial is the real constraint). For <xref ref-type="fig" rid="pcbi-1003489-g007">Fig. 7C</xref>, we used high amplitude oscillations, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e438" xlink:type="simple"/></inline-formula>, and, for each temperature level (which defines the phase of the oscillation), computed average population firing as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e439" xlink:type="simple"/></inline-formula> and the average response entropy as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e440" xlink:type="simple"/></inline-formula>, with a neuron's response entropy defined as <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e441" xlink:type="simple"/></inline-formula>.</p>
<p>For the flickering experiment (<xref ref-type="fig" rid="pcbi-1003489-g008">Fig. 8</xref>), we stored two consecutive patterns, <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e442" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e443" xlink:type="simple"/></inline-formula> (corresponding to the two contexts in the original experiments), and simulated the effects of having stored another 8 successive patterns independently across synapses as described above. For creating inputs to the network, cues were sampled independently in each time step from the input distribution conditioned on the pattern (<inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e444" xlink:type="simple"/></inline-formula> or <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e445" xlink:type="simple"/></inline-formula>) corresponding to the current context, and hence their statistics changed abruptly at a context switch. For recall, we used oscillatory dynamics (as in <xref ref-type="fig" rid="pcbi-1003489-g007">Fig. 7</xref>, with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e446" xlink:type="simple"/></inline-formula>) with one minor modification: instead of taking a single relatively reliable recall cue as the input, each neuron integrated the evidence from the most recent past of several highly unreliable cues (75 cues, each with <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e447" xlink:type="simple"/></inline-formula>) by simply summing them up (this is optimal in our framework under the assumption that all 75 cues are i.i.d., which is violated at a context switch). For constructing the actual figure, we started the simulations using <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e448" xlink:type="simple"/></inline-formula> and switched to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e449" xlink:type="simple"/></inline-formula> at time <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e450" xlink:type="simple"/></inline-formula>, marked by the vertical green bar. As the effective recall cue was obtained by integrating over a period of several time steps, there is a corresponding time-window after the switch during which this effective recall cue is ambiguous (due to the integration of conflicting evidence coming from two different contexts), and hence the posterior is determined primarily by the evidence from the weights, which is inherently multimodal. We computed the correlation between the response of the network (at the peak of the oscillation, corresponding to <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e451" xlink:type="simple"/></inline-formula>) and the two actually stored patterns <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e452" xlink:type="simple"/></inline-formula>, and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e453" xlink:type="simple"/></inline-formula>, which are displayed in <xref ref-type="fig" rid="pcbi-1003489-g008">Fig. 8</xref>.</p>
</sec></sec><sec id="s5">
<title>Supporting Information</title>
<supplementary-material id="pcbi.1003489.s001" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003489.s001" position="float" xlink:type="simple"><label>Figure S1</label><caption>
<p><bold>Different schemes for representing the posterior through recall dynamics.</bold> <bold>A.</bold> Schematic representation of possible strategies for constructing recall dynamics corresponding to the posterior (heat map): starting from the recall cue (green), maximum a posteriori (MAP, black line) dynamics follow the local gradient to a possibly local maximum of the posterior thus exhibiting attractor dynamics; sampling based dynamics (MCMC, gray dots) move stochastically in the state space, such that the amount of time spent in a certain region of the state space is proportional to the mass of the distribution in that region. For the purposes of illustration, the case of analog patterns is shown. <bold>B.</bold> The corresponding neuronal transfer functions (the expression for the total current to a neuron is identical in all variants, see <xref ref-type="disp-formula" rid="pcbi.1003489.e080">Eq. 3</xref>). <bold>C.</bold> Comparison of retrieval performance using different retrieval dynamics. Control level was <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e454" xlink:type="simple"/></inline-formula> (not shown). All simulation parameters had the default values, as defined in the main text.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003489.s002" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003489.s002" position="float" xlink:type="simple"><label>Figure S2</label><caption>
<p><bold>Representing recall uncertainty.</bold> Relationship between the variability of neural responses during retrieval, measured by the average neural response entropy as shown in <xref ref-type="fig" rid="pcbi-1003489-g007">Fig. 7C</xref>, and the final (r.m.s.) retrieval error associated with the response. Colors label the age of the pattern to be retrieved (see color bar on right). Simulation used default parameters (see <xref ref-type="sec" rid="s4">Methods</xref>).</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003489.s003" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003489.s003" position="float" xlink:type="simple"><label>Figure S3</label><caption>
<p><bold>Recall performance for standard attractor dynamics.</bold> A total of 10 patterns was stored in a recurrent network by the cascade rule, either the pre- (blue) or the postsynaptically gated form (red). All parameters were set to their default values. Retrieval followed standard attractor dynamics which ignore the prior over pattern ages and the recall cue – beyond the initial condition (see <xref ref-type="supplementary-material" rid="pcbi.1003489.s007">Text S2</xref> for details). Gray dashed line shows retrieval performance for the optimal dynamics (without approximations). (This performance is formally identical for pre- and post-synaptically gated plasticity.) Black dashed line shows the usual control level, corresponding to an optimized feedforward network.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003489.s004" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003489.s004" position="float" xlink:type="simple"><label>Figure S4</label><caption>
<p><bold>Oscillations as tempered transitions.</bold> Schematic depiction of the effects on the posterior induced by modulating the temperature parameter for a one-dimensional analog distribution. Tempered transitions cycles through several distributions indexed by the inverse temperature parameter <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e455" xlink:type="simple"/></inline-formula> taking values between <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e456" xlink:type="simple"/></inline-formula> (depending on oscillation depth) and <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e457" xlink:type="simple"/></inline-formula>. Sampling at the high temperature (low <inline-formula><inline-graphic xlink:href="info:doi/10.1371/journal.pcbi.1003489.e458" xlink:type="simple"/></inline-formula>) distributions allows the dynamics to explore the full state space.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003489.s005" mimetype="image/tiff" xlink:href="info:doi/10.1371/journal.pcbi.1003489.s005" position="float" xlink:type="simple"><label>Figure S5</label><caption>
<p><bold>Single synapse signal vs. recall performance.</bold> Mutual information between pre- and postsynaptic activity at a synapse and the weight of that synapse (gray) and recall performance in the network (black) as a function of cascade depth. Arrows show optima of the two curves.</p>
<p>(TIF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003489.s006" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003489.s006" position="float" xlink:type="simple"><label>Text S1</label><caption>
<p><bold>Alternative recall dynamics.</bold></p>
<p>(PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003489.s007" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003489.s007" position="float" xlink:type="simple"><label>Text S2</label><caption>
<p><bold>Comparison to standard attractor dynamics.</bold></p>
<p>(PDF)</p>
</caption></supplementary-material><supplementary-material id="pcbi.1003489.s008" mimetype="application/pdf" xlink:href="info:doi/10.1371/journal.pcbi.1003489.s008" position="float" xlink:type="simple"><label>Text S3</label><caption>
<p><bold>Error rates for old patterns.</bold></p>
<p>(PDF)</p>
</caption></supplementary-material></sec></body>
<back><ref-list>
<title>References</title>
<ref id="pcbi.1003489-Treves1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Treves</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name> (<year>1994</year>) <article-title>Computational analysis of the role of the hippocampus in memory</article-title>. <source>Hippocampus</source> <volume>4</volume>: <fpage>374</fpage>–<lpage>391</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Treves2"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Treves</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Skaggs</surname><given-names>WE</given-names></name>, <name name-style="western"><surname>Barnes</surname><given-names>CA</given-names></name> (<year>1996</year>) <article-title>How much of the hippocampus can be explained by functional constraints?</article-title> <source>Hippocampus</source> <volume>6</volume>: <fpage>666</fpage>–<lpage>674</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Amaral1"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amaral</surname><given-names>DG</given-names></name> (<year>1993</year>) <article-title>Emerging principles of intrinsic hippocampal organization</article-title>. <source>Current opinion in neurobiology</source> <volume>3</volume>: <fpage>225</fpage>–<lpage>229</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Hopfield1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name> (<year>1982</year>) <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proc Natl Acad Sci</source> <volume>76</volume>: <fpage>2554</fpage>–<lpage>2558</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Amit1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Gutfreund</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>1985</year>) <article-title>Storing infinite numbers of patterns in a spin-glass model of neural networks</article-title>. <source>Phys Rev Lett</source> <volume>55</volume>: <fpage>1530</fpage>–<lpage>1533</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Amit2"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Gutfreund</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>1987</year>) <article-title>Information storage in neural networks with low levels of activity</article-title>. <source>Phys Rev A</source> <volume>35</volume>: <fpage>2293</fpage>–<lpage>2303</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Willshaw1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Willshaw</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Buneman</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Longuet-Higgins</surname><given-names>H</given-names></name> (<year>1969</year>) <article-title>Non-holographic associative memory</article-title>. <source>Nature</source> <volume>222</volume>: <fpage>960</fpage>–<lpage>962</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Hertz1"><label>8</label>
<mixed-citation publication-type="book" xlink:type="simple">Hertz JA, Krogh AS, Palmer RG (1991) Introduction to the theory of neural computation, volume 1. Westview Press.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Treves3"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Treves</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Rolls</surname><given-names>ET</given-names></name> (<year>1991</year>) <article-title>What determines the capacity of autoassociative memories in the brain?</article-title> <source>Network</source> <volume>2</volume>: <fpage>371</fpage>–<lpage>397</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Amit3"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Amit</surname><given-names>DJ</given-names></name>, <name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name> (<year>1994</year>) <article-title>Learning in neural networks with material synapses</article-title>. <source>Neural Computation</source> <volume>6</volume>: <fpage>957</fpage>–<lpage>982</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Hebb1"><label>11</label>
<mixed-citation publication-type="book" xlink:type="simple">Hebb D (1949) The organization of behavior. New York: Wiley.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Nakazawa1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Nakazawa</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Quirk</surname><given-names>MC</given-names></name>, <name name-style="western"><surname>Chitwood</surname><given-names>RA</given-names></name>, <name name-style="western"><surname>Watanabe</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Yeckel</surname><given-names>MF</given-names></name>, <etal>et al</etal>. (<year>2002</year>) <article-title>Requirement for hippocampal CA3 NMDA receptors in associative memory recall</article-title>. <source>Science</source> <volume>297</volume>: <fpage>211</fpage>–<lpage>8</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Wills1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wills</surname><given-names>TJ</given-names></name>, <name name-style="western"><surname>Lever</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Cacucci</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Burgess</surname><given-names>N</given-names></name>, <name name-style="western"><surname>O'Keefe</surname><given-names>J</given-names></name> (<year>2005</year>) <article-title>Attractor dynamics in the hippocampal representation of the local environment</article-title>. <source>Science</source> <volume>308</volume>: <fpage>873</fpage>–<lpage>876</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Lengyel1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lengyel</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Kwag</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Paulsen</surname><given-names>O</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name> (<year>2005</year>) <article-title>Matching storage and recall: hippocampal spike timingdependent plasticity and phase response curves</article-title>. <source>Nature Neuroscience</source> <volume>8</volume>: <fpage>1677</fpage>–<lpage>1683</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Abbott1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name>, <name name-style="western"><surname>Nelson</surname><given-names>SB</given-names></name> (<year>2000</year>) <article-title>Synaptic plasticity: taming the beast</article-title>. <source>Nature Neuroscience</source> <volume>3</volume>: <fpage>1178</fpage>–<lpage>1183</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Bienenstock1"><label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bienenstock</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Cooper</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Munro</surname><given-names>P</given-names></name> (<year>1982</year>) <article-title>Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex</article-title>. <source>The Journal of Neuroscience</source> <volume>2</volume>: <fpage>32</fpage>–<lpage>48</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Fusi1"><label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Drew</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name> (<year>2005</year>) <article-title>Cascade models of synaptically stored memories</article-title>. <source>Neuron</source> <volume>45</volume>: <fpage>599</fpage>–<lpage>611</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Abraham1"><label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abraham</surname><given-names>WC</given-names></name>, <name name-style="western"><surname>Bear</surname><given-names>MF</given-names></name> (<year>1996</year>) <article-title>Metaplasticity: the plasticity of synaptic plasticity</article-title>. <source>Trends in Neurosciences</source> <volume>19</volume>: <fpage>126</fpage>–<lpage>130</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Abraham2"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Abraham</surname><given-names>WC</given-names></name> (<year>2008</year>) <article-title>Metaplasticity: tuning synapses and networks for plasticity</article-title>. <source>Nature Reviews Neuroscience</source> <volume>9</volume>: <fpage>387</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Hulme1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hulme</surname><given-names>SR</given-names></name>, <name name-style="western"><surname>Jones</surname><given-names>OD</given-names></name>, <name name-style="western"><surname>Abraham</surname><given-names>WC</given-names></name> (<year>2013</year>) <article-title>Emerging roles of metaplasticity in behaviour and disease</article-title>. <source>Trends in Neurosciences</source> <volume>36</volume>: <fpage>353</fpage>–<lpage>362</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Debanne1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Debanne</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Gähwiler</surname><given-names>BH</given-names></name>, <name name-style="western"><surname>Thompson</surname><given-names>SM</given-names></name> (<year>1999</year>) <article-title>Heterogeneity of synaptic plasticity at unitary CA3-CA1 and CA3-CA3 connections in rat hippocampal slice cultures</article-title>. <source>Journal of Neuroscience</source> <volume>19</volume>: <fpage>10664</fpage>–<lpage>10671</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Huang1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huang</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Amit</surname><given-names>Y</given-names></name> (<year>2011</year>) <article-title>Capacity analysis in multi-state synaptic models: a retrieval probability perspective</article-title>. <source>Journal of Computational Neuroscience</source> <volume>30</volume>: <fpage>699</fpage>–<lpage>720</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Thompson1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Thompson</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Moyer</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Disterhoft</surname><given-names>J</given-names></name> (<year>1996</year>) <article-title>Transient changes in excitability of rabbit CA3 neurons with a time course appropriate to support memory consolidation</article-title>. <source>Journal of Neurophysiology</source> <volume>76</volume>: <fpage>1836</fpage>–<lpage>1849</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Moyer1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moyer</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Power</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Thompson</surname><given-names>L</given-names></name> (<year>2000</year>) <article-title>Increased excitability of aged rabbit CA1 neurons after trace eyeblink conditioning</article-title>. <source>The Journal of Neuroscience</source> <volume>20</volume>: <fpage>5476</fpage>–<lpage>5482</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Zhang1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Linden</surname><given-names>DJ</given-names></name> (<year>2003</year>) <article-title>The other side of the engram: experience-driven changes in neuronal intrinsic excitability</article-title>. <source>Nature Reviews Neuroscience</source> <volume>4</volume>: <fpage>885</fpage>–<lpage>900</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Klausberger1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Klausberger</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Magill</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Márton</surname><given-names>LF</given-names></name>, <name name-style="western"><surname>Roberts</surname><given-names>JDB</given-names></name>, <name name-style="western"><surname>Cobden</surname><given-names>PM</given-names></name>, <etal>et al</etal>. (<year>2003</year>) <article-title>Brain-state- and cell-typespecific firing of hippocampal interneurons in vivo</article-title>. <source>Nature</source> <volume>421</volume>: <fpage>844</fpage>–<lpage>848</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Lapray1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lapray</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Lasztoczi</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Lagler</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Viney</surname><given-names>TJ</given-names></name>, <name name-style="western"><surname>Katona</surname><given-names>L</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Behavior-dependent specialization of identified hippocampal interneurons</article-title>. <source>Nature Neuroscience</source> <volume>15</volume>: <fpage>1265</fpage>–<lpage>1271</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Kullmann1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kullmann</surname><given-names>DM</given-names></name> (<year>2011</year>) <article-title>Interneuron networks in the hippocampus</article-title>. <source>Current opinion in neurobiology</source> <volume>21</volume>: <fpage>709</fpage>–<lpage>716</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-MacKay1"><label>29</label>
<mixed-citation publication-type="other" xlink:type="simple">MacKay DJ (1991) Maximum entropy connections: Neural networks. In: Maximum entropy and Bayesian methods, Springer. pp. 237–244.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Sommer1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sommer</surname><given-names>FT</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name> (<year>1998</year>) <article-title>Bayesian retrieval in associative memories with storage errors</article-title>. <source>IEEE transactions on neural networks</source> <volume>9</volume>: <fpage>705</fpage>–<lpage>713</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Lengyel2"><label>31</label>
<mixed-citation publication-type="other" xlink:type="simple">Lengyel M, Dayan P (2007) Uncertainty, phase and oscillatory hippocampal recall. In: Advances in Neural Information Processing Systems 19, MIT Press.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Freund1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Freund</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Buzsáki</surname><given-names>G</given-names></name> (<year>1996</year>) <article-title>Interneurons of the hippocampus</article-title>. <source>Hippocampus</source> <volume>6</volume>: <fpage>347</fpage>–<lpage>470</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Desai1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Desai</surname><given-names>NS</given-names></name>, <name name-style="western"><surname>Rutherford</surname><given-names>LC</given-names></name>, <name name-style="western"><surname>Turrigiano</surname><given-names>GG</given-names></name> (<year>1999</year>) <article-title>Plasticity in the intrinsic excitability of cortical pyramidal neurons</article-title>. <source>Nature Neuroscience</source> <volume>2</volume>: <fpage>515</fpage>–<lpage>520</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Wyble1"><label>34</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wyble</surname><given-names>BP</given-names></name>, <name name-style="western"><surname>Linster</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Hasselmo</surname><given-names>ME</given-names></name> (<year>2000</year>) <article-title>Size of CA1-evoked synaptic potentials is related to theta rhythm phase in rat hippocampus</article-title>. <source>Journal of Neurophysiology</source> <volume>83</volume>: <fpage>2138</fpage>–<lpage>2144</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Klausberger2"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Klausberger</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Márton</surname><given-names>LF</given-names></name>, <name name-style="western"><surname>Baude</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Roberts</surname><given-names>JDB</given-names></name>, <name name-style="western"><surname>Magill</surname><given-names>PJ</given-names></name>, <etal>et al</etal>. (<year>2003</year>) <article-title>Spike timing of dendrite-targeting bistratified cells during hippocampal network oscillations in vivo</article-title>. <source>Nature Neuroscience</source> <volume>7</volume>: <fpage>41</fpage>–<lpage>47</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Marr1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Marr</surname><given-names>D</given-names></name> (<year>1971</year>) <article-title>Simple memory: A theory for archicortex</article-title>. <source>Philosophical Transactions of the Royal Society of London Series B, Biological Sciences</source> <volume>262</volume>: <fpage>23</fpage>–<lpage>81</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Tsien1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tsien</surname><given-names>JZ</given-names></name>, <name name-style="western"><surname>Huerta</surname><given-names>PT</given-names></name>, <name name-style="western"><surname>Tonegawa</surname><given-names>S</given-names></name> (<year>1996</year>) <article-title>The essential role of hippocampal CA1 NMDA receptor–dependent synaptic plasticity in spatial memory</article-title>. <source>Cell</source> <volume>87</volume>: <fpage>1327</fpage>–<lpage>1338</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-BenDayanRubin1"><label>38</label>
<mixed-citation publication-type="other" xlink:type="simple">Ben Dayan Rubin D, Fusi S (2007) Long memory lifetimes require complex synapses and limited sparseness. Frontiers in Computational Neuroscience: 1–7.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Lahiri1"><label>39</label>
<mixed-citation publication-type="book" xlink:type="simple">Lahiri S, Ganguli S (2013) A memory frontier for complex synapses. In: Burges C, Bottou L, Welling M, Ghahramani Z, Weinberger K, editors, Advances in Neural Information Processing Systems 26. pp. 1034–1042. URL <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/4872-a-memory-frontier-for-complex-synapses.pdf" xlink:type="simple">http://papers.nips.cc/paper/4872-a-memory-frontier-for-complex-synapses.pdf</ext-link>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Barbour1"><label>40</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barbour</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Hakim</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Nadal</surname><given-names>JP</given-names></name> (<year>2007</year>) <article-title>What can we learn from synaptic weight distributions?</article-title> <source>Trends in Neurosciences</source> <volume>30</volume>: <fpage>622</fpage>–<lpage>629</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Ikegaya1"><label>41</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ikegaya</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Sasaki</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Ishikawa</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Honma</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Tao</surname><given-names>K</given-names></name>, <etal>et al</etal>. (<year>2013</year>) <article-title>Interpyramid spike transmission stabilizes the sparseness of recurrent network activity</article-title>. <source>Cerebral Cortex</source> <volume>23</volume>: <fpage>293</fpage>–<lpage>304</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Anderson1"><label>42</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Anderson</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Schooler</surname><given-names>LJ</given-names></name> (<year>1991</year>) <article-title>Reflections of the environment in memory</article-title>. <source>Psychological science</source> <volume>2</volume>: <fpage>396</fpage>–<lpage>408</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Savin1"><label>43</label>
<mixed-citation publication-type="book" xlink:type="simple">Savin C, Dayan P, Lengyel M (2013) Correlations strike back (again): the case of associative memory retrieval. In: Burges C, Bottou L, Welling M, Ghahramani Z, Weinberger K, editors, Advances in Neural Information Processing Systems 26. pp. 288–296. URL <ext-link ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/4871-correlations-strike-back-again-the-case-of-associative" xlink:type="simple">http://papers.nips.cc/paper/4871-correlations-strike-back-again-the-case-of-associative</ext-link></mixed-citation>
</ref>
<ref id="pcbi.1003489-Pouget1"><label>44</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pouget</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Zemel</surname><given-names>RS</given-names></name> (<year>2003</year>) <article-title>Inference and computation with population codes</article-title>. <source>Annual Review of Neuroscience</source> <volume>26</volume>: <fpage>381</fpage>–<lpage>410</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Fiser1"><label>45</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fiser</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Berkes</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Orbán</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Lengyel</surname><given-names>M</given-names></name> (<year>2010</year>) <article-title>Statistically optimal perception and learning: from behavior to neural representations</article-title>. <source>Trends in Cognitive Sciences</source> <volume>14</volume>: <fpage>119</fpage>–<lpage>130</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Jezek1"><label>46</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jezek</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Henriksen</surname><given-names>EJ</given-names></name>, <name name-style="western"><surname>Treves</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Moser</surname><given-names>EI</given-names></name>, <name name-style="western"><surname>Moser</surname><given-names>MB</given-names></name> (<year>2011</year>) <article-title>Theta-paced flickering between place-cell maps in the hippocampus</article-title>. <source>Nature</source> <volume>478</volume>: <fpage>246</fpage>–<lpage>249</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Hastings1"><label>47</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hastings</surname><given-names>WK</given-names></name> (<year>1970</year>) <article-title>Monte Carlo sampling methods using Markov chains and their applications</article-title>. <source>Biometrika</source> <volume>75</volume>: <fpage>97</fpage>–<lpage>109</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Simons1"><label>48</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Simons</surname><given-names>JS</given-names></name>, <name name-style="western"><surname>Spiers</surname><given-names>HJ</given-names></name> (<year>2003</year>) <article-title>Prefrontal and medial temporal lobe interactions in long-term memory</article-title>. <source>Nature Reviews Neuroscience</source> <volume>4</volume>: <fpage>637</fpage>–<lpage>648</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Savin2"><label>49</label>
<mixed-citation publication-type="other" xlink:type="simple">Savin C, Dayan P, Lengyel M (2011) Two is better than one: distinct roles for familiarity and recollection in retrieving palimpsest memories. In: Shawe-Taylor J, Zemel R, Bartlett P, Pereira F, Weinberger K, editors, Advances in Neural Information Processing Systems 24, MIT Press. pp. 1305–1313.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Jackson1"><label>50</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jackson</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Redish</surname><given-names>AD</given-names></name> (<year>2007</year>) <article-title>Network dynamics of hippocampal cell-assemblies resemble multiple spatial maps within single tasks</article-title>. <source>Hippocampus</source> <volume>17</volume>: <fpage>1209</fpage>–<lpage>1229</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Dupret1"><label>51</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dupret</surname><given-names>D</given-names></name>, <name name-style="western"><surname>O'Neill</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Csicsvari</surname><given-names>J</given-names></name> (<year>2013</year>) <article-title>Dynamic reconfiguration of hippocampal interneuron circuits during spatial learning</article-title>. <source>Neuron</source> <volume>78</volume>: <fpage>166</fpage>–<lpage>180</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Dayan1"><label>52</label>
<mixed-citation publication-type="book" xlink:type="simple">Dayan P, Abbott L (2001) Theoretical Neuroscience. MIT Press.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Hopfield2"><label>53</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name> (<year>1984</year>) <article-title>Neurons with graded response have collective computational properties like those of two-state neurons</article-title>. <source>Proc Natl Acad Sci</source> <volume>81</volume>: <fpage>3088</fpage>–<lpage>3092</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Buesing1"><label>54</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Buesing</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Bill</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Nessler</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Maass</surname><given-names>W</given-names></name> (<year>2011</year>) <article-title>Neural dynamics as sampling: A model for stochastic computation in recurrent networks of spiking neurons</article-title>. <source>PLoS Computational Biology</source> <volume>7</volume>: <fpage>e1002211</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Leibold1"><label>55</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leibold</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Kempter</surname><given-names>R</given-names></name> (<year>2008</year>) <article-title>Sparseness constrains the prolongation of memory lifetime via synaptic metaplasticity</article-title>. <source>Cerebral cortex (New York, NY: 1991)</source> <volume>18</volume>: <fpage>67</fpage>–<lpage>77</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Fusi2"><label>56</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name> (<year>2007</year>) <article-title>Limits on the memory storage capacity of bounded synapses</article-title>. <source>Nature Neuroscience</source> <volume>10</volume>: <fpage>485</fpage>–<lpage>493</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Ganguly1"><label>57</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ganguly</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Kiss</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Poo</surname><given-names>M</given-names></name> (<year>2000</year>) <article-title>Enhancement of presynaptic neuronal excitability by correlated presynaptic and postsynaptic spiking</article-title>. <source>Nature Neuroscience</source> <volume>3</volume>: <fpage>1018</fpage>–<lpage>1026</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Li1"><label>58</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Li</surname><given-names>CY</given-names></name>, <name name-style="western"><surname>Lu</surname><given-names>JT</given-names></name>, <name name-style="western"><surname>Wu</surname><given-names>CP</given-names></name>, <name name-style="western"><surname>Duan</surname><given-names>Sm</given-names></name>, <name name-style="western"><surname>Poo</surname><given-names>MM</given-names></name> (<year>2004</year>) <article-title>Bidirectional modification of presynaptic neuronal excitability accompanying spike timing-dependent synaptic plasticity</article-title>. <source>Neuron</source> <volume>41</volume>: <fpage>257</fpage>–<lpage>268</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Fldik1"><label>59</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Földiák</surname><given-names>P</given-names></name> (<year>1990</year>) <article-title>Forming sparse representations by local anti-Hebbian learning</article-title>. <source>Biological Cybernetics</source> <volume>64</volume>: <fpage>165</fpage>–<lpage>170</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Triesch1"><label>60</label>
<mixed-citation publication-type="book" xlink:type="simple">Triesch J (2005) Synergies between intrinsic and synaptic plasticity in individual model neurons. In: Saul LK, Weiss Y, Bottou L, editors, Advances in Neural Information Processing Systems 17, Cambridge, MA: MIT Press. pp. 1417–1424.</mixed-citation>
</ref>
<ref id="pcbi.1003489-vanVreeswijk1"><label>61</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Vreeswijk</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Sompolinsky</surname><given-names>H</given-names></name> (<year>1996</year>) <article-title>Chaos in neuronal networks with balanced excitatory and inhibitory activity</article-title>. <source>Science</source> <volume>274</volume>: <fpage>1724</fpage>–<lpage>1726</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Bernacchia1"><label>62</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bernacchia</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Seo</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>XJ</given-names></name> (<year>2011</year>) <article-title>A reservoir of time constants for memory traces in cortical neurons</article-title>. <source>Nature Neuroscience</source> <volume>14</volume>: <fpage>366</fpage>–<lpage>372</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Vogels1"><label>63</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vogels</surname><given-names>TP</given-names></name>, <name name-style="western"><surname>Sprekeler</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Zenke</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Clopath</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name> (<year>2011</year>) <article-title>Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks</article-title>. <source>Science</source> <volume>334</volume>: <fpage>1569</fpage>–<lpage>1573</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Neal1"><label>64</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Neal</surname><given-names>RM</given-names></name> (<year>1996</year>) <article-title>Sampling from multimodal distributions using tempered transitions</article-title>. <source>Statistics and Computing</source> <volume>6</volume>: <fpage>353</fpage>–<lpage>366</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Iba1"><label>65</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Iba</surname><given-names>Y</given-names></name> (<year>2001</year>) <article-title>Extended ensemble Monte Carlo</article-title>. <source>Int J Mod Phys</source> <volume>12</volume>: <fpage>653</fpage>–<lpage>656</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Wang1"><label>66</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname><given-names>XJ</given-names></name>, <name name-style="western"><surname>Buzsáki</surname><given-names>G</given-names></name> (<year>1996</year>) <article-title>Gamma oscillation by synaptic inhibition in a hippocampal interneuronal network model</article-title>. <source>The Journal of Neuroscience</source> <volume>16</volume>: <fpage>6402</fpage>–<lpage>6413</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-McNaughton1"><label>67</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McNaughton</surname><given-names>BL</given-names></name>, <name name-style="western"><surname>Morris</surname><given-names>RGM</given-names></name> (<year>1987</year>) <article-title>Hippocampal synaptic enhancement and information storage within a distributed memory system</article-title>. <source>Trends in Neurosciences</source> <volume>10</volume>: <fpage>408</fpage>–<lpage>415</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Song1"><label>68</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Song</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Sjöström</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Reigl</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Nelson</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Chklovskii</surname><given-names>DB</given-names></name> (<year>2005</year>) <article-title>Highly nonrandom features of synaptic connectivity in local cortical circuits</article-title>. <source>PLoS Biology</source> <volume>3</volume>: <fpage>e68</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Dragoi1"><label>69</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dragoi</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>KD</given-names></name>, <name name-style="western"><surname>Buzsáki</surname><given-names>G</given-names></name> (<year>2003</year>) <article-title>Place representation within hippocampal networks is modified by long-term potentiation</article-title>. <source>Neuron</source> <volume>39</volume>: <fpage>843</fpage>–<lpage>853</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Moser1"><label>70</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Moser</surname><given-names>EI</given-names></name>, <name name-style="western"><surname>Kropff</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Moser</surname><given-names>MB</given-names></name> (<year>2008</year>) <article-title>Place cells, grid cells, and the brain's spatial representation system</article-title>. <source>Annual Review of Neuroscience</source> <volume>31</volume>: <fpage>69</fpage>–<lpage>89</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Liu1"><label>71</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liu</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Ramirez</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Pang</surname><given-names>PT</given-names></name>, <name name-style="western"><surname>Puryear</surname><given-names>CB</given-names></name>, <name name-style="western"><surname>Govindarajan</surname><given-names>A</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>Optogenetic stimulation of a hippocampal engram activates fear memory recall</article-title>. <source>Nature</source> <volume>484</volume>: <fpage>381</fpage>–<lpage>385</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Mori1"><label>72</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Mori</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Gerber</surname><given-names>U</given-names></name> (<year>2002</year>) <article-title>Slow feedback inhibition in the CA3 area of the rat hippocampus by synergistic synaptic activation of mGluR1 and mGluR5</article-title>. <source>The Journal of physiology</source> <volume>544</volume>: <fpage>793</fpage>–<lpage>799</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Freund2"><label>73</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Freund</surname><given-names>TF</given-names></name>, <name name-style="western"><surname>Katona</surname><given-names>I</given-names></name> (<year>2007</year>) <article-title>Perisomatic inhibition</article-title>. <source>Neuron</source> <volume>56</volume>: <fpage>33</fpage>–<lpage>42</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Roudi1"><label>74</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Roudi</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Latham</surname><given-names>PE</given-names></name> (<year>2007</year>) <article-title>A balanced memory network</article-title>. <source>PLoS Computational Biology</source> <volume>3</volume>: <fpage>e141</fpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Haider1"><label>75</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Haider</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Häusser</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Carandini</surname><given-names>M</given-names></name> (<year>2013</year>) <article-title>Inhibition dominates sensory responses in the awake cortex</article-title>. <source>Nature</source> <volume>493</volume>: <fpage>97</fpage>–<lpage>100</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Harvey1"><label>76</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Harvey</surname><given-names>CD</given-names></name>, <name name-style="western"><surname>Collman</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Dombeck</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Tank</surname><given-names>DW</given-names></name> (<year>2009</year>) <article-title>Intracellular dynamics of hippocampal place cells during virtual navigation</article-title>. <source>Nature</source> <volume>461</volume>: <fpage>941</fpage>–<lpage>946</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Wittner1"><label>77</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wittner</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Henze</surname><given-names>DA</given-names></name>, <name name-style="western"><surname>Záborszky</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Buzsáki</surname><given-names>G</given-names></name> (<year>2006</year>) <article-title>Hippocampal CA3 pyramidal cells selectively innervate aspiny interneurons</article-title>. <source>The European Journal of Neuroscience</source> <volume>24</volume>: <fpage>1286</fpage>–<lpage>1298</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Miles1"><label>78</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Miles</surname><given-names>R</given-names></name> (<year>1990</year>) <article-title>Synaptic excitation of inhibitory cells by single CA3 hippocampal pyramidal cells of the guinea-pig in vitro</article-title>. <source>The Journal of Physiology</source> <volume>428</volume>: <fpage>61</fpage>–<lpage>77</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Maurer1"><label>79</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maurer</surname><given-names>AP</given-names></name>, <name name-style="western"><surname>Cowen</surname><given-names>SL</given-names></name>, <name name-style="western"><surname>Burke</surname><given-names>SN</given-names></name>, <name name-style="western"><surname>Barnes</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>McNaughton</surname><given-names>BL</given-names></name> (<year>2006</year>) <article-title>Phase precession in hippocampal interneurons showing strong functional coupling to individual pyramidal cells</article-title>. <source>The Journal of Neuroscience</source> <volume>26</volume>: <fpage>13485</fpage>–<lpage>13492</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Bourne1"><label>80</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bourne</surname><given-names>JN</given-names></name>, <name name-style="western"><surname>Harris</surname><given-names>KM</given-names></name> (<year>2010</year>) <article-title>Coordination of size and number of excitatory and inhibitory synapses results in a balanced structural plasticity along mature hippocampal CA1 dendrites during LTP</article-title>. <source>Hippocampus</source> <volume>21</volume>: <fpage>354</fpage>–<lpage>373</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Sohal1"><label>81</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sohal</surname><given-names>VS</given-names></name>, <name name-style="western"><surname>Hasselmo</surname><given-names>ME</given-names></name> (<year>1998</year>) <article-title>Changes in GABAB modulation during a theta cycle may be analogous to the fall of temperature during annealing</article-title>. <source>Neural computation</source> <volume>10</volume>: <fpage>869</fpage>–<lpage>882</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Csicsvari1"><label>82</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Csicsvari</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Jamieson</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Wise</surname><given-names>KD</given-names></name>, <name name-style="western"><surname>Buzsáki</surname><given-names>G</given-names></name> (<year>2003</year>) <article-title>Mechanisms of gamma oscillations in the hippocampus of the behaving rat</article-title>. <source>Neuron</source> <volume>37</volume>: <fpage>311</fpage>–<lpage>322</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Colgin1"><label>83</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Colgin</surname><given-names>LL</given-names></name>, <name name-style="western"><surname>Denninger</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Fyhn</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Hafting</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Bonnevie</surname><given-names>T</given-names></name>, <etal>et al</etal>. (<year>2009</year>) <article-title>Frequency of gamma oscillations routes flow of information in the hippocampus</article-title>. <source>Nature</source> <volume>462</volume>: <fpage>353</fpage>–<lpage>357</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Carr1"><label>84</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carr</surname><given-names>MF</given-names></name>, <name name-style="western"><surname>Frank</surname><given-names>LM</given-names></name> (<year>2012</year>) <article-title>A single microcircuit with multiple functions: state dependent information processing in the hippocampus</article-title>. <source>Current opinion in neurobiology</source> <volume>22</volume>: <fpage>704</fpage>–<lpage>708</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Montgomery1"><label>85</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Montgomery</surname><given-names>SM</given-names></name>, <name name-style="western"><surname>Buzsáki</surname><given-names>G</given-names></name> (<year>2007</year>) <article-title>Gamma oscillations dynamically couple hippocampal CA3 and CA1 regions during memory task performance</article-title>. <source>Proc Natl Acad Sci</source> <volume>104</volume>: <fpage>14495</fpage>–<lpage>14500</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Leo1"><label>86</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leão</surname><given-names>RN</given-names></name>, <name name-style="western"><surname>Mikulovic</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Leão</surname><given-names>KE</given-names></name>, <name name-style="western"><surname>Munguba</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Gezelius</surname><given-names>H</given-names></name>, <etal>et al</etal>. (<year>2012</year>) <article-title>OLM interneurons differentially modulate CA3 and entorhinal inputs to hippocampal CA1 neurons</article-title>. <source>Nature Neuroscience</source> <volume>15</volume>: <fpage>1524</fpage>–<lpage>1530</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Varga1"><label>87</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Varga</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Golshani</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Soltesz</surname><given-names>I</given-names></name> (<year>2012</year>) <article-title>Frequency-invariant temporal ordering of interneuronal discharges during hippocampal oscillations in awake mice</article-title>. <source>Proc Natl Acad Sci</source> <volume>109</volume>: <fpage>E2726</fpage>–<lpage>34</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Steinvorth1"><label>88</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Steinvorth</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Ulbert</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Schomer</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Halgren</surname><given-names>E</given-names></name> (<year>2010</year>) <article-title>Human entorhinal gamma and theta oscillations selective for remote autobiographical memory</article-title>. <source>Hippocampus</source> <volume>20</volume>: <fpage>166</fpage>–<lpage>73</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Smith1"><label>89</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname><given-names>JD</given-names></name>, <name name-style="western"><surname>Shields</surname><given-names>WE</given-names></name>, <name name-style="western"><surname>Washburn</surname><given-names>DA</given-names></name> (<year>2003</year>) <article-title>The comparative psychology of uncertainty monitoring and metacognition</article-title>. <source>Behavioral and brain sciences</source> <volume>26</volume>: <fpage>317</fpage>–<lpage>339</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Smith2"><label>90</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname><given-names>JD</given-names></name> (<year>2009</year>) <article-title>The study of animal metacognition</article-title>. <source>Trends in Cognitive Sciences</source> <volume>13</volume>: <fpage>389</fpage>–<lpage>396</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Berkes1"><label>91</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Berkes</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Orbán</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Lengyel</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Fiser</surname><given-names>J</given-names></name> (<year>2011</year>) <article-title>Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment</article-title>. <source>Science</source> <volume>331</volume>: <fpage>83</fpage>–<lpage>87</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Sundareswara1"><label>92</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sundareswara</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Schrater</surname><given-names>PR</given-names></name> (<year>2008</year>) <article-title>Perceptual multistability predicted by search model for Bayesian decisions</article-title>. <source>Journal of Vision</source> <volume>8</volume>: <fpage>12.1</fpage>–<lpage>19</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Gershman1"><label>93</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gershman</surname><given-names>SJ</given-names></name>, <name name-style="western"><surname>Vul</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Tenenbaum</surname><given-names>JB</given-names></name> (<year>2012</year>) <article-title>Multistability and perceptual inference</article-title>. <source>Neural computation</source> <volume>24</volume>: <fpage>1</fpage>–<lpage>24</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Hoyer1"><label>94</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hoyer</surname><given-names>PO</given-names></name>, <name name-style="western"><surname>Hyvarinen</surname><given-names>A</given-names></name> (<year>2003</year>) <article-title>Interpreting neural response variability as Monte Carlo sampling of the posterior</article-title>. <source>Advances in neural information processing systems</source> <volume>15</volume>: <fpage>277</fpage>–<lpage>284</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Reichert1"><label>95</label>
<mixed-citation publication-type="book" xlink:type="simple">Reichert D, Seriès P, Storkey AJ (2011) Neuronal adaptation for sampling-based probabilistic inference in perceptual bistability. In: Shawe-Taylor J, Zemel R, Bartlett P, Pereira F, Weinberger K, editors, Advances in Neural Information Processing Systems 24, MIT Press. pp. 2357–2365.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Deng1"><label>96</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Deng</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Mayford</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Gage</surname><given-names>FH</given-names></name> (<year>2013</year>) <article-title>Selection of distinct populations of dentate granule cells in response to inputs as a mechanism for pattern separation in mice</article-title>. <source>eLife</source> <volume>2</volume>: <fpage>e00312</fpage>–<lpage>e00312</lpage>.</mixed-citation>
</ref>
<ref id="pcbi.1003489-Hasselmo1"><label>97</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hasselmo</surname><given-names>ME</given-names></name>, <name name-style="western"><surname>Bower</surname><given-names>JM</given-names></name> (<year>1993</year>) <article-title>Acetylcholine and memory</article-title>. <source>Trends in Neurosciences</source> <volume>16</volume>: <fpage>218</fpage>–<lpage>222</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>