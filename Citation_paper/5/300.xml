<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="EN">
  <front>
    <journal-meta><journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id><journal-id journal-id-type="publisher-id">plos</journal-id><journal-id journal-id-type="pmc">plosone</journal-id><!--===== Grouping journal title elements =====--><journal-title-group><journal-title>PLoS ONE</journal-title></journal-title-group><issn pub-type="epub">1932-6203</issn><publisher>
        <publisher-name>Public Library of Science</publisher-name>
        <publisher-loc>San Francisco, USA</publisher-loc>
      </publisher></journal-meta>
    <article-meta><article-id pub-id-type="publisher-id">PONE-D-11-14751</article-id><article-id pub-id-type="doi">10.1371/journal.pone.0025048</article-id><article-categories>
        <subj-group subj-group-type="heading">
          <subject>Research Article</subject>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Biology</subject>
          <subj-group>
            <subject>Anatomy and physiology</subject>
            <subj-group>
              <subject>Neurological system</subject>
              <subj-group>
                <subject>Synapses</subject>
              </subj-group>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Biochemistry</subject>
            <subj-group>
              <subject>Neurochemistry</subject>
              <subj-group>
                <subject>Synaptic plasticity</subject>
              </subj-group>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Neuroscience</subject>
            <subj-group>
              <subject>Developmental neuroscience</subject>
              <subj-group>
                <subject>Synaptic plasticity</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Neurophysiology</subject>
              <subj-group>
                <subject>Synapses</subject>
              </subj-group>
            </subj-group>
            <subj-group>
              <subject>Learning and memory</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Applied mathematics</subject>
            <subj-group>
              <subject>Game theory</subject>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Probability theory</subject>
            <subj-group>
              <subject>Stochastic processes</subject>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Medicine</subject>
          <subj-group>
            <subject>Anatomy and physiology</subject>
            <subj-group>
              <subject>Neurological system</subject>
              <subj-group>
                <subject>Synapses</subject>
              </subj-group>
            </subj-group>
          </subj-group>
          <subj-group>
            <subject>Mental health</subject>
            <subj-group>
              <subject>Psychology</subject>
              <subj-group>
                <subject>Cognitive psychology</subject>
                <subj-group>
                  <subject>Learning</subject>
                </subj-group>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Physics</subject>
          <subj-group>
            <subject>Statistical mechanics</subject>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline-v2">
          <subject>Social and behavioral sciences</subject>
          <subj-group>
            <subject>Psychology</subject>
            <subj-group>
              <subject>Cognitive psychology</subject>
              <subj-group>
                <subject>Learning</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
        <subj-group subj-group-type="Discipline">
          <subject>Mental Health</subject>
          <subject>Physiology</subject>
          <subject>Neuroscience</subject>
          <subject>Physics</subject>
          <subject>Biochemistry</subject>
          <subject>Mathematics</subject>
        </subj-group>
      </article-categories><title-group><article-title>Learning with a Network of Competing Synapses</article-title><alt-title alt-title-type="running-head">Learning with a Network of Competing Synapses</alt-title></title-group><contrib-group>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Bhat</surname>
            <given-names>Ajaz Ahmad</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Mahajan</surname>
            <given-names>Gaurang</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
        </contrib>
        <contrib contrib-type="author" xlink:type="simple">
          <name name-style="western">
            <surname>Mehta</surname>
            <given-names>Anita</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">
            <sup>1</sup>
          </xref>
          <xref ref-type="aff" rid="aff2">
            <sup>2</sup>
          </xref>
          <xref ref-type="corresp" rid="cor1">
            <sup>*</sup>
          </xref>
        </contrib>
      </contrib-group><aff id="aff1"><label>1</label><addr-line>S N Bose National Centre for Basic Sciences, Salt Lake, Calcutta, India</addr-line>       </aff><aff id="aff2"><label>2</label><addr-line>Institut de Physique Théorique, CEA Saclay, Gif-sur-Yvette, France</addr-line>       </aff><contrib-group>
        <contrib contrib-type="editor" xlink:type="simple">
          <name name-style="western">
            <surname>Perc</surname>
            <given-names>Matjaz</given-names>
          </name>
          <role>Editor</role>
          <xref ref-type="aff" rid="edit1"/>
        </contrib>
      </contrib-group><aff id="edit1">University of Maribor, Slovenia</aff><author-notes>
        <corresp id="cor1">* E-mail: <email xlink:type="simple">anita@bose.res.in</email></corresp>
        <fn fn-type="con">
          <p>Conceived and designed the experiments: GM AM. Performed the experiments: AAB GM. Wrote the paper: AAB GM AM.</p>
        </fn>
      <fn fn-type="conflict">
        <p>The authors have declared that no competing interests exist.</p>
      </fn></author-notes><pub-date pub-type="collection">
        <year>2011</year>
      </pub-date><pub-date pub-type="epub">
        <day>28</day>
        <month>9</month>
        <year>2011</year>
      </pub-date><volume>6</volume><issue>9</issue><elocation-id>e25048</elocation-id><history>
        <date date-type="received">
          <day>1</day>
          <month>8</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>23</day>
          <month>8</month>
          <year>2011</year>
        </date>
      </history><!--===== Grouping copyright info into permissions =====--><permissions><copyright-year>2011</copyright-year><copyright-holder>Bhat et al</copyright-holder><license><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions><abstract>
        <p>Competition between synapses arises in some forms of correlation-based plasticity. Here we propose a game theory-inspired model of synaptic interactions whose dynamics is driven by competition between synapses in their weak and strong states, which are characterized by different timescales. The learning of inputs and memory are meaningfully definable in an effective description of networked synaptic populations. We study, numerically and analytically, the dynamic responses of the effective system to various signal types, particularly with reference to an existing empirical motor adaptation model. The dependence of the system-level behavior on the synaptic parameters, and the signal strength, is brought out in a clear manner, thus illuminating issues such as those of optimal performance, and the functional role of multiple timescales.</p>
      </abstract><funding-group><funding-statement>This work was funded by the Department of Science and Technology, Government of India, under the project “Cognitive Science Initiative.” The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts>
        <page-count count="9"/>
      </counts></article-meta>
  </front>
  <body>
    <sec id="s1">
      <title>Introduction</title>
      <p>Natural neural systems possess the capacity for generating purposeful, relevant and directed behavior in a complex, uncertain and ever-changing environment. At the heart of this capacity is their ability to show adaptive behavioral changes in the face of varying external conditions, to learn efficiently and to retain information reliably in memory. Given that the external sensory world is a complex one and contains a spectrum of dynamic processes spanning a gamut of timescales <xref ref-type="bibr" rid="pone.0025048-Kiebel1">[1]</xref>–<xref ref-type="bibr" rid="pone.0025048-Yu1">[3]</xref>, it may be expected on general grounds that the dynamical system underlying information processing in brains must have a multiplicity of spatial and temporal scales built in, to take cognizance of, respond to, and deal with its complex multi-scale environment. On the one hand, that neural mechanisms of adaptation and cognition involve short as well as long timescale dynamic phenomena, is amply evidenced by experimental work <xref ref-type="bibr" rid="pone.0025048-BrombergMartin1">[4]</xref>–<xref ref-type="bibr" rid="pone.0025048-Ulanovsky1">[7]</xref>. On the other, several theoretical studies <xref ref-type="bibr" rid="pone.0025048-Drew1">[8]</xref>–<xref ref-type="bibr" rid="pone.0025048-Leibold1">[13]</xref> carried out in recent times have explored how the introduction of multiple timescales into computational models of learning and adaptation can affect their functional properties, expanding their capabilities and strengthening links with experiment. To give a few representative examples: the introduction of multiple degrees of plasticity in a model for synaptic “metaplasticity” was shown to achieve a desirable balance between receptivity to new stimuli while remaining immune to degradation of older memories, and to give rise to a power law-type forgetting, not just at the system level <xref ref-type="bibr" rid="pone.0025048-Fusi1">[9]</xref> (consistent with psychophysical findings), but also at the level of a single synapse <xref ref-type="bibr" rid="pone.0025048-Mehta1">[14]</xref>; a neural network model <xref ref-type="bibr" rid="pone.0025048-Yamashita1">[11]</xref> for motor learning with two membrane time constants was able to demonstrate efficient motor learning, generating a functional hierarchy of motor movement, with more elaborate actions being composed out of sequences of shorter, elementary building-block motor ‘primitives’ strung together; an empirical model of motor adaptation <xref ref-type="bibr" rid="pone.0025048-Smith1">[10]</xref> that contained mutually coupled slow and fast timescale sub-systems, produced better agreement (in relation to alternative models) with a series of experimental findings on hand-reaching behavior, which included such phenomena as savings, anterograde interference, and adaptation rebound.</p>
      <p>While learning is integral to neural systems and functionally beneficial at the level of a single individual, many studies have focused on the collective effects of [simple forms of] individual learning and decision-making, i.e. in populations of interacting individuals, or agents. Such distributed systems, exemplifying social or ecological group behavior <xref ref-type="bibr" rid="pone.0025048-Couzin1">[15]</xref>, also share similarities with interacting systems of statistical physics, in the nature of the local “rules” followed by the individual units as well as in the emergent behavior at the macro level, which can under some circumstances display a high degree of order and coordination <xref ref-type="bibr" rid="pone.0025048-Castellano1">[16]</xref>. Game-theoretic approaches <xref ref-type="bibr" rid="pone.0025048-Camerer1">[17]</xref>–<xref ref-type="bibr" rid="pone.0025048-Perc1">[19]</xref> are sometimes brought to bear on such issues, their underlying idea being that the behavior of an individual (its “strategy”) is to a large extent determined by what the other individuals are doing. The strategic choices of an individual are thus guided by those of the others, through considerations of the relative “payoffs” (returns) obtainable in interactive games. In this context, a stochastic model of strategic decision-making was introduced in <xref ref-type="bibr" rid="pone.0025048-Mehta2">[20]</xref>, which captures the essence of the above-stated notion, i.e. selection from among a set of <italic>competing</italic> strategies based on a comparison of the <italic>expected</italic> payoffs from them. Depending upon which of the available strategic alternatives (that are being wielded by the other agents) is found to have the most favorable “outcome” in the local vicinity, every individual appropriately revises its strategic choice.</p>
      <p>Competition between prevalent strategies and adaptive changes at the individual level characterize the sociologically motivated model of <xref ref-type="bibr" rid="pone.0025048-Mehta2">[20]</xref>. Given that these two features of competition and adaptation also generally occur across the framework of activity-induced synaptic plasticity, which is the primary mechanism for learning in biological neural systems <xref ref-type="bibr" rid="pone.0025048-Abbott1">[21]</xref>, <xref ref-type="bibr" rid="pone.0025048-Gerstner1">[22]</xref>, it might be interesting to consider a translation of the notions in <xref ref-type="bibr" rid="pone.0025048-Mehta2">[20]</xref> to the latter context, as a cross-pollinatory attempt of sorts. In other words, a model for synaptic plasticity that incorporates the brand of competition present in the agent-based strategic learning model could be envisaged. A model was delineated in ref. <xref ref-type="bibr" rid="pone.0025048-Mahajan1">[23]</xref> along these lines, with the types or weights of a plastic synapse taking the place of strategies. This model inherently possesses more than a single timescale, which are interpreted here in terms of the [different] effects of each synapse type on the activation rate of a connected neuron. It turns out to be possible to define a rather simple framework for learning and memory, which involves subjecting the network of interconnected neural units to external signals, and following the changes in the average behavior (in terms of the relative abundances of the different synaptic states) of the system as it responds to the external input. A salient result emerging from the analytical approximation carried out in ref. <xref ref-type="bibr" rid="pone.0025048-Mahajan1">[23]</xref> points to the benefit of choosing disparate synaptic timescales for obtaining longer retention times at the network level. This finding echoes earlier results on two timescales of some theoretical analyses mentioned in the opening paragraph above <xref ref-type="bibr" rid="pone.0025048-Smith1">[10]</xref>, <xref ref-type="bibr" rid="pone.0025048-Yamashita1">[11]</xref>. Of possible and perhaps broader significance is also the fact that in our setup, there is a clear-cut connection between the micro-level physical observables and the macroscopic learning/forgetting rates, at least in the analytically tractable <italic>effective</italic> representation within which we work in <xref ref-type="bibr" rid="pone.0025048-Mahajan1">[23]</xref>. This should allow for a more transparent approach to dealing with questions of optimization of the system performance under different learning protocols, so that behavior is given a microscopically understandable basis.</p>
      <p>Taking a cue from the numerical investigations of the motor adaptation model in <xref ref-type="bibr" rid="pone.0025048-Smith1">[10]</xref> to which we have alluded in <xref ref-type="bibr" rid="pone.0025048-Mahajan1">[23]</xref>, here we shall explore the dynamical outcomes of our model of competitive synaptic interactions under a variety of applied time-dependent external signals. This is expected to reveal in more detail the dependence of the system-level adaptational properties on the synaptic time constants, and thus the model's functional scope as well. Also, given the inherent nonlinearity of this model arising from the synaptic interactions at the basic level, it is reasonable to suppose that it would be better suited to modeling memory. Seen in the context of the linear two-timescale system of ref. <xref ref-type="bibr" rid="pone.0025048-Smith1">[10]</xref>, which is essentially empirical in nature, our analysis should provide some clues as to the sort of microscopic approach that would be needed towards obtaining an adequate theoretical underpinning of the ideas presented there.</p>
      <p>This paper is organized along the following lines: the next subsection provides, by way of background, an outline of the original strategic learning model, followed by an account of the model for synaptic plasticity that adopts elements of the former, and which has been treated in detail in ref. <xref ref-type="bibr" rid="pone.0025048-Mahajan1">[23]</xref>. In the subsequent section, the mean-field representation of the network model is briefly summarized. Working within the limits of this effective description, we illustrate with an example how the choice of synaptic parameters has a bearing on the collective timescales associated with learning and forgetting when the system is subjected to a signal. This idea is elaborated further in Section 4, which extends the foregoing analysis to an analytical-cum-numerical exploration of different forms of input signals and the dynamical responses they elicit. These inputs are meant to reflect, at least in essence, some of the experimental protocols considered in <xref ref-type="bibr" rid="pone.0025048-Smith1">[10]</xref>. Finally, the overall picture emerging from our findings is put in perspective, particularly in relation to ref. <xref ref-type="bibr" rid="pone.0025048-Smith1">[10]</xref>, in Section 5, and potential directions for taking the work further are briefly noted.</p>
      <sec id="s1a">
        <title>Background: A model for strategic learning</title>
        <p>The starting point for our proposal in the present work is the model of competitive learning that was introduced, and analyzed from a physics perspective, in <xref ref-type="bibr" rid="pone.0025048-Mehta2">[20]</xref>. Although originally intended to describe sociological phenomena like the diffusion of innovations in connected societies, it can be applied just as well to represent any process which involves, at the elemental level, selection from among a set of competing alternatives.</p>
        <p>In its original formulation, a distributed population of interacting agents is arranged on the sites of a regular lattice, each being ascribed one of two categories: fast (F) or slow (S). In an evolutionary scenario, for example, these types could stand for two contrasting behavioral strategies prevalent in the population. The type associated with every site is not a given, but can keep changing over time as a function of its nearest neighbors' types; this is where competitive selection plays a role. Thus, every agent/site regularly revises its strategic choice, being guided by a pair of rules: a <italic>majority</italic>-type rule reflecting an inherent tendency to side with the local neighborhood, which is followed by an adaptive <italic>performance-based</italic> rule, involving the selection of the type that is perceived to be locally more successful. The notion of success is measured in terms of the random outcomes of the agents in some “game”, with a favorable outcome being ascribed to every F-type (S-type) individual with an <italic>independent</italic> probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e001" xlink:type="simple"/></inline-formula> (resp. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e002" xlink:type="simple"/></inline-formula>). Thus, if an agent is surrounded by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e003" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e004" xlink:type="simple"/></inline-formula>) nearest neighbors of the F (S) type, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e005" xlink:type="simple"/></inline-formula> (resp. <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e006" xlink:type="simple"/></inline-formula>) of which turn out to be successful in a particular trial, the agent arrives at a decision on whether or not to convert by comparing the ratios <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e007" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e008" xlink:type="simple"/></inline-formula>; if, for example, a site is currently associated with the F state, then it will switch to the S type provided that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e009" xlink:type="simple"/></inline-formula>, and remain unchanged otherwise. This rule can be immediately understood by noticing that the ratios just mentioned are nothing but the average payoff per individual ascribed by a site to each of the two types in its neighborhood, assuming of course that success yields a payoff of unity and failure, zero.</p>
        <p>It goes without saying that the above outcome-based updates naturally introduce an element of stochasticity into the population dynamics, owing to the random fluctuations inherent in the estimation of the relative payoffs. In ref. <xref ref-type="bibr" rid="pone.0025048-Mehta2">[20]</xref>, a detailed analysis of this model was carried out under the assumption of <italic>coexistence</italic>, i.e. when <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e010" xlink:type="simple"/></inline-formula>. Its collective behavior, as a function of the parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e011" xlink:type="simple"/></inline-formula>, was shown to exhibit multiple dynamic phases separated by critical phase transitions.</p>
        <p>In ref. <xref ref-type="bibr" rid="pone.0025048-Mahajan1">[23]</xref>, the notion of competition embodied by the above model has been reformulated in the synaptic context. After all, adaptivity is a feature common to both settings, and it is therefore not unnatural to consider the embedding of the rules of the agent-based learning model into a model for synaptic plasticity, and to understand the implications of doing so for suitably defined learning and memory. This was initiated in ref. <xref ref-type="bibr" rid="pone.0025048-Mahajan1">[23]</xref>, and in the present work will be explored in greater detail.</p>
      </sec>
    </sec>
    <sec id="s2">
      <title>Results</title>
      <sec id="s2a">
        <title>Model and effective description</title>
        <p>To begin with we shall sketch our model of competitive synaptic interactions that is based on game theory-inspired ideas. We consider a network consisting of neural units connected by undirected, binary, plastic synapses. (It is pertinent to mention here that in working with symmetric, i.e. undirected synapses, we are following in the footsteps of several previous theoretical studies on neural networks (e.g. <xref ref-type="bibr" rid="pone.0025048-Hopfield1">[24]</xref>–<xref ref-type="bibr" rid="pone.0025048-Ackley1">[26]</xref>); on the other hand, the binary property approximates synapses with discrete weight states, which also appear in previous modeling work <xref ref-type="bibr" rid="pone.0025048-Barrett1">[27]</xref>, <xref ref-type="bibr" rid="pone.0025048-Baldassi1">[28]</xref> and find some experimental support as well <xref ref-type="bibr" rid="pone.0025048-Petersen1">[29]</xref>, <xref ref-type="bibr" rid="pone.0025048-OConnor1">[30]</xref>). Synapses sharing a connected neural unit are treated as mutual neighbors. In a one-dimensional formulation, like the one depicted in <xref ref-type="fig" rid="pone-0025048-g001">Fig. 1</xref>, each synapse will thus be associated with two synaptic neighbors. For simplicity the neurons can be represented by binary threshold units, and the two states of the binary synapse, which are inter-convertible by definition, are assumed to have different weights, which we label as ‘strong’ and ‘weak’ types.</p>
        <fig id="pone-0025048-g001" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0025048.g001</object-id>
          <label>Figure 1</label>
          <caption>
            <title>A model for plastic synapses.</title>
            <p>(a) A networked population of binary synapses connecting neurons in a one-dimensional chain. The synapse <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e012" xlink:type="simple"/></inline-formula>, under consideration for an update, has a ‘strong’ type and a ‘weak’ type neighbor. (b) Two examples of synaptic weight changes (the synaptic configuration is same as above): when neuron B is activated and neuron A is not (upper example), the synaptic current has negative polarity (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e013" xlink:type="simple"/></inline-formula>) and the weight of synapse <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e014" xlink:type="simple"/></inline-formula> is depressed. When both neurons get activated (lower example), the current has net zero polarity (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e015" xlink:type="simple"/></inline-formula>), and therefore the synaptic weight remains unaffected.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0025048.g001" xlink:type="simple"/>
        </fig>
        <p>Under the influence of some ongoing neural activity in the network, the synapses undergo plastic switching from one state to the other. In order to motivate the specific plasticity rules that we introduce, we point out that in the configuration shown in <xref ref-type="fig" rid="pone-0025048-g001">Fig. 1</xref>, where the middle synapse <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e016" xlink:type="simple"/></inline-formula> is under consideration for a state update, the neurons A and B share this middle synapse in common; thus, in comparing how often the two neurons are found activated, one can factor out the influence of the common synapse, when considering averages, and effectively treat the time-averaged activation frequency of either neuron as being determined only by the single, <italic>other</italic> synapse that the neuron is connected to. This “ignoring of the common denominator” essentially implies that the state of neuron A, say, in <xref ref-type="fig" rid="pone-0025048-g001">Fig. 1</xref> can be considered quite reasonably as an “outcome” to be associated with synapse <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e017" xlink:type="simple"/></inline-formula>, and similarly with neuron B and synapse <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e018" xlink:type="simple"/></inline-formula>; thus, neurons can be thought of as taking on the identities of the respective synapses. Recalling the obvious similarity of this situation with that of the abstract model of <xref ref-type="bibr" rid="pone.0025048-Mehta2">[20]</xref>, and taking this analogy further, we set forth the following rules governing the synaptic weight changes, which have an anti-Hebbian flavor: for clarity, we shall first associate with every possible pair of neural outcomes a “polarized” signal, that depends on the states of the two neurons as well as the types of both the adjacent synapses (i.e. the neighbors of the synapse that is being considered for updating at that moment). A positively (negatively) polarized signal is realized when the synapse connects a strong neuron with a weak neuron, <italic>and</italic> the strong (weak) neuron alone is activated. All other possible combinations of neuron types and activation states are associated with zero or unpolarized current. With this definition of polarity, it is proposed that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e019" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e020" xlink:type="simple"/></inline-formula>) whenever there is a positively (negatively) polarized current, and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e021" xlink:type="simple"/></inline-formula> in all other cases. Furthermore, to be consistent with the binary nature of the synapses, it is assumed that a strengthening event would effect a weak to strong conversion, while leaving an already strong synapse unchanged (the corresponding logic would hold for a weak synapse). Thus, loosely speaking, the two synapses adjacent to any given synapse “compete” to decide its type, and this continues to happen repeatedly across the entire network. Competition, albeit in other forms, is found to occur in some other models of correlation-based plasticity also <xref ref-type="bibr" rid="pone.0025048-Gerstner1">[22]</xref>, <xref ref-type="bibr" rid="pone.0025048-Miller1">[31]</xref>, <xref ref-type="bibr" rid="pone.0025048-Song1">[32]</xref>.</p>
        <p>In order to make some mathematical headway in analyzing such a network of interacting synapses, we shall consider the update dynamics of a single <italic>effective</italic> synapse, that in some sense represents the average state of the whole network. To begin with, in such a picture, the neural outcomes are assumed to be uncorrelated at different locations, and treated as independent random variables, with the probability for activation being obtainable from the time-averaged activation frequency of the neuron. Consistent with the situation described in the previous paragraph, that the effect of the common synapse can be left out on average in comparing the outcomes of its connected neurons, we associate, with each neuron, a probability for activation at any instant that is <italic>only</italic> a function of the other neighboring synapse, being equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e022" xlink:type="simple"/></inline-formula> (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e023" xlink:type="simple"/></inline-formula>) for a strong (weak) type synapse. Thus, in <xref ref-type="fig" rid="pone-0025048-g001">Fig. 1(b)</xref>, the probability for activation of neuron A is equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e024" xlink:type="simple"/></inline-formula>, and that for neuron B is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e025" xlink:type="simple"/></inline-formula>; both are independent of the state of the middle synapse that is under consideration for updating.</p>
        <p>Having defined these quantities it is rather straightforward to work out the probabilities for potentiating or depressing events to occur at a candidate synapse, given the identities of its neighbors. This is illustrated with an example. Say a synapse has one neighbor of each type, as is depicted in <xref ref-type="fig" rid="pone-0025048-g001">fig. 1</xref>. For this configuration, a total of four outcomes for the neuronal pair A–B are possible. There will be no weight changes if both neurons get activated (giving an unpolarized synaptic current) or if both remain silent; the likelihood of this happening is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e026" xlink:type="simple"/></inline-formula>. A depressing event (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e027" xlink:type="simple"/></inline-formula>) occurs if neuron B fires but neuron A remains inactive, and this has a probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e028" xlink:type="simple"/></inline-formula>. The only remaining possibility is that neuron A gets activated and neuron B does not. This occurs with a probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e029" xlink:type="simple"/></inline-formula>, and is accompanied by potentiation (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e030" xlink:type="simple"/></inline-formula>).</p>
        <p>Our rules for activity-induced weight changes may have been motivated by game-theoretic notions of strategic competition, but when seen in the light of earlier work on rate-based models of synaptic plasticity, a case can be made for their reasonableness at least in relation to other earlier proposals in the field. In continuous-time models, the firing rate of the neuron, rather than its membrane potential, is taken as the basic dynamical variable, and synaptic plasticity is a continuous process that depends on the firing rates of the pre- and post-synaptic neurons. The dynamical equation describing the time evolution of the synaptic weight usually involves some non-linear function of pre/post-synaptic activities and the weight itself, and in some cases, a dependence on averages of the firing rates over some temporal window has also been motivated <xref ref-type="bibr" rid="pone.0025048-Gerstner1">[22]</xref>, <xref ref-type="bibr" rid="pone.0025048-Dayan1">[33]</xref>, <xref ref-type="bibr" rid="pone.0025048-Bienenstock1">[34]</xref>. Drawing on such approaches, we speculate that the rules for synaptic plasticity proposed in the previous paragraph might also be realizable through an iterative, discrete equation symbolically expressed as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e031" xlink:type="simple"/></inline-formula> <xref ref-type="bibr" rid="pone.0025048-Gerstner1">[22]</xref>, where the form of the non-linear function <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e032" xlink:type="simple"/></inline-formula> is chosen to provide an appropriate fit to the plasticity rules. Here, <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e033" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e034" xlink:type="simple"/></inline-formula> represent the activity states of the two connected neurons, which are binary variables in the present set-up, being either active (1) or inactive (0). The symmetric form of the argument of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e035" xlink:type="simple"/></inline-formula> is in keeping with the bi-directional nature of the synapses and ensures that the synaptic response is insensitive to the spatial direction of any current, while still being sensitive to its polarity. The inclusion of time averages of the activity of the connected neurons allows for a characterization of the strengths of the neighboring synapses in this picture, and hence allows for the determination of the polarity of any current at the synapse.</p>
        <p>Returning to our previously mentioned intention of having an analytically pliable representation of the system dynamics – even though it could be numerically simulated by using a range of updates, as in the case of the game-theoretic model <xref ref-type="bibr" rid="pone.0025048-Mehta2">[20]</xref> – we consider a mean-field version of the model. The idea behind the mean-field approximation is that we look at the average behavior in an infinite system. This, at one stroke, deals with two problems: first, there are no fluctuations associated with system size, and second, the approximation that we have made in ignoring the “self-coupling” of the synapse is better realized.</p>
        <p>In the mean-field representation, every synapse is assigned a probability (uniform over the lattice) to be either strong (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e036" xlink:type="simple"/></inline-formula>) or weak (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e037" xlink:type="simple"/></inline-formula>), so that spatial variation is ignored, as are fluctuations and correlations. This single effective degree of freedom allows for a description of the system in terms of its fixed point dynamics. The rate of change of the probability <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e038" xlink:type="simple"/></inline-formula>, say, (which in the limit of large system size is equivalent to the fraction of strong units) with time, is computed by taking into account only the nearest-neighbor synaptic interactions, via the rules defined earlier. The dynamical equation for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e039" xlink:type="simple"/></inline-formula> assumes the following form:<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0025048.e040" xlink:type="simple"/><label>(1)</label></disp-formula>with the transition probabilities being given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0025048.e041" xlink:type="simple"/><label>(2)</label></disp-formula>The fractions of strong and weak types are, of course, normalized by definition: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e042" xlink:type="simple"/></inline-formula>.</p>
        <p>The implicitly time-dependent transition probabilities, which incorporate the effect of nearest-neighbor coupling, introduce non-linearity into the dynamics, an obvious departure from the linear coupled equations of <xref ref-type="bibr" rid="pone.0025048-Smith1">[10]</xref>. The deterministic dynamics of Eq. 1 yields stationary states (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e043" xlink:type="simple"/></inline-formula>) to which the system would relax exponentially starting from an arbitrary initial state. Besides the trivial unstable fixed points at 0 and 1 corresponding to homogeneous, absorbing states (all units being one or the other type), the algebraic equation <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e044" xlink:type="simple"/></inline-formula> also possesses a stable solution; this is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0025048.e045" xlink:type="simple"/><label>(3)</label></disp-formula>(Of course, in the presence of fluctuations, e.g. associated with finite system sizes in mean-field, or in the full solution of the stochastic equations, we would expect the trivial fixed points to be absorbing, and the stable fixed point associated with Eq. 3 to be metastable). The time scale for relaxation to this fixed point is the other dynamically relevant quantity, which again can be extracted from Eq. 1 and is given by<disp-formula><graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0025048.e046" xlink:type="simple"/><label>(4)</label></disp-formula>This system-level relaxation time is the central quantity with regard to learning and forgetting protocols (see e.g. <xref ref-type="bibr" rid="pone.0025048-Smith1">[10]</xref>). It depends on the synapse-level outcome probabilities <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e047" xlink:type="simple"/></inline-formula>, and varies with the location of the corresponding fixed point. It is instructive to illustrate this dependence in the (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e048" xlink:type="simple"/></inline-formula>) plane, and this has been done in <xref ref-type="fig" rid="pone-0025048-g002">Fig. 2</xref>. Such a picture suggests a possible way of defining learning and retention in the coarse-grained representation. To do so, we first define a general time-dependent signal as a ‘perturbation’ of the system parameters (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e049" xlink:type="simple"/></inline-formula>) having the following form: <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e050" xlink:type="simple"/></inline-formula>. This choice of signal definition is motivated by taking into account the fact that <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e051" xlink:type="simple"/></inline-formula> plays the role of a ‘biasing field’. This has been argued earlier <xref ref-type="bibr" rid="pone.0025048-Mehta2">[20]</xref> by means of an analogy with spin models; it can also be inferred from the results of applying linear response theory to the original model <xref ref-type="bibr" rid="pone.0025048-Bhat1">[35]</xref>. Moreover, this way the signal is being applied to both the parameters, rather than preferentially to only one of them. Thus, the application of a signal of this form has the effect of introducing a time dependence into the system parameters.</p>
        <fig id="pone-0025048-g002" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0025048.g002</object-id>
          <label>Figure 2</label>
          <caption>
            <title>Relaxation timescales.</title>
            <p>Inverse time constant for relaxation (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e052" xlink:type="simple"/></inline-formula>) as a function of the synaptic parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e053" xlink:type="simple"/></inline-formula>, obtained in the one-dimensional effective representation (see Eq. 4). The dashed line corresponds to the diagonal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e054" xlink:type="simple"/></inline-formula>, along which the range of imposable signals is maximized. The dark regions near the corners correspond to default configurations with long retention times.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0025048.g002" xlink:type="simple"/>
        </fig>
        <p>It is easy to work out the consequence of the above signal definition for the simplest example of a constant input signal: the fixed point would shift to a new location along a <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e055" xlink:type="simple"/></inline-formula> constant line in the (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e056" xlink:type="simple"/></inline-formula>) plane. One can, then, imagine a protocol whereby a constant signal is switched on at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e057" xlink:type="simple"/></inline-formula> and persists up to a time <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e058" xlink:type="simple"/></inline-formula>, following which the system reverts to its original state. Learning and forgetting are both exponential relaxation processes in this setting, and two timescales naturally enter the picture: the learning time constant for moving to the new stable state (after the signal is applied), and the forgetting time constant for reverting to the default fixed point once the signal is turned off. It may be noted that, since the relaxation timescale is a function of the parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e059" xlink:type="simple"/></inline-formula> of the <italic>end</italic> state, whatever that may be, it is in general different for the learning and the forgetting: the former depends on the values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e060" xlink:type="simple"/></inline-formula> in the presence of the signal, while the latter depends on the unperturbed state.</p>
        <p>One of the strengths of the preceding approach is that performance optimization can be directly related to the microscopic parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e061" xlink:type="simple"/></inline-formula> (in contrast to the approach of <xref ref-type="bibr" rid="pone.0025048-Smith1">[10]</xref> where optimization relied on the relative values of multiplicative constants). <xref ref-type="fig" rid="pone-0025048-g002">Fig. 2</xref> suggests an approach to optimizing the performance in the particular scenario under consideration, i.e. achieving long forgetting times and typically shorter learning times: by choosing the default parameters in such a way that the unperturbed state of the system lies near the lower right corner (or the upper left corner), the timescale for retention, being only a function of the unperturbed state, can be made very long, with the average timescale for learning applied signals being shorter. This limit corresponds to having a wide separation between the timescales associated with the two parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e062" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e063" xlink:type="simple"/></inline-formula>. (If, alternatively, one were to choose the default values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e064" xlink:type="simple"/></inline-formula> to lie closer to the middle of the graph, the forgetting time constant would be shortened, clearly an undesirable feature.) It may be noted that translating the default state along the diagonal line given by <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e065" xlink:type="simple"/></inline-formula> only modifies the retention time, while leaving the range of signals that can be absorbed, and thus the <italic>average</italic> learning time, unchanged. Additionally, one observes that given the form of the signal as defined above, which can only produce shifts parallel to the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e066" xlink:type="simple"/></inline-formula> diagonal, the range of allowed signals is maximal when the system stays on this diagonal, rather than on any other line parallel to it.</p>
        <p>It should be clear from the preceding discussion that we now have a framework in place for studying the responses of the effective system to arbitrary input signals having more general forms of time dependence. We would like to build on the simplest case considered above, and in the next section, carry out a similar exercise of linking system performance to the physically meaningful synaptic parameters for other signal profiles representative of more complex learning protocols. In particular, it would be worthwhile to ascertain if optimal parameter settings can similarly be located in the parameter space of the effective system.</p>
      </sec>
      <sec id="s2b">
        <title>Analyzing various protocols</title>
        <p>We start by reconsidering the first example discussed above, that of a constant signal which is present for a specific duration of time. This signal profile is depicted in <xref ref-type="fig" rid="pone-0025048-g003">Fig. 3(a)</xref>. The response of the system in terms of the time-dependent relative abundance of strong synapses is illustrated for some different choices of the parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e067" xlink:type="simple"/></inline-formula> in <xref ref-type="fig" rid="pone-0025048-g004">Fig. 4</xref>; these curves have been obtained by numerically evolving the effective equation for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e068" xlink:type="simple"/></inline-formula>. The preceding analysis led us to infer that the system shows long forgetting times (associated with exponential relaxation) when the synaptic time constants are well separated, which is also observable in <xref ref-type="fig" rid="pone-0025048-g004">Fig. 4</xref>. Moreover, the effective mean-field representation has the property that the learning time (but not the retention time) shows a dependence on the signal strength as well as the default values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e069" xlink:type="simple"/></inline-formula>. Since, in some situations, quick learning is as important as slow forgetting, another quantity of interest is the ratio of forgetting to learning times (which we label as <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e070" xlink:type="simple"/></inline-formula>). This quantity will obviously be a function of the signal strength (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e071" xlink:type="simple"/></inline-formula>) too; thus, it is not hard to imagine that for a given system, there will be a particular value of the signal strength that will yield an optimal value for <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e072" xlink:type="simple"/></inline-formula>. Continuing to confine ourselves to the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e073" xlink:type="simple"/></inline-formula> diagonal (so as to maximize the range of allowable inputs, as mentioned earlier) so that the default system is essentially parametrized by a single variable (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e074" xlink:type="simple"/></inline-formula>, say) now, the performance with respect to the parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e075" xlink:type="simple"/></inline-formula> can be visualized, as before, on a two-dimensional (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e076" xlink:type="simple"/></inline-formula>) plane. This is shown in the color-coded plots of <xref ref-type="fig" rid="pone-0025048-g005">Figs. 5(a)</xref> &amp; (b), which correspond to the analytically computed values and the estimates obtained by numerical simulation (see <xref ref-type="sec" rid="s4">Methods</xref> section for details) respectively; they show hardly any difference. Recall that for any given value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e077" xlink:type="simple"/></inline-formula>, only a certain range of signal values is meaningfully imposable, and this fact is reflected in the phase diagram which does not span the entire range of the <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e078" xlink:type="simple"/></inline-formula> plane. While the dependence of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e079" xlink:type="simple"/></inline-formula> on the value of the signal strength <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e080" xlink:type="simple"/></inline-formula> is readily apparent, it is also clear that close to the extremal values of the parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e081" xlink:type="simple"/></inline-formula> (which correspond to having disparate synaptic efficacies) the system does much better <italic>overall</italic>, with higher values of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e082" xlink:type="simple"/></inline-formula> being attainable in these limits, over a wider range of signal strengths. This result is in agreement with our earlier finding regarding the “functional” benefit of having well-separated synaptic parameters <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e083" xlink:type="simple"/></inline-formula>.</p>
        <fig id="pone-0025048-g003" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0025048.g003</object-id>
          <label>Figure 3</label>
          <caption>
            <title>Temporal forms of the protocols analyzed in the text.</title>
            <p>(a) Signal - No Signal (De-adaptation); (b) Signal - Half-Signal (Downscaling); (c) Signal - Reverse Signal (AI).</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0025048.g003" xlink:type="simple"/>
        </fig>
        <fig id="pone-0025048-g004" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0025048.g004</object-id>
          <label>Figure 4</label>
          <caption>
            <title>De-adaptation protocol.</title>
            <p>Time dependence of the dynamic variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e084" xlink:type="simple"/></inline-formula> representing the state of the synaptic population in the effective description, to a signal that is imposed until the system reaches saturation (see <xref ref-type="fig" rid="pone-0025048-g003">Fig. 3(a)</xref>). The curves for different settings of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e085" xlink:type="simple"/></inline-formula> have been translated vertically to meet the baseline for comparative analysis. The black, red and blue curves correspond to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e086" xlink:type="simple"/></inline-formula> (0.5, 0.5), (0.3, 0.7) and (0.2, 0.8) respectively, and the signal strength is fixed at <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e087" xlink:type="simple"/></inline-formula>.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0025048.g004" xlink:type="simple"/>
        </fig>
        <fig id="pone-0025048-g005" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0025048.g005</object-id>
          <label>Figure 5</label>
          <caption>
            <title>De-adaptation and synaptic parameters.</title>
            <p>Variation of the ratio <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e088" xlink:type="simple"/></inline-formula> of the forgetting timescale to the learning timescale for the Signal - No Signal protocol over the two-dimensional <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e089" xlink:type="simple"/></inline-formula> plane (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e090" xlink:type="simple"/></inline-formula> has been chosen here). The values of ln <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e091" xlink:type="simple"/></inline-formula> are shown color-coded in the panel to the right of each plot. Both the analytical (a) and numerical (b) estimates are shown.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0025048.g005" xlink:type="simple"/>
        </fig>
        <p>The signal example that was just considered also appears in a simulation of deadaptation in the context of the motor adaptation model of <xref ref-type="bibr" rid="pone.0025048-Smith1">[10]</xref>. Proceeding along these lines, we next consider two other types of signals, which also are intended to be variants of experimental paradigms considered in <xref ref-type="bibr" rid="pone.0025048-Smith1">[10]</xref>. These are referred to as Downscaling and Anterograde Interference (AI), and their forms are shown in <xref ref-type="fig" rid="pone-0025048-g003">Figs. 3(b)</xref> &amp; (c).</p>
        <p>Let us consider the downscaling signal first. It consists of a phase of constant input, followed by a phase during which the original input is reduced (in our case halved) in magnitude, rather than being completely removed. Just as before, the temporal response of the effective variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e092" xlink:type="simple"/></inline-formula> to such a signal is illustrated in <xref ref-type="fig" rid="pone-0025048-g006">Fig. 6</xref> with different realizations of the synaptic parameter <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e093" xlink:type="simple"/></inline-formula> and some fixed value of signal strength; these curves have been obtained by simulating the effective equation, Eq. 1, by evolving from the default fixed point state. In order to obtain a clearer picture of the variation of the behavior with the basic synaptic parameter as well as the strength of the signal, we consider the corresponding visualization in <xref ref-type="fig" rid="pone-0025048-g007">Figs. 7(a)</xref> &amp; (b), which display the analytical and numerical estimates respectively in the allowed region. As before, the two results are extremely similar, although obviously not identical. The quantity that is depicted here is the ratio between the downscaling time (i.e. the timescale associated with relaxing to the fixed point corresponding to the half signal) and the timescale for the initial learning (i.e. the relaxation time to go from the original, unperturbed default state to the fixed point in the presence of the full signal). The joint dependence on the two parameters is brought out in a fairly clear manner, especially to do with the following idea: configurations with <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e094" xlink:type="simple"/></inline-formula> closer to extremal values have a slower downscaling rate in general, although the performance of any given system certainly depends on the choice of signal strength also.</p>
        <fig id="pone-0025048-g006" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0025048.g006</object-id>
          <label>Figure 6</label>
          <caption>
            <title>Downscaling protocol.</title>
            <p>Time variation of the effective variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e095" xlink:type="simple"/></inline-formula> in response to a signal that is reduced by half after a certain period of time (see <xref ref-type="fig" rid="pone-0025048-g003">Fig. 3(b)</xref>). The curves for different choices of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e096" xlink:type="simple"/></inline-formula> have been translated vertically to meet the baseline for comparative analysis. The black, red and blue curves correspond to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e097" xlink:type="simple"/></inline-formula> (0.5, 0.5), (0.3, 0.7) and (0.2, 0.8) respectively, and the (fixed) signal value is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e098" xlink:type="simple"/></inline-formula>.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0025048.g006" xlink:type="simple"/>
        </fig>
        <fig id="pone-0025048-g007" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0025048.g007</object-id>
          <label>Figure 7</label>
          <caption>
            <title>Downscaling and synaptic parameters.</title>
            <p>Variation of the ratio of the downscaling timescale to the timescale of initial learning for the Signal - Half Signal protocol over the two-dimensional <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e099" xlink:type="simple"/></inline-formula> plane (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e100" xlink:type="simple"/></inline-formula> is set equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e101" xlink:type="simple"/></inline-formula>). The values of the logarithm of the ratio are shown color-coded in the panel to the right of each plot. Both the analytical (a) and numerical (b) estimates are shown.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0025048.g007" xlink:type="simple"/>
        </fig>
        <p>We next deal with the signal type mimicking the protocol for anterograde interference (refer to <xref ref-type="fig" rid="pone-0025048-g003">Fig. 3(c)</xref>). The basic idea here is to apply a constant input signal for a specific duration, and to follow this up by reversing the <italic>sign</italic> of the applied signal, while leaving its magnitude unchanged. Such a scenario was considered in the original context <xref ref-type="bibr" rid="pone.0025048-Smith1">[10]</xref> in order to probe the effect that previous learning might have on the subsequent adaptation to an oppositely directed input; in other words, whether past learning could remain imprinted at a deeper level even after appearing to have been erased, thus interfering with the receptiveness to future inputs. For our effective model, the response to such a paradigm is exemplified by the numerically obtained curves in <xref ref-type="fig" rid="pone-0025048-g008">Fig. 8</xref> for three different parameter (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e102" xlink:type="simple"/></inline-formula>) choices and a fixed signal strength. Once again, we consider the functional dependence on the synaptic time constants, examining the ratio of the relearning time (for relaxation to the reversed, negative input) to the initial learning time (the timescale for exponential relaxation to the fixed point corresponding to the originally imposed signal), both of which of course also depend on the value of the signal. (For details of the accompanying numerical simulation, see the <xref ref-type="sec" rid="s4">Methods</xref> section.) <xref ref-type="fig" rid="pone-0025048-g009">Figures 9(a)</xref> &amp; (b) show a more complex response of the system than we have seen hitherto. It is clear that both the signal strength <italic>and</italic> its orientation need to be factored in; thus for a typical default value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e103" xlink:type="simple"/></inline-formula> lying in (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e104" xlink:type="simple"/></inline-formula>), a positive signal (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e105" xlink:type="simple"/></inline-formula>) shows more anterograde interference than a negative one. (The situation is reversed when we consider <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e106" xlink:type="simple"/></inline-formula> in (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e107" xlink:type="simple"/></inline-formula>).) This is explained by the observation that a positive signal adds to the strength of the stronger synaptic type, causing more retention of the original signal, and hence a greater time associated with unlearning this to learn an oppositely directed signal.</p>
        <fig id="pone-0025048-g008" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0025048.g008</object-id>
          <label>Figure 8</label>
          <caption>
            <title>Anterograde interference.</title>
            <p>Response of the synaptic population variable <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e108" xlink:type="simple"/></inline-formula> to the application of a signal followed by its reversal (see <xref ref-type="fig" rid="pone-0025048-g003">Fig. 3(c)</xref>). The curves for different choices of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e109" xlink:type="simple"/></inline-formula> have been translated vertically to meet the baseline for comparative analysis. The black, red and blue curves correspond to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e110" xlink:type="simple"/></inline-formula> (0.5, 0.5), (0.3, 0.7) and (0.2, 0.8) respectively, and the (fixed) signal value is <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e111" xlink:type="simple"/></inline-formula>.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0025048.g008" xlink:type="simple"/>
        </fig>
        <fig id="pone-0025048-g009" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pone.0025048.g009</object-id>
          <label>Figure 9</label>
          <caption>
            <title>Anterograde interference and synaptic parameters.</title>
            <p>Variation of the ratio of the timescale for learning the reversed signal to the timescale for initial learning for the Signal - Reversed Signal protocol over the two-dimensional <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e112" xlink:type="simple"/></inline-formula> plane (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e113" xlink:type="simple"/></inline-formula> is set equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e114" xlink:type="simple"/></inline-formula>). The values of the logarithm of the ratio are shown color-coded in the panel to the right of each plot. Both analytical (a) and numerical (b) estimates are displayed.</p>
          </caption>
          <graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0025048.g009" xlink:type="simple"/>
        </fig>
        <p>Our observations on the various protocols have been based on analyses carried out within a mean-field framework, and involve comparisons between timescales of learning and forgetting. These are obtained analytically, as well as by independent simulation, by examining the dynamical relaxation to the fixed points associated with the “bare” system as well as with the system in the presence of a signal. If, therefore, a signal is relearnt before it has been entirely forgotten, it is conceivable that “savings” <xref ref-type="bibr" rid="pone.0025048-Smith1">[10]</xref> will be manifested; in particular, this effect may be expected to be quite significant where forgetting is slow, i.e. where <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e115" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e116" xlink:type="simple"/></inline-formula> are well separated. This is also consistent with the fact that we get reasonable results for another history-dependent phenomenon, that of anterograde interference, where the system takes longer to learn the reversed signal in the relevant parameter regimes. Our main reason for not probing this further in this paper is that the mean-field approximation, which we have chosen for analytical reasons, is not really the best way to look at the developing correlations associated with history; on the other hand, the full numerical simulation of the exact model will automatically introduce the necessary correlations, and this is a subject that we leave for future work.</p>
      </sec>
    </sec>
    <sec id="s3">
      <title>Discussion</title>
      <p>A theoretical model such as ours, which introduces game theory-inspired notions of competitive interactions into the field of activity-dependent synaptic plasticity, naturally raises questions about its experimental utility. The main motivation for our model was in fact experimental work concerning motor adaptation <xref ref-type="bibr" rid="pone.0025048-Smith1">[10]</xref>, where we wished to provide a more microscopic basis to the coupled linear equations presented there. While our equation is formally similar to that presented in <xref ref-type="bibr" rid="pone.0025048-Smith1">[10]</xref>, the important difference is that it is non-linear, and that microscopic observables, rather than parameters, determine the timescales associated with learning and forgetting. The formal similarity however suggests that many of the protocols can be applied to our mean-field system, with rather similar results to <xref ref-type="bibr" rid="pone.0025048-Smith1">[10]</xref>, as shown in the last two sections. It suggests also that if correlations are included by solving our full equations to a level that is higher than mean-field, we will be able to incorporate some of the more sophisticated features of the experiments of <xref ref-type="bibr" rid="pone.0025048-Smith1">[10]</xref>. The non-linearity of our dynamical equations, and the fact that they show rudiments of network-level memory even in this fairly simple representation, are strong positives. Additionally, learning times in our model scale with the strength of the applied signal, but not the forgetting time, a feature that may actually have some empirical support <xref ref-type="bibr" rid="pone.0025048-Bogartz1">[36]</xref>. This feature introduces an additional dimension of complexity into the dynamics, suggesting for instance, that to every system there corresponds a particular choice of “preferred” signal strength to which it is most receptive, in the sense of learning it the quickest (for an extension of this to the phenomenon of hearing, see <xref ref-type="bibr" rid="pone.0025048-Camalet1">[37]</xref>). More importantly, the present analytical approach provides a rather transparent link between the quantities characterizing the system performance in this case the various relaxation rates and those defined at the fundamental micro-level of neurons and synapses (<inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e117" xlink:type="simple"/></inline-formula> and <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e118" xlink:type="simple"/></inline-formula>). This obviates the need to have to fish around for optimal parameters, and provides a better handle on matters having to do with the locating of the right synaptic parameters corresponding to a particular behavior.</p>
      <p>Referring back to the plots of the previous section for the protocols considered, specifically those corresponding to de-adaptation and downscaling, we note that the choice of disparate synaptic timescales (directly relatable to the synaptic weights) is in general associated with more favorable performance, in the sense of efficient learning and longer retention, even when signal strengths are variable. This broad conclusion is consistent with the findings of ref. <xref ref-type="bibr" rid="pone.0025048-Smith1">[10]</xref>, whose model consisting of coupled fast and slow components also shows markedly improved agreement with the results of hand-reaching experiments, in comparison with its single-timescale competitors.</p>
      <p>Before concluding, we mention some possible extensions of this work. One can, of course, go beyond mean field, retaining spatial information, and correlations, in the determination of the neuronal outcomes. The effect of doing so would be to introduce additional degrees of freedom, and corresponding timescales, into the description of the system dynamics. In a pair-approximation <xref ref-type="bibr" rid="pone.0025048-Mehta2">[20]</xref> for instance, which takes into account the correlations between nearest neighbors, the macro-level dynamics would contain <italic>two</italic> relaxation rates, and show richer behavior. Thus, working with such a higher-order effective representation might lead to a more fruitful link with experiment. An interesting possibility would also be to look at directed synapses, which would in fact amount to changing the update rules <xref ref-type="bibr" rid="pone.0025048-Bhat1">[35]</xref> of the present model. Also, we would like to incorporate ideas from spike-timing-dependent plasticity <xref ref-type="bibr" rid="pone.0025048-Babadi1">[38]</xref> into our model, to give some of our timescales a firmer microscopic underpinning. Last but by no means least, the inclusion of our model synapses into realistic networks is a major goal, to put our work in the context of recent models of brain learning <xref ref-type="bibr" rid="pone.0025048-deArcangelis1">[39]</xref>; the use of game theory in evolving networks <xref ref-type="bibr" rid="pone.0025048-Szolnoki1">[40]</xref> would be of particular use in this endeavour.</p>
      <p>To summarize, we have explored a novel model for synaptic plasticity, sketched in earlier work <xref ref-type="bibr" rid="pone.0025048-Mahajan1">[23]</xref>, which incorporates a notion of competition (between synapses in their distinct states) appropriate to decision-making paradigms in fluctuating circumstances. An approximate, coarse-grained description for obtaining learning behavior has been explored at length. While highlighting interesting features, including the effect of choosing dissimilar timescales along the lines of a body of previous work, our approach also suggests that more involved quantitative treatment may lead to more concrete connections with experiment.</p>
    </sec>
    <sec id="s4" sec-type="methods">
      <title>Methods</title>
      <p>The effective equation Eq. 1 is evolved numerically in the presence of the chosen signal type <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e119" xlink:type="simple"/></inline-formula>. For every simulation, the starting value of <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e120" xlink:type="simple"/></inline-formula> is assumed to correspond to the fixed point of the default, unperturbed system. Once a signal <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e121" xlink:type="simple"/></inline-formula> is turned on, the system is allowed to evolve from its initial state until the time that it “saturates” at a new state; this is implemented by imposing the condition that the fractional change in <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e122" xlink:type="simple"/></inline-formula> over one time-step be less than a certain tolerance limit (chosen to be equal to <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e123" xlink:type="simple"/></inline-formula> here). The signal is assumed to remain applied until this condition is reached, at which point the system may be considered to have “learnt” the signal. Thereafter the signal is removed or changed to a new value depending on the protocol. The time taken to reach saturation, beginning from the moment the signal was switched on, is taken to be the timescale for the corresponding process.</p>
      <p>In the de-adaptation and downscaling simulations, the determination of both the involved timescales (initial learning followed by forgetting or downscaling respectively) is straightforward and follows the procedure outlined above. The third protocol (signal - reversed signal) involves reversing the sign of an applied signal, and estimating the time for learning this reversed signal. For obtaining this time, the system is first allowed to evolve back from the stable state that was attained in the presence of the initially applied signal, to the <italic>default level</italic> (i.e. the unperturbed system state). The time for learning the reversed signal is only measured from this point onwards, until <inline-formula><inline-graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0025048.e124" xlink:type="simple"/></inline-formula> saturates at the fixed point corresponding to the reversed signal (following the procedure of the previous paragraph). In other words, the system is first allowed to “forget” the initially imposed signal (by getting back to its default state), and then made to “learn” the sign-reversed counterpart.</p>
    </sec>
  </body>
  <back>
    <ack>
      <p>AM thanks the Institut de Physique Théorique, where part of this work was carried out, for its hospitality.</p>
    </ack>
    <ref-list>
      <title>References</title>
      <ref id="pone.0025048-Kiebel1">
        <label>1</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Kiebel</surname><given-names>SJ</given-names></name><name name-style="western"><surname>Daunizeau</surname><given-names>J</given-names></name><name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name></person-group>             <year>2008</year>             <article-title>A Hierarchy of Time-Scales and the Brain.</article-title>             <source>PLoS Comput Biol</source>             <volume>4</volume>             <issue>11</issue>             <fpage>e1000209</fpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Voss1">
        <label>2</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Voss</surname><given-names>RF</given-names></name><name name-style="western"><surname>Clarke</surname><given-names>J</given-names></name></person-group>             <year>1975</year>             <article-title>1/f noise in music and speech.</article-title>             <source>Nature</source>             <volume>258</volume>             <fpage>317</fpage>             <lpage>318</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Yu1">
        <label>3</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Yu</surname><given-names>Y</given-names></name><name name-style="western"><surname>Romero</surname><given-names>R</given-names></name><name name-style="western"><surname>Lee</surname><given-names>TS</given-names></name></person-group>             <year>2005</year>             <article-title>Preference of Sensory Neural Coding for 1/f Signals.</article-title>             <source>Phys Rev Lett</source>             <volume>94</volume>             <issue>10</issue>             <fpage>108103</fpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-BrombergMartin1">
        <label>4</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bromberg-Martin</surname><given-names>SE</given-names></name><name name-style="western"><surname>Matsumoto</surname><given-names>M</given-names></name><name name-style="western"><surname>Nakahara</surname><given-names>H</given-names></name><name name-style="western"><surname>Hikosaka</surname><given-names>O</given-names></name></person-group>             <year>2010</year>             <article-title>Multiple timescales of memory in lateral habenula and dopamine neurons.</article-title>             <source>Neuron</source>             <volume>67</volume>             <issue>3</issue>             <fpage>499</fpage>             <lpage>510</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Hasson1">
        <label>5</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hasson</surname><given-names>U</given-names></name><name name-style="western"><surname>Yang</surname><given-names>E</given-names></name><name name-style="western"><surname>Vallines</surname><given-names>I</given-names></name><name name-style="western"><surname>Heeger</surname><given-names>DJ</given-names></name><name name-style="western"><surname>Rubin</surname><given-names>N</given-names></name></person-group>             <year>2008</year>             <article-title>A Hierarchy of Temporal Receptive Windows in Human Cortex.</article-title>             <source>The Journal of Neuroscience</source>             <volume>28</volume>             <issue>10</issue>             <fpage>2539</fpage>             <lpage>2550</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Fairhall1">
        <label>6</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fairhall</surname><given-names>AL</given-names></name><name name-style="western"><surname>Lewen</surname><given-names>GD</given-names></name><name name-style="western"><surname>Bialek</surname><given-names>W</given-names></name><name name-style="western"><surname>Steveninck</surname><given-names>RRDRV</given-names></name></person-group>             <year>2000</year>             <article-title>Multiple Timescales of Adaptation in a Neural Code.</article-title>             <comment>In Proceedings of NIPS 124–130</comment>          </element-citation>
      </ref>
      <ref id="pone.0025048-Ulanovsky1">
        <label>7</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ulanovsky</surname><given-names>N</given-names></name><name name-style="western"><surname>Las</surname><given-names>L</given-names></name><name name-style="western"><surname>Farkas</surname><given-names>D</given-names></name><name name-style="western"><surname>Nelken</surname><given-names>I</given-names></name></person-group>             <year>2004</year>             <article-title>Multiple Time Scales of Adaptation in Auditory Cortex Neurons.</article-title>             <source>The Journal of Neuroscience</source>             <volume>24</volume>             <issue>46</issue>             <fpage>10440</fpage>             <lpage>10453</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Drew1">
        <label>8</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Drew</surname><given-names>P</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>L</given-names></name></person-group>             <year>2006</year>             <article-title>Models and Properties of Power-Law Adaptation in Neural Systems.</article-title>             <source>J Neurophysiol</source>             <volume>96</volume>             <fpage>826</fpage>             <lpage>833</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Fusi1">
        <label>9</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name><name name-style="western"><surname>Drew</surname><given-names>PJ</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name></person-group>             <year>2005</year>             <article-title>Cascade Models of Synaptically Stored Memories.</article-title>             <source>Neuron</source>             <volume>45</volume>             <fpage>599</fpage>             <lpage>611</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Smith1">
        <label>10</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Smith</surname><given-names>MA</given-names></name><name name-style="western"><surname>Ghazizadeh</surname><given-names>A</given-names></name><name name-style="western"><surname>Shadmehr</surname><given-names>R</given-names></name></person-group>             <year>2006</year>             <article-title>Interacting adaptive processes with different timescales underlie short-term motor learning.</article-title>             <source>PLoS Biol</source>             <volume>4</volume>             <issue>6</issue>             <fpage>e179</fpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Yamashita1">
        <label>11</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Yamashita</surname><given-names>Y</given-names></name><name name-style="western"><surname>Tani</surname><given-names>J</given-names></name></person-group>             <year>2008</year>             <article-title>Emergence of functional hierarchy in a multiple timescale neural network model: a humanoid robot experiment.</article-title>             <source>PLoS Comput Biol</source>             <volume>4</volume>             <issue>11</issue>             <fpage>e1000220</fpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Fusi2">
        <label>12</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Fusi</surname><given-names>S</given-names></name><name name-style="western"><surname>Asaad</surname><given-names>WF</given-names></name><name name-style="western"><surname>Miller</surname><given-names>EK</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X-J</given-names></name></person-group>             <year>2007</year>             <article-title>A neural circuit model of flexible sensori-motor mapping: learning and forgetting on multiple timescales.</article-title>             <source>Neuron</source>             <volume>54</volume>             <fpage>319</fpage>             <lpage>333</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Leibold1">
        <label>13</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Leibold</surname><given-names>C</given-names></name><name name-style="western"><surname>Kempter</surname><given-names>R</given-names></name></person-group>             <year>2008</year>             <article-title>Sparseness Constrains the Prolongation of Memory Lifetime via Synaptic Metaplasticity.</article-title>             <source>Cereb Cortex</source>             <volume>18</volume>             <issue>1</issue>             <fpage>67</fpage>             <lpage>77</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Mehta1">
        <label>14</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mehta</surname><given-names>A</given-names></name><name name-style="western"><surname>Luck</surname><given-names>JM</given-names></name></person-group>             <year>2011</year>             <article-title>Power-law forgetting in synapses with metaplasticity.</article-title>             <comment>In press, JSTAT</comment>          </element-citation>
      </ref>
      <ref id="pone.0025048-Couzin1">
        <label>15</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Couzin</surname><given-names>ID</given-names></name></person-group>             <year>2009</year>             <article-title>Collective cognition in animal groups.</article-title>             <source>Trends in Cognit Sc</source>             <volume>13</volume>             <issue>1</issue>             <fpage>36</fpage>             <lpage>43</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Castellano1">
        <label>16</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Castellano</surname><given-names>C</given-names></name><name name-style="western"><surname>Fortunato</surname><given-names>S</given-names></name><name name-style="western"><surname>Loreto</surname><given-names>V</given-names></name></person-group>             <year>2009</year>             <article-title>Statistical physics of social dynamics.</article-title>             <source>Rev of Modern Phys</source>             <volume>81</volume>             <issue>2</issue>             <fpage>591</fpage>             <lpage>646</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Camerer1">
        <label>17</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Camerer</surname><given-names>CF</given-names></name></person-group>             <year>2003</year>             <article-title>Behavioural studies of strategic thinking in games.</article-title>             <source>Trends Cogn Sci,</source>             <volume>7</volume>             <fpage>225</fpage>             <lpage>231</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Szaboa1">
        <label>18</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Szaboa</surname><given-names>G</given-names></name><name name-style="western"><surname>Fath</surname><given-names>G</given-names></name></person-group>             <year>2007</year>             <article-title>Evolutionary games on graphs.</article-title>             <source>Physics Reports</source>             <volume>446</volume>             <fpage>97</fpage>             <lpage>216</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Perc1">
        <label>19</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Perc</surname><given-names>M</given-names></name><name name-style="western"><surname>Szolnoki</surname><given-names>A</given-names></name></person-group>             <year>2010</year>             <article-title>Coevolutionary games - A mini review.</article-title>             <source>BioSystems</source>             <volume>99</volume>             <fpage>109</fpage>             <lpage>125</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Mehta2">
        <label>20</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mehta</surname><given-names>A</given-names></name><name name-style="western"><surname>Luck</surname><given-names>JM</given-names></name></person-group>             <year>1999</year>             <article-title>Models of competitive learning: Complex dynamics, intermittent conversions, and oscillatory coarsening.</article-title>             <source>Phys Rev E</source>             <volume>60</volume>             <issue>5</issue>             <fpage>5218</fpage>             <lpage>5230</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Abbott1">
        <label>21</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name><name name-style="western"><surname>Nelson</surname><given-names>SB</given-names></name></person-group>             <year>2000</year>             <article-title>Synaptic Plasticity: Taming the beast.</article-title>             <source>Nat Neurosci</source>             <volume>3</volume>             <fpage>1178</fpage>             <lpage>1183</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Gerstner1">
        <label>22</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gerstner</surname><given-names>W</given-names></name><name name-style="western"><surname>Kistler</surname><given-names>WM</given-names></name></person-group>             <year>2002</year>             <source>Spiking Neuron Models: Single Neurons, Populations, Plasticity</source>             <publisher-name>Cambridge University Press: Cambridge</publisher-name>          </element-citation>
      </ref>
      <ref id="pone.0025048-Mahajan1">
        <label>23</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Mahajan</surname><given-names>G</given-names></name><name name-style="western"><surname>Mehta</surname><given-names>A</given-names></name></person-group>             <year>2011</year>             <article-title>Competing synapses with two timescales as a basis for learning and forgetting.</article-title>             <source>Europhys Lett</source>             <volume>95</volume>             <fpage>48008</fpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Hopfield1">
        <label>24</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group>             <year>1982</year>             <article-title>Neural networks and physical systems with emergent collective computational abilities.</article-title>             <source>Proc Nat Acad Sci (USA)</source>             <volume>79</volume>             <fpage>2554</fpage>             <lpage>2558</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Gardner1">
        <label>25</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Gardner</surname><given-names>E</given-names></name><name name-style="western"><surname>Derrida</surname><given-names>B</given-names></name></person-group>             <year>1988</year>             <article-title>Optimal storage properties of neural network models.</article-title>             <source>J Phys A: Math Gen</source>             <volume>21</volume>             <fpage>271</fpage>             <lpage>284</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Ackley1">
        <label>26</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Ackley</surname><given-names>D</given-names></name><name name-style="western"><surname>Hinton</surname><given-names>G</given-names></name><name name-style="western"><surname>Sejnowski</surname><given-names>T</given-names></name></person-group>             <year>1985</year>             <article-title>A learning algorithm for Boltzmann machines.</article-title>             <source>Cognitive Science</source>             <volume>9</volume>             <issue>1</issue>             <fpage>147</fpage>             <lpage>169</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Barrett1">
        <label>27</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Barrett</surname><given-names>AB</given-names></name><name name-style="western"><surname>van Rossum</surname><given-names>MCW</given-names></name></person-group>             <year>2008</year>             <article-title>Optimal learning rules for discrete synapses.</article-title>             <source>PLoS Comput Biol</source>             <volume>4</volume>             <issue>11</issue>             <fpage>e10000230</fpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Baldassi1">
        <label>28</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Baldassi</surname><given-names>C</given-names></name><name name-style="western"><surname>Braunstein</surname><given-names>A</given-names></name><name name-style="western"><surname>Brunel</surname><given-names>N</given-names></name><name name-style="western"><surname>Zecchina</surname><given-names>R</given-names></name></person-group>             <year>2007</year>             <article-title>Efficient supervised learning in networks with binary synapses.</article-title>             <source>Proc Nat Acad Sci (USA)</source>             <volume>104</volume>             <issue>26</issue>             <fpage>11079</fpage>             <lpage>11084</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Petersen1">
        <label>29</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Petersen</surname><given-names>CCH</given-names></name><name name-style="western"><surname>Malenka</surname><given-names>RC</given-names></name><name name-style="western"><surname>Nicoll</surname><given-names>RA</given-names></name><name name-style="western"><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group>             <year>1998</year>             <article-title>All-or-none potentiation at CA3-CA1 synapses.</article-title>             <source>Proc Nat Acad Sci (USA)</source>             <volume>95</volume>             <fpage>4732</fpage>             <lpage>4737</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-OConnor1">
        <label>30</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>O'Connor</surname><given-names>DH</given-names></name><name name-style="western"><surname>Wittenberg</surname><given-names>GM</given-names></name><name name-style="western"><surname>Wang</surname><given-names>SSH</given-names></name></person-group>             <year>2005</year>             <article-title>Graded bidirectional synaptic plasticity is composed of switch-like unitary events.</article-title>             <source>Proc Nat Acad Sci (USA)</source>             <volume>102</volume>             <fpage>9679</fpage>             <lpage>9684</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Miller1">
        <label>31</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Miller</surname><given-names>KD</given-names></name></person-group>             <year>1996</year>             <article-title>Synaptic Economics: Competition and Cooperation in Correlation-Based Synaptic Plasticity.</article-title>             <source>Neuron</source>             <volume>17</volume>             <fpage>371</fpage>             <lpage>374</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Song1">
        <label>32</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Song</surname><given-names>S</given-names></name><name name-style="western"><surname>Miller</surname><given-names>KD</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name></person-group>             <year>2000</year>             <article-title>Competitive Hebbian learning through spike-timing-dependent synaptic plasticity.</article-title>             <source>Nature Neurosci</source>             <volume>3</volume>             <fpage>919</fpage>             <lpage>926</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Dayan1">
        <label>33</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Dayan</surname><given-names>P</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name></person-group>             <year>2001</year>             <source>Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems</source>             <publisher-name>MIT Press: Cambridge</publisher-name>          </element-citation>
      </ref>
      <ref id="pone.0025048-Bienenstock1">
        <label>34</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bienenstock</surname><given-names>EL</given-names></name><name name-style="western"><surname>Cooper</surname><given-names>LN</given-names></name><name name-style="western"><surname>Munro</surname><given-names>PW</given-names></name></person-group>             <year>1982</year>             <article-title>Theory for the development of neuron selectivity: Orientation specificity and binocular interaction in visual cortex.</article-title>             <source>J Neurosci</source>             <volume>2</volume>             <fpage>32</fpage>             <lpage>48</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Bhat1">
        <label>35</label>
        <element-citation publication-type="other" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bhat</surname><given-names>AA</given-names></name><name name-style="western"><surname>Mehta</surname><given-names>A</given-names></name></person-group>             <year>2011</year>             <article-title>Varying facets of a model of competitive learning: the role of updates and memory.</article-title>             <comment>arXiv: 1105.0523</comment>          </element-citation>
      </ref>
      <ref id="pone.0025048-Bogartz1">
        <label>36</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Bogartz</surname><given-names>RS</given-names></name></person-group>             <year>1990</year>             <article-title>Evaluating forgetting curves psychologically.</article-title>             <source>J Expt Psych: Learning, Memory, and Cognition</source>             <volume>16</volume>             <issue>1</issue>             <fpage>138</fpage>             <lpage>148</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Camalet1">
        <label>37</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Camalet</surname><given-names>S</given-names></name><name name-style="western"><surname>Duke</surname><given-names>T</given-names></name><name name-style="western"><surname>Julicher</surname><given-names>F</given-names></name><name name-style="western"><surname>Prost</surname><given-names>J</given-names></name></person-group>             <year>2000</year>             <article-title>Auditory sensitivity provided by self-tuned critical oscillations of hair cells.</article-title>             <source>Proc Nat Acad Sci (USA)</source>             <volume>97</volume>             <fpage>3183</fpage>             <lpage>3188</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Babadi1">
        <label>38</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Babadi</surname><given-names>B</given-names></name><name name-style="western"><surname>Abbott</surname><given-names>LF</given-names></name></person-group>             <year>2010</year>             <article-title>Intrinsic Stability of Temporally Shifted Spike-Timing Dependent Plasticity.</article-title>             <source>PLoS Comput Biol</source>             <volume>6</volume>             <issue>11</issue>             <fpage>e1000961</fpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-deArcangelis1">
        <label>39</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>de Arcangelis</surname><given-names>L</given-names></name><name name-style="western"><surname>Herrmann</surname><given-names>HJ</given-names></name></person-group>             <year>2010</year>             <article-title>Learning as a phenomenon occurring in a critical state.</article-title>             <source>Proc Natl Acad Sci</source>             <volume>107</volume>             <fpage>3977</fpage>             <lpage>3981</lpage>          </element-citation>
      </ref>
      <ref id="pone.0025048-Szolnoki1">
        <label>40</label>
        <element-citation publication-type="journal" xlink:type="simple">             <person-group person-group-type="author"><name name-style="western"><surname>Szolnoki</surname><given-names>A</given-names></name><name name-style="western"><surname>Perc</surname><given-names>M</given-names></name></person-group>             <year>2009</year>             <article-title>Resolving social dilemmas on evolving random networks. Europhys Lett 86:30007; Szolnoki A, Perc M (2009) Emergence of multilevel selection in the prisoner's dilemma game on coevolving random networks.</article-title>             <source>New Journal of Physics</source>             <volume>11</volume>             <fpage>093033</fpage>          </element-citation>
      </ref>
    </ref-list>
    
  </back>
</article>