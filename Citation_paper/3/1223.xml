<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pone.0207119</article-id>
<article-id pub-id-type="publisher-id">PONE-D-18-26498</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Diagnostic medicine</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Radiology and imaging</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Functional magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Reaction time</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Reaction time</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neural networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject><subj-group><subject>Color vision</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject><subj-group><subject>Color vision</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Sensory perception</subject><subj-group><subject>Vision</subject><subj-group><subject>Color vision</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Graph measures in task-based fMRI: Functional integration during read-out of visual and auditory information</article-title>
<alt-title alt-title-type="running-head">Graph measures of visual and auditory information integration in fMRI</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" xlink:type="simple">
<name name-style="western">
<surname>Quante</surname>
<given-names>Laura</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" equal-contrib="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0691-794X</contrib-id>
<name name-style="western">
<surname>Kluger</surname>
<given-names>Daniel S.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Visualization</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Bürkner</surname>
<given-names>Paul C.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Ekman</surname>
<given-names>Matthias</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Formal analysis</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Schubotz</surname>
<given-names>Ricarda I.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Conceptualization</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Project administration</role>
<role content-type="http://credit.casrai.org/">Supervision</role>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff004"><sup>4</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Psychology, University of Münster, Münster, Germany</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Otto-Creutzfeldt-Center for Cognitive and Behavioral Neuroscience, University of Münster, Münster, Germany</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Donders Institute for Brain, Cognition and Behaviour, Radboud University Nijmegen, Nijmegen, Netherlands</addr-line></aff>
<aff id="aff004"><label>4</label> <addr-line>Department of Neurology, University Hospital Cologne, Cologne, Germany</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Lin</surname>
<given-names>Pan</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>South-Central University for Nationalities, CHINA</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">d.kluger@wwu.de</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>15</day>
<month>11</month>
<year>2018</year>
</pub-date>
<pub-date pub-type="collection">
<year>2018</year>
</pub-date>
<volume>13</volume>
<issue>11</issue>
<elocation-id>e0207119</elocation-id>
<history>
<date date-type="received">
<day>10</day>
<month>9</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>25</day>
<month>10</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-year>2018</copyright-year>
<copyright-holder>Quante et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0207119"/>
<abstract>
<p>This study investigated how attending to auditory and visual information systematically changes graph theoretical measures of integration and functional connectivity between three network modules: auditory, visual, and a joint task core. Functional MRI BOLD activity was recorded while healthy volunteers attended to colour and/or pitch information presented within an audiovisual stimulus sequence. Network nodes and modules were based on peak voxels of BOLD contrasts, including colour and pitch sensitive brain regions as well as the dorsal attention network. Network edges represented correlations between nodes’ activity and were computed separately for each condition. Connection strength was increased between the task and the visual module when participants attended to colour, and between the task and the auditory module when they attended to pitch. Moreover, several nodal graph measures showed consistent changes to attentional modulation in form of stronger integration of sensory regions in response to attention. Together, these findings corroborate dynamical adjustments of both modality-specific and modality-independent functional brain networks in response to task demands and their representation in graph theoretical measures.</p>
</abstract>
<funding-group>
<funding-statement>The authors received no specific funding for this work.</funding-statement>
</funding-group>
<counts>
<fig-count count="4"/>
<table-count count="6"/>
<page-count count="18"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>The data underlying the results presented in the study are available from the Open Science Framework and are accessible under the following URL: <ext-link ext-link-type="uri" xlink:href="https://osf.io/m9hc3/" xlink:type="simple">https://osf.io/m9hc3/</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Integration and segregation in global brain communication are necessary prerequisites for complex behavior [<xref ref-type="bibr" rid="pone.0207119.ref001">1</xref>–<xref ref-type="bibr" rid="pone.0207119.ref003">3</xref>]. While segregation means that distributed areas work independently from one another and serve specialised functions, integration denotes a global coordinative coupling of functionally distinct brain regions.</p>
<p>Using network analysis and graph theoretical measures, the human brain has been shown to be organised in functionally specialised modules [<xref ref-type="bibr" rid="pone.0207119.ref004">4</xref>–<xref ref-type="bibr" rid="pone.0207119.ref008">8</xref>] and a small number of highly connected and topologically central brain regions, the <italic>connective core</italic> (also called <italic>rich club</italic>; [<xref ref-type="bibr" rid="pone.0207119.ref009">9</xref>–<xref ref-type="bibr" rid="pone.0207119.ref012">12</xref>]). The core’s connectivity profile makes it the ideal structure to integrate information from different brain regions. Following Shanahan’s <italic>connective core hypothesis</italic> [<xref ref-type="bibr" rid="pone.0207119.ref013">13</xref>], this central module enables and guides communication between all other brain regions. Parallel computation and competition between brain regions–as coordinated by the connective core—result in the formation of dynamic coalitions of specific brain regions, which in turn determine behavior. Such coalitions are formed in a serial manner, corresponding to a sequential shift between states of integration and segregation [<xref ref-type="bibr" rid="pone.0207119.ref003">3</xref>, <xref ref-type="bibr" rid="pone.0207119.ref013">13</xref>–<xref ref-type="bibr" rid="pone.0207119.ref014">14</xref>].</p>
<p>Empirical studies have indeed shown that the level of integration within the brain changes depending on task demands [<xref ref-type="bibr" rid="pone.0207119.ref015">15</xref>–<xref ref-type="bibr" rid="pone.0207119.ref018">18</xref>]. Cohen and D’Esposito, for example, found that a working memory task was accompanied by greater integrative communication within the brain when compared to a simple motor task [<xref ref-type="bibr" rid="pone.0207119.ref018">18</xref>]. In addition, Cole and colleagues [<xref ref-type="bibr" rid="pone.0207119.ref019">19</xref>] demonstrated that the fronto-parietal network shows flexible functional connectivity depending on task demands, supporting the idea of a connective core dynamically orchestrating brain processes. Further support for this idea was provided by Ekman and colleagues [<xref ref-type="bibr" rid="pone.0207119.ref020">20</xref>]: In their study, participants prepared for a colour/motion discrimination task. During preparation, colour regions showed higher integration with core regions when colour discrimination was prepared, and reduced integration when motion discrimination was prepared (vice versa for motion regions). In other words, connections between core and periphery dynamically and systematically changed depending on the task to prepare.</p>
<p>In the present study, we investigated changes in core-periphery interaction when participants attended to auditory vs. visual information. Participants were required to attend to colour, pitch, or both colour and pitch information in an audiovisual stimulus sequence. To ensure modality-specific attentional engagement, subjects were asked to not only attend to but read out visual and auditory information to perform a visual search task at the end of each sequence. Importantly, when they interpreted colour, pitch or both colour and pitch as requested, this visual search was facilitated, and performance was enhanced. Note that we defined the joint task network as "core" whereas visual and auditory areas were defined as "periphery". Functionally, the joint task network was specific to the present paradigm and taken to reflect the translation of pitch and/or colour information into the spatial domain for the subsequent search task. Based on Shanahan [<xref ref-type="bibr" rid="pone.0207119.ref013">13</xref>] and in line with Ekman and colleagues [<xref ref-type="bibr" rid="pone.0207119.ref020">20</xref>], we hypothesised temporarily stronger links between task core and task-relevant sensory areas. Specifically, we expected increased functional connectivity between the core and the visual module when participants attended to visual stimuli, and between core and the auditory module when participants attended to auditory stimuli. In either case, there should be stronger integration of the task-specific regions within the network. We assessed integration in terms of functional connectivity and different graph theoretical measures.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Materials and methods</title>
<sec id="sec003">
<title>Participants</title>
<p>Twenty-eight right-handed volunteers (18–29 years, mean 23.7 ± 2.83 SD years old, 7 male) with normal or corrected-to-normal vision participated in the experiment after giving written informed consent. None of them reported a history of medical, neurological or psychiatric disorders or substance abuse. Participants were compensated with course credit or payment. Two additional participants were excluded because of drop out and heavy leg movements during the fMRI session. The study protocol was conducted in accordance with ethical standards of the Declaration of Helsinki and approved by the local ethics committee of the University of Münster.</p>
</sec>
<sec id="sec004">
<title>Task</title>
<p>Participants exploited auditory and visual information from a 12-second stimulus sequence to subsequently predict the location of a target in a visual search display (<xref ref-type="fig" rid="pone.0207119.g001">Fig 1</xref>). Every trial started with a fixation cross (200 ms) followed by a cue (1300 ms). The deterministic cue indicated which source of information was predictive of the target location and therefore allowed participants to focus on auditory information (A) and/or visual information (V). The cues consisted of two letters (<italic>VA</italic>, <italic>VX</italic>, <italic>AX</italic>, or <italic>XX</italic>) and instructed participants to read out both visual and auditory information (VA), only auditory information (AX), only visual information (VX), or that neither modality was informative (XX). If indicated by the cue, visual information (colour) predicted that the target would appear in the upper or lower half of the search display (e.g., red–upper half, blue–lower half) and auditory information (pitch) indicated that the target would appear in the right or left half of the search display (e.g., high–right half, low–left half). Stimulus-target associations (i.e., red/blue colour–upper/lower half and low/high pitch–right/left side) were balanced across participants. Importantly, in case of condition VA, participants could restrict their visual search to one quarter of the search display, in condition VX and AX to one half. In condition XX, visual search could not be spatially restricted.</p>
<fig id="pone.0207119.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0207119.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Illustration of the trial design for the four experimental conditions.</title>
<p>The area framed in red represents the display location that could be predicted by the participant based on information provided by the audio-visual sequence (note that during the real presentation, no red frame was visible). Functional connectivity analysis was based on data recorded during the entire audio-visual sequence (12 s).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0207119.g001" xlink:type="simple"/>
</fig>
<p>The end of the audiovisual sequence was followed by a fixation cross (200 ms) and a blank screen (500 ms) before the visual search display appeared. When the search display appeared, participants were asked to respond with their right index finger to indicate when they detected the target. Next, the response display showed up and participants indicated whether the target was a regular letter “L” (left middle finger) or rotated (left index finger). After the second button press, the next trial started.</p>
<p>Every participant was presented 48 VA trials, 24 VX trials, 24 AX trials, and 48 XX trials in pseudo-randomised order, with overall balanced transition probabilities.</p>
</sec>
<sec id="sec005">
<title>Stimuli</title>
<sec id="sec006">
<title>Audio-visual sequences</title>
<p>Participants were presented sequences of 11 audio-visual stimuli which consisted of a visual pattern and a piano-like chord played simultaneously. The visual pattern was a random arrangement of 25 non-overlapping coloured dots, with a dot diameter of 0.3° of visual angle. The dot cloud covered a square area of about 7° of visual angle and was centered on the screen. Each dot was coloured in one of the following five colours: red (RGB values: 255, 0, 0), blue (0, 0, 255), or intermediate mixtures thereof (255, 0, 128; 255, 0, 255; 128, 0, 255). The background was grey (160, 160, 160). Individual stimuli were created with <italic>MATLAB</italic> (The MathWorks, Inc., Natick, Massachusetts, USA) and compiled to videos with <italic>Windows Movie Maker</italic> (Microsoft Cooperation). In each video, stimuli 1 to 10 were presented for 1250 ms, and the last stimulus was presented for 2000 ms. There was a crossfading period of 250 ms, where participants were presented a merged image of two consecutive stimuli. This created a smooth transition from one stimulus to another, resulting in a video duration of 12 seconds. Each video started with a standard stimulus, consisting of a balanced mixture of coloured dots (5 dots per colour). Over the course of the next 10 stimuli presented in each trial, dots changed their colour incrementally until every dot had the same colour (either red or blue). We generated eight different standard stimuli and three types of stepwise changes, leading to 2 final states (red, blue) x 8 standard stimuli x 3 sequence types = 48 different videos.</p>
<p>Auditory stimuli were constructed using synthetic string samples from the <italic>EastWest Colossus sound library</italic> (Native Instruments, Berlin, Germany). Trials were made up of ten successive variations of a five-note <italic>standard</italic> chord (C<sup>maj9</sup>, c–d–e–g–b): Starting with the chord split over five octaves (one note per octave, lasting 1000 ms), notes were individually transposed to the next octave in pseudorandom order every 1000 ms. Following a transposition, the respective note resonated for another 250 ms, creating a smooth transition between chord variations. This way, all notes were gradually shifted upwards or downwards over the course of an auditory sequence until the final chord (lasting 2000 ms) exclusively comprised notes from either the highest or the lowest octave. All notes and corresponding frequencies across octaves are shown in <xref ref-type="table" rid="pone.0207119.t001">Table 1</xref>. As for the visual sequences, we generated 48 different auditory sequences.</p>
<table-wrap id="pone.0207119.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0207119.t001</object-id>
<label>Table 1</label> <caption><title>Notes and corresponding frequencies in Hz across octaves.</title></caption>
<alternatives>
<graphic id="pone.0207119.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0207119.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center" colspan="6">Octave</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" rowspan="6">Note</td>
<td align="center"/>
<td align="center">2</td>
<td align="center">3</td>
<td align="center">4</td>
<td align="center">5</td>
<td align="center">6</td>
</tr>
<tr>
<td align="center">c</td>
<td align="center">65.41</td>
<td align="center">130.81</td>
<td align="center">261.63</td>
<td align="center">523.25</td>
<td align="center">1046.50</td>
</tr>
<tr>
<td align="center">d</td>
<td align="center">73.42</td>
<td align="center">146.83</td>
<td align="center">293.66</td>
<td align="center">587.33</td>
<td align="center">1174.66</td>
</tr>
<tr>
<td align="center">e</td>
<td align="center">82.41</td>
<td align="center">164.81</td>
<td align="center">329.63</td>
<td align="center">659.25</td>
<td align="center">1318.51</td>
</tr>
<tr>
<td align="center">g</td>
<td align="center">98.00</td>
<td align="center">196.00</td>
<td align="center">392.00</td>
<td align="center">783.99</td>
<td align="center">1567.98</td>
</tr>
<tr>
<td align="center">b</td>
<td align="center">123.47</td>
<td align="center">246.94</td>
<td align="center">493.88</td>
<td align="center">987.77</td>
<td align="center">1975.53</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>For every participant, auditory and visual sequences were randomly paired to build 12 audio-visual sequences of each type (as classified by the final stimulus: blue-high, blue-low, red-high and red-low). Every audio-visual sequence was presented three times per participant: once within condition VA, once within condition XX and once within conditions VX or AX.</p>
</sec>
<sec id="sec007">
<title>Visual search display</title>
<p>The visual search display consisted of 80 black letters “T” (Calibri font, 0.4 x 0.2° of visual angle), 40 of which were presented upright and the other 40 upside-down, on a grey background (160, 160, 160). Letters were randomly distributed in a square area of 8.7° of visual angle. For each trial, one letter “T” was replaced by the target (L or rotated L).</p>
</sec>
</sec>
<sec id="sec008">
<title>Procedure</title>
<p>The experiment was programmed and run using <italic>Presentation</italic> software (Neurobehavioral Systems, San Francisco, CA, USA).</p>
<p>Participants completed a training session on the first day. The training session comprised 12 trials to learn the association between colour (red and blue) and display location (upper and lower half, respectively), 12 trials to learn the association between pitch (high and low) and display location (right and left half, respectively), as well as 24 trials to learn the association between the combination of colour and pitch and the corresponding display quarter. Afterwards, participants performed 48 training trials of the actual paradigm, i.e. using colour and pitch information to prepare for the upcoming target location (condition VA).</p>
<p>On the second day, the experiment was carried out in the fMRI scanner. Before the experiment, an echo planar imaging sequence was turned on and participants manually adjusted the sound volume. The scanner session was followed by a questionnaire evaluating the participants’ behavior during the session (e.g., strategies, concentration, and difficulties performing the task).</p>
</sec>
<sec id="sec009">
<title>Data acquisition</title>
<p>Whole-brain images were collected on a 3T Siemens Magnetom Prisma MR tomograph (Siemens, Erlangen, Germany) using a 20-channel head coil. Functional images were acquired using a gradient T2*-weighted single-shot gradient-echo planar sequence sensitive to BOLD contrast (64 × 64 data acquisition matrix, 192 mm field of view, 90° flip angle, repetition time = 2000 ms, echo time = 30 ms). Each volume consisted of 33 adjacent axial slices with a slice thickness of 3 mm and a gap of 1 mm, resulting in a voxel size of 3 × 3 × 4 mm<sup>3</sup>. Images were acquired in interleaved order parallel to the AC-PC line to provide a whole-brain coverage. Structural data were acquired for each participant using a standard Siemens 3-D T1-weighted MPRAGE sequence for detailed reconstruction of anatomy with isotropic voxels (1 × 1 × 1 mm<sup>3</sup>) in a 256-mm field of view (256 × 256 matrix, 192 slices, repetition time = 2130 ms, echo time = 2.28 ms). Participants’ hands were placed on four-button response boxes. Index and middle fingers were placed on the response buttons. To minimise head motion, the head was tightly fixated with cushions, and earplugs were provided to attenuate scanner noise. Auditory stimuli were presented via headphones (MR confon, Magdeburg, Germany). Visual stimuli were projected on a screen positioned behind the participant’s head by a video projector (JVC, Bad Vilbel, Germany). Participants viewed the screen by a 45° mirror, which was fixated on the top of the head coil and adjusted for each participant to provide a good view of the entire screen.</p>
</sec>
<sec id="sec010">
<title>Behavioral data analysis</title>
<p>Behavioral performance was assessed via error rates and reaction times of correctly answered trials. To test for differences between conditions, pairwise <italic>t</italic>-tests were conducted using the statistic software package <italic>R</italic> (R Foundation for Statistical Computing, Vienna, Austria). Results of the paired <italic>t</italic>-tests were corrected for multiple comparisons at <italic>p</italic> &lt; .05 using Bonferroni correction.</p>
</sec>
<sec id="sec011">
<title>fMRI data analysis</title>
<sec id="sec012">
<title>fMRI data preprocessing</title>
<p>Brain image preprocessing and basic statistical analyses were conducted using <italic>SPM12</italic> (Wellcome Trust Centre for Neuroimaging, London, United Kingdom). Functional images were realigned based on three rotation and three translation parameters, slice time corrected, and co-registered to the structural scan. Structural scans were segmented into grey matter, white matter and cerebrospinal fluid. Structural and functional scans were normalised to the Montreal Neuroimaging Institute (MNI) template. Functional images were high-pass filtered (128 s period cutoff) and spatially smoothed with an 8 mm FWHM Gaussian kernel. For connectivity analyses, we additionally performed linear detrending and despiking, and regressed out the six realignment parameters, their temporal derivatives, and the first five PCA components for both white matter and ventricle voxels from the unsmoothed preprocessed functional images using the <italic>CONN</italic> toolbox [<xref ref-type="bibr" rid="pone.0207119.ref021">21</xref>]. For confound removal, the toolbox uses the aCompCor strategy [<xref ref-type="bibr" rid="pone.0207119.ref022">22</xref>].</p>
</sec>
<sec id="sec013">
<title>Design specifications</title>
<p>Event-related BOLD responses were estimated using a general linear model approach. The model comprised a total of 15 regressors plus intercept. Regressors of interest represented the four main conditions (VA, VX, AX, and XX), each modeled with an event duration of 12 seconds and convolved with the canonical hemodynamic response function. As regressors of nuisance, we included right and left button presses, cue presentation, visual search and area restriction of the search (as a parametric effect with levels -1, 0, and 1 corresponding to full display, half of the display, and quarter of the display, respectively), as well as the six realignment parameters. The first four regressors of nuisance were convolved with the canonical hemodynamic response function, with the first three being defined as actual events and with visual search being modeled with an event duration corresponding to the reaction time of that trial. Four contrasts were generated for each participant: VX&gt;AX, VA&gt;XX, VX&gt;XX, and AX&gt;XX. For each voxel, resulting contrast weights entered one-sample <italic>t</italic>-tests. General task activations (i.e., attention to visual stimuli, auditory stimuli, or both) were assessed by the conjunction VA&gt;XX ∩ VX&gt;XX ∩ AX&gt;XX. To correct for multiple comparisons, false discovery rate (FDR) correction was used (<italic>p</italic> &lt; .05).</p>
</sec>
<sec id="sec014">
<title>Network construction</title>
<p>Nodes represented contrast peak voxels, and edges were defined as correlations between confound-corrected BOLD time series (<italic>BOLD series</italic> hereafter). Sixteen peak voxels were chosen from contrast VA&gt;XX ∩ VX&gt;XX ∩ AX&gt;XX, and six peak voxels each from contrasts VX&gt;AX and AX&gt;VX, resulting in a total of 28 nodes. Every node was created by surrounding the corresponding peak voxel with a 6-mm-radius sphere using <italic>AFNI</italic>’s <italic>3dUndump</italic> and used to average, for every participant, BOLD series of voxels within a sphere (n<sub>voxels</sub> = 33; see [<xref ref-type="bibr" rid="pone.0207119.ref023">23</xref>]). This approach aims to get reliable time series averages while simultaneously retaining a certain degree of functional node specificity.</p>
<p>Averaged BOLD series were used to calculate Pearson correlations between all pairs of nodes. Correlation coefficients were Fisher’s Z transformed. To generate condition-specific correlation matrices, BOLD series were restricted to relevant volumes from the audio-visual sequences (six TRs per trial) of correctly answered trials, shifted by six seconds to account for the hemodynamic lag. In case of conditions VA and XX, 24 correctly answered trials were randomly chosen to balance the number of trials between conditions (minimum number of trials per subject and condition was 22). Accordingly, every correlation coefficient was based on at least 132 data points.</p>
<p>Matrix construction was done by using customised Python code (Python Software Foundation), utilizing the packages <italic>NumPy</italic> and <italic>NiBabel</italic>, and resulted in one 28 x 28 matrix per participant and condition (112 matrices in total). Connection strength was represented by positive and negative correlation coefficients, whereas negative correlations were set to zero for graph measure calculation (average percentage of negative correlations = 26.6%, range = 6.9–49.7%). MATLAB and the <italic>Brain Connectivity Toolbox</italic> [<xref ref-type="bibr" rid="pone.0207119.ref024">24</xref>] were used to compute connection strength and weighted graph measures. Nodes were grouped into three modules based on contrasts (task module: VA&gt;XX ∩ VX&gt;XX ∩ AX&gt;XX, visual module: VX&gt;AX, and auditory module: AX&gt;VX). The joint task network represented the core of all brain areas involved in task performance.</p>
</sec>
<sec id="sec015">
<title>Network analysis</title>
<p>To measure integration and core-periphery interaction, we analyzed connection strengths within and between modules and nodal graph measures by fitting generalised linear multilevel models (GLMMs, [<xref ref-type="bibr" rid="pone.0207119.ref025">25</xref>]). These models include fixed and random effects and thereby account for intra-individual homogeneity and inter-individual heterogeneity [<xref ref-type="bibr" rid="pone.0207119.ref026">26</xref>]. The R package <italic>brms</italic> [<xref ref-type="bibr" rid="pone.0207119.ref027">27</xref>–<xref ref-type="bibr" rid="pone.0207119.ref028">28</xref>], an interface to the programming language <italic>Stan</italic> ([<xref ref-type="bibr" rid="pone.0207119.ref029">29</xref>]; <ext-link ext-link-type="uri" xlink:href="http://mc-stan.org/" xlink:type="simple">http://mc-stan.org/</ext-link>), was used to estimate the GLMMs in a Bayesian framework [<xref ref-type="bibr" rid="pone.0207119.ref030">30</xref>–<xref ref-type="bibr" rid="pone.0207119.ref031">31</xref>]. We used the default priors of brms, which are chosen to be only weakly informative, thus having only negligible impact on the obtained estimates [<xref ref-type="bibr" rid="pone.0207119.ref027">27</xref>]. The posterior distribution over model parameters was estimated by means of a Markov Chain Monte Carlo procedure and the NUTS sampling algorithm [<xref ref-type="bibr" rid="pone.0207119.ref032">32</xref>] was used to draw samples (two independent chains with 2000 iterations each, of which the first 1000 were used as warm-up, leaving a total of 2000 posterior samples). The mean of the posterior distribution and a credible interval (usually a two-sided 95% credible interval; 95% CI) were used to summarise each model parameter. A 95% CI can be interpreted in the way that a given parameter lies within this interval with 95% probability. If desired, a parameter can be interpreted as significantly different from zero (on a 5% level) if the corresponding 95% CI does not contain zero. In all models, effect coding was used, which means that group means are contrasted with the grand mean by setting a reference category of every independent variable to -1. Because it cannot be assumed that graph measures are normally distributed [<xref ref-type="bibr" rid="pone.0207119.ref033">33</xref>], we chose response distributions that best fit the observed data. Convergence of all Bayesian GLMMs was assessed via the effective sample size for each parameter and the Gelman-Rubin statistic Rhat [<xref ref-type="bibr" rid="pone.0207119.ref034">34</xref>].</p>
<p>To quantify effects of connection type (i.e. within-module and between-modules connections), and attentional modulation on connection strength, we analyzed the data by fitting a GLMM with fixed effects of <italic>visual read-out</italic> (VR; yes, no), <italic>auditory read-out</italic> (AR; yes, no) and <italic>connection type</italic> (CT; visual-visual, auditory-auditory, core-core, visual-auditory, visual-core, auditory-core) and random effects for participant and node. The model specification in R formula (brms) was: Connection Strength ~ VR * AR * CT + (VR * AR | participant) + (VR * AR | node). We chose a skewed normal distribution to describe the distribution of the dependent variable, because the distribution of correlations was right-skewed. For effect coding, the following categories were chosen as reference (i.e., coded with -1): connection type = core-core, visual read-out = no, and auditory read-out = no.</p>
<p>In addition to connection strength, we calculated the following seven weighted graph measures for every node to quantify topological features of the network: betweenness centrality, characteristic path length, clustering coefficient, core closeness, nodal efficiency, participation coefficient, and strength (see <xref ref-type="supplementary-material" rid="pone.0207119.s001">S1 Table</xref> for detailed information).</p>
<p>For each graph measure, we fitted a GLMM with fixed effects of <italic>visual read-out</italic> (VR; yes, no), <italic>auditory read-out</italic> (AR; yes, no) and <italic>module</italic> (MOD; visual, auditory, core) and random effects for participant and node. The model specification in R formula (brms) was: Graph Measure ~ VR * AR * MOD + (VR * AR | participant) + (VR * AR | node). For effect coding, the following categories were chosen as reference (i.e., coded with -1): module = core, visual read-out = no, and auditory read-out = no.</p>
</sec>
</sec>
</sec>
<sec id="sec016" sec-type="results">
<title>Results</title>
<sec id="sec017">
<title>Behavioral results</title>
<p>Performance during training and fMRI session was assessed through error rates and reaction times on correctly answered trails. The average error rate of the fourth part of the training, which corresponded to condition VA, was 3.27% (± 3.42 SD) and the average reaction time was 1659.80 ms (± 527.91 SD), both demonstrating participants’ ability to perform the task correctly. The average error rate of the fMRI session was 1.61% (± 1.52 SD; range 0–4.86%) and did not differ significantly between conditions (<italic>p</italic> &gt; .05 for all pairwise <italic>t</italic>-tests). The average reaction time was 2507.48 ms (± 613.93 SD) and differed significantly between conditions (<italic>p</italic> ≤ .001 for all pairwise <italic>t</italic>-tests), indicating that participants used the provided visual and auditory information to predict the target location. See <xref ref-type="fig" rid="pone.0207119.g002">Fig 2</xref> and <xref ref-type="table" rid="pone.0207119.t002">Table 2</xref> for average reaction times and error rates per condition.</p>
<fig id="pone.0207119.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0207119.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Mean reaction times and error rates for each condition of the fMRI experiment.</title>
<p>Error bars represent confidence limits (95%). VA = visual and auditory read-out, VX = only visual read-out, AX = only auditory read-out, XX = control condition (no read-out). Reaction times show that visual search was not informed (restricted) in the XX condition, more restricted in the VX and the AX conditions, and most restricted in the VA condition.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0207119.g002" xlink:type="simple"/>
</fig>
<table-wrap id="pone.0207119.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0207119.t002</object-id>
<label>Table 2</label> <caption><title>Mean and standard deviation of reaction times and error rates during the fMRI session.</title> <p>Condition abbreviation see text and legend of <xref ref-type="fig" rid="pone.0207119.g002">Fig 2</xref>.</p></caption>
<alternatives>
<graphic id="pone.0207119.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0207119.t002" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center">VA</th>
<th align="center">VX</th>
<th align="center">AX</th>
<th align="center">XX</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Reaction time (ms)</td>
<td align="center">1551 (549)</td>
<td align="center">2211 (735)</td>
<td align="center">2657 (917)</td>
<td align="center">3611 (749)</td>
</tr>
<tr>
<td align="left">Error rate (%)</td>
<td align="center">1.64 (2.49)</td>
<td align="center">1.19 (2.23)</td>
<td align="center">1.19 (2.50)</td>
<td align="center">2.01 (2.57)</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec018">
<title>fMRI results</title>
<sec id="sec019">
<title>Contrasts</title>
<p>In order to derive nodes for our network analysis, we focused on the following three contrasts (see <xref ref-type="table" rid="pone.0207119.t003">Table 3</xref>): Group-level activations related to visual read-out (VX &gt; AX) were found bilaterally in the occipital cortex and fusiform gyrus (BA 18/19/37) and in the superior parietal lobule (BA 7). We used the three most strongly activated voxels of the biggest cluster within the left and right hemisphere as nodes. The reverse contrast (AX &gt; VX), representing auditory read-out, revealed significant bilateral activations in the superior temporal gyrus (BA 22). As nodes we used the three peak voxels of the left cluster, the peak voxel of the right cluster and two additional right hemisphere voxels, which corresponded to the significant left hemisphere voxels. This procedure ensured equal numbers of nodes within the visual and auditory modules. The conjunction of visual and/or auditory read-out relative to the passive condition (VA&gt;XX ∩ VX&gt;XX ∩ AX&gt;XX) yielded significant bilateral activations in the superior and inferior parietal lobule along the intraparietal sulcus (BA 7/39/40), in the dorsal premotor cortex and inferior frontal junction (BA 6/44), and in lobule VI of the cerebellum. Sixteen voxels were chosen from this contrast to represent nodes of the task module.</p>
<table-wrap id="pone.0207119.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0207119.t003</object-id>
<label>Table 3</label> <caption><title>Peak voxel and local maxima.</title></caption>
<alternatives>
<graphic id="pone.0207119.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0207119.t003" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center" colspan="2" rowspan="3">Area</th>
<th align="center" colspan="4">Peak voxels and local maxima</th>
<th align="center"/>
<th align="center" colspan="2" rowspan="3">Cluster size</th>
<th align="center" colspan="2">p-value</th>
<th align="center" rowspan="3">Node</th>
</tr>
<tr>
<th align="center" colspan="3">MNI coordinates</th>
<th align="center" rowspan="2">z-value</th>
<th align="center">p-value</th>
<th align="center" colspan="2"/>
</tr>
<tr>
<th align="center">x</th>
<th align="center">y</th>
<th align="center">z</th>
<th align="center"/>
<th align="center" colspan="2"/>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="13"><bold>Visual (VX &gt; AX)</bold></td>
</tr>
<tr>
<td align="left" rowspan="3">Occipital cortex and fusiform gyrus (BA 18/19/37)</td>
<td align="center" rowspan="3">L</td>
<td align="center">-27</td>
<td align="center">-88</td>
<td align="center">2</td>
<td align="center">5.79</td>
<td align="center">&lt; .001</td>
<td align="center">1287</td>
<td align="center" colspan="2">&lt; .001 (&lt; .001)</td>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center">-42</td>
<td align="center">-70</td>
<td align="center">-10</td>
<td align="center">5.44</td>
<td align="center">&lt; .001</td>
<td align="center"/>
<td align="center" colspan="2"/>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center">-33</td>
<td align="center">-76</td>
<td align="center">-13</td>
<td align="center">5.37</td>
<td align="center">&lt; .001</td>
<td align="center"/>
<td align="center" colspan="2"/>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="left" rowspan="4">Occipital cortex and fusiform gyrus (BA 18/19/37), superior parietal lobule (BA 7)</td>
<td align="center" rowspan="4">R</td>
<td align="center">33</td>
<td align="center">-82</td>
<td align="center">5</td>
<td align="center">5.78</td>
<td align="center">&lt; .001</td>
<td align="center">1892</td>
<td align="center" colspan="2">&lt; .001 (&lt; .001)</td>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center">42</td>
<td align="center">-73</td>
<td align="center">-7</td>
<td align="center">5.49</td>
<td align="center">&lt; .001</td>
<td align="center"/>
<td align="center" colspan="2"/>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center">48</td>
<td align="center">-61</td>
<td align="center">-10</td>
<td align="center">5.45</td>
<td align="center">&lt; .001</td>
<td align="center"/>
<td align="center" colspan="2"/>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center">24</td>
<td align="center">-55</td>
<td align="center">56</td>
<td align="center">4.74</td>
<td align="center"/>
<td align="center"/>
<td align="center" colspan="2"/>
<td align="center" colspan="2">no</td>
</tr>
<tr>
<td align="left" rowspan="2">Superior parietal lobule (BA 7)</td>
<td align="center" rowspan="2">L</td>
<td align="center">-18</td>
<td align="center">-58</td>
<td align="center">68</td>
<td align="center">4.12</td>
<td align="center">.001</td>
<td align="center">221</td>
<td align="center" colspan="2">.004 (.044)</td>
<td align="center" colspan="2">no</td>
</tr>
<tr>
<td align="center">-27</td>
<td align="center">-55</td>
<td align="center">62</td>
<td align="center">3.97</td>
<td align="center">.002</td>
<td align="center"/>
<td align="center" colspan="2"/>
<td align="center" colspan="2">no</td>
</tr>
<tr>
<td align="left">Thalamus</td>
<td align="center">L</td>
<td align="center">-18</td>
<td align="center">-31</td>
<td align="center">11</td>
<td align="center">3.75</td>
<td align="center">.003</td>
<td align="center">114</td>
<td align="center" colspan="2">.028 (.279)</td>
<td align="center" colspan="2">no</td>
</tr>
<tr>
<td align="left">Prefrontal cortex (BA 10/32)</td>
<td align="center">L</td>
<td align="center">-12</td>
<td align="center">38</td>
<td align="center">-4</td>
<td align="center">3.61</td>
<td align="center">.005</td>
<td align="center">258</td>
<td align="center" colspan="2">.002 (.024)</td>
<td align="center" colspan="2">no</td>
</tr>
<tr>
<td align="left">Putamen</td>
<td align="center">R</td>
<td align="center">36</td>
<td align="center">-7</td>
<td align="center">2</td>
<td align="center">3.59</td>
<td align="center">.005</td>
<td align="center">153</td>
<td align="center" colspan="2">.013 (.140)</td>
<td align="center" colspan="2">no</td>
</tr>
<tr>
<td align="left" colspan="13"><bold>Auditory (AX &gt; VX)</bold></td>
</tr>
<tr>
<td align="left" rowspan="6">Superior temporal gyrus (STG; BA 22/41/42)</td>
<td align="center" rowspan="3">L</td>
<td align="center">-54</td>
<td align="center">-37</td>
<td align="center">17</td>
<td align="center">4.64</td>
<td align="center">.022</td>
<td align="center">60</td>
<td align="center" colspan="2">.008 (.011)</td>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center">-65</td>
<td align="center">-34</td>
<td align="center">11</td>
<td align="center">3.48</td>
<td align="center"/>
<td align="center">n.s.</td>
<td align="center" colspan="2"/>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center">-66</td>
<td align="center">-25</td>
<td align="center">5</td>
<td align="center">2.47</td>
<td align="center"/>
<td align="center">n.s.</td>
<td align="center" colspan="2"/>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center" rowspan="3">R</td>
<td align="center">66</td>
<td align="center">-34</td>
<td align="center">11</td>
<td align="center">4.62</td>
<td align="center">.022</td>
<td align="center">125</td>
<td align="center" colspan="2">&lt; .001 (.001)</td>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center">54</td>
<td align="center">-34</td>
<td align="center">11</td>
<td align="center">4.28</td>
<td align="center">.022</td>
<td align="center"/>
<td align="center" colspan="2"/>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center">66</td>
<td align="center">-25</td>
<td align="center">5</td>
<td align="center">4.27</td>
<td align="center">.022</td>
<td align="center"/>
<td align="center" colspan="2"/>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="left" colspan="13"><bold>Task (VA&gt;XX ∩ VX&gt;XX ∩ AX&gt;XX)</bold></td>
</tr>
<tr>
<td align="left" rowspan="3">Inferior and superior parietal lobule (BA 7/39/40)</td>
<td align="center" rowspan="3">L</td>
<td align="center">-33</td>
<td align="center">-43</td>
<td align="center">41</td>
<td align="center">5.98</td>
<td align="center">&lt; .001</td>
<td align="center">993</td>
<td align="center" colspan="2">&lt; .001 (&lt; .001)</td>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center">-27</td>
<td align="center">-55</td>
<td align="center">44</td>
<td align="center">5.98</td>
<td align="center">&lt; .001</td>
<td align="center"/>
<td align="center" colspan="2"/>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center">-18</td>
<td align="center">-70</td>
<td align="center">53</td>
<td align="center">5.09</td>
<td align="center">&lt; .001</td>
<td align="center"/>
<td align="center" colspan="2"/>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="left" rowspan="2">Inferior and superior parietal lobule (BA 7/39/40)</td>
<td align="center" rowspan="2">R</td>
<td align="center">12</td>
<td align="center">-64</td>
<td align="center">56</td>
<td align="center">4.59</td>
<td align="center">&lt; .001</td>
<td align="center">323</td>
<td align="center" colspan="2">&lt; .001 (.002)</td>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center">30</td>
<td align="center">-55</td>
<td align="center">41</td>
<td align="center">4.50</td>
<td align="center">&lt; .001</td>
<td align="center"/>
<td align="center" colspan="2"/>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="left" rowspan="7">Dorsal premotor cortex and inferior frontal junction (BA 6/44)</td>
<td align="center" rowspan="4">L</td>
<td align="center">-45</td>
<td align="center">2</td>
<td align="center">26</td>
<td align="center">5.26</td>
<td align="center">&lt; .001</td>
<td align="center">1011</td>
<td align="center" colspan="2">&lt; .001 (&lt; .001)</td>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center">-57</td>
<td align="center">5</td>
<td align="center">20</td>
<td align="center">5.16</td>
<td align="center">&lt; .001</td>
<td align="center"/>
<td align="center" colspan="2"/>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center">-30</td>
<td align="center">-1</td>
<td align="center">59</td>
<td align="center">4.87</td>
<td align="center">&lt; .001</td>
<td align="center"/>
<td align="center" colspan="2"/>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center">-5</td>
<td align="center">6</td>
<td align="center">57</td>
<td align="center">3.76</td>
<td align="center"/>
<td align="center"/>
<td align="center" colspan="2"/>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center" rowspan="3">R</td>
<td align="center">42</td>
<td align="center">5</td>
<td align="center">26</td>
<td align="center">4.94</td>
<td align="center">&lt; .001</td>
<td align="center">431</td>
<td align="center" colspan="2">&lt; .001 (&lt; .001)</td>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center">24</td>
<td align="center">2</td>
<td align="center">59</td>
<td align="center">4.22</td>
<td align="center">.001</td>
<td align="center"/>
<td align="center" colspan="2"/>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center">36</td>
<td align="center">-4</td>
<td align="center">41</td>
<td align="center">4.07</td>
<td align="center">.002</td>
<td align="center"/>
<td align="center" colspan="2"/>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="left" rowspan="4">Cerebellum (lobule VI)</td>
<td align="center">L</td>
<td align="center">-24</td>
<td align="center">-64</td>
<td align="center">-25</td>
<td align="center">4.04</td>
<td align="center">.002</td>
<td align="center">113</td>
<td align="center" colspan="2">.015 (.148)</td>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center"/>
<td align="center">-27</td>
<td align="center">-55</td>
<td align="center">-28</td>
<td align="center">3.98</td>
<td align="center">.002</td>
<td align="center"/>
<td align="center" colspan="2"/>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center"/>
<td align="center">-12</td>
<td align="center">-70</td>
<td align="center">-25</td>
<td align="center">3.46</td>
<td align="center">.009</td>
<td align="center"/>
<td align="center" colspan="2"/>
<td align="center" colspan="2">yes</td>
</tr>
<tr>
<td align="center">R</td>
<td align="center">30</td>
<td align="center">-64</td>
<td align="center">-25</td>
<td align="center">4.10</td>
<td align="center"/>
<td align="center"/>
<td align="center" colspan="2"/>
<td align="center" colspan="2">yes</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t003fn001"><p><italic>Note</italic>. Shown are local peaks of significant clusters (n.s. = not significant).</p></fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="sec020">
<title>Network analysis</title>
<p>Mean, standard deviation and range of all conditions are provided for graph measures and connection strength in <xref ref-type="supplementary-material" rid="pone.0207119.s002">S2 Table</xref>. All GLMMs converged successfully with an effective sample size of 300 or more for every relevant parameter and Rhat values of less than 1.1.</p>
<p>Connection strength of all connection types differed from the grand mean, with within-module connections being stronger and between-module connections being weaker (see <xref ref-type="fig" rid="pone.0207119.g003">Fig 3</xref> and <xref ref-type="table" rid="pone.0207119.t004">Table 4</xref>). When visual information was read out, connections between the visual and task module were stronger (<italic>b</italic> = 0.007, 95%-CI = [0.003, 0.011]) and connections within the auditory module were weaker (<italic>b</italic> = -0.009, [-0.017, -0.001]; see <xref ref-type="fig" rid="pone.0207119.g004">Fig 4</xref>). When auditory information was read out, connections between the auditory and task module (<italic>b</italic> = 0.008, [0.004, 0.011]) and connections within the auditory module were stronger (<italic>b</italic> = 0.015, [0.008, 0.023]), and connections within the visual module (<italic>b</italic> = -0.015, [-0.023, -0.007]) and connections between the visual and auditory module were weaker (<italic>b</italic> = -0.01, [-0.015, -0.005]).</p>
<fig id="pone.0207119.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0207119.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Connection strength for each connection type.</title>
<p>Error bars represent 95% credible intervals. CC = connections within the task core, CA = connections between task core and auditory module, CV = connections between task core and visual module, AA = connections within auditory module, AV connections between auditory and visual module, VV = connections within visual module.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0207119.g003" xlink:type="simple"/>
</fig>
<fig id="pone.0207119.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0207119.g004</object-id>
<label>Fig 4</label>
<caption>
<title/>
<p>(A)–(C) Significant activations of the three contrasts of interest (FDR corrected, p &lt; .05). (D) Nodes of the network analysis based on contrasts. The figure was generated with BrainNet Viewer [<xref ref-type="bibr" rid="pone.0207119.ref035">35</xref>]. (E) Connectivity graph of condition VX; only positive connections are displayed. A similar module structure was observed for conditions VA, AX, and XX. The figure was generated with Gephi [<xref ref-type="bibr" rid="pone.0207119.ref036">36</xref>] using the ForceAtlas2 algorithm [<xref ref-type="bibr" rid="pone.0207119.ref037">37</xref>]. Arrangement of nodes and edge width are based on connectivity strength.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0207119.g004" xlink:type="simple"/>
</fig>
<table-wrap id="pone.0207119.t004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0207119.t004</object-id>
<label>Table 4</label> <caption><title>GLMM for connection strength: Fixed effects.</title></caption>
<alternatives>
<graphic id="pone.0207119.t004g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0207119.t004" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Population-Level Effect</th>
<th align="center">Estimate</th>
<th align="center">l-95% CI</th>
<th align="center">u-95% CI</th>
<th align="center"/>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Intercept</td>
<td align="center">0.176</td>
<td align="center">0.141</td>
<td align="center">0.210</td>
<td align="center">*</td>
</tr>
<tr>
<td align="left">VR_yes</td>
<td align="center">0.003</td>
<td align="center">-0.003</td>
<td align="center">0.009</td>
<td align="center"/>
</tr>
<tr>
<td align="left">AR_yes</td>
<td align="center">0.001</td>
<td align="center">-0.005</td>
<td align="center">0.007</td>
<td align="center"/>
</tr>
<tr>
<td align="left">CT_TA</td>
<td align="center">-0.122</td>
<td align="center">-0.143</td>
<td align="center">-0.103</td>
<td align="center">*</td>
</tr>
<tr>
<td align="left">CT_TV</td>
<td align="center">-0.087</td>
<td align="center">-0.108</td>
<td align="center">-0.067</td>
<td align="center">*</td>
</tr>
<tr>
<td align="left">CT_AA</td>
<td align="center">0.082</td>
<td align="center">0.042</td>
<td align="center">0.124</td>
<td align="center">*</td>
</tr>
<tr>
<td align="left">CT_AV</td>
<td align="center">-0.131</td>
<td align="center">-0.148</td>
<td align="center">-0.113</td>
<td align="center">*</td>
</tr>
<tr>
<td align="left">CT_VV</td>
<td align="center">0.212</td>
<td align="center">0.172</td>
<td align="center">0.254</td>
<td align="center">*</td>
</tr>
<tr>
<td align="left">VR_yes * AR_yes</td>
<td align="center">0.000</td>
<td align="center">-0.004</td>
<td align="center">0.005</td>
<td align="center"/>
</tr>
<tr>
<td align="left">VR_yes * CT_TA</td>
<td align="center">0.000</td>
<td align="center">-0.004</td>
<td align="center">0.004</td>
<td align="center"/>
</tr>
<tr>
<td align="left">VR_yes * CT_TV</td>
<td align="center">0.007</td>
<td align="center">0.003</td>
<td align="center">0.011</td>
<td align="center">*</td>
</tr>
<tr>
<td align="left">VR_yes * CT_AA</td>
<td align="center">-0.009</td>
<td align="center">-0.017</td>
<td align="center">-0.001</td>
<td align="center">*</td>
</tr>
<tr>
<td align="left">VR_yes * CT_AV</td>
<td align="center">0.003</td>
<td align="center">-0.002</td>
<td align="center">0.008</td>
<td align="center"/>
</tr>
<tr>
<td align="left">VR_yes * CT_VV</td>
<td align="center">0.000</td>
<td align="center">-0.007</td>
<td align="center">0.008</td>
<td align="center"/>
</tr>
<tr>
<td align="left">AR_yes * CT_TA</td>
<td align="center">0.008</td>
<td align="center">0.004</td>
<td align="center">0.011</td>
<td align="center">*</td>
</tr>
<tr>
<td align="left">AR_yes * CT_TV</td>
<td align="center">-0.004</td>
<td align="center">-0.007</td>
<td align="center">0.000</td>
<td align="center"/>
</tr>
<tr>
<td align="left">AR_yes * CT_AA</td>
<td align="center">0.015</td>
<td align="center">0.008</td>
<td align="center">0.023</td>
<td align="center">*</td>
</tr>
<tr>
<td align="left">AR_yes * CT_AV</td>
<td align="center">-0.010</td>
<td align="center">-0.015</td>
<td align="center">-0.005</td>
<td align="center">*</td>
</tr>
<tr>
<td align="left">AR_yes * CT_VV</td>
<td align="center">-0.015</td>
<td align="center">-0.023</td>
<td align="center">-0.007</td>
<td align="center">*</td>
</tr>
<tr>
<td align="left">VR_yes * AR_yes * CT_TA</td>
<td align="center">0.001</td>
<td align="center">-0.003</td>
<td align="center">0.004</td>
<td align="center"/>
</tr>
<tr>
<td align="left">VR_yes * AR_yes * CT_TV</td>
<td align="center">0.002</td>
<td align="center">-0.001</td>
<td align="center">0.006</td>
<td align="center"/>
</tr>
<tr>
<td align="left">VR_yes * AR_yes * CT_AA</td>
<td align="center">-0.004</td>
<td align="center">-0.011</td>
<td align="center">0.003</td>
<td align="center"/>
</tr>
<tr>
<td align="left">VR_yes * AR_yes * CT_AV</td>
<td align="center">0.000</td>
<td align="center">-0.004</td>
<td align="center">0.005</td>
<td align="center"/>
</tr>
<tr>
<td align="left">VR_yes * AR_yes * CT_VV</td>
<td align="center">0.001</td>
<td align="center">-0.006</td>
<td align="center">0.008</td>
<td align="center"/>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t004fn001"><p><italic>Note</italic>. Credible intervals not including zero are marked with *. VR = visual read-out, AR = auditory read-out, CT = connection type with A = auditory, T = task, V = visual.</p></fn>
</table-wrap-foot>
</table-wrap>
<p><xref ref-type="table" rid="pone.0207119.t005">Table 5</xref> displays credible intervals for the population-level effects of the GLMMs fitted for each graph measure.</p>
<table-wrap id="pone.0207119.t005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0207119.t005</object-id>
<label>Table 5</label> <caption><title>Credible intervals of fixed effects of graph measure GLMMs.</title></caption>
<alternatives>
<graphic id="pone.0207119.t005g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0207119.t005" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Population-Level Effect</th>
<th align="center">Betweenness centrality</th>
<th align="center">Characteristic path length</th>
<th align="center">Clustering coefficient</th>
<th align="center">Core closeness</th>
<th align="center">Nodal efficiency</th>
<th align="center">Participation coefficient</th>
<th align="center">Strength</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Intercept</td>
<td align="center"><bold>[2.88; 3.32]</bold></td>
<td align="center"><bold>[1.73; 1.88]</bold></td>
<td align="center"><bold>[-1.94; -1.71]</bold></td>
<td align="center"><bold>[-4.63; -4.43]</bold></td>
<td align="center"><bold>[0.157; 0.181]</bold></td>
<td align="center"><bold>[0.439; 0.505]</bold></td>
<td align="center"><bold>[1.16; 1.42]</bold></td>
</tr>
<tr>
<td align="left">VR_yes</td>
<td align="center">[-0.10; 0.03]</td>
<td align="center">[-0.04; 0.00]</td>
<td align="center">[-0.01; 0.04]</td>
<td align="center">[-0.00; 0.03]</td>
<td align="center">[0.000; 0.006]</td>
<td align="center">[-0.001; 0.014]</td>
<td align="center">[-0.00; 0.04]</td>
</tr>
<tr>
<td align="left">AR_yes</td>
<td align="center">[-0.08; 0.04]</td>
<td align="center">[-0.03; 0.01]</td>
<td align="center">[-0.02; 0.04]</td>
<td align="center">[-0.00; 0.04]</td>
<td align="center">[-0.002; 0.005]</td>
<td align="center">[-0.009; 0.001]</td>
<td align="center">[-0.01; 0.04]</td>
</tr>
<tr>
<td align="left">MOD_aud</td>
<td align="center">[-0.35; 0.30]</td>
<td align="center"><bold>[0.06; 0.17]</bold></td>
<td align="center"><bold>[-0.30; -0.12]</bold></td>
<td align="center"><bold>[-0.33; -0.15]</bold></td>
<td align="center"><bold>[-0.028; -0.010]</bold></td>
<td align="center"><bold>[0.021; 0.105]</bold></td>
<td align="center"><bold>[-0.40; -0.12]</bold></td>
</tr>
<tr>
<td align="left">MOD_vis</td>
<td align="center">[-0.31; 0.32]</td>
<td align="center">[-0.09; 0.02]</td>
<td align="center">[-0.02; 0.18]</td>
<td align="center">[-0.15; 0.03]</td>
<td align="center">[-0.006; 0.014]</td>
<td align="center"><bold>[0.001; 0.081]</bold></td>
<td align="center">[-0.05; 0.24]</td>
</tr>
<tr>
<td align="left">VR_yes * AR_yes</td>
<td align="center">[-0.09; 0.04]</td>
<td align="center">[-0.02; 0.01]</td>
<td align="center">[-0.03; 0.02]</td>
<td align="center">[-0.02; 0.02]</td>
<td align="center">[-0.002; 0.003]</td>
<td align="center">[-0.005; 0.006]</td>
<td align="center">[-0.02; 0.02]</td>
</tr>
<tr>
<td align="left">VR_yes * MOD_aud</td>
<td align="center">[-0.14; 0.04]</td>
<td align="center">[-0.00; 0.02]</td>
<td align="center">[-0.03; 0.01]</td>
<td align="center">[-0.02; 0.01]</td>
<td align="center">[-0.004; 0.000]</td>
<td align="center">[-0.006; 0.006]</td>
<td align="center">[-0.04; 0.01]</td>
</tr>
<tr>
<td align="left">VR_yes * MOD_vis</td>
<td align="center">[-0.07; 0.11]</td>
<td align="center"><bold>[-0.02; -0.002]</bold></td>
<td align="center">[-0.00; 0.04]</td>
<td align="center"><bold>[0.01; 0.03]</bold></td>
<td align="center">[0.000; 0.003]</td>
<td align="center">[-0.007; 0.006]</td>
<td align="center">[-0.00; 0.04]</td>
</tr>
<tr>
<td align="left">AR_yes * MOD_aud</td>
<td align="center">[-0.05; 0.13]</td>
<td align="center"><bold>[-0.03; -0.01]</bold></td>
<td align="center"><bold>[0.02; 0.06]</bold></td>
<td align="center"><bold>[0.01; 0.04]</bold></td>
<td align="center"><bold>[0.001; 0.004]</bold></td>
<td align="center">[-0.004; 0.007]</td>
<td align="center"><bold>[0.02; 0.07]</bold></td>
</tr>
<tr>
<td align="left">AR_yes * MOD_vis</td>
<td align="center">[-0.15; 0.04]</td>
<td align="center"><bold>[0.02; 0.04]</bold></td>
<td align="center"><bold>[-0.06; -0.03]</bold></td>
<td align="center"><bold>[-0.04; -0.02]</bold></td>
<td align="center"><bold>[-0.005; -0.002]</bold></td>
<td align="center">[-0.008; 0.003]</td>
<td align="center"><bold>[-0.08; -0.03]</bold></td>
</tr>
<tr>
<td align="left">VR_yes * AR_yes * MOD_aud</td>
<td align="center">[-0.13; 0.05]</td>
<td align="center">[-0.00; 0.02]</td>
<td align="center">[-0.02; 0.02]</td>
<td align="center">[-0.02; 0.01]</td>
<td align="center">[-0.002; 0.001]</td>
<td align="center">[-0.008; 0.004]</td>
<td align="center">[-0.03; 0.01]</td>
</tr>
<tr>
<td align="left">VR_yes * AR_yes * MOD_vis</td>
<td align="center">[-0.08; 0.10]</td>
<td align="center">[-0.02; 0.00]</td>
<td align="center">[-0.01; 0.02]</td>
<td align="center">[0.00; 0.03]</td>
<td align="center">[-0.001; 0.002]</td>
<td align="center">[-0.004; 0.008]</td>
<td align="center">[-0.01; 0.04]</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t005fn001"><p><italic>Note</italic>. Credible intervals not including zero are written in bold. VR = visual read-out, AR = auditory read-out, MOD = module with vis = visual, aud = auditory.</p></fn>
</table-wrap-foot>
</table-wrap>
<p>Nodes within the auditory module had longer paths to other nodes (<italic>b</italic> = 0.118, 95%-CI [0.061, 0.174]), showed reduced clustering (<italic>b</italic> = -0.208, [-0.298, -0.116]), a greater distance to the core (i.e., task module; <italic>b</italic> = -0.236, [-0.326, -0.146]), decreased efficiency (<italic>b</italic> = -0.019, [-0.028, -0.010]) and decreased strength (<italic>b</italic> = -0.262, [-0.404, -0.117]) as well as higher participation in different modules (<italic>b</italic> = 0.063, [0.021, 0.105]), when compared to the grand mean.</p>
<p>Nodes within the visual module showed higher participation in different modules (<italic>b</italic> = 0.042, [0.001, 0.081]), when compared to the grand mean.</p>
<p>When auditory information was read out, auditory nodes had shorter paths to other nodes (<italic>b</italic> = -0.019, [-0.028, -0.010], showed enhanced clustering (<italic>b</italic> = 0.037, [0.018, 0.055]), a smaller distance to the core (<italic>b</italic> = 0.024, [0.012, 0.036]), and increased efficiency (<italic>b</italic> = 0.002, [0.001, 0.004] as well as increased strength (<italic>b</italic> = 0.046, [0.023, 0.069]). The opposite pattern was observed for visual nodes: They had longer paths to other nodes (<italic>b</italic> = 0.026, [0.017, 0.035]), showed reduced clustering (b = -0.044, [-0.062, -0.026]), a greater distance to the core (<italic>b</italic> = -0.031, [-0.043, -0.019]), as well as decreased efficiency (<italic>b</italic> = -0.004, [-0.005, -0.002]) and decreased strength (<italic>b</italic> = -0.057, [-0.079, -0.034]).</p>
<p>When visual information was read out, visual nodes had shorter paths to other nodes (b = -0.011, [-0.021, -0.002]) and were closer to the core (b = 0.018, [0.005, 0.030]). A summary of network analysis results is provided in <xref ref-type="table" rid="pone.0207119.t006">Table 6</xref>.</p>
<table-wrap id="pone.0207119.t006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0207119.t006</object-id>
<label>Table 6</label> <caption><title>Summary of results from network analysis.</title></caption>
<alternatives>
<graphic id="pone.0207119.t006g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0207119.t006" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center"/>
<th align="center" colspan="4">Auditory read-out</th>
<th align="center" colspan="4">Visual read-out</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" rowspan="4"><bold>Connection strength</bold></td>
<td align="center" colspan="2">A–T</td>
<td align="center" colspan="2">↑</td>
<td align="center" colspan="2">V–T</td>
<td align="center" colspan="2">↑</td>
</tr>
<tr>
<td align="center" colspan="2">A–A</td>
<td align="center" colspan="2">↑</td>
<td align="center" colspan="2">A–A</td>
<td align="center" colspan="2">↓</td>
</tr>
<tr>
<td align="center" colspan="2">V–V</td>
<td align="center" colspan="2">↓</td>
<td align="center" colspan="2"/>
<td align="center" colspan="2"/>
</tr>
<tr>
<td align="center" colspan="2">V–A</td>
<td align="center" colspan="2">↓</td>
<td align="center" colspan="2"/>
<td align="center" colspan="2"/>
</tr>
<tr>
<td align="center" rowspan="6"><bold>Graph measures</bold></td>
<td align="center" colspan="2">Auditory nodes</td>
<td align="center" colspan="2">Visual nodes</td>
<td align="center" colspan="2">Auditory nodes</td>
<td align="center" colspan="2">Visual nodes</td>
</tr>
<tr>
<td align="center">PL</td>
<td align="center">↓</td>
<td align="center">PL</td>
<td align="center">↑</td>
<td align="center"/>
<td align="center"/>
<td align="center">PL</td>
<td align="center">↓</td>
</tr>
<tr>
<td align="center">CL</td>
<td align="center">↑</td>
<td align="center">CL</td>
<td align="center">↓</td>
<td align="center"/>
<td align="center"/>
<td align="center"/>
<td align="center"/>
</tr>
<tr>
<td align="center">CCL</td>
<td align="center">↑</td>
<td align="center">CCL</td>
<td align="center">↓</td>
<td align="center"/>
<td align="center"/>
<td align="center">CCL</td>
<td align="center">↑</td>
</tr>
<tr>
<td align="center">E</td>
<td align="center">↑</td>
<td align="center">E</td>
<td align="center">↓</td>
<td align="center"/>
<td align="center"/>
<td align="center"/>
<td align="center"/>
</tr>
<tr>
<td align="center">S</td>
<td align="center">↑</td>
<td align="center">S</td>
<td align="center">↓</td>
<td align="center"/>
<td align="center"/>
<td align="center"/>
<td align="center"/>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t006fn001"><p><italic>Note</italic>. V = Visual module, A = Auditory module, T = task module (core); PL = characteristic path length, CL = clustering coefficient, CCL = core closeness, E = nodal efficiency, S = strength.</p></fn>
</table-wrap-foot>
</table-wrap>
</sec>
</sec>
</sec>
<sec id="sec021" sec-type="conclusions">
<title>Discussion</title>
<p>This study investigated how attending to auditory and visual information systematically changes graph theoretical measures of integration and functional connectivity between three network modules: auditory, visual, and joint task core. Supporting our hypotheses, connection strength was increased between the task and the visual module when participants attended to colour, and between the task and the auditory module when they attended to pitch. Moreover, several nodal graph measures showed consistent changes to attentional modulation in form of stronger integration of sensory regions in response to attention. Together, these findings corroborate dynamical adjustments of both modality-specific and modality-independent functional brain networks in response to task demands and their representation in graph theoretical measures.</p>
<p>In a first step, we analyzed functional segregation, i.e., the specialization of a brain region for a specific function, to determine our network nodes. By computing BOLD contrasts, we established three sets of areas specific for visual read-out, for auditory read-out, and for the common task shared by both. Attention to either visual or auditory information provided by the audiovisual stimulus sequence led to enhanced activity in relevant sensory brain regions. The read-out of colour information led to activations in V2, V4, and inferior-temporal cortex which are all involved in colour processing [<xref ref-type="bibr" rid="pone.0207119.ref038">38</xref>–<xref ref-type="bibr" rid="pone.0207119.ref039">39</xref>]. Accompanying activity in the dorsal visual stream presumably resulted from the spatial arrangement of the colour-changing circles [<xref ref-type="bibr" rid="pone.0207119.ref040">40</xref>–<xref ref-type="bibr" rid="pone.0207119.ref041">41</xref>], but was omitted for node definition. In case of auditory read-out, we found bilateral activity in Heschl’s gyrus (BA 41 and 42) reflecting pitch processing [<xref ref-type="bibr" rid="pone.0207119.ref042">42</xref>–<xref ref-type="bibr" rid="pone.0207119.ref043">43</xref>], and in posterior STG (BA 22) reflecting complex auditory processing, as required for chords and melodies [<xref ref-type="bibr" rid="pone.0207119.ref042">42</xref>, <xref ref-type="bibr" rid="pone.0207119.ref044">44</xref>–<xref ref-type="bibr" rid="pone.0207119.ref045">45</xref>]. Right hemisphere activations were slightly more pronounced, as often found for music [<xref ref-type="bibr" rid="pone.0207119.ref042">42</xref>, <xref ref-type="bibr" rid="pone.0207119.ref044">44</xref>–<xref ref-type="bibr" rid="pone.0207119.ref046">46</xref>]. No matter whether participants attended to pitch, colour, or both, the common task was translating pitch and colour information into the spatial domain to prepare for the upcoming visual search. This task elicited bilateral activity in the frontal eye fields/dorsal premotor cortex, superior parietal lobule, and intraparietal sulcus–brain regions known as the dorsal attention network (DAN; [<xref ref-type="bibr" rid="pone.0207119.ref047">47</xref>–<xref ref-type="bibr" rid="pone.0207119.ref050">50</xref>]). The DAN is involved in spatial and non-spatial allocation of attention [<xref ref-type="bibr" rid="pone.0207119.ref051">51</xref>–<xref ref-type="bibr" rid="pone.0207119.ref053">53</xref>] and arbitrary sensorimotor mapping [<xref ref-type="bibr" rid="pone.0207119.ref054">54</xref>–<xref ref-type="bibr" rid="pone.0207119.ref056">56</xref>], both crucial functions for the present task. Notably, DAN activity is not restricted to the visual domain but also found for touch [<xref ref-type="bibr" rid="pone.0207119.ref057">57</xref>–<xref ref-type="bibr" rid="pone.0207119.ref058">58</xref>] and audition [<xref ref-type="bibr" rid="pone.0207119.ref059">59</xref>–<xref ref-type="bibr" rid="pone.0207119.ref061">61</xref>], illustrating the network’s multimodality [<xref ref-type="bibr" rid="pone.0207119.ref062">62</xref>–<xref ref-type="bibr" rid="pone.0207119.ref063">63</xref>]. As for all fronto-parietal networks residing in the association cortex, the DAN is highly and reciprocally interconnected to lower-level primary and secondary sensory cortices [<xref ref-type="bibr" rid="pone.0207119.ref064">64</xref>–<xref ref-type="bibr" rid="pone.0207119.ref066">66</xref>], making it the ideal core among our network modules.</p>
<p>In a second step, we analyzed functional integration, i.e., the coordinative coupling of functionally distinct brain regions. Peak voxels of the calculated BOLD contrasts constituted the centers of our network nodes, and every node was assigned to either the visual, auditory, or task network (core). Note that the definition of our core was not based on graph theoretical measures but theoretical assumptions. We were particularly interested in the interaction between core and sensory modules and the functional integration of individual nodes. Functional connectivity between nodes was measured in terms of (Z-transformed) Pearson correlations between two nodes’ BOLD time series, and weighted graph measures were based on all positive correlations. The graph theoretical analysis revealed that several nodal graph measures were sensitive to attentional modulation. During auditory read-out, auditory nodes were characterised by shorter characteristic paths, increased node strength, increased nodal efficiency as well as increased clustering. These measures can be interpreted as reflecting enhanced network communication and, therefore, stronger integration [<xref ref-type="bibr" rid="pone.0207119.ref024">24</xref>]. Interestingly, the opposite pattern was observed for visual nodes in the same periods, which means that they were less integrated when attention was directed to auditory information. In contrast, when visual information was read out, visual nodes exhibited shorter paths, which again demonstrated enhanced integration during attention. In accordance with our hypotheses, attention to either visual or auditory information led to enhanced integration of corresponding sensory nodes. Visual nodes even showed reduced integration when attention was directed to audition. Although not part of our original hypotheses, the finding of reduced integration of visual nodes during auditory task-relevance replicates the results from an earlier study [<xref ref-type="bibr" rid="pone.0207119.ref020">20</xref>] and demonstrates the network’s ability to flexibly adapt to current task demands. More specifically, both studies reported reduced clustering as well as core closeness for task-irrelevant areas. Conceptually, the clustering coefficient reflects interconnectivity, meaning that a highly clustered network consists of nodes whose neighbours are themselves neighbours. Reduced clustering of visual nodes during auditory read-out–importantly, along with <italic>enhanced</italic> clustering of auditory nodes—thus reflects an exceptionally effective adaptation of network connectivity. Fittingly, reduced core closeness of visual nodes during auditory read-out (again, along with <italic>enhanced</italic> core closeness of auditory nodes) shows that not only within-module connections but also core-periphery interactions were flexibly and efficiently reorganised to fit current task demands. Our results thus further extend previous findings in that enhanced integration measures are not specific to the visual system but also apply to auditory circuits, indicating a modality-independent adaptive process.</p>
<p>To further assess the interactions between sensory modules and task core, we considered effects of visual and auditory attention on within-module and between-modules connection strength by fitting a GLMM. The analysis of between-modules connections revealed a dynamic, task-dependent coupling of our task core and sensory modules. As hypothesised, modality-specific regions were more strongly connected to the task core when the corresponding modality was attended to. Previous studies have shown this separately for the visual [<xref ref-type="bibr" rid="pone.0207119.ref067">67</xref>–<xref ref-type="bibr" rid="pone.0207119.ref071">71</xref>] and the auditory domain [<xref ref-type="bibr" rid="pone.0207119.ref072">72</xref>], whereas our study is the first to demonstrate this effect for both modalities within one experiment and based on combined audiovisual stimuli. Our results suggest that during attention, core-periphery communication is dynamically adjusted to fulfill task demands. While core-periphery communication was equally modulated by attention for both visual and auditory modules, we found reduced connection strength within the module processing the unattended modality as well as increased connection strength within the auditory module in case of auditory read-out. Notably, the latter effect was absent for the visual module during visual read-out. These results are in contrast to studies reporting reduced connection strength within the visual module in a visual task when compared to rest [<xref ref-type="bibr" rid="pone.0207119.ref069">69</xref>] or passive viewing [<xref ref-type="bibr" rid="pone.0207119.ref071">71</xref>]. Thus, further work is needed to clarify differences between visual and auditory within-module modulation.</p>
</sec>
<sec id="sec022" sec-type="conclusions">
<title>Conclusion</title>
<p>Using a graph theoretical analysis approach to task-based fMRI, core-periphery interaction and integration for vision and audition can be demonstrated within one experiment and based on the same stimuli. In response to visual and auditory selective attention, increased functional connectivity between task-relevant sensory regions and the dorsal attention network, and nodal graph measures signify enhanced integration of sensory nodes in response to attention. These findings illustrate the brain’s ability to dynamically adjust network communication to fulfill task-demands. Given that the use of graph theoretical measures in task-based fMRI research is still in its infancy, this study adds to the recently burgeoning evidence that graph measures are valuable for capturing dynamic cognitive processes.</p>
</sec>
<sec id="sec023">
<title>Supporting information</title>
<supplementary-material id="pone.0207119.s001" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pone.0207119.s001" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Description, formula, value range, response distribution, and link function of analyzed graph measures.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0207119.s002" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pone.0207119.s002" xlink:type="simple">
<label>S2 Table</label>
<caption>
<title>Mean, standard deviation, and range of all conditions for the assessed graph measures and connection strength.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Monika Mertens, Theresa Eckes, Katharina Thiel, Klara Hagelweide, and Irina Kaltwasser for their assistance in data collection, Ima Trempler for valuable comments on the manuscript, and Joachim Böttger for technical support.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0207119.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tononi</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Sporns</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Edelman</surname> <given-names>GM</given-names></name>. <article-title>A measure for brain complexity: relating functional segregation and integration in the nervous system</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>1994</year>; <volume>91</volume>: <fpage>5033</fpage>–<lpage>5037</lpage>. <object-id pub-id-type="pmid">8197179</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Friston</surname> <given-names>K</given-names></name>. <article-title>Beyond phrenology: what can neuroimaging tell us about distributed circuitry?</article-title> <source>Annu Rev Neurosci</source>. <year>2002</year>; <volume>25</volume>: <fpage>221</fpage>–<lpage>250</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1146/annurev.neuro.25.112701.142846" xlink:type="simple">10.1146/annurev.neuro.25.112701.142846</ext-link></comment> <object-id pub-id-type="pmid">12052909</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sporns</surname> <given-names>O</given-names></name>. <article-title>Network attributes for segregation and integration in the human brain</article-title>. <source>Curr Opin Neurobiol</source>. <year>2013</year>; <volume>23</volume>: <fpage>162</fpage>–<lpage>171</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.conb.2012.11.015" xlink:type="simple">10.1016/j.conb.2012.11.015</ext-link></comment> <object-id pub-id-type="pmid">23294553</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beckmann</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>DeLuca</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Devlin</surname> <given-names>JT</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>SM</given-names></name>. <article-title>Investigations into resting-state connectivity using independent component analysis</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>2005</year>; <volume>360</volume>: <fpage>1001</fpage>–<lpage>1013</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2005.1634" xlink:type="simple">10.1098/rstb.2005.1634</ext-link></comment> <object-id pub-id-type="pmid">16087444</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Damoiseaux</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Rombouts</surname> <given-names>SARB</given-names></name>, <name name-style="western"><surname>Barkhof</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Scheltens</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Stam</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>SM</given-names></name>, <etal>et al</etal>. <article-title>Consistent resting-state networks across healthy subjects</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2006</year>; <volume>103</volume>: <fpage>13848</fpage>–<lpage>13853</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0601417103" xlink:type="simple">10.1073/pnas.0601417103</ext-link></comment> <object-id pub-id-type="pmid">16945915</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>De Luca</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Beckmann</surname> <given-names>CF</given-names></name>, <name name-style="western"><surname>De Stefano</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Matthews</surname> <given-names>PM</given-names></name>, <name name-style="western"><surname>Smith</surname> <given-names>SM</given-names></name>. <article-title>fMRI resting state networks define distinct modes of long-distance interactions in the human brain</article-title>. <source>Neuroimage</source>. <year>2006</year>; <volume>29</volume>: <fpage>1359</fpage>–<lpage>1367</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2005.08.035" xlink:type="simple">10.1016/j.neuroimage.2005.08.035</ext-link></comment> <object-id pub-id-type="pmid">16260155</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Fox</surname> <given-names>PT</given-names></name>, <name name-style="western"><surname>Miller</surname> <given-names>KL</given-names></name>, <name name-style="western"><surname>Glahn</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Fox</surname> <given-names>PM</given-names></name>, <name name-style="western"><surname>Mackay</surname> <given-names>CE</given-names></name>, <etal>et al</etal>. <article-title>Correspondence of the brain's functional architecture during activation and rest</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2009</year>; <volume>106</volume>: <fpage>13040</fpage>–<lpage>13045</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0905267106" xlink:type="simple">10.1073/pnas.0905267106</ext-link></comment> <object-id pub-id-type="pmid">19620724</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Power</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Cohen</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Nelson</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Wig</surname> <given-names>GS</given-names></name>, <name name-style="western"><surname>Barnes</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Church</surname> <given-names>JA</given-names></name>, <etal>et al</etal>. <article-title>Functional network organization of the human brain</article-title>. <source>Neuron</source>. <year>2011</year>; <volume>72</volume>: <fpage>665</fpage>–<lpage>678</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2011.09.006" xlink:type="simple">10.1016/j.neuron.2011.09.006</ext-link></comment> <object-id pub-id-type="pmid">22099467</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hagmann</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Cammoun</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Gigandet</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Meuli</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Honey</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Wedeen</surname> <given-names>VJ</given-names></name>, <etal>et al</etal>. <article-title>Mapping the structural core of human cerebral cortex</article-title>. <source>PLoS Biology</source>. <year>2008</year>; <volume>6</volume>: <fpage>e159</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pbio.0060159" xlink:type="simple">10.1371/journal.pbio.0060159</ext-link></comment> <object-id pub-id-type="pmid">18597554</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van den Heuvel</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Sporns</surname> <given-names>O</given-names></name>. <article-title>Rich-club organization of the human connectome</article-title>. <source>J Neurosci</source>. <year>2011</year>; <volume>31</volume>: <fpage>15775</fpage>–<lpage>15786</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3539-11.2011" xlink:type="simple">10.1523/JNEUROSCI.3539-11.2011</ext-link></comment> <object-id pub-id-type="pmid">22049421</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van den Heuvel</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Kahn</surname> <given-names>RS</given-names></name>, <name name-style="western"><surname>Goñi</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Sporns</surname> <given-names>O</given-names></name>. <article-title>High-cost, high-capacity backbone for global brain communication</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2012</year>; <volume>109</volume>: <fpage>11372</fpage>–<lpage>11377</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1203593109" xlink:type="simple">10.1073/pnas.1203593109</ext-link></comment> <object-id pub-id-type="pmid">22711833</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van den Heuvel</surname> <given-names>MP</given-names></name>, <name name-style="western"><surname>Sporns</surname> <given-names>O</given-names></name>. <article-title>An anatomical substrate for integration among functional networks in human cortex</article-title>. <source>J Neurosci</source>. <year>2013</year>, <volume>33</volume>: <fpage>14489</fpage>–<lpage>14500</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2128-13.2013" xlink:type="simple">10.1523/JNEUROSCI.2128-13.2013</ext-link></comment> <object-id pub-id-type="pmid">24005300</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shanahan</surname> <given-names>M</given-names></name>. <article-title>The brain's connective core and its role in animal cognition</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>2012</year>; <volume>367</volume>: <fpage>2704</fpage>–<lpage>2714</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2012.0128" xlink:type="simple">10.1098/rstb.2012.0128</ext-link></comment> <object-id pub-id-type="pmid">22927569</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tognoli</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Kelso</surname> <given-names>JS</given-names></name>. <article-title>The metastable brain</article-title>. <source>Neuron</source>. <year>2014</year>; <volume>81</volume>: <fpage>35</fpage>–<lpage>48</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2013.12.022" xlink:type="simple">10.1016/j.neuron.2013.12.022</ext-link></comment> <object-id pub-id-type="pmid">24411730</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kinnison</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Padmala</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Choi</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Pessoa</surname> <given-names>L</given-names></name>. <article-title>Network analysis reveals increased integration during emotional and motivational processing</article-title>. <source>J Neurosci</source>. <year>2012</year>; <volume>32</volume>: <fpage>8361</fpage>–<lpage>8372</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0821-12.2012" xlink:type="simple">10.1523/JNEUROSCI.0821-12.2012</ext-link></comment> <object-id pub-id-type="pmid">22699916</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Andric</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hasson</surname> <given-names>U</given-names></name>. <article-title>Global features of functional brain networks change with contextual disorder</article-title>. <source>Neuroimage</source>. <year>2015</year>; <volume>117</volume>: <fpage>103</fpage>–<lpage>113</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2015.05.025" xlink:type="simple">10.1016/j.neuroimage.2015.05.025</ext-link></comment> <object-id pub-id-type="pmid">25988223</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vatansever</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Menon</surname> <given-names>DK</given-names></name>, <name name-style="western"><surname>Manktelow</surname> <given-names>AE</given-names></name>, <name name-style="western"><surname>Sahakian</surname> <given-names>BJ</given-names></name>, <name name-style="western"><surname>Stamatakis</surname> <given-names>EA</given-names></name>. <article-title>Default mode dynamics for global functional integration</article-title>. <source>J Neurosci</source>. <year>2015</year>; <volume>35</volume>: <fpage>15254</fpage>–<lpage>15262</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2135-15.2015" xlink:type="simple">10.1523/JNEUROSCI.2135-15.2015</ext-link></comment> <object-id pub-id-type="pmid">26586814</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cohen</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>D'Esposito</surname> <given-names>M</given-names></name>. <article-title>The segregation and integration of distinct brain networks and their relationship to cognition</article-title>. <source>J Neurosci</source>. <year>2016</year>; <volume>36</volume>: <fpage>12083</fpage>–<lpage>12094</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2965-15.2016" xlink:type="simple">10.1523/JNEUROSCI.2965-15.2016</ext-link></comment> <object-id pub-id-type="pmid">27903719</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cole</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Reynolds</surname> <given-names>JR</given-names></name>, <name name-style="western"><surname>Power</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Repovs</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Anticevic</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Braver</surname> <given-names>TS</given-names></name>. <article-title>Multi-task connectivity reveals flexible hubs for adaptive task control</article-title>. <source>Nat Neurosci</source>. <year>2013</year>; <volume>16</volume>: <fpage>1348</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3470" xlink:type="simple">10.1038/nn.3470</ext-link></comment> <object-id pub-id-type="pmid">23892552</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ekman</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Derrfuss</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Tittgemeyer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fiebach</surname> <given-names>CJ</given-names></name>. <article-title>Predicting errors from reconfiguration patterns in human brain networks</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2012</year>; <volume>109</volume>: <fpage>16714</fpage>–<lpage>16719</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1207523109" xlink:type="simple">10.1073/pnas.1207523109</ext-link></comment> <object-id pub-id-type="pmid">23012417</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Whitfield-Gabrieli</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Nieto-Castanon</surname> <given-names>A</given-names></name>. <article-title>Conn: a functional connectivity toolbox for correlated and anticorrelated brain networks</article-title>. <source>Brain Connect</source>. <year>2012</year>; <volume>2</volume>: <fpage>125</fpage>–<lpage>141</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1089/brain.2012.0073" xlink:type="simple">10.1089/brain.2012.0073</ext-link></comment> <object-id pub-id-type="pmid">22642651</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Behzadi</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Restom</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Liau</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>TT</given-names></name>. <article-title>A component based noise correction method (CompCor) for BOLD and perfusion based fMRI</article-title>. <source>Neuroimage</source>. <year>2007</year>; <volume>37</volume>: <fpage>90</fpage>–<lpage>101</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2007.04.042" xlink:type="simple">10.1016/j.neuroimage.2007.04.042</ext-link></comment> <object-id pub-id-type="pmid">17560126</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ekman</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fiebach</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Melzer</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Tittgemeyer</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Derfuss</surname> <given-names>J</given-names></name>. <article-title>Different roles of direct and indirect frontoparietal pathways for individual working memory capacity</article-title>. <source>J Neurosci</source>. <year>2016</year>; <volume>36</volume>: <fpage>2894</fpage>–<lpage>2903</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1376-14.2016" xlink:type="simple">10.1523/JNEUROSCI.1376-14.2016</ext-link></comment> <object-id pub-id-type="pmid">26961945</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rubinov</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Sporns</surname> <given-names>O</given-names></name>. <article-title>Complex network measures of brain connectivity: uses and interpretations</article-title>. <source>Neuroimage</source>. <year>2010</year>; <volume>52</volume>: <fpage>1059</fpage>–<lpage>1069</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2009.10.003" xlink:type="simple">10.1016/j.neuroimage.2009.10.003</ext-link></comment> <object-id pub-id-type="pmid">19819337</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref025"><label>25</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Gelman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hill</surname> <given-names>J</given-names></name>. <chapter-title>Data analysis using regression and multilevel/hierarchical models</chapter-title>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge UP</publisher-name>; <year>2006</year>.</mixed-citation></ref>
<ref id="pone.0207119.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tuerlinckx</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Rijmen</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Verbeke</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Boeck</surname> <given-names>P</given-names></name>. <article-title>Statistical inference in generalized linear mixed models: A review</article-title>. <source>Br J Math Stat Psychol</source>. <year>2006</year>; <volume>59</volume>: <fpage>225</fpage>–<lpage>255</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1348/000711005X79857" xlink:type="simple">10.1348/000711005X79857</ext-link></comment> <object-id pub-id-type="pmid">17067411</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bürkner</surname> <given-names>PC</given-names></name>. <article-title>brms: An R package for Bayesian multilevel models using Stan</article-title>. <source>J Stat Softw</source>. <year>2016</year>; <volume>80</volume>: <fpage>1</fpage>–<lpage>28</lpage>.</mixed-citation></ref>
<ref id="pone.0207119.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bürkner</surname> <given-names>PC</given-names></name>. <article-title>Advanced Bayesian Multilevel Modeling with the R Package brms</article-title>. <source>The R Journal</source>. <year>2018</year>; Forthcoming.</mixed-citation></ref>
<ref id="pone.0207119.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Carpenter</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Gelman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hoffman</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Goodrich</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Betancour</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Brubaker</surname> <given-names>MA</given-names></name>, <etal>et al</etal>. <article-title>Stan: A probabilistic programming language</article-title>. <source>J Stat Softw</source>. <year>2017</year>; <volume>76</volume>: <fpage>1</fpage>–<lpage>43</lpage>.</mixed-citation></ref>
<ref id="pone.0207119.ref030"><label>30</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Kruschke</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Vanpaemel</surname> <given-names>W</given-names></name>. <chapter-title>Bayesian estimation in hierarchical models</chapter-title>. In: <name name-style="western"><surname>Busemeyer</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Townsend</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>ZJ</given-names></name>, <name name-style="western"><surname>Eidels</surname> <given-names>A</given-names></name>, editors. <source>The Oxford Handbook of Computational and Mathematical Psychology</source>. <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford UP</publisher-name>; <year>2015</year>. pp. <fpage>279</fpage>–<lpage>299</lpage>.</mixed-citation></ref>
<ref id="pone.0207119.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kruschke</surname> <given-names>JK</given-names></name>, <name name-style="western"><surname>Liddell</surname> <given-names>TM</given-names></name>. <article-title>The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective</article-title>. <source>Psychon Bull Rev</source>. <year>2018</year>; <volume>25</volume>: <fpage>178</fpage>–<lpage>206</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/s13423-016-1221-4" xlink:type="simple">10.3758/s13423-016-1221-4</ext-link></comment> <object-id pub-id-type="pmid">28176294</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hoffman</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Gelman</surname> <given-names>A</given-names></name>. <article-title>The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo</article-title>. <source>J Mach Learn Res</source>. <year>2014</year>; <volume>15</volume>: <fpage>1593</fpage>–<lpage>1623</lpage>.</mixed-citation></ref>
<ref id="pone.0207119.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>De Vico Fallani</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Richiardi</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Chavez</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Achard</surname> <given-names>S</given-names></name>. <article-title>Graph analysis of functional brain networks: practical issues in translational neuroscience</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>2014</year>; <volume>369</volume>: <fpage>1</fpage>–<lpage>17</lpage>.</mixed-citation></ref>
<ref id="pone.0207119.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gelman</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rubin</surname> <given-names>DB</given-names></name>. <article-title>Inference from iterative simulation using multiple sequences</article-title>. <source>Stat Sci</source>. <year>1992</year>; <fpage>457</fpage>–<lpage>472</lpage>.</mixed-citation></ref>
<ref id="pone.0207119.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Xia</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>He</surname> <given-names>Y</given-names></name>. <article-title>BrainNet Viewer: a network visualization tool for human brain connectomics</article-title>. <source>PLOS One</source>. <year>2013</year>; <volume>8</volume>: <fpage>e68910</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0068910" xlink:type="simple">10.1371/journal.pone.0068910</ext-link></comment> <object-id pub-id-type="pmid">23861951</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref036"><label>36</label><mixed-citation publication-type="other" xlink:type="simple">Bastian M, Heymann S, Jacomy M. Gephi: an open source software for exploring and manipulating networks. Proceedings of the Third International ICWSM Conference. 2009; 361–362.</mixed-citation></ref>
<ref id="pone.0207119.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jacomy</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Venturini</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Heymann</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Bastian</surname> <given-names>M</given-names></name>. <article-title>ForceAtlas2, a continuous graph layout algorithm for handy network visualization designed for the Gephi software</article-title>. <source>PLOS One</source>. <year>2014</year>; <volume>9</volume>: <fpage>e98679</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0098679" xlink:type="simple">10.1371/journal.pone.0098679</ext-link></comment> <object-id pub-id-type="pmid">24914678</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gegenfurtner</surname> <given-names>KR</given-names></name>. <article-title>Cortical mechanisms of colour vision</article-title>. <source>Nat Rev Neurosci</source>. <year>2003</year>; <volume>4</volume>: <fpage>563</fpage>–<lpage>572</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn1138" xlink:type="simple">10.1038/nrn1138</ext-link></comment> <object-id pub-id-type="pmid">12838331</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shapley</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Hawken</surname> <given-names>MJ</given-names></name>. <article-title>Color in the cortex: single-and double-opponent cells</article-title>. <source>Vision Res</source>. <year>2011</year>; <volume>51</volume>: <fpage>701</fpage>–<lpage>717</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2011.02.012" xlink:type="simple">10.1016/j.visres.2011.02.012</ext-link></comment> <object-id pub-id-type="pmid">21333672</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rizzolatti</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Matelli</surname> <given-names>M</given-names></name>.<article-title>Two different streams form the dorsal visual system: anatomy and functions</article-title>. <source>Exp Brain Res</source>. <year>2003</year>; <volume>153</volume>: <fpage>146</fpage>–<lpage>157</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00221-003-1588-0" xlink:type="simple">10.1007/s00221-003-1588-0</ext-link></comment> <object-id pub-id-type="pmid">14610633</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perry</surname> <given-names>CJ</given-names></name>, <name name-style="western"><surname>Fallah</surname> <given-names>M</given-names></name>. <article-title>Feature integration and object representations along the dorsal stream visual hierarchy</article-title>. <source>Front Comput Neurosci</source>. <year>2014</year>; <volume>8</volume>: <fpage>1</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fncom.2014.00001" xlink:type="simple">10.3389/fncom.2014.00001</ext-link></comment></mixed-citation></ref>
<ref id="pone.0207119.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Koelsch</surname> <given-names>S</given-names></name>. <article-title>Toward a neural basis of music perception–a review and updated model</article-title>. <source>Front Psychol</source>. <year>2011</year>; <volume>2</volume>: <fpage>1</fpage>–<lpage>20</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2011.00001" xlink:type="simple">10.3389/fpsyg.2011.00001</ext-link></comment></mixed-citation></ref>
<ref id="pone.0207119.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alho</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Rinne</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Herron</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Woods</surname> <given-names>DL</given-names></name>. <article-title>Stimulus-dependent activations and attention-related modulations in the auditory cortex: a meta-analysis of fMRI studies</article-title>. <source>Hear Res</source>. <year>2014</year>; <volume>307</volume>: <fpage>29</fpage>–<lpage>41</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.heares.2013.08.001" xlink:type="simple">10.1016/j.heares.2013.08.001</ext-link></comment> <object-id pub-id-type="pmid">23938208</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Griffiths</surname> <given-names>TD</given-names></name>, <name name-style="western"><surname>Warren</surname> <given-names>JD</given-names></name>. <article-title>The planum temporale as a computational hub</article-title>. <source>Trends Neurosci</source>. <year>2002</year>; <volume>25</volume>: <fpage>348</fpage>–<lpage>353</lpage>. <object-id pub-id-type="pmid">12079762</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Griffiths</surname> <given-names>TD</given-names></name>. <article-title>Functional imaging of pitch analysis</article-title>. <source>Ann N Y Acad Sci</source>. <year>2003</year>; <volume>999</volume>: <fpage>40</fpage>–<lpage>49</lpage>. <object-id pub-id-type="pmid">14681116</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zatorre</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Belin</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Penhune</surname> <given-names>VB</given-names></name>. <article-title>Structure and function of auditory cortex: music and speech</article-title>. <source>Trends Cogn Sci</source>. <year>2002</year>; <volume>6</volume>: <fpage>37</fpage>–<lpage>46</lpage>. <object-id pub-id-type="pmid">11849614</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Corbetta</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Shulman</surname> <given-names>GL</given-names></name>. <article-title>Control of goal-directed and stimulus-driven attention in the brain</article-title>. <source>Nat Rev Neurosci</source>. <year>2002</year>; <volume>3</volume>: <fpage>201</fpage>–<lpage>215</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn755" xlink:type="simple">10.1038/nrn755</ext-link></comment> <object-id pub-id-type="pmid">11994752</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fox</surname> <given-names>MD</given-names></name>, <name name-style="western"><surname>Snyder</surname> <given-names>AZ</given-names></name>, <name name-style="western"><surname>Vincent</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Corbetta</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Van Essen</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Raichle</surname> <given-names>ME</given-names></name>. <article-title>The human brain is intrinsically organized into dynamic, anticorrelated functional networks</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2005</year>; <volume>102</volume>: <fpage>9673</fpage>–<lpage>9678</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0504136102" xlink:type="simple">10.1073/pnas.0504136102</ext-link></comment> <object-id pub-id-type="pmid">15976020</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ptak</surname> <given-names>R</given-names></name>. <article-title>The frontoparietal attention network of the human brain: action, saliency, and a priority map of the environment</article-title>. <source>Neuroscientist</source>. <year>2012</year>; <volume>18</volume>: <fpage>502</fpage>–<lpage>515</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/1073858411409051" xlink:type="simple">10.1177/1073858411409051</ext-link></comment> <object-id pub-id-type="pmid">21636849</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vossel</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Geng</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Fink</surname> <given-names>GR</given-names></name>. <article-title>Dorsal and ventral attention systems: distinct neural circuits but collaborative roles</article-title>. <source>Neuroscientist</source>. <year>2014</year>; <volume>20</volume>: <fpage>150</fpage>–<lpage>159</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/1073858413494269" xlink:type="simple">10.1177/1073858413494269</ext-link></comment> <object-id pub-id-type="pmid">23835449</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Egner</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Monti</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Trittschuh</surname> <given-names>EH</given-names></name>, <name name-style="western"><surname>Wieneke</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Hirsch</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Mesulam</surname> <given-names>MM</given-names></name>. <article-title>Neural integration of top-down spatial and feature-based information in visual search</article-title>. <source>J Neurosci</source>. <year>2008</year>; <volume>28</volume>: <fpage>6141</fpage>–<lpage>6151</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1262-08.2008" xlink:type="simple">10.1523/JNEUROSCI.1262-08.2008</ext-link></comment> <object-id pub-id-type="pmid">18550756</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liu</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Hospadaruk</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Zhu</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Gardner</surname> <given-names>JL</given-names></name>. <article-title>Feature-specific attentional priority signals in human cortex</article-title>. <source>J Neurosci</source>. <year>2011</year>; <volume>31</volume>: <fpage>4484</fpage>–<lpage>4495</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5745-10.2011" xlink:type="simple">10.1523/JNEUROSCI.5745-10.2011</ext-link></comment> <object-id pub-id-type="pmid">21430149</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jerde</surname> <given-names>TA</given-names></name>, <name name-style="western"><surname>Merriam</surname> <given-names>EP</given-names></name>, <name name-style="western"><surname>Riggall</surname> <given-names>AC</given-names></name>, <name name-style="western"><surname>Hedges</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Curtis</surname> <given-names>CE</given-names></name>. <article-title>Prioritized maps of space in human frontoparietal cortex</article-title>. <source>J Neurosci</source>. <year>2012</year>; <volume>32</volume>: <fpage>17382</fpage>–<lpage>17390</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3810-12.2012" xlink:type="simple">10.1523/JNEUROSCI.3810-12.2012</ext-link></comment> <object-id pub-id-type="pmid">23197729</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wise</surname> <given-names>SP</given-names></name>, <name name-style="western"><surname>Di Pellegrino</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Boussaoud</surname> <given-names>D</given-names></name>. <article-title>The premotor cortex and nonstandard sensorimotor mapping</article-title>. <source>Can J Physiol Pharmacol</source>. <year>1996</year>; <volume>74</volume>: <fpage>469</fpage>–<lpage>482</lpage>. <object-id pub-id-type="pmid">8828893</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grefkes</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Fink</surname> <given-names>GR</given-names></name>. <article-title>The functional organization of the intraparietal sulcus in humans and monkeys</article-title>. <source>J Anat</source>. <year>2005</year>; <volume>207</volume>: <fpage>3</fpage>–<lpage>17</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1469-7580.2005.00426.x" xlink:type="simple">10.1111/j.1469-7580.2005.00426.x</ext-link></comment> <object-id pub-id-type="pmid">16011542</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brovelli</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Badier</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Bonini</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Bartolomei</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Coulon</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Auzias</surname> <given-names>G</given-names></name>. <article-title>Dynamic reconfiguration of visuomotor-related functional connectivity networks</article-title>. <source>J Neurosci</source>. <year>2017</year>; <volume>37</volume>: <fpage>839</fpage>–<lpage>853</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1672-16.2016" xlink:type="simple">10.1523/JNEUROSCI.1672-16.2016</ext-link></comment> <object-id pub-id-type="pmid">28123020</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Macaluso</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Driver</surname> <given-names>J</given-names></name>. <article-title>Spatial attention and crossmodal interactions between vision and touch</article-title>. <source>Neuropsychologia</source>. <year>2001</year>; <volume>39</volume>: <fpage>1304</fpage>–<lpage>1316</lpage>. <object-id pub-id-type="pmid">11566313</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Macaluso</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Frith</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Driver</surname> <given-names>J</given-names></name>. <article-title>Supramodal effects of covert spatial orienting triggered by visual or tactile events</article-title>. <source>J Cogn Neurosci</source>. <year>2002</year>; <volume>14</volume>: <fpage>389</fpage>–<lpage>401</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/089892902317361912" xlink:type="simple">10.1162/089892902317361912</ext-link></comment> <object-id pub-id-type="pmid">11970799</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shomstein</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Yantis</surname> <given-names>S</given-names></name>. <article-title>Control of attention shifts between vision and audition in human cortex</article-title>. <source>J Neurosci</source>. <year>2004</year>; <volume>24</volume>: <fpage>10702</fpage>–<lpage>10706</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2939-04.2004" xlink:type="simple">10.1523/JNEUROSCI.2939-04.2004</ext-link></comment> <object-id pub-id-type="pmid">15564587</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref060"><label>60</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Smith</surname> <given-names>DV</given-names></name>, <name name-style="western"><surname>Davis</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Niu</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Healy</surname> <given-names>EW</given-names></name>, <name name-style="western"><surname>Bonilha</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Fridriksson</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Spatial attention evokes similar activation patterns for visual and auditory stimuli</article-title>. <source>J Cogn Neurosci</source>. <year>2010</year>; <volume>22</volume>: <fpage>347</fpage>–<lpage>361</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn.2009.21241" xlink:type="simple">10.1162/jocn.2009.21241</ext-link></comment> <object-id pub-id-type="pmid">19400684</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Green</surname> <given-names>JJ</given-names></name>, <name name-style="western"><surname>Doesburg</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Ward</surname> <given-names>LM</given-names></name>, <name name-style="western"><surname>McDonald</surname> <given-names>JJ</given-names></name>. <article-title>Electrical neuroimaging of voluntary audiospatial attention: evidence for a supramodal attention control network</article-title>. <source>J Neurosci</source>. <year>2011</year>; <volume>31</volume>: <fpage>3560</fpage>–<lpage>3564</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.5758-10.2011" xlink:type="simple">10.1523/JNEUROSCI.5758-10.2011</ext-link></comment> <object-id pub-id-type="pmid">21389212</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Macaluso</surname> <given-names>E</given-names></name> &amp; <name name-style="western"><surname>Driver</surname> <given-names>J</given-names></name>. <article-title>Multisensory spatial interactions: a window onto functional integration in the human brain</article-title>. <source>Trends Neurosci</source>. <year>2005</year>; <volume>28</volume>: <fpage>264</fpage>–<lpage>271</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tins.2005.03.008" xlink:type="simple">10.1016/j.tins.2005.03.008</ext-link></comment> <object-id pub-id-type="pmid">15866201</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Macaluso</surname> <given-names>E</given-names></name>. <article-title>Orienting of spatial attention and the interplay between the senses</article-title>. <source>Cortex</source>. <year>2010</year>; <volume>46</volume>: <fpage>282</fpage>–<lpage>297</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cortex.2009.05.010" xlink:type="simple">10.1016/j.cortex.2009.05.010</ext-link></comment> <object-id pub-id-type="pmid">19540475</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref064"><label>64</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bressler</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Tang</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Sylvester</surname> <given-names>CM</given-names></name>, <name name-style="western"><surname>Shulman</surname> <given-names>GL</given-names></name>, <name name-style="western"><surname>Corbetta</surname> <given-names>M</given-names></name>. <article-title>Top-down control of human visual cortex by frontal and parietal cortex in anticipatory visual spatial attention</article-title>. <source>J Neurosci</source>. <year>2008</year>; <volume>28</volume>: <fpage>10056</fpage>–<lpage>10061</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.1776-08.2008" xlink:type="simple">10.1523/JNEUROSCI.1776-08.2008</ext-link></comment> <object-id pub-id-type="pmid">18829963</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vossel</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Weidner</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Driver</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Friston</surname> <given-names>KJ</given-names></name>, <name name-style="western"><surname>Fink</surname> <given-names>GR</given-names></name>. <article-title>Deconstructing the architecture of dorsal and ventral attention systems with dynamic causal modeling</article-title>. <source>J Neurosci</source>. <year>2012</year>; <volume>32</volume>: <fpage>10637</fpage>–<lpage>10648</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.0414-12.2012" xlink:type="simple">10.1523/JNEUROSCI.0414-12.2012</ext-link></comment> <object-id pub-id-type="pmid">22855813</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yeo</surname> <given-names>BT</given-names></name>, <name name-style="western"><surname>Krienen</surname> <given-names>FM</given-names></name>, <name name-style="western"><surname>Chee</surname> <given-names>MW</given-names></name>, <name name-style="western"><surname>Buckner</surname> <given-names>RL</given-names></name>. <article-title>Estimates of segregation and overlap of functional connectivity networks in the human cerebral cortex</article-title>. <source>Neuroimage</source>. <year>2014</year>; <volume>88</volume>: <fpage>212</fpage>–<lpage>227</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2013.10.046" xlink:type="simple">10.1016/j.neuroimage.2013.10.046</ext-link></comment> <object-id pub-id-type="pmid">24185018</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Chadick</surname> <given-names>JZ</given-names></name>, <name name-style="western"><surname>Gazzaley</surname> <given-names>A</given-names></name>. <article-title>Differential coupling of visual cortex with default or frontal-parietal network based on goals</article-title>. <source>Nat Neurosci</source>. <year>2011</year>; <volume>14</volume>: <fpage>830</fpage>–<lpage>832</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.2823" xlink:type="simple">10.1038/nn.2823</ext-link></comment> <object-id pub-id-type="pmid">21623362</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alnæs</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kaufmann</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Richard</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Duff</surname> <given-names>EP</given-names></name>, <name name-style="western"><surname>Sneve</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Endestad</surname> <given-names>T</given-names></name>, <etal>et al</etal>. <article-title>Attentional load modulates large-scale functional brain connectivity beyond the core attention networks</article-title>. <source>Neuroimage</source>. <year>2015</year>; <volume>109</volume>: <fpage>260</fpage>–<lpage>272</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2015.01.026" xlink:type="simple">10.1016/j.neuroimage.2015.01.026</ext-link></comment> <object-id pub-id-type="pmid">25595500</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Spadone</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Della Penna</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sestieri</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Betti</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Tosoni</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Perrucci</surname> <given-names>MG</given-names></name>, <etal>et al</etal>. <article-title>Dynamic reorganization of human resting-state networks during visuospatial attention</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2015</year>; <volume>112</volume>: <fpage>8112</fpage>–<lpage>8117</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1415439112" xlink:type="simple">10.1073/pnas.1415439112</ext-link></comment> <object-id pub-id-type="pmid">26080395</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref070"><label>70</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wen</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Liang</surname> <given-names>B</given-names></name>, <name name-style="western"><surname>Zhang</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Reconfiguration of the brain functional network associated with visual task demands</article-title>. <source>PLOS One</source>. <year>2015</year>; <volume>10</volume>: <fpage>e0132518</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0132518" xlink:type="simple">10.1371/journal.pone.0132518</ext-link></comment> <object-id pub-id-type="pmid">26146993</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref071"><label>71</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kwon</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Watanabe</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Fischer</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Bartels</surname> <given-names>A</given-names></name>. <article-title>Attention reorganizes connectivity across networks in a frequency specific manner</article-title>. <source>Neuroimage</source>. <year>2017</year>; <volume>144</volume>: <fpage>217</fpage>–<lpage>226</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2016.10.014" xlink:type="simple">10.1016/j.neuroimage.2016.10.014</ext-link></comment> <object-id pub-id-type="pmid">27732887</object-id></mixed-citation></ref>
<ref id="pone.0207119.ref072"><label>72</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sadaghiani</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Poline</surname> <given-names>JB</given-names></name>, <name name-style="western"><surname>Kleinschmidt</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>D’Esposito</surname> <given-names>M</given-names></name>. <article-title>Ongoing dynamics in large-scale functional connectivity predict perception</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2015</year>; <volume>112</volume>: <fpage>8463</fpage>–<lpage>8468</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1420687112" xlink:type="simple">10.1073/pnas.1420687112</ext-link></comment> <object-id pub-id-type="pmid">26106164</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>