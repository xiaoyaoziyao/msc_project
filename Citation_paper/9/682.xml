<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pone.0186916</article-id>
<article-id pub-id-type="publisher-id">PONE-D-17-11899</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Bioassays and physiological analysis</subject><subj-group><subject>Electrophysiological techniques</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Physiology</subject><subj-group><subject>Electrophysiology</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neurophysiology</subject><subj-group><subject>Brain electrophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Brain mapping</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Clinical medicine</subject><subj-group><subject>Clinical neurophysiology</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Neuroimaging</subject><subj-group><subject>Electroencephalography</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Scalp</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Scalp</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Physics</subject><subj-group><subject>Thermodynamics</subject><subj-group><subject>Entropy</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Amygdala</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Brain</subject><subj-group><subject>Amygdala</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Bioassays and physiological analysis</subject><subj-group><subject>Electrophysiological techniques</subject><subj-group><subject>Membrane electrophysiology</subject><subj-group><subject>Electrode recording</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Diagnostic medicine</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Imaging techniques</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Radiology and imaging</subject><subj-group><subject>Diagnostic radiology</subject><subj-group><subject>Magnetic resonance imaging</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Discrimination of emotional states from scalp- and intracranial EEG using multiscale Rényi entropy</article-title>
<alt-title alt-title-type="running-head">Discrimination of emotional states with Renyi entropy</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-6141-1933</contrib-id>
<name name-style="western">
<surname>Tonoyan</surname>
<given-names>Yelena</given-names>
</name>
<role content-type="http://credit.casrai.org/">Data curation</role>
<role content-type="http://credit.casrai.org/">Investigation</role>
<role content-type="http://credit.casrai.org/">Methodology</role>
<role content-type="http://credit.casrai.org/">Software</role>
<role content-type="http://credit.casrai.org/">Writing – original draft</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Chanwimalueang</surname>
<given-names>Theerasak</given-names>
</name>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Mandic</surname>
<given-names>Danilo P.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Writing – review &amp; editing</role>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Van Hulle</surname>
<given-names>Marc M.</given-names>
</name>
<role content-type="http://credit.casrai.org/">Supervision</role>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Research Group Neurophysiology, Laboratory for Neuro- and Psychophysiology, Leuven, Belgium</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Communication and Signal Processing Research Group, Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>van Luijtelaar</surname>
<given-names>Gilles</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Radboud Universiteit, NETHERLANDS</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">elenatonoyan@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>3</day>
<month>11</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="collection">
<year>2017</year>
</pub-date>
<volume>12</volume>
<issue>11</issue>
<elocation-id>e0186916</elocation-id>
<history>
<date date-type="received">
<day>11</day>
<month>4</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>10</day>
<month>10</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>Tonoyan et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0186916"/>
<abstract>
<p>A data-adaptive, multiscale version of Rényi’s quadratic entropy (RQE) is introduced for emotional state discrimination from EEG recordings. The algorithm is applied to scalp EEG recordings of 30 participants watching 4 emotionally-charged video clips taken from a validated public database. Krippendorff’s inter-rater statistic reveals that multiscale RQE of the mid-frontal scalp electrodes best discriminates between five emotional states. Multiscale RQE is also applied to joint scalp EEG, amygdala- and occipital pole intracranial recordings of an implanted patient watching a neutral and an emotionally charged video clip. Unlike for the neutral video clip, the RQEs of the mid-frontal scalp electrodes and the amygdala-implanted electrodes are observed to coincide in the time range where the crux of the emotionally-charged video clip is revealed. In addition, also during this time range, phase synchrony between the amygdala and mid-frontal recordings is maximal, as well as our 30 participants’ inter-rater agreement on the same video clip. A source reconstruction exercise using intracranial recordings supports our assertion that amygdala could contribute to mid-frontal scalp EEG. On the contrary, no such contribution was observed for the occipital pole’s intracranial recordings. Our results suggest that emotional states discriminated from mid-frontal scalp EEG are likely to be mirrored by differences in amygdala activations in particular when recorded in response to emotionally-charged scenes.</p>
</abstract>
<funding-group>
<funding-statement>YT is supported by the Interuniversity Attraction Poles Programme Belgian Science Policy (IUAP P7/11). MMVH is supported by research grants received from the Financing program (PFV/10/008), an interdisciplinary research project (IDO/12/007), and an industrial research fund project (IOF/HB/12/021) of the KU Leuven, the Belgian Fund for Scientific Research – Flanders (G088314N, G0A0914N), the Interuniversity Attraction Poles Programme – Belgian Science Policy (IUAP P7/11), the Flemish Regional Ministry of Education (Belgium) (GOA 10/019), and the Hercules Foundation (AKUL 043). DM is supported by an EPSRC grant (EP/K025643/1).</funding-statement>
</funding-group>
<counts>
<fig-count count="8"/>
<table-count count="1"/>
<page-count count="22"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All raw data files are available from the KU Leuven Box database (<ext-link ext-link-type="uri" xlink:href="https://kuleuven.app.box.com/v/EntropyEmotion?sortColumn=date&amp;sortDirection=desc&amp;pageSize=20&amp;pageNumber=0" xlink:type="simple">https://kuleuven.app.box.com/v/EntropyEmotion?sortColumn=date&amp;sortDirection=desc&amp;pageSize=20&amp;pageNumber=0</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>1 Introduction</title>
<p>The identification and discrimination of emotional states from EEG is considered notoriously challenging, mainly due to the difficulty to gauge electromagnetic activity elicited by cortical structures involved in processing emotional information [<xref ref-type="bibr" rid="pone.0186916.ref001">1</xref>], yet potentially useful for a broad range of important applications such as diagnosing and treating patients with dysfunctional processing of emotional information [<xref ref-type="bibr" rid="pone.0186916.ref002">2</xref>], emotion-sensitive interactive games, affective interfaces, and emotion-sensitive tutoring systems [<xref ref-type="bibr" rid="pone.0186916.ref003">3</xref>–<xref ref-type="bibr" rid="pone.0186916.ref005">5</xref>]. This article sets out to address these challenges, but rather than evaluating the power in the standard EEG frequency bands, as is traditionally done [<xref ref-type="bibr" rid="pone.0186916.ref006">6</xref>–<xref ref-type="bibr" rid="pone.0186916.ref013">13</xref>], or amplitudes and latencies of event-related potentials (ERPs) in response to emotion-evoking stimuli [<xref ref-type="bibr" rid="pone.0186916.ref014">14</xref>–<xref ref-type="bibr" rid="pone.0186916.ref019">19</xref>], we conduct the analysis within the realm of complexity science [<xref ref-type="bibr" rid="pone.0186916.ref020">20</xref>].</p>
<p>Several entropy-based metrics of signal complexity have already been proposed for discriminating emotional states. Aftanas and co-workers [<xref ref-type="bibr" rid="pone.0186916.ref021">21</xref>] showed that, when viewing images evoking negative and positive emotions, higher values of EEG correlation dimension complexity are observed compared to viewing neutral images. Hosseini and Naghibi-Sistani [<xref ref-type="bibr" rid="pone.0186916.ref022">22</xref>] applied two entropic metrics (approximate and wavelet entropy) to discriminate between two emotional states (calm-neutral and negative-excited) in response to viewing sequences of emotion inducing pictures and achieved 73.25% classification accuracy. Jie et al. [<xref ref-type="bibr" rid="pone.0186916.ref023">23</xref>] applied sample entropy to EEG data obtained from two binary emotion recognition tasks (positive vs. negative emotion both with high arousal, and music clips with different arousal levels) and achieved 80.43% and 79.11% classification performance.</p>
<p>A promising development in signal complexity analysis is Rényi entropy (RE). Originally introduced as a generalization of Shannon entropy [<xref ref-type="bibr" rid="pone.0186916.ref024">24</xref>], RE has enjoyed several successful EEG-based clinical applications [<xref ref-type="bibr" rid="pone.0186916.ref013">13</xref>,<xref ref-type="bibr" rid="pone.0186916.ref025">25</xref>]. However, RE has been less utilised for EEG-based mental and affective state detection. Sourina and co-workers [<xref ref-type="bibr" rid="pone.0186916.ref026">26</xref>] used an RE-based variant (Hausdorff dimension) to calculate the fractal dimension of EEG for real time emotion quantification and classification, and Bajaj and Pachori [<xref ref-type="bibr" rid="pone.0186916.ref027">27</xref>] used RE together with other EEG complexity measures for emotion detection.</p>
<p>However, one should also be aware that, at least according to some authors [<xref ref-type="bibr" rid="pone.0186916.ref028">28</xref>], neither strictly periodic nor completely random signals should be regarded complex but rather signals that possess long range correlations across multiple temporal scales. As the cited studies assess entropy on a single scale, and since the used entropy metrics become maximal for random signals, their outcome could be confounding randomness with complexity. To avoid this confusion, Costa and co-workers [<xref ref-type="bibr" rid="pone.0186916.ref029">29</xref>] accounted for the interdependence between entropy and scale and proposed to calculate entropy, in their case sample entropy, on different temporal scales (whence, multiscale sample entropy, MSE), using the so-called coarse graining approach (averaging the signal over non-overlapping windows of increasing length). However, as the latter factually corresponds to a smoothing operation, predominantly low-frequency signal components were captured. To overcome this, empirical mode decomposition (EMD) [<xref ref-type="bibr" rid="pone.0186916.ref030">30</xref>] and its multivariate extension (MEMD) [<xref ref-type="bibr" rid="pone.0186916.ref031">31</xref>] have been suggested: a fully data-driven, time-frequency technique that decomposes a signal into a finite set of amplitude/frequency modulated components, called intrinsic mode functions (IMFs). A further improvement is multivariate MSE (MMSE), to account for both within and cross-channel dependencies, further combined with MEMD into MEMD-enhanced MMSE [<xref ref-type="bibr" rid="pone.0186916.ref032">32</xref>]. When applying EMD to EEG recordings, entropy can be estimated in each IMF individually. Sharma et al. [<xref ref-type="bibr" rid="pone.0186916.ref033">33</xref>] recently applied this technique to several entropic complexity measures, including RE, to predict focal epileptic seizures from EEG. Also recently, Tonoyan and co-workers [<xref ref-type="bibr" rid="pone.0186916.ref034">34</xref>] used MEMD in combination with MSE to discriminate 5 emotional states from mid-frontal EEG recordings when viewing emotionally charged video clips compiled by Schaefer and co-workers [<xref ref-type="bibr" rid="pone.0186916.ref035">35</xref>]. Albeit the results were encouraging, MSE-based entropy estimation turned out not only to be sensitive to the choice of window length but also computationally intensive.</p>
<p>Our aim for this study is to curb these drawbacks by revisiting MEMD-based EEG signal complexity in response to emotionally charged video clips, but now based on Rényi’s quadratic entropy (RQE) applied to whole scalp recordings of 30 subjects. We use Krippendorff’s inter-rater statistic [<xref ref-type="bibr" rid="pone.0186916.ref036">36</xref>] to identify the scalp electrodes that best discriminate between emotional states across subjects. We also recorded intracranial EEG (iEEG) of the amygdala and the occipital cortex jointly with scalp EEG of a patient viewing emotional and neutral video clips. The amygdala is considered to be strongly involved in emotional processing [<xref ref-type="bibr" rid="pone.0186916.ref037">37</xref>–<xref ref-type="bibr" rid="pone.0186916.ref048">48</xref>]. Motivated by Makeig and co-workers [<xref ref-type="bibr" rid="pone.0186916.ref049">49</xref>], who considered similar signal processing techniques for analyzing Human Intracranial Electrophysiology (HIE) and scalp EEG, we apply the same MEMD-based RQE method to the intracranial recordings and verify whether we can discriminate between the neutral and the emotional movies in a way similar to the jointly recorded scalp EEG.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>2. Materials and methods</title>
<sec id="sec003">
<title>2.1 Materials</title>
<sec id="sec004">
<title>2.1.1 Video clips</title>
<p>We considered emotionally-charged video clips of the public database developed by Schaefer and co-workers [<xref ref-type="bibr" rid="pone.0186916.ref035">35</xref>] (<ext-link ext-link-type="uri" xlink:href="http://nemo.psp.ucl.ac.be/FilmStim/" xlink:type="simple">http://nemo.psp.ucl.ac.be/FilmStim/</ext-link>) (<xref ref-type="table" rid="pone.0186916.t001">Table 1</xref>). The spoken language of these video clips was French or dubbed into French. Each video clip in the database is labeled in terms of emotional category (fear, anger, sadness, disgust, amusement, tenderness, neutral, further called “standard label”), which Schaefer and co-workers obtained by asking their participants to report what they personally felt in reaction to the video clips, not what they believed people would generally feel. The average duration of all video clips was approximately 3 minutes.</p>
<table-wrap id="pone.0186916.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0186916.t001</object-id>
<label>Table 1</label> <caption><title>Names, labels and affect scores of the video clips used in our main and control experiments.</title> <p>Name of videos between quotes and scene numbers between round brackets, standard labels between square brackets, positive or negative emotional affect scores (mean scores between round brackets), and our participants’ self-labels (underlined followed by the number of respondents between brackets). Video clips were taken from <ext-link ext-link-type="uri" xlink:href="http://nemo.psp.ucl.ac.be/FilmStim/" xlink:type="simple">http://nemo.psp.ucl.ac.be/FilmStim/</ext-link>, standard labels and affect scores from Table 1 in [<xref ref-type="bibr" rid="pone.0186916.ref035">35</xref>] or provided by Alexandre Schaefer (personal communication).</p></caption>
<alternatives>
<graphic id="pone.0186916.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0186916.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="center" colspan="2">Videos</th>
<th align="center" colspan="2">[Standard label] self-labels</th>
<th align="center" colspan="2">Affect scores</th>
</tr>
<tr>
<th align="center">Main</th>
<th align="center">Control</th>
<th align="center">Main</th>
<th align="center">Control</th>
<th align="center">Main</th>
<th align="center">Control</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">‘Sleepers’</td>
<td align="center">‘Schindler’s List (3)’</td>
<td align="center">[Anger] <underline>Anger</underline>(14)/<underline>Disgust</underline> (16)</td>
<td align="center">[Anger] <underline>Anger</underline> (6)</td>
<td align="center">Negative (2.46)</td>
<td align="center">Negative (2.14)</td>
</tr>
<tr>
<td align="center">‘Life is beautiful’ (4)</td>
<td align="center">‘The eight day’</td>
<td align="center">[Tenderness] <underline>Tenderness</underline> (30)</td>
<td align="center">[Tenderness] <underline>Tenderness</underline> (6)</td>
<td align="center">Positive (2.49)</td>
<td align="center">Positive (2.07)</td>
</tr>
<tr>
<td align="center">‘City of angels’</td>
<td align="center">‘Life is beautiful’ (1)</td>
<td align="center">[Sadness] <underline>Sadness</underline> (30)</td>
<td align="center">[Sadness] <underline>Sadness</underline> (6)</td>
<td align="center">Negative (1.51)</td>
<td align="center">Negative (2.01)</td>
</tr>
<tr>
<td align="center">‘La cité de la peur’</td>
<td align="center">‘The visitors’</td>
<td align="center">[Amusement] <underline>Amusement</underline> (30)</td>
<td align="center">[Amusement] <underline>Amusement</underline> (6)</td>
<td align="center">Positive (2.31)</td>
<td align="center">Positive (2.25)</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
<p>We performed two experiments, further called main and control. For our main experiment, we selected two sets of 4 video clips (out of 70 video clips), all with top ten scores in their respective emotional categories. For the first set, 3 out of 4 videos (disgust, amusement, tenderness) had, in addition, also top ten positive or negative affect scores (cf., <xref ref-type="table" rid="pone.0186916.t001">Table 1</xref>); the 4<sup>th</sup> video (sadness) did not have a top ten affect score. It was selected to verify the statement of Aftanas and co-workers [<xref ref-type="bibr" rid="pone.0186916.ref021">21</xref>] that higher complexity values can be observed for affective stimuli compared to more neutral ones. As these videos have different emotional labels and affect scores, we want to see whether this translates into a difference in EEG complexity values. For the second study, 4 video clips were chosen but not from the top ten affect score list: as these videos shared lower affect scores, we used them in our experiment as controls. All videos were presented in random order to our participants.</p>
<p>For the implanted patient, as the implant serves a medical purpose, the time window that we could dispose of to perform experiments was restricted. Hence, we used only two video clips: fragment 3 of ‘Schindler’s list’ as an emotional video (standard labeled as ‘anger’) and the weather forecast as an example of a more neutral video. The data recorded in this patient was used as a case study for assessing the differential activation of the amygdala in response to the two video clips and the putative relation between amygdala activation and scalp EEG.</p>
<p>After watching each video clip, all participants (including the implanted patient) were asked to categorize the evoked emotion (after providing them with the same list as used in Schaefer et al.’s: fear, anger, sadness, disgust, amusement, tenderness, neutral). We further call these the ‘self-labels’. Note that our participants were not informed about the video clips’ standard labels. Only in the case of ‘anger’ the self-labels were not univocal as 16 participants reported ‘disgust’ instead (‘Sleepers’, i.e., a video about child abuse). To summarize, we have 3 labels (<xref ref-type="table" rid="pone.0186916.t001">Table 1</xref>):</p>
<list list-type="bullet">
<list-item><p>standard label: the emotional category of each video clip, taken from Schaefer et al.;</p></list-item>
<list-item><p>self-label: the emotional category of each video clip selected by our participants from the list of standard labels used by Schaefer et al.;</p></list-item>
<list-item><p>self-reported emotional affect scores of each video, also taken from Schaefer et al.</p></list-item>
</list>
<p>Unless noted otherwise, we will use the self-labels for labeling the entropy curves.</p>
</sec>
<sec id="sec005">
<title>2.1.2 Participants</title>
<p>The main experiment was performed with 30 healthy volunteers (20 female, 10 male, mean age = 32.48, SD = 15.77, age range 19–70) who master French language (i.e., French as mother tongue or French-Dutch bilinguals). We also recruited 2 non-French speaking volunteers (1 Flemish-Dutch speaking 24 yo male, 1 Armenian speaking 28 yo female). For the experiment with the control videos (with low affect scores), we tested 6 volunteers (2 female, 4 male, mean age = 27, SD = 1.6, age range 25–30). Volunteers were recruited via emails, social media posts, flyers, and billboard announcements. Some were university graduate students (KU Leuven, VUB), often being regular subjects in our EEG experiments, and were paid. No participant had any known neurological or psychiatric disorder. Ethical approval for this study was granted by an independent ethical committee (“Commissie voor Medische Ethiek” of UZ Leuven, our University Hospital). The study was conducted in accordance with the most recent version of the Declaration of Helsinki (2013). Before participating in the experiment, all recruited participants were informed about the goal of the study, what would be their task, and what would be done with the recorded data (privacy), after which they read and signed the informed consent form that was previously approved by the said ethical committee. All EEG recordings were performed between 10/12/2014 and 13/05/2015. The raw scalp EEG recordings are available from <ext-link ext-link-type="uri" xlink:href="https://kuleuven.box.com/v/EntropyEmotion" xlink:type="simple">https://kuleuven.box.com/v/EntropyEmotion</ext-link></p>
</sec>
<sec id="sec006">
<title>2.1.3 EEG recording and preprocessing</title>
<p>Participants were tested in a sound-attenuated, darkened room with a constant temperature of 20 degrees, sitting in front of an LCD screen. Each participant’s task was to watch the video clips and report the emotional categories. When viewing the video clips, EEG was recorded<sup>5</sup> continuously using 32 active electrodes, evenly distributed over the entire scalp (positioning and naming convention following a subset of the extended 10–20 system) using a BioSemi ActiveTwo system (BioSemi, Amsterdam, the Netherlands) as well as an electro-oculogram (EOG) using the set-up of Croft and co-workers [<xref ref-type="bibr" rid="pone.0186916.ref050">50</xref>]. The EEG signals were re-referenced offline from the original common mode sense reference [<xref ref-type="bibr" rid="pone.0186916.ref051">51</xref>] (CMS, positioned next to electrode Pz) to the average of two additional electrodes that were placed on the subject’s mastoids. The duration of the experiment excluding electrode setup was 20 minutes. The EEG signals were filtered using a 4<sup>th</sup>-order Butterworth bandpass filter with range 0.5–30 Hz. The original sampling rate of 2048 Hz was downsampled to 128 Hz (including anti-aliasing) to reduce computational costs. Finally, the EOG signal was utilized for removing eye artifacts following the Revised Artifact-Aligned Averaging (RAAA) procedure described in [<xref ref-type="bibr" rid="pone.0186916.ref050">50</xref>].</p>
</sec>
<sec id="sec007">
<title>2.1.4 Intracranial EEG recording and preprocessing</title>
<p>We also recorded intracranial EEG (iEEG), also termed Electrocorticography (ECoG), from a patient, scheduled for resective surgery, as part of her epileptic seizure treatment, when viewing an emotional and a neutral video clip. The patient was implanted with an intracranial electrode (depth electrode with 10 contacts, contact size 2.4/1.1 mm (overall length/diameter) and 4 mm inter-contact spacing) in the right hippocampus until the amygdala (2 contact points present) and with a subdural electrode grid (4x5 electrodes with 4 mm electrode diameter, 2.3 mm electrode exposure and 10 or 15 mm inter-contact spacing) over the right occipital cortex (<xref ref-type="supplementary-material" rid="pone.0186916.s005">S5 Appendix</xref>). Recordings were made with a Micromed digital video compatible EEG recording system (Micromed Spa, Mogliano Veneto, Italy). The sampling frequency of the recording was set to 1028 Hz. Offline pre-processing and downsampling were done using the same parameters as for the scalp-recorded EEG, except that a CMS reference per intracranial electrode (10 contacts) and grid (20 contacts) was used. Ethical approval for this study was granted by an independent ethical committee (“Commissie voor Medische Ethiek” of UZ Gent) and conducted in accordance with the most recent version of the Declaration of Helsinki (2013). Before participating in the experiment, the patient was informed about the goal of the study, what would be her task, and what would be done with the recorded data (privacy), after which she read and signed the informed consent form that was previously approved by the said ethical committee. The recordings were done on 29/02/2016.</p>
</sec>
</sec>
<sec id="sec008" sec-type="materials|methods">
<title>2.2 Methods</title>
<sec id="sec009">
<title>2.2.1 Sample entropy</title>
<p>We compare 3 entropy-based methods. The first one is Sample Entropy (SE) [<xref ref-type="bibr" rid="pone.0186916.ref052">52</xref>]. It corresponds to the conditional probability that two sequences that are similar to each other for <italic>m</italic> consecutive data points (samples), within tolerance level <italic>r</italic>, and remain similar when one more data point to each sequence is added. Formally, <italic>SE</italic> is expressed as follows:
<disp-formula id="pone.0186916.e001">
<alternatives>
<graphic id="pone.0186916.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0186916.e001" xlink:type="simple"/>
<mml:math display="block" id="M1">
<mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac>
</mml:math>
</alternatives>
</disp-formula>
where <italic>B</italic><sup><italic>m</italic></sup>(<italic>r</italic>) is the probability that the similarity between two sequences of length <italic>m</italic> obeys the tolerance level <italic>r</italic>, <italic>A</italic><sup><italic>m</italic>+1</sup> (<italic>r</italic>) the probability that the similarity between the same two sequences but now extended with one data point, thus of length <italic>m</italic> + 1, also obeys <italic>r</italic>, and <italic>N</italic> the number of sequences. The tolerance level <italic>r</italic> is usually set to a percentage of the standard deviation of the normalized data; for our case we selected 15% [<xref ref-type="bibr" rid="pone.0186916.ref031">31</xref>].</p>
<p>In order to estimate sample entropy for a multivariate case (MSE), the sequences are formulated as follows. Recalling multivariate embedding theory [<xref ref-type="bibr" rid="pone.0186916.ref053">53</xref>], for <italic>d</italic>-variate time series <inline-formula id="pone.0186916.e002"><alternatives><graphic id="pone.0186916.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0186916.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1,2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>d</mml:mi></mml:math></alternatives></inline-formula>, the multivariate embedded sequence (so-called a composite delay vector) can be constructed as:
<disp-formula id="pone.0186916.e003">
<alternatives>
<graphic id="pone.0186916.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0186916.e003" xlink:type="simple"/>
<mml:math display="block" id="M3">
<mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mspace width="4pt"/><mml:mo>=</mml:mo><mml:mspace width="4pt"/><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>]</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
where <inline-formula id="pone.0186916.e004"><alternatives><graphic id="pone.0186916.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0186916.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi mathvariant="bold">m</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, <inline-formula id="pone.0186916.e005"><alternatives><graphic id="pone.0186916.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0186916.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> is the embedding vector and <bold><italic>τ</italic></bold> = [<italic>τ</italic><sub>1</sub>, <italic>τ</italic><sub>2</sub>, …, <italic>τ</italic><sub><italic>d</italic></sub>] the time lag vector. The above sample entropy definition is then applied to the composite delay vector.</p>
<p>In our case, we have <italic>d</italic> electrode channels, <italic>n</italic> the number of samples or data points in a sequence (further called ‘snippet’ as its length is small in practice), and <italic>N</italic> the number of sequences. The computational complexity of MSE for multichannel EEG recordings is qubic: <italic>ϑ</italic>(<italic>dNn</italic> + <italic>dn</italic>).</p>
</sec>
<sec id="sec010">
<title>2.2.2 Rényi’s entropy</title>
<p>Consider a discrete random variable <italic>x</italic> that adopts <italic>n</italic> values with probabilities <italic>p</italic><sub>1</sub>, <italic>p</italic><sub>2</sub>, …, <italic>p</italic><sub><italic>n</italic>.</sub> When the <italic>k</italic>-th value delivers <italic>I</italic><sub><italic>k</italic></sub> bits of information, then the total amount of information becomes the so-called Shannon’s entropy:
<disp-formula id="pone.0186916.e006">
<alternatives>
<graphic id="pone.0186916.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0186916.e006" xlink:type="simple"/>
<mml:math display="block" id="M6">
<mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mspace width="4pt"/><mml:mo>=</mml:mo><mml:mspace width="4pt"/><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula></p>
<p>In (<xref ref-type="disp-formula" rid="pone.0186916.e006">1</xref>), a linear averaging operator is assumed however, in general, for any monotonic function <italic>g(x)</italic> with an inverse <italic>g</italic><sup>-1</sup>(<italic>x</italic>), the general mean associated with <italic>g</italic>(<italic>x</italic>) for a set of real values {<italic>x</italic><sub><italic>k</italic></sub>, <italic>k</italic> = 1, …, <italic>n</italic>} with probabilities {<italic>p</italic><sub><italic>k</italic></sub>, <italic>k</italic> = 1, …, <italic>n</italic>} can be written as:
<disp-formula id="pone.0186916.e007">
<alternatives>
<graphic id="pone.0186916.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0186916.e007" xlink:type="simple"/>
<mml:math display="block" id="M7">
<mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mspace width="4pt"/><mml:mo>=</mml:mo><mml:mspace width="4pt"/><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula></p>
<p>Hence, using the general mean, the total amount of information will be:
<disp-formula id="pone.0186916.e008">
<alternatives>
<graphic id="pone.0186916.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0186916.e008" xlink:type="simple"/>
<mml:math display="block" id="M8">
<mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
with <italic>g</italic>(<italic>x</italic>) a Kolmogorov-Nagumo invertible function. Rényi proved that, when the criterion of additivity of independent events applies to (<xref ref-type="disp-formula" rid="pone.0186916.e008">2</xref>), then the range of usable functions <italic>g</italic>(<italic>x</italic>) is dramatically restricted. There are two possible classes:</p>
<list list-type="bullet">
<list-item><p><italic>g</italic>(<italic>x</italic>) = <italic>c</italic>, where <italic>c</italic> is a constant, so the general mean becomes linear and Shannon entropy (<xref ref-type="disp-formula" rid="pone.0186916.e006">1</xref>) is obtained;</p></list-item>
<list-item><p><italic>g</italic>(<italic>x</italic>) = <italic>c</italic>2<sup>(1−<italic>α</italic>)</sup><sup><italic>x</italic></sup>, which implies that the entropy definition becomes:
<disp-formula id="pone.0186916.e009">
<alternatives>
<graphic id="pone.0186916.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0186916.e009" xlink:type="simple"/>
<mml:math display="block" id="M9">
<mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mtext>log</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msubsup><mml:mi>p</mml:mi><mml:mi>k</mml:mi><mml:mi>α</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mi>α</mml:mi><mml:mo>≠</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="4pt"/><mml:mspace width="4pt"/><mml:mi>α</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
which is called Rényi’s information measure of order <italic>α</italic>.</p></list-item>
</list>
<p>We are particularly interested in the case where <italic>α</italic> = 2, further called Rényi’s quadratic entropy (RQE), as the term between brackets in (<xref ref-type="disp-formula" rid="pone.0186916.e009">3</xref>) corresponds to the expected value of the probability density function (PDF). We wish to estimate RQE directly from the sampled signal using a Gaussian kernel for Parzen’s density estimate:
<disp-formula id="pone.0186916.e010">
<alternatives>
<graphic id="pone.0186916.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0186916.e010" xlink:type="simple"/>
<mml:math display="block" id="M10">
<mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mspace width="4pt"/><mml:mo>=</mml:mo><mml:mspace width="4pt"/><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>k</mml:mi><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
with <italic>N</italic> the number of samples and <italic>σ</italic> the kernel size or bandwith parameter. Hence, we obtain:
<disp-formula id="pone.0186916.e011">
<alternatives>
<graphic id="pone.0186916.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0186916.e011" xlink:type="simple"/>
<mml:math display="block" id="M11">
<mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
<disp-formula id="pone.0186916.e012">
<alternatives>
<graphic id="pone.0186916.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0186916.e012" xlink:type="simple"/>
<mml:math display="block" id="M12">
<mml:msub><mml:mrow><mml:mi>I</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mspace width="4pt"/><mml:mo>=</mml:mo><mml:mspace width="4pt"/><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mspace width="4pt"/><mml:mo>=</mml:mo><mml:mspace width="4pt"/><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>k</mml:mi><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
where IP is the information potential[<xref ref-type="bibr" rid="pone.0186916.ref054">54</xref>]. Note that, for the multivariate case, the average of the univariate kernels is taken. For ease of reference, we will call it multivariate RQE (MRQE). The bandwidth <italic>σ</italic> is a free parameter that can be chosen according to Silverman’s rule:
<disp-formula id="pone.0186916.e013">
<alternatives>
<graphic id="pone.0186916.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0186916.e013" xlink:type="simple"/>
<mml:math display="block" id="M13">
<mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo>
</mml:math>
</alternatives>
</disp-formula>
where <italic>n</italic> is the number of data points in a snippet, <italic>d</italic> the data dimensionality (number of electrodes), and <italic>σ</italic><sub><italic>X</italic></sub> the standard deviation.</p>
<p>The computational complexity of MRQE is quadratic: <italic>ϑ</italic>(<italic>dn</italic> + <italic>n</italic>).</p>
</sec>
<sec id="sec011">
<title>2.2.3 Kernel-based Shannon entropy</title>
<p>As a third entropy-based method, we consider Shannon entropy (ShE):
<disp-formula id="pone.0186916.e014">
<alternatives>
<graphic id="pone.0186916.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0186916.e014" xlink:type="simple"/>
<mml:math display="block" id="M14">
<mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mtext>log</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
where <italic>p</italic><sub><italic>k</italic></sub> is a probability density function (PDF). As in the case of RQE, we estimate the PDF directly from the samples using a Gaussian kernel for Parzen’s density estimate:
<disp-formula id="pone.0186916.e015">
<alternatives>
<graphic id="pone.0186916.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0186916.e015" xlink:type="simple"/>
<mml:math display="block" id="M15">
<mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mspace width="4pt"/><mml:mo>=</mml:mo><mml:mspace width="4pt"/><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>k</mml:mi><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
</disp-formula>
where <italic>n</italic>is the number of data points in a snippet and <italic>σ</italic> the kernel size or bandwidth parameter. Similarly to RQE, we develop the multivariate case with the Shannon entropy (MShE). The computational complexity of MShE is quadratic as well:<italic>ϑ</italic>(<italic>dn</italic> + <italic>n</italic>).</p>
</sec>
<sec id="sec012">
<title>2.2.4 Multivariate Empirical Mode Decomposition (MEMD)</title>
<p>With Empirical Mode Decomposition (EMD) a signal of length <italic>M</italic> data points is split into a <italic>l</italic> = <italic>log</italic><sub>2</sub><italic>M</italic> narrow-band, amplitude/frequency modulated components called Intrinsic Mode Functions (IMFs) [<xref ref-type="bibr" rid="pone.0186916.ref055">55</xref>]:
<disp-formula id="pone.0186916.e016">
<alternatives>
<graphic id="pone.0186916.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0186916.e016" xlink:type="simple"/>
<mml:math display="block" id="M16">
<mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi><mml:mi>M</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mrow><mml:mi>I</mml:mi><mml:mi>M</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mspace width="4pt"/><mml:mo>…</mml:mo><mml:mo>+</mml:mo><mml:mspace width="4pt"/><mml:msub><mml:mrow><mml:mi>I</mml:mi><mml:mi>M</mml:mi><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo>
</mml:math>
</alternatives>
</disp-formula></p>
<p>IMF<sub>1</sub> corresponds to the highest frequency component and the subsequent <italic>IMFs</italic> to lower, more narrow-banded frequency components. The last component, <italic>IMF<sub>n</sub></italic>, is the trend in the signal and is usually omitted from further analysis. The multivariate extension of EMD (MEMD) [<xref ref-type="bibr" rid="pone.0186916.ref031">31</xref>] aligns similar frequency bands of multiple channels, thus, providing an assessment of their possible interdependence (mode alignment property).</p>
</sec>
<sec id="sec013">
<title>2.2.5 MEMD-enhanced multiscale, multivariate entropy</title>
<p>There are at least two ways to compute a multiscale version of entropy when using MEMD: we can compute multivariate entropy (MSE, MRQE or MShE) for each scale individually (IMFs) or for their accumulated scales (cumulative IMFs, CIMFs). We explain the algorithm for the CIMF case:</p>
<list list-type="order">
<list-item><p>obtain the IMFs for each subject’s entire recording length of each video clip (<italic>M</italic> data points, with <italic>M</italic> in our case between 30000 and 52000, depending on video clip length) by applying the MEMD method to a given number <italic>d</italic> of EEG electrodes;</p></list-item>
<list-item><p>accumulate the IMFs one by one, starting with the first one, CIMF<sub>1</sub> = IMF<sub>1</sub>, and then add the second IMF to the first, CIMF<sub>2</sub> = IMF<sub>1</sub> + IMF<sub>2</sub>, and so on, until all IMFs are added;</p></list-item>
<list-item><p>for each CIMF, calculate multivariate entropy (<italic>d</italic> electrodes) for each non-overlapping snippet of a prior defined EEG signal track and take the average over all those <italic>N</italic> snippets (MSE) or calculate the univariate entropy per electrode for those snippets and take the average over all <italic>d</italic> electrodes (MRQE, MShE);</p></list-item>
<list-item><p>plot the entropy estimates as a function of CIMF<sub>1</sub>,CIMF<sub>2</sub>, … to obtain the MEMD-enhanced multiscale, multivariate entropy curve of the targeted EEG signal track (i.e., MEMD-enhanced MMSE, MMRQE, MMShE)</p></list-item>
</list>
<p>Note that the total number of IMFs is log<sub>2</sub> <italic>M</italic> [<xref ref-type="bibr" rid="pone.0186916.ref055">55</xref>] but the used algorithm selects by itself the number of IMFs in a data-driven, subject-dependent way (in our case, between 15 and 17 IMFs), hence, for clarity’s sake, we decided to show 15 IMFs for all subjects. Both operations imply that the original signal is only approximated by CIMF<sub>n</sub>, so their entropies could be different.</p>
<p>For the interested reader, the Matlab code for the multiscale MSE algorithm can we found at <ext-link ext-link-type="uri" xlink:href="http://www.commsp.ee.ic.ac.uk/~mandic/research/Complexity_Stuff.htm" xlink:type="simple">http://www.commsp.ee.ic.ac.uk/~mandic/research/Complexity_Stuff.htm</ext-link>, and the Matlab code for the multivariate EMD at <ext-link ext-link-type="uri" xlink:href="http://www.commsp.ee.ic.ac.uk/~mandic/research/emd.htm" xlink:type="simple">http://www.commsp.ee.ic.ac.uk/~mandic/research/emd.htm</ext-link></p>
</sec>
<sec id="sec014">
<title>2.2.6 Scalp space projection of intracranial recordings</title>
<p>In order to explore the relation between scalp- and intracranial recordings, in response to emotion-evoking and neutral video clips, we performed a source reconstruction analysis with the Brainstorm toolbox as it can work directly with intracranial electrodes. We started with the patient’s post-implantation MRI headscan to define the standard Montreal Neurological Institute (MNI) stereotactic coordinates of the depth electrode’s contact points in the amygdala-hippocampal complex and the electrode positions of the grid covering the occipital cortex. Then, we used the patient’s MRI scan before implantation to extract the cortex envelopes from the MRI scan (inner and outer skull, scalp surface and cortex) using the Brainsuite software[<xref ref-type="bibr" rid="pone.0186916.ref056">56</xref>]. Fiducial points were selected manually. For the forward model, Boundary Element Model (BEM) surfaces were created using 15000 dipoles on the entire MRI volume so as to include deep sources, using the OpenMEEG BEM model [<xref ref-type="bibr" rid="pone.0186916.ref057">57</xref>]. For the inverse modeling, as there was no resting state data available, the identity matrix was selected for the noise covariance, and the sLORETA algorithm used for distributed source modeling [<xref ref-type="bibr" rid="pone.0186916.ref058">58</xref>].</p>
<p>The depth electrode had 10 contact points but, based on anatomical grounds, the first 2 contact points were considered to be in the amygdala. However, as their recordings did not display a linear relation (no obvious Pearson correlation, see <xref ref-type="supplementary-material" rid="pone.0186916.s004">S4 Appendix</xref>), but instead the recordings of the second contact point seemed to correlate with those of contact points 3 to 6, we only used electrode 1 to simulate data from that contact point to see how its projects back on scalp space. In order to obtain a sizeable estimate of amygdala activation, we assumed that 1 cm<sup>3</sup> of amygdala tissue, in proportion to the entire head volume, corresponds to about 27 amygdala dipoles out of 15000 in total. We put all values of the headmodel to zero, except for the said 27 dipoles, which we filled with contact point 1’s recordings. As the headmodel for the entire MRI volume is not constrained in orientation and orientation, but needs to be defined for our simulations, we chose four possible orientations for simulating our deep source—the X-, Y-, and Z-directions, and also the equally mixed version, with an equal weight in each direction—, and projected the result on the entire scalp.</p>
<p>Next, we wanted examine whether the sources that generated the intracranial recordings of the electrode grid covering the occipital lobe also contribute to the frontal scalp recordings. For this, we first solved the inverse problem, thus starting from the 20 electrode grid recordings, and then addressed the forward problem to see how these sources show up in the frontal part of the scalp.</p>
</sec>
</sec>
</sec>
<sec id="sec015" sec-type="results">
<title>3. Results</title>
<sec id="sec016">
<title>3.1 Discriminability of emotional categories</title>
<p>MEMD-enhanced multiscale, multivariate entropy estimates of the preprocessed scalp EEG recordings were computed only over the last 100s of each video clip, as the video clips were construed by Schaefer and co-workers to have their climax at the very end (see also section 3.5). For the sake of comparison, in the same way as in, we considered electrodes F3 and F4 (mid-frontal area) (<italic>d</italic> = 2) and also took <italic>N</italic> = 1000 non-overlapping snippets of length 100 ms (<italic>m</italic> = 12), and averaged the entropy estimates across snippets of each participant. The MEMD-enhanced MMSE, -MMRQE and -MMShE curves for all 30 participants, grouped by their self-reported emotional categories (see <xref ref-type="table" rid="pone.0186916.t001">Table 1</xref>), are shown in <xref ref-type="fig" rid="pone.0186916.g001">Fig 1</xref>. We observe that the complexity curves of the three entropies are similar with similar discriminabilities between emotional states. We also observe that the curve labeled ‘sadness’ has the lowest entropy values—note it also has the lowest affect score of all video clips used—, thus, confirming Aftanas and co-workers’ assertion [<xref ref-type="bibr" rid="pone.0186916.ref021">21</xref>]. However, when assessing the effect of snippet length, we observe that the MEMD-enhanced MMRQE curves are less sensitive to the choice of snippet length compared to those of the other 2 approaches (see <xref ref-type="supplementary-material" rid="pone.0186916.s001">S1 Appendix</xref>). We also calculated the entropies of the original (uniscale) EEG. We observe that the uniscale case exhibits a much lower discriminability (factually only 2 groups of plots remain), clearly showing the advantage of the multiscale approach. We also calculated the MEMD-enhanced MMRQE curves for the 2 non-French speaking participants (<xref ref-type="supplementary-material" rid="pone.0186916.s006">S6 Appendix</xref>.). The curves are now factually indistinguishable except for the ‘Life is beautiful’ video clip (self-labeled as tenderness). Hence, the physical parameters of the video clips are not explaining the discriminability of the MMRQE curves (see also the low interrater agreement of the occipital and auditory cortex responses in <xref ref-type="fig" rid="pone.0186916.g002">Fig 2</xref>, left panel, discussed further).</p>
<fig id="pone.0186916.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0186916.g001</object-id>
<label>Fig 1</label>
<caption>
<title>MEMD-enhanced MMSE, MMRQE and MMShE curves, main experiment.</title>
<p>Curves are labeled in terms of our 30 participants’ self-reports (self-labels) and corresponding uniscale entropy values for the original EEG recordings (cf., horizontal axis tick labeled ‘raw’). Error bars are standard errors of average MMSE, MMRQE and MMShE values per subject. Results shown for 100 ms snippet length. Color convention: purple = anger, red = amusement, green = disgust, black = tenderness, blue = sadness.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0186916.g001" xlink:type="simple"/>
</fig>
<fig id="pone.0186916.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0186916.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Scalp distribution of Krippendorf's alpha coefficient for the main and control group.</title>
<p>The coefficient shows the degree to which the participants’ RQEs for CIMF<sub>6</sub> are in agreement.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0186916.g002" xlink:type="simple"/>
</fig>
<p>We also computed the MEMD-enhanced MMRQE curves of the 6 participants to the control experiment (see <xref ref-type="supplementary-material" rid="pone.0186916.s007">S7 Appendix</xref>). Note that the 4 video clips were not taken from the top ten list of affect scores (<xref ref-type="table" rid="pone.0186916.t001">Table 1</xref>). The individual curves are much less discriminable and their ranking—in terms of self-labels—is now subject-dependent.</p>
<p>As the peak frequency of IMF<sub><italic>i</italic></sub> is given by <inline-formula id="pone.0186916.e017"><alternatives><graphic id="pone.0186916.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0186916.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:mfrac><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mspace width="4pt"/><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula> [<xref ref-type="bibr" rid="pone.0186916.ref055">55</xref>], given the <italic>sampling rate</italic> of 128 Hz, the frequency range spanned by the first 6 IMFs is from 1 Hz to 32 Hz and this corresponds to the bandwidth of our Butterworth filter implemented in the preprocessing (0.5–30 Hz). This also explains the asymptotic behavior of our entropy curve results, whence, we will further focus on CIMF<sub>6</sub>. When applying a linear mixed model [<xref ref-type="bibr" rid="pone.0186916.ref059">59</xref>] to the CIMF<sub>6</sub> results of MMRQE, with self-label, gender and age as fixed effects, and subject as random effect, we found that self-label is significant (&lt; 0.0001), whereas age (p = 0.083) and gender are not (p = 0.68).</p>
<p>For comparison’s sake, we also considered the case where entropy is plotted as a function of IMF index, thus, by considering separate scales. The result is shown in <xref ref-type="supplementary-material" rid="pone.0186916.s002">S2 Appendix</xref>.: we observe a much lower discriminability between emotional states compared to the cumulative case (CIMF).</p>
<p>Finally, as the computational complexity of MMRQE is quadratic and that of MMSE cubic the former is also advantageous for computational efficiency reasons.</p>
</sec>
<sec id="sec017">
<title>3.2 Discriminability of multiscale RQE per electrode</title>
<p>We statistically assessed to what extent the RQEs per self-reported emotional category are in agreement across subjects (inter-rater reliability, inter-rater agreement). We used the so-called interval version of Krippendorf's alpha statistic (using the <italic>kriAlpha</italic> function in Matlab) for our 30 participants (main experiment), given their 5 self-reported emotional categories (self-labels), and determined the scalp distribution (per electrode, thus, univariate) of the alpha statistic of the RQEs and restricted ourselves to CIMF<sub>6</sub>, to simplify the comparison. The results are shown in <xref ref-type="fig" rid="pone.0186916.g002">Fig 2</xref> (left panel) when using 100ms snippets on the last 100s of each video, and when computing the EMD for all 32 electrodes jointly (MEMD) and retaining CIMF<sub>6</sub>. We observe that the frontal region of the scalp has the highest alpha coefficient with much smaller values for the occipital and auditory cortices. For comparison’s sake, we also computed the scalp distribution of the alpha coefficient for the original (uniscale) EEG data (see <xref ref-type="supplementary-material" rid="pone.0186916.s003">S3 Appendix</xref>: the coefficients now turn out to be negative, implying a disconcordance in the RQE values between subjects). In order to assess the statistical significance of our results, we compared the alpha coefficient of each electrode of the main group (30 subjects) with that of the corresponding electrode of the control group (6 subjects). Hereto, we determined a distribution of the alpha coefficient of the control group using a simple block bootstrap strategy (Matlab’s <italic>bootstrp</italic> function) in which the alpha coefficient is computed for 1000 subsets of 5 subjects (random sampling with replacement). The result for the control group is shown in <xref ref-type="fig" rid="pone.0186916.g002">Fig 2</xref> (right panel). The scalp plot of the <italic>p</italic> values resulting from a simple t-test is shown in <xref ref-type="fig" rid="pone.0186916.g003">Fig 3</xref>. One observes that the most discriminative electrodes are F3, FC1, FC5, T7, CP5, CP2, C4, T8, FC6, F4, F8 (using a 5% significance level).</p>
<fig id="pone.0186916.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0186916.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Scalp distribution of <italic>p</italic> values of the difference in alpha coefficient between main and control groups.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0186916.g003" xlink:type="simple"/>
</fig>
<p>In order to identify which subset of electrodes best discriminate between the 5 self-reported emotional categories, we conducted a ‘greedy’ search on the alpha coefficients of the main group (30 participants) computed from the last 100 seconds of each movie. We started the search from F3, the most significant electrode in <xref ref-type="fig" rid="pone.0186916.g002">Fig 2</xref> (left panel), computed CIMF<sub>6</sub> for this electrode only (to avoid effects from other electrodes), and plotted its alpha coefficient based on the main group’s RQE. Then, we added the next most significant channel, F4, compute CIMF<sub>6</sub> for F3-F4, and plotted the alpha coefficient based on multivariate RQE (channels F3-F4, thus d = 2), and so on (<xref ref-type="fig" rid="pone.0186916.g004">Fig 4</xref>). One observes that F3-F4-Fz show best discriminability. This confirms that, at least for our case, emotion discrimination can be achieved using only 3 mid-frontal channels, which also reduces the computational cost for the MEMD-enhanced MMRQE algorithm (remember that the algorithm’s complexity scales with number of channels).</p>
<fig id="pone.0186916.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0186916.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Clustering electrodes based on alpha statistics (main group).</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0186916.g004" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec018">
<title>3.3 Temporal evolution of multiscale RQE discriminability</title>
<p>We also considered the temporal evolution of Krippendorf’s alpha coefficient of the main group but now using the entire length of each video clip. We used 100ms snippets, the 3 mid-frontal (F3, F4, Fz) electrodes selected by our clustering analysis for MEMD calculation (<xref ref-type="fig" rid="pone.0186916.g004">Fig 4</xref>), and compute the multivariate RQE (MRQE, <italic>d</italic> = 3) for CIMF<sub>6</sub>. We observe from <xref ref-type="fig" rid="pone.0186916.g005">Fig 5</xref> that the alpha coefficient, and whence the discriminability across participants, increases and remains relatively stable towards the end of the video clip. This confirms our motivation to focus on the last 100s of the EEG recordings.</p>
<fig id="pone.0186916.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0186916.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Evolution of Krippendorf’s alpha statistic of MRQE for CIMF<sub>6</sub> over entire video clips, plotted for mid-frontal electrodes.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0186916.g005" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec019">
<title>3.4 Multiscale RQE of intracranial EEG</title>
<p>We also computed the MEMD-enhanced MMRQE for the intracranial EEG recordings in the amygdala (2 contact points) and over the occipital area (20 contact points) when the patient viewed an emotional- (fragment from ‘Schindler’s list’, self-labeled by the patient as ‘anger’) and a neutral (weather forecast, self-labeled as ‘neutral’) video clip (<xref ref-type="fig" rid="pone.0186916.g006">Fig 6</xref>). As in the scalp EEG case, we selected the last 100s track of each video clip, determined the MEMD for the 2 amygdala contact points (depth electrode) and separately for the 20 occipital contact points (subdural electrode grid), and finally plotted the multiscale, multivariate RQE (MMRQE) using non-overlapping 100ms snippets. We observe that the MMRQE curves for the amygdala are well separable compared to those of the occipital area (<xref ref-type="fig" rid="pone.0186916.g006">Fig 6</xref>, left vs. right panel).</p>
<fig id="pone.0186916.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0186916.g006</object-id>
<label>Fig 6</label>
<caption>
<title>MEMD-enhanced MMRQE curves for intracranial EEG recordings.</title>
<p>Shown are the results for the (a) amygdala- and (b) occipital implants in a patient viewing emotional and neutral video clips (blue vs. red labeled curves). Error bars correspond to standard errors of individual snippet MMRQEs.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0186916.g006" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec020">
<title>3.5 Temporal evolution of multiscale RQE for joint scalp/intracranial EEG</title>
<p>Finally, we computed MRQE for CIMF<sub>6</sub> using the joint scalp (mid-frontal) EEG and intracranial EEG (amygdala) recorded when viewing both video clips (emotional and neutral). Note that the MEMD is computed with reference to the listed electrodes in each case. The results are shown in <xref ref-type="fig" rid="pone.0186916.g007">Fig 7</xref>. We observe that, for the ‘anger’ video, the MRQE curves of the intracranial EEG (amygdala) and mid-frontal scalp EEG seem to converge and even overlap at the end of the videos (<xref ref-type="fig" rid="pone.0186916.g007">Fig 7</xref>, left panel). In order to statistically assess this observation, we modeled the temporal evolution of both MRQEs for 8 successive, non-overlapping time intervals where each interval consists of 20 successive recording samples (interval between 2 successive samples is 1s). Then, for each recording, we adopted a linear mixed model approach [<xref ref-type="bibr" rid="pone.0186916.ref059">59</xref>,<xref ref-type="bibr" rid="pone.0186916.ref060">60</xref>] with MRQE as continuous outcome and time intervals as fixed effects:
<disp-formula id="pone.0186916.e018">
<alternatives>
<graphic id="pone.0186916.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0186916.e018" xlink:type="simple"/>
<mml:math display="block" id="M18">
<mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>R</mml:mi><mml:mi>Q</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mi mathvariant="normal">*</mml:mi><mml:msub><mml:mrow><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mi mathvariant="normal">*</mml:mi><mml:msub><mml:mrow><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub>
</mml:math>
</alternatives>
</disp-formula>
where subscript <italic>i</italic> refers to the modeled recording, <italic>α</italic> the overall intercept, <italic>β</italic><sub>1</sub> … <italic>β</italic><sub>7</sub> the overall time interval-specific slopes, the latent variables <inline-formula id="pone.0186916.e019"><alternatives><graphic id="pone.0186916.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0186916.e019" xlink:type="simple"/><mml:math display="inline" id="M19"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>,</mml:mo><mml:mo>(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mi mathvariant="bold-italic"> </mml:mi></mml:math></alternatives></inline-formula> random intercept and -slope, and <italic>ε</italic><sub><italic>ij</italic></sub>~<italic>N</italic>(0, <italic>σ</italic><sup>2</sup>) random noise. To correct for possible EEG recording-specific differences, random intercept and random slope were used. Finally, we tested the hypothesis that the MRQEs of both signals (scalp and intracranial) at a given time interval were from the same distribution. For all time intervals, expect the two last ones (40 ms in total), the hypothesis is rejected, hence the MRQE curves indeed become indistinguishable at the end of the ‘anger’ video. Note that this also corresponds to the time range producing best discriminability (alpha coefficient) of the main group’s scalp EEG recordings (section 3.3), where the crux of the video unfolds. On the contrary, no such overlap is observed for the ‘neutral’ video (<xref ref-type="fig" rid="pone.0186916.g007">Fig 7</xref>, right panel). All analyses were performed using SAS, release 9.4.</p>
<fig id="pone.0186916.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0186916.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Temporal evolution of multivariate RQE.</title>
<p>Shown are the results for CIMF<sub>6</sub> for amygdala iEEG and mid-frontal scalp EEG (F3, F4, Fz electrodes) in response to emotional and neutral video clips.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0186916.g007" xlink:type="simple"/>
</fig>
<p>In order to gain insight into the origin of the overlap between the two MRQE curves, we analyzed phase synchrony between the mid-frontal scalp EEG and intracranial amygdala signals, using the phase locking statistics (PLS) technique introduced by Lachaux and co-workers [<xref ref-type="bibr" rid="pone.0186916.ref061">61</xref>]. The results are shown in <xref ref-type="fig" rid="pone.0186916.g008">Fig 8</xref> (left panel). After a transitional period, at the onset of both videos, phase synchrony of the emotional video (‘anger’) increases sharply towards the end, which is not the case for the neutral video as phase synchrony stays constant. As these results are from only one subject, there is not enough statistical power to decide whether the difference between the two curves and between the low and high synchrony values of the emotional video are significant. We therefore applied the surrogate data method called Iterative Amplitude Adjusted Fourier Transformed (IAAFT), originally proposed by Schreiber and Schmitz [<xref ref-type="bibr" rid="pone.0186916.ref062">62</xref>] [<xref ref-type="bibr" rid="pone.0186916.ref063">63</xref>], to the amygdala and mid-frontal recordings of each video. The IAAFT algorithm has ability to generate surrogate signals with identical amplitude distributions and approximately identical amplitude spectra while the cross-correlation between the original signals is destroyed. We used Matlab’s Chaotic Systems Toolbox (<ext-link ext-link-type="uri" xlink:href="https://nl.mathworks.com/matlabcentral/fileexchange/1597-chaotic-systems-toolbox/content/IAAFT.m" xlink:type="simple">https://nl.mathworks.com/matlabcentral/fileexchange/1597-chaotic-systems-toolbox/content/IAAFT.m</ext-link>) with c = 10, maxiter = 1000, frequency range 0.05–30 Hz, and trial length 200ms.</p>
<fig id="pone.0186916.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0186916.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Phase synchrony between mid-frontal EEG and amygdala.</title>
<p>Shown are the results for the (a) emotional (‘anger’) and neutral video clips (left) and for the corresponding surrogate signal distributions (right) (red = neural video, blue = emotional video).</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0186916.g008" xlink:type="simple"/>
</fig>
<p>The results are illustrated in <xref ref-type="fig" rid="pone.0186916.g008">Fig 8</xref> (right panel). It can be seen that the increasing phase synchrony of the emotional video is significantly different from its surrogate distribution whereas the phase synchrony of the neutral video is indistinguishable from its surrogate distribution. The high value of the phase synchrony at the end of the emotional video supports the similarity in MRQE values between the scalp and intracranial recordings (<xref ref-type="fig" rid="pone.0186916.g007">Fig 7</xref>): this means that these recordings exhibit strong phase synchrony and share similar degrees of complexity near the end of the emotional video.</p>
<p>Finally, using Brainstorm, we simulated for each of the 4 different amygdala source orientations (see <xref ref-type="sec" rid="sec002">Methods</xref> section 2.2.6) the EEG signal (128 channel full scalp configuration) in response to the available amygdala recordings of both video clips. As above, we computed phase synchrony between simulated and actually recorded mid-frontal (F3,Fz,F4) EEG and again applied the IAAFT algorithm to generate surrogates. The results are shown in <xref ref-type="supplementary-material" rid="pone.0186916.s008">S8 Appendix</xref> for the case where the amygdala dipole orientations are equally weighted in X-, Y-, and Z-direction (“mixed” case, see <xref ref-type="sec" rid="sec002">Methods</xref> section 2.2.6); the results for the X-, Y-, and Z-oriented dipoles are comparable. We observe that phase synchrony behaves quite similarly to <xref ref-type="fig" rid="pone.0186916.g008">Fig 8</xref>. The phase synchrony result in <xref ref-type="supplementary-material" rid="pone.0186916.s008">S8 Appendix</xref> supports our hypothesis that, at least for the last part of the emotional video, amygdala could indeed be contributing to the synchrony observed in our mid-frontal scalp recordings, but we hasten to add that other sources (which we did not model here) could be contributing as well. When repeating the exercise for the sources that evoked the occipital electrode grid response and also simulate their EEG activity on the scalp, we observed no significant phase synchrony levels for the mid-frontal electrodes neither any difference between the neutral and emotional videos. This result is also in line with our complexity results (<xref ref-type="fig" rid="pone.0186916.g008">Fig 8</xref>).</p>
</sec>
</sec>
<sec id="sec021" sec-type="conclusions">
<title>4. Discussion</title>
<p>Recently, several algorithms were proposed linking EEG signal complexity (in particular entropy) to distinct emotional states. Jie and co-workers [<xref ref-type="bibr" rid="pone.0186916.ref023">23</xref>] performed emotion recognition from univariate, uniscale sample entropy (SE) of prefrontal EEG recordings. However, the EEG recordings of subjects whose emotion self-reports were not in line with the pre-defined ones were excluded from their analysis. In our previous work [<xref ref-type="bibr" rid="pone.0186916.ref034">34</xref>], we used MEMD-enhanced multiscale, multivariate sample entropy (MMSE) to discriminate multiple emotional states from EEG recordings in response to emotion-evoking video clips. Contrary to Jie and co-workers, we used self-reports of emotional category for labeling the signal complexity results. Furthermore, unlike previous studies [<xref ref-type="bibr" rid="pone.0186916.ref064">64</xref>], the emotional state was discriminated even from a single video clip (single-trial design). This strategy was also adopted in the pertinent article but in the context of MEMD-enhanced MMRQE, a multiscale, multivariate version of Rényi’s quadratic entropy (RQE). Albeit the results were similar, the Rényi-based approach revealed two advantages. Firstly, MMRQE’s computational complexity is quadratic whereas MMSE’s [<xref ref-type="bibr" rid="pone.0186916.ref034">34</xref>] cubic, which rendered the Rényi-based approach computationally much more feasible for joint electrode analyses and electrode selection strategies. Secondly, the MMSE-based method is much more sensitive to snippet length as the discriminability of emotional categories quickly degrades with snippet length. For MMRQE we observed a much broader range of usable snippet lengths.</p>
<p>It has been claimed that changes in emotional state (also in relation to dysfunctions) can be discerned from frontal lobe recordings [<xref ref-type="bibr" rid="pone.0186916.ref065">65</xref>,<xref ref-type="bibr" rid="pone.0186916.ref066">66</xref>]. In [<xref ref-type="bibr" rid="pone.0186916.ref067">67</xref>] it was shown that the orbital frontal cortex plays a critical role in cognitive control of emotion (especially in the case of suppressing emotional responses), and activity in this region reflects subsequent appraisal processes related to viewing emotional stimuli (see also [<xref ref-type="bibr" rid="pone.0186916.ref038">38</xref>]). Jie et al. [<xref ref-type="bibr" rid="pone.0186916.ref023">23</xref>] achieved emotion recognition from prefrontal EEG recordings and Tonoyan et al.[<xref ref-type="bibr" rid="pone.0186916.ref034">34</xref>] from mid-frontal recordings. By using Krippendorff’s inter-rater reliability analysis in combination with a greedy search for optimal electrodes, we showed in the pertinent article that the mid-frontal electrodes F3, Fz, F4 scored best in discriminating emotional states (<xref ref-type="fig" rid="pone.0186916.g004">Fig 4</xref>) but this does not preclude the contribution of central and temporal electrodes, as reported in other studies [<xref ref-type="bibr" rid="pone.0186916.ref037">37</xref>,<xref ref-type="bibr" rid="pone.0186916.ref044">44</xref>,<xref ref-type="bibr" rid="pone.0186916.ref068">68</xref>]. Interestingly, EEG signals over the occipital and auditory cortices exhibited a much lower discriminability which implies that our results are probably not explainable by sensory or auditory activations elicited when viewing video clips.</p>
<p>The question arises whether EEG signal complexity could be used to ‘label’ arbitrary video clips in terms of emotional states. There are several observations that demote our expectations. Firstly, there is the concern on the labeling side: the same video clip could receive different emotional category reports from different viewers as we observed for the ‘Sleepers’ movie (<xref ref-type="table" rid="pone.0186916.t001">Table 1</xref>). Another concern, maybe the most important one, is that signal complexity is not static but rather evolves when watching the video clip and this affected the discriminability of our signal complexity results in terms of self-reported emotional categories (see <xref ref-type="fig" rid="pone.0186916.g005">Fig 5</xref>). However, we also observed that discriminability increased towards the end of the video clips. This is in alignment with the way the video clips were constructed by Schaefer and co-workers: the clips have their climax at the end (roughly the last 100s). We therefore recommend to restrict EEG complexity analysis to the most affective scenes as for those the highest inter-rater agreement can be expected.</p>
<p>How to interpret signal complexity then? One suggestion is to relate it to affect score: the affect scores of the video clips we used are available in [<xref ref-type="bibr" rid="pone.0186916.ref035">35</xref>] and are averages of self-reported affect scores based on 10 positive and 10 negative items (each on a 5 point scale). The video clip ‘City of angels’ showed both a lower affect score (<xref ref-type="table" rid="pone.0186916.t001">Table 1</xref>) and a lower MMRQE curve (<xref ref-type="fig" rid="pone.0186916.g001">Fig 1</xref>) than the ‘La cité de la peur’ video clip, which is in alignment with the assertion of Aftanas and co-workers [<xref ref-type="bibr" rid="pone.0186916.ref021">21</xref>] that higher complexity values can be observed for more affective stimuli. However, we could not observe such a relationship for the other video clips. This could be due to the ways the affect scores of [<xref ref-type="bibr" rid="pone.0186916.ref035">35</xref>] (20 item check list per clip) and our emotional self-scores were collected (one emotional category per clip), and whether differences in reported affect scores are significant. (Note that no standard deviations are provided in [<xref ref-type="bibr" rid="pone.0186916.ref035">35</xref>].) Video clips with low affect score seem to be more difficult to discriminate based on signal complexity, as we showed for the control experiment <xref ref-type="supplementary-material" rid="pone.0186916.s007">S7 Appendix</xref>): albeit the range of the RQE values was similar to that of the main experiment, the MMRQE curves were much less discriminable and their ranking varied across participants. By using video clips with the highest affect scores (cf., 3 of the 4 video clips of the main experiment), or by contrasting highest and lowest affect score video clips (cf., ‘City of angels’ video clip), we could achieve discriminability based on signal complexity (<xref ref-type="fig" rid="pone.0186916.g001">Fig 1</xref>). Affect score, thus, seems to play a role in signal complexity but not completely. Another suggestion is to relate signal complexity to emotional arousal as these self-reports are also available in [<xref ref-type="bibr" rid="pone.0186916.ref035">35</xref>]. As shown in our earlier work, and it also pertains to our MMRQE results, the relation between signal complexity and emotional arousal was not significant (using a linear mixed model regression analysis) neither for the whole video clip (since in [<xref ref-type="bibr" rid="pone.0186916.ref035">35</xref>] the self-reports were collected in this way) nor for the last 100 s (‘climax’).</p>
<p>In order to shed light on the significance of scalp EEG signal complexity, we also examined intracranial EEGs from the amygdala recorded in a patient when viewing emotional and neutral video clips. The amygdala is considered to be strongly involved in emotional processing. Guillory and co-workers [<xref ref-type="bibr" rid="pone.0186916.ref069">69</xref>] reviewed 64 invasive studies on human emotion including amygdala [<xref ref-type="bibr" rid="pone.0186916.ref037">37</xref>–<xref ref-type="bibr" rid="pone.0186916.ref046">46</xref>]. Moreover, Sergerie and co-workers [<xref ref-type="bibr" rid="pone.0186916.ref047">47</xref>] showed that the amygdala responds to all visual emotional stimuli regardless of their valence and Aggleton and Mishkin even claimed it is the gateway to sensory emotion [<xref ref-type="bibr" rid="pone.0186916.ref048">48</xref>]. By applying the same MEMD-based MMRQE method to the intracranial recordings, we observed a clear discrimination between the MMRQE curves of the neutral and the emotional movies (labeled by the patient as evoking anger) for the amygdala but not for the occipital area (<xref ref-type="fig" rid="pone.0186916.g006">Fig 6</xref>). This result is in agreement with several studies that prove the involvement of amygdala in negative emotion processing. Oya et al. [<xref ref-type="bibr" rid="pone.0186916.ref040">40</xref>] proved that significant changes in gamma power amplitude in amygdala were selectively obtained in response to visual images judged to be aversive but not in response to those that were judged pleasant or neutral. Naccache and co-workers [<xref ref-type="bibr" rid="pone.0186916.ref042">42</xref>] obtained differential ERP responses in amygdala to negative emotional words. We observed that the multiscale RQE computed for the mid-frontal scalp electrodes and the amygdala electrodes overlapped and their phase synchrony reached a maximum at the end of the emotional video where the crux is revealed. On the contrary, no such overlap or phase synchrony was observed for the ‘neutral’ video. This was also observed for the phase synchrony between the original- and reconstructed mid-frontal scalp activations, reconstructed from our amygdala recordings using our head model. In summary, our findings seem to suggest that emotional state discrimination from scalp EEG is more likely to be supported by differences in amygdala activation when in response to emotionally-charged scenes.</p>
</sec>
<sec id="sec022" sec-type="conclusions">
<title>Conclusion</title>
<p>We introduced multiscale, multivariate Rényi quadratic entropy (MMRQE) for analyzing EEG recordings of 30 participants viewing 4 emotionally-charged video clips taken from a validated database [<xref ref-type="bibr" rid="pone.0186916.ref035">35</xref>]. We compared our approach to the multiscale, multivariate version of sample entropy (MMSE), and showed that the results were similar but with the advantage that MMRQE is less computationally intensive and less sensitive to track (snippet) length. We also applied our method to intracranial EEG recordings of the amygdala and observed that the RQEs of the emotional and neutral video clips could be discriminated and that the MMRQE of the emotional video coincided with that of the mid-frontal scalp electrodes at the end of the clip where the climax of the video is revealed. This was also confirmed by the increasing phase synchrony levels between the mid-frontal and amygdala recordings. This provides conclusive evidence of the proposed multiscale entropy-based method in discriminating emotional states.</p>
</sec>
<sec id="sec023">
<title>Supporting information</title>
<supplementary-material id="pone.0186916.s001" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pone.0186916.s001" xlink:type="simple">
<label>S1 Appendix</label>
<caption>
<title>Effect of snippet length.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0186916.s002" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pone.0186916.s002" xlink:type="simple">
<label>S2 Appendix</label>
<caption>
<title>MMRQE plotted in terms of individual IMFs.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0186916.s003" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pone.0186916.s003" xlink:type="simple">
<label>S3 Appendix</label>
<caption>
<title>Scalp distribution of original EEG’s alpha coefficient.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0186916.s004" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pone.0186916.s004" xlink:type="simple">
<label>S4 Appendix</label>
<caption>
<title>Correlation matrix of depth electrode.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0186916.s005" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pone.0186916.s005" xlink:type="simple">
<label>S5 Appendix</label>
<caption>
<title>Location of subdural electrode grid.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0186916.s006" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pone.0186916.s006" xlink:type="simple">
<label>S6 Appendix</label>
<caption>
<title>MMRQE results for non-French speaking subjects, main experiment.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0186916.s007" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pone.0186916.s007" xlink:type="simple">
<label>S7 Appendix</label>
<caption>
<title>MMRQE results, control experiment.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0186916.s008" mimetype="application/pdf" position="float" xlink:href="info:doi/10.1371/journal.pone.0186916.s008" xlink:type="simple">
<label>S8 Appendix</label>
<caption>
<title>Phase synchrony between simulated and mid-frontal EEG.</title>
<p>(PDF)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>The authors are grateful to Prof. Alexandre Schaefer (School of Business, Monash University, Victoria, Malaysia) for his valuable input to the interpretation of the video clip data and to Prof. Alfred Meurs, Dr. Evelien Carrette, and Prof. Paul Boon (Neurology Department, Ghent University Hospital), Prof. Dirk Van Roost (Department of Neurosurgery, Ghent University Hospital) for their assistance in the ECoG experiment and Mansoureh Fahimi Hnazaee (Laboratory of Neuro- and Psychophysiology, KU Leuven) for performing the source reconstruction analysis. YT is supported by the Interuniversity Attraction Poles Programme Belgian Science Policy (IUAP P7/11). MMVH is supported by research grants received from the Financing program (PFV/10/008), an interdisciplinary research project (IDO/12/007), and an industrial research fund project (IOF/HB/12/021) of the KU Leuven, the Belgian Fund for Scientific Research–Flanders (G088314N, G0A0914N), the Interuniversity Attraction Poles Programme–Belgian Science Policy (IUAP P7/11), the Flemish Regional Ministry of Education (Belgium) (GOA 10/019), and the Hercules Foundation (AKUL 043).Part of the reported work was done while MVH was a Visiting Academic at Imperial College London. DM is supported by an EPSRC grant (EP/K025643/1).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0186916.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>John</surname> <given-names>ER</given-names></name>, <name name-style="western"><surname>Prichep</surname> <given-names>LS</given-names></name>, <name name-style="western"><surname>Fridman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Easton</surname> <given-names>P</given-names></name>. <article-title>Neurometrics: computer-assisted differential diagnosis of brain dysfunctions</article-title>. <source>Sci</source> [Internet]. <year>1988</year> <month>Jan</month> <day>8</day>;<volume>239</volume> (<issue>4836</issue>): <fpage>162</fpage>–<lpage>9</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.sciencemag.org/content/239/4836/162.abstract" xlink:type="simple">http://www.sciencemag.org/content/239/4836/162.abstract</ext-link></mixed-citation></ref>
<ref id="pone.0186916.ref002"><label>2</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Nunez</surname> <given-names>PL</given-names></name>, <name name-style="western"><surname>Cutillo</surname> <given-names>BA</given-names></name>. <source>Neocortical dynamics and human EEG rhythms</source>. <publisher-name>Oxford University Press</publisher-name>, <publisher-loc>USA</publisher-loc>; <year>1995</year>.</mixed-citation></ref>
<ref id="pone.0186916.ref003"><label>3</label><mixed-citation publication-type="other" xlink:type="simple">Choppin A. EEG-based human interface for disabled individuals: Emotion expression with neural networks. Unpubl master’s thesis. 2000;</mixed-citation></ref>
<ref id="pone.0186916.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Millán</surname> <given-names>J d R</given-names></name>, <name name-style="western"><surname>Rupp</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Müller-Putz</surname> <given-names>GR</given-names></name>, <name name-style="western"><surname>Murray-Smith</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Giugliemma</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Tangermann</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>Combining brain–computer interfaces and assistive technologies: state-of-the-art and challenges</article-title>. <source>Front Neurosci</source>. <year>2010</year>;<volume>4</volume>.</mixed-citation></ref>
<ref id="pone.0186916.ref005"><label>5</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Nijholt</surname> <given-names>A</given-names></name>. <chapter-title>BCI for games: A “state of the art”survey</chapter-title>. In: <source>Entertainment Computing-ICEC 2008</source>. <publisher-name>Springer</publisher-name>; <year>2009</year>. p. <fpage>225</fpage>–<lpage>8</lpage>.</mixed-citation></ref>
<ref id="pone.0186916.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Davidson</surname> <given-names>RJ</given-names></name>. <article-title>Anterior cerebral asymmetry and the nature of emotion</article-title>. <source>Brain Cogn</source> [Internet]. <year>1992</year> <month>Sep</month> [cited 2015 Jul 7];<volume>20</volume>(<issue>1</issue>):<fpage>125</fpage>–<lpage>51</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/027826269290065T" xlink:type="simple">http://www.sciencedirect.com/science/article/pii/027826269290065T</ext-link> <object-id pub-id-type="pmid">1389117</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kostyunina</surname> <given-names>MB</given-names></name>, <name name-style="western"><surname>Kulikov</surname> <given-names>MA</given-names></name>. <article-title>Frequency characteristics of EEG spectra in the emotions</article-title>. <source>Neurosci Behav Physiol</source> [Internet]. <year>1996</year>;<volume>26</volume>(<issue>4</issue>):<fpage>340</fpage>–<lpage>3</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://europepmc.org/abstract/MED/8912339" xlink:type="simple">http://europepmc.org/abstract/MED/8912339</ext-link> <object-id pub-id-type="pmid">8912339</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shemiakina</surname> <given-names>N V</given-names></name>, <name name-style="western"><surname>Dan’ko</surname> <given-names>SG</given-names></name>. <article-title>Influence of the emotional perception of a signal on the electroencephalographic correlates of the creative activity</article-title>. <source>Fiziol Cheloveka</source>. <year>2004</year>;<volume>30</volume>(<issue>2</issue>):<fpage>22</fpage>–<lpage>9</lpage>. <object-id pub-id-type="pmid">15150971</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jaušovec</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Jaušovec</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Gerlič</surname> <given-names>I</given-names></name>. <article-title>Differences in event-related and induced EEG patterns in the theta and alpha frequency bands related to human emotional intelligence</article-title>. <source>Neurosci Lett</source>. <year>2001</year>;<volume>311</volume>(<issue>2</issue>):<fpage>93</fpage>–<lpage>6</lpage>. <object-id pub-id-type="pmid">11567786</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vecchiato</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Astolfi</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Fallani</surname> <given-names>FDV</given-names></name>, <name name-style="western"><surname>Cincotti</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Mattia</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Salinari</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Changes in brain activity during the observation of TV commercials by using EEG, GSR and HR measurements</article-title>. <source>Brain Topogr</source>. <year>2010</year>;<volume>23</volume>(<issue>2</issue>):<fpage>165</fpage>–<lpage>79</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s10548-009-0127-0" xlink:type="simple">10.1007/s10548-009-0127-0</ext-link></comment> <object-id pub-id-type="pmid">20033272</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bekkedal</surname> <given-names>MY V</given-names></name>, <name name-style="western"><surname>Rossi</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Panksepp</surname> <given-names>J</given-names></name>. <article-title>Human brain EEG indices of emotions: delineating responses to affective vocalizations by measuring frontal theta event-related synchronization</article-title>. <source>Neurosci Biobehav Rev</source>. <year>2011</year>;<volume>35</volume>(<issue>9</issue>):<fpage>1959</fpage>–<lpage>70</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neubiorev.2011.05.001" xlink:type="simple">10.1016/j.neubiorev.2011.05.001</ext-link></comment> <object-id pub-id-type="pmid">21596060</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname> <given-names>X-W</given-names></name>, <name name-style="western"><surname>Nie</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Lu</surname> <given-names>B-L</given-names></name>. <article-title>Emotional state classification from EEG data using machine learning approach</article-title>. <source>Neurocomputing</source>. <year>2014</year>;<volume>129</volume>:<fpage>94</fpage>–<lpage>106</lpage>.</mixed-citation></ref>
<ref id="pone.0186916.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Daly</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Malik</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hwang</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Roesch</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Weaver</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Kirke</surname> <given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Neural correlates of emotional responses to music: an EEG study</article-title>. <source>Neurosci Lett</source>. <year>2014</year>;<volume>573</volume>:<fpage>52</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neulet.2014.05.003" xlink:type="simple">10.1016/j.neulet.2014.05.003</ext-link></comment> <object-id pub-id-type="pmid">24820541</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Paulmann</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kotz</surname> <given-names>SA</given-names></name>. <article-title>An ERP investigation on the temporal dynamics of emotional prosody and emotional semantics in pseudo-and lexical-sentence context</article-title>. <source>Brain Lang</source>. <year>2008</year>;<volume>105</volume>(<issue>1</issue>):<fpage>59</fpage>–<lpage>69</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.bandl.2007.11.005" xlink:type="simple">10.1016/j.bandl.2007.11.005</ext-link></comment> <object-id pub-id-type="pmid">18177699</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Spreckelmeyer</surname> <given-names>KN</given-names></name>, <name name-style="western"><surname>Kutas</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Urbach</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Altenmüller</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Münte</surname> <given-names>TF</given-names></name>. <article-title>Neural processing of vocal emotion and identity</article-title>. <source>Brain Cogn</source>. <year>2009</year>;<volume>69</volume>(<issue>1</issue>):<fpage>121</fpage>–<lpage>6</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.bandc.2008.06.003" xlink:type="simple">10.1016/j.bandc.2008.06.003</ext-link></comment> <object-id pub-id-type="pmid">18644670</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Balconi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lucchiari</surname> <given-names>C</given-names></name>. <article-title>EEG correlates (event-related desynchronization) of emotional face elaboration: a temporal analysis</article-title>. <source>Neurosci Lett</source>. <year>2006</year>;<volume>392</volume>(<issue>1</issue>):<fpage>118</fpage>–<lpage>23</lpage>.</mixed-citation></ref>
<ref id="pone.0186916.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>He</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Luo</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Zhu</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Gu</surname> <given-names>R</given-names></name>, <etal>et al</etal>. <article-title>Three stages of emotional word processing: an ERP study with rapid serial visual presentation</article-title>. <source>Soc Cogn Affect Neurosci</source>. <year>2014</year>;<volume>9</volume>(<issue>12</issue>):<fpage>1897</fpage>–<lpage>903</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/scan/nst188" xlink:type="simple">10.1093/scan/nst188</ext-link></comment> <object-id pub-id-type="pmid">24526185</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jessen</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Kotz</surname> <given-names>SA</given-names></name>. <article-title>The temporal dynamics of processing emotions from vocal, facial, and bodily expressions</article-title>. <source>Neuroimage</source>. <year>2011</year>;<volume>58</volume>(<issue>2</issue>):<fpage>665</fpage>–<lpage>74</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2011.06.035" xlink:type="simple">10.1016/j.neuroimage.2011.06.035</ext-link></comment> <object-id pub-id-type="pmid">21718792</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stefanics</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Csukly</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Komlósi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Czobor</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Czigler</surname> <given-names>I</given-names></name>. <article-title>Processing of unattended facial emotions: a visual mismatch negativity study</article-title>. <source>Neuroimage</source>. <year>2012</year>;<volume>59</volume>(<issue>3</issue>):<fpage>3042</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2011.10.041" xlink:type="simple">10.1016/j.neuroimage.2011.10.041</ext-link></comment> <object-id pub-id-type="pmid">22037000</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Goldberger</surname> <given-names>AL</given-names></name>, <name name-style="western"><surname>Peng</surname> <given-names>C-K</given-names></name>, <name name-style="western"><surname>Lipsitz</surname> <given-names>LA</given-names></name>. <article-title>What is physiologic complexity and how does it change with aging and disease?</article-title> <source>Neurobiol Aging</source>. <year>2002</year>;<volume>23</volume>(<issue>1</issue>):<fpage>23</fpage>–<lpage>6</lpage>. <object-id pub-id-type="pmid">11755014</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Aftanas</surname> <given-names>LI</given-names></name>, <name name-style="western"><surname>Lotova</surname> <given-names>N V</given-names></name>, <name name-style="western"><surname>Koshkarov</surname> <given-names>VI</given-names></name>, <name name-style="western"><surname>Makhnev</surname> <given-names>VP</given-names></name>, <name name-style="western"><surname>Mordvintsev</surname> <given-names>YN</given-names></name>, <name name-style="western"><surname>Popov</surname> <given-names>SA</given-names></name>. <article-title>Non-linear dynamic complexity of the human EEG during evoked emotions</article-title>. <source>Int J Psychophysiol</source>. <year>1998</year>;<volume>28</volume>(<issue>1</issue>):<fpage>63</fpage>–<lpage>76</lpage>. <object-id pub-id-type="pmid">9506311</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hosseini</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Naghibi-Sistani</surname> <given-names>MB</given-names></name>. <article-title>Emotion recognition method using entropy analysis of EEG signals</article-title>. <source>Int J Image, Graph Signal Process</source>. <year>2011</year>;<volume>3</volume>(<issue>5</issue>):<fpage>30</fpage>.</mixed-citation></ref>
<ref id="pone.0186916.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jie</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Cao</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Li</surname> <given-names>L</given-names></name>. <article-title>Emotion recognition based on the sample entropy of EEG</article-title>. <source>Biomed Mater Eng</source>. <year>2013</year>;<volume>24</volume>(<issue>1</issue>):<fpage>1185</fpage>–<lpage>92</lpage>.</mixed-citation></ref>
<ref id="pone.0186916.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rényi</surname> <given-names>a</given-names></name>. <article-title>On measures of entropy and information</article-title>. <source>Entropy</source> [Internet]. <year>1961</year>;<volume>547</volume>(<issue>c</issue>):<fpage>547</fpage>–<lpage>61</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.maths.gla.ac.uk/~tl/Renyi.pdf" xlink:type="simple">http://www.maths.gla.ac.uk/~tl/Renyi.pdf</ext-link></mixed-citation></ref>
<ref id="pone.0186916.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Arefian</surname> <given-names>NM</given-names></name>, <name name-style="western"><surname>Zali</surname> <given-names>AR</given-names></name>, <name name-style="western"><surname>Seddighi</surname> <given-names>AS</given-names></name>, <name name-style="western"><surname>Fathi</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Teymourian</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Dabir</surname> <given-names>S</given-names></name>, <etal>et al</etal>. <article-title>Clinical analysis of eeg parameters in prediction of the depth of anesthesia in different stages: a comparative study</article-title>. <source>Tanaffos</source>. <year>2009</year>;<volume>8</volume>(<issue>2</issue>):<fpage>46</fpage>–<lpage>53</lpage>.</mixed-citation></ref>
<ref id="pone.0186916.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sourina</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Wang</surname> <given-names>Q</given-names></name>, <name name-style="western"><surname>Liu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Nguyen</surname> <given-names>MK</given-names></name>. <article-title>A real-time fracal-based brain state recognition from EEG and its application</article-title>. <source>Proc Biosignals</source> [Internet]. <year>2011</year>;<fpage>82</fpage>–<lpage>91</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.ntu.edu.sg/home/eosourina/Papers/BIOSIGNALS_2011_164_CR.pdf" xlink:type="simple">http://www.ntu.edu.sg/home/eosourina/Papers/BIOSIGNALS_2011_164_CR.pdf</ext-link></mixed-citation></ref>
<ref id="pone.0186916.ref027"><label>27</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Bajaj</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Pachori</surname> <given-names>RB</given-names></name>. <chapter-title>Detection of human emotions using features based on the multiwavelet transform of EEG signals</chapter-title>. In: <source>Brain-Computer Interfaces</source>. <publisher-name>Springer</publisher-name>; <year>2015</year>. p. <fpage>215</fpage>–<lpage>40</lpage>.</mixed-citation></ref>
<ref id="pone.0186916.ref028"><label>28</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Kantz</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Schreiber</surname> <given-names>T</given-names></name>. <source>Nonlinear time series analysis</source>. <volume>Vol. 7</volume>. <publisher-name>Cambridge university press</publisher-name>; <year>2004</year>.</mixed-citation></ref>
<ref id="pone.0186916.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Costa</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Goldberger</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Peng</surname> <given-names>C-K</given-names></name>. <article-title>Multiscale entropy analysis of biological signals</article-title>. <source>Phys Rev E</source> [Internet]. <year>2005</year> <month>Feb</month> [cited 2014 Mar 24];<volume>71</volume>(<issue>2</issue>):<fpage>021906</fpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://link.aps.org/doi/10.1103/PhysRevE.71.021906" xlink:type="simple">http://link.aps.org/doi/10.1103/PhysRevE.71.021906</ext-link></mixed-citation></ref>
<ref id="pone.0186916.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Huang</surname> <given-names>NE</given-names></name>, <name name-style="western"><surname>Shen</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Long</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>MC</given-names></name>, <name name-style="western"><surname>Shih</surname> <given-names>HH</given-names></name>, <name name-style="western"><surname>Zheng</surname> <given-names>Q</given-names></name>, <etal>et al</etal>. <article-title>The empirical mode decomposition and the Hilbert spectrum for nonlinear and non-stationary time series analysis</article-title>. <source>Proc R Soc A Math Phys Eng Sci</source> [Internet]. <year>1998</year> <month>Mar</month> <day>8</day> [cited 2014 Apr 28];<volume>454</volume>(<issue>1971</issue>):<fpage>903</fpage>–<lpage>95</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.1998.0193" xlink:type="simple">http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.1998.0193</ext-link></mixed-citation></ref>
<ref id="pone.0186916.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rehman</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Mandic</surname> <given-names>DP</given-names></name>. <article-title>Multivariate empirical mode decomposition</article-title>. <source>Proc R Soc A Math Phys Eng Sci</source> [Internet]. <year>2009</year> <month>Dec</month> <day>23</day> [cited 2014 Mar 24];<volume>466</volume>(<issue>2117</issue>):<fpage>1291</fpage>–<lpage>302</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.2009.0502" xlink:type="simple">http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.2009.0502</ext-link></mixed-citation></ref>
<ref id="pone.0186916.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ahmed</surname> <given-names>MU</given-names></name>, <name name-style="western"><surname>Rehman</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Looney</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Rutkowski</surname> <given-names>TM</given-names></name>, <name name-style="western"><surname>Mandic</surname> <given-names>DP</given-names></name>. <article-title>Dynamical complexity of human responses: a multivariate data-adaptive framework</article-title>. <source>Bull Polish Acad Sci Tech Sci</source> [Internet]. <year>2012</year>;<volume>60</volume>(<issue>3</issue>):<fpage>433</fpage>–<lpage>45</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.degruyter.com/view/j/bpasts.2012.60.issue-3/v10175-012-0055-0/v10175-012-0055-0.xml" xlink:type="simple">http://www.degruyter.com/view/j/bpasts.2012.60.issue-3/v10175-012-0055-0/v10175-012-0055-0.xml</ext-link></mixed-citation></ref>
<ref id="pone.0186916.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sharma</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Pachori</surname> <given-names>RB</given-names></name>, <name name-style="western"><surname>Acharya</surname> <given-names>UR</given-names></name>. <article-title>Application of Entropy Measures on Intrinsic Mode Functions for the Automated Identification of Focal Electroencephalogram Signals</article-title>. <source>Entropy</source>. <year>2015</year>;<volume>17</volume>(<issue>2</issue>):<fpage>669</fpage>–<lpage>91</lpage>.</mixed-citation></ref>
<ref id="pone.0186916.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tonoyan</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Looney</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Mandic</surname> <given-names>DP</given-names></name>, <name name-style="western"><surname>Van Hulle</surname> <given-names>MM</given-names></name>. <article-title>Discriminating Multiple Emotional States from EEG Using a Data-Adaptive, Multiscale Information-Theoretic Approach</article-title>. <source>Int J Neural Syst</source>. <year>2016</year>;</mixed-citation></ref>
<ref id="pone.0186916.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schaefer</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Nils</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Sanchez</surname> <given-names>X</given-names></name>, <name name-style="western"><surname>Philippot</surname> <given-names>P</given-names></name>. <article-title>Assessing the effectiveness of a large database of emotion-eliciting films: A new tool for emotion researchers</article-title>. <source>Cogn Emot</source> [Internet]. <year>2010</year>;<volume>24</volume>(<issue>7</issue>):<fpage>1153</fpage>–<lpage>72</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.informaworld.com/smpp/title~content=t713682755" xlink:type="simple">http://www.informaworld.com/smpp/title~content=t713682755</ext-link></mixed-citation></ref>
<ref id="pone.0186916.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hayes</surname> <given-names>AF</given-names></name>, <name name-style="western"><surname>Krippendorff</surname> <given-names>K</given-names></name>. <article-title>Answering the call for a standard reliability measure for coding data</article-title>. <source>Commun Methods Meas</source>. <year>2007</year>;<volume>1</volume>(<issue>1</issue>):<fpage>77</fpage>–<lpage>89</lpage>.</mixed-citation></ref>
<ref id="pone.0186916.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fish</surname> <given-names>DR</given-names></name>, <name name-style="western"><surname>Gloor</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Quesney</surname> <given-names>FL</given-names></name>, <name name-style="western"><surname>Oliver</surname> <given-names>A</given-names></name>. <article-title>Clinical responses to electrical brain stimulation of the temporal and frontal lobes in patients with epilepsy</article-title>. <source>Brain</source>. <year>1993</year>;<volume>116</volume>(<issue>2</issue>):<fpage>397</fpage>–<lpage>414</lpage>.</mixed-citation></ref>
<ref id="pone.0186916.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>HALGREN</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>WALTER</surname> <given-names>RD</given-names></name>, <name name-style="western"><surname>CHERLOW</surname> <given-names>DG</given-names></name>, <name name-style="western"><surname>CRANDALL</surname> <given-names>PH</given-names></name>. <article-title>Mental phenomena evoked by electrical stimulation of the human hippocampal formation and amygdala</article-title>. <source>Brain</source>. <year>1978</year>;<volume>101</volume>(<issue>1</issue>):<fpage>83</fpage>–<lpage>115</lpage>. <object-id pub-id-type="pmid">638728</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fried</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>MacDonald</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Wilson</surname> <given-names>CL</given-names></name>. <article-title>Single neuron activity in human hippocampus and amygdala during recognition of faces and objects</article-title>. <source>Neuron</source>. <year>1997</year>;<volume>18</volume>(<issue>5</issue>):<fpage>753</fpage>–<lpage>65</lpage>. <object-id pub-id-type="pmid">9182800</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Oya</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Kawasaki</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Howard</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Adolphs</surname> <given-names>R</given-names></name>. <article-title>Electrophysiological responses in the human amygdala discriminate emotion categories of complex visual stimuli</article-title>. <source>J Neurosci</source>. <year>2002</year>;<volume>22</volume>(<issue>21</issue>):<fpage>9502</fpage>–<lpage>12</lpage>. <object-id pub-id-type="pmid">12417674</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Krolak-Salmon</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Hénaff</surname> <given-names>M-A</given-names></name>, <name name-style="western"><surname>Vighetto</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bertrand</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Mauguière</surname> <given-names>F</given-names></name>. <article-title>Early amygdala reaction to fear spreading in occipital, temporal, and frontal cortex: a depth electrode ERP study in human</article-title>. <source>Neuron</source>. <year>2004</year>;<volume>42</volume>(<issue>4</issue>):<fpage>665</fpage>–<lpage>76</lpage>. <object-id pub-id-type="pmid">15157426</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Naccache</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Gaillard</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Adam</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Hasboun</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Clémenceau</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Baulac</surname> <given-names>M</given-names></name>, <etal>et al</etal>. <article-title>A direct intracranial record of emotions evoked by subliminal words</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2005</year>;<volume>102</volume>(<issue>21</issue>):<fpage>7713</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0500542102" xlink:type="simple">10.1073/pnas.0500542102</ext-link></comment> <object-id pub-id-type="pmid">15897465</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lanteaume</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Khalfa</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Régis</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Marquis</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Chauvel</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Bartolomei</surname> <given-names>F</given-names></name>. <article-title>Emotion induction after direct intracerebral stimulations of human amygdala</article-title>. <source>Cereb Cortex</source>. <year>2007</year>;<volume>17</volume>(<issue>6</issue>):<fpage>1307</fpage>–<lpage>13</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhl041" xlink:type="simple">10.1093/cercor/bhl041</ext-link></comment> <object-id pub-id-type="pmid">16880223</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meletti</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Tassi</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Mai</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Fini</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Tassinari</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Russo</surname> <given-names>G Lo</given-names></name>. <article-title>Emotions induced by intracerebral electrical stimulation of the temporal lobe</article-title>. <source>Epilepsia</source>. <year>2006</year>;<volume>47</volume>(<issue>s5</issue>):<fpage>47</fpage>–<lpage>51</lpage>.</mixed-citation></ref>
<ref id="pone.0186916.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pourtois</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Spinelli</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Seeck</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Vuilleumier</surname> <given-names>P</given-names></name>. <article-title>Temporal precedence of emotion over attention modulations in the lateral amygdala: Intracranial ERP evidence from a patient with temporal lobe epilepsy</article-title>. <source>Cogn Affect Behav Neurosci</source>. <year>2010</year>;<volume>10</volume>(<issue>1</issue>):<fpage>83</fpage>–<lpage>93</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/CABN.10.1.83" xlink:type="simple">10.3758/CABN.10.1.83</ext-link></comment> <object-id pub-id-type="pmid">20233957</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sato</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Kochiyama</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Uono</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Matsuda</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Usui</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Inoue</surname> <given-names>Y</given-names></name>, <etal>et al</etal>. <article-title>Rapid amygdala gamma oscillations in response to fearful facial expressions</article-title>. <source>Neuropsychologia</source>. <year>2011</year>;<volume>49</volume>(<issue>4</issue>):<fpage>612</fpage>–<lpage>7</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuropsychologia.2010.12.025" xlink:type="simple">10.1016/j.neuropsychologia.2010.12.025</ext-link></comment> <object-id pub-id-type="pmid">21182851</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sergerie</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Chochol</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Armony</surname> <given-names>JL</given-names></name>. <article-title>The role of the amygdala in emotional processing: a quantitative meta-analysis of functional neuroimaging studies</article-title>. <source>Neurosci Biobehav Rev</source>. <year>2008</year>;<volume>32</volume>(<issue>4</issue>):<fpage>811</fpage>–<lpage>30</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neubiorev.2007.12.002" xlink:type="simple">10.1016/j.neubiorev.2007.12.002</ext-link></comment> <object-id pub-id-type="pmid">18316124</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Aggleton</surname> <given-names>JP</given-names></name>, <name name-style="western"><surname>Mishkin</surname> <given-names>M</given-names></name>. <article-title>The amygdala: sensory gateway to the emotions</article-title>. <source>Emot Theory, Res Exp</source>. <year>1986</year>;<volume>3</volume>:<fpage>281</fpage>–<lpage>99</lpage>.</mixed-citation></ref>
<ref id="pone.0186916.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Makeig</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Debener</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Onton</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Delorme</surname> <given-names>A</given-names></name>. <article-title>Mining event-related brain dynamics</article-title>. <source>Trends Cogn Sci</source>. <year>2004</year>;<volume>8</volume>(<issue>5</issue>):<fpage>204</fpage>–<lpage>10</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2004.03.008" xlink:type="simple">10.1016/j.tics.2004.03.008</ext-link></comment> <object-id pub-id-type="pmid">15120678</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Croft</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Chandler</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Barry</surname> <given-names>RJ</given-names></name>, <name name-style="western"><surname>Cooper</surname> <given-names>NR</given-names></name>, <name name-style="western"><surname>Clarke</surname> <given-names>AR</given-names></name>. <article-title>EOG correction: a comparison of four methods. Psychophysiology</article-title> [<source>Internet]</source>. <year>2005</year> <month>Jan</month> [cited 2015 Jul 7];<volume>42</volume>(<issue>1</issue>):<fpage>16</fpage>–<lpage>24</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/15720577" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/15720577</ext-link></mixed-citation></ref>
<ref id="pone.0186916.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van Rijn</surname> <given-names>ACM</given-names></name>, <name name-style="western"><surname>Peper</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Grimbergen</surname> <given-names>CA</given-names></name>. <article-title>High-quality recording of bioelectric events</article-title>. <source>Med Biol Eng Comput</source>. <year>1991</year>;<volume>29</volume>(<issue>4</issue>):<fpage>433</fpage>–<lpage>40</lpage>. <object-id pub-id-type="pmid">1787761</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref052"><label>52</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Richman</surname> <given-names>JS</given-names></name>, <name name-style="western"><surname>Moorman</surname> <given-names>JR</given-names></name>. <article-title>Physiological time-series analysis using approximate entropy and sample entropy</article-title>. <source>Am J Physiol Heart Circ Physiol</source> [Internet]. <year>2000</year> <month>Jun</month>;<volume>278</volume>(<issue>6</issue>):<fpage>H2039</fpage>–<lpage>49</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/10843903" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/10843903</ext-link> <object-id pub-id-type="pmid">10843903</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref053"><label>53</label><mixed-citation publication-type="other" xlink:type="simple">Gautama T, Mandic DP, Van Hulle MM. A differential entropy based method for determining the optimal embedding parameters of a signal. In: Acoustics, Speech, and Signal Processing, 2003 Proceedings(ICASSP’03) 2003 IEEE International Conference on. IEEE; 2003. p. VI– 29.</mixed-citation></ref>
<ref id="pone.0186916.ref054"><label>54</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Principe</surname> <given-names>JC</given-names></name>. <source>Information theoretic learning: Renyi’s entropy and kernel perspectives</source>. <publisher-name>Springer Science &amp; Business Media</publisher-name>; <year>2010</year>.</mixed-citation></ref>
<ref id="pone.0186916.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Costa</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Priplata</surname> <given-names>AA</given-names></name>, <name name-style="western"><surname>Lipsitz</surname> <given-names>LA</given-names></name>, <name name-style="western"><surname>Wu</surname> <given-names>Z</given-names></name>, <name name-style="western"><surname>Huang</surname> <given-names>NE</given-names></name>, <name name-style="western"><surname>Goldberger</surname> <given-names>AL</given-names></name>, <etal>et al</etal>. <article-title>Noise and poise: enhancement of postural complexity in the elderly with a stochastic-resonance–based therapy</article-title>. <source>EPL (Europhysics Lett</source>. <year>2007</year>;<volume>77</volume>(<issue>6</issue>):<fpage>68008</fpage>.</mixed-citation></ref>
<ref id="pone.0186916.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shattuck</surname> <given-names>DW</given-names></name>, <name name-style="western"><surname>Leahy</surname> <given-names>RM</given-names></name>. <article-title>BrainSuite: an automated cortical surface identification tool</article-title>. <source>Med Image Anal</source>. <year>2002</year>;<volume>6</volume>(<issue>2</issue>):<fpage>129</fpage>–<lpage>42</lpage>. <object-id pub-id-type="pmid">12045000</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref057"><label>57</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gramfort</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Papadopoulo</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Olivi</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Clerc</surname> <given-names>M</given-names></name>. <article-title>OpenMEEG: opensource software for quasistatic bioelectromagnetics</article-title>. <source>Biomed Eng Online</source>. <year>2010</year>;<volume>9</volume>(<issue>1</issue>):<fpage>45</fpage>.</mixed-citation></ref>
<ref id="pone.0186916.ref058"><label>58</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pascual-Marqui</surname> <given-names>RD</given-names></name>. <article-title>Standardized low-resolution brain electromagnetic tomography (sLORETA): technical details</article-title>. <source>Methods Find Exp Clin Pharmacol</source>. <year>2002</year>;<volume>24</volume>(<issue>Suppl D</issue>):<fpage>5</fpage>–<lpage>12</lpage>.</mixed-citation></ref>
<ref id="pone.0186916.ref059"><label>59</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laird</surname> <given-names>NM</given-names></name>, <name name-style="western"><surname>Ware</surname> <given-names>JH</given-names></name>. <article-title>Random-effects models for longitudinal data</article-title>. <source>Biometrics</source>. <year>1982</year>;<fpage>963</fpage>–<lpage>74</lpage>. <object-id pub-id-type="pmid">7168798</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref060"><label>60</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Verbeke</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Molenberghs</surname> <given-names>G</given-names></name>. <source>Linear mixed models for longitudinal data</source>. <publisher-name>Springer Science &amp; Business Media</publisher-name>; <year>2009</year>.</mixed-citation></ref>
<ref id="pone.0186916.ref061"><label>61</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lachaux</surname> <given-names>J-P</given-names></name>, <name name-style="western"><surname>Rodriguez</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Martinerie</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Varela</surname> <given-names>FJ</given-names></name>. <article-title>Measuring phase synchrony in brain signals</article-title>. <source>Hum Brain Mapp</source>. <year>1999</year>;<volume>8</volume>(<issue>4</issue>):<fpage>194</fpage>–<lpage>208</lpage>. <object-id pub-id-type="pmid">10619414</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref062"><label>62</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gautama</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Mandic</surname> <given-names>DP</given-names></name>, <name name-style="western"><surname>Van Hulle</surname> <given-names>MM</given-names></name>. <article-title>Signal nonlinearity in fMRI: a comparison between BOLD and MION</article-title>. <source>IEEE Trans Med Imaging</source>. <year>2003</year>;<volume>22</volume>(<issue>5</issue>):<fpage>636</fpage>–<lpage>44</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/TMI.2003.812248" xlink:type="simple">10.1109/TMI.2003.812248</ext-link></comment> <object-id pub-id-type="pmid">12846432</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref063"><label>63</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schreiber</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Schmitz</surname> <given-names>A</given-names></name>. <article-title>Improved surrogate data for nonlinearity tests</article-title>. <source>Phys Rev Lett</source>. <year>1996</year>;<volume>77</volume>(<issue>4</issue>):<fpage>635</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1103/PhysRevLett.77.635" xlink:type="simple">10.1103/PhysRevLett.77.635</ext-link></comment> <object-id pub-id-type="pmid">10062864</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref064"><label>64</label><mixed-citation publication-type="other" xlink:type="simple">Soleymani M, Koelstra S, Patras I, Pun T. Continuous emotion detection in response to music videos. Face Gesture 2011 [Internet]. 2011 Mar;803–8. <ext-link ext-link-type="uri" xlink:href="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5771352" xlink:type="simple">http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5771352</ext-link></mixed-citation></ref>
<ref id="pone.0186916.ref065"><label>65</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bartolic</surname> <given-names>EI</given-names></name>, <name name-style="western"><surname>Basso</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Schefft</surname> <given-names>BK</given-names></name>, <name name-style="western"><surname>Glauser</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Titanic-Schefft</surname> <given-names>M</given-names></name>. <article-title>Effects of experimentally-induced emotional states on frontal lobe cognitive task performance</article-title>. <source>Neuropsychologia</source>. <year>1999</year>;<volume>37</volume>(<issue>6</issue>):<fpage>677</fpage>–<lpage>83</lpage>. <object-id pub-id-type="pmid">10390029</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref066"><label>66</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stuss</surname> <given-names>DT</given-names></name>, <name name-style="western"><surname>Gow</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Hetherington</surname> <given-names>CR</given-names></name>. <article-title>“No longer Gage”: frontal lobe dysfunction and emotional changes</article-title>. <source>J Consult Clin Psychol</source>. <year>1992</year>;<volume>60</volume>(<issue>3</issue>):<fpage>349</fpage>. <object-id pub-id-type="pmid">1619089</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref067"><label>67</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ochsner</surname> <given-names>KN</given-names></name>, <name name-style="western"><surname>Gross</surname> <given-names>JJ</given-names></name>. <article-title>The cognitive control of emotion</article-title>. <source>Trends Cogn Sci</source>. <year>2005</year>;<volume>9</volume>(<issue>5</issue>):<fpage>242</fpage>–<lpage>9</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2005.03.010" xlink:type="simple">10.1016/j.tics.2005.03.010</ext-link></comment> <object-id pub-id-type="pmid">15866151</object-id></mixed-citation></ref>
<ref id="pone.0186916.ref068"><label>68</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Holt</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Kunkel</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Weiss</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Goff</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Wright</surname> <given-names>CI</given-names></name>, <name name-style="western"><surname>Shin</surname> <given-names>LM</given-names></name>, <etal>et al</etal>. <article-title>Increased medial temporal lobe activation during the passive viewing of emotional and neutral facial expressions in schizophrenia</article-title>. <source>Schizophr Res</source>. <year>2006</year>;<volume>82</volume>(<issue>2</issue>):<fpage>153</fpage>–<lpage>62</lpage>.</mixed-citation></ref>
<ref id="pone.0186916.ref069"><label>69</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Guillory</surname> <given-names>SA</given-names></name>, <name name-style="western"><surname>Bujarski</surname> <given-names>KA</given-names></name>. <article-title>Exploring emotions using invasive methods: review of 60 years of human intracranial electrophysiology</article-title>. <source>Soc Cogn Affect Neurosci</source>. <year>2014</year>;nsu002.</mixed-citation></ref>
</ref-list>
</back>
</article>