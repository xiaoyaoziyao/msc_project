<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article article-type="research-article" dtd-version="3.0" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-15-51094</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0150611</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Data visualization</subject><subj-group><subject>Infographics</subject><subj-group><subject>Graphs</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Applied mathematics</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject><subj-group><subject>Algorithms</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Genome analysis</subject><subj-group><subject>Genetic networks</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Genomics</subject><subj-group><subject>Genome analysis</subject><subj-group><subject>Genetic networks</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Computer and information sciences</subject><subj-group><subject>Network analysis</subject><subj-group><subject>Genetic networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Optimization</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Simulation and modeling</subject></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Cell biology</subject><subj-group><subject>Cellular structures and organelles</subject><subj-group><subject>Cell signaling structures</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Molecular biology</subject><subj-group><subject>Molecular biology techniques</subject><subj-group><subject>Artificial genetic recombination</subject><subj-group><subject>Gene knockout</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Research and analysis methods</subject><subj-group><subject>Molecular biology techniques</subject><subj-group><subject>Artificial genetic recombination</subject><subj-group><subject>Gene knockout</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Computational biology</subject><subj-group><subject>Gene regulatory networks</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Genetics</subject><subj-group><subject>Gene regulatory networks</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Reconstructing Causal Biological Networks through Active Learning</article-title>
<alt-title alt-title-type="running-head">Reconstructing Causal Biological Networks through Active Learning</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Cho</surname> <given-names>Hyunghoon</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Berger</surname> <given-names>Bonnie</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Peng</surname> <given-names>Jian</given-names></name>
<xref ref-type="aff" rid="aff001"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff002"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff003"><sup>3</sup></xref>
<xref ref-type="corresp" rid="cor001">*</xref>
</contrib>
</contrib-group>
<aff id="aff001">
<label>1</label>
<addr-line>Computer Science and Artificial Intelligence Laboratory, MIT, Cambridge, MA, United States of America</addr-line>
</aff>
<aff id="aff002">
<label>2</label>
<addr-line>Department of Mathematics, MIT, Cambridge, MA, United States of America</addr-line>
</aff>
<aff id="aff003">
<label>3</label>
<addr-line>Department of Computer Science, University of Illinois at Urbana-Champaign, Champaign, IL, United States of America</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Fröhlich</surname> <given-names>Holger</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>University of Bonn, Bonn-Aachen International Center for IT, GERMANY</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: HC BB JP. Performed the experiments: HC. Analyzed the data: HC. Wrote the paper: HC BB JP.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">bab@mit.edu</email> (BB); <email xlink:type="simple">jianpeng@illinois.edu</email> (JP)</corresp>
</author-notes>
<pub-date pub-type="collection">
<year>2016</year>
</pub-date>
<pub-date pub-type="epub">
<day>1</day>
<month>3</month>
<year>2016</year>
</pub-date>
<volume>11</volume>
<issue>3</issue>
<elocation-id>e0150611</elocation-id>
<history>
<date date-type="received">
<day>23</day>
<month>11</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>16</day>
<month>2</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Cho et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0150611"/>
<abstract>
<p>Reverse-engineering of biological networks is a central problem in systems biology. The use of intervention data, such as gene knockouts or knockdowns, is typically used for teasing apart causal relationships among genes. Under time or resource constraints, one needs to carefully choose which intervention experiments to carry out. Previous approaches for selecting most informative interventions have largely been focused on discrete Bayesian networks. However, continuous Bayesian networks are of great practical interest, especially in the study of complex biological systems and their quantitative properties. In this work, we present an efficient, information-theoretic active learning algorithm for Gaussian Bayesian networks (GBNs), which serve as important models for gene regulatory networks. In addition to providing linear-algebraic insights unique to GBNs, leading to significant runtime improvements, we demonstrate the effectiveness of our method on data simulated with GBNs and the DREAM4 network inference challenge data sets. Our method generally leads to faster recovery of underlying network structure and faster convergence to final distribution of confidence scores over candidate graph structures using the full data, in comparison to random selection of intervention experiments.</p>
</abstract>
<funding-group>
<funding-statement>The authors have no support or funding to report.</funding-statement>
</funding-group>
<counts>
<fig-count count="6"/>
<table-count count="0"/>
<page-count count="15"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>We provide the simulation data and an implementation of our algorithm in Supporting Information. Sachs et al. data is available as Supporting Online Material of their paper. DREAM4 data sets are publicly available at <ext-link ext-link-type="uri" xlink:href="http://dreamchallenges.org" xlink:type="simple">http://dreamchallenges.org</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Molecules in a living cell interact with each other in a coordinated fashion to carry out important biological functions. Building a rich network of these interactions can greatly facilitate our understanding of human diseases by providing useful mechanistic interpretations of various phenotypes. Recent advances in high-throughput technologies have given rise to numerous algorithms for reverse-engineering interaction networks from molecular observations, as they provide an efficient and systematic way of analyzing the molecular state of a large number of genes. One class of such interaction networks that has generated much interest in recent years is transcriptional gene regulatory networks, which specify the set of genes that influence a given gene’s expression level. This type of pattern can be naturally modeled in a causal graph or Bayesian network.</p>
<p>Bayesian networks provide a compact way of representing causal relationships among random variables [<xref ref-type="bibr" rid="pone.0150611.ref001">1</xref>]. Given a directed acyclic graph (DAG) over the variables of interest, an edge <italic>X</italic> → <italic>Y</italic> encodes a causal influence of <italic>X</italic> on <italic>Y</italic>. However, when the given data consists of only passive observations of the underlying system, the causal structure is only identifiable up to Markov equivalence classes. To overcome this limitation, intervention experiments, in which some variables are controlled to take specific values, can be used to guarantee full identifiability given enough data [<xref ref-type="bibr" rid="pone.0150611.ref002">2</xref>]. For example, intervention on <italic>X</italic> only affects the behavior of <italic>Y</italic> in <italic>X</italic> → <italic>Y</italic>, but not in <italic>X</italic> ← <italic>Y</italic>; otherwise, if given only observational data for <italic>X</italic> and <italic>Y</italic>, these two graphs are indistinguishable. The importance of interventions for inferring biological networks has been noted in numerous studies [<xref ref-type="bibr" rid="pone.0150611.ref003">3</xref>–<xref ref-type="bibr" rid="pone.0150611.ref006">6</xref>]. In practical settings, interventions are typically performed via gene knockouts or knockdowns, i.e., by completely or partially reducing the expression level of one or more genes using experimental perturbations.</p>
<p>A key insight behind active learning is that not every variable is equally informative when intervened. For instance, if <italic>X</italic> does not have any children in every graph of a Markov equivalence class, perturbing <italic>X</italic> will not lead to any visible impact that can further distinguish the graphs. Thus, when the number of experiments that can be performed is limited, it is important to choose interventions which are most informative. In particular, it is generally not feasible to perform all possible interventions when joint interventions of multiple variables are considered.</p>
<p>Several researchers have developed active learning frameworks for causal structure learning during the last decade. In the Bayesian setting, Tong and Koller [<xref ref-type="bibr" rid="pone.0150611.ref007">7</xref>] and Murphy [<xref ref-type="bibr" rid="pone.0150611.ref008">8</xref>] both proposed decision-theoretic frameworks based on the expected reduction in uncertainty over edge directions and the expected change in posterior distribution over graph structures, respectively. While these approaches have been shown to be effective, they have been studied only in the context of discrete Bayesian networks. However, most molecular measurements are continuous, and hence they are more naturally described using continuous Bayesian networks. Based on this motivation, there have been a number of papers in the network inference literature which use Gaussian Bayesian networks (GBNs) as the underlying model, in which each variable is continuous and is modeled as a function of its parents with added Gaussian noise [<xref ref-type="bibr" rid="pone.0150611.ref009">9</xref>–<xref ref-type="bibr" rid="pone.0150611.ref012">12</xref>]. We contribute to this line of work by deriving the first Bayesian active learning algorithm for GBNs, where the informativeness of each candidate intervention is estimated via Bayesian inference, treating the graph as a latent random variable, and the most informative intervention is chosen. In the non-Bayesian setting, Hauser et al. [<xref ref-type="bibr" rid="pone.0150611.ref013">13</xref>], Eberhardt [<xref ref-type="bibr" rid="pone.0150611.ref002">2</xref>], and He and Geng [<xref ref-type="bibr" rid="pone.0150611.ref014">14</xref>] proposed active learning algorithms based on graph-theoretic insights, where the goal is to orient the most number of undirected edges in a Markov equivalence class with an intervention. Notably, these approaches aim only to determine the direction of edges in a given undirected graph (skeleton) estimated from observational data, and thus cannot handle errors already incorporated into the skeleton as a result of limited sample sizes and noisy observations. In this regard, our approach makes more effective use of intervention data by using it to improve the skeleton in addition to determining causal directions.</p>
<p>In this paper, we derive an efficient active learning algorithm for biological networks based on the framework of Murphy [<xref ref-type="bibr" rid="pone.0150611.ref008">8</xref>]. In addition to introducing an optimization technique unique to GBNs that leads to significant runtime improvement, we empirically validate the effectiveness of our algorithm on two data sets. Our results support the potential of active learning for uncovering casual structure in continuous-valued biological networks. Furthermore, our work enables researchers to effectively prioritize higher order joint perturbation experiments in a principled manner. This ability has the potential to accelerate the discovery of causal interactions between proteins, which are fundamental to advancing translational medicine and refining our understanding of biological systems.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec003">
<title>Learning Gaussian Bayesian networks with interventions</title>
<sec id="sec004">
<title>Gaussian Bayesian networks</title>
<p>Let <inline-formula id="pone.0150611.e001"><alternatives><graphic id="pone.0150611.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e001" xlink:type="simple"/><mml:math display="inline" id="M1"><mml:mrow><mml:mi mathvariant="script">X</mml:mi> <mml:mo>=</mml:mo> <mml:mo>{</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi>X</mml:mi> <mml:mi>n</mml:mi></mml:msub> <mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> be a set of random variables and <inline-formula id="pone.0150611.e002"><alternatives><graphic id="pone.0150611.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e002" xlink:type="simple"/><mml:math display="inline" id="M2"><mml:mrow><mml:mi>G</mml:mi> <mml:mo>=</mml:mo> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">X</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> be a directed acyclic graph (DAG) over <inline-formula id="pone.0150611.e003"><alternatives><graphic id="pone.0150611.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e003" xlink:type="simple"/><mml:math display="inline" id="M3"><mml:mi mathvariant="script">X</mml:mi></mml:math></alternatives></inline-formula>, where <inline-formula id="pone.0150611.e004"><alternatives><graphic id="pone.0150611.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e004" xlink:type="simple"/><mml:math display="inline" id="M4"><mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">E</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> if and only if there is a directed edge from <italic>X</italic><sub><italic>i</italic></sub> to <italic>X</italic><sub><italic>j</italic></sub>. Let <inline-formula id="pone.0150611.e005"><alternatives><graphic id="pone.0150611.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e005" xlink:type="simple"/><mml:math display="inline" id="M5"><mml:mrow><mml:mi>P</mml:mi> <mml:msub><mml:mi>a</mml:mi> <mml:mi>G</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>{</mml:mo> <mml:mi>i</mml:mi> <mml:mo>∣</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>i</mml:mi> <mml:mo>,</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">E</mml:mi> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> be the parent set of <italic>X</italic><sub><italic>j</italic></sub> in <italic>G</italic>. In a Gaussian Bayesian network (GBN), the conditional probability distribution (CPD) of each variable given the parents is defined to be a linear Gaussian distribution:
<disp-formula id="pone.0150611.e006"><alternatives><graphic id="pone.0150611.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>X</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow> <mml:msub><mml:mi>X</mml:mi> <mml:mrow><mml:mi>P</mml:mi> <mml:msub><mml:mi>a</mml:mi> <mml:mi>G</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>P</mml:mi> <mml:msub><mml:mi>a</mml:mi> <mml:mi>G</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>X</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mspace width="0.277778em"/><mml:msubsup><mml:mi>σ</mml:mi> <mml:mrow><mml:mi>j</mml:mi></mml:mrow> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where <italic>X</italic><sub><italic>S</italic></sub> := {<italic>X</italic><sub><italic>i</italic></sub>}<sub><italic>i</italic> ∈ <italic>S</italic></sub>. Note <italic>m</italic><sub><italic>j</italic></sub> and <inline-formula id="pone.0150611.e007"><alternatives><graphic id="pone.0150611.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e007" xlink:type="simple"/><mml:math display="inline" id="M7"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula> represent the base level and conditional variance of <italic>X</italic><sub><italic>j</italic></sub>, respectively, and <italic>w</italic><sub><italic>ij</italic></sub> represents the weight of causal effect along the edge (<italic>i</italic>, <italic>j</italic>). For compactness, we denote the set of parameters {<italic>m</italic><sub><italic>j</italic></sub>}, {<italic>w</italic><sub><italic>ij</italic></sub>}, and <inline-formula id="pone.0150611.e008"><alternatives><graphic id="pone.0150611.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e008" xlink:type="simple"/><mml:math display="inline" id="M8"><mml:mrow><mml:mo>{</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> for a particular graph <italic>G</italic> as Θ<sup><italic>G</italic></sup>. A GBN model <italic>M</italic> = (<italic>G</italic>, Θ<sup><italic>G</italic></sup>) fully defines a joint probability density function (PDF) over <inline-formula id="pone.0150611.e009"><alternatives><graphic id="pone.0150611.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e009" xlink:type="simple"/><mml:math display="inline" id="M9"><mml:mi mathvariant="script">X</mml:mi></mml:math></alternatives></inline-formula> as a product of Gaussian PDFs, and the set of independence assumptions that the joint PDF satisfies is encoded in the structure <italic>G</italic>. Furthermore, it can be shown that the joint PDF defined by <italic>M</italic> is, in fact, multivariate normal.</p>
</sec>
<sec id="sec005">
<title>Structure learning with observational and intervention data</title>
<p>A standard approach to inferring Bayesian network structure from data involves defining a score that reflects how well a given graph explains the data and searching for high-scoring graphs in the space of DAGs or causal node orderings. Typically, a Markov chain Monte Carlo (MCMC) method based on random walks is used to explore the space of candidate graph structures and to select the highest-scoring graph structure. In this section, we describe a Bayesian scoring function, which evaluates the posterior probability of a structure given the data. This scoring function constitutes an important component of the active learning algorithm we will develop next.</p>
<p>Given an instance of <italic>observational</italic> data where every variable is observed, <bold><italic>x</italic></bold> = (<italic>x</italic><sub>1</sub>, …, <italic>x</italic><sub><italic>n</italic></sub>), the likelihood <italic>p</italic>(<bold><italic>x</italic></bold>|<italic>G</italic>, Θ<sup><italic>G</italic></sup>) of a GBN model <italic>M</italic> = (<italic>G</italic>, Θ<sup><italic>G</italic></sup>) can be expressed as
<disp-formula id="pone.0150611.e010"><alternatives><graphic id="pone.0150611.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e010" xlink:type="simple"/><mml:math display="block" id="M10"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>P</mml:mi> <mml:msub><mml:mi>a</mml:mi> <mml:mi>G</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mspace width="0.277778em"/><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
where <italic>N</italic>(⋅; <italic>μ</italic>, <italic>σ</italic><sup>2</sup>) is the normal PDF with mean <italic>μ</italic> and variance <italic>σ</italic><sup>2</sup>.</p>
<p>Under an intervention (e.g., gene knockout or RNAi), a subset of random variables in <inline-formula id="pone.0150611.e011"><alternatives><graphic id="pone.0150611.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e011" xlink:type="simple"/><mml:math display="inline" id="M11"><mml:mi mathvariant="script">X</mml:mi></mml:math></alternatives></inline-formula> are clamped at specific values and the remaining variables are assumed to be jointly sampled from a modified graph where the incoming edges of the clamped nodes are removed. Intuitively, this ensures that the nodes <italic>upstream</italic> of the clamped nodes are unaffected by the intervention. Let <italic>I</italic> denote the setup of an intervention experiment and <bold><italic>x</italic></bold> = (<italic>x</italic><sub>1</sub>, …, <italic>x</italic><sub><italic>n</italic></sub>) be the outcome. For each (<italic>i</italic>, <italic>c</italic><sub><italic>i</italic></sub>) ∈ <italic>I</italic>, the value of <italic>X</italic><sub><italic>i</italic></sub> is clamped at a constant <italic>c</italic><sub><italic>i</italic></sub> (i.e., <italic>x</italic><sub><italic>i</italic></sub> = <italic>c</italic><sub><italic>i</italic></sub>). The likelihood function <italic>p</italic>(<bold><italic>x</italic></bold>|<italic>I</italic>, <italic>G</italic>, Θ<sup><italic>G</italic></sup>) for an intervention data instance (<bold><italic>x</italic></bold>, <italic>I</italic>) is given by
<disp-formula id="pone.0150611.e012"><alternatives><graphic id="pone.0150611.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munder><mml:mo>∏</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mo>·</mml:mo> <mml:mo>)</mml:mo> <mml:mo>∉</mml:mo> <mml:mi>I</mml:mi></mml:mrow></mml:munder> <mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>x</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>P</mml:mi> <mml:msub><mml:mi>a</mml:mi> <mml:mi>G</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msub><mml:mi>x</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mspace width="0.222222em"/><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
The only difference from the observational case is that the product is now only over the nodes that are not clamped. When no variables are clamped (<italic>I</italic> = ∅), the above expression is consistent with <xref ref-type="disp-formula" rid="pone.0150611.e010">Eq (1)</xref>.</p>
<p>Now, let <italic>D</italic> be a sequence of <italic>m</italic> data instances, <bold><italic>x</italic></bold><sup>(1)</sup>, …, <bold><italic>x</italic></bold><sup>(<italic>m</italic>)</sup>, and <inline-formula id="pone.0150611.e013"><alternatives><graphic id="pone.0150611.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:mi mathvariant="script">I</mml:mi></mml:math></alternatives></inline-formula> be the sequence of corresponding experimental setups, <italic>I</italic><sup>(1)</sup>, …, <italic>I</italic><sup>(<italic>m</italic>)</sup>. This can be viewed as a collection of both observational (<italic>I</italic> = ∅) and intervention (<italic>I</italic> ≠ ∅) experiments. The <italic>complete likelihood function</italic> <inline-formula id="pone.0150611.e014"><alternatives><graphic id="pone.0150611.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e014" xlink:type="simple"/><mml:math display="inline" id="M14"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>D</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>G</mml:mi> <mml:mo>,</mml:mo> <mml:msup><mml:mo>Θ</mml:mo> <mml:mi>G</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> for the data set is given by
<disp-formula id="pone.0150611.e015"><alternatives><graphic id="pone.0150611.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:munder><mml:mo>∏</mml:mo> <mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mo>·</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∉</mml:mo> <mml:msup><mml:mi>I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:munder> <mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>;</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>P</mml:mi> <mml:msub><mml:mi>a</mml:mi> <mml:mi>G</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:mspace width="0.222222em"/><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>By arranging terms for each family (i.e., a node and its parents) across data instances, this can be rewritten as
<disp-formula id="pone.0150611.e016"><alternatives><graphic id="pone.0150611.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:munder><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>k</mml:mi> <mml:mo>:</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>,</mml:mo> <mml:mo>·</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∉</mml:mo> <mml:msup><mml:mi>I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:munder> <mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>;</mml:mo> <mml:msub><mml:mi>m</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>∈</mml:mo> <mml:mi>P</mml:mi> <mml:msub><mml:mi>a</mml:mi> <mml:mi>G</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>j</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder> <mml:msub><mml:mi>w</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:msubsup><mml:mi>x</mml:mi> <mml:mi>i</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>k</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:mspace width="0.222222em"/><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>The fact that the likelihood over intervention data still decomposes into family-specific terms (each over a mutually exclusive set of parameters) enables the use of a conjugate prior similar to the one introduced by Geiger and Heckerman [<xref ref-type="bibr" rid="pone.0150611.ref015">15</xref>] that gives us a closed-form expression for the posterior. Here we impose an <italic>independent</italic> normal-inverse Gamma prior over each set of family-specific parameters, <inline-formula id="pone.0150611.e017"><alternatives><graphic id="pone.0150611.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e017" xlink:type="simple"/><mml:math display="inline" id="M17"><mml:msubsup><mml:mo>Θ</mml:mo> <mml:mi>j</mml:mi> <mml:mi>G</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>, which consists of <italic>m</italic><sub><italic>j</italic></sub>, {<italic>w</italic><sub><italic>ij</italic></sub>}<sub><italic>i</italic> ∈ <italic>Pa</italic><sub><italic>G</italic></sub>(<italic>j</italic>)</sub>, and <inline-formula id="pone.0150611.e018"><alternatives><graphic id="pone.0150611.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:math></alternatives></inline-formula>. An advantage of this representation is that we are now able to compute the complete posterior scoring function by simply calculating the posterior for each family and multiplying them together.</p>
<p>Specifically, for each node <italic>j</italic>, let <bold><italic>θ</italic></bold><sub><italic>j</italic></sub> be a column vector (<italic>m</italic><sub><italic>j</italic></sub>, <italic>w</italic><sub><italic>p</italic><sub>1</sub> <italic>j</italic></sub>, …, <italic>w</italic><sub><italic>p</italic><sub><italic>d</italic></sub> <italic>j</italic></sub>) where <italic>p</italic><sub>1</sub>, …, <italic>p</italic><sub><italic>d</italic></sub> is an enumeration of elements in <italic>Pa</italic><sub><italic>G</italic></sub>(<italic>j</italic>). Let <italic>k</italic><sub>1</sub>, …, <italic>k</italic><sub><italic>t</italic></sub> be an enumeration of {<italic>k</italic> : <italic>j</italic> ∉ <italic>I</italic><sup>(<italic>k</italic>)</sup>} (i.e., instances where <italic>X</italic><sub><italic>j</italic></sub> is not clamped). We define a <italic>family-specific data set</italic> (<bold><italic>X</italic></bold><sub><italic>j</italic></sub>, <bold><italic>y</italic></bold><sub><italic>j</italic></sub>) for node <italic>j</italic> as
<disp-formula id="pone.0150611.e019"><alternatives><graphic id="pone.0150611.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e019" xlink:type="simple"/><mml:math display="block" id="M19"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:msubsup><mml:mi>x</mml:mi> <mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>k</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mtd> <mml:mtd><mml:mo>⋯</mml:mo></mml:mtd> <mml:mtd><mml:msubsup><mml:mi>x</mml:mi> <mml:msub><mml:mi>p</mml:mi> <mml:mi>d</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>k</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋮</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋱</mml:mo></mml:mtd> <mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd> <mml:mtd><mml:msubsup><mml:mi>x</mml:mi> <mml:msub><mml:mi>p</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>k</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mtd> <mml:mtd><mml:mo>⋯</mml:mo></mml:mtd> <mml:mtd><mml:msubsup><mml:mi>x</mml:mi> <mml:msub><mml:mi>p</mml:mi> <mml:mi>d</mml:mi></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>k</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo> <mml:mo>,</mml:mo> <mml:mspace width="4pt"/><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mo>[</mml:mo> <mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mi>x</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>k</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr> <mml:mtr><mml:mtd><mml:msubsup><mml:mi>x</mml:mi> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>k</mml:mi> <mml:mi>t</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable> <mml:mo>]</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
which depends on <italic>G</italic>, <italic>D</italic>, and <inline-formula id="pone.0150611.e020"><alternatives><graphic id="pone.0150611.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e020" xlink:type="simple"/><mml:math display="inline" id="M20"><mml:mi mathvariant="script">I</mml:mi></mml:math></alternatives></inline-formula>. Now, if we assume the following prior distribution for <inline-formula id="pone.0150611.e021"><alternatives><graphic id="pone.0150611.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e021" xlink:type="simple"/><mml:math display="inline" id="M21"><mml:msubsup><mml:mo>Θ</mml:mo> <mml:mi>j</mml:mi> <mml:mi>G</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>:
<disp-formula id="pone.0150611.e022"><alternatives><graphic id="pone.0150611.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mtd> <mml:mtd><mml:mo>∼</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mtext>Inv-Gamma</mml:mtext> <mml:mo>(</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd> <mml:mtd><mml:mo>∼</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>σ</mml:mi> <mml:mi>j</mml:mi> <mml:mn>2</mml:mn></mml:msubsup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
with hyperparameters <italic>α</italic><sub><italic>j</italic></sub>, <italic>β</italic><sub><italic>j</italic></sub>, <bold><italic>μ</italic></bold><sub><italic>j</italic></sub>, and <bold>Λ</bold><sub><italic>j</italic></sub>, then the posterior distribution <inline-formula id="pone.0150611.e023"><alternatives><graphic id="pone.0150611.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e023" xlink:type="simple"/><mml:math display="inline" id="M23"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mo>Θ</mml:mo> <mml:mi>j</mml:mi> <mml:mi>G</mml:mi></mml:msubsup> <mml:mo stretchy="false">|</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>G</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> has the same form as the prior, with the following updated parameters:
<disp-formula id="pone.0150611.e024"><alternatives><graphic id="pone.0150611.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>:</mml:mo> <mml:mo>=</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">X</mml:mi> <mml:mi>j</mml:mi> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(3)</label></disp-formula> <disp-formula id="pone.0150611.e025"><alternatives><graphic id="pone.0150611.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">μ</mml:mi> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>:</mml:mo> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">X</mml:mi> <mml:mi>j</mml:mi> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(4)</label></disp-formula> <disp-formula id="pone.0150611.e026"><alternatives><graphic id="pone.0150611.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e026" xlink:type="simple"/><mml:math display="block" id="M26"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>α</mml:mi> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>:</mml:mo> <mml:mo>=</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mfrac><mml:mrow><mml:mo stretchy="false">|</mml:mo> <mml:mi>D</mml:mi> <mml:mo stretchy="false">|</mml:mo></mml:mrow> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula> <disp-formula id="pone.0150611.e027"><alternatives><graphic id="pone.0150611.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e027" xlink:type="simple"/><mml:math display="block" id="M27"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>β</mml:mi> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>:</mml:mo> <mml:mo>=</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mn>2</mml:mn></mml:mfrac> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mi>j</mml:mi> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">μ</mml:mi> <mml:mi>j</mml:mi> <mml:mi>T</mml:mi></mml:msubsup> <mml:msub><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi mathvariant="bold-italic">μ</mml:mi> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mi>T</mml:mi></mml:msup> <mml:msubsup><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:msubsup><mml:mi mathvariant="bold-italic">μ</mml:mi> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula></p>
<p>Moreover, the <italic>marginal likelihood function</italic> <inline-formula id="pone.0150611.e028"><alternatives><graphic id="pone.0150611.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e028" xlink:type="simple"/><mml:math display="inline" id="M28"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>D</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>G</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, which usually requires a challenging step of integrating out the model parameters Θ<sup><italic>G</italic></sup> to compute, can now be analytically obtained as
<disp-formula id="pone.0150611.e029"><alternatives><graphic id="pone.0150611.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e029" xlink:type="simple"/><mml:math display="block" id="M29"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mn>2</mml:mn> <mml:mi>π</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mi>c</mml:mi> <mml:mo>(</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup> <mml:munderover><mml:mo>∏</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:msqrt><mml:mfrac><mml:mrow><mml:mo form="prefix" movablelimits="true">det</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo form="prefix" movablelimits="true">det</mml:mo> <mml:mo>(</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:msqrt> <mml:mo>·</mml:mo> <mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>β</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msub><mml:mi>α</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:msup> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mi>β</mml:mi> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:msubsup><mml:mi>α</mml:mi> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup></mml:msup></mml:mfrac> <mml:mo>·</mml:mo> <mml:mfrac><mml:mrow><mml:mo>Γ</mml:mo> <mml:mo>(</mml:mo> <mml:msubsup><mml:mi>α</mml:mi> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>Γ</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mi>α</mml:mi> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula>
where <inline-formula id="pone.0150611.e030"><alternatives><graphic id="pone.0150611.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e030" xlink:type="simple"/><mml:math display="inline" id="M30"><mml:mrow><mml:mi>c</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mrow><mml:mi>n</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi>D</mml:mi> <mml:mo stretchy="false">|</mml:mo></mml:mrow> <mml:mo>-</mml:mo> <mml:msub><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>I</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">I</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo stretchy="false">|</mml:mo> <mml:mi>I</mml:mi> <mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> is the sum of the number of unclamped variables in each data instance.</p>
<p>Since <inline-formula id="pone.0150611.e031"><alternatives><graphic id="pone.0150611.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e031" xlink:type="simple"/><mml:math display="inline" id="M31"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo> <mml:mo>∝</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>D</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>G</mml:mi> <mml:mo>)</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, given the analytical expression for marginal likelihood, one can explore the posterior distribution over the space of candidate graph structures using the Metropolis-Hastings (MH) algorithm [<xref ref-type="bibr" rid="pone.0150611.ref016">16</xref>, <xref ref-type="bibr" rid="pone.0150611.ref017">17</xref>]. Unfortunately, an in-depth discussion of different ways in which one can set up various components of this procedure, including the design of search space, prior over graphs, and proposal distribution, is out of the scope of this paper. The output of this algorithm is a set of sampled graph structures drawn from the posterior <inline-formula id="pone.0150611.e032"><alternatives><graphic id="pone.0150611.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e032" xlink:type="simple"/><mml:math display="inline" id="M32"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, which intuitively represents how strongly we believe each candidate graph structure to be the underlying model for the given data. This output can be summarized in a number of ways to construct the finalized model. The most common approach is to employ <italic>Bayesian model averaging</italic>, in which a feature of interest <italic>f</italic> (e.g., presence of edge) is averaged over all graph samples to obtain <inline-formula id="pone.0150611.e033"><alternatives><graphic id="pone.0150611.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e033" xlink:type="simple"/><mml:math display="inline" id="M33"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:mi>f</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
</sec>
</sec>
<sec id="sec006">
<title>Prioritizing interventions via active learning</title>
<p>Most network inference methods, including the one presented in the previous section, assume that the data set is obtained and fixed prior to learning. However, in a real world setting, one can perform additional intervention experiments and combine them with existing data to improve the quality of learned networks. An active learning framework allows us to reason about how <italic>informative</italic> each candidate experiment is, thus enabling a more efficient design of intervention experiments when subjected to time or resource constraints.</p>
<p>Here, we present our active learning algorithm for inferring the structure of GBNs. We adopt the information-theoretic framework developed by Murphy [<xref ref-type="bibr" rid="pone.0150611.ref008">8</xref>] and introduce an optimization based on linear-algebraic insights unique to GBNs which serve to improve the overall complexity of the algorithm over a naive implementation.</p>
<sec id="sec007">
<title>Greedy selection</title>
<p>Let <inline-formula id="pone.0150611.e034"><alternatives><graphic id="pone.0150611.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e034" xlink:type="simple"/><mml:math display="inline" id="M34"><mml:mi mathvariant="script">C</mml:mi></mml:math></alternatives></inline-formula> be the set of candidate intervention experiments. Following Murphy [<xref ref-type="bibr" rid="pone.0150611.ref008">8</xref>], we define the <italic>I</italic><sup>⋆</sup> to be the optimal experiment which maximizes the <italic>mutual information</italic> (MI) between the resultant outcome <inline-formula id="pone.0150611.e035"><alternatives><graphic id="pone.0150611.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e035" xlink:type="simple"/><mml:math display="inline" id="M35"><mml:mi mathvariant="script">X</mml:mi></mml:math></alternatives></inline-formula> and <italic>G</italic>, given the current data set <inline-formula id="pone.0150611.e036"><alternatives><graphic id="pone.0150611.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e036" xlink:type="simple"/><mml:math display="inline" id="M36"><mml:mrow><mml:mo>(</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>. In other words,
<disp-formula id="pone.0150611.e037"><alternatives><graphic id="pone.0150611.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e037" xlink:type="simple"/><mml:math display="block" id="M37"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>I</mml:mi> <mml:mo>⋆</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:msub><mml:mrow><mml:mo form="prefix">arg</mml:mo> <mml:mo form="prefix" movablelimits="true">max</mml:mo></mml:mrow> <mml:mrow><mml:mi>I</mml:mi> <mml:mo>∈</mml:mo> <mml:mi mathvariant="script">C</mml:mi></mml:mrow></mml:msub> <mml:mi>ψ</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
where the objective function <inline-formula id="pone.0150611.e038"><alternatives><graphic id="pone.0150611.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:mrow><mml:mi>ψ</mml:mi> <mml:mo>(</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo> <mml:mo>:</mml:mo> <mml:mo>=</mml:mo> <mml:mtext>MI</mml:mtext> <mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo>;</mml:mo> <mml:mi mathvariant="script">X</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> can be alternatively expressed in two different ways as
<disp-formula id="pone.0150611.e039"><alternatives><graphic id="pone.0150611.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e039" xlink:type="simple"/><mml:math display="block" id="M39"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi mathvariant="script">X</mml:mi> <mml:mo>∼</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mo>·</mml:mo> <mml:mo stretchy="false">|</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mtext>KL</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi mathvariant="script">X</mml:mi> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="4pt"/><mml:mo>∥</mml:mo> <mml:mspace width="4pt"/><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
and
<disp-formula id="pone.0150611.e040"><alternatives><graphic id="pone.0150611.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e040" xlink:type="simple"/><mml:math display="block" id="M40"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi>G</mml:mi> <mml:mo>∼</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mo>·</mml:mo> <mml:mo stretchy="false">|</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mrow><mml:mo>[</mml:mo> <mml:mtext>KL</mml:mtext> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="script">X</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi>G</mml:mi> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="4pt"/><mml:mo>∥</mml:mo> <mml:mspace width="4pt"/><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="script">X</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>]</mml:mo></mml:mrow> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
KL(⋅‖⋅) denotes the Kullback-Leibler divergence. <xref ref-type="disp-formula" rid="pone.0150611.e039">Eq (8)</xref> provides a useful insight that the optimal intervention is the one that is expected to cause the largest change (measured by divergence) in our belief over the candidate graph structures. On the other hand, <xref ref-type="disp-formula" rid="pone.0150611.e040">Eq (9)</xref> turns out to be easier to compute. In particular, based on <xref ref-type="disp-formula" rid="pone.0150611.e040">Eq (9)</xref>, <italic>ψ</italic>(<italic>I</italic>) can be expressed as
<disp-formula id="pone.0150611.e041"><alternatives><graphic id="pone.0150611.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e041" xlink:type="simple"/><mml:math display="block" id="M41"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi>G</mml:mi> <mml:mo>∼</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mo>·</mml:mo> <mml:mo stretchy="false">|</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mo>[</mml:mo> <mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi mathvariant="script">X</mml:mi> <mml:mo>∼</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mo>·</mml:mo> <mml:mo stretchy="false">|</mml:mo> <mml:mi>G</mml:mi> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mo>[</mml:mo> <mml:mo>Δ</mml:mo> <mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">X</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo> <mml:mo>]</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
where
<disp-formula id="pone.0150611.e042"><alternatives><graphic id="pone.0150611.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e042" xlink:type="simple"/><mml:math display="block" id="M42"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Δ</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">X</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mo form="prefix">log</mml:mo> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">X</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi>G</mml:mi> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mrow><mml:mi>G</mml:mi> <mml:mo>∼</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mo>·</mml:mo> <mml:mo stretchy="false">|</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msub> <mml:mo>[</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">X</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi>G</mml:mi> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo> <mml:mo>]</mml:mo></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
Apart from the expectations, the only term that needs to be evaluated is the marginal likelihood <inline-formula id="pone.0150611.e043"><alternatives><graphic id="pone.0150611.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">X</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi>G</mml:mi> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, for which we have an analytical expression as given in <xref ref-type="disp-formula" rid="pone.0150611.e029">Eq (7)</xref> (with <inline-formula id="pone.0150611.e044"><alternatives><graphic id="pone.0150611.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e044" xlink:type="simple"/><mml:math display="inline" id="M44"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> as the new prior).</p>
<p>Computing expectations over <italic>G</italic> and <inline-formula id="pone.0150611.e045"><alternatives><graphic id="pone.0150611.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:mi mathvariant="script">X</mml:mi></mml:math></alternatives></inline-formula> are both intractable, so we replace them with approximations based on random samples. Let <italic>G</italic><sub>1</sub>, …, <italic>G</italic><sub><italic>S</italic></sub> be random samples from the posterior distribution <inline-formula id="pone.0150611.e046"><alternatives><graphic id="pone.0150611.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, which can be obtained using an MCMC method as previously described. To avoid drawing separate samples of <inline-formula id="pone.0150611.e047"><alternatives><graphic id="pone.0150611.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e047" xlink:type="simple"/><mml:math display="inline" id="M47"><mml:mi mathvariant="script">X</mml:mi></mml:math></alternatives></inline-formula> for each graph sample for computational reasons, we use importance sampling for the inner expectation over <inline-formula id="pone.0150611.e048"><alternatives><graphic id="pone.0150611.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e048" xlink:type="simple"/><mml:math display="inline" id="M48"><mml:mi mathvariant="script">X</mml:mi></mml:math></alternatives></inline-formula> with a sampling distribution <inline-formula id="pone.0150611.e049"><alternatives><graphic id="pone.0150611.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:mrow><mml:mi>q</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">X</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> that is independent of <italic>G</italic>. In our experiments, we used <inline-formula id="pone.0150611.e050"><alternatives><graphic id="pone.0150611.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e050" xlink:type="simple"/><mml:math display="inline" id="M50"><mml:mrow><mml:mi>q</mml:mi> <mml:mo>:</mml:mo> <mml:mo>=</mml:mo> <mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="script">X</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:msup><mml:mi>G</mml:mi> <mml:mo>∘</mml:mo></mml:msup> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> where <italic>G</italic><sup>∘</sup> is the graph with no edges where every variable is independent. Letting <bold><italic>x</italic></bold><sub>1</sub>, …, <bold><italic>x</italic></bold><sub><italic>R</italic></sub> be random samples from <italic>q</italic>, <xref ref-type="disp-formula" rid="pone.0150611.e041">Eq (10)</xref> can be approximated as
<disp-formula id="pone.0150611.e051"><alternatives><graphic id="pone.0150611.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e051" xlink:type="simple"/><mml:math display="block" id="M51"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>S</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>S</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>r</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>R</mml:mi></mml:munderover> <mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>r</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo form="prefix">log</mml:mo> <mml:mo>(</mml:mo> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>r</mml:mi></mml:msub> <mml:mo stretchy="false">|</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:mi>S</mml:mi></mml:mfrac> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:msup><mml:mi>s</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>S</mml:mi></mml:msubsup> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>r</mml:mi></mml:msub> <mml:mo stretchy="false">|</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:msup><mml:mi>s</mml:mi> <mml:mo>′</mml:mo></mml:msup></mml:msub> <mml:mo>,</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac> <mml:mo>)</mml:mo> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula>
where <inline-formula id="pone.0150611.e052"><alternatives><graphic id="pone.0150611.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:mrow><mml:msub><mml:mi>v</mml:mi> <mml:mrow><mml:mi>r</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>r</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo>/</mml:mo> <mml:msubsup><mml:mo>∑</mml:mo> <mml:mrow><mml:msup><mml:mi>r</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>R</mml:mi></mml:msubsup> <mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:msup><mml:mi>r</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> with
<disp-formula id="pone.0150611.e053"><alternatives><graphic id="pone.0150611.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e053" xlink:type="simple"/><mml:math display="block" id="M53"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi> <mml:mo>˜</mml:mo></mml:mover> <mml:mrow><mml:mi>r</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub> <mml:mo>:</mml:mo> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>r</mml:mi></mml:msub> <mml:mo stretchy="false">|</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>q</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>r</mml:mi></mml:msub> <mml:mo stretchy="false">|</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
<p>The overall active learning procedure, with the optimization technique discussed in the following section, is outlined in Algorithm 1 and <xref ref-type="fig" rid="pone.0150611.g001">Fig 1</xref>. We provide a MATLAB implementation of our algorithm in <xref ref-type="supplementary-material" rid="pone.0150611.s002">S1 Code</xref>.</p>
<p><bold>Algorithm 1</bold> Active learning for GBN</p>
<p specific-use="line"><bold>Require</bold>: Candidate graph structures <inline-formula id="pone.0150611.e054"><alternatives><graphic id="pone.0150611.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e054" xlink:type="simple"/><mml:math display="inline" id="M54"><mml:mi mathvariant="script">G</mml:mi></mml:math></alternatives></inline-formula>, prior over graphs <italic>p</italic>(<italic>G</italic>), initial data set <inline-formula id="pone.0150611.e055"><alternatives><graphic id="pone.0150611.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e055" xlink:type="simple"/><mml:math display="inline" id="M55"><mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi>D</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="script">I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, candidate interventions <inline-formula id="pone.0150611.e056"><alternatives><graphic id="pone.0150611.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e056" xlink:type="simple"/><mml:math display="inline" id="M56"><mml:mi mathvariant="script">C</mml:mi></mml:math></alternatives></inline-formula>, number of nodes <italic>n</italic>, number of additional experiments to perform <italic>T</italic>, number of graph samples <italic>S</italic>, number of samples for experimental outcome <italic>R</italic></p>
<p specific-use="line"> Sample <inline-formula id="pone.0150611.e057"><alternatives><graphic id="pone.0150611.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e057" xlink:type="simple"/><mml:math display="inline" id="M57"><mml:mrow><mml:msubsup><mml:mi>G</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>G</mml:mi> <mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>∼</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:msup><mml:mi>D</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="script">I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> via MCMC</p>
<p specific-use="line"> <bold>for</bold> <italic>t</italic> = 1 <bold>to</bold> <italic>T</italic> <bold>do</bold></p>
<p specific-use="line">  <bold>for all</bold> <inline-formula id="pone.0150611.e058">
<alternatives>
<graphic id="pone.0150611.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e058" xlink:type="simple"/>
<mml:math display="inline" id="M58">
<mml:mrow>
<mml:mi>I</mml:mi>
<mml:mo>∈</mml:mo>
<mml:mi mathvariant="script">C</mml:mi>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula> <bold>do</bold></p>
<p specific-use="line">   Sample <bold><italic>x</italic></bold><sub>1</sub>, …, <bold><italic>x</italic></bold><sub><italic>R</italic></sub> from <inline-formula id="pone.0150611.e059"><alternatives><graphic id="pone.0150611.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e059" xlink:type="simple"/><mml:math display="inline" id="M59"><mml:mrow><mml:mi>q</mml:mi> <mml:mo>(</mml:mo> <mml:mi mathvariant="bold">x</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:msup><mml:mi>D</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="script">I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula></p>
<p specific-use="line">   <bold>for</bold> <italic>s</italic> = 1 <bold>to</bold> <italic>S</italic> <bold>do</bold></p>
<p specific-use="line">    <bold>for</bold> <italic>j</italic> = 1 <bold>to</bold> <italic>n</italic> <bold>do</bold></p>
<p specific-use="line">     Using Eqs (<xref ref-type="disp-formula" rid="pone.0150611.e024">3</xref>)–(<xref ref-type="disp-formula" rid="pone.0150611.e027">6</xref>), compute <italic>α</italic><sub><italic>j</italic></sub>, <italic>β</italic><sub><italic>j</italic></sub>, <bold><italic>μ</italic></bold><sub><italic>j</italic></sub>, and <bold>Λ</bold><sub><italic>j</italic></sub> of <inline-formula id="pone.0150611.e060"><alternatives><graphic id="pone.0150611.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e060" xlink:type="simple"/><mml:math display="inline" id="M60"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msubsup><mml:mo>Θ</mml:mo> <mml:mi>j</mml:mi> <mml:msubsup><mml:mi>G</mml:mi> <mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:msubsup> <mml:mo stretchy="false">|</mml:mo> <mml:msup><mml:mi>D</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="script">I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>G</mml:mi> <mml:mi>s</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula></p>
<p specific-use="line">     Compute <inline-formula id="pone.0150611.e061"><alternatives><graphic id="pone.0150611.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e061" xlink:type="simple"/><mml:math display="inline" id="M61"><mml:msubsup><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> and det(<bold>Λ</bold><sub><italic>j</italic></sub>)</p>
<p specific-use="line">    <bold>end for</bold></p>
<p specific-use="line">    <bold>for</bold> <italic>r</italic> = 1 <bold>to</bold> <italic>R</italic> <bold>do</bold></p>
<p specific-use="line">     Using Eqs (<xref ref-type="disp-formula" rid="pone.0150611.e029">7</xref>), (<xref ref-type="disp-formula" rid="pone.0150611.e072">13</xref>) and (<xref ref-type="disp-formula" rid="pone.0150611.e073">14</xref>), compute <inline-formula id="pone.0150611.e062"><alternatives><graphic id="pone.0150611.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e062" xlink:type="simple"/><mml:math display="inline" id="M62"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>r</mml:mi></mml:msub> <mml:mo stretchy="false">|</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msup><mml:mi>D</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="script">I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula></p>
<p specific-use="line">    <bold>end for</bold></p>
<p specific-use="line">   <bold>end for</bold></p>
<p specific-use="line">   Using <xref ref-type="disp-formula" rid="pone.0150611.e051">Eq (12)</xref>, estimate <italic>ψ</italic>(<italic>I</italic>)</p>
<p specific-use="line">  <bold>end for</bold></p>
<p specific-use="line">  <inline-formula id="pone.0150611.e063">
<alternatives>
<graphic id="pone.0150611.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e063" xlink:type="simple"/>
<mml:math display="inline" id="M63">
<mml:mrow>
<mml:msup>
<mml:mi>I</mml:mi>
<mml:mo>⋆</mml:mo>
</mml:msup>
<mml:mo>⇐</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mo form="prefix">arg</mml:mo>
<mml:mspace width="3.33333pt"/>
<mml:mo form="prefix" movablelimits="true">max</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mi>I</mml:mi>
<mml:mo>∈</mml:mo>
<mml:mi mathvariant="script">C</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mi>ψ</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>I</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</alternatives>
</inline-formula>
</p>
<p specific-use="line">  Perform experiment under <italic>I</italic><sup>⋆</sup>, record the outcome <bold><italic>x</italic></bold></p>
<p specific-use="line">  <italic>D</italic><sup>(<italic>t</italic>)</sup> ⇐ (<italic>D</italic><sup>(<italic>t</italic>−1)</sup>,<bold><italic>x</italic></bold>), <inline-formula id="pone.0150611.e064"><alternatives><graphic id="pone.0150611.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e064" xlink:type="simple"/><mml:math display="inline" id="M64"><mml:mrow><mml:msup><mml:mi mathvariant="script">I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>⇐</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mi mathvariant="script">I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi>I</mml:mi> <mml:mo>⋆</mml:mo></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula></p>
<p specific-use="line">  Sample <inline-formula id="pone.0150611.e065"><alternatives><graphic id="pone.0150611.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e065" xlink:type="simple"/><mml:math display="inline" id="M65"><mml:mrow><mml:msubsup><mml:mi>G</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>G</mml:mi> <mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>∼</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>G</mml:mi> <mml:mo stretchy="false">|</mml:mo> <mml:msup><mml:mi>D</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="script">I</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> via MCMC, initialize with <inline-formula id="pone.0150611.e066"><alternatives><graphic id="pone.0150611.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e066" xlink:type="simple"/><mml:math display="inline" id="M66"><mml:mrow><mml:msubsup><mml:mi>G</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>G</mml:mi> <mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>t</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula></p>
<p specific-use="line"> <bold>end for</bold></p>
<p specific-use="line"> <bold>return</bold> averaged model of <inline-formula id="pone.0150611.e067"><alternatives><graphic id="pone.0150611.e067g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e067" xlink:type="simple"/><mml:math display="inline" id="M67"><mml:mrow><mml:msubsup><mml:mi>G</mml:mi> <mml:mn>1</mml:mn> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>T</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup> <mml:mo>,</mml:mo> <mml:mo>⋯</mml:mo> <mml:mo>,</mml:mo> <mml:msubsup><mml:mi>G</mml:mi> <mml:mi>S</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>T</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula></p>
<fig id="pone.0150611.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150611.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Active learning framework for network reconstruction.</title>
<p>We first estimate our belief over candidate graph structures based on the initial data set that contains observational and/or intervention samples. Then, we iteratively acquire new data instances by carrying out the optimal intervention experiment predicted to cause the largest change in our belief (in expectation) and updating the belief. The final belief is summarized into a predicted network via Bayesian model averaging.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150611.g001" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec008">
<title>Efficient calculation of marginal likelihood</title>
<p>The computational bottleneck of our algorithm is in the evaluation of <inline-formula id="pone.0150611.e068"><alternatives><graphic id="pone.0150611.e068g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e068" xlink:type="simple"/><mml:math display="inline" id="M68"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>r</mml:mi></mml:msub> <mml:mo stretchy="false">|</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> for every combination of <italic>I</italic>, <italic>r</italic>, and <italic>s</italic>. This involves calculating the posterior parameters for <italic>G</italic><sub><italic>s</italic></sub> given <inline-formula id="pone.0150611.e069"><alternatives><graphic id="pone.0150611.e069g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e069" xlink:type="simple"/><mml:math display="inline" id="M69"><mml:mrow><mml:mo>(</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and also the updated posterior after observing (<bold><italic>x</italic></bold><sub><italic>r</italic></sub>, <italic>I</italic>). The former need only be computed once for each <italic>G</italic><sub><italic>s</italic></sub>. For the latter, the fact that only a single instance is added to the data set allows a more efficient computation of <inline-formula id="pone.0150611.e070"><alternatives><graphic id="pone.0150611.e070g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e070" xlink:type="simple"/><mml:math display="inline" id="M70"><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></alternatives></inline-formula> in <xref ref-type="disp-formula" rid="pone.0150611.e025">Eq (4)</xref> and <inline-formula id="pone.0150611.e071"><alternatives><graphic id="pone.0150611.e071g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e071" xlink:type="simple"/><mml:math display="inline" id="M71"><mml:mrow><mml:mo form="prefix" movablelimits="true">det</mml:mo> <mml:mo>(</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> in <xref ref-type="disp-formula" rid="pone.0150611.e029">Eq (7)</xref>. In particular, an application of the Sherman-Morrison formula and the matrix determinant lemma gives us:
<disp-formula id="pone.0150611.e072"><alternatives><graphic id="pone.0150611.e072g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e072" xlink:type="simple"/><mml:math display="block" id="M72"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>v</mml:mi> <mml:msup><mml:mi>v</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:msubsup><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mo>-</mml:mo> <mml:mfrac><mml:mrow><mml:msubsup><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mi>v</mml:mi> <mml:msup><mml:mi>v</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:msubsup><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>v</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:msubsup><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mi>v</mml:mi></mml:mrow></mml:mfrac> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula> <disp-formula id="pone.0150611.e073"><alternatives><graphic id="pone.0150611.e073g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e073" xlink:type="simple"/><mml:math display="block" id="M73"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo form="prefix" movablelimits="true">det</mml:mo> <mml:mo>(</mml:mo> <mml:msubsup><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi> <mml:mo>′</mml:mo></mml:msubsup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo form="prefix" movablelimits="true">det</mml:mo> <mml:mo>(</mml:mo> <mml:msub><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi>v</mml:mi> <mml:msup><mml:mi>v</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>(</mml:mo> <mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>v</mml:mi> <mml:mi>T</mml:mi></mml:msup> <mml:msubsup><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msubsup> <mml:mi>v</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo form="prefix" movablelimits="true">det</mml:mo> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mo mathvariant="bold">Λ</mml:mo> <mml:mi>j</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula>
where <italic>v</italic><sup><italic>T</italic></sup> is the row of the family-specific data set <bold><italic>X</italic></bold><sub><italic>j</italic></sub> that corresponds to the new outcome <bold><italic>x</italic></bold><sub><italic>r</italic></sub>. Essentially, by saving the inverse and determinant of <bold>Λ</bold><sub><italic>j</italic></sub> for each <italic>G</italic><sub><italic>s</italic></sub>, one can reduce the compute time of <inline-formula id="pone.0150611.e074"><alternatives><graphic id="pone.0150611.e074g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e074" xlink:type="simple"/><mml:math display="inline" id="M74"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi> <mml:mi>r</mml:mi></mml:msub> <mml:mo stretchy="false">|</mml:mo> <mml:mi>I</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi>G</mml:mi> <mml:mi>s</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> from <italic>O</italic>(<italic>md</italic><sup>2</sup>) to <italic>O</italic>(<italic>d</italic><sup>2</sup>) where <italic>m</italic> is the number of samples in the data and <italic>d</italic> is the upper bound on the number of parents each node can take.</p>
</sec>
<sec id="sec009">
<title>Evaluation of network reconstruction performance</title>
<p>We assessed the performance of our learning algorithm in several different ways. To analyze how accurately we learned the underlying causal structure, we followed the evaluation scheme used in the DREAM4 challenge [<xref ref-type="bibr" rid="pone.0150611.ref018">18</xref>] and calculated the area under receiver operating characteristic curve (AUROC) and the area under precision recall curve (AUPRC) based on a ranked list of edges. The absolute value of the expected maximum a posteriori (MAP) edge weight <inline-formula id="pone.0150611.e075"><alternatives><graphic id="pone.0150611.e075g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e075" xlink:type="simple"/><mml:math display="inline" id="M75"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi> <mml:mo>[</mml:mo> <mml:msubsup><mml:mover accent="true"><mml:mi>w</mml:mi> <mml:mo>^</mml:mo></mml:mover> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow> <mml:mtext>MAP</mml:mtext></mml:msubsup> <mml:mo stretchy="false">|</mml:mo> <mml:mi>D</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="script">I</mml:mi> <mml:mo>]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, approximated using graph samples from the posterior, was used as the score for each edge. On our simulated data, we also calculated the mean-squared error (MSE) of the expected MAP edge weights (over <italic>n</italic>(<italic>n</italic> − 1) possible edges) since we have access to the true parameters.</p>
<p>In addition to analyzing the trajectory of different accuracy measures over the course of the iterative learning procedure where one intervention experiment is added at a time, we also looked at a metric that is agnostic to whether we have access to the ground truth network. When we are given a data set with pre-generated interventions and their outcomes, we can retroactively evaluate, given any subset of the data set, how close we are to the final belief over candidate graph structures obtained using the whole data set. The final belief is expected to better reflect the ground truth, and thus faster convergence to the final belief is desirable in most cases. Intuitively, this evaluates how much information we lose if we only had enough resources to perform a small subset of the intervention experiments provided. We measure this by calculating the KL divergence of the final belief from the current belief over 5000 randomly chosen candidate graphs.</p>
</sec>
</sec>
</sec>
<sec id="sec010" sec-type="results">
<title>Results</title>
<sec id="sec011">
<title>GBNs can capture causal relationships in biological data</title>
<p>We first set out to test whether the model assumptions of GBNs (acyclicity and Gaussianity) are too restrictive to be effectively applied to real biological data. We ran our algorithm on gene expression data collected by Sachs et al. [<xref ref-type="bibr" rid="pone.0150611.ref005">5</xref>], which consists of 7,466 single cell expression profiles of 11 phosphorylated proteins involved in a signaling pathway of human primary T cells. A subset of measurements were taken from cells under perturbation induced by different reagents that activate/inhibit a particular protein in the pathway. We applied the same Bayesian structure learning algorithm for GBNs used in our framework to recover the ground truth signaling pathway (adopted from Sachs et al. [<xref ref-type="bibr" rid="pone.0150611.ref005">5</xref>]), and were able to predict causal links among the proteins with reasonable accuracy (0.65 AUROC and 0.30 AUPRC, averaged across five runs of MCMC). This shows that GBNs can detect edges in a real network despite the model assumptions. In addition, our inference algorithm outperformed GIES, a state-of-the-art non-Bayesian approach [<xref ref-type="bibr" rid="pone.0150611.ref019">19</xref>] for learning GBNs, providing further support for our Bayesian learning approach (<xref ref-type="fig" rid="pone.0150611.g002">Fig 2</xref>). Notably, the inclusion of intervention samples did not improve prediction accuracy on this data set. As previously pointed out by Mooij et al. [<xref ref-type="bibr" rid="pone.0150611.ref020">20</xref>], this odd behavior is likely due to the fact that the experimental perturbation employed by Sachs et al. [<xref ref-type="bibr" rid="pone.0150611.ref005">5</xref>] modifies the <italic>activity</italic> of the target protein instead of its <italic>abundance</italic>, which is the intended setting of our method. It is worth noting that, while Sachs et al. [<xref ref-type="bibr" rid="pone.0150611.ref005">5</xref>] reconstructs the ground truth network with greater accuracy, this is likely dependent on a carefully chosen discretization of the input data [<xref ref-type="bibr" rid="pone.0150611.ref019">19</xref>, <xref ref-type="bibr" rid="pone.0150611.ref020">20</xref>], which is precisely the type of tuning we aim to avoid by using <italic>continuous</italic> Bayesian networks.</p>
<fig id="pone.0150611.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150611.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Reconstruction performance on single cell gene expression data.</title>
<p>We applied our Bayesian structure learning algorithm based on GBNs to uncover the signaling pathway of 11 human proteins from expression data provided by Sachs et al. [<xref ref-type="bibr" rid="pone.0150611.ref005">5</xref>]. MAP estimates of edge weights calculated using 1,000 posterior graph samples are used to generate a ranked list of (directed) edges for evaluation of accuracy. The data points for GIES are taken from Hauser and Bühlmann [<xref ref-type="bibr" rid="pone.0150611.ref019">19</xref>] for comparison. The result suggests GBNs can uncover causal edges in real biological networks, and that our approach is more effective than GIES.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150611.g002" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec012">
<title>Active learning accelerates network reconstruction on simulated data</title>
<p>To demonstrate the effectiveness of our active learning algorithm, we randomly generated a GBN with 10 nodes (<xref ref-type="fig" rid="pone.0150611.g003">Fig 3</xref>) as ground truth and generated a collection of observational and intervention samples from the model. Given this simulated data, we set out to compare the reconstruction performance of an active learner with that of a random learner, which selects intervention experiments uniformly at random.</p>
<fig id="pone.0150611.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150611.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Reconstruction performance on simulated data from a GBN.</title>
<p>We compared edge prediction performance between active and random learners, summarized over five trials. The dotted lines are drawn at one standard deviation from the mean in each direction. Active learner achieves higher accuracy and faster convergence than random learner.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150611.g003" xlink:type="simple"/>
</fig>
<p>The parameters of the ground truth GBN are generated as follows. Each edge weight <italic>w</italic><sub><italic>ij</italic></sub> is uniformly sampled from (−1,−.25) ∪ (.25,1). The base level <italic>m</italic><sub><italic>j</italic></sub> of each node is sampled from <inline-formula id="pone.0150611.e076"><alternatives><graphic id="pone.0150611.e076g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0150611.e076" xlink:type="simple"/><mml:math display="inline" id="M76"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mn>1</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, and the noise level <italic>σ</italic><sub><italic>j</italic></sub> is set to 0.05 for all nodes. After populating the parameters, we sampled 10 observational instances to be used as the initial data set and ran both active and random learners until they iteratively selected 20 additional intervention experiments. Here, we only consider single variable knockout (clamping at zero) as possible interventions. For the convergence analysis, two instances of each knockout were pre-generated and the learners were limited to using them without replacement.</p>
<p>For the MH algorithm used for sampling graphs from the posterior distribution at each iteration, we used a proposal distribution that assigns uniform weight to each DAG in the neighborhood that is reachable by a single-edge insertion, deletion, or reversal, following the suggestions of Giudici et al. [<xref ref-type="bibr" rid="pone.0150611.ref021">21</xref>]. Also, <italic>p</italic>(<italic>G</italic>) was set to be uniform over DAGs with maximum in-degree of five; imposing a limit on the number of parents is a commonly used heuristic in the literature [<xref ref-type="bibr" rid="pone.0150611.ref022">22</xref>]. On the initial data set, we used a burn-in of 10,000 steps and thinning of 100 steps to obtain the first batch of graph samples. For the subsequent belief updates, we propagated each graph sample by 100 steps to obtain the new batch. Note that the change in posterior distribution after each iteration is relatively small because only one additional data instance is added. We used 1,000 graph samples and 100 experimental outcome samples (i.e., <italic>S</italic> = 1000, <italic>R</italic> = 100).</p>
<p>The results are summarized in <xref ref-type="fig" rid="pone.0150611.g003">Fig 3</xref>. We observe that our active learning algorithm achieves consistently higher accuracy than random learner across all three metrics (MSE, AUPRC, AUROC) after the first few iterations, leading to higher final accuracy overall. We also observe a faster convergence rate for our method. In particular, our algorithm achieved a belief that is close (divergence &lt; 1) to the final belief after seven interventions, while random learner reached the same level only after almost twice as many interventions.</p>
</sec>
<sec id="sec013">
<title>Active learning accelerates network reconstruction on DREAM4 benchmark data</title>
<p>We next asked whether we can achieve a similar improvement on a data set that more closely resembles biological data. To this end, we tested our method on data from the DREAM4 10-node in-silico network reconstruction challenge [<xref ref-type="bibr" rid="pone.0150611.ref018">18</xref>], which is a commonly used benchmark data for network inference algorithms. They provide five networks with different structures, all chosen to reflect common topological properties of real gene regulatory networks in <italic>E. coli</italic> or <italic>S. cerevisiae</italic>, which include feedback loops. Stochastic differential equations and a realistic noise model of microarray data sets are used to generate expression data from each network. We jointly considered the wild type and 10 multifactorial perturbation data as the initial observational data set (11 instances total), and ran active and random learners to prioritize 20 intervention samples, which consist of one knockout and one knockdown per gene. We made a simplifying assumption that the learner knows the resulting expression level of the target gene in a knockdown experiment. It is straightforward to properly address this uncertainty in a practical setting by taking the expectation with respect to the target variable using a sampling approach.</p>
<p>The results from the DREAM4 analysis are summarized in <xref ref-type="fig" rid="pone.0150611.g004">Fig 4</xref>. Since our method is based on acyclic graphs, we focused our analysis on data sets 4 and 5, which are generated from networks that contain fewer and weaker (i.e., longer) cycles than the remaining data sets. We observe a clear performance improvement by our active learning algorithm in terms of the speed at which we recover the underlying causal structure. Furthermore, the convergence rate of our method was consistently and significantly faster on both data sets. Note that the final accuracy of our method is comparable to earlier work that also applied GBNs to analyze the DREAM4 data set [<xref ref-type="bibr" rid="pone.0150611.ref012">12</xref>]. The results on data sets 1–3 along with their ground truth networks are provided in <xref ref-type="supplementary-material" rid="pone.0150611.s001">S1 Fig</xref>. In the case where the model assumption is heavily violated (i.e., there are relatively numerous and short cycles), our method still achieves significantly faster convergence to the final belief. However, due to the cyclic nature of these data sets, our method achieves generally lower final accuracies on these data sets and does not show a clear improvement over the random learner.</p>
<fig id="pone.0150611.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150611.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Reconstruction performance on DREAM4 benchmark data.</title>
<p>The results are summarized over five trials. The dotted lines are drawn at one standard deviation from the mean in each direction. Active learner achieves higher accuracy and faster convergence than random learner.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150611.g004" xlink:type="simple"/>
</fig>
<p>He and Geng [<xref ref-type="bibr" rid="pone.0150611.ref014">14</xref>] previously proposed an algorithm that prioritizes interventions to maximally orient the edges with ambiguous direction in a Markov-equivalence class, estimated by a standard network inference algorithm, PC [<xref ref-type="bibr" rid="pone.0150611.ref023">23</xref>]. Given enough observational samples, the PC algorithm recovers the graph structure up to Markov-equivalence based on conditional independence tests. We wish to emphasize that our Bayesian inference framework, unlike He and Geng’s approach, takes advantage of intervention samples not only for determining edge directions but also for refining the undirected skeleton of the graph. Such an approach is essential in a practical setting where the observational data is limited in both quantity and quality, which can lead to numerous incorrect or missing edges in the skeleton. We empirically observed in the DREAM4 data sets that our active learning method predicts the ground truth skeleton with higher accuracy than PC (<xref ref-type="fig" rid="pone.0150611.g005">Fig 5</xref>). Moreover, our method outperformed GIES [<xref ref-type="bibr" rid="pone.0150611.ref019">19</xref>] when applied to the full DREAM4 data (without prioritization). GIES employs a greedy search over candidate graphs taking both observational and intervention data into account. These results suggest that our learning approach more effectively uncovers the true graph structure than other methods developed for network inference based on intervention data.</p>
<fig id="pone.0150611.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150611.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Performance comparison with PC and GIES on DREAM4 data sets.</title>
<p>We evaluated the final prediction accuracy of our active learning algorithm in identifying edges in the undirected skeleton of the ground truth network. The resulting precision-recall (PR) curves were compared to PC with different values of <italic>α</italic> (significance level) in {0.01, 0.05, 0.1, 0.2, 0.3} using only observational data and to GIES using both observational and intervention data. We used the implementations of PC and GIES provided in the <monospace>pcalg</monospace> package in R. The dashed lines are drawn at one standard deviation from the mean in each direction based on five random trials. Our performance generally dominates that of PC and GIES, suggesting the effectiveness of our Bayesian learning approach.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150611.g005" xlink:type="simple"/>
</fig>
<p>Lastly, we tested the extent to which our optimization based on rank-one updates to the matrix inverse and determinant improves the runtime of our algorithm. The cumulative runtime of the iterative learning procedure on our simulated data is shown in <xref ref-type="fig" rid="pone.0150611.g006">Fig 6</xref>. Overall, our optimization is accountable for ∼30% reduction in runtime. We expect the improvement to be even more significant on data sets with more samples. Note that this analysis was conducted using a single 3.47 GHz Intel Xeon X5690 CPU for fairness of comparison even though our algorithm easily lends itself to parallelism and one can obtain significantly faster runtimes with multiple CPUs. However, despite our runtime improvement, we note that our method is currently intended only for small-scale networks (e.g., &lt;30 nodes), as is the case for most Bayesian network inference algorithms due to the super-exponential growth of the number of candidate graphs with respect to the number of nodes. We expect our method to be most effective for studies where practitioners aim to tease apart causal influences among a small set of genes or proteins of interest, such as a group of genes that belong to a specific biological process.</p>
<fig id="pone.0150611.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0150611.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Runtime improvement of our method on simulated data.</title>
<p>The results are summarized over three trials (error bands are not visible due to low variance). Our optimization technique specific to GBNs leads to significant improvement in runtime.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0150611.g006" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec014" sec-type="conclusions">
<title>Discussion</title>
<p>In this paper, we derived an efficient active learning algorithm for Gaussian Bayesian networks and demonstrated its effectiveness on several data sets. We showed that our algorithm achieves a clear improvement in uncovering the true network as long as the underlying causal structure does not significantly violate the acyclicity assumption inherent in the GBN models. Even under violation of model assumption, we were able to observe superior convergence rate of the active learner, which further supports the effectiveness of our method.</p>
<p>There are several important ways in which this work could be improved for better applicability in systems biology in the future. First, we could develop a systematic way of selecting a batch of intervention experiments to be performed simultaneously, which is a more suitable setup for high-throughput assays. Second, we could further adopt our method to support perturbation experiments in which we only observe the response of a single reporter gene, whose phenotype (e.g., luminescence) is easier to quantify than systematic expression profiling. Third, it would be interesting to look for better ways to find optimal intervention other than exhaustive enumeration followed by linear search for the optimal solution. This capability is especially of interest as we consider higher-order interventions of multiple variables, in order to counter the combinatorial explosion in the number of candidate interventions to consider.</p>
</sec>
<sec id="sec015">
<title>Supporting Information</title>
<supplementary-material id="pone.0150611.s001" mimetype="application/eps" position="float" xlink:href="info:doi/10.1371/journal.pone.0150611.s001" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Reconstruction performance on DREAM4 data sets 1–3.</title>
<p>Even when the ground truth network contains numerous short cycles, our method still achieves significantly faster convergence to the final belief. However, due to the violation of model assumption, our method achieves generally lower final accuracies than those of data sets 4 and 5 and does not clearly outperform random learner. The results are summarized over five trials. The dotted lines are drawn at one standard deviation from the mean in each direction.</p>
<p>(EPS)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0150611.s002" mimetype="application/zip" position="float" xlink:href="info:doi/10.1371/journal.pone.0150611.s002" xlink:type="simple">
<label>S1 Code</label>
<caption>
<title>MATLAB implementation of our algorithm with an example data set.</title>
<p>(ZIP)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pone.0150611.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pearl</surname> <given-names>J</given-names></name>. <article-title>Causality: models, reasoning and inference</article-title>. <source>Econometric Theory</source>. <year>2003</year>;<volume>19</volume>:<fpage>675</fpage>–<lpage>685</lpage>.</mixed-citation>
</ref>
<ref id="pone.0150611.ref002">
<label>2</label>
<mixed-citation publication-type="other" xlink:type="simple">Eberhardt F. Almost optimal intervention sets for causal discovery. Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence. 2012; p. 161–168.</mixed-citation>
</ref>
<ref id="pone.0150611.ref003">
<label>3</label>
<mixed-citation publication-type="other" xlink:type="simple">Markowetz F, Spang R. Evaluating the effect of perturbations in reconstructing network topologies. Proceedings of the 3rd International Workshop on Distributed Statistical Computing. 2003;2.</mixed-citation>
</ref>
<ref id="pone.0150611.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Pe’er</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Regev</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Elidan</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Friedman</surname> <given-names>N</given-names></name>. <article-title>Inferring subnetworks from perturbed expression profiles</article-title>. <source>Bioinformatics</source>. <year>2001</year>;<volume>17</volume>(<issue>suppl 1</issue>):<fpage>S215</fpage>–<lpage>S224</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/17.suppl_1.S215" xlink:type="simple">10.1093/bioinformatics/17.suppl_1.S215</ext-link></comment> <object-id pub-id-type="pmid">11473012</object-id></mixed-citation>
</ref>
<ref id="pone.0150611.ref005">
<label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sachs</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Perez</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Pe’er</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Lauffenburger</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Nolan</surname> <given-names>GP</given-names></name>. <article-title>Causal protein-signaling networks derived from multiparameter single-cell data</article-title>. <source>Science</source>. <year>2005</year>;<volume>308</volume>(<issue>5721</issue>):<fpage>523</fpage>–<lpage>529</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1126/science.1105809" xlink:type="simple">10.1126/science.1105809</ext-link></comment> <object-id pub-id-type="pmid">15845847</object-id></mixed-citation>
</ref>
<ref id="pone.0150611.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Werhli</surname> <given-names>AV</given-names></name>, <name name-style="western"><surname>Grzegorczyk</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Husmeier</surname> <given-names>D</given-names></name>. <article-title>Comparative evaluation of reverse engineering gene regulatory networks with relevance networks, graphical gaussian models and bayesian networks</article-title>. <source>Bioinformatics</source>. <year>2006</year>;<volume>22</volume>(<issue>20</issue>):<fpage>2523</fpage>–<lpage>2531</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/btl391" xlink:type="simple">10.1093/bioinformatics/btl391</ext-link></comment> <object-id pub-id-type="pmid">16844710</object-id></mixed-citation>
</ref>
<ref id="pone.0150611.ref007">
<label>7</label>
<mixed-citation publication-type="other" xlink:type="simple">Tong S, Koller D. Active learning for structure in Bayesian networks. Proceedings of the 17th International Joint Conference on Artificial Intelligence. 2001;2:863–869.</mixed-citation>
</ref>
<ref id="pone.0150611.ref008">
<label>8</label>
<mixed-citation publication-type="other" xlink:type="simple">Murphy KP. Active learning of causal Bayes net structure. Technical Report. 2001;.</mixed-citation>
</ref>
<ref id="pone.0150611.ref009">
<label>9</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Grzegorczyk</surname> <given-names>M</given-names></name>. <chapter-title>An introduction to Gaussian Bayesian networks</chapter-title>. In: <source>Systems Biology in Drug Discovery and Development</source>. <publisher-name>Springer</publisher-name>; <year>2010</year>. p. <fpage>121</fpage>–<lpage>147</lpage>.</mixed-citation>
</ref>
<ref id="pone.0150611.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Maathuis</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Kalisch</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bühlmann</surname> <given-names>P</given-names></name>. <article-title>Estimating high-dimensional intervention effects from observational data</article-title>. <source>The Annals of Statistics</source>. <year>2009</year>;<volume>37</volume>(<issue>6A</issue>):<fpage>3133</fpage>–<lpage>3164</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/09-AOS685" xlink:type="simple">10.1214/09-AOS685</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150611.ref011">
<label>11</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Maathuis</surname> <given-names>MH</given-names></name>, <name name-style="western"><surname>Colombo</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Kalisch</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Bühlmann</surname> <given-names>P</given-names></name>. <article-title>Predicting causal effects in large-scale systems from observational data</article-title>. <source>Nature Methods</source>. <year>2010</year>;<volume>7</volume>(<issue>4</issue>):<fpage>247</fpage>–<lpage>248</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth0410-247" xlink:type="simple">10.1038/nmeth0410-247</ext-link></comment> <object-id pub-id-type="pmid">20354511</object-id></mixed-citation>
</ref>
<ref id="pone.0150611.ref012">
<label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Rau</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Jaffrézic</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Nuel</surname> <given-names>G</given-names></name>. <article-title>Joint estimation of causal effects from observational and intervention gene expression data</article-title>. <source>BMC Systems Biology</source>. <year>2013</year>;<volume>7</volume>(<issue>1</issue>):<fpage>111</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1186/1752-0509-7-111" xlink:type="simple">10.1186/1752-0509-7-111</ext-link></comment> <object-id pub-id-type="pmid">24172639</object-id></mixed-citation>
</ref>
<ref id="pone.0150611.ref013">
<label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hauser</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bühlmann</surname> <given-names>P</given-names></name>. <article-title>Two optimal strategies for active learning of causal models from interventional data</article-title>. <source>International Journal of Approximate Reasoning</source>. <year>2014</year>;<volume>55</volume>(<issue>4</issue>):<fpage>926</fpage>–<lpage>939</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.ijar.2013.11.007" xlink:type="simple">10.1016/j.ijar.2013.11.007</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150611.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>He</surname> <given-names>YB</given-names></name>, <name name-style="western"><surname>Geng</surname> <given-names>Z</given-names></name>. <article-title>Active learning of causal networks with intervention experiments and optimal designs</article-title>. <source>Journal of Machine Learning Research</source>. <year>2008</year>;<volume>9</volume>(<issue>11</issue>).</mixed-citation>
</ref>
<ref id="pone.0150611.ref015">
<label>15</label>
<mixed-citation publication-type="other" xlink:type="simple">Geiger D, Heckerman D. Learning Gaussian networks. Proceedings of the 10th International Conference on Uncertainty in Artificial Intelligence. 1994;.</mixed-citation>
</ref>
<ref id="pone.0150611.ref016">
<label>16</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hastings</surname> <given-names>WK</given-names></name>. <article-title>Monte Carlo sampling methods using Markov chains and their applications</article-title>. <source>Biometrika</source>. <year>1970</year>;<volume>57</volume>(<issue>1</issue>):<fpage>97</fpage>–<lpage>109</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/biomet/57.1.97" xlink:type="simple">10.1093/biomet/57.1.97</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150611.ref017">
<label>17</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Metropolis</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Rosenbluth</surname> <given-names>AW</given-names></name>, <name name-style="western"><surname>Rosenbluth</surname> <given-names>MN</given-names></name>, <name name-style="western"><surname>Teller</surname> <given-names>AH</given-names></name>, <name name-style="western"><surname>Teller</surname> <given-names>E</given-names></name>. <article-title>Equation of state calculations by fast computing machines</article-title>. <source>Journal of Chemical Physics</source>. <year>1953</year>;<volume>21</volume>(<issue>6</issue>):<fpage>1087</fpage>–<lpage>1092</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1063/1.1699114" xlink:type="simple">10.1063/1.1699114</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150611.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Greenfield</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Madar</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ostrer</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Bonneau</surname> <given-names>R</given-names></name>. <article-title>DREAM4: Combining genetic and dynamic information to identify biological networks and dynamical models</article-title>. <source>PLoS ONE</source>. <year>2010</year>;<volume>5</volume>:<fpage>13397</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0013397" xlink:type="simple">10.1371/journal.pone.0013397</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150611.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Hauser</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Bühlmann</surname> <given-names>P</given-names></name>. <article-title>Jointly interventional and observational data: estimation of interventional Markov equivalence classes of directed acyclic graphs</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>. <year>2015</year>;<volume>77</volume>(<issue>1</issue>):<fpage>291</fpage>–<lpage>318</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/rssb.12071" xlink:type="simple">10.1111/rssb.12071</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150611.ref020">
<label>20</label>
<mixed-citation publication-type="other" xlink:type="simple">Mooij JM, Heskes T. Cyclic causal discovery from continuous equilibrium data. Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence. 2013;.</mixed-citation>
</ref>
<ref id="pone.0150611.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Giudici</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Castelo</surname> <given-names>R</given-names></name>. <article-title>Improving Markov chain Monte Carlo model search for data mining</article-title>. <source>Machine learning</source>. <year>2003</year>;<volume>50</volume>(<issue>1–2</issue>):<fpage>127</fpage>–<lpage>158</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1023/A:1020202028934" xlink:type="simple">10.1023/A:1020202028934</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0150611.ref022">
<label>22</label>
<mixed-citation publication-type="other" xlink:type="simple">Friedman N, Nachman I, Pe’er D. Learning Bayesian network structure from massive datasets: the “sparse candidate” algorithm. Proceedings of the 15th Conference on Uncertainty in Artificial Intelligence. 1999;.</mixed-citation>
</ref>
<ref id="pone.0150611.ref023">
<label>23</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Spirtes</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Glymour</surname> <given-names>CN</given-names></name>, <name name-style="western"><surname>Scheines</surname> <given-names>R</given-names></name>. <source>Causation, prediction, and search</source>. vol. <volume>81</volume>. <publisher-name>MIT press</publisher-name>; <year>2000</year>.</mixed-citation>
</ref>
</ref-list>
</back>
</article>