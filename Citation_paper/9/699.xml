<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article article-type="research-article" dtd-version="3.0" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-15-51364</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0149958</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject><subj-group><subject>Normal distribution</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Diagnostic medicine</subject><subj-group><subject>Clinical laboratory sciences</subject><subj-group><subject>Forensics</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Law and legal sciences</subject><subj-group><subject>Forensics</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Random variables</subject><subj-group><subject>Covariance</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability distribution</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Sociology</subject><subj-group><subject>Criminology</subject><subj-group><subject>Crime</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Random variables</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Probability theory</subject><subj-group><subject>Probability density</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Physical sciences</subject><subj-group><subject>Mathematics</subject><subj-group><subject>Operator theory</subject><subj-group><subject>Kernel functions</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Gaussian Mixture Models of Between-Source Variation for Likelihood Ratio Computation from Multivariate Data</article-title>
<alt-title alt-title-type="running-head">GMMs of Between-Source Variation for LR Computation from Multivariate Data</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Franco-Pedroso</surname> <given-names>Javier</given-names></name>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Ramos</surname> <given-names>Daniel</given-names></name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Gonzalez-Rodriguez</surname> <given-names>Joaquin</given-names></name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
</contrib-group>
<aff id="aff001">
<addr-line>ATVS-Biometric Recognition Group, Universidad Autonoma de Madrid, Madrid, Spain</addr-line>
</aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Han</surname> <given-names>Gang</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1">
<addr-line>Taxas A&amp;M University, UNITED STATES</addr-line>
</aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: JFP DR JGR. Performed the experiments: JFP. Analyzed the data: JFP DR. Contributed reagents/materials/analysis tools: JFP DR. Wrote the paper: JFP DR JGR.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">javier.franco@uam.es</email></corresp>
</author-notes>
<pub-date pub-type="collection">
<year>2016</year>
</pub-date>
<pub-date pub-type="epub">
<day>22</day>
<month>2</month>
<year>2016</year>
</pub-date>
<volume>11</volume>
<issue>2</issue>
<elocation-id>e0149958</elocation-id>
<history>
<date date-type="received">
<day>25</day>
<month>11</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>27</day>
<month>1</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-year>2016</copyright-year>
<copyright-holder>Franco-Pedroso et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0149958"/>
<abstract>
<p>In forensic science, trace evidence found at a crime scene and on suspect has to be evaluated from the measurements performed on them, usually in the form of multivariate data (for example, several chemical compound or physical characteristics). In order to assess the strength of that evidence, the likelihood ratio framework is being increasingly adopted. Several methods have been derived in order to obtain likelihood ratios directly from univariate or multivariate data by modelling both the variation appearing between observations (or features) coming from the same source (within-source variation) and that appearing between observations coming from different sources (between-source variation). In the widely used multivariate kernel likelihood-ratio, the within-source distribution is assumed to be normally distributed and constant among different sources and the between-source variation is modelled through a kernel density function (KDF). In order to better fit the observed distribution of the between-source variation, this paper presents a different approach in which a Gaussian mixture model (GMM) is used instead of a KDF. As it will be shown, this approach provides better-calibrated likelihood ratios as measured by the log-likelihood ratio cost (<italic>C</italic><sub><italic>llr</italic></sub>) in experiments performed on freely available forensic datasets involving different trace evidences: inks, glass fragments and car paints.</p>
</abstract>
<funding-group>
<funding-statement>JFP recieved funding from "Ministerio de Economia y Competitividad (ES)" (<ext-link ext-link-type="uri" xlink:href="http://www.mineco.gob.es/" xlink:type="simple">http://www.mineco.gob.es/</ext-link>) through the project "CMC-V2: Caracterizacion, Modelado y Compensacion de Variabilidad en la Señal de Voz", with grant number TEC2012-37585-C02-01. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="14"/>
<table-count count="5"/>
<page-count count="25"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>Glass-fragments dataset is from the article "Aitken, C. G. G., Lucy, D. Evaluation of trace evidence in the form of multivariate data. Journal of the Royal Statistical Society: Series C (Applied Statistics). 2004;53:109–122. doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1046/j.0035-9254.2003.05271.x" xlink:type="simple">10.1046/j.0035-9254.2003.05271.x</ext-link>" and can be downloaded from: <ext-link ext-link-type="uri" xlink:href="http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1467-9876/homepage/glass-data.txt" xlink:type="simple">http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1467-9876/homepage/glass-data.txt</ext-link> Inks and car-paints datasets are from the book "Grzegorz Zadora, Agnieszka Martyna, Daniel Ramos, Colin Aitken. Statistical 515 Analysis in Forensic Science: Evidential Values of Multivariate Physicochemical 516 Data. Wiley; January 2014." and can be downloaded from: <ext-link ext-link-type="uri" xlink:href="http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470972106.html" xlink:type="simple">http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470972106.html</ext-link>.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>A likelihood ratio represents a ratio of likelihoods between two competing hypothesis. In the context of forensic science, these two hypotheses are that of the prosecution, <italic>H</italic><sub><italic>p</italic></sub> (for instance, the suspect originated the crime scene mark), and that of the defence, <italic>H</italic><sub><italic>d</italic></sub> (for instance, the suspect is not the origin of the crime scene mark). If some samples of a given material coming from a known source (<italic>control</italic> data) and some others coming from an unknown source (<italic>recovered</italic> data) are given, both known as <italic>the evidence</italic> (<italic>E</italic>), and some other information (<italic>I</italic>) related to the crime is available, the trier of fact (judge or jury) looks for the ratio between the probabilities of the <italic>H</italic><sub><italic>p</italic></sub> and <italic>H</italic><sub><italic>d</italic></sub> hypotheses given by
<disp-formula id="pone.0149958.e001"><alternatives><graphic id="pone.0149958.e001g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e001" xlink:type="simple"/><mml:math display="block" id="M1"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mi>p</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>E</mml:mi> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mi>d</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>E</mml:mi> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(1)</label></disp-formula>
expressing the relative strength of one hypothesis versus the other.</p>
<p>However, the role of the forensic scientist must be restricted to evaluate the likelihood of the evidence assuming that any of the competing hypothesis is true, and it is not the evaluation of any other information different from that needed to evaluate the strength of the evidence. Using Bayesian theory, the above described ratio can be decomposed in the following way:
<disp-formula id="pone.0149958.e002"><alternatives><graphic id="pone.0149958.e002g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e002" xlink:type="simple"/><mml:math display="block" id="M2"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mi>p</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>E</mml:mi> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mi>d</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>E</mml:mi> <mml:mo>,</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mi>p</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mi>d</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>·</mml:mo> <mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>E</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mi>p</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:mi>E</mml:mi> <mml:mo>|</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mi>d</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mi>p</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mi>P</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mi>H</mml:mi> <mml:mi>d</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:mi>I</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac> <mml:mo>·</mml:mo> <mml:mi>L</mml:mi> <mml:mi>R</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(2)</label></disp-formula>
making a clear separation of the role of the forensic scientist and that of the judge or jury. Thus, the likelihood ratio (<italic>LR</italic>) strengthens (<italic>LR</italic> &gt; 1) or weakens (<italic>LR</italic> &lt; 1) the probabilities of the propositions, in the light of the newly observed evidence. In the process of assigning/computing the <italic>LR</italic>, additional data, usually known in forensics as <italic>background population</italic>, is needed to obtain the likelihood of the parameters for the model used.</p>
<p>A possible statement of the hypotheses at the source level [<xref ref-type="bibr" rid="pone.0149958.ref001">1</xref>] is:</p>
<list list-type="bullet">
<list-item>
<p><italic>H</italic><sub><italic>p</italic></sub>: the samples found at the crime scene and those obtained from the suspect come from a common source.</p>
</list-item>
<list-item>
<p><italic>H</italic><sub><italic>d</italic></sub>: the samples found at the crime scene and those obtained from the suspect come from different sources.</p>
</list-item>
</list>
<p>Other forms of the hypotheses are possible [<xref ref-type="bibr" rid="pone.0149958.ref001">1</xref>], but the analysis is outside the scope of this paper.</p>
<p>Likelihood ratios can be either directly derived from the data through the application of some probabilistic models (also known as feature-based LRs) or by transforming simple raw scores from a recognition system through a calibration step [<xref ref-type="bibr" rid="pone.0149958.ref002">2</xref>] (also known as score-based LRs). The score-based approach has been mainly used for biometric systems [<xref ref-type="bibr" rid="pone.0149958.ref003">3</xref>], in which the pattern recognition process does not follow a probabilistic model but a pattern matching procedure [<xref ref-type="bibr" rid="pone.0149958.ref004">4</xref>], the assumed conditions does not exactly hold (e.g. observations are not i.i.d. or do not follow a normal distribution), or the number of dimensions in the feature space makes the problem intractable (e.g. image vectors [<xref ref-type="bibr" rid="pone.0149958.ref005">5</xref>] or GMM-means supervectors [<xref ref-type="bibr" rid="pone.0149958.ref006">6</xref>]). However, recent approaches in face and speaker recognition modalities have begun to apply probabilistic methods with the aid of dimensionality reduction techniques [<xref ref-type="bibr" rid="pone.0149958.ref007">7</xref>–<xref ref-type="bibr" rid="pone.0149958.ref009">9</xref>]. On the other hand, the feature-based approach is usually followed in applied statistics to forensic science [<xref ref-type="bibr" rid="pone.0149958.ref010">10</xref>–<xref ref-type="bibr" rid="pone.0149958.ref012">12</xref>], where the observations are quite stable features whose within-source variation can be modelled by a normal distribution (for instance, measurements of the concentration of some chemical compounds).</p>
<p>A widely used approach within forensics [<xref ref-type="bibr" rid="pone.0149958.ref012">12</xref>–<xref ref-type="bibr" rid="pone.0149958.ref014">14</xref>] is that presented in [<xref ref-type="bibr" rid="pone.0149958.ref010">10</xref>], where the likelihood ratio is computed from multivariate data through the application of a two-level random effect model taking into account the variation <italic>i</italic>) between samples coming from the same source, known as <italic>within-source</italic> variation, and <italic>ii</italic>) between samples coming from different sources, known as <italic>between-source</italic> variation. Within-source variation is taken to be constant and normally distributed, and expressions for both normal and non-normal distribution for the between-source variation are given. When a normal distribution can not be assumed for the between-source variation, a kernel density function (KDF) [<xref ref-type="bibr" rid="pone.0149958.ref015">15</xref>] is used. However, as it will be shown, this KDF approach overestimates the between-source density function in some areas of the feature space for datasets where sources are grouped in several clusters.</p>
<p>In order to avoid this problem, an alternative approach is presented in this work, in which the between-source distribution is represented by means of a Gaussian mixture model (GMM) [<xref ref-type="bibr" rid="pone.0149958.ref016">16</xref>, <xref ref-type="bibr" rid="pone.0149958.ref017">17</xref>], whose parameters are obtained through a maximum-likelihood (ML) criterion, with the aim of obtaining a better representation of how the parameter being modelled (sources mean) varies across the different sources observed in the background population. As being also a probabilistic method for clustering data, GMMs provide a better representation of such kind of datasets, which leads to obtain better calibrated likelihood ratios.</p>
<p>The rest of the paper is organized as follows. In Section [<xref ref-type="sec" rid="sec002">Likelihood ratio computation</xref>], the likelihood ratio computation method is presented and the generative model defined. Section [<xref ref-type="sec" rid="sec004">Models for between-source distribution</xref>] describes the expressions to be used for a normally distributed between-source variation and those to be used when it is represented by means of a Gaussian mixture; for this latter case, the KDF expression used in [<xref ref-type="bibr" rid="pone.0149958.ref010">10</xref>] is also shown. In Section [<xref ref-type="sec" rid="sec007">GMMs for non-normal between-source distributions</xref>], the GMM training process is described, and the differences between using the KDF and the GMM approaches are highlighted. Section [<xref ref-type="sec" rid="sec011">Experimental framework</xref>] describes the forensic databases, the experimental protocols and the evaluation metrics, while the results are presented and discussed in Section [<xref ref-type="sec" rid="sec015">Results and Discussion</xref>]. Finally, conclusions are drawn in Section [<xref ref-type="sec" rid="sec019">Conclusions</xref>].</p>
</sec>
<sec id="sec002">
<title>Likelihood ratio computation</title>
<p>In order to compute the likelihood ratio, the probability of the evidence has to be evaluated under the two competing hypothesis, <italic>H</italic><sub><italic>p</italic></sub> and <italic>H</italic><sub><italic>d</italic></sub>, where the evidence consists in both the control (<bold>y</bold><sub>1</sub>) and the recovered (<bold>y</bold><sub>2</sub>) datasets (see the mathematical notation given in the [<xref ref-type="sec" rid="sec020">Appendix</xref>]). If <italic>H</italic><sub><italic>p</italic></sub> is assumed true, the joint probability of both datasets has to be evaluated; on the other hand, if <italic>H</italic><sub><italic>d</italic></sub> is assumed true, each dataset is generated from a different source and hence they are independent.</p>
<disp-formula id="pone.0149958.e003"><alternatives><graphic id="pone.0149958.e003g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e003" xlink:type="simple"/><mml:math display="block" id="M3"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>L</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mtext>y</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>y</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mtext>y</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>y</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mtext>y</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>y</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext>y</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>·</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext>y</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(3)</label></disp-formula>
<p>If a generative model with parameters Λ for the observed samples is assumed, the Bayesian solution is obtained by integrating out these parameters (if they vary from one source to another) for a given distribution which is usually obtained from a background population dataset, <italic>p</italic>(Λ|<bold>X</bold>).</p>
<disp-formula id="pone.0149958.e004"><alternatives><graphic id="pone.0149958.e004g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e004" xlink:type="simple"/><mml:math display="block" id="M4"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mtext>y</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>y</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext>y</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>·</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext>y</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mo>∫</mml:mo><mml:mo>Λ</mml:mo></mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext>y</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>y</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:mo>Λ</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>Λ</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi><mml:mo>Λ</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mo>∫</mml:mo><mml:mo>Λ</mml:mo></mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext>y</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:mo>Λ</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext>y</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:mo>Λ</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mo>Λ</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi><mml:mo>Λ</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(4)</label></disp-formula>
<p>Final expressions for the numerator and denominator of the likelihood ratio will depend on the assumed generative model, which defines both the parameters Λ and the specific density functions. In this Section, we will describe the generative model used in [<xref ref-type="bibr" rid="pone.0149958.ref010">10</xref>], and the within-source distribution will be defined.</p>
<sec id="sec003">
<title>The generative model</title>
<p>The two-level random effect model [<xref ref-type="bibr" rid="pone.0149958.ref018">18</xref>] used in [<xref ref-type="bibr" rid="pone.0149958.ref010">10</xref>] can be seen as a generative model in which a particular observed feature vector <bold>x</bold><sub><italic>ij</italic></sub> coming from source <italic>i</italic> is generated through
<disp-formula id="pone.0149958.e005"><alternatives><graphic id="pone.0149958.e005g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e005" xlink:type="simple"/><mml:math display="block" id="M5"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mtext>x</mml:mtext> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>ψ</mml:mi> <mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(5)</label></disp-formula>
where <italic>θ</italic><sub><italic>i</italic></sub> is a realization of the source random variable Θ and <italic>ψ</italic><sub><italic>j</italic></sub> is a realization of the additive random noise <italic>Ψ</italic> representing its within-source variation. This noisy term is taken to be constant among different sources and randomly distributed following
<disp-formula id="pone.0149958.e006"><alternatives><graphic id="pone.0149958.e006g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e006" xlink:type="simple"/><mml:math display="block" id="M6"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>Ψ</mml:mo> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(6)</label></disp-formula>
where <bold>W</bold> is the within-source covariance matrix. Thus, the conditional distribution of the random variable X<sub><italic>i</italic></sub> (from which <bold>x</bold><sub><italic>ij</italic></sub> is drawn), given a particular source <italic>i</italic>, follows a normal distribution with mean <italic>θ</italic><sub><italic>i</italic></sub> and covariance matrix <bold>W</bold> <disp-formula id="pone.0149958.e007"><alternatives><graphic id="pone.0149958.e007g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e007" xlink:type="simple"/><mml:math display="block" id="M7"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="normal">X</mml:mi> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>=</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(7)</label></disp-formula></p>
<p>Within-source covariance matrix can be computed from a background population dataset, comprising <italic>N</italic> = <italic>m</italic> ⋅ <italic>n</italic> samples coming from <italic>m</italic> different sources, through
<disp-formula id="pone.0149958.e008"><alternatives><graphic id="pone.0149958.e008g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e008" xlink:type="simple"/><mml:math display="block" id="M8"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi mathvariant="bold">S</mml:mi> <mml:mi>w</mml:mi></mml:msub> <mml:mrow><mml:mi>N</mml:mi> <mml:mo>-</mml:mo> <mml:mi>m</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(8)</label></disp-formula>
being <bold>S</bold><sub><italic>w</italic></sub> the within-source scatter matrix given by
<disp-formula id="pone.0149958.e009"><alternatives><graphic id="pone.0149958.e009g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e009" xlink:type="simple"/><mml:math display="block" id="M9"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold">S</mml:mi> <mml:mi>w</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>x</mml:mtext> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>x</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>x</mml:mtext> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>x</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mi mathvariant="normal">T</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(9)</label></disp-formula>
where <inline-formula id="pone.0149958.e010"><alternatives><graphic id="pone.0149958.e010g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e010" xlink:type="simple"/><mml:math display="inline" id="M10"><mml:msub><mml:mover accent="true"><mml:mtext>x</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula> is the average of a set of <italic>n</italic> feature vectors from source <italic>i</italic>.</p>
<p>As the assumed generative model has only one varying parameter, <italic>θ</italic>, characterizing the particular source, and the observed samples are assumed i.i.d. conditioned on the knowledge of <italic>θ</italic>, the numerator and the denominator of the likelihood ratio given in <xref ref-type="disp-formula" rid="pone.0149958.e004">Eq 4</xref> can be expressed, respectively, by
<disp-formula id="pone.0149958.e011"><alternatives><graphic id="pone.0149958.e011g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e011" xlink:type="simple"/><mml:math display="block" id="M11"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mo>∫</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>1</mml:mn></mml:msub> <mml:mo>|</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>2</mml:mn></mml:msub> <mml:mo>|</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">X</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(10)</label></disp-formula>
where the parameter <italic>θ</italic> jointly varies for both control and recovered conditional probabilities, as they are assumed to come from the same source (say <italic>θ</italic><sub>1</sub> = <italic>θ</italic><sub>2</sub> = <italic>θ</italic>), and
<disp-formula id="pone.0149958.e012"><alternatives><graphic id="pone.0149958.e012g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e012" xlink:type="simple"/><mml:math display="block" id="M12"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mo>∫</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>1</mml:mn></mml:msub> <mml:mo>|</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">X</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>θ</mml:mi> <mml:mo>×</mml:mo> <mml:msub><mml:mo>∫</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>2</mml:mn></mml:msub> <mml:mo>|</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">X</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(11)</label></disp-formula>
where these conditional probabilities can be integrated out independently as they are assumed to come from different sources (say <italic>θ</italic><sub>1</sub> ≠ <italic>θ</italic><sub>2</sub>).</p>
<p>Similarly to the random variable <italic>X</italic><sub><italic>ij</italic></sub>, the conditional distribution of a random variable <inline-formula id="pone.0149958.e013"><alternatives><graphic id="pone.0149958.e013g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e013" xlink:type="simple"/><mml:math display="inline" id="M13"><mml:msub><mml:mover accent="true"><mml:mi mathvariant="normal">X</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula> representing the average of a set of <italic>n</italic> feature vectors {<bold>x</bold><sub>1</sub>,<bold>x</bold><sub>2</sub>, ‥,<bold>x</bold><sub><italic>n</italic></sub>} coming from a particular source <italic>i</italic> is given by
<disp-formula id="pone.0149958.e014"><alternatives><graphic id="pone.0149958.e014g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e014" xlink:type="simple"/><mml:math display="block" id="M14"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathvariant="normal">X</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mrow><mml:mo>|</mml:mo></mml:mrow> <mml:mrow><mml:mo>(</mml:mo> <mml:mo>Θ</mml:mo> <mml:mo>=</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">D</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mi mathvariant="bold">D</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi mathvariant="bold">W</mml:mi> <mml:mi>n</mml:mi></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(12)</label></disp-formula></p>
<p>Thus, when evaluating the conditional probability of a set of <italic>n</italic><sub>1</sub> control samples, <bold>y</bold><sub>1</sub>, or a set of <italic>n</italic><sub>2</sub> recovered samples, <bold>y</bold><sub>2</sub>, they will be evaluated in terms of their sample mean. That is,
<disp-formula id="pone.0149958.e015"><alternatives><graphic id="pone.0149958.e015g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e015" xlink:type="simple"/><mml:math display="block" id="M15"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mi>l</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">W</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mo>|</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mfrac><mml:mi mathvariant="bold">W</mml:mi> <mml:msub><mml:mi>n</mml:mi> <mml:mi>l</mml:mi></mml:msub></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>θ</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(13)</label></disp-formula></p>
<p>This leads to the following expressions for the previously shown integrals:
<disp-formula id="pone.0149958.e016"><alternatives><graphic id="pone.0149958.e016g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e016" xlink:type="simple"/><mml:math display="block" id="M16"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mo>∫</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">X</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(14)</label></disp-formula>
and
<disp-formula id="pone.0149958.e017"><alternatives><graphic id="pone.0149958.e017g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e017" xlink:type="simple"/><mml:math display="block" id="M17"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mo>∫</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">X</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(15)</label></disp-formula>
where only the distribution of the parameter <italic>θ</italic> remains undefined.</p>
</sec>
</sec>
<sec id="sec004">
<title>Models for between-source distribution</title>
<p>Regarding the distribution <italic>p</italic>(<italic>θ</italic>|<bold>X</bold>) from which the parameter characterizing the source <italic>θ</italic> is drawn, its shape depends on how the between-source variation is modelled. In this Section, two different types of distribution of such parameter, obtained from a background population, are shown. First, we will describe the expressions for a normally distributed between-source variation. While this is not the case under analysis in this work, it will serve to derive the expressions for the non-normal case, which is expressed in terms of a weighted sum of Gaussian densities.</p>
<sec id="sec005">
<title>Normal case</title>
<p>If sources means can be assumed normally distributed, <inline-formula id="pone.0149958.e018"><alternatives><graphic id="pone.0149958.e018g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e018" xlink:type="simple"/><mml:math display="inline" id="M18"><mml:mrow><mml:mo>Θ</mml:mo> <mml:mo>∼</mml:mo> <mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, then
<disp-formula id="pone.0149958.e019"><alternatives><graphic id="pone.0149958.e019g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e019" xlink:type="simple"/><mml:math display="block" id="M19"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">X</mml:mi> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(16)</label></disp-formula>
where <italic>μ</italic> and <bold>B</bold> are, respectively, the mean vector and the covariance matrix of the between-source distribution. These <italic>hyperparameters</italic> can be obtained from a background population (with <italic>m</italic> sources, <italic>n</italic> samples per source and <italic>N</italic> total samples) through
<disp-formula id="pone.0149958.e020"><alternatives><graphic id="pone.0149958.e020g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e020" xlink:type="simple"/><mml:math display="block" id="M20"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>m</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:msub><mml:mover accent="true"><mml:mtext>x</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(17)</label></disp-formula>
and
<disp-formula id="pone.0149958.e021"><alternatives><graphic id="pone.0149958.e021g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e021" xlink:type="simple"/><mml:math display="block" id="M21"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">B</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:msub><mml:mi mathvariant="bold">S</mml:mi> <mml:mi>b</mml:mi></mml:msub> <mml:mrow><mml:mi>m</mml:mi> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mfrac> <mml:mo>-</mml:mo> <mml:mfrac><mml:msub><mml:mi mathvariant="bold">S</mml:mi> <mml:mi>w</mml:mi></mml:msub> <mml:mrow><mml:mi>n</mml:mi> <mml:mo>(</mml:mo> <mml:mi>N</mml:mi> <mml:mo>-</mml:mo> <mml:mi>m</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(18)</label></disp-formula>
where the between-source scatter matrix, <bold>S</bold><sub><italic>b</italic></sub>, is given by
<disp-formula id="pone.0149958.e022"><alternatives><graphic id="pone.0149958.e022g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e022" xlink:type="simple"/><mml:math display="block" id="M22"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold">S</mml:mi> <mml:mi>b</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>x</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>x</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mi mathvariant="normal">T</mml:mi></mml:msup></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(19)</label></disp-formula></p>
<p>Using this distribution for the parameter <italic>θ</italic> of the generative model, the integrals involved in the likelihood ratio computation can be written
<disp-formula id="pone.0149958.e023"><alternatives><graphic id="pone.0149958.e023g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e023" xlink:type="simple"/><mml:math display="block" id="M23"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mo>∫</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(20)</label></disp-formula>
and
<disp-formula id="pone.0149958.e024"><alternatives><graphic id="pone.0149958.e024g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e024" xlink:type="simple"/><mml:math display="block" id="M24"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:msub><mml:mo>∫</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(21)</label></disp-formula></p>
<p>Using the Gaussian identities given in the Appendix, the numerator of the likelihood ratio can be shown to be equal to:
<disp-formula id="pone.0149958.e025"><alternatives><graphic id="pone.0149958.e025g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e025" xlink:type="simple"/><mml:math display="block" id="M25"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>*</mml:mo></mml:msup> <mml:mo>;</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold">D</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(22)</label></disp-formula>
where
<disp-formula id="pone.0149958.e026"><alternatives><graphic id="pone.0149958.e026g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e026" xlink:type="simple"/><mml:math display="block" id="M26"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>*</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(23)</label></disp-formula>
and
<disp-formula id="pone.0149958.e027"><alternatives><graphic id="pone.0149958.e027g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e027" xlink:type="simple"/><mml:math display="block" id="M27"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi mathvariant="bold">D</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(24)</label></disp-formula></p>
<p>Finally, each of the integrals in the denominator is given by
<disp-formula id="pone.0149958.e028"><alternatives><graphic id="pone.0149958.e028g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e028" xlink:type="simple"/><mml:math display="block" id="M28"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(25)</label></disp-formula></p>
</sec>
<sec id="sec006">
<title>Non-normal case</title>
<p>When the normal assumption does not hold for the distribution of sources means among the background population data, the between-source distribution <italic>p</italic>(<italic>θ</italic>|<bold>X</bold>) can be approximated by a weighted sum of <italic>C</italic> Gaussian densities in the following form:
<disp-formula id="pone.0149958.e029"><alternatives><graphic id="pone.0149958.e029g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e029" xlink:type="simple"/><mml:math display="block" id="M29"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">X</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>C</mml:mi></mml:munderover> <mml:msub><mml:mi>π</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="normal">Σ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(26)</label></disp-formula>
where {<italic>π</italic><sub><italic>k</italic></sub>}<sub><italic>c</italic> = 1, …, <italic>C</italic></sub> are the weighting factors and have the following constraints
<disp-formula id="pone.0149958.e030"><alternatives><graphic id="pone.0149958.e030g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e030" xlink:type="simple"/><mml:math display="block" id="M30"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn> <mml:mo>≤</mml:mo> <mml:msub><mml:mi>π</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>≤</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>C</mml:mi></mml:munderover> <mml:msub><mml:mi>π</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(27)</label></disp-formula></p>
<p>With this distribution as the prior probability for the parameter <italic>θ</italic> of the generative model, the integrals involved in the likelihood ratio computation can be written
<disp-formula id="pone.0149958.e031"><alternatives><graphic id="pone.0149958.e031g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e031" xlink:type="simple"/><mml:math display="block" id="M31"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mo>∫</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mrow><mml:mo>{</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>C</mml:mi></mml:munderover> <mml:msub><mml:mi>π</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="normal">Σ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>C</mml:mi></mml:munderover> <mml:msub><mml:mi>π</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:msub><mml:mo>∫</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mrow><mml:mo>{</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="normal">Σ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(28)</label></disp-formula>
and
<disp-formula id="pone.0149958.e032"><alternatives><graphic id="pone.0149958.e032g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e032" xlink:type="simple"/><mml:math display="block" id="M32"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mo>∫</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mrow><mml:mo>{</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>C</mml:mi></mml:munderover> <mml:msub><mml:mi>π</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="normal">Σ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>C</mml:mi></mml:munderover> <mml:msub><mml:mi>π</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:msub><mml:mo>∫</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mrow><mml:mo>{</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="normal">Σ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(29)</label></disp-formula></p>
<p>As it can be seen, the Gaussian mixture expressions become a weighted sum of the expressions given for the normal case, and so the probabilities involved in the likelihood ratio computation can be easily derived, resulting in
<disp-formula id="pone.0149958.e033"><alternatives><graphic id="pone.0149958.e033g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e033" xlink:type="simple"/><mml:math display="block" id="M33"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>=</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>C</mml:mi></mml:munderover> <mml:msub><mml:mi>π</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>*</mml:mo></mml:msup> <mml:mo>;</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msup><mml:mi mathvariant="bold">D</mml:mi> <mml:mo>*</mml:mo></mml:msup> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="normal">Σ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(30)</label></disp-formula>
and
<disp-formula id="pone.0149958.e034"><alternatives><graphic id="pone.0149958.e034g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e034" xlink:type="simple"/><mml:math display="block" id="M34"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>C</mml:mi></mml:munderover> <mml:msub><mml:mi>π</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="normal">Σ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>,</mml:mo> <mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:mi>l</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn> <mml:mo>,</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(31)</label></disp-formula></p>
<p>In [<xref ref-type="bibr" rid="pone.0149958.ref010">10</xref>], between-source distribution <italic>p</italic>(<italic>θ</italic>|<bold>X</bold>) is approximated through a KDF [<xref ref-type="bibr" rid="pone.0149958.ref015">15</xref>] where the kernel function <italic>K</italic>(⋅) is taken to be a multivariate normal function with smoothing parameter, or <italic>bandwidth</italic>, <bold>H</bold> = <italic>h</italic><sup>2</sup> <bold>B</bold>:
<disp-formula id="pone.0149958.e035"><alternatives><graphic id="pone.0149958.e035g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e035" xlink:type="simple"/><mml:math display="block" id="M35"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">X</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msup><mml:mrow><mml:mi>m</mml:mi> <mml:mo>|</mml:mo> <mml:mi mathvariant="bold">H</mml:mi> <mml:mo>|</mml:mo></mml:mrow> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mi>K</mml:mi> <mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mrow><mml:mi>θ</mml:mi> <mml:mo>-</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>x</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:mrow> <mml:msup><mml:mi mathvariant="bold">H</mml:mi> <mml:mrow><mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mfenced> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>m</mml:mi></mml:mfrac> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>x</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msup><mml:mi>h</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(32)</label></disp-formula>
where
<disp-formula id="pone.0149958.e036"><alternatives><graphic id="pone.0149958.e036g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e036" xlink:type="simple"/><mml:math display="block" id="M36"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>h</mml:mi> <mml:mo>=</mml:mo> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mn>4</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:mi>d</mml:mi> <mml:mo>+</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mfenced> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mi>d</mml:mi> <mml:mo>+</mml:mo> <mml:mn>4</mml:mn></mml:mrow></mml:mfrac></mml:msup> <mml:msup><mml:mi>m</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn> <mml:mo>/</mml:mo> <mml:mo>(</mml:mo> <mml:mi>d</mml:mi> <mml:mo>+</mml:mo> <mml:mn>4</mml:mn> <mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(33)</label></disp-formula></p>
<p>As it can be seen, this Gaussian KDF is in fact a Gaussian mixture whose parameters, equating terms in <xref ref-type="disp-formula" rid="pone.0149958.e029">Eq 26</xref>, are given by
<disp-formula id="pone.0149958.e037"><alternatives><graphic id="pone.0149958.e037g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e037" xlink:type="simple"/><mml:math display="block" id="M37"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>C</mml:mi> <mml:mo>=</mml:mo> <mml:mi>m</mml:mi> <mml:mo>,</mml:mo> <mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:msub><mml:mi>π</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mi>m</mml:mi></mml:mfrac> <mml:mo>,</mml:mo> <mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:msub><mml:mi>μ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>x</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:mspace width="0.166667em"/><mml:mspace width="0.166667em"/><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>=</mml:mo> <mml:msup><mml:mi>h</mml:mi> <mml:mn>2</mml:mn></mml:msup> <mml:mi mathvariant="bold">B</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(34)</label></disp-formula></p>
<p>Thus, the between-source variation is approximated by an equally weighted sum of multivariate Gaussian functions placed at every source mean present in the background population, <inline-formula id="pone.0149958.e038"><alternatives><graphic id="pone.0149958.e038g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e038" xlink:type="simple"/><mml:math display="inline" id="M38"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula>, being their covariance matrices given by <italic>h</italic><sup>2</sup> <bold>B</bold>. That is, a weighted version of the between-source variation is <italic>translated</italic> to each source mean present in the background. As we will show later on, this will lead to overestimations of the between-source density in some areas of the feature space.</p>
</sec>
</sec>
<sec id="sec007">
<title>GMMs for non-normal between-source distributions</title>
<p>In this work, we propose to use a Gaussian Mixture Model (GMM) trained by means of a maximum-likelihood (ML) criterion in order to represent the distribution of the parameter <italic>θ</italic> characterizing the source. This model assumes that the observations are generated from a mixture of a finite number of Gaussian densities with unknown <italic>hyperparameters</italic>. Thus, it has been widely used to model the distribution of datasets in which the observations are grouped in several clusters, being each of them represented by a Gaussian density. In the case at hand, the observations are the means of the sources (<inline-formula id="pone.0149958.e039"><alternatives><graphic id="pone.0149958.e039g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e039" xlink:type="simple"/><mml:math display="inline" id="M39"><mml:msub><mml:mover accent="true"><mml:mtext>x</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula>) present in the background population dataset (<bold>X</bold>), from which the distribution <italic>p</italic>(<italic>θ</italic>|<bold>X</bold>) is going to be modelled.</p>
<sec id="sec008">
<title>GMM training</title>
<p>Maximum likelihood (ML) is a method of determining the parameters <italic>Φ</italic> of a model that makes the observed samples the most probable given that model. Conversely to KDF, where the parameters (<inline-formula id="pone.0149958.e040"><alternatives><graphic id="pone.0149958.e040g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e040" xlink:type="simple"/><mml:math display="inline" id="M40"><mml:msub><mml:mover accent="true"><mml:mtext>x</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula>, <bold>H</bold>) are first established and the density function <italic>p</italic>(<italic>θ</italic>|<bold>X</bold>) arises from them, in the GMM approach the density function is obtained by maximizing the likelihood of the observed data given the model, <italic>p</italic>(<bold>X</bold>|<italic>Φ</italic>), from which the optimum parameters of the model are derived. In the case of a GMM of <italic>C</italic> components in the form of <xref ref-type="disp-formula" rid="pone.0149958.e029">Eq 26</xref>, the ML parameters of the model, <italic>Φ</italic> = {<italic>π</italic><sub><italic>c</italic></sub>, <italic>μ</italic><sub><italic>c</italic></sub>, <italic>Σ</italic><sub><italic>c</italic></sub>}<sub><italic>c</italic> = 1, …, <italic>C</italic></sub>, are obtained [<xref ref-type="bibr" rid="pone.0149958.ref017">17</xref>] by maximizing the following log-likelihood:
<disp-formula id="pone.0149958.e041"><alternatives><graphic id="pone.0149958.e041g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e041" xlink:type="simple"/><mml:math display="block" id="M41"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo form="prefix">ln</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">X</mml:mi> <mml:mo>|</mml:mo> <mml:mo>Φ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:mo form="prefix">ln</mml:mo> <mml:mfenced close="}" open="{" separators=""><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>C</mml:mi></mml:munderover> <mml:msub><mml:mi>π</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>x</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="normal">Σ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mfenced></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(35)</label></disp-formula></p>
<p>This can be done through the well known expectation-maximization (EM) algorithm [<xref ref-type="bibr" rid="pone.0149958.ref017">17</xref>, <xref ref-type="bibr" rid="pone.0149958.ref019">19</xref>], which is an iterative method that successively updates the parameters <italic>Φ</italic> of the model until convergence. A recipe for this iterative process can be found in [<xref ref-type="bibr" rid="pone.0149958.ref017">17</xref>].</p>
<p>For a faster convergence of the algorithm, usually some steps of the <italic>k-means</italic> algorithm [<xref ref-type="bibr" rid="pone.0149958.ref017">17</xref>, <xref ref-type="bibr" rid="pone.0149958.ref020">20</xref>] are previously iterated in order to obtain a good initialization of the GMM, as this clustering method provides the mean vectors {<italic>μ</italic><sub><italic>c</italic></sub>}<sub><italic>c</italic> = 1, …, <italic>C</italic></sub> (known as <italic>centroids</italic>) and the initial assignment of samples to clusters, from which {<italic>π</italic><sub><italic>c</italic></sub>}<sub><italic>c</italic> = 1, …, <italic>C</italic></sub> and {<italic>Σ</italic><sub><italic>c</italic></sub>}<sub><italic>c</italic> = 1, …, <italic>C</italic></sub> can be obtained.</p>
<p>The specific number of components, <italic>C</italic>, can be set by different methods. If the feature vectors are low-dimensional, the number of components can be visually estimated by inspecting a 2-D or 3-D projection of the background population data; however, depending on the structure of the data, there can be a lot of ambiguity in this process. Another option is to apply the <italic>elbow method</italic> [<xref ref-type="bibr" rid="pone.0149958.ref021">21</xref>] in the initial clustering stage, in which the cost function is plotted for different (increasing) number of clusters; for the first number of clusters there will be a great change when increasing the number of clusters, but at some point the marginal gain will drop indicating the proper number of clusters. A similar method can be applied by training GMMs for different numbers of components and evaluating the gain in terms of likelihood when increasing the number of them. Finally, similarly to the previous approach, if different GMMs for different number of components are trained, some model selection methods, like the Bayesian information criterion (BIC) [<xref ref-type="bibr" rid="pone.0149958.ref022">22</xref>] or the Akaike information criterion (AIC) [<xref ref-type="bibr" rid="pone.0149958.ref023">23</xref>], can be applied.</p>
<p>In this work, results are reported for several number of components in order to analyse how the evaluation metrics vary depending on this parameter, and the proper number of components related to the log-likelihood of the background data given the between-source density. For a given number of components, the <italic>k-means</italic> algorithm is iterated until convergence previously to the EM algorithm. In order to avoid local minima in <italic>k-means</italic> clustering, 100 random initializations are performed for a given number of components.</p>
</sec>
<sec id="sec009">
<title>GMM versus KDF</title>
<p>For the purpose of illustrating the differences between KDF and GMM approaches, a synthetic 2-dimensional dataset has been generated (see <xref ref-type="fig" rid="pone.0149958.g001">Fig 1</xref>), in which 10 samples from 50 sources are drawn from normal distributions with the same covariance matrix (having then the same within-source variation). Sources means are drawn from 2 different normal distributions (25 sources each), each centred at a different separated point of the feature space, and one having a larger variance than the other in one of the dimensions. As a consequence, samples coming from different sources are grouped in two clearly separated clusters, one of them having a larger local intra-cluster between-source variation than the other. Also, the overall between-source variation is higher in one of the dimensions.</p>
<fig id="pone.0149958.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149958.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Synthetic dataset.</title>
<p>Samples from a 2-dimensional synthetic dataset in which sources are grouped in two separate clusters.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149958.g001" xlink:type="simple"/>
</fig>
<p>As already shown in Section [Models for between-source distribution], the density function <italic>p</italic>(<italic>θ</italic>|<bold>X</bold>) given by KDF approach is an equally weighted sum of Gaussian densities centred at each background source mean with covariance matrices <italic>h</italic><sup>2</sup> <bold>B</bold> (see <xref ref-type="disp-formula" rid="pone.0149958.e035">Eq 32</xref>). Thus, a weighted version of the overall between-source variation is <italic>translated</italic> to every source mean, reproducing this variation locally at each source mean. The resulting density function <italic>p</italic>(<italic>θ</italic>|<bold>X</bold>) for our synthetic dataset can be seen in <xref ref-type="fig" rid="pone.0149958.g002">Fig 2</xref>, where it is shown that the local intra-cluster between-source variation in dimension 1 is highly overestimated for both clusters, and slightly overestimated in dimension 2 for one of them due to the larger variation in the other one.</p>
<fig id="pone.0149958.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149958.g002</object-id>
<label>Fig 2</label>
<caption>
<title>KDF modelling of between-source variation in the synthetic dataset.</title>
<p>(Above) Sources means and level contours of the between-source density function. (Below) 3-dimensional representation of the between-source density function.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149958.g002" xlink:type="simple"/>
</fig>
<p>Conversely to KDF, in the GMM approach the Gaussian components are not forced to be centred at each source mean present in the background population, but a smaller number of components can be established allowing different sources means being generated from the same Gaussian component. Moreover, covariance matrices are neither fixed in advance, allowing to be locally learned for each component. As a consequence, the resulting density function can better fit the local between-source variation and the clustered nature of the dataset, as it is shown in <xref ref-type="fig" rid="pone.0149958.g003">Fig 3</xref> for a 2-component GMM.</p>
<fig id="pone.0149958.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149958.g003</object-id>
<label>Fig 3</label>
<caption>
<title>GMM modelling of between-source variation in the synthetic dataset.</title>
<p>(Above) Sources means and level contours of the between-source density function. (Below) 3-dimensional representation of the between-source density function.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149958.g003" xlink:type="simple"/>
</fig>
<p>However, care must be taken in order to avoid <italic>overfitting</italic> when computing the density function through maximum likelihood. For a ML-trained GMM, the degree of fitting to the background data can be controlled through both the number of components <italic>C</italic> of the mixture and the number of EM iterations. In this work, for a given number of components, only two EM iterations are performed in order to avoid <italic>overfitting</italic>.</p>
</sec>
<sec id="sec010">
<title>Accounting for within-source variation in the background population</title>
<p>When training a GMM from background sources means by maximizing the log-likelihood in <xref ref-type="disp-formula" rid="pone.0149958.e041">Eq 35</xref>, it is assumed that there is no uncertainty in these mean values. However, the number of samples per source in the background population can be limited in forensic scenarios, and so these means cannot be reliably computed. In order to account for the uncertainty in these mean values, every observation belonging to those sources can be used to train a GMM by maximizing the following log-likelihood:
<disp-formula id="pone.0149958.e042"><alternatives><graphic id="pone.0149958.e042g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e042" xlink:type="simple"/><mml:math display="block" id="M42"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo form="prefix">ln</mml:mo> <mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">X</mml:mi> <mml:mo>|</mml:mo> <mml:mo>Φ</mml:mo> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>i</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>m</mml:mi></mml:munderover> <mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>j</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>n</mml:mi></mml:munderover> <mml:mo form="prefix">ln</mml:mo> <mml:mfenced close="}" open="{" separators=""><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mo>=</mml:mo> <mml:mn>1</mml:mn></mml:mrow> <mml:mi>C</mml:mi></mml:munderover> <mml:msub><mml:mi>π</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>x</mml:mtext> <mml:mrow><mml:mi>i</mml:mi> <mml:mi>j</mml:mi></mml:mrow></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mi>μ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="normal">Σ</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mfenced></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(36)</label></disp-formula></p>
<p>While there can be not much difference in the values obtained for components means <italic>μ</italic><sub><italic>c</italic></sub> in a well balanced background dataset (same number of samples per source), taking into account the variation of the samples from each source around its mean value through <xref ref-type="disp-formula" rid="pone.0149958.e042">Eq 36</xref> provides a more conservative background density, as every background sample is considered as a possible mean value of a source. Furthermore, this also helps to avoid Gaussian collapsing when a reduced number of sources are assigned to a particular component. The effect on our synthetic dataset is shown in <xref ref-type="fig" rid="pone.0149958.g004">Fig 4</xref>, where the Gaussian densities are placed at the same locations as in <xref ref-type="fig" rid="pone.0149958.g003">Fig 3</xref> but larger variances and covariances are obtained, specially for the cluster with lower intra-cluster between-source variation.</p>
<fig id="pone.0149958.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149958.g004</object-id>
<label>Fig 4</label>
<caption>
<title>GMM modelling of between-source variation in the synthetic dataset when taking into account the background within-source variation.</title>
<p>(Above) Sources means and level contours of the between-source density function. (Below) 3-dimensional representation of the between-source density function.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149958.g004" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec011">
<title>Experimental framework</title>
<sec id="sec012">
<title>Forensic datasets</title>
<p>In order to test the approach proposed in this work, several types of forensic datasets have been used, being one of them the glass-fragments dataset also used in [<xref ref-type="bibr" rid="pone.0149958.ref010">10</xref>], which can be downloaded from <ext-link ext-link-type="uri" xlink:href="http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1467-9876/homepage/glass-data.txt" xlink:type="simple">http://onlinelibrary.wiley.com/journal/10.1111/(ISSN)1467-9876/homepage/glass-data.txt</ext-link>. A detailed description of the other two datasets can be found in [<xref ref-type="bibr" rid="pone.0149958.ref012">12</xref>], and can be downloaded from <ext-link ext-link-type="uri" xlink:href="http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470972106.html" xlink:type="simple">http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0470972106.html</ext-link>.</p>
<list list-type="bullet">
<list-item>
<p><italic>Inks</italic>. For this dataset, the features are the measurements of the <italic>d</italic> = 3 chromaticity coordinates <italic>r</italic>, <italic>g</italic> and <italic>b</italic> (being <italic>r</italic> + <italic>g</italic> + <italic>b</italic> = 1) taken on samples of blue inks. The dataset comprises the measurements on <italic>n</italic> = 10 samples for each of the <italic>m</italic> = 40 different ink sources.</p>
</list-item>
<list-item>
<p><italic>Glass fragments</italic>. For this dataset, the features are the measurements of the concentrations in <italic>d</italic> = 3 elemental ratios taken on glass fragments: log(Ca/K), log(Ca/Si) and log(Ca/Fe). The dataset comprises the measurements on <italic>n</italic> = 5 fragments for each of the <italic>m</italic> = 62 different glass sources.</p>
</list-item>
<list-item>
<p><italic>Car paints</italic>. For this dataset, the features are the measurements of <italic>d</italic> = 7 organic components present in the top layer of different acrylic car paintings. The dataset comprises the measurements on <italic>n</italic> = 3 samples for each of the <italic>m</italic> = 36 different car-paint sources.</p>
</list-item>
</list>
<p>
<xref ref-type="table" rid="pone.0149958.t001">Table 1</xref> gathers the already mentioned characteristics of these three datasets, while Figs <xref ref-type="fig" rid="pone.0149958.g005">5</xref>, <xref ref-type="fig" rid="pone.0149958.g006">6</xref> and <xref ref-type="fig" rid="pone.0149958.g007">7</xref> show 2-dimensional projections of their sources means. As it can be seen, sources means in the last two datasets (glass fragments and car paints) present a clustered nature, while those in the first one (inks) are normally distributed [<xref ref-type="bibr" rid="pone.0149958.ref012">12</xref>].</p>
<table-wrap id="pone.0149958.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149958.t001</object-id>
<label>Table 1</label>
<caption>
<title>Datasets summary.</title>
</caption>
<alternatives>
<graphic id="pone.0149958.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149958.t001" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center">m</th>
<th align="center">n</th>
<th align="center">d</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Inks</td>
<td align="center">40</td>
<td align="center">10</td>
<td align="center">3</td>
</tr>
<tr>
<td align="left">Glass fragments</td>
<td align="center">62</td>
<td align="center">5</td>
<td align="center">3</td>
</tr>
<tr>
<td align="left">Car paints</td>
<td align="center">36</td>
<td align="center">3</td>
<td align="center">7</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t001fn001">
<p>m, number of sources; n, number of samples per source; d, number of features.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<fig id="pone.0149958.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149958.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Sources means in the inks dataset.</title>
<p>The three 2-dimensional projections of the sources means.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149958.g005" xlink:type="simple"/>
</fig>
<fig id="pone.0149958.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149958.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Sources means in the glass-fragments dataset.</title>
<p>The three 2-dimensional projections of the sources means.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149958.g006" xlink:type="simple"/>
</fig>
<fig id="pone.0149958.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149958.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Sources means in the car-paints dataset.</title>
<p>Three 2-dimensional projections of the sources means.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149958.g007" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec013">
<title>Protocols</title>
<p>The protocol followed in [<xref ref-type="bibr" rid="pone.0149958.ref010">10</xref>] used the whole glass-fragment dataset in order to obtain the between-source probability density function <italic>p</italic>(<italic>θ</italic>|<bold>X</bold>). Then, for each source, the first 3 samples (out of 5) were used as control data and the last 3 were used as recovered data, having so both datasets one sample in common. While this <italic>non-partitioning</italic> protocol alleviates the lack of data due to the small size of the dataset, it may lead to overoptimistic results as the different subsets (background, control and recovered) are overlapped.</p>
<p>In this work, a <italic>cross-validation</italic> protocol is also used in order to avoid overoptimistic results, in which the dataset is divided into two non-overlapping subsets devoted to:</p>
<list list-type="bullet">
<list-item>
<p>obtain the between-source distribution <italic>p</italic>(<italic>θ</italic>|<bold>X</bold>) (<italic>known data</italic> or <italic>training subset</italic>), and</p>
</list-item>
<list-item>
<p>compute same-source and different-source likelihood ratios (<italic>unknown data</italic> or <italic>testing subset</italic>). This subset is further divided into two non-overlapping halves acting as control and recovered data.</p>
</list-item>
</list>
<p>In order to alleviate the lack of data, this procedure is carried out in the following way. For each of the <italic>m</italic>(<italic>m</italic> − 1)/2 possible pairs of sources in the dataset, all the samples belonging to those two sources are taken apart from the dataset in order to be used as the <italic>testing subset</italic>, being the remaining sources used as the <italic>training subset</italic>. Each of the two sources in the <italic>testing subset</italic> is divided into two non-overlapping halves ({1a, 1b} and {2a, 2b}) that can be used either as control or recovered data to perform 2 same-source comparisons (1a-1b, 2a-2b) and 4 different-source comparisons (1a-2a, 1a-2b, 1b-2a, 1b-2b). Although the same control and recovered data from a particular source is used in all the different pairs in which it is involved, as the remaining sources change for each different pair, different between-source distributions <italic>p</italic>(<italic>θ</italic>|<bold>X</bold>) are involved in likelihood ratio computations. This procedure allow us to perform a total number of <italic>m</italic>(<italic>m</italic> − 1) same-source comparisons and 2 × <italic>m</italic>(<italic>m</italic> − 1) different-source comparisons for a given dataset, instead of the <italic>m</italic> same-source comparisons and <italic>m</italic>(<italic>m</italic> − 1)/2 different-source comparisons performed in [<xref ref-type="bibr" rid="pone.0149958.ref010">10</xref>], while the between-source distribution <italic>p</italic>(<italic>θ</italic>|<bold>X</bold>) used in every comparison is obtained from <italic>m</italic> − 2 different sources instead of <italic>m</italic>. The specific number of comparisons for each evaluation protocol on the different datasets are given in <xref ref-type="table" rid="pone.0149958.t002">Table 2</xref>.</p>
<table-wrap id="pone.0149958.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149958.t002</object-id>
<label>Table 2</label>
<caption>
<title>Number of same-source and different-source comparisons in each dataset for the non-partitioning and cross-validation protocols.</title>
</caption>
<alternatives>
<graphic id="pone.0149958.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149958.t002" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center" colspan="2">Non-partitioning</th>
<th align="center" colspan="2">Cross-validation</th>
</tr>
<tr>
<th align="left"/>
<th align="center">Same-source</th>
<th align="center">Different-source</th>
<th align="center">Same-source</th>
<th align="center">Different-source</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Glass fragments</td>
<td align="center">62</td>
<td align="center">1891</td>
<td align="center">3782</td>
<td align="center">7564</td>
</tr>
<tr>
<td align="left">Inks</td>
<td align="center">40</td>
<td align="center">780</td>
<td align="center">1560</td>
<td align="center">3120</td>
</tr>
<tr>
<td align="left">Car paints</td>
<td align="center">36</td>
<td align="center">630</td>
<td align="center">1260</td>
<td align="center">2520</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec014">
<title>Evaluation Metrics</title>
<p>The main evaluation metric used in order to compare the different approaches is the log-likelihood ratio cost function (<italic>C</italic><sub><italic>llr</italic></sub>) [<xref ref-type="bibr" rid="pone.0149958.ref002">2</xref>, <xref ref-type="bibr" rid="pone.0149958.ref024">24</xref>], which evaluates both the <italic>discrimination</italic> abilities of the computed log-likelihood ratios and the goodness of their <italic>calibration</italic>. Given a set of log-likelihood ratios <inline-formula id="pone.0149958.e043"><alternatives><graphic id="pone.0149958.e043g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e043" xlink:type="simple"/><mml:math display="inline" id="M43"><mml:mrow><mml:mrow><mml:mo>{</mml:mo> <mml:mi mathvariant="script">L</mml:mi> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mrow><mml:mo>{</mml:mo> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>.</mml:mo> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>C</mml:mi></mml:msub> <mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> obtained from <italic>C</italic> comparisons, the <italic>C</italic><sub><italic>llr</italic></sub> can be computed in the following way:
<disp-formula id="pone.0149958.e044"><alternatives><graphic id="pone.0149958.e044g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e044" xlink:type="simple"/><mml:math display="block" id="M44"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>C</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>l</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub> <mml:mrow><mml:mo>(</mml:mo> <mml:mrow><mml:mo>{</mml:mo> <mml:mi mathvariant="script">L</mml:mi> <mml:mo>}</mml:mo></mml:mrow> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:mrow><mml:mn>2</mml:mn> <mml:mo form="prefix">log</mml:mo> <mml:mn>2</mml:mn></mml:mrow></mml:mfrac> <mml:mfenced close=")" open="(" separators=""><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>N</mml:mi> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mfrac> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mo>∈</mml:mo> <mml:mrow><mml:mi>s</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:munder> <mml:mo form="prefix">log</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:mrow><mml:mo>-</mml:mo> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mfenced> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>N</mml:mi> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mfrac> <mml:munder><mml:mo>∑</mml:mo> <mml:mrow><mml:mi>c</mml:mi> <mml:mo>∈</mml:mo> <mml:mrow><mml:mi>d</mml:mi> <mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:munder> <mml:mo form="prefix">log</mml:mo> <mml:mfenced close=")" open="(" separators=""><mml:mn>1</mml:mn> <mml:mo>+</mml:mo> <mml:msup><mml:mi>e</mml:mi> <mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>c</mml:mi></mml:msub></mml:msup></mml:mfenced></mml:mstyle></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(37)</label></disp-formula>
where ‘<italic>ss</italic>’ is the set of <italic>N</italic><sub><italic>ss</italic></sub> same-source comparisons and ‘<italic>ds</italic>’ is the set of <italic>N</italic><sub><italic>ds</italic></sub> different-source comparisons. As it is a cost function, the larger the <italic>C</italic><sub><italic>llr</italic></sub> value, the worse the verification method, being <italic>C</italic><sub><italic>llr</italic></sub> = 0 the minimum achievable cost. Note also that this metric allows to define a <italic>neutral reference</italic> which does not provide support for any of the two hypothesis (that is, <inline-formula id="pone.0149958.e045"><alternatives><graphic id="pone.0149958.e045g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e045" xlink:type="simple"/><mml:math display="inline" id="M45"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi> <mml:mi>c</mml:mi></mml:msub> <mml:mspace width="4pt"/><mml:mo>=</mml:mo> <mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula> for every comparison), providing a reference value of <italic>C</italic><sub><italic>llr</italic></sub> = 1. Thus, a verification method for which <italic>C</italic><sub><italic>llr</italic></sub> is larger than 1 means that it is providing misleading likelihood ratios.</p>
<p>An important aspect of the <italic>C</italic><sub><italic>llr</italic></sub> is that it can be decomposed into two additive terms, one due to the discrimination abilities (<inline-formula id="pone.0149958.e046"><alternatives><graphic id="pone.0149958.e046g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e046" xlink:type="simple"/><mml:math display="inline" id="M46"><mml:msubsup><mml:mi>C</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>l</mml:mi> <mml:mi>r</mml:mi></mml:mrow> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) and another one due to the calibration of the verification method (<inline-formula id="pone.0149958.e047"><alternatives><graphic id="pone.0149958.e047g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e047" xlink:type="simple"/><mml:math display="inline" id="M47"><mml:msubsup><mml:mi>C</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>l</mml:mi> <mml:mi>r</mml:mi></mml:mrow> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>a</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) where
<disp-formula id="pone.0149958.e048"><alternatives><graphic id="pone.0149958.e048g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e048" xlink:type="simple"/><mml:math display="block" id="M48"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>l</mml:mi> <mml:mi>r</mml:mi></mml:mrow> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>a</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msubsup> <mml:mo>=</mml:mo> <mml:msub><mml:mi>C</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>l</mml:mi> <mml:mi>r</mml:mi></mml:mrow></mml:msub> <mml:mo>-</mml:mo> <mml:msubsup><mml:mi>C</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>l</mml:mi> <mml:mi>r</mml:mi></mml:mrow> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(38)</label></disp-formula>
and <inline-formula id="pone.0149958.e049"><alternatives><graphic id="pone.0149958.e049g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e049" xlink:type="simple"/><mml:math display="inline" id="M49"><mml:msubsup><mml:mi>C</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>l</mml:mi> <mml:mi>r</mml:mi></mml:mrow> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> is obtained by means of the <italic>Pool Adjacent Violators</italic> (PAV) algorithm [<xref ref-type="bibr" rid="pone.0149958.ref025">25</xref>, <xref ref-type="bibr" rid="pone.0149958.ref026">26</xref>] and represents the minimum achievable <italic>C</italic><sub><italic>llr</italic></sub> in the case of having an optimally calibrated log-likelihood ratios set <inline-formula id="pone.0149958.e050"><alternatives><graphic id="pone.0149958.e050g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e050" xlink:type="simple"/><mml:math display="inline" id="M50"><mml:mrow><mml:mo>{</mml:mo> <mml:msup><mml:mi mathvariant="script">L</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> (details can be found in [<xref ref-type="bibr" rid="pone.0149958.ref024">24</xref>]).</p>
<p>In order to show the performance over a wide range of prior probabilities, the Empirical Croos-Entropy (ECE) plots [<xref ref-type="bibr" rid="pone.0149958.ref027">27</xref>, <xref ref-type="bibr" rid="pone.0149958.ref028">28</xref>] will be used. These figures (see, for example, <xref ref-type="fig" rid="pone.0149958.g008">Fig 8</xref>) graphically represent what would be the accuracy (solid curve) when using the set of logLR values <inline-formula id="pone.0149958.e051"><alternatives><graphic id="pone.0149958.e051g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e051" xlink:type="simple"/><mml:math display="inline" id="M51"><mml:mrow><mml:mo>{</mml:mo> <mml:mi mathvariant="script">L</mml:mi> <mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> for each of the prior probabilities (represented as logarithmic odds) in the given range. Additionally, the discriminating power is also plotted (dashed curve) for the optimally calibrated (ideal) logLRs set <inline-formula id="pone.0149958.e052"><alternatives><graphic id="pone.0149958.e052g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e052" xlink:type="simple"/><mml:math display="inline" id="M52"><mml:mrow><mml:mo>{</mml:mo> <mml:msup><mml:mi mathvariant="script">L</mml:mi> <mml:mo>′</mml:mo></mml:msup> <mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>, along with the neutral reference (dotted curve).</p>
<fig id="pone.0149958.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149958.g008</object-id>
<label>Fig 8</label>
<caption>
<title>ECE plots for the KDF and GMM approaches on the inks dataset when applying the <italic>cross-validation</italic> protocol.</title>
<p>GMM is trained by maximizing <xref ref-type="disp-formula" rid="pone.0149958.e041">Eq 35</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149958.g008" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec015" sec-type="conclusions">
<title>Results and Discussion</title>
<sec id="sec016">
<title>Inks dataset</title>
<p>For this dataset, as the background sources means are normally distributed, GMMs with a single component has been trained by maximizing either <xref ref-type="disp-formula" rid="pone.0149958.e041">Eq 35</xref> or <xref ref-type="disp-formula" rid="pone.0149958.e042">Eq 36</xref>. <xref ref-type="table" rid="pone.0149958.t003">Table 3</xref> shows the detailed results (<italic>C</italic><sub><italic>llr</italic></sub>, <inline-formula id="pone.0149958.e053"><alternatives><graphic id="pone.0149958.e053g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e053" xlink:type="simple"/><mml:math display="inline" id="M53"><mml:msubsup><mml:mi>C</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>l</mml:mi> <mml:mi>r</mml:mi></mml:mrow> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0149958.e054"><alternatives><graphic id="pone.0149958.e054g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e054" xlink:type="simple"/><mml:math display="inline" id="M54"><mml:msubsup><mml:mi>C</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>l</mml:mi> <mml:mi>r</mml:mi></mml:mrow> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>a</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) for KDF and GMM approaches (<xref ref-type="disp-formula" rid="pone.0149958.e041">Eq 35</xref> and <xref ref-type="disp-formula" rid="pone.0149958.e042">Eq 36</xref>) when applying both the <italic>non-partitioning</italic> and the <italic>cross-validation</italic> protocols.</p>
<table-wrap id="pone.0149958.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149958.t003</object-id>
<label>Table 3</label>
<caption>
<title>Performance of KDF and GMM approaches on the inks dataset for the <italic>non-partitioning</italic> and <italic>cross-validation</italic> protocols.</title>
</caption>
<alternatives>
<graphic id="pone.0149958.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149958.t003" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center" colspan="3">Non-partitioning</th>
<th align="center" colspan="3">Cross-validation</th>
</tr>
<tr>
<th align="left"/>
<th align="center"><inline-formula id="pone.0149958.e055"><alternatives><graphic id="pone.0149958.e055g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e055" xlink:type="simple"/><mml:math display="inline" id="M55"><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula></th>
<th align="center"><inline-formula id="pone.0149958.e056"><alternatives><graphic id="pone.0149958.e056g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e056" xlink:type="simple"/><mml:math display="inline" id="M56"><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula></th>
<th align="center"><italic>C</italic><sub><italic>llr</italic></sub></th>
<th align="center"><inline-formula id="pone.0149958.e057"><alternatives><graphic id="pone.0149958.e057g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e057" xlink:type="simple"/><mml:math display="inline" id="M57"><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula></th>
<th align="center"><inline-formula id="pone.0149958.e058"><alternatives><graphic id="pone.0149958.e058g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e058" xlink:type="simple"/><mml:math display="inline" id="M58"><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula></th>
<th align="center"><italic>C</italic><sub><italic>llr</italic></sub></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">KDF</td>
<td align="char" char=".">0.1459</td>
<td align="char" char=".">0.0224</td>
<td align="char" char=".">0.1684</td>
<td align="char" char=".">0.1558</td>
<td align="char" char="."><bold>0.0214</bold></td>
<td align="char" char=".">0.1778</td>
</tr>
<tr>
<td align="left">GMM (<xref ref-type="disp-formula" rid="pone.0149958.e041">Eq 35</xref>)</td>
<td align="char" char="."><bold>0.1430</bold></td>
<td align="char" char="."><bold>0.0223</bold></td>
<td align="char" char="."><bold>0.1653</bold></td>
<td align="char" char="."><bold>0.1533</bold></td>
<td align="char" char=".">0.0223</td>
<td align="char" char="."><bold>0.1756</bold></td>
</tr>
<tr>
<td align="left">GMM (<xref ref-type="disp-formula" rid="pone.0149958.e042">Eq 36</xref>)</td>
<td align="char" char=".">0.1453</td>
<td align="char" char=".">0.0271</td>
<td align="char" char=".">0.1724</td>
<td align="char" char=".">0.1569</td>
<td align="char" char=".">0.0286</td>
<td align="char" char=".">0.1855</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t003fn001">
<p><inline-formula id="pone.0149958.e059"><alternatives><graphic id="pone.0149958.e059g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e059" xlink:type="simple"/><mml:math display="inline" id="M59"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula></p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>First, it should be noted that results in the <italic>non-partitioning</italic> protocol are slightly better for every method as it is an overoptimistic framework where data is shared between training and testing subsets. Regarding the comparison between methods, it can be seen that no significant improvement is obtained by the GMM approach as the sources means for this dataset do not present a clustered nature. Moreover, among the two GMM variants, the results obtained when maximizing <xref ref-type="disp-formula" rid="pone.0149958.e041">Eq 35</xref> are slightly better, presumably due to the fact that enough number of samples per source are available (<italic>n</italic> = 10), compared to the number of features (<italic>d</italic> = 3), to compute reliable sources means, and further uncertainty accounted for <xref ref-type="disp-formula" rid="pone.0149958.e042">Eq 36</xref> seems to be counter-productive.</p>
<p>Finally, <xref ref-type="fig" rid="pone.0149958.g008">Fig 8</xref> show ECE plots for KDF and GMM (<xref ref-type="disp-formula" rid="pone.0149958.e041">Eq 35</xref>) approaches when applying the <italic>cross-validation</italic> protocol, where it can be seen that both present similar performance for a wide range of prior probabilities.</p>
</sec>
<sec id="sec017">
<title>Glass-fragments dataset</title>
<p>For this dataset, several GMMs have been trained, by maximizing <xref ref-type="disp-formula" rid="pone.0149958.e041">Eq 35</xref>, in order to analyse how the main evaluation metric (<italic>C</italic><sub><italic>llr</italic></sub>) varies as a function of the number of components, <italic>C</italic>. In the experiments carried out, the maximum number of components has been limited to 6 in order to avoid Gaussian collapsing due to a reduced number of observations (sources means) per component (62 total sources in the whole dataset). Results for the <italic>non-partitioning</italic> protocol can be seen in <xref ref-type="fig" rid="pone.0149958.g009">Fig 9</xref> for both KDF and GMM (<xref ref-type="disp-formula" rid="pone.0149958.e041">Eq 35</xref>) approaches, where also the log-likelihood of the background data (sources means) given the between-source density has been plotted.</p>
<fig id="pone.0149958.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149958.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Analysis of the number of GMM components for the glass-fragments dataset when applying the <italic>non-partitioning</italic> protocol.</title>
<p>GMM is trained by maximizing <xref ref-type="disp-formula" rid="pone.0149958.e041">Eq 35</xref>. (Above) Log-likelihood ratio cost. (Below) Log-likelihood of the background data given the between-source density function.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149958.g009" xlink:type="simple"/>
</fig>
<p>As it was expected for this <italic>non-partitioning</italic> protocol, <italic>C</italic><sub><italic>llr</italic></sub> decreases as the number of components increases, due to the shared data between training and testing subsets, which can lead to overfit the background density. However, as soon as the log-likelihood for the GMM surpass that obtained for the KDF density, better results are obtained with the GMM approach. It is also worth noting that this happens for a number of components (2–3) around that which could be expected from visual inspection of the 2-dimensional projections shown in <xref ref-type="fig" rid="pone.0149958.g006">Fig 6</xref>.</p>
<p>
<xref ref-type="fig" rid="pone.0149958.g010">Fig 10</xref> show the same analysis for the <italic>cross-validation</italic> protocol. In this case, the log-likelihood is not plotted as the GMM change for every testing sources-pair (being trained on the remaining sources). Similar conclusions than before can be drawn, but here the overfitting problem affecting the <italic>non-partitioning</italic> protocol is revealed, as the <italic>C</italic><sub><italic>llr</italic></sub> for the <italic>cross-validation</italic> protocol reaches a minimum value for a given number of components (<italic>C</italic> = 4) and then increases. Results are also shown for GMMs trained by maximizing <xref ref-type="disp-formula" rid="pone.0149958.e042">Eq 36</xref>, with similar conclusions but slightly better results, presumably due to the small number of samples per source (<italic>n</italic> = 5) compared to the number of features (<italic>d</italic> = 3).</p>
<fig id="pone.0149958.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149958.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Analysis of the number of GMM components for the glass-fragments dataset when applying the <italic>cross-validation</italic> protocol.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149958.g010" xlink:type="simple"/>
</fig>
<p>
<xref ref-type="table" rid="pone.0149958.t004">Table 4</xref> shows the detailed results (<italic>C</italic><sub><italic>llr</italic></sub>, <inline-formula id="pone.0149958.e060"><alternatives><graphic id="pone.0149958.e060g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e060" xlink:type="simple"/><mml:math display="inline" id="M60"><mml:msubsup><mml:mi>C</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>l</mml:mi> <mml:mi>r</mml:mi></mml:mrow> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0149958.e061"><alternatives><graphic id="pone.0149958.e061g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e061" xlink:type="simple"/><mml:math display="inline" id="M61"><mml:msubsup><mml:mi>C</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>l</mml:mi> <mml:mi>r</mml:mi></mml:mrow> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>a</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) for KDF and GMM approaches (<xref ref-type="disp-formula" rid="pone.0149958.e041">Eq 35</xref> and <xref ref-type="disp-formula" rid="pone.0149958.e042">Eq 36</xref>) when applying both the <italic>non-partitioning</italic> and the <italic>cross-validation</italic> protocols. For GMM approaches, results are given for the optimum number of components (<italic>C</italic> = 4) when the <italic>cross-validation</italic> protocol is applied. Again, as the <italic>non-partitioning</italic> protocol constitutes an over-optimistic framework, results are slightly better for every method compared to the <italic>cross-validation</italic> protocol. This is also the reason of obtaining better results when GMMs are trained by maximizing <xref ref-type="disp-formula" rid="pone.0149958.e042">Eq 36</xref>, as the same sources are present in both training and testing subsets. However, when the <italic>cross-validation</italic> protocol is applied, there is no shared data between those subsets, and so the additional uncertainty accounted by <xref ref-type="disp-formula" rid="pone.0149958.e042">Eq 36</xref> provides slightly better results. In any case, both GMM approaches outperform the KDF one due to their better calibration properties for this clustered dataset.</p>
<table-wrap id="pone.0149958.t004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149958.t004</object-id>
<label>Table 4</label>
<caption>
<title>Performance of KDF and GMM approaches on the glass-fragments dataset for the <italic>non-partitioning</italic> and <italic>cross-validation</italic> protocols.</title>
</caption>
<alternatives>
<graphic id="pone.0149958.t004g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149958.t004" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center" colspan="3">Non-partitioning</th>
<th align="center" colspan="3">Cross-validation</th>
</tr>
<tr>
<th align="left"/>
<th align="center"><inline-formula id="pone.0149958.e062"><alternatives><graphic id="pone.0149958.e062g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e062" xlink:type="simple"/><mml:math display="inline" id="M62"><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula></th>
<th align="center"><inline-formula id="pone.0149958.e063"><alternatives><graphic id="pone.0149958.e063g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e063" xlink:type="simple"/><mml:math display="inline" id="M63"><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula></th>
<th align="center"><italic>C</italic><sub><italic>llr</italic></sub></th>
<th align="center"><inline-formula id="pone.0149958.e064"><alternatives><graphic id="pone.0149958.e064g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e064" xlink:type="simple"/><mml:math display="inline" id="M64"><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula></th>
<th align="center"><inline-formula id="pone.0149958.e065"><alternatives><graphic id="pone.0149958.e065g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e065" xlink:type="simple"/><mml:math display="inline" id="M65"><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula></th>
<th align="center"><italic>C</italic><sub><italic>llr</italic></sub></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">KDF</td>
<td align="char" char=".">0.0787</td>
<td align="char" char=".">0.0394</td>
<td align="char" char=".">0.1182</td>
<td align="char" char=".">0.0850</td>
<td align="char" char=".">0.0410</td>
<td align="char" char=".">0.1260</td>
</tr>
<tr>
<td align="left">GMM (<xref ref-type="disp-formula" rid="pone.0149958.e041">Eq 35</xref>)</td>
<td align="char" char="."><bold>0.0785</bold></td>
<td align="char" char="."><bold>0.0223</bold></td>
<td align="char" char="."><bold>0.1008</bold></td>
<td align="char" char=".">0.0863</td>
<td align="char" char=".">0.0291</td>
<td align="char" char=".">0.1154</td>
</tr>
<tr>
<td align="left">GMM (<xref ref-type="disp-formula" rid="pone.0149958.e042">Eq 36</xref>)</td>
<td align="char" char="."><bold>0.0785</bold></td>
<td align="char" char=".">0.0229</td>
<td align="char" char=".">0.1013</td>
<td align="char" char="."><bold>0.0862</bold></td>
<td align="char" char="."><bold>0.0282</bold></td>
<td align="char" char="."><bold>0.1144</bold></td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t004fn001">
<p><inline-formula id="pone.0149958.e066"><alternatives><graphic id="pone.0149958.e066g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e066" xlink:type="simple"/><mml:math display="inline" id="M66"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula></p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Finally, <xref ref-type="fig" rid="pone.0149958.g011">Fig 11</xref> shows the comparative results between KDF and GMM (<xref ref-type="disp-formula" rid="pone.0149958.e042">Eq 36</xref>) in the form of ECE plots when the <italic>cross-validation</italic> protocol is applied.</p>
<fig id="pone.0149958.g011" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149958.g011</object-id>
<label>Fig 11</label>
<caption>
<title>ECE plots for the KDF and GMM approaches on the glass-fragments dataset when applying the <italic>cross-validation</italic> protocol.</title>
<p>GMM is trained by maximizing <xref ref-type="disp-formula" rid="pone.0149958.e042">Eq 36</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149958.g011" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec018">
<title>Car-paints dataset</title>
<p>An equivalent analysis to that shown for the glass-fragments dataset has been performed for the car-paints one. <xref ref-type="fig" rid="pone.0149958.g012">Fig 12</xref> shows both the <italic>C</italic><sub><italic>llr</italic></sub> and the log-likelihood of the background data given the model (trained by maximizing <xref ref-type="disp-formula" rid="pone.0149958.e041">Eq 35</xref>) as a function of the number of components for the <italic>non-partitioning</italic> protocol. Similarly to what happened with the previous dataset, <italic>C</italic><sub><italic>llr</italic></sub> decreases as the number of components increases, and as soon as the log-likelihood for the GMM surpass that obtained for the KDF density, better results are obtained with the GMM approach. Again, this happens for a number of components (3–4) around that which could be expected from visual inspection of some of the 2-dimensional projections shown in <xref ref-type="fig" rid="pone.0149958.g007">Fig 7</xref>.</p>
<fig id="pone.0149958.g012" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149958.g012</object-id>
<label>Fig 12</label>
<caption>
<title>Analysis of the number of GMM components for the car-paints dataset when applying the <italic>non-partitioning</italic> protocol.</title>
<p>GMM is trained by maximizing <xref ref-type="disp-formula" rid="pone.0149958.e041">Eq 35</xref>. (Above) Log-likelihood ratio cost. (Below) Log-likelihood of the background data given the between-source density function.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149958.g012" xlink:type="simple"/>
</fig>
<p>
<xref ref-type="fig" rid="pone.0149958.g013">Fig 13</xref> show the same analysis for the <italic>cross-validation</italic> protocol (without showing the log-likelihood plot), where it can be seen (solid line) that, similarly to what happened with the glass-fragments dataset, a minimum <italic>C</italic><sub><italic>llr</italic></sub> value is reached for a particular number of components (<italic>C</italic> = 3) and then it increases. However, when plotting results for GMMs trained by maximizing <xref ref-type="disp-formula" rid="pone.0149958.e042">Eq 36</xref> instead (dot-dashed line), the number of components for which the minimum <italic>C</italic><sub><italic>llr</italic></sub> value is reached is slightly larger (<italic>C</italic> = 5); this also happens for the <italic>non-partition</italic> protocol, as the log-likelihood of the training data (observations) given the model for the GMM do not surpass that of the KDF until a larger number of components (<italic>C</italic> = 4) is reached.</p>
<fig id="pone.0149958.g013" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149958.g013</object-id>
<label>Fig 13</label>
<caption>
<title>Analysis of the number of GMM components for the car-paints dataset when applying the <italic>cross-validation</italic> protocol.</title>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149958.g013" xlink:type="simple"/>
</fig>
<p>
<xref ref-type="table" rid="pone.0149958.t005">Table 5</xref> shows the detailed results (<italic>C</italic><sub><italic>llr</italic></sub>, <inline-formula id="pone.0149958.e067"><alternatives><graphic id="pone.0149958.e067g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e067" xlink:type="simple"/><mml:math display="inline" id="M67"><mml:msubsup><mml:mi>C</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>l</mml:mi> <mml:mi>r</mml:mi></mml:mrow> <mml:mrow><mml:mi>m</mml:mi> <mml:mi>i</mml:mi> <mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula> and <inline-formula id="pone.0149958.e068"><alternatives><graphic id="pone.0149958.e068g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e068" xlink:type="simple"/><mml:math display="inline" id="M68"><mml:msubsup><mml:mi>C</mml:mi> <mml:mrow><mml:mi>l</mml:mi> <mml:mi>l</mml:mi> <mml:mi>r</mml:mi></mml:mrow> <mml:mrow><mml:mi>c</mml:mi> <mml:mi>a</mml:mi> <mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>) for KDF and GMM approaches (<xref ref-type="disp-formula" rid="pone.0149958.e041">Eq 35</xref> and <xref ref-type="disp-formula" rid="pone.0149958.e042">Eq 36</xref>) when applying both the <italic>non-partitioning</italic> and the <italic>cross-validation</italic> protocols. For GMM approaches, results are given for the optimum number of components (<italic>C</italic> = 4 for <xref ref-type="disp-formula" rid="pone.0149958.e041">Eq 35</xref>, <italic>C</italic> = 5 for <xref ref-type="disp-formula" rid="pone.0149958.e042">Eq 36</xref>) when the <italic>cross-validation</italic> protocol is applied. Similar conclusions to those obtained for the glass-fragments dataset can be drawn, but much better results are obtained by GMMs approaches presumably due to the distance among clusters, which lead to KDF densities which overestimate the between-source distribution in some areas of the feature space (as shown in <xref ref-type="fig" rid="pone.0149958.g002">Fig 2</xref> for the synthetic dataset). Among GMM approaches, the maximization of <xref ref-type="disp-formula" rid="pone.0149958.e042">Eq 36</xref> leads to much better results for the <italic>cross-validation</italic> protocol due to the small number of samples per source (<italic>n</italic> = 3) compared to the number of features (<italic>d</italic> = 7), which lead to unreliably computed sources means when training GMMs by maximizing <xref ref-type="disp-formula" rid="pone.0149958.e041">Eq 35</xref>.</p>
<table-wrap id="pone.0149958.t005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149958.t005</object-id>
<label>Table 5</label>
<caption>
<title>Performance of KDF and GMM approaches on the car-paints dataset for the <italic>non-partitioning</italic> and <italic>cross-validation</italic> protocols.</title>
</caption>
<alternatives>
<graphic id="pone.0149958.t005g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149958.t005" xlink:type="simple"/>
<table border="0" frame="box" rules="all">
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left"/>
<th align="center" colspan="3">Non-partitioning</th>
<th align="center" colspan="3">Cross-validation</th>
</tr>
<tr>
<th align="left"/>
<th align="center"><inline-formula id="pone.0149958.e069"><alternatives><graphic id="pone.0149958.e069g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e069" xlink:type="simple"/><mml:math display="inline" id="M69"><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula></th>
<th align="center"><inline-formula id="pone.0149958.e070"><alternatives><graphic id="pone.0149958.e070g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e070" xlink:type="simple"/><mml:math display="inline" id="M70"><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula></th>
<th align="center"><italic>C</italic><sub><italic>llr</italic></sub></th>
<th align="center"><inline-formula id="pone.0149958.e071"><alternatives><graphic id="pone.0149958.e071g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e071" xlink:type="simple"/><mml:math display="inline" id="M71"><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula></th>
<th align="center"><inline-formula id="pone.0149958.e072"><alternatives><graphic id="pone.0149958.e072g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e072" xlink:type="simple"/><mml:math display="inline" id="M72"><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula></th>
<th align="center"><italic>C</italic><sub><italic>llr</italic></sub></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">KDF</td>
<td align="char" char=".">0.0819</td>
<td align="char" char=".">0.1388</td>
<td align="char" char=".">0.2208</td>
<td align="char" char=".">0.0972</td>
<td align="char" char=".">0.1786</td>
<td align="char" char=".">0.2759</td>
</tr>
<tr>
<td align="left">GMM (<xref ref-type="disp-formula" rid="pone.0149958.e041">Eq 35</xref>)</td>
<td align="char" char="."><bold>0.0715</bold></td>
<td align="char" char="."><bold>0.0671</bold></td>
<td align="char" char="."><bold>0.1386</bold></td>
<td align="char" char=".">0.0968</td>
<td align="char" char=".">0.1769</td>
<td align="char" char=".">0.2737</td>
</tr>
<tr>
<td align="left">GMM (<xref ref-type="disp-formula" rid="pone.0149958.e042">Eq 36</xref>)</td>
<td align="char" char="."><bold>0.0715</bold></td>
<td align="char" char=".">0.0729</td>
<td align="char" char=".">0.1443</td>
<td align="char" char="."><bold>0.0899</bold></td>
<td align="char" char="."><bold>0.0934</bold></td>
<td align="char" char="."><bold>0.1833</bold></td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t005fn001">
<p><inline-formula id="pone.0149958.e073"><alternatives><graphic id="pone.0149958.e073g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e073" xlink:type="simple"/><mml:math display="inline" id="M73"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula></p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Finally, <xref ref-type="fig" rid="pone.0149958.g014">Fig 14</xref> shows the comparative results between KDF and GMM (<xref ref-type="disp-formula" rid="pone.0149958.e042">Eq 36</xref>) in the form of ECE plots when the <italic>cross-validation</italic> protocol is applied.</p>
<fig id="pone.0149958.g014" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0149958.g014</object-id>
<label>Fig 14</label>
<caption>
<title>ECE plots for the KDF and GMM approaches on the car-paints dataset when applying the <italic>cross-validation</italic> protocol.</title>
<p>GMM is trained by maximizing <xref ref-type="disp-formula" rid="pone.0149958.e042">Eq 36</xref>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0149958.g014" xlink:type="simple"/>
</fig>
</sec>
</sec>
<sec id="sec019" sec-type="conclusions">
<title>Conclusions</title>
<p>In this work, we present a new approach for computing likelihood ratios from multivariate data in which the between-source distribution is obtained through ML training of the parameters of a GMM. Using the same generative model as in [<xref ref-type="bibr" rid="pone.0149958.ref010">10</xref>], a common derivation of the LR expressions is presented for both Gaussian KDF and GMM, in which the between-source distribution is represented in terms of a weighted sum of Gaussian densities. Then, differences between KDF and GMM approaches are highlighted, and the effects on the obtained probability density are shown for a synthetic dataset. Furthermore, a variant in GMM training has been tested in order to account for the uncertainty in sources means when few samples per source are available in the background data.</p>
<p>The proposed approach has been tested on three different forensic datasets and compared with the KDF approach. Additionally to the <italic>non-partitioning</italic> protocol applied in [<xref ref-type="bibr" rid="pone.0149958.ref010">10</xref>], a more realistic <italic>cross-validation</italic> protocol is applied in order to avoid overoptimistic results, as ML-trained GMMs can overfit the background population density. Performance is evaluated in terms of the log-likelihood ratio cost function (<italic>C</italic><sub><italic>llr</italic></sub>), which allows to decompose the performance in a term due to the discrimination abilities and another one due to the calibration properties. ECE plots have been used to show the behaviour in a wide range of prior probabilities, which is needed in forensic science.</p>
<p>Results show that, although KDF and GMM approaches present similar discrimination abilities, when the datasets have a <italic>clustered</italic> nature, the between-source distribution is better described by a GMM, leading to better calibrated likelihood ratios. If clusters are not easily distinguishable, the between-source distribution still can be modelled by one single component, obtaining similar results to the KDF approach. Specially remarkable are the results obtained for the car-paints dataset, where ∼50% improvement in terms of calibration performance is obtained.</p>
</sec>
<sec id="sec020">
<title>Appendix</title>
<sec id="sec021">
<title>Mathematical notation</title>
<p>Throughout this work we consider multivariate data in the form of <italic>d</italic>-dimensional column vectors <bold>x</bold> = (<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, …, <italic>x</italic><sub><italic>d</italic></sub>)<sup><italic>T</italic></sup>. Following the same notation as in [<xref ref-type="bibr" rid="pone.0149958.ref010">10</xref>], a set of <italic>n</italic> elements of such data belonging to the same particular source <italic>i</italic> are denoted by <bold>x</bold><sub><italic>i</italic></sub> = {<bold>x</bold><sub><italic>ij</italic></sub>}<sub><italic>j</italic> = 1, ‥, <italic>n</italic></sub> = {<bold>x</bold><sub><italic>i</italic>1</sub>,<bold>x</bold><sub><italic>i</italic>2</sub>, …,<bold>x</bold><sub><italic>in</italic></sub>}, while their sample mean is denoted by <inline-formula id="pone.0149958.e074"><alternatives><graphic id="pone.0149958.e074g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e074" xlink:type="simple"/><mml:math display="inline" id="M74"><mml:msub><mml:mover accent="true"><mml:mtext>x</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula>. Similarly, <bold>x</bold><sub><italic>i</italic></sub> is used to denote background data while <bold>y</bold><sub><italic>l</italic></sub> is used to denote either control (<bold>y</bold><sub>1</sub>) or recovered data (<bold>y</bold><sub>2</sub>). The set of feature vectors coming from different sources present in the background data is denoted by <bold>X</bold>.</p>
<p>In general, column vectors are denoted by bold lower-case letters and matrices by bold upper-case letters, while scalar quantities are denoted by lower-case italic letters. Random variables are denoted by upper-case non-italic letters. <italic>P</italic>(⋅) is used to indicate the probability of a certain event, while <italic>p</italic>(⋅) denotes a probability density function. We denote a <italic>d</italic>-dimensional Gaussian distribution with mean <italic>μ</italic> and covariance matrix <italic>Σ</italic> by <inline-formula id="pone.0149958.e075"><alternatives><graphic id="pone.0149958.e075g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e075" xlink:type="simple"/><mml:math display="inline" id="M75"><mml:mrow><mml:mi mathvariant="script">N</mml:mi> <mml:mo>(</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="normal">Σ</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> and the corresponding probability density function by <italic>N</italic>(<bold>x</bold>;<italic>μ</italic>, <italic>Σ</italic>) (<inline-formula id="pone.0149958.e076"><alternatives><graphic id="pone.0149958.e076g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e076" xlink:type="simple"/><mml:math display="inline" id="M76"><mml:mrow><mml:mtext>x</mml:mtext> <mml:mo>∈</mml:mo> <mml:msup><mml:mi mathvariant="double-struck">R</mml:mi> <mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>).</p>
</sec>
<sec id="sec022">
<title>Multivariate Gaussian function</title>
<disp-formula id="pone.0149958.e077">
<alternatives>
<graphic id="pone.0149958.e077g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e077" xlink:type="simple"/>
<mml:math display="block" id="M77">
<mml:mtable displaystyle="true">
<mml:mtr>
<mml:mtd columnalign="right">
<mml:mrow>
<mml:mi>N</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mtext>x</mml:mtext>
<mml:mo>;</mml:mo>
<mml:mi>μ</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi mathvariant="normal">Σ</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mo>=</mml:mo>
<mml:msup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mn>2</mml:mn>
<mml:mi>π</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>-</mml:mo>
<mml:mi>d</mml:mi>
<mml:mo>/</mml:mo>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msup>
<mml:msup>
<mml:mfenced close="|" open="|">
<mml:mi mathvariant="normal">Σ</mml:mi>
</mml:mfenced>
<mml:mrow>
<mml:mo>-</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>/</mml:mo>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mo form="prefix">exp</mml:mo>
<mml:mfenced close="}" open="{" separators="">
<mml:mo>-</mml:mo>
<mml:mfrac>
<mml:mn>1</mml:mn>
<mml:mn>2</mml:mn>
</mml:mfrac>
<mml:msup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mtext>x</mml:mtext>
<mml:mo>-</mml:mo>
<mml:mi>μ</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mi mathvariant="normal">T</mml:mi>
</mml:msup>
<mml:msup>
<mml:mrow>
<mml:mi mathvariant="normal">Σ</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo>-</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mtext>x</mml:mtext>
<mml:mo>-</mml:mo>
<mml:mi>μ</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mfenced>
<mml:mo>=</mml:mo>
<mml:mi>N</mml:mi>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi>μ</mml:mi>
<mml:mo>;</mml:mo>
<mml:mtext>x</mml:mtext>
<mml:mo>,</mml:mo>
<mml:mi mathvariant="normal">Σ</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:math>
</alternatives>
<label>(39)</label>
</disp-formula>
</sec>
<sec id="sec023">
<title>Gaussian identities</title>
<sec id="sec024">
<title>Product of two multivariate Gaussian functions</title>
<disp-formula id="pone.0149958.e078">
<alternatives>
<graphic id="pone.0149958.e078g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e078" xlink:type="simple"/>
<mml:math display="block" id="M78">
<mml:mtable displaystyle="true">
<mml:mtr>
<mml:mtd columnalign="right">
<mml:mrow>
<mml:mi>N</mml:mi>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">x</mml:mi>
<mml:mo>;</mml:mo>
<mml:mi mathvariant="bold">a</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi mathvariant="bold">A</mml:mi>
<mml:mo>)</mml:mo>
<mml:mo>·</mml:mo>
<mml:mi>N</mml:mi>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">x</mml:mi>
<mml:mo>;</mml:mo>
<mml:mi mathvariant="bold">b</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi mathvariant="bold">B</mml:mi>
<mml:mo>)</mml:mo>
<mml:mo>=</mml:mo>
<mml:mi>N</mml:mi>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">a</mml:mi>
<mml:mo>;</mml:mo>
<mml:mi mathvariant="bold">b</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi mathvariant="bold">a</mml:mi>
<mml:mo>+</mml:mo>
<mml:mi mathvariant="bold">B</mml:mi>
<mml:mo>)</mml:mo>
<mml:mo>·</mml:mo>
<mml:mi>N</mml:mi>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">x</mml:mi>
<mml:mo>;</mml:mo>
<mml:mi mathvariant="bold">c</mml:mi>
<mml:mo>,</mml:mo>
<mml:mi mathvariant="bold">C</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:math>
</alternatives>
<label>(40)</label>
</disp-formula>
<disp-formula id="pone.0149958.e079">
<alternatives>
<graphic id="pone.0149958.e079g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e079" xlink:type="simple"/>
<mml:math display="block" id="M79">
<mml:mtable displaystyle="true">
<mml:mtr>
<mml:mtd columnalign="right">
<mml:mrow>
<mml:mi mathvariant="bold">c</mml:mi>
<mml:mo>=</mml:mo>
<mml:mi mathvariant="bold">B</mml:mi>
<mml:msup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">A</mml:mi>
<mml:mo>+</mml:mo>
<mml:mi mathvariant="bold">B</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>-</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mi mathvariant="bold">a</mml:mi>
<mml:mo>+</mml:mo>
<mml:mi mathvariant="bold">A</mml:mi>
<mml:msup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">A</mml:mi>
<mml:mo>+</mml:mo>
<mml:mi mathvariant="bold">B</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>-</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mi mathvariant="bold">b</mml:mi>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:math>
</alternatives>
<label>(41)</label>
</disp-formula>
<disp-formula id="pone.0149958.e080">
<alternatives>
<graphic id="pone.0149958.e080g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e080" xlink:type="simple"/>
<mml:math display="block" id="M80">
<mml:mtable displaystyle="true">
<mml:mtr>
<mml:mtd columnalign="right">
<mml:mrow>
<mml:mi mathvariant="bold">C</mml:mi>
<mml:mo>=</mml:mo>
<mml:mi mathvariant="bold">A</mml:mi>
<mml:msup>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mi mathvariant="bold">A</mml:mi>
<mml:mo>+</mml:mo>
<mml:mi mathvariant="bold">B</mml:mi>
<mml:mo>)</mml:mo>
</mml:mrow>
<mml:mrow>
<mml:mo>-</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msup>
<mml:mi mathvariant="bold">B</mml:mi>
</mml:mrow>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:math>
</alternatives>
<label>(42)</label>
</disp-formula>
</sec>
<sec id="sec025">
<title>Convolution of two multivariate Gaussian functions</title>
<disp-formula id="pone.0149958.e081"><alternatives><graphic id="pone.0149958.e081g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e081" xlink:type="simple"/><mml:math display="block" id="M81"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mo>∫</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">a</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">b</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">B</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>;</mml:mo><mml:mi mathvariant="bold">a</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold">b</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">A</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold">B</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives><label>(43)</label></disp-formula>
</sec>
</sec>
<sec id="sec026">
<title>Expressions for a normal between-source distribution</title>
<sec id="sec027">
<title>Derivation of the numerator</title>
<p>First, we solve the product of the two Gaussian functions depending on either the control or the recovered data means, obtaining the following expression
<disp-formula id="pone.0149958.e082"><alternatives><graphic id="pone.0149958.e082g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e082" xlink:type="simple"/><mml:math display="block" id="M82"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mo>∫</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mrow><mml:mo>{</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mo>∫</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mrow><mml:mo>{</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mo>∫</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mrow><mml:mo>{</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(44)</label></disp-formula>
where
<disp-formula id="pone.0149958.e083"><alternatives><graphic id="pone.0149958.e083g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e083" xlink:type="simple"/><mml:math display="block" id="M83"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">z</mml:mi> <mml:mo>=</mml:mo> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(45)</label></disp-formula>
and
<disp-formula id="pone.0149958.e084"><alternatives><graphic id="pone.0149958.e084g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e084" xlink:type="simple"/><mml:math display="block" id="M84"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>=</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(46)</label></disp-formula></p>
<p>Being <inline-formula id="pone.0149958.e085"><alternatives><graphic id="pone.0149958.e085g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e085" xlink:type="simple"/><mml:math display="inline" id="M85"><mml:mrow><mml:mi>N</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula> independent of <italic>θ</italic>, we can solve the remaining integral as a convolution of two Gaussian functions:
<disp-formula id="pone.0149958.e086"><alternatives><graphic id="pone.0149958.e086g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e086" xlink:type="simple"/><mml:math display="block" id="M86"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:msub><mml:mo>∫</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mrow><mml:mo>{</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:msub><mml:mo>∫</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mrow><mml:mo>{</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>-</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi mathvariant="bold">z</mml:mi> <mml:mo>;</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(47)</label></disp-formula></p>
<p>Finally, replacing <bold>D</bold><sub><italic>l</italic></sub> = <bold>W</bold>/<italic>n</italic><sub><italic>l</italic></sub>, <italic>l</italic> = 1, 2, in <bold>z</bold> and <bold>Z</bold> <disp-formula id="pone.0149958.e087"><alternatives><graphic id="pone.0149958.e087g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e087" xlink:type="simple"/><mml:math display="block" id="M87"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">z</mml:mi> <mml:mo>=</mml:mo> <mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mi mathvariant="bold">W</mml:mi> <mml:msub><mml:mi>n</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mi mathvariant="bold">W</mml:mi> <mml:msub><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mfrac></mml:mfenced> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mi mathvariant="bold">W</mml:mi> <mml:msub><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mfrac> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mfrac><mml:mi mathvariant="bold">W</mml:mi> <mml:msub><mml:mi>n</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mfrac> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub></mml:mfenced> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mfrac> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>n</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mfrac> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub></mml:mrow> <mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>n</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mfrac></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub></mml:mrow> <mml:mrow><mml:msub><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:msup><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(48)</label></disp-formula> <disp-formula id="pone.0149958.e088"><alternatives><graphic id="pone.0149958.e088g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e088" xlink:type="simple"/><mml:math display="block" id="M88"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi mathvariant="bold">Z</mml:mi> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi mathvariant="bold">W</mml:mi> <mml:msub><mml:mi>n</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mfrac> <mml:msup><mml:mrow><mml:mo>(</mml:mo> <mml:mfrac><mml:mi mathvariant="bold">W</mml:mi> <mml:msub><mml:mi>n</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mi mathvariant="bold">W</mml:mi> <mml:msub><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mrow><mml:mo>-</mml:mo> <mml:mn>1</mml:mn></mml:mrow></mml:msup> <mml:mfrac><mml:mi mathvariant="bold">W</mml:mi> <mml:msub><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mfrac><mml:mi mathvariant="bold">W</mml:mi> <mml:mrow><mml:msub><mml:mi>n</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:msub><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac> <mml:mrow><mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>n</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mn>1</mml:mn> <mml:msub><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mfrac></mml:mrow></mml:mfrac> <mml:mo>=</mml:mo> <mml:mfrac><mml:mi mathvariant="bold">W</mml:mi> <mml:mrow><mml:msub><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(49)</label></disp-formula>
we obtain
<disp-formula id="pone.0149958.e089"><alternatives><graphic id="pone.0149958.e089g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e089" xlink:type="simple"/><mml:math display="block" id="M89"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>1</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>,</mml:mo> <mml:mfrac><mml:mi mathvariant="bold">W</mml:mi> <mml:msub><mml:mi>n</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mfrac> <mml:mo>+</mml:mo> <mml:mfrac><mml:mi mathvariant="bold">W</mml:mi> <mml:msub><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mfrac> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msup><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mo>*</mml:mo></mml:msup> <mml:mo>;</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:mfrac><mml:mi mathvariant="bold">W</mml:mi> <mml:mrow><mml:msub><mml:mi>n</mml:mi> <mml:mn>1</mml:mn></mml:msub> <mml:mo>+</mml:mo> <mml:msub><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(50)</label></disp-formula></p>
</sec>
<sec id="sec028">
<title>Derivation of the denominator</title>
<p>Each of the integrals in the denominator of the LR can be solved by the convolution of two Gaussian functions
<disp-formula id="pone.0149958.e090"><alternatives><graphic id="pone.0149958.e090g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e090" xlink:type="simple"/><mml:math display="block" id="M90"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow></mml:mtd> <mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mo>∫</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mrow><mml:mo>{</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:msub><mml:mo>∫</mml:mo> <mml:mi>θ</mml:mi></mml:msub> <mml:mrow><mml:mo>{</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mo>-</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mn>0</mml:mn> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:mi>θ</mml:mi> <mml:mo>;</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>}</mml:mo></mml:mrow> <mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">d</mml:mi> <mml:mi>θ</mml:mi></mml:mrow></mml:mtd></mml:mtr> <mml:mtr><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:msub><mml:mi mathvariant="bold">D</mml:mi> <mml:mi>l</mml:mi></mml:msub> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mi>l</mml:mi></mml:msub> <mml:mo>;</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:mfrac><mml:mi mathvariant="bold">W</mml:mi> <mml:msub><mml:mi>n</mml:mi> <mml:mi>l</mml:mi></mml:msub></mml:mfrac> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(51)</label></disp-formula>
giving the following final expression for the denominator of the LR under the between-source normal assumption:
<disp-formula id="pone.0149958.e091"><alternatives><graphic id="pone.0149958.e091g" mimetype="image" position="anchor" xlink:href="info:doi/10.1371/journal.pone.0149958.e091" xlink:type="simple"/><mml:math display="block" id="M91"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>p</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>1</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mi>P</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mtext>y</mml:mtext> <mml:mn>2</mml:mn></mml:msub> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>=</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>1</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:mfrac><mml:mi mathvariant="bold">W</mml:mi> <mml:msub><mml:mi>n</mml:mi> <mml:mn>1</mml:mn></mml:msub></mml:mfrac> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow> <mml:mo>·</mml:mo> <mml:mi>N</mml:mi> <mml:mrow><mml:mo>(</mml:mo> <mml:msub><mml:mover accent="true"><mml:mtext>y</mml:mtext> <mml:mo>¯</mml:mo></mml:mover> <mml:mn>2</mml:mn></mml:msub> <mml:mo>;</mml:mo> <mml:mi>μ</mml:mi> <mml:mo>,</mml:mo> <mml:mfrac><mml:mi mathvariant="bold">W</mml:mi> <mml:msub><mml:mi>n</mml:mi> <mml:mn>2</mml:mn></mml:msub></mml:mfrac> <mml:mo>+</mml:mo> <mml:mi mathvariant="bold">B</mml:mi> <mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives> <label>(52)</label></disp-formula></p>
</sec>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="pone.0149958.ref001">
<label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Cook</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Evett</surname> <given-names>IW</given-names></name>, <name name-style="western"><surname>Jackson</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Jones</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Lambert</surname> <given-names>JA</given-names></name>. <article-title>A hierarchy of propositions: deciding which level to address in casework</article-title>. <source>Science and Justice</source>. <year>1998</year>;<volume>38</volume>(<issue>4</issue>):<fpage>231</fpage>–<lpage>239</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/S1355-0306(98)72117-3" xlink:type="simple">10.1016/S1355-0306(98)72117-3</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149958.ref002">
<label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>van Leeuwen</surname> <given-names>DA</given-names></name>, <name name-style="western"><surname>Brümmer</surname> <given-names>N</given-names></name>. <article-title>An Introduction to Application-Independent Evaluation of Speaker Recognition Systems</article-title>. <source>Speaker Classification I: Lecture Notes in Computer Science</source>.<year>2007</year>;<volume>4343</volume>:<fpage>330</fpage>–<lpage>353</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1007/978-3-540-74200-5_19" xlink:type="simple">10.1007/978-3-540-74200-5_19</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149958.ref003">
<label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Gonzalez-Rodriguez</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Rose</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Ramos</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Toledano</surname> <given-names>DT</given-names></name>, <name name-style="western"><surname>Ortega-Garcia</surname> <given-names>J</given-names></name>. <article-title>Emulating DNA: Rigorous Quantification of Evidential Weight in Transparent and Testable Forensic Speaker Recognition</article-title>. <source>IEEE Transactions on Audio, Speech, and Language Processing</source>. <year>2007</year>;<volume>15</volume>(<issue>7</issue>):<fpage>2104</fpage>–<lpage>2115</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TASL.2007.902747" xlink:type="simple">10.1109/TASL.2007.902747</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149958.ref004">
<label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Jain</surname> <given-names>AK</given-names></name>, <name name-style="western"><surname>Ross</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Pankanti</surname> <given-names>S</given-names></name>. <article-title>Biometrics: a tool for information security</article-title>. <source>IEEE Transactions on Information Forensics and Security</source>. <year>2006</year>;<volume>1</volume>(<issue>2</issue>):<fpage>125</fpage>–<lpage>143</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TIFS.2006.873653" xlink:type="simple">10.1109/TIFS.2006.873653</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149958.ref005">
<label>5</label>
<mixed-citation publication-type="other" xlink:type="simple">Turk MA, Pentland AP. Face recognition using eigenfaces. Proceedings of the 1991 CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 1991;586–591.</mixed-citation>
</ref>
<ref id="pone.0149958.ref006">
<label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Campbell</surname> <given-names>WM</given-names></name>, <name name-style="western"><surname>Sturim</surname> <given-names>DE</given-names></name>, <name name-style="western"><surname>Reynolds</surname> <given-names>DA</given-names></name>. <article-title>Support vector machines using GMM supervectors for speaker verification</article-title>. <source>IEEE Signal Processing Letters</source>. <year>2006</year>;<volume>13</volume>(<issue>5</issue>):<fpage>308</fpage>–<lpage>311</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/LSP.2006.870086" xlink:type="simple">10.1109/LSP.2006.870086</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149958.ref007">
<label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Li</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Fu</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Mohammed</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Elder</surname> <given-names>JH</given-names></name>, <name name-style="western"><surname>Prince</surname> <given-names>SJD</given-names></name>. <article-title>Probabilistic Models for Inference about Identity. IEEE Transactions on Pattern Analysis and Machine Intelligence</article-title>. <source>January</source> <year>2012</year>;<volume>34</volume>(<issue>1</issue>):<fpage>144</fpage>–<lpage>157</lpage>.</mixed-citation>
</ref>
<ref id="pone.0149958.ref008">
<label>8</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Sizov</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Lee</surname> <given-names>KA</given-names></name>, <name name-style="western"><surname>Kinnunen</surname> <given-names>T</given-names></name>. <article-title>Unifying Probabilistic Linear Discriminant Analysis Variants in Biometric Authentication. Structural, Syntactic, and Statistical Pattern Recognition</article-title>. <source>Lecture Notes in Computer Science</source>. <year>2014</year>;<volume>8621</volume>:<fpage>464</fpage>–<lpage>475</lpage>.</mixed-citation>
</ref>
<ref id="pone.0149958.ref009">
<label>9</label>
<mixed-citation publication-type="other" xlink:type="simple">Borgstrom BJ, McCree A. Supervector Bayesian speaker comparison. Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2013;7693–7697.</mixed-citation>
</ref>
<ref id="pone.0149958.ref010">
<label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Aitken</surname> <given-names>CGG</given-names></name>, <name name-style="western"><surname>Lucy</surname> <given-names>D</given-names></name>. <article-title>Evaluation of trace evidence in the form of multivariate data</article-title>. <source>Journal of the Royal Statistical Society: Series C (Applied Statistics)</source>. <year>2004</year>;<volume>53</volume>:<fpage>109</fpage>–<lpage>122</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1046/j.0035-9254.2003.05271.x" xlink:type="simple">10.1046/j.0035-9254.2003.05271.x</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149958.ref011">
<label>11</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Aitken</surname> <given-names>CGG</given-names></name>, <name name-style="western"><surname>Taroni</surname> <given-names>F</given-names></name>. <source>Statistics and the Evaluation of Evidence for Forensic Scientists</source>, <edition>2nd</edition> Edition. <publisher-name>Wiley</publisher-name>; <month>July</month> <year>2004</year>.</mixed-citation>
</ref>
<ref id="pone.0149958.ref012">
<label>12</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Zadora</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Martyna</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Ramos</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Aitken</surname> <given-names>C</given-names></name>. <source>Statistical Analysis in Forensic Science: Evidential Values of Multivariate Physicochemical Data</source>. <publisher-name>Wiley</publisher-name>; <month>January</month> <year>2014</year>.</mixed-citation>
</ref>
<ref id="pone.0149958.ref013">
<label>13</label>
<mixed-citation publication-type="other" xlink:type="simple">Rose P. Forensic Voice Comparison with Monophthongal Formant Trajectories—a likelihood ratio-based discrimination of “Schwa” vowel acoustics in a close social group of young Australian females. Proceedings of the 40th ICASSP International Conference on Acoustics, Speech and Signal Processing. 2015;4819–4823.</mixed-citation>
</ref>
<ref id="pone.0149958.ref014">
<label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Bolck</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Weyermann</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Dujourdy</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Esseiva</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>van den Berg</surname> <given-names>J</given-names></name>. <article-title>Different likelihood ratio approaches to evaluate the strength of evidence of MDMA tablet comparisons</article-title>. <source>Forensic Science International</source>. <year>2009</year>;<volume>191</volume>(<issue>1–3</issue>):<fpage>42</fpage>–<lpage>51</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.forsciint.2009.06.006" xlink:type="simple">10.1016/j.forsciint.2009.06.006</ext-link></comment> <object-id pub-id-type="pmid">19608360</object-id></mixed-citation>
</ref>
<ref id="pone.0149958.ref015">
<label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Epanechnikov</surname> <given-names>VA</given-names></name>. <article-title>Non-Parametric Estimation of a Multivariate Probability Density</article-title>. <source>Theory Probab. Appl.</source>, <volume>14</volume>(<issue>1</issue>):<fpage>153</fpage>–<lpage>158</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1137/1114019" xlink:type="simple">10.1137/1114019</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149958.ref016">
<label>16</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>McLachlan</surname> <given-names>GJ</given-names></name>, <name name-style="western"><surname>Basford</surname> <given-names>KE</given-names></name>. <source>Mixture models: Inference and applications to clustering</source>. <publisher-name>Applied Statistics</publisher-name>. <year>1988</year>.</mixed-citation>
</ref>
<ref id="pone.0149958.ref017">
<label>17</label>
<mixed-citation publication-type="book" xlink:type="simple">
<name name-style="western"><surname>Bishop</surname> <given-names>CM</given-names></name>. <source>Pattern Recognition and Machine Learning</source>. <publisher-name>Springer</publisher-name>; <year>2006</year>.</mixed-citation>
</ref>
<ref id="pone.0149958.ref018">
<label>18</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Laird</surname> <given-names>NM</given-names></name>, <name name-style="western"><surname>Ware</surname> <given-names>JH</given-names></name>. <article-title>Random-Effects Models for Longitudinal Data</article-title>. <source>Biometrics</source>. <year>1982</year>;<volume>38</volume>(<issue>4</issue>):<fpage>963</fpage>–<lpage>974</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.2307/2529876" xlink:type="simple">10.2307/2529876</ext-link></comment> <object-id pub-id-type="pmid">7168798</object-id></mixed-citation>
</ref>
<ref id="pone.0149958.ref019">
<label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Dempster</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Laird</surname> <given-names>NM</given-names></name>, <name name-style="western"><surname>Rubin</surname> <given-names>DB</given-names></name>. <article-title>Maximum Likelihood from Incomplete Data via the EM Algorithm</article-title>. <source>Journal of the Royal Statistical Society, Series B</source>. <year>1977</year>;<volume>39</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>38</lpage>.</mixed-citation>
</ref>
<ref id="pone.0149958.ref020">
<label>20</label>
<mixed-citation publication-type="other" xlink:type="simple">MacQueen JB. Some Methods for classification and Analysis of Multivariate Observations. Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability 1. University of California Press. 1967;281–297.</mixed-citation>
</ref>
<ref id="pone.0149958.ref021">
<label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ketchen</surname> <given-names>DJ</given-names></name>, <name name-style="western"><surname>Shook</surname> <given-names>CL</given-names></name>. <article-title>The application of cluster analysis in Strategic Management Research: An analysis and critique</article-title>. <source>Strategic Management Journal</source>. <year>1996</year>;<volume>17</volume>(<issue>6</issue>):<fpage>441</fpage>–<lpage>458</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1002/(SICI)1097-0266(199606)17:6%3C441::AID-SMJ819%3E3.0.CO;2-G" xlink:type="simple">10.1002/(SICI)1097-0266(199606)17:6%3C441::AID-SMJ819%3E3.0.CO;2-G</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149958.ref022">
<label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Schwarz</surname> <given-names>G</given-names></name>. <article-title>Estimating the dimension of a model</article-title>. <source>Annals of Statistics</source>. <year>1978</year>;<volume>6</volume>(<issue>2</issue>):<fpage>461</fpage>–<lpage>464</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1214/aos/1176344136" xlink:type="simple">10.1214/aos/1176344136</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149958.ref023">
<label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Akaike</surname> <given-names>H</given-names></name>. <article-title>A new look at the statistical model identification</article-title>. <source>IEEE Transactions on Automatic Control</source>. <year>1974</year>;<volume>19</volume>(<issue>6</issue>):<fpage>716</fpage>–<lpage>723</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/TAC.1974.1100705" xlink:type="simple">10.1109/TAC.1974.1100705</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149958.ref024">
<label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Brümmer</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>du Preez</surname> <given-names>J</given-names></name>. <article-title>Application-independent evaluation of speaker detection</article-title>. <source>Computer Speech and Language</source>. <year>2006</year>;<volume>20</volume>:<fpage>230</fpage>–<lpage>275</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.csl.2005.08.001" xlink:type="simple">10.1016/j.csl.2005.08.001</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149958.ref025">
<label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ahuja</surname> <given-names>RK</given-names></name>, <name name-style="western"><surname>Orlin</surname> <given-names>JB</given-names></name>. <article-title>A fast scaling algorithm for minimizing separable convex functions subject to chain constraints</article-title>. <source>Operations Research</source>. <year>2001</year>;<volume>49</volume>:<fpage>784</fpage>–<lpage>789</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1287/opre.49.5.784.10601" xlink:type="simple">10.1287/opre.49.5.784.10601</ext-link></comment></mixed-citation>
</ref>
<ref id="pone.0149958.ref026">
<label>26</label>
<mixed-citation publication-type="other" xlink:type="simple">Zadrozny B, Elkan C. Transforming classifier scores into accurate multiclass probability estimates. Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 2002;694–699.</mixed-citation>
</ref>
<ref id="pone.0149958.ref027">
<label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ramos</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Gonzalez-Rodriguez</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Zadora</surname> <given-names>G</given-names></name>, <name name-style="western"><surname>Aitken</surname> <given-names>C</given-names></name>. <article-title>Information-Theoretical Assessment of the Performance of Likelihood Ratio Models</article-title>. <source>Journal of Forensic Sciences</source>. <month>November</month> <year>2013</year>;<volume>58</volume>(<issue>6</issue>):<fpage>1503</fpage>–<lpage>1518</lpage>.</mixed-citation>
</ref>
<ref id="pone.0149958.ref028">
<label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple">
<name name-style="western"><surname>Ramos</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Gonzalez-Rodriguez</surname> <given-names>J</given-names></name>. <article-title>Reliable support: measuring calibration of likelihood ratios</article-title>. <source>Forensic Science International</source>. <year>2013</year>;<volume>230</volume>:<fpage>156</fpage>–<lpage>169</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/j.forsciint.2013.04.014" xlink:type="simple">10.1016/j.forsciint.2013.04.014</ext-link></comment> <object-id pub-id-type="pmid">23664798</object-id></mixed-citation>
</ref>
</ref-list>
</back>
</article>