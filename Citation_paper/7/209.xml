<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id><journal-title-group>
<journal-title>PLoS ONE</journal-title></journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, USA</publisher-loc></publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">PONE-D-12-34668</article-id>
<article-id pub-id-type="doi">10.1371/journal.pone.0069684</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Biology</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive neuroscience</subject><subj-group><subject>Cognition</subject></subj-group></subj-group><subj-group><subject>Neuroimaging</subject><subj-group><subject>fMRI</subject></subj-group></subj-group><subj-group><subject>Sensory systems</subject><subj-group><subject>Visual system</subject></subj-group></subj-group><subj-group><subject>Neuroanatomy</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Medicine</subject><subj-group><subject>Mental health</subject><subj-group><subject>Psychology</subject><subj-group><subject>Experimental psychology</subject><subject>Social psychology</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v2"><subject>Social and behavioral sciences</subject><subj-group><subject>Anthropology</subject><subj-group><subject>Anthropometry</subject></subj-group></subj-group><subj-group><subject>Psychology</subject><subj-group><subject>Experimental psychology</subject><subject>Social psychology</subject></subj-group></subj-group><subj-group><subject>Sociology</subject><subj-group><subject>Social stratification</subject><subject>Social systems</subject></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Multivoxel Patterns in Fusiform Face Area Differentiate Faces by Sex and Race</article-title>
<alt-title alt-title-type="running-head">Sex and Race Representations in Fusiform Face Area</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Contreras</surname><given-names>Juan Manuel</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1"><sup>*</sup></xref></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Banaji</surname><given-names>Mahzarin R.</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
<contrib contrib-type="author" xlink:type="simple"><name name-style="western"><surname>Mitchell</surname><given-names>Jason P.</given-names></name><xref ref-type="aff" rid="aff1"/></contrib>
</contrib-group>
<aff id="aff1"><addr-line>Department of Psychology, Harvard University, Cambridge, Massachusetts, United States of America</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple"><name name-style="western"><surname>Yovel</surname><given-names>Galit</given-names></name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/></contrib>
</contrib-group>
<aff id="edit1"><addr-line>Tel Aviv University, Israel</addr-line></aff>
<author-notes>
<corresp id="cor1">* E-mail: <email xlink:type="simple">jmcontr@fas.harvard.edu</email></corresp>
<fn fn-type="conflict"><p>The authors have declared that no competing interests exist.</p></fn>
<fn fn-type="con"><p>Conceived and designed the experiments: JMC MRB JPM. Performed the experiments: JMC. Analyzed the data: JMC JPM. Contributed reagents/materials/analysis tools: JMC. Wrote the paper: JMC MRB JPM.</p></fn>
</author-notes>
<pub-date pub-type="collection"><year>2013</year></pub-date>
<pub-date pub-type="epub"><day>31</day><month>7</month><year>2013</year></pub-date>
<volume>8</volume>
<issue>7</issue>
<elocation-id>e69684</elocation-id>
<history>
<date date-type="received"><day>6</day><month>11</month><year>2012</year></date>
<date date-type="accepted"><day>17</day><month>6</month><year>2013</year></date>
</history>
<permissions>
<copyright-year>2013</copyright-year>
<copyright-holder>Contreras et al</copyright-holder><license xlink:type="simple"><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p></license></permissions>
<abstract>
<p>Although prior research suggests that fusiform gyrus represents the sex and race of faces, it remains unclear whether fusiform face area (FFA)–the portion of fusiform gyrus that is functionally-defined by its preferential response to faces–contains such representations. Here, we used functional magnetic resonance imaging to evaluate whether FFA represents faces by sex and race. Participants were scanned while they categorized the sex and race of unfamiliar Black men, Black women, White men, and White women. Multivariate pattern analysis revealed that multivoxel patterns in FFA–but not other face-selective brain regions, other category-selective brain regions, or early visual cortex–differentiated faces by sex and race. Specifically, patterns of voxel-based responses were more similar between individuals of the same sex than between men and women, and between individuals of the same race than between Black and White individuals. By showing that FFA represents the sex and race of faces, this research contributes to our emerging understanding of how the human brain perceives individuals from two fundamental social categories.</p>
</abstract>
<funding-group><funding-statement>JMC was supported by graduate fellowships from the Department of Education and the National Science Foundation of the United States. These organizations had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement></funding-group><counts><page-count count="6"/></counts></article-meta>
</front>
<body><sec id="s1">
<title>Introduction</title>
<p>One of the seminal breakthroughs in cognitive neuroscience was the discovery of a region of fusiform gyrus that responds preferentially to human faces, dubbed fusiform face area [FFA; 1,2]. FFA is thought to extract the physical information that distinguishes the faces of different people; that is, to represent face identity (for review, see <xref ref-type="bibr" rid="pone.0069684-Kanwisher2">[3]</xref>). Familiar faces elicit more neural activity in FFA than unrecognized faces <xref ref-type="bibr" rid="pone.0069684-GrillSpector1">[4]</xref>, and lesions to FFA impair face recognition <xref ref-type="bibr" rid="pone.0069684-Barton1">[5]</xref>. Moreover, experiments using neural adaptation–in which repeated presentation of a stimulus property decreases neural activity in brain regions that represent the property <xref ref-type="bibr" rid="pone.0069684-GrillSpector2">[6]</xref>–suggest that FFA is more sensitive to changes in face identity than to physical changes unrelated to face identity <xref ref-type="bibr" rid="pone.0069684-Rotshtein1">[7]</xref>, <xref ref-type="bibr" rid="pone.0069684-DaviesThompson1">[8]</xref>; cf. <xref ref-type="bibr" rid="pone.0069684-Andrews1">[9]</xref>, <xref ref-type="bibr" rid="pone.0069684-Xu1">[10]</xref>.</p>
<p>But it is impossible to identify people by their faces without accurately categorizing their sex and race. The sex and race of a face determine how its identity is represented, inextricably linking face identity to these two social categories (for review, see <xref ref-type="bibr" rid="pone.0069684-Rhodes1">[11]</xref>). Indeed, face morphology shows pronounced sexual dimorphism and racial differences <xref ref-type="bibr" rid="pone.0069684-Ferrario1">[12]</xref>, <xref ref-type="bibr" rid="pone.0069684-Farkas1">[13]</xref>. Recently, a set of studies have used multivariate pattern analysis (MVPA) to investigate whether fusiform gyrus represents the sex and race of faces. Univariate data analyses average the responses of multiple voxels. This spatial averaging reduces the information content of the data, which can exist at the level of the individual responses of multiple voxels, or <italic>multivoxel patterns</italic> <xref ref-type="bibr" rid="pone.0069684-Kriegeskorte1">[14]</xref>. In contrast, MVPA interrogates these patterns to reveal the representations that a brain region contains (for review, see <xref ref-type="bibr" rid="pone.0069684-Weil1">[15]</xref>). For example, a brain region in which faces of men and women elicit distinct multivoxel patterns but faces of the same sex yield similar patterns may represent sex.</p>
<p>Two studies have suggested that fusiform gyrus represents the sex and race of faces. In one study, participants in a functional magnetic resonance imaging (fMRI) scanner viewed faces of famous and unfamiliar men and women <xref ref-type="bibr" rid="pone.0069684-Kaul1">[16]</xref>. Pattern classifiers decoded the sex of the faces from fusiform gyrus. In another study, participants were scanned while viewing faces of unfamiliar Black and White individuals <xref ref-type="bibr" rid="pone.0069684-Ratner1">[17]</xref>. Pattern classifiers decoded the race of the faces from fusiform gyrus. However, the sex finding has not been tested in FFA and the race finding has not been replicated reliably in FFA. Multivoxel patterns in FFA from participants who viewed the faces of Black and White individuals differentiated faces by race only for participants who showed high anti-Black bias <xref ref-type="bibr" rid="pone.0069684-Brosch1">[18]</xref>. A different study in which participants viewed photographs of Asian and White faces found that multivoxel patterns in FFA cannot distinguish faces by race <xref ref-type="bibr" rid="pone.0069684-Natu1">[19]</xref>. Therefore, these studies suggest that fusiform gyrus may represent sex and race. However, evidence on whether FFA represents race is mixed (one negative result and one qualified positive result) and no study of which we are aware has examined whether FFA represents sex.</p>
<p>Additionally, the studies that decoded social categories from fusiform gyrus <xref ref-type="bibr" rid="pone.0069684-Kaul1">[16]</xref>, <xref ref-type="bibr" rid="pone.0069684-Ratner1">[17]</xref>, <xref ref-type="bibr" rid="pone.0069684-Brosch1">[18]</xref> have an important limitation. They did not equate physical differences between photographs of social categories that were unrelated to their facial structure, such as luminance and contrast as well as high-level differences like hair length. Consequently, the distinct patterns associated with social categories may not have reflected face differences. Consistent with this concern, the pattern classifiers in these studies decoded the social categories of faces in early visual cortex, which is not face-selective.</p>
<p>The present experiment continues the study of race representations in FFA and begins the study of sex representations in this face-selective brain region by scanning participants while they categorized faces of unfamiliar Black men, Black women, White men, and White women by sex and race. The goal of the present experiment is to determine if, despite the significant variability in the appearance of the people in the photographs, distinct pattern of voxels represent female and male faces as well as Black and White faces, suggesting that FFA includes representations of such social category information. We avoid the important limitation of insufficiently-controlled stimuli in two ways. First, we used photographs that are uniform in appearance and emotional expression, cropping face-irrelevant features (e.g., hairstyle) and background. Also, we controlled for low-level visual differences by equalizing luminance and contrast across social categories. Second, our stimuli orthogonalize sex and race so that if FFA differentiates faces by sex <italic>and</italic> race, this is unlikely to be caused by photograph differences unrelated to facial structure.</p>
</sec><sec id="s2" sec-type="methods">
<title>Method</title>
<sec id="s2a">
<title>Participants</title>
<p>Participants provided their written informed consent in a manner approved for this study by the Committee on the Use of Human Subjects in Research at Harvard University, which specifically approved this study. Seventeen college students and community members from Cambridge, MA, participated in this study (9 female; age range 18–34, <italic>M</italic> = 22.18). All participants were right-handed, had no history of neurological problems, and described themselves as White.</p>
</sec><sec id="s2b">
<title>Stimuli and Behavioral Procedure</title>
<p>In a <italic>categorization task</italic>, participants viewed 192 photographs of unfamiliar Black men, Black women, White men, and White women (48 photographs in each condition). Because previous research is limited by insufficient stimuli control, the present stimuli were meticulously standardized to rule out alternative interpretations of any results. Photographs were collected from a variety of different online databases and depicted young adults facing forward with mouths closed, neutral expression, and eye gaze directed at the camera. The photographs were grayscaled and cropped to squares, their background was removed, and the luminance and contrast of the faces were equalized across conditions using in-house MATLAB code (MathWorks, Natick, MA). For example, the grayscaled images of Black and White faces differed in luminance, measured in 8-bit RGB integers (<italic>M</italic><sub>Blacks</sub> = 106.67, <italic>M</italic><sub>Whites</sub> = 144.52), <italic>t</italic>(95) = 8.11, <italic>p</italic>&lt;10<sup>−12</sup>, but preprocessing removed this difference (<italic>M</italic><sub>Blacks</sub> = 130, <italic>M</italic><sub>Whites</sub> = 130).</p>
<p>In each scanning run, participants categorized the faces either by sex (man, woman) or by race (Black, White) using the index and middle fingers of their right hand, which rested on a button box. Each run was pseudorandomly assigned a categorization dimension (sex, race). Before each run, participants were instructed as to which categorization dimension (sex or race) to use and which button would correspond to each social category. Then, participants completed 10 practice trials on a set of 10 faces not used in the categorization task. Across runs, we counterbalanced the button assignments in such a way that each social category was assigned to each finger an equal number of times and each photograph was categorized once with the index finger and once with the middle finger.</p>
<p>Each trial lasted 2000 ms. For the first 500 ms, a photograph was shown in the center of the screen. For the remaining 1500 ms of each trial, the photograph was replaced with a white fixation crosshair, which encouraged participants to attend to the photographs closely. Photographs were segregated into 8 runs, each of which consisted of 48 photographs (12 in each of the four social categories, e.g., Black men). To optimize estimation of the event-related fMRI response, trials were intermixed in a pseudorandom order and separated by a variable stimulus interval (0–10 s) during which participants passively viewed a white fixation crosshair in the center of the screen <xref ref-type="bibr" rid="pone.0069684-Dale1">[20]</xref>.</p>
<p>After the categorization task, participants completed two runs of a canonical <italic>face localizer</italic> used to identify cortical regions responsive to faces <xref ref-type="bibr" rid="pone.0069684-Kanwisher1">[1]</xref>. In each run, participants viewed photographs of human faces, human bodies, scenes, household objects, and scrambled versions of the household objects. Each photograph appeared for 1 s and was followed by a blank screen for 333 ms. Each category was blocked together to yield 10 blocks of 11 photographs each, 2 blocks per category. One photograph in each block was presented twice in a row, and participants were instructed to press a button when they detected this repetition. The blocks were separated by a stimulus interval that lasted 12 s and were presented in a pseudorandom order, such that participants could not anticipate the category of the upcoming block. During the task, participants fixated on a small, black circle that appeared in the center of the screen throughout the entire experiment (including the presentation of the photographs).</p>
</sec><sec id="s2c">
<title>Functional Imaging Procedure</title>
<p>Imaging data were acquired on a 3.0 Tesla Siemens Tim Trio scanner (Siemens, Erlangen, Germany) with a standard head coil at the Center for Brain Science at Harvard University. Functional runs used a gradient-echo, echo-planar pulse sequence (TR = 3000 ms; TE = 28 ms; flip angle = 85°; field of view = 216×216 mm; matrix = 72×72; in-plane resolution = 2.5×2.5 mm; slice thickness = 2.5 mm). Forty-five interleaved axial slices parallel to the AC-PC line were obtained to cover most of the cerebrum; portions of superior parietal lobe were not covered. The categorization task consisted of 8 runs of 43 volume acquisitions each and the face localizer consisted of 2 runs of 98 volume acquisitions each. Each of the functional runs was preceded by 8 s of gradient and radio frequency pulses that allowed the scanner to reach steady-state magnetization. After the functional runs in the experiment, a high-resolution T1-weighted structural scan (MEMPRAGE) was conducted.</p>
</sec><sec id="s2d">
<title>Functional Imaging Data Analysis</title>
<sec id="s2d1">
<title>Univariate analyses</title>
<p>FMRI data were preprocessed and analyzed using Statistical Parametric Mapping 8 (SPM8; Wellcome Department of Cognitive Neurology, London, United Kingdom) and in-house MATLAB code (MathWorks, Natick, MA) written by Dylan Wagner (Dartmouth College, Hanover, NH). To correct for head movement, a rigid-body transformation realigned images within each run and across all runs using the first functional image as a reference. Realigned images were unwarped to reduce any additional distortions caused by head movement. Unwarped data were normalized into a stereotaxic space (2-mm isotropic voxels) based on the SPM8 EPI template that conforms to the ICBM 152 brain template space and approximates the Talairach and Tournoux atlas space. Normalized images were spatially smoothed using a Gaussian kernel (8-mm full-width-at-half-maximum) to maximize signal-to-noise ratio and reduce the impact of individual differences in functional neuroanatomy. Finally, individual runs were analyzed on a participant-by-participant basis to find outlier volumes with Artifact Detection Toolbox (ART; McGovern Institute for Brain Research, Cambridge, MA). Outliers were defined as volumes in which participant head movement exceeded 0.5 mm or 1° and volumes in which overall signal were more than three standard deviations outside the mean global signal for the entire run.</p>
<p>For each participant, a general linear model (GLM) was constructed to include task effects and nuisance regressors (run mean, linear trend to account for signal drift over time, six movement parameters computed during realignment, and, if any, outlier scans identified by ART and trials in which participants did not provide a response). To compute unweighted (<italic>β</italic>) and weighted (<italic>t</italic>) parameter estimates for each condition at each voxel, the GLM was convolved with a canonical hemodynamic response function (HRF). The GLM of the categorization task was also convolved with the temporal and spatial derivatives of the HRF, which explain a significant portion of BOLD variability above and beyond the canonical model in event-related designs <xref ref-type="bibr" rid="pone.0069684-Henson1">[21]</xref>. Trials were modeled as events of durations equal to their respective reaction times to account for differences in response times (RTs) across conditions <xref ref-type="bibr" rid="pone.0069684-Grinband1">[22]</xref>.</p>
<p>Comparisons of interest were implemented as linear contrasts. In the categorization task, linear contrasts identified significant voxels with a voxel-wise statistical criterion of <italic>p</italic>&lt;.005. Regions-of-interest (ROIs) were required to exceed 75 voxels in extent, establishing an experiment-wide statistical threshold of <italic>p</italic>&lt;.05, corrected for multiple comparisons, on the basis of Monte Carlo simulations <xref ref-type="bibr" rid="pone.0069684-Slotnick1">[23]</xref>. In the face localizer, ROIs were identified for each participant with a voxel-wise statistical criterion of, at most, <italic>p</italic>&lt;.05 (median <italic>p</italic> = .005). Additional statistical comparisons between conditions were conducted in MATLAB using ANOVA on the parameter estimates associated with each trial type.</p>
</sec><sec id="s2d2">
<title>Multivariate analyses</title>
<p>Preprocessing and GLM estimation were identical to those for the univariate analysis of the face categorization task, except that normalized images were spatially smoothed using a smaller Gaussian kernel (5-mm full-width-at-half-maximum).</p>
<p>Trials were conditionalized by sex (men, women), race (Black, White) and run type (odd, even) to yield eight conditions (e.g., <italic>Black men-even</italic>). Linear contrasts compared each condition to baseline. Following Misaki, Kim, Bandettini, and Kriegeskorte <xref ref-type="bibr" rid="pone.0069684-Misaki1">[24]</xref>, these parameter estimates were used for the rest of the analysis to reduce the influence of noisy voxels. The parameter estimates were extracted from each of the ROIs defined by the face localizer and correlated in three ways: same-sex correlations (<italic>Black men-odd with White men-even</italic>, <italic>Black men-even with White men-odd</italic>, <italic>Black women-odd with White women-even</italic>, <italic>Black women-even with White women-odd</italic>), same-race correlations (<italic>Black men-odd with Black women-even</italic>, <italic>Black men-even with Black women-odd</italic>, <italic>White men-odd with White women-even</italic>, <italic>White men-even with White women-odd</italic>), and different-category correlations (<italic>Black men-odd with White women-even</italic>, <italic>White men-odd with Black women-even</italic>, <italic>Black women-odd with White men-even</italic>, <italic>White women-odd with Black men-even</italic>).</p>
<p>Correlations were Fisher-transformed to <italic>z</italic>-values and averaged to yield one same-sex correlation, one same-race correlation, and one different-category correlation. Then, the different-category correlation was subtracted from each of the other average correlations to yield two correlation differences. Finally, one-tailed, one-sample <italic>t</italic>-tests determined if these correlation differences were reliably greater than zero across participants.</p>
</sec></sec></sec><sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Behavioral Data</title>
<p><xref ref-type="table" rid="pone-0069684-t001">Table 1</xref> displays means and standard deviations of responses and RTs. Participants categorized faces more accurately and more quickly by sex (<italic>M</italic><sub>accuracy</sub><italic> = </italic>0.98, <italic>M</italic><sub>RT</sub><italic> = </italic>670 ms) than race (<italic>M</italic><sub>accuracy</sub><italic> = </italic>0.95, <italic>M</italic><sub>RT</sub><italic> = </italic>712 ms), <italic>ts</italic>(16) &gt;5.65, <italic>ps</italic> &lt;10<sup>−5</sup>, <italic>Cohen’s d</italic>s &gt;1.41. Participants categorized men (<italic>M</italic><sub>accuracy</sub><italic> = </italic>0.97, <italic>M</italic><sub>RT</sub><italic> = </italic>684 ms) more accurately and more quickly than women (<italic>M</italic><sub>accuracy</sub><italic> = </italic>0.96, <italic>M</italic><sub>RT</sub><italic> = </italic>699 ms), <italic>ts</italic>(16) &gt;2.25, <italic>ps</italic> &lt;.04, <italic>d</italic>s &gt;0.56. Although participants were no more accurate to categorize Black (<italic>M</italic><sub>accuracy</sub><italic> = </italic>0.96) than White faces (<italic>M</italic><sub>accuracy</sub><italic> = </italic>0.96), <italic>p</italic> = .15, they were faster to categorize Black (<italic>M</italic><sub>RT</sub><italic> = </italic>683 ms) than White faces (<italic>M</italic><sub>RT</sub><italic> = </italic>699 ms), <italic>t</italic>(16) = 3.05, <italic>p</italic>&lt;.01, <italic>d</italic> = 0.76. The sex and race of photographs did not interact in participants’ accuracy and RT, whether collapsing across sex and race runs, within sex runs, or within race runs, all <italic>p</italic>s &gt;.22. Moreover, the 3-way interaction of photograph sex, photograph race, and run (sex, race) was not statistically reliable for accuracy and RT, all <italic>p</italic>s &gt;.28.</p>
<table-wrap id="pone-0069684-t001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0069684.t001</object-id><label>Table 1</label><caption>
<title>Participants’ responses and response latencies from the categorization task.</title>
</caption><alternatives><graphic id="pone-0069684-t001-1" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0069684.t001" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td colspan="2" align="left" rowspan="1">Accuracies</td>
<td colspan="2" align="left" rowspan="1">Response Latencies</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">Sex</td>
<td align="left" rowspan="1" colspan="1">Race</td>
<td align="left" rowspan="1" colspan="1">Sex</td>
<td align="left" rowspan="1" colspan="1">Race</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">White men</td>
<td align="left" rowspan="1" colspan="1">0.95<sup>acd</sup> (0.04)</td>
<td align="left" rowspan="1" colspan="1">0.98<sup>bd</sup> (0.02)</td>
<td align="left" rowspan="1" colspan="1">706<sup>acd</sup> (65)</td>
<td align="left" rowspan="1" colspan="1">679<sup>bd</sup> (64)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">White women</td>
<td align="left" rowspan="1" colspan="1">0.94<sup>cdd</sup> (0.05)</td>
<td align="left" rowspan="1" colspan="1">0.97<sup>ad</sup> (0.03)</td>
<td align="left" rowspan="1" colspan="1">722<sup>cdd</sup> (86)</td>
<td align="left" rowspan="1" colspan="1">692<sup>ab</sup> (78)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Black men</td>
<td align="left" rowspan="1" colspan="1">0.96<sup>acd</sup> (0.04)</td>
<td align="left" rowspan="1" colspan="1">0.98<sup>bd</sup> (0.03)</td>
<td align="left" rowspan="1" colspan="1">700<sup>acd</sup> (60)</td>
<td align="left" rowspan="1" colspan="1">650<sup>ed</sup> (65)</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Black women</td>
<td align="left" rowspan="1" colspan="1">0.94<sup>c d</sup> (0.05)</td>
<td align="left" rowspan="1" colspan="1">0.98<sup>bd</sup> (0.02)</td>
<td align="left" rowspan="1" colspan="1">722<sup>ddd</sup> (67)</td>
<td align="left" rowspan="1" colspan="1">661<sup>fd</sup> (55)</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt101"><label/><p>Note: Means and, in parentheses, standard deviations. Accuracies are displayed in proportions of correct categorizations. Response times are displayed in milliseconds. For each dependent variable, means sharing a superscript do not differ significantly at <italic>p</italic>&lt;.05, as computed in paired-samples <italic>t</italic>-tests.</p></fn></table-wrap-foot></table-wrap></sec><sec id="s3b">
<title>Functional Imaging Data</title>
<sec id="s3b1">
<title>Univariate analyses</title>
<p>The face localizer was used to identify FFA and control brain regions independently (<xref ref-type="table" rid="pone-0069684-t002">Table 2</xref>). Replicating previous research <xref ref-type="bibr" rid="pone.0069684-Kanwisher1">[1]</xref>, <xref ref-type="bibr" rid="pone.0069684-McCarthy1">[2]</xref>, the contrast of <italic>faces</italic>&gt;[<italic>bodies+scenes+objects+scrambled objects</italic>] identified a bilateral region of fusiform gyrus that corresponds to FFA. As face-selective control regions, this contrast also identified a bilateral region of inferior occipital gyrus that corresponds to occipital face area (OFA) <xref ref-type="bibr" rid="pone.0069684-Gauthier1">[25]</xref>, and a bilateral region of superior temporal sulcus (STS) <xref ref-type="bibr" rid="pone.0069684-Puce1">[26]</xref>. As control regions that are category-selective but not face-selective, the contrast of <italic>scenes</italic>&gt;<italic>objects</italic> identified a bilateral region of parahippocampal gyrus that corresponds to parahippocampal place area (PPA) <xref ref-type="bibr" rid="pone.0069684-Epstein1">[27]</xref>. Additionally, the contrast of <italic>objects</italic>&gt;<italic>scrambled objects</italic> identified a bilateral region of lateral occipital cortex that corresponds to lateral occipital complex (LOC) <xref ref-type="bibr" rid="pone.0069684-Malach1">[28]</xref>.</p>
<table-wrap id="pone-0069684-t002" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0069684.t002</object-id><label>Table 2</label><caption>
<title>Brain regions identified in whole-brain, random-effects contrasts in the categorization task, <italic>p</italic>&lt;.05, corrected for multiple comparisons.</title>
</caption><alternatives><graphic id="pone-0069684-t002-2" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0069684.t002" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td colspan="5" align="left" rowspan="1">Faces&gt;[Bodies+Scenes+Objects+Scrambled Objects]</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>Region</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>x</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>y</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>z</italic></td>
<td align="left" rowspan="1" colspan="1">Participants</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Fusiform gyrus (FFA)</td>
<td align="left" rowspan="1" colspan="1">38.8</td>
<td align="left" rowspan="1" colspan="1">−44.3</td>
<td align="left" rowspan="1" colspan="1">−18.5</td>
<td align="left" rowspan="1" colspan="1">16</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">−37.1</td>
<td align="left" rowspan="1" colspan="1">−47.6</td>
<td align="left" rowspan="1" colspan="1">−17.3</td>
<td align="left" rowspan="1" colspan="1">16</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Inferior occipitalgyrus (OFA)</td>
<td align="left" rowspan="1" colspan="1">33.3</td>
<td align="left" rowspan="1" colspan="1">−76.7</td>
<td align="left" rowspan="1" colspan="1">−8.9</td>
<td align="left" rowspan="1" colspan="1">14</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">−33.1</td>
<td align="left" rowspan="1" colspan="1">−77.0</td>
<td align="left" rowspan="1" colspan="1">−6.55</td>
<td align="left" rowspan="1" colspan="1">11</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Superior temporalsulcus (STS)</td>
<td align="left" rowspan="1" colspan="1">49.8</td>
<td align="left" rowspan="1" colspan="1">−43.4</td>
<td align="left" rowspan="1" colspan="1">13.9</td>
<td align="left" rowspan="1" colspan="1">16</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">−49.8</td>
<td align="left" rowspan="1" colspan="1">−52.8</td>
<td align="left" rowspan="1" colspan="1">21.3</td>
<td align="left" rowspan="1" colspan="1">9</td>
</tr>
</tbody>
</table>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td colspan="5" align="left" rowspan="1">Scenes&gt;Objects</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>Region</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>x</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>y</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>z</italic></td>
<td align="left" rowspan="1" colspan="1">Participants</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Parahippocampalgyrus (PPA)</td>
<td align="left" rowspan="1" colspan="1">23.4</td>
<td align="left" rowspan="1" colspan="1">−39.5</td>
<td align="left" rowspan="1" colspan="1">−7.4</td>
<td align="left" rowspan="1" colspan="1">16</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">−24.1</td>
<td align="left" rowspan="1" colspan="1">−42.9</td>
<td align="left" rowspan="1" colspan="1">−4.8</td>
<td align="left" rowspan="1" colspan="1">16</td>
</tr>
</tbody>
</table>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td colspan="5" align="left" rowspan="1">Objects&gt;Scrambled Objects</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>Region</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>X</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>y</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>z</italic></td>
<td align="left" rowspan="1" colspan="1">Participants</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Lateral occipitalcortex (LOC)</td>
<td align="left" rowspan="1" colspan="1">40.5</td>
<td align="left" rowspan="1" colspan="1">−66.3</td>
<td align="left" rowspan="1" colspan="1">−5.0</td>
<td align="left" rowspan="1" colspan="1">8</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">−42.0</td>
<td align="left" rowspan="1" colspan="1">−63.7</td>
<td align="left" rowspan="1" colspan="1">−6.7</td>
<td align="left" rowspan="1" colspan="1">10</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt102"><label/><p>Note: From left to right, columns list the names of regions obtained from whole-brain, random-effects contrasts, the mean stereotaxic Montreal Neurological Institute coordinates of their peak voxels across participants, and the number of participants (<italic>N</italic> = 17) in whom these brain regions were identified at <italic>p</italic>&lt;.05, corrected for multiple comparisons. FFA = fusiform face area, OFA = occipital face area, STS = superior temporal sulcus, PPA = parahippocampal place area, LOC = lateral occipital complex.</p></fn></table-wrap-foot></table-wrap>
<p>For completeness, univariate analyses of the categorization task examined potential differences between photographs as a function of their sex and race. For these analyses, trials were conditionalized by sex (men, women) and race (Black, White; <xref ref-type="table" rid="pone-0069684-t003">Table 3</xref>).</p>
<table-wrap id="pone-0069684-t003" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0069684.t003</object-id><label>Table 3</label><caption>
<title>Brain regions identified in whole-brain, random-effects contrasts in the face localizer task, <italic>p</italic>&lt;.05, corrected for multiple comparisons, sorted in descending order by the <italic>t</italic>-statistic of their peak voxel (<italic>t</italic>).</title>
</caption><alternatives><graphic id="pone-0069684-t003-3" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0069684.t003" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td colspan="6" align="left" rowspan="1">Men&gt;Women</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">No brain regions identified.</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
</tr>
<tr>
<td colspan="6" align="left" rowspan="1">Women&gt;Men</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>Region</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>x</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>y</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>z</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>k</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>t</italic></td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Cerebellum</td>
<td align="left" rowspan="1" colspan="1">0</td>
<td align="left" rowspan="1" colspan="1">−61</td>
<td align="left" rowspan="1" colspan="1">−16</td>
<td align="left" rowspan="1" colspan="1">204</td>
<td align="left" rowspan="1" colspan="1">5.18</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Inferior frontal gyrus</td>
<td align="left" rowspan="1" colspan="1">−28</td>
<td align="left" rowspan="1" colspan="1">15</td>
<td align="left" rowspan="1" colspan="1">−20</td>
<td align="left" rowspan="1" colspan="1">231</td>
<td align="left" rowspan="1" colspan="1">4.71</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Superior frontal gyrus</td>
<td align="left" rowspan="1" colspan="1">20</td>
<td align="left" rowspan="1" colspan="1">61</td>
<td align="left" rowspan="1" colspan="1">−6</td>
<td align="left" rowspan="1" colspan="1">89</td>
<td align="left" rowspan="1" colspan="1">4.50</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Cingulate gyrus</td>
<td align="left" rowspan="1" colspan="1">4</td>
<td align="left" rowspan="1" colspan="1">−29</td>
<td align="left" rowspan="1" colspan="1">34</td>
<td align="left" rowspan="1" colspan="1">75</td>
<td align="left" rowspan="1" colspan="1">3.99</td>
</tr>
</tbody>
</table>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td colspan="6" align="left" rowspan="1">White&gt;Black</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>Region</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>x</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>y</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>z</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>k</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>t</italic></td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Middle frontal gyrus</td>
<td align="left" rowspan="1" colspan="1">−16</td>
<td align="left" rowspan="1" colspan="1">33</td>
<td align="left" rowspan="1" colspan="1">−8</td>
<td align="left" rowspan="1" colspan="1">437</td>
<td align="left" rowspan="1" colspan="1">7.73</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1">14</td>
<td align="left" rowspan="1" colspan="1">35</td>
<td align="left" rowspan="1" colspan="1">−12</td>
<td align="left" rowspan="1" colspan="1">162</td>
<td align="left" rowspan="1" colspan="1">6.06</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Cerebellum</td>
<td align="left" rowspan="1" colspan="1">−12</td>
<td align="left" rowspan="1" colspan="1">−57</td>
<td align="left" rowspan="1" colspan="1">−32</td>
<td align="left" rowspan="1" colspan="1">82</td>
<td align="left" rowspan="1" colspan="1">5.08</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Cingulate gyrus</td>
<td align="left" rowspan="1" colspan="1">−20</td>
<td align="left" rowspan="1" colspan="1">−31</td>
<td align="left" rowspan="1" colspan="1">44</td>
<td align="left" rowspan="1" colspan="1">112</td>
<td align="left" rowspan="1" colspan="1">4.83</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Precuneus</td>
<td align="left" rowspan="1" colspan="1">−16</td>
<td align="left" rowspan="1" colspan="1">−45</td>
<td align="left" rowspan="1" colspan="1">22</td>
<td align="left" rowspan="1" colspan="1">105</td>
<td align="left" rowspan="1" colspan="1">4.12</td>
</tr>
</tbody>
</table>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td colspan="6" align="left" rowspan="1">Black&gt;White</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>Region</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>x</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>y</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>z</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>k</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>t</italic></td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">White matter</td>
<td align="left" rowspan="1" colspan="1">−18</td>
<td align="left" rowspan="1" colspan="1">−81</td>
<td align="left" rowspan="1" colspan="1">2</td>
<td align="left" rowspan="1" colspan="1">126</td>
<td align="left" rowspan="1" colspan="1">5.36</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1">Supramarginal gyrus</td>
<td align="left" rowspan="1" colspan="1">48</td>
<td align="left" rowspan="1" colspan="1">−53</td>
<td align="left" rowspan="1" colspan="1">34</td>
<td align="left" rowspan="1" colspan="1">142</td>
<td align="left" rowspan="1" colspan="1">4.60</td>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt103"><label/><p>Note: From left to right, columns list the names of regions obtained from whole-brain, random-effects contrasts, the stereotaxic Montreal Neurological Institute coordinates of their peak voxels, their size in number of voxels (<italic>k</italic>), and the <italic>t</italic>-statistic of their peak voxel (<italic>t</italic>).</p></fn></table-wrap-foot></table-wrap></sec><sec id="s3b2">
<title>Multivariate analyses</title>
<p>First, we examined whether FFA maintains distinct representations of female and male faces; that is, whether multivoxel patterns in FFA show higher correlations between photographs of individuals of the same sex than between photographs of men and women (<xref ref-type="fig" rid="pone-0069684-g001">Figure 1</xref>). Consistent with the hypothesis that FFA distinguishes faces by sex, pattern correlations in FFA were higher between photographs of the same sex than between photographs of men and women (right FFA, <italic>t</italic>(15) = 3.03, <italic>p</italic>&lt;.005, Cohen’s <italic>d</italic> = 0.78; left FFA, <italic>t</italic>(15) = 2.73, <italic>p</italic>&lt;.008, Cohen’s <italic>d</italic> = 0.70). The correlation differences of right and left FFA were equivalent, <italic>t</italic>(14) = 0.69, <italic>p</italic> = 0.50, suggesting that both regions distinguished faces by sex to a similar degree.</p>
<fig id="pone-0069684-g001" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0069684.g001</object-id><label>Figure 1</label><caption>
<title>Bar graphs display mean correlation differences expressed in <italic>z</italic>-scores (<italic>same-sex&gt;different-category</italic> in red, <italic>same-race&gt;different-category</italic> in blue).</title>
<p>An asterisk denotes a correlation difference that is reliably greater than zero across participants, <italic>p</italic>&lt;.05. Error bars represent 95% confidence intervals in within-subject comparisons <xref ref-type="bibr" rid="pone.0069684-Masson1">[39]</xref>. R and L as the first letters of a region-of-interest’s (ROI) acronym denote the brain hemisphere in which the ROI is localized. FFA = fusiform face area, OFA = occipital face area, STS = superior temporal sulcus, PPA = parahippocampal place area, LOC = lateral occipital complex.</p>
</caption><graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0069684.g001" position="float" xlink:type="simple"/></fig>
<p>Second, we examined whether FFA maintains distinct representations of Black and White faces; that is, whether multivoxel patterns in FFA show higher correlations between photographs of individuals of the same race than between photographs of Black and White individuals (<xref ref-type="fig" rid="pone-0069684-g001">Figure 1</xref>). Consistent with the hypothesis that FFA distinguishes faces by race, pattern correlations in FFA were higher between photographs of the same race than between photographs of Black and White faces (right FFA, <italic>t</italic>(15) = 1.72, <italic>p</italic> = .05, Cohen’s <italic>d</italic> = 0.44; left FFA, <italic>t</italic>(15) = 2.21, <italic>p</italic>&lt;.02, Cohen’s <italic>d</italic> = 0.57). The correlation differences of right and left FFA were equivalent, <italic>t</italic>(14) = 1.01, <italic>p</italic> = 0.33, suggesting that both regions distinguished faces by race to a similar degree.</p>
<p>The correlation differences that suggest distinct representations of female and male faces and Black and White faces in FFA are statistically reliable with a small sample, although they are not corrected for multiple comparisons (<xref ref-type="fig" rid="pone-0069684-g001">Figure 1</xref>). However, the corresponding effect sizes are not small. The correlation differences that correspond to sex representations have effect sizes that approach a large effect size (Cohen’s <italic>d</italic> = 0.8) <xref ref-type="bibr" rid="pone.0069684-Cohen1">[29]</xref>, whereas the correlation differences that correspond to race representations have effect sizes that hover around a medium effect (Cohen’s <italic>d</italic> = 0.5) <xref ref-type="bibr" rid="pone.0069684-Cohen1">[29]</xref>.</p>
<p>We speculated that FFA might be the only face-selective brain region to represent the sex and race of faces because it is the face-selective region that is most sensitive to face identity <xref ref-type="bibr" rid="pone.0069684-Kanwisher2">[3]</xref>. To test this hypothesis, we repeated the MVPA with patterns extracted from other brain regions defined by the face localizer, which included ones previously implicated in face processing like OFA and STS <xref ref-type="bibr" rid="pone.0069684-Kanwisher2">[3]</xref> (<xref ref-type="fig" rid="pone-0069684-g001">Figure 1</xref>). Neither right nor left OFA or STS distinguished faces by social category reliably, <italic>p</italic>s &gt;.13. This suggests that FFA is alone among face-selective brain regions in decoding the sex and race of faces. Because face information may exist in category-selective cortex outside of FFA <xref ref-type="bibr" rid="pone.0069684-OpdeBeeck1">[30]</xref>, <xref ref-type="bibr" rid="pone.0069684-Haxby1">[31]</xref>, we repeated the pattern similarity analyses with patterns extracted from place-selective PPA and object-selective LOC (<xref ref-type="fig" rid="pone-0069684-g001">Figure 1</xref>). Neither right nor left PPA or LOC distinguished faces by social category reliably, <italic>p</italic>s &gt;.26. This suggests that other category-selective brain regions lack sex and race information about faces.</p>
<p>However, FFA may differentiate photographs not by facial properties that vary between social categories, but by lower-level physical differences between the photographs. Many of these low-level physical differences were removed by careful photograph selection and intensive preprocessing (see <italic>Method: Stimuli and behavioral procedure</italic>), but we wanted to test this alternative hypothesis empirically. Therefore, we analyzed multivoxel patterns from early visual cortex, which processes lower-level visual features. To do so, we used the stereotaxic coordinates of the center of mass of the right ([<italic>x y z</italic>] = 25, −82, −15) and left ([<italic>x y z</italic>] =  −29, −80, −18) foveal confluence of brain areas V1, V2, and V3, which represents the central portion of the visual field, as functionally-defined by Dougherty <italic>et al</italic>. <xref ref-type="bibr" rid="pone.0069684-Dougherty1">[32]</xref> using retinotopic mapping <xref ref-type="bibr" rid="pone.0069684-Engel1">[33]</xref>. We extracted patterns from 8-mm spheres centered on these stereotaxic coordinates and repeated the pattern similarity analyses with these patterns. Neither the right nor the left foveal confluence distinguished faces by social category reliably, <italic>p</italic>s &gt;.66. This suggests that low-level visual differences between the photographs do not cause multivoxel patterns in FFA to differentiate faces by sex and race.</p>
<p>As one more way to determine whether low-level visual differences between the stimuli resulted in distinct multivoxel patterns for faces of different social categories, information-based functional brain mapping with multivariate spherical searchlights <xref ref-type="bibr" rid="pone.0069684-Kriegeskorte1">[14]</xref> was conducted to determine if any portion of occipital lobe differentiated faces by sex or race. For each voxel in the brain, we extracted the parameter estimates of each of the eight contrasts (e.g., <italic>Black men-even</italic>) within a spherical neighborhood (8-mm radius; neighborhood size in resampled voxels, <italic>M</italic> = 254, <italic>SD</italic> = 11) similar in shape to those used by Kriegeskorte and colleagues <xref ref-type="bibr" rid="pone.0069684-Kriegeskorte1">[14]</xref>. For each neighborhood, a same-sex correlation difference and a same-race correlation difference were computed as before (see <italic>Method: Functional imaging data analysis</italic>) and assigned to the center voxel. This analysis yielded two correlation difference maps expressed in <italic>z</italic>-scores for each participant, indexing the degree to which each voxel exists in a neighborhood in which multivoxel patterns differentiate female from male faces (first map) and Black from White faces (second map). Finally, a univariate, random-effects analysis identified brain regions in each map that showed correlation differences reliably larger than zero across participants. For each voxel in each map, we performed a right-tailed one<italic>-</italic>sample <italic>t</italic>-test against zero with the corresponding <italic>z</italic>-values from all participants. Correcting for multiple comparisons (see <italic>Method: Functional imaging data analysis</italic>), no brain regions in occipital lobe showed distinct multivoxel patterns for female and male faces or Black and White faces (<xref ref-type="table" rid="pone-0069684-t004">Table 4</xref>).</p>
<table-wrap id="pone-0069684-t004" position="float"><object-id pub-id-type="doi">10.1371/journal.pone.0069684.t004</object-id><label>Table 4</label><caption>
<title>Brain regions identified in whole-brain, random-effects contrasts from the multivariate searchlight analyses, <italic>p</italic>&lt;.05, corrected for multiple comparisons.</title>
</caption><alternatives><graphic id="pone-0069684-t004-4" position="float" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0069684.t004" xlink:type="simple"/>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td colspan="6" align="left" rowspan="1">Same-Sex&gt;Different-Category</td>
</tr>
<tr>
<td align="left" rowspan="1" colspan="1"><italic>Region</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>x</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>y</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>z</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>k</italic></td>
<td align="left" rowspan="1" colspan="1"><italic>t</italic></td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">Cerebellum</td>
<td align="left" rowspan="1" colspan="1">18</td>
<td align="left" rowspan="1" colspan="1">−29</td>
<td align="left" rowspan="1" colspan="1">−26</td>
<td align="left" rowspan="1" colspan="1">77</td>
<td align="left" rowspan="1" colspan="1">5.20</td>
</tr>
</tbody>
</table>
<table><colgroup span="1"><col align="left" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/><col align="center" span="1"/></colgroup>
<thead>
<tr>
<td colspan="6" align="left" rowspan="1">Same-Race&gt;Different-Category</td>
</tr>
</thead>
<tbody>
<tr>
<td align="left" rowspan="1" colspan="1">No brain regions identified.</td>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
<td align="left" rowspan="1" colspan="1"/>
</tr>
</tbody>
</table>
</alternatives><table-wrap-foot><fn id="nt104"><label/><p>Note: From left to right, columns list the names of regions obtained from whole-brain, random-effects contrasts, the stereotaxic Montreal Neurological Institute coordinates of their peak voxels, their size in number of voxels (<italic>k</italic>), and their mean weighted parameter estimate (<italic>t</italic>).</p></fn></table-wrap-foot></table-wrap>
<p>Finally, we investigated whether participants’ task (categorization by sex or race) influenced multivoxel patterns in FFA. To do so, we tested for effects of categorization dimension in two different ways. First, trials were conditionalized by sex (men, women), race (Black, White), categorization dimension (sex, race), and run type (odd, even) to yield 16 conditions (e.g., <italic>Black men categorized by sex-even</italic>). The same correlation differences as before (<italic>same-sex&gt;different-category</italic>, <italic>same-race&gt;different-category</italic>) were calculated separately for each categorization dimension (e.g., <italic>same-sex categorized by sex&gt;different-category categorized by sex</italic>). None of these correlation differences were reliably larger than zero in right and left FFA, <italic>p</italic>s &gt;.16. The discrepancy between these results and the positive results of the analysis in which trials were not conditionalized by categorization dimension are most likely caused by differences in statistical power. The analysis that involves conditionalizing by categorization dimension has half as many trials per condition as the original analysis, endowing the former with an inferior ability to detect small differences between multivoxel patterns across conditions.</p>
<p>Second, trials were conditionalized by categorization dimension (sex, race) and run type (odd, even) to yield 4 conditions (<italic>race-odd, race-even, sex-odd, sex-even</italic>). We computed <italic>same-categorization</italic> correlations (<italic>race-odd with race-even</italic>, <italic>sex-odd with sex-even</italic>) and <italic>different-categorization</italic> correlations (<italic>race-odd with sex-even</italic>, <italic>sex-odd with race-even</italic>). The average different-categorization correlation was subtracted from the average same-categorization correlation to yield a correlation difference. However, this correlation difference was not reliably larger than zero in right and left FFA, <italic>p</italic>s &gt;.24.</p>
</sec></sec></sec><sec id="s4">
<title>Discussion</title>
<p>Previous studies suggested that fusiform gyrus represents the sex and race of faces <xref ref-type="bibr" rid="pone.0069684-Kaul1">[16]</xref>, <xref ref-type="bibr" rid="pone.0069684-Ratner1">[17]</xref>, although whether FFA in particular represents this information was unclear <xref ref-type="bibr" rid="pone.0069684-Brosch1">[18]</xref>, <xref ref-type="bibr" rid="pone.0069684-Natu1">[19]</xref>. In the present experiment, we observed that multivoxel patterns in bilateral FFA distinguished faces by sex and race. Participants variably categorized photographs of unfamiliar Black men, Black women, White men, and White women by sex and race. Despite the significant variability in the appearance of the people in the photographs, a distinct pattern of voxels distinguished between female and male faces and between Black and White faces, suggesting that bilateral FFA includes representations of such social category information. The differences in multivoxel patterns that suggest distinct representations of male and female faces and Black and White faces in FFA were small but statistically reliable. Moreover, their effect sizes are in a range that makes them medium to large effects <xref ref-type="bibr" rid="pone.0069684-Cohen1">[29]</xref>.</p>
<p>These social category representations may be components of face identity representations, which are thought to exist in FFA <xref ref-type="bibr" rid="pone.0069684-Kanwisher2">[3]</xref>. Because face identity is inextricably linked to social categories like age, sex, and race <xref ref-type="bibr" rid="pone.0069684-Rhodes1">[11]</xref>, it seems reasonable that FFA might represent face identity as well as the social categories of faces. FFA could be the neuroanatomical locus in which social categories that are relevant to face identity (i.e., age, race, and sex) are integrated to form holistic representations of individual faces. This hypothesis is consistent with behavioral research that suggests that the human brain codes face identity with reference to social categories <xref ref-type="bibr" rid="pone.0069684-Rhodes2">[34]</xref>.</p>
<p>Analyses of multivoxel patterns from other brain regions suggest that representations of the sex and race of faces may be unique to FFA. Patterns extracted from other face-selective brain regions (OFA and STS), other category-selective brain regions (PPA and LOC), and early visual cortex (foveal confluence of V1, V2, and V3) did not differentiate faces by sex or race. The null results from patterns in early visual cortex suggest that the careful selection and intensive preprocessing of the stimuli removed low-level physical differences unrelated to the sex and race of the stimuli that might have existed in the original photographs. These null results are especially important in this experiment because previous studies that decoded the sex or race of faces from fusiform gyrus also decoded sex and race from early visual cortex <xref ref-type="bibr" rid="pone.0069684-Kaul1">[16]</xref>, <xref ref-type="bibr" rid="pone.0069684-Ratner1">[17]</xref>, <xref ref-type="bibr" rid="pone.0069684-Brosch1">[18]</xref>.</p>
<p>FFA is thought to process perceptual rather than semantic aspects of person perception <xref ref-type="bibr" rid="pone.0069684-Kanwisher2">[3]</xref>; cf. <xref ref-type="bibr" rid="pone.0069684-vandenHurk1">[35]</xref>. For this reason, the sex and race information that FFA represents is unlikely to be semantic; that is, FFA may “tell” faces apart by sex and race without “knowing” what these differences mean. Nonetheless, FFA may play a critical role in social categorization. One of the most fruitful future directions for research on sex and race representations in FFA may be to investigate how this information guides semantic retrieval about social categories in more anterior regions of temporal lobe, which have been consistently implicated in semantics about people generally (for review, see <xref ref-type="bibr" rid="pone.0069684-Wong1">[36]</xref>) and in stereotypes specifically <xref ref-type="bibr" rid="pone.0069684-Contreras1">[37]</xref>. Evidence exists to suggest that stereotyping can modulate neural activity in FFA <xref ref-type="bibr" rid="pone.0069684-Quadflieg1">[38]</xref>, but how representations in FFA might inform higher-order social processes like stereotyping is unknown.</p>
<p>In sum, the present experiment suggests that FFA distinguishes faces by social categories like sex and race. In this way, the current research contributes to our emerging understanding of how the human brain perceives individuals from different social categories.</p>
</sec></body>
<back>
<ack>
<p>The authors thank Anna Leshinskaya, Michael Cohen, Katharine Dobos, Ahn Ton, and Rachel Wong for advice and assistance. The face localizer was modified from a template provided graciously by Vision Sciences Laboratory at Harvard University.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0069684-Kanwisher1"><label>1</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kanwisher</surname><given-names>N</given-names></name>, <name name-style="western"><surname>McDermott</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Chun</surname><given-names>MM</given-names></name> (<year>1997</year>) <article-title>The fusiform face area: A module in human extrastriate cortex specialized for face perception</article-title>. <source>J Neurosci</source> <volume>17</volume>: <fpage>4302</fpage>–<lpage>4311</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-McCarthy1"><label>2</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>McCarthy</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Puce</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Gore</surname><given-names>JC</given-names></name>, <name name-style="western"><surname>Allison</surname><given-names>T</given-names></name> (<year>1997</year>) <article-title>Face-specific processing in the human fusiform gyrus</article-title>. <source>J Cogn Neurosci</source> <volume>9</volume>: <fpage>605</fpage>–<lpage>610</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Kanwisher2"><label>3</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kanwisher</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Yovel</surname><given-names>G</given-names></name> (<year>2006</year>) <article-title>The fusiform face area: A cortical region specialized for the perception of faces</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source> <volume>361</volume>: <fpage>2109</fpage>–<lpage>2128</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-GrillSpector1"><label>4</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grill-Spector</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Knouf</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Kanwisher</surname><given-names>N</given-names></name> (<year>2004</year>) <article-title>The fusiform face area subserves face perception, not generic within-category identification</article-title>. <source>Nat Neurosci</source> <volume>7</volume>: <fpage>555</fpage>–<lpage>562</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Barton1"><label>5</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barton</surname><given-names>JJ</given-names></name>, <name name-style="western"><surname>Press</surname><given-names>DZ</given-names></name>, <name name-style="western"><surname>Keenan</surname><given-names>JP</given-names></name>, <name name-style="western"><surname>O'Connor</surname><given-names>M</given-names></name> (<year>2002</year>) <article-title>Lesions of the fusiform face area impair perception of facial configuration in prosopagnosia</article-title>. <source>Neurol</source> <volume>58</volume>: <fpage>71</fpage>–<lpage>78</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-GrillSpector2"><label>6</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grill-Spector</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Malach</surname><given-names>R</given-names></name> (<year>2001</year>) <article-title>fMR-adaptation: A tool for studying the functional properties of human cortical neurons</article-title>. <source>Acta Psychol</source> <volume>107</volume>: <fpage>293</fpage>–<lpage>321</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Rotshtein1"><label>7</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rotshtein</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Henson</surname><given-names>RN</given-names></name>, <name name-style="western"><surname>Treves</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Driver</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Dolan</surname><given-names>RJ</given-names></name> (<year>2005</year>) <article-title>Morphing Marilyn into Maggie dissociates physical and identity face representations in the brain</article-title>. <source>Nat Neurosci</source> <volume>8</volume>: <fpage>107</fpage>–<lpage>113</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-DaviesThompson1"><label>8</label>
<mixed-citation publication-type="other" xlink:type="simple">Davies-Thompson J, Newling K, Andrews TJ (2012) Image-invariant responses in face-selective regions do not explain the perceptual advantage for familiar face recognition. Cereb Cortex.</mixed-citation>
</ref>
<ref id="pone.0069684-Andrews1"><label>9</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Andrews</surname><given-names>TJ</given-names></name>, <name name-style="western"><surname>Ewbank</surname><given-names>MP</given-names></name> (<year>2004</year>) <article-title>Distinct representations for facial identity and changeable aspects of faces in the human temporal lobe</article-title>. <source>Neuroimage</source> <volume>23</volume>: <fpage>905</fpage>–<lpage>913</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Xu1"><label>10</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Xu</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Yue</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Lescroart</surname><given-names>MD</given-names></name>, <name name-style="western"><surname>Biederman</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>JG</given-names></name> (<year>2009</year>) <article-title>Adaptation in the fusiform face area (FFA): Image or person?</article-title> <source>Vision Res</source> <volume>49</volume>: <fpage>2800</fpage>–<lpage>2807</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Rhodes1"><label>11</label>
<mixed-citation publication-type="other" xlink:type="simple">Rhodes G, Jaquet E (2011) Aftereffects reveal that adaptive face-coding mechanisms are selective for race and sex. In: Jr RAA, Ambady N, Nakayama K, Shimojo S, editors. The science of social vision. New York City: Oxford University Press. 347–362.</mixed-citation>
</ref>
<ref id="pone.0069684-Ferrario1"><label>12</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ferrario</surname><given-names>VF</given-names></name>, <name name-style="western"><surname>Sforza</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Pizzini</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Vogel</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Miani</surname><given-names>A</given-names></name> (<year>1993</year>) <article-title>Sexual dimorphism in the human face assessed by euclidean distance matrix analysis</article-title>. <source>J Anat</source> <volume>183</volume>: <fpage>593</fpage>–<lpage>600</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Farkas1"><label>13</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Farkas</surname><given-names>LG</given-names></name>, <name name-style="western"><surname>Katic</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Forrest</surname><given-names>CR</given-names></name> (<year>2005</year>) <article-title>International anthropometric study of facial morphology in various ethnic groups/races</article-title>. <source>J Craniofac Surg</source> <volume>16</volume>: <fpage>615</fpage>–<lpage>646</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Kriegeskorte1"><label>14</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kriegeskorte</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Goebel</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Bandettini</surname><given-names>P</given-names></name> (<year>2006</year>) <article-title>Information-based functional brain mapping</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>103</volume>: <fpage>3863</fpage>–<lpage>3868</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Weil1"><label>15</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Weil</surname><given-names>RS</given-names></name>, <name name-style="western"><surname>Rees</surname><given-names>G</given-names></name> (<year>2010</year>) <article-title>Decoding the neural correlates of consciousness</article-title>. <source>Curr Opin Neurol</source> <volume>23</volume>: <fpage>649</fpage>–<lpage>655</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Kaul1"><label>16</label>
<mixed-citation publication-type="other" xlink:type="simple">Kaul C, Rees G, Ishai A (2011) The gender of face stimuli is represented in multiple regions in the human brain. Front Hum Neurosci 4.</mixed-citation>
</ref>
<ref id="pone.0069684-Ratner1"><label>17</label>
<mixed-citation publication-type="other" xlink:type="simple">Ratner KG, Kaul C, Van Bavel JJ (2012) Is race erased? Decoding race from patterns of neural activity when skin color is not diagnostic of group boundaries. Soc Cogn Affect Neurosci.</mixed-citation>
</ref>
<ref id="pone.0069684-Brosch1"><label>18</label>
<mixed-citation publication-type="other" xlink:type="simple">Brosch T, Bar-David E, Phelps EA (2012) Implicit race bias decreases the similarity of the neural representations of Black and White faces. Psychol Sci.</mixed-citation>
</ref>
<ref id="pone.0069684-Natu1"><label>19</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Natu</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Raboy</surname><given-names>D</given-names></name>, <name name-style="western"><surname>O'Toole</surname><given-names>AJ</given-names></name> (<year>2011</year>) <article-title>Neural correlates of own- and other-race face perception: Spatial and temporal response differences</article-title>. <source>Neuroimage</source> <volume>54</volume>: <fpage>2547</fpage>–<lpage>2555</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Dale1"><label>20</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dale</surname><given-names>AM</given-names></name> (<year>1999</year>) <article-title>Optimal experimental design for event-related fMRI</article-title>. <source>Hum Brain Mapp</source> <volume>8</volume>: <fpage>109</fpage>–<lpage>114</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Henson1"><label>21</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Henson</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Rugg</surname><given-names>MD</given-names></name>, <name name-style="western"><surname>Friston</surname><given-names>KJ</given-names></name> (<year>2001</year>) <article-title>The choice of basis functions in event-related fMRI</article-title>. <source>Neuroimage</source> <volume>13</volume>: <fpage>S149</fpage>–<lpage>S149</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Grinband1"><label>22</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grinband</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Wager</surname><given-names>TD</given-names></name>, <name name-style="western"><surname>Lindquist</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ferrera</surname><given-names>VP</given-names></name>, <name name-style="western"><surname>Hirsch</surname><given-names>J</given-names></name> (<year>2008</year>) <article-title>Detection of time-varying signals in event-related fMRI designs</article-title>. <source>Neuroimage</source> <volume>43</volume>: <fpage>509</fpage>–<lpage>520</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Slotnick1"><label>23</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Slotnick</surname><given-names>SD</given-names></name>, <name name-style="western"><surname>Moo</surname><given-names>LR</given-names></name>, <name name-style="western"><surname>Segal</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Hart</surname><given-names>J</given-names><suffix>Jr</suffix></name> (<year>2003</year>) <article-title>Distinct prefrontal cortex activity associated with item memory and source memory for visual shapes</article-title>. <source>Brain Res Cogn Brain Res</source> <volume>17</volume>: <fpage>75</fpage>–<lpage>82</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Misaki1"><label>24</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Misaki</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Bandettini</surname><given-names>PA</given-names></name>, <name name-style="western"><surname>Kriegeskorte</surname><given-names>N</given-names></name> (<year>2010</year>) <article-title>Comparison of multivariate classifiers and response normalizations for pattern-information fMRI</article-title>. <source>Neuroimage</source> <volume>53</volume>: <fpage>103</fpage>–<lpage>118</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Gauthier1"><label>25</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Gauthier</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Tarr</surname><given-names>MJ</given-names></name>, <name name-style="western"><surname>Moylan</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Skudlarski</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Gore</surname><given-names>JC</given-names></name>, <etal>et al</etal>. (<year>2000</year>) <article-title>The fusiform “face area” is part of a network that processes faces at the individual level</article-title>. <source>J Cogn Neurosci</source> <volume>12</volume>: <fpage>495</fpage>–<lpage>504</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Puce1"><label>26</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Puce</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Allison</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Bentin</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Gore</surname><given-names>JC</given-names></name>, <name name-style="western"><surname>McCarthy</surname><given-names>G</given-names></name> (<year>1998</year>) <article-title>Temporal cortex activation in humans viewing eye and mouth movements</article-title>. <source>J Neurosci</source> <volume>18</volume>: <fpage>2188</fpage>–<lpage>2199</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Epstein1"><label>27</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Epstein</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Kanwisher</surname><given-names>N</given-names></name> (<year>1998</year>) <article-title>A cortical representation of the local visual environment</article-title>. <source>Nature</source> <volume>392</volume>: <fpage>598</fpage>–<lpage>601</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Malach1"><label>28</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Malach</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Reppas</surname><given-names>JB</given-names></name>, <name name-style="western"><surname>Benson</surname><given-names>RR</given-names></name>, <name name-style="western"><surname>Kwong</surname><given-names>KK</given-names></name>, <name name-style="western"><surname>Jiang</surname><given-names>H</given-names></name>, <etal>et al</etal>. (<year>1995</year>) <article-title>Object-related activity revealed by functional magnetic resonance imaging in human occipital cortex</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>92</volume>: <fpage>8135</fpage>–<lpage>8139</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Cohen1"><label>29</label>
<mixed-citation publication-type="other" xlink:type="simple">Cohen J (1988) Statistical power analysis for the behavioral sciences. Hillsdale, NJ: Lawrence Erlbaum Associates.</mixed-citation>
</ref>
<ref id="pone.0069684-OpdeBeeck1"><label>30</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Op de Beeck</surname><given-names>HP</given-names></name>, <name name-style="western"><surname>Brants</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Baeck</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Wagemans</surname><given-names>J</given-names></name> (<year>2010</year>) <article-title>Distributed subordinate specificity for bodies, faces, and buildings in human ventral visual cortex</article-title>. <source>Neuroimage</source> <volume>49</volume>: <fpage>3414</fpage>–<lpage>3425</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Haxby1"><label>31</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Haxby</surname><given-names>JV</given-names></name>, <name name-style="western"><surname>Gobbini</surname><given-names>MI</given-names></name>, <name name-style="western"><surname>Furey</surname><given-names>ML</given-names></name>, <name name-style="western"><surname>Ishai</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Schouten</surname><given-names>JL</given-names></name>, <etal>et al</etal>. (<year>2001</year>) <article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title>. <source>Science</source> <volume>293</volume>: <fpage>2425</fpage>–<lpage>2430</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Dougherty1"><label>32</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dougherty</surname><given-names>RF</given-names></name>, <name name-style="western"><surname>Koch</surname><given-names>VM</given-names></name>, <name name-style="western"><surname>Brewer</surname><given-names>AA</given-names></name>, <name name-style="western"><surname>Fischer</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Modersitzki</surname><given-names>J</given-names></name>, <etal>et al</etal>. (<year>2003</year>) <article-title>Visual field representations and locations of visual areas V1/2/3 in human visual cortex</article-title>. <source>J Vis</source> <volume>3</volume>: <fpage>586</fpage>–<lpage>598</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Engel1"><label>33</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Engel</surname><given-names>SA</given-names></name>, <name name-style="western"><surname>Rumelhart</surname><given-names>DE</given-names></name>, <name name-style="western"><surname>Wandell</surname><given-names>BA</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>AT</given-names></name>, <name name-style="western"><surname>Glover</surname><given-names>GH</given-names></name>, <etal>et al</etal>. (<year>1994</year>) <article-title>fMRI of human visual cortex</article-title>. <source>Nature</source> <volume>369</volume>: <fpage>525</fpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Rhodes2"><label>34</label>
<mixed-citation publication-type="other" xlink:type="simple">Rhodes G, Leopold DA (2011) Adaptive norm-based coding of face identity. In: Calder AJ, Rhodes G, Johnson MH, Haxby JV, editors. The Oxford handbook of face perception New York City: Oxford University Press. 263–286.</mixed-citation>
</ref>
<ref id="pone.0069684-vandenHurk1"><label>35</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>van den Hurk</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Gentile</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Jansma</surname><given-names>BM</given-names></name> (<year>2011</year>) <article-title>What's behind a face: Person context coding in fusiform face area as revealed by multivoxel pattern analysis</article-title>. <source>Cereb Cortex</source> <volume>21</volume>: <fpage>2893</fpage>–<lpage>2899</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Wong1"><label>36</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wong</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Gallate</surname><given-names>J</given-names></name> (<year>2012</year>) <article-title>The function of the anterior temporal lobe: A review of the empirical evidence</article-title>. <source>Brain Res</source> <volume>1449</volume>: <fpage>94</fpage>–<lpage>116</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Contreras1"><label>37</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Contreras</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Banaji</surname><given-names>MR</given-names></name>, <name name-style="western"><surname>Mitchell</surname><given-names>JP</given-names></name> (<year>2012</year>) <article-title>Dissociable neural correlates of stereotypes and other forms of semantic knowledge</article-title>. <source>Soc Cogn Affect Neurosci</source> <volume>7</volume>: <fpage>764</fpage>–<lpage>770</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Quadflieg1"><label>38</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Quadflieg</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Flannigan</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Waiter</surname><given-names>GD</given-names></name>, <name name-style="western"><surname>Rossion</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Wig</surname><given-names>GS</given-names></name>, <etal>et al</etal>. (<year>2011</year>) <article-title>Stereotype-based modulation of person perception</article-title>. <source>Neuroimage</source> <volume>57</volume>: <fpage>549</fpage>–<lpage>557</lpage>.</mixed-citation>
</ref>
<ref id="pone.0069684-Masson1"><label>39</label>
<mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Masson</surname><given-names>MEJ</given-names></name>, <name name-style="western"><surname>Loftus</surname><given-names>GR</given-names></name> (<year>2003</year>) <article-title>Using confidence intervals for graphically based data interpretation</article-title>. <source>Can J Exp Psychol</source> <volume>57</volume>: <fpage>203</fpage>–<lpage>220</lpage>.</mixed-citation>
</ref>
</ref-list></back>
</article>