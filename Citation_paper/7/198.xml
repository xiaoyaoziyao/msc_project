<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v3.0 20080202//EN" "http://dtd.nlm.nih.gov/publishing/3.0/journalpublishing3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="3.0" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pone.0127618</article-id>
<article-id pub-id-type="publisher-id">PONE-D-14-56023</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Why Verbalization of Non-Verbal Memory Reduces Recognition Accuracy: A Computational Approach to Verbal Overshadowing</article-title>
<alt-title alt-title-type="running-head">A Computational Approach to Verbal Overshadowing</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Hatano</surname>
<given-names>Aya</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Ueno</surname>
<given-names>Taiji</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
<xref rid="aff002" ref-type="aff"><sup>2</sup></xref>
<xref rid="aff003" ref-type="aff"><sup>3</sup></xref>
<xref rid="cor001" ref-type="corresp">*</xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Kitagami</surname>
<given-names>Shinji</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Kawaguchi</surname>
<given-names>Jun</given-names>
</name>
<xref rid="aff001" ref-type="aff"><sup>1</sup></xref>
</contrib>
</contrib-group>
<aff id="aff001"><label>1</label> <addr-line>Department of Psychology, Graduate School of Environmental Studies, Nagoya University, Nagoya, Japan</addr-line></aff>
<aff id="aff002"><label>2</label> <addr-line>Psychology Department, University of York, York, United Kingdom</addr-line></aff>
<aff id="aff003"><label>3</label> <addr-line>Japan Society for the Promotion of Science, Tokyo, Japan</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Hills</surname>
<given-names>Peter James</given-names>
</name>
<role>Academic Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>Bournemouth University, UNITED KINGDOM</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con" id="contrib001">
<p>Conceived and designed the experiments: TU SK JK. Performed the experiments: AH TU. Analyzed the data: AH TU. Contributed reagents/materials/analysis tools: TU SK JK. Wrote the paper: AH TU SK JK.</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">ueno.taiji@d.mbox.nagoya-u.ac.jp</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>10</day>
<month>6</month>
<year>2015</year>
</pub-date>
<pub-date pub-type="collection">
<year>2015</year>
</pub-date>
<volume>10</volume>
<issue>6</issue>
<elocation-id>e0127618</elocation-id>
<history>
<date date-type="received">
<day>14</day>
<month>12</month>
<year>2014</year>
</date>
<date date-type="accepted">
<day>17</day>
<month>4</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-year>2015</copyright-year>
<copyright-holder>Hatano et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0127618" xlink:type="simple"/>
<abstract>
<p>Verbal overshadowing refers to a phenomenon whereby verbalization of non-verbal stimuli (e.g., facial features) during the maintenance phase (after the target information is no longer available from the sensory inputs) impairs subsequent non-verbal recognition accuracy. Two primary mechanisms have been proposed for verbal overshadowing, namely the recoding interference hypothesis, and the transfer-inappropriate processing shift. The former assumes that verbalization renders non-verbal representations less accurate. In contrast, the latter assumes that verbalization shifts processing operations to a verbal mode and increases the chance of failing to return to non-verbal, face-specific processing operations (i.e., intact, yet inaccessible non-verbal representations). To date, certain psychological phenomena have been advocated as inconsistent with the recoding-interference hypothesis. These include a decline in non-verbal memory performance following verbalization of non-target faces, and occasional failures to detect a significant correlation between the accuracy of verbal descriptions and the non-verbal memory performance. Contrary to these arguments against the recoding interference hypothesis, however, the present computational model instantiated core processing principles of the recoding interference hypothesis to simulate face recognition, and nonetheless successfully reproduced these behavioral phenomena, as well as the standard verbal overshadowing. These results demonstrate the plausibility of the recoding interference hypothesis to account for verbal overshadowing, and suggest there is no need to implement separable mechanisms (e.g., operation-specific representations, different processing principles, etc.). In addition, detailed inspections of the internal processing of the model clarified how verbalization rendered internal representations less accurate and how such representations led to reduced recognition accuracy, thereby offering a computationally grounded explanation. Finally, the model also provided an explanation as to why some studies have failed to report verbal overshadowing. Thus, the present study suggests it is not constructive to discuss whether verbal overshadowing exists or not in an all-or-none manner, and instead suggests a better experimental paradigm to further explore this phenomenon.</p>
</abstract>
<funding-group>
<funding-statement>All of the funds below came from Japan Society for the Promotion of Science. The URL is as follows: <ext-link ext-link-type="uri" xlink:href="https://www.jsps.go.jp/english/index.html" xlink:type="simple">https://www.jsps.go.jp/english/index.html</ext-link>. (1) Grants-in-Aid for JSPS fellow #262696 (14J02696) (to TU). (2) JSPS Grants-in-Aid for Scientific Research on Innovative Areas 23101006, JSPS Grants-in-Aid Scientific Research (C) 26380984 (to SK). (3) JSPS Grants-in-Aid for Scientific Research (B) 21330168 and 25285200 (to JK). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
</funding-group>
<counts>
<fig-count count="10"/>
<table-count count="0"/>
<page-count count="22"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability" xlink:type="simple">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information files.</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Verbal overshadowing refers to a detrimental effect that verbalization has on accuracy/reaction time performance in non-verbal memory tasks (e.g., for faces, voices, or tastes). Schooler and Engstler-Schooler [<xref rid="pone.0127618.ref001" ref-type="bibr">1</xref>] first established a paradigm to investigate this phenomenon. Specifically, participants watched a video of a bank robbery for 30 seconds. After this, half of them were asked to verbalize the appearance of the bank robber, whereas the remaining half was not. Subsequently, all of the participants took an eight-alternative forced choice face recognition test (a photo line-up). Recognition performance was worse for those who had verbalized the appearance of the bank robber than those who had not. This phenomenon was termed <italic>verbal overshadowing</italic>. This phenomenon can be observed in daily life and in legal settings (e.g., eyewitness reports)[<xref rid="pone.0127618.ref002" ref-type="bibr">2</xref>]. As such, elucidation of the mechanisms underlying this phenomenon is significant for applied issues as well as being relevant for theories of cognition. Meissner and Brigham’s meta-analysis confirmed the robustness of this phenomenon [<xref rid="pone.0127618.ref003" ref-type="bibr">3</xref>], and very recently, a large-scale study has also confirmed its robustness [<xref rid="pone.0127618.ref004" ref-type="bibr">4</xref>]. Typically, the incorrect responses occur as more frequent selections of a distractor face (i.e., false alarms), rather than as “not present” responses in N-alternative forced choice recognition tasks [<xref rid="pone.0127618.ref005" ref-type="bibr">5</xref>]. Thus, the nature of distractors modulates the effect of verbalization. More specifically, verbal overshadowing occurs when distractors are relatively similar to the target face [<xref rid="pone.0127618.ref006" ref-type="bibr">6</xref>]. Related to this, if a target is a typical face (thus, more similar to distractors), verbal overshadowing is more readily observed [<xref rid="pone.0127618.ref007" ref-type="bibr">7</xref>].</p>
<p>Verbal overshadowing is observed for various kinds of non-verbal stimuli, including facial images [<xref rid="pone.0127618.ref008" ref-type="bibr">8</xref>], voices [<xref rid="pone.0127618.ref009" ref-type="bibr">9</xref>,<xref rid="pone.0127618.ref010" ref-type="bibr">10</xref>], and tastes [<xref rid="pone.0127618.ref011" ref-type="bibr">11</xref>,<xref rid="pone.0127618.ref012" ref-type="bibr">12</xref>]. In the present study, we use the term verbal overshadowing in a narrow sense to refer to non-verbal memory performance impaired by verbalization. Other studies sometimes use this term more extensively beyond the meaning as per the original memory studies, including to refer to reduced accuracy in insight problems [<xref rid="pone.0127618.ref013" ref-type="bibr">13</xref>] or analogies [<xref rid="pone.0127618.ref014" ref-type="bibr">14</xref>], and so on. Of course, it is tempting and might be parsimonious for a single theory to explain the similar findings across domains. However, any attempt to develop a theory within a domain (i.e., memory) should be conservatively restricted to that domain, unless there is a strong a-priori reason to extend the explanation more widely. So far, two primary explanations have been offered. The first is the <italic>recoding interference hypothesis</italic> [<xref rid="pone.0127618.ref001" ref-type="bibr">1</xref>,<xref rid="pone.0127618.ref003" ref-type="bibr">3</xref>], which assumes that verbalizing non-verbal memory renders the (rich and sophisticated) visual representations imperfect and less accurate. This recoded representation is assumed to be used in subsequent face recognition, resulting in a reduction in accuracy. Thus, the recoding interference hypothesis predicts that verbal overshadowing will occur more readily if participants generate less accurate verbal descriptions. This assumed process links Bruce and Young ‘s [<xref rid="pone.0127618.ref015" ref-type="bibr">15</xref>] face recognition model, which assumes that both visual code and verbal code affect face recognition. Consistent with the RIF account, Meissner, Brigham, and Kelley [<xref rid="pone.0127618.ref005" ref-type="bibr">5</xref>] found clearer verbal overshadowing when participants were prompted to generate as many descriptions as possible, no matter how accurate they thought those descriptions were. Conversely, verbal overshadowing was attenuated when the participants were warned not to generate a description unless they were confident with its preciseness. These relationships were further supported by positive subject wise correlations between recognition accuracy and accuracy of the verbal descriptions. Related to this, Meissner and Brigham [<xref rid="pone.0127618.ref003" ref-type="bibr">3</xref>], in their meta-analysis, also demonstrated that verbal overshadowing is liable to occur when participants verbalize a target in detail. The authors explained that a detailed description more frequently elicits inaccurate verbal descriptions, and these inaccurate descriptions in turn create an inaccurate (recoded) representation.</p>
<p>In contrast, the other proposed mechanism underlying verbal overshadowing is the <italic>transfer-inappropriate processing shift (TIPS) hypothesis</italic> [<xref rid="pone.0127618.ref016" ref-type="bibr">16</xref>–<xref rid="pone.0127618.ref019" ref-type="bibr">19</xref>], which proposes that verbalization requires a <italic>shift</italic> into verbal processing, and this shift obstructs the application of non-verbal (face-specific) processing in the subsequent face recognition phase. Like the transfer-appropriate processing account of memory[<xref rid="pone.0127618.ref020" ref-type="bibr">20</xref>], the TIPS hypothesis assumes that non-verbal stimuli should be best recognized in non-verbal, face-specific processing operations, such as configural/holistic processing [<xref rid="pone.0127618.ref021" ref-type="bibr">21</xref>,<xref rid="pone.0127618.ref022" ref-type="bibr">22</xref>]. Specifically, in the encoding phase, a target face may be encoded into a rich and sophisticated representation by non-verbal, face-specific processing operations. However, describing the face during the retention phase may enhance verbal processing operations. Then, these verbal processing operations may be inappropriately carried over into the subsequent recognition phase. As a result, participants cannot apply face-specific processing and fail at choosing the correct target. Taken together, both theories assume that face stimuli are best represented in a non-verbal format, and such a sophisticated non-verbal representation is necessary for face recognition [<xref rid="pone.0127618.ref021" ref-type="bibr">21</xref>]. However, the two accounts differ in terms of the status of such a non-verbal representation after verbalization. The recoding interference hypothesis assumes that verbalization renders the representation itself less accurate, whereas the TIPS hypothesis assumes that the non-verbal representation is unavailable. Thus, the latter assumes that verbalization requires a shift in the processing itself, with the non-verbal representation itself remaining intact [<xref rid="pone.0127618.ref023" ref-type="bibr">23</xref>], but this shift obstructs access to the intact non-verbal representation.</p>
<p>So far, both supporting evidence and counterevidence has accumulated from human experiments for both theories. For example, Schooler [<xref rid="pone.0127618.ref019" ref-type="bibr">19</xref>] pointed out that the recoding interference hypothesis cannot explain some results that TIPS can. For example, Dodson, Johnson, and Schooler [<xref rid="pone.0127618.ref023" ref-type="bibr">23</xref>] demonstrated that verbalization of a non-target face (e.g., a parent’s face) also impaired recognition of the target face [<xref rid="pone.0127618.ref017" ref-type="bibr">17</xref>,<xref rid="pone.0127618.ref018" ref-type="bibr">18</xref>,<xref rid="pone.0127618.ref024" ref-type="bibr">24</xref>]. If an inaccurate target representation made by verbalization is responsible for memory impairments (i.e., the recoding interference hypothesis), verbalizing an unrelated face would not affect subsequent recognition. Additional evidence against the recoding interference hypothesis is that some studies have failed to replicate a positive correlation between recognition accuracy and the verbal description accuracy of the target face [<xref rid="pone.0127618.ref006" ref-type="bibr">6</xref>,<xref rid="pone.0127618.ref008" ref-type="bibr">8</xref>,<xref rid="pone.0127618.ref025" ref-type="bibr">25</xref>]. Consequently, Schooler [<xref rid="pone.0127618.ref019" ref-type="bibr">19</xref>] has argued that a crucial aspect of verbal overshadowing is not the inaccurate verbal representation, but a processing shift that in turn obstructs the re-application of non-verbal operations. Thus, it is worth emphasizing that the TIPS has a more modularistic relationship (as exemplified by terms such as “shift” and “apply”) with non-verbal and verbal processing than the recoding interference hypothesis. The TIPS hypothesis clearly assumes an <italic>operation-specific (process-specific) representation</italic>, and argues this can accommodate the behavioral data.</p>
<p>These experimental data have greatly contributed to testing the competing theories. The next step is to bring these informal (i.e., descriptive) theories into an engineering framework by taking a computational modeling approach. The rationale behind this approach is that if a theory is correct, then a computational model that instantiates the core processing principles of that theory should reproduce the relevant phenomena. Moreover, given the experimental evidence against the recoding interference hypothesis (above), it is theoretically meaningful to simulate such “counterevidence” in a working computational model of the recoding interference hypothesis (meaning it is no longer counterevidence), which can trigger a more thorough debate between two theories.</p>
<p>Finally, the present computational model also addressed the replicability issues in verbal overshadowing. Our hypothesis is that this is due to (a) the use of single-trial testing methods, (b) individual differences, and (c) relatively uncontrolled extraneous variables (e.g., how many distinctive facial features the target and distractors share). These issues will be addressed following the main simulation experiments.</p>
</sec>
<sec id="sec002" sec-type="materials|methods">
<title>Method</title>
<sec id="sec003">
<title>Ethics statement</title>
<p>We did not collect any human data. This work takes a computational modelling approach to simulate human behavioral data, which were taken from existing literature.</p>
</sec>
<sec id="sec004">
<title>Model architecture</title>
<p>The PDP model was built using the Light Efficient Neural Simulator (LENS) software (<ext-link ext-link-type="uri" xlink:href="http://tedlab.mit.edu/~dr/Lens/" xlink:type="simple">http://tedlab.mit.edu/~dr/Lens/</ext-link>). <xref rid="pone.0127618.g001" ref-type="fig">Fig 1</xref> shows an abstract connectionist framework for face processing (left) and the actual model architecture (right). The retinotopic layer (4,200 units) received a noise-filtered face input. Then, the activations spread to the other layers, and a clear (i.e., non-filtered) face image was represented in the visual image layer (upper-right layer, 4,200 units), whereas verbal information of the face features was represented in the verbal layer (6 units). The details of the tasks and mappings will be explained later. These three peripheral layers were connected bi-directionally through a single hidden layer (20 units). In order to reduce the computational demands in this large model, units in a peripheral layer were connected to the hidden layer only when the external input/target value of that unit had variations (i.e., sparse connectivity).</p>
<fig id="pone.0127618.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0127618.g001</object-id>
<label>Fig 1</label>
<caption>
<title>An abstract connectionist framework for face processing (left) and an actual model architecture (right).</title>
<p>The number of units in each layer is indicated in parentheses. Each solid bi-directional arrow represents connectivity from one group of units to another.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0127618.g001" position="float" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec005">
<title>Stimuli (Representations)</title>
<p>We will first explain how the 64 face items (image files) were selected. Then, we will describe how we created the representations (vector patterns) of these 64 items that a computational model received/generated (<xref rid="pone.0127618.g002" ref-type="fig">Fig 2</xref> shows four examples of these 64 items). Specifically, we first selected two verbal labels for each of the three face features (i.e., <italic>drooping/slanted eyes</italic>; <italic>long/button-shaped noses</italic>; <italic>thick/downturned lips</italic>; see the 6<sup>th</sup> column of <xref rid="pone.0127618.g002" ref-type="fig">Fig 2</xref>). Then, for each of these six verbal labels, two subordinate types were created (i.e., big/small <italic>slanted eyes</italic>, big/small <italic>drooping eyes</italic>; big/small <italic>long nose</italic>, big/small <italic>button-shaped nose</italic>; bottom/top <italic>thick lip</italic>, normal/parallel <italic>downturned lip</italic>, see the 2<sup>nd</sup> column of <xref rid="pone.0127618.g002" ref-type="fig">Fig 2</xref>). In order to make the model trainable, we deliberately kept the stimuli as simple as possible, by excluding other distinctive features such as hair. By crossing these four specific types of eyes/noses/lips, 64 (i.e., 4 × 4 × 4) unique face patterns were formed.</p>
<fig id="pone.0127618.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0127618.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Four example items.</title>
<p><bold>The 3</bold><sup><bold>rd</bold></sup> <bold>through 5</bold><sup><bold>th</bold></sup> <bold>columns show the representations (Hinton diagrams) in each layer.</bold> The leftmost column shows the item ID numbers for each face pattern. The second column shows the unique combination of the specific (shown in bold) features of each face, which the model could not discriminate by the activation patterns of the verbal layer (i.e., 5<sup><bold>th</bold></sup> column). The third column shows the retinotopic patterns of each item. The fourth column shows the target patterns of the visual image layer. The fifth and sixth columns show the activation patterns (and their labels) in the verbal layer.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0127618.g002" position="float" xlink:type="simple"/>
</fig>
<p>Next, the representations (vector patterns) in each layer for these 64 items were created via the following steps. First, visual image files for the 64 faces (70 × 60 pixels) were created (see <xref rid="pone.0127618.s001" ref-type="supplementary-material">S1 Fig</xref>) using montage software (<ext-link ext-link-type="uri" xlink:href="http://www1.mahoroba.ne.jp/~matumoto/nitaroS.html" xlink:type="simple">http://www1.mahoroba.ne.jp/~matumoto/nitaroS.html</ext-link>). Then, each of these image files was binarized (i.e., a black pixel was 1 and a white pixel was 0), forming a 4,200-bit vector pattern. These were used as the targets for the visual image layer (see the 4<sup>th</sup> column of <xref rid="pone.0127618.g002" ref-type="fig">Fig 2</xref>). Next, in order to create the input patterns of these items to the retinotopic layer (3<sup>rd</sup> column of <xref rid="pone.0127618.g002" ref-type="fig">Fig 2</xref>), the images were smoothed by Gaussian convolution (<italic>SD</italic> = 0.2, see <xref rid="pone.0127618.s002" ref-type="supplementary-material">S2 Fig</xref>). It is still debatable which noise function would be the best to represent noisy retinotopic map [<xref rid="pone.0127618.ref026" ref-type="bibr">26</xref>]. Therefore, we simply followed the existing literature [<xref rid="pone.0127618.ref027" ref-type="bibr">27</xref>]. Finally, as shown in the 5<sup>th</sup> and 6<sup>th</sup> columns of <xref rid="pone.0127618.g002" ref-type="fig">Fig 2</xref>, the 6-bit vector patterns in the verbal layer represented one of the six verbal labels for the face features in a localist manner (i.e., <italic>drooping/slanted eyes</italic>; <italic>long/button-shaped noses</italic>; <italic>thick/downturned lips</italic>), not the specific features within a verbal label (e.g., big slanted eyes). Therefore, the activation patterns in the verbal layer could not discriminate two items sharing the same verbal labels (e.g., 1<sup>st</sup> and 2<sup>nd</sup> rows of <xref rid="pone.0127618.g002" ref-type="fig">Fig 2</xref>).</p>
</sec>
<sec id="sec006">
<title>Tasks</title>
<sec id="sec007">
<title>Visual recognition task</title>
<p>In this task, a noise-filtered face image was presented to the retinotopic layer. Then, the model had to map this retinotopic input (third column of <xref rid="pone.0127618.g002" ref-type="fig">Fig 2</xref>) into a non-filtered, item-specific visual representation in the visual image layer (fourth column of <xref rid="pone.0127618.g002" ref-type="fig">Fig 2</xref>). Thus, like past computational models [<xref rid="pone.0127618.ref027" ref-type="bibr">27</xref>,<xref rid="pone.0127618.ref028" ref-type="bibr">28</xref>], face recognition was simulated in terms of whether item-specific information was computed from a noisy input. In each trial, the network was allowed to cycle 10 time steps. At each step, the activation spread to the next layer, gradually being scaled by the values of the interconnecting weights. The net input was computed by considering the net input at the previous time step, using the following equations:
<disp-formula id="pone.0127618.e001">
<alternatives>
<graphic id="pone.0127618.e001g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0127618.e001" xlink:type="simple"/>
<mml:math display="block" id="M1" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>.</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>.</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>0.1</mml:mn><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>.</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>•</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>.</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>.</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(1)</label>
</disp-formula>
<disp-formula id="pone.0127618.e002">
<alternatives>
<graphic id="pone.0127618.e002g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0127618.e002" xlink:type="simple"/>
<mml:math display="block" id="M2" overflow="scroll">
<mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>.</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>.</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow>
</mml:math>
</alternatives>
<label>(2)</label>
</disp-formula>
Where <italic>s</italic><sub><italic>i</italic>.<italic>t</italic></sub> is the net input at time step <italic>t</italic> from all projections <italic>j</italic> to the unit <italic>i</italic>. <italic>a</italic><sub><italic>i</italic>.<italic>t</italic></sub> is the activation of the units <italic>i</italic> at time step <italic>t</italic>, which is a logistic function of the net input, ranging from 0 to 1.</p>
<p>The target, correct activation value of each unit, was assigned to the visual image layer, and the output pattern in this layer was compared to this target pattern from the first cycle. If the output-target difference dropped below 0.5 for all units in the visual image layer, then the next trial (next face) began, even if this was before the 10<sup>th</sup> cycle. Error (difference between the output and the target) of the target units was estimated by a cross entropy method [<xref rid="pone.0127618.ref029" ref-type="bibr">29</xref>].</p>
</sec>
<sec id="sec008">
<title>Verbalization task</title>
<p>In this task, the model received the same retinotopic input as the visual recognition task (see above), but was required to activate the correct units of the verbal layer (i.e., correct verbal labels, see 5<sup>th</sup> column of <xref rid="pone.0127618.g002" ref-type="fig">Fig 2</xref>). As <xref rid="pone.0127618.g002" ref-type="fig">Fig 2</xref> shows, the model had to activate three units, each of which corresponded to the eyes/nose/lip labels, respectively. Like the visual recognition task, the model was allowed to cycle 10 steps in each trial. The training for each trial was terminated according to the same criterion as the visual recognition task.</p>
</sec>
<sec id="sec009">
<title>Mental imagery task</title>
<p>In this task, the verbal layer received an external input, and the visual image layer activity was compared to its target pattern. The model had to compute the correct non-filtered, face image from the verbal labels. The retinotopic layer did not receive any external input. It should be noted that the accuracy in this task never reached 100% because sometimes a different target pattern would be computed from the same input pattern (i.e., one-to-many mapping). Taking the 1<sup>st</sup> and 2<sup>nd</sup> rows of <xref rid="pone.0127618.g002" ref-type="fig">Fig 2</xref> as examples, the visual image layer patterns (i.e., targets) were different, but their verbal layer activations (i.e., inputs in the mental imagery task) were identical. Therefore, the same units in the verbal layer (e.g., <italic>drooping eyes</italic>) were activated for these two cases, but different output patterns (big or thin eyes) should be generated in the visual image layer. The model could not decide which pattern to compute in the visual image layer from the identical input pattern, as a result of which accuracy never reached 100%. This is consistent with a situation that occurs when humans imagine the appearance of a person upon hearing a verbal description of facial features. Humans also cannot specify a unique person from the various people who share the same verbal labels.</p>
</sec>
<sec id="sec010">
<title>Training</title>
<p>As will be explained later, half of the 64 faces were used for training, and the remaining half was used for testing generalization. The model was first trained on the visual recognition task (pre training) until its accuracy exceeded 50% because human babies first learn visual recognition before acquiring language. Then, the training trials for the other two tasks were interleaved with the visual recognition task. After training each item 3,000 times in each of the three tasks (after the pre-training of visual recognition), the model’s performance was satisfactory (see later for details).</p>
<p>The connection strength was adjusted by a back-propagation through time algorithm. The model did not add previous learning outcomes (weight changes) to the current training (i.e., the momentum parameter was set to zero). If the difference between the output and the target activation value was within 0.1, then no error derivative was back-propagated from that unit. The model was trained with a learning rate of 0.05, and a decay parameter set to 0.00000001. The batch size was 1. A small amount of Gaussian noise (<italic>SD</italic> = 0.2) was added to the input to the hidden layer to encourage this layer to adopt more polarized outputs. These parameter values are common in the neural network literature [<xref rid="pone.0127618.ref030" ref-type="bibr">30</xref>,<xref rid="pone.0127618.ref031" ref-type="bibr">31</xref>], and we simply followed these values.</p>
</sec>
</sec>
</sec>
<sec id="sec011" sec-type="results">
<title>Results</title>
<sec id="sec012">
<title>Training performance</title>
<p>Five models were trained on three tasks: a visual recognition task, a verbalization task, and a mental imagery task. As mentioned above, a randomly selected half of the items (i.e., 32 items) was used during training whereas the remaining half was not. This manipulation resulted in creating 32 “old” (trained) faces for the model and 32 “new” (untrained) faces. First, the model was able to activate the correct verbal units (i.e., verbalization task) for all 32 trained and 32 untrained items (i.e., generalization occurred). With regard to the mental imagery task, this never reached 100% accuracy.</p>
<p>Next, we evaluated visual recognition performance by examining whether or not the model could differentiate Person X from others. Such differentiation should indicate that the model was successfully trained to compute item-specific information for each trained/untrained item without confusing with others, and the results supported that the model was able to do this [<xref rid="pone.0127618.ref032" ref-type="bibr">32</xref>,<xref rid="pone.0127618.ref033" ref-type="bibr">33</xref>]. <xref rid="pone.0127618.g003" ref-type="fig">Fig 3A</xref> (upper half) shows two successful examples in the visual recognition task. Specifically, these two faces shared the same mouth/nose features, yet the upper, trained (i.e., “old”), case had <italic>thin</italic> whereas the lower, untrained (i.e., “new”), case had <italic>big</italic> drooping eyes. A visual inspection of the visual image layer in the right column suggests that the model was able to discriminate these two visually similar cases, and generated item-specific outputs in the layer. Furthermore, the output vector in the visual image layer was compared to all 64 target vector patterns to provide a more systematic evaluation. The rationale behind this nearest-neighborhood criterion analysis is that, if the closest vector pattern is its target vector (i.e., smallest Euclidean distance), then the model represented correct item-specific information in the visual image layer. With this scoring method, accuracy for visual recognition was 100% for the 32 trained (“old”) items and 84% (SD = 0.05%) for the 32 untrained (“new”) items. These indicated the model was computed the correct verbal and visual representations from a noisy retinotopic input of the 32 old faces and generalized the acquired knowledge to discriminate 32 untrained “new” faces from the “old” faces.</p>
<fig id="pone.0127618.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0127618.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Hinton diagrams of the face recognition outcomes for two visually-similar trained/untrained faces, respectively.</title>
<p>The diagrams in the left column denote the retinotopic input, and those in the right column denote the generated outputs in the visual image layer. The upper half (a) shows the outcomes of the face recognition without prior verbalization of facial features whereas the lower half shows those after verbalization of facial features (b).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0127618.g003" position="float" xlink:type="simple"/>
</fig>
<p>Next, a multidimensional scaling analysis (MDS) was conducted to compress the multidimensional information represented in the hidden layer so that internal activities of the model could be visualized. Specifically, we measured hidden layer activities at every 10 time ticks when the network activities were settling down into the stable attractor state. <xref rid="pone.0127618.g004" ref-type="fig">Fig 4</xref> shows this settling process for three representative items. Specifically, the filled and open circular markers represent face recognition for the old face with (thin) drooping eyes and a new face with (big) drooping eyes, respectively (see <xref rid="pone.0127618.g003" ref-type="fig">Fig 3A</xref>). As the proximity of these two plots shows, face recognition for these two visually similar cases involved highly similar internal activities. In other words, discriminating these visually similar faces required the computation of fine-grained internal representations. On the other hand, the plot of the squares, which represents the recognition for the old face with slanted eyes, is distant from the circle plots in the multidimensional space. This indicates that the settling process of the recognition for a visually dissimilar face required a dissimilar internal representation to compute. These suggest that, although their attractor states were unique for each item, similar inputs were associated with similar/neighboring attractor basins while dissimilar input fell into a dissimilar (distant) attractor basin. This acquired knowledge about the similarity structure was generalized to represent an untrained “new” face correctly, thus uniquely, in the visual image layer.</p>
<fig id="pone.0127618.g004" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0127618.g004</object-id>
<label>Fig 4</label>
<caption>
<title>Multidimensional scaling analysis of the internal activities during settling in the face recognition task.</title>
<p>Circular and square markers denote the face recognition activities without verbalization. The circular markers represent face recognition for two visually similar faces (“drooping eyes”) whereas the square markers represent that for visually dissimilar one (“slanted eyes”). Triangular markers denote activities during and after verbalization. To aid visual understanding, the 3-D plots were projected to the 2-D spaces (left and bottom planes).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0127618.g004" position="float" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec013">
<title>Instantiation of verbalization process in the model</title>
<p>Once the model’s internal processes during face recognition were understood and visualized, the next step was to investigate the impact of verbalization on this internal activity. The verbalization procedure was instantiated to the model using the following procedure. First, some units in the verbal layer received an external input of 1.0, and the outputs of these verbal units were determined by the following equation:
<disp-formula id="pone.0127618.e003">
<alternatives>
<graphic id="pone.0127618.e003g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0127618.e003" xlink:type="simple"/>
<mml:math display="block" id="M3" overflow="scroll">
<mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.25em"/><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo>*</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.25em"/><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow>
</mml:math>
</alternatives>
<label>(3)</label>
</disp-formula>
Where initial output is a logistic function of the summed weighted inputs from other units (see prior <xref rid="pone.0127618.e001" ref-type="disp-formula">Eq 1</xref> <xref rid="pone.0127618.e002" ref-type="disp-formula">Eq 2</xref>). Thus, this equation scaled the contribution of the external input and that of the internal activities (from the hidden layer) to unit activities (i.e., soft-clamping). Then, the activations spread from the verbal layer to the other layers, including the visual image layer, for 10 cycles. After 10 cycles, these external inputs to the verbal layer were removed or maintained, depending on the experimental conditions that were simulated (see later), and the model was presented as a retinotopic input. After this, the previous procedure for testing face recognition was applied.</p>
<p>An example outcome of this recognition after verbalization is shown in <xref rid="pone.0127618.g003" ref-type="fig">Fig 3B</xref> (lower half) and <xref rid="pone.0127618.g004" ref-type="fig">Fig 4</xref> (triangle markers). As we mentioned above, the right column of <xref rid="pone.0127618.g003" ref-type="fig">Fig 3B</xref> shows the generated outputs in the visual image layer based upon the retinotopic inputs for two visually similar “old” and “new” faces. Although these two retinotopic inputs were the same examples as those in <xref rid="pone.0127618.g003" ref-type="fig">Fig 3A</xref> (upper half), these were confused with each other after verbalization (<xref rid="pone.0127618.g003" ref-type="fig">Fig 3B</xref>). Thus, external activation of three verbal units for drooping eyes, long-nosed, and for down-turned mouth made the model represent an old face in the visual image layer even though a new face was presented in the retinotopic layer. The internal activities during verbalization and during face recognition (after verbalization) of the untrained “new” face were plotted in the same multidimensional space (triangle markers in <xref rid="pone.0127618.g004" ref-type="fig">Fig 4</xref>). During verbalization (open-triangle markers), the hidden layer activities settled down near to the attractor basin of the visually similar, yet trained “old” face (i.e., filled-circle markers). Thus, even after the retinotopic input for a “new” face was presented, the internal representation did not move closer to its correct attractor (i.e., open-circle markers) but was captured by the wrong attractor basin (i.e., filled-circle markers). As a result, a confusion (false alarm) occurred.</p>
<p>Thus, crucially, the present model instantiated the core processing principles of the recoding interference hypothesis [<xref rid="pone.0127618.ref002" ref-type="bibr">2</xref>,<xref rid="pone.0127618.ref003" ref-type="bibr">3</xref>,<xref rid="pone.0127618.ref005" ref-type="bibr">5</xref>]. Namely, verbalization changed (recoded) the nature of representations, rather than shifting the types of processing (i.e., TIPS, [<xref rid="pone.0127618.ref019" ref-type="bibr">19</xref>]). Such a fully interacting system represented a face in a distributed manner, and such a distributed representation was affected (not shifted to another process-specific representation) by verbalization. Then, this recoded representation was used for (or affected) subsequent visual recognition, and resulted in a failure in computing item-specific information.</p>
<p>Before moving to the next analysis, it is worth emphasizing that in this simulation, a face recognition phase followed immediately after verbalization. In this sense, this phenomenon might be closely allied to a so-called priming effect [<xref rid="pone.0127618.ref034" ref-type="bibr">34</xref>]. It will be discussed later.</p>
<p>So far, we have demonstrated the model’s training performance, generalization ability, and we have visualized the change in the internal activities following prior verbalization. Next, in order to provide a more direct comparison with human data in the existing verbal overshadowing literature, “old”/“new” judgment-based recognition accuracy was measured in the model.</p>
</sec>
<sec id="sec014">
<title>Simulation 1. “Old”/“New” recognition judgment on the basis of polarity values</title>
<p>Given that most of the empirical studies on verbal overshadowing involved an “old”/“new” recognition judgment or an N-alternative forced choice task, it is worth measuring the corresponding performance of the model. For this purpose, the first step was to measure a variable that is known to affect “old”/“new” judgments of each item. One such a candidate is an item’s familiarity [<xref rid="pone.0127618.ref035" ref-type="bibr">35</xref>–<xref rid="pone.0127618.ref037" ref-type="bibr">37</xref>], as is assumed in a signal detection theory [<xref rid="pone.0127618.ref037" ref-type="bibr">37</xref>]. If it is possible to measure how familiar each item is to the model, then we can systematically assess the model’s “old”/“new” judgment on each item.</p>
<p>For this purpose, we measured how closely to the polar values (i.e., 0 or 1, hereafter <italic>polarity</italic>) a retinotopic face input activated the units in the visual image layer [<xref rid="pone.0127618.ref038" ref-type="bibr">38</xref>,<xref rid="pone.0127618.ref039" ref-type="bibr">39</xref>]. The rationale behind this polarity analysis is as follows: A unit activity near to one of the polar values means that the model is confident about what to do with that input, regardless of whether it is active or inactive. This is because some units are specifically tuned to presented stimuli during learning [<xref rid="pone.0127618.ref040" ref-type="bibr">40</xref>]. If some units respond more accurately to an item by chance, these activations are reinforced by a learning algorithm to respond more strongly to the item the next time it is presented. In contrast, other units are suppressed so as not to send noise. Thus, a network generates an output near to a polar value for trained, familiar items, whereas an output near to the middle of the (sigmoidal) range occurs for an untrained, unfamiliar item. In other words, whether activity is near to a polar value indicates how unambiguously each unit responds to the input, thereby forming an index of familiarity. Specifically, polarity for each unit was calculated using the following equation:
<disp-formula id="pone.0127618.e004">
<alternatives>
<graphic id="pone.0127618.e004g" position="anchor" mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0127618.e004" xlink:type="simple"/>
<mml:math display="block" id="M4" overflow="scroll">
<mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.25em"/><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.25em"/><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>*</mml:mo><mml:mtext>log</mml:mtext><mml:mspace width="0.15em"/><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>*</mml:mo><mml:mtext>log</mml:mtext><mml:mspace width="0.15em"/><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow>
</mml:math>
</alternatives>
<label>(4)</label>
</disp-formula>
Where <italic>a</italic><sub><italic>i</italic></sub> refers to the activity of <italic>unit</italic><sub><italic>i</italic></sub>. Thus, as the equation above shows, polarity ranges from 0 to 1. A high polarity value denotes that the unit elicits close to a 1 or 0 response (close to the asymptote of the sigmoid), whereas a lower polarity value denotes that the unit produces a response near 0.5 (i.e., close to the linear range of the sigmoid). If polarity distributions for all 32 “old” faces are higher than those for the “new” faces, then a criterion value can be set to optimize the “old”/“new” judgment accuracy to be above chance.</p>
<p><xref rid="pone.0127618.g005" ref-type="fig">Fig 5</xref> shows the outcome of this analysis. Polarity values for each unit were measured on the 10<sup>th</sup> cycle of a visual recognition trial, and were averaged across the units in the visual image layer. To produce a smoother distribution curve, the outcomes were averaged across five different models (each initiated with a different random status). In this figure, the polarity distribution of the 32 “old” faces (circular markers) is located to the right of the polarity distribution of the 32 “new” faces (triangular markers). The dotted vertical line at polarity value of 0.942 indicates the “old”/“new” decision criterion that produced comparable performance to humans. With this criterion, 100% of the “old” items were categorized as “old” (i.e., hits), whereas 30% of the new items were correctly categorized as “new” (i.e., correct rejections). Thus, the averaged “old”/“new” judgment accuracy resulted in 65% correct performance (SE = 0.02). This performance is compatible with normal human recognition accuracy, as reported in the verbal overshadowing literature (e.g., 64% correct, [<xref rid="pone.0127618.ref001" ref-type="bibr">1</xref>]). Note that human accuracy refers to the proportion of the participants who correctly chose the target face in a single-trial test. Thus, the polarity analysis also suggests that the model was successfully trained for face recognition in terms of “old”/“new” recognition, based on the polarity values.</p>
<fig id="pone.0127618.g005" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0127618.g005</object-id>
<label>Fig 5</label>
<caption>
<title>Distribution of polarity values for the “old” and “new” faces.</title>
<p>Simulation 1 (without verbalization). The dotted vertical line indicates the decision criterion that produces “old”/“new” judgment performance comparable to humans.</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0127618.g005" position="float" xlink:type="simple"/>
</fig>
<sec id="sec015">
<title>Testing the effect of verbalization on the “old”/“new” decision on the basis of polarity</title>
<p>The next series of analyses concerned whether verbalization changes the polarity distributions in <xref rid="pone.0127618.g005" ref-type="fig">Fig 5</xref>, in comparison to the control condition (i.e., without verbalization). Overlapping of the two distributions to a greater extent suggests a lower “old”/“new” judgment accuracy. In contrast, if the distributions remain unchanged or overlap less, this suggests that recognition performance would remain unchanged or even improve (i.e., no deleterious effect of verbalization).</p>
<p>The verbalization procedure was instantiated via the same procedure as mentioned above (10 cycles after the external input to the verbal layer). The polarity values were measured at the 10<sup>th</sup> cycle after a retinotopic input. A subsequent series of simulations aimed to reproduce the human data on verbal overshadowing produced from various experimental paradigms, by manipulating (1) which verbal units to activate, and (2) when to remove the external verbal inputs.</p>
</sec>
</sec>
<sec id="sec016">
<title>Simulation 2. Target-distractor similarity: Similar condition</title>
<p>First, we examined whether the model was able to reproduce standard verbal overshadowing. For this purpose, we instantiated the experimental situation where verbal overshadowing is most readily observed (see <xref rid="sec001" ref-type="sec">Introduction</xref>). Several studies have suggested that participants are more likely to fail target recognition with a similar distractor or a typical face [<xref rid="pone.0127618.ref006" ref-type="bibr">6</xref>,<xref rid="pone.0127618.ref007" ref-type="bibr">7</xref>]. Such a high similarity condition was instantiated in the model here. Specifically, if the target and distractors are similar to each other, a logical consequence is that human participants must have verbalized information which is plausible (i.e., consistent) for <italic>both</italic> the target and distractors before recognition. Therefore, a simulation of these human experiments involved the external activation of “correct” (i.e., consistent with the to-be-judged retinotopic input, be it “old” or “new”) verbal units before presentation of a retinotopic input. This external verbal input was maintained even after the retinotopic input presentation, as it was natural to assume that the participants “believed” these verbalized features were correct, even during a face recognition test.</p>
<p><xref rid="pone.0127618.g006" ref-type="fig">Fig 6</xref> shows the resultant polarity distributions. Verbalization moved the polarity distributions such that the “old” and “new” distributions overlapped to a greater extent. That is, the model could not discriminate the “old” faces from “new” on the basis of polarity values (i.e., familiarity). Indeed, the “old”/“new” decision with the same criterion value as the control condition (i.e., <xref rid="pone.0127618.g005" ref-type="fig">Fig 5</xref>, without verbalization) resulted in lower accuracy (50% correct, SE = 0, <italic>t</italic> (4) = 6.69, <italic>p</italic> = .002). Thus, the model successfully reproduced standard verbal overshadowing.</p>
<fig id="pone.0127618.g006" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0127618.g006</object-id>
<label>Fig 6</label>
<caption>
<title>Distribution of polarity values for the ‘old’/‘new’ faces in Simulation 2 (after verbalization): High target-distractor similarity.</title>
<p>The dotted vertical line indicates the decision criterion that was set in the control condition (<xref rid="pone.0127618.g005" ref-type="fig">Fig 5</xref>).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0127618.g006" position="float" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec017">
<title>Simulation 3. Target-distractor similarity: Dissimilar condition</title>
<p>Next, it is known that verbalization does not impair visual recognition when the dissimilar distractors or atypical faces were presented [<xref rid="pone.0127618.ref006" ref-type="bibr">6</xref>,<xref rid="pone.0127618.ref007" ref-type="bibr">7</xref>]. Under such experimental situations, it is natural to assume that the generated verbal descriptions should be correct/consistent with the target appearance, but less so with the distractors. Therefore, a simulation of these experimental situations involved the external activation of the consistent verbal units as the retinotopic input when testing an “old” face. In contrast, the inconsistent verbal units were externally activated before presenting an untrained “new” retinotopic input. The resultant polarity distributions are shown in <xref rid="pone.0127618.g007" ref-type="fig">Fig 7</xref>. These distributions were separated to a greater extent than the control condition (without verbalization in <xref rid="pone.0127618.g005" ref-type="fig">Fig 5</xref>). Indeed, “old”/”new” judgments with the same criterion as the control condition did not reveal a detrimental effect of verbalization (66% correct, SE = 0.06, <italic>t</italic> (4) = -0.11, <italic>p</italic> = .914, <italic>n</italic>.<italic>s</italic>.), consistent with human experiments.</p>
<fig id="pone.0127618.g007" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0127618.g007</object-id>
<label>Fig 7</label>
<caption>
<title>Distribution of polarity values for the ‘old’/‘new’ faces in Simulation 3 (after verbalization): Low target-distractor similarity.</title>
<p>The dotted vertical line indicates the decision criterion that was set in the control condition (<xref rid="pone.0127618.g005" ref-type="fig">Fig 5</xref>).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0127618.g007" position="float" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec018">
<title>Simulation 4. Reproduction and understanding of counterevidence against the recoding interference hypothesis: Verbalization of an irrelevant face</title>
<p>As we demonstrated by the MDS analysis (<xref rid="pone.0127618.g004" ref-type="fig">Fig 4</xref>), the present model instantiated the core processing principles of the recoding interference hypothesis. Therefore, it is of utility in accounting for the experimental phenomena that weigh against the recoding interference hypothesis.</p>
<p>The first target was the effect of verbalizing an irrelevant face (e.g., [<xref rid="pone.0127618.ref023" ref-type="bibr">23</xref>]). Unlike the simulations so far, the external verbal inputs were not consistent with the features of the retinotopic face input. This is equivalent to human experiments where individuals were asked to describe an unrelated face before recognition. In addition, it is natural to assume that individuals in such experiments do not believe their descriptions are relevant to the target features. In order to reflect this assumption in a simulation, the external input to the verbal layer was removed when a retinotopic input was presented. The resultant polarity distributions are shown in <xref rid="pone.0127618.g008" ref-type="fig">Fig 8</xref>. In this case, the distributions for the “old” and “new” faces were more coincident than in the control condition (<xref rid="pone.0127618.g005" ref-type="fig">Fig 5</xref>). It follows that the “old”/”new” decision performance declined (<italic>M</italic> = 50% correct, SE = 0.03, <italic>t</italic> (4) = 6.55, <italic>p</italic> = .002) when evaluated with the same criterion as the control condition, a signature of verbal overshadowing. Thus, it is unnecessary to incorporate a modular process into a theory, such as a “shift of processing” (e.g., TIPS). Even without such an additional mechanism, one can account for the detrimental effect of verbalizing an unrelated face (e.g., [<xref rid="pone.0127618.ref023" ref-type="bibr">23</xref>]).</p>
<fig id="pone.0127618.g008" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0127618.g008</object-id>
<label>Fig 8</label>
<caption>
<title>Distribution of polarity values for the ‘old’/‘new’ faces in Simulation 4 (after verbalization): Verbalization of an irrelevant face.</title>
<p>The dotted vertical line indicates the decision criterion that was set in the control condition (<xref rid="pone.0127618.g005" ref-type="fig">Fig 5</xref>).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0127618.g008" position="float" xlink:type="simple"/>
</fig>
</sec>
<sec id="sec019">
<title>Simulation 5. Reproduction and understanding of counterevidence against the recoding interference hypothesis: Relationship with verbal description accuracy</title>
<p>Another piece of counterevidence against the recoding interference is that some studies have failed to detect a significant correlation between description accuracy and the effect size of verbal overshadowing [<xref rid="pone.0127618.ref001" ref-type="bibr">1</xref>,<xref rid="pone.0127618.ref006" ref-type="bibr">6</xref>,<xref rid="pone.0127618.ref008" ref-type="bibr">8</xref>,<xref rid="pone.0127618.ref025" ref-type="bibr">25</xref>]. Our hypothesis is that this detection failure may stem from a possible confound with how accurately the verbal descriptions capture the <italic>distractor</italic> faces, not just the target face, only the latter of which has been investigated in existing literature. Indeed, our simulations so far have already demonstrated this idea. Specifically, Simulation 2 showed that correct description of distractors declined recognition accuracy even though the description accuracy for the target face was perfect. Therefore, this demonstration indicates the necessity to control for or to systematically investigate the effect of this <italic>distractor description accuracy</italic>, which refers to how closely the descriptions of the target coincidentally capture distractor faces. As mentioned in the Methods, the model represented a face in terms of three types of verbal information (eyes: drooping/slanted, nose: long/button-shaped, mouth: thick/downturned). Thus, it was possible to instantiate four levels of verbal description accuracy (i.e., 0%, 33.33%, 66.66%, or 100%) by activating from 0 to 3 of the verbal units correctly.</p>
<p><xref rid="pone.0127618.g009" ref-type="fig">Fig 9</xref> shows the polarity distributions for the old faces (a) and new faces (b) as a function of how accurately/consistently the externally activated verbal units captured the retinotopic input. Clearly, both distributions moved towards the left (i.e., lower polarity) as description accuracy decreased. Crucially, a series of comparisons between one of the distributions in <xref rid="pone.0127618.g009" ref-type="fig">Fig 9A</xref> (“old” faces) and one in <xref rid="pone.0127618.g009" ref-type="fig">Fig 9B</xref> (“new” faces) demonstrated a <italic>non-significant correlation</italic> between target description accuracy and the “old”/“new” recognition judgment performance, as is occasionally observed in human experiments [<xref rid="pone.0127618.ref001" ref-type="bibr">1</xref>,<xref rid="pone.0127618.ref006" ref-type="bibr">6</xref>,<xref rid="pone.0127618.ref008" ref-type="bibr">8</xref>,<xref rid="pone.0127618.ref025" ref-type="bibr">25</xref>]. Specifically, we will describe three cases in descending order of target description accuracies, where the magnitude of verbal overshadowing did not necessarily decline in parallel. As a first case, imagine that the verbal descriptions captured the target face features perfectly (i.e., the rightmost distribution in <xref rid="pone.0127618.g009" ref-type="fig">Fig 9A</xref>). In this situation, if these descriptions happened to capture the face features of the distractors perfectly (i.e., the rightmost distribution in <xref rid="pone.0127618.g009" ref-type="fig">Fig 9B</xref>), “yes”/“no” judgment performance declined (50% correct accuracy, see <xref rid="sec016" ref-type="sec">Simulation 2</xref>), a signature of verbal overshadowing. Next, imagine the target description captured 66.6% of the target face correctly (the dark gray marker in <xref rid="pone.0127618.g009" ref-type="fig">Fig 9A</xref>). In this situation, if these descriptions happened to capture only 33.3% of the distractor faces correctly (i.e., the light gray marker in <xref rid="pone.0127618.g009" ref-type="fig">Fig 9B</xref>), then the two distributions did not overlap as much as in the first case (above), which means that the judgment accuracy declined to a lesser extent (54% correct, SE = 0.01, <italic>t</italic> (4) = 4.57, <italic>p</italic> = .01). That is, the second case showed a smaller effect of verbal overshadowing than the first case, despite the lower target description accuracy. Finally, a third case was more critical. If we compare the distribution of the light gray markers in <xref rid="pone.0127618.g009" ref-type="fig">Fig 9A</xref> (i.e., 33.3% accurate) with the leftmost distribution of <xref rid="pone.0127618.g009" ref-type="fig">Fig 9B</xref> (i.e., 0% accurate for “new” faces), again the two distributions do not overlap greatly, indicating no decline in recognition accuracy (62% correct, SE = 0.05, <italic>t</italic> (4) = 0.67, <italic>p</italic> = .537). Therefore, the last case showed the highest recognition accuracy despite having the lowest target description accuracy of the three cases. Taken together, these three cases illustrate that recognition accuracy did not decline in parallel with target description accuracy.</p>
<fig id="pone.0127618.g009" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0127618.g009</object-id>
<label>Fig 9</label>
<caption>
<title>Polarity distributions as a function of description accuracy for (a) “old” and (b) “new” faces.</title>
<p>The dotted vertical line denotes the criterion that was set in the control condition (<xref rid="pone.0127618.g005" ref-type="fig">Fig 5</xref>).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0127618.g009" position="float" xlink:type="simple"/>
</fig>
<p>As is clear from these analyses, the crucial factor was not only target description accuracy but also distractor description accuracy. The more accurately the generated verbal descriptions happened to capture the distractor faces correctly, the more the polarity (familiarity) distribution for distractors is shifted rightward.</p>
</sec>
<sec id="sec020">
<title>Simulation 6. Explaining the “unreliability” of verbal overshadowing in terms of individual differences in subjects/items</title>
<p>Finally, one of the key issues in verbal overshadowing is the occasional replication failures in the literature, although the recent large-scale replication project has successfully demonstrated a significant effect of verbalization [<xref rid="pone.0127618.ref004" ref-type="bibr">4</xref>]. Therefore, we aimed to clarify what modulates the detectability of this phenomenon, focusing on two factors, namely individual differences among subjects and those among items.</p>
<p>First, as mentioned above, <xref rid="pone.0127618.g006" ref-type="fig">Fig 6</xref> showed the polarity distributions for the trained “old” and for the untrained “new” faces, <italic>averaged</italic> across five different models (with different initial status) when the target-distractor similarity was high. However, once we split this averaged histogram (<xref rid="pone.0127618.g006" ref-type="fig">Fig 6</xref>) into individual models (<xref rid="pone.0127618.g010" ref-type="fig">Fig 10</xref>), then clear individual differences were observable. Specifically, the two polarity distributions did not overlap for some of the individual models. Thus, this simulation demonstrates that verbal overshadowing is sensitive to individual differences.</p>
<fig id="pone.0127618.g010" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0127618.g010</object-id>
<label>Fig 10</label>
<caption>
<title>Individual model polarity distributions for the “old”/“new” faces in the similar target-distractor condition.</title>
<p>The average of these 5 models corresponds to <xref rid="pone.0127618.g006" ref-type="fig">Fig 6</xref> (Simulation 2).</p>
</caption>
<graphic mimetype="image" xlink:href="info:doi/10.1371/journal.pone.0127618.g010" position="float" xlink:type="simple"/>
</fig>
<p>The final analysis involved differences among individual items. So far, we have shown two polarity histograms (distributions) for <italic>all</italic> 32 trained (“old”) and <italic>all</italic> 32 untrained (“new”) faces, and then estimated the model’s “old”/“new” judgment performance based on these distributions. However, a standard verbal overshadowing experiment involves a single-trial testing procedure. It is possible to instantiate such a single-trial testing procedure in the present model and to observe the outcome. Specifically, imagine one item (target) is randomly chosen from the polarity distribution of the trained “old” items (e.g., <xref rid="pone.0127618.g010" ref-type="fig">Fig 10</xref>). Similarly, N items (distractors) are randomly chosen from the polarity distribution of the untrained “new” items in the same figure (<xref rid="pone.0127618.g010" ref-type="fig">Fig 10</xref>). As a result, it is unsurprising that the polarity value of one of the randomly-selected “new” items is higher, by chance, than the single “old” item. That is, a single-trial testing procedure in the model is quite sensitive to differences in individual item polarity. Thus, unless the polarity distributions for the “old” and “new” faces do not overlap at all, a familiarity-based “old”/“new” decision performance can be noisy in a single-trial testing procedure.</p>
</sec>
</sec>
<sec id="sec021" sec-type="conclusions">
<title>Discussion</title>
<p>The present study used a computational modeling approach to understand the mechanism by which verbalization of non-verbal memory affects subsequent non-verbal recognition performance, an effect known as verbal overshadowing. So far, there are two primary explanations of verbal overshadowing: the recoding interference hypothesis and the transfer-inappropriate processing shift (TIPS) hypothesis. The key difference between these hypotheses is whether an operation-specific representation is postulated or not.</p>
<p>The present model instantiated the core processing principles of the recoding interference hypothesis rather than TIPS. Indeed, verbalization in the model changed the pattern of activations in the internal layer (<xref rid="pone.0127618.g004" ref-type="fig">Fig 4</xref>), which in turn impaired the subsequent computation of the item-specific representation of a retinotopic face input (Figs <xref rid="pone.0127618.g003" ref-type="fig">3B</xref> and <xref rid="pone.0127618.g004" ref-type="fig">4</xref>). Thus, an implemented computational model allows an explicit instantiation of one theory without ambiguity, and can advance the understanding of how cognition emerges from the model.</p>
<sec id="sec022">
<title>Computationally grounded recoding interference hypothesis for verbal overshadowing</title>
<p>The present model successfully simulated standard verbal overshadowing (Simulations 1–3), and the detailed analysis of the internal representations (<xref rid="pone.0127618.g004" ref-type="fig">Fig 4</xref>) demonstrated that accuracy was reduced by the mechanism assumed by the recoding interference hypothesis. More crucially, such a model also reproduced the experimental phenomena that had been advocated as “counterevidence” against the recoding interference hypothesis (Simulations 4–5). These phenomena included the fact that verbalization of a non-target face impairs recognition of a target face (e.g., [<xref rid="pone.0127618.ref023" ref-type="bibr">23</xref>]) and the fact that the accuracy of verbal descriptions does not linearly explain its negative effect on subsequent face recognition [<xref rid="pone.0127618.ref001" ref-type="bibr">1</xref>,<xref rid="pone.0127618.ref006" ref-type="bibr">6</xref>,<xref rid="pone.0127618.ref008" ref-type="bibr">8</xref>,<xref rid="pone.0127618.ref025" ref-type="bibr">25</xref>]. These demonstrations mean that these phenomena are predictable from the recoding interference hypothesis, contrary to prior expectations.</p>
<p>An explicit implementation of a computational model aims to advance the understanding of a cognitive behavior, not to merely reproduce it. Therefore, it is crucial to explain how these phenomena emerge from the implemented model. First, by verbalizing irrelevant information (e.g., non-target face), the network settles down into an attractor that captures information irrelevant to the target face (<xref rid="pone.0127618.s003" ref-type="supplementary-material">S3 Fig</xref>). That is, given that the attractor states in the hidden layer captured the visual similarity of inputs (see <xref rid="pone.0127618.g004" ref-type="fig">Fig 4</xref>), the resultant internal representation by verbalization was dissimilar from that required for recognition of the target face. As a result, the network was captured by a distant attractor state, and it was difficult to travel into the correct attractor basin in order to compute target-specific information. Thus, verbalization induces noise in the hidden layer activities. In such a situation, the “old” items lose an advantage of training, and the resultant polarity distributions for the “old” and “new” items overlap to a greater extent, thereby reducing the “old”/“new” judgment accuracy.</p>
<p>Second, the present model also provides an account for why target description accuracy does not linearly predict recognition accuracy. If we focus on only target description accuracy and the polarity values for “old” faces only, then certainly these two correlate with each other (i.e., <xref rid="pone.0127618.g009" ref-type="fig">Fig 9A</xref>). However, Simulation 5 demonstrated that the generated verbal descriptions affected not just the polarity (familiarity) values of “old” items but also those of “new” items, as a function of how accurately the descriptions captured the distractor faces (<xref rid="pone.0127618.g009" ref-type="fig">Fig 9B</xref>). This was the reason why target description accuracy in isolation does not necessarily predict the effect size of verbal overshadowing in a linear fashion. The past experimental studies have not considered the role of distractor description accuracy.</p>
</sec>
<sec id="sec023">
<title>Distributed representation of a fully interactive system vs. operation-specific representation in each of multiple modules for face recognition</title>
<p>Studying verbal overshadowing is not only for the sake of this phenomenon. Rather, an investigation of the mechanism in this domain has implications for other domains as well. Indeed, the controversy between the recoding interference hypothesis and the transfer-inappropriate hypothesis mirrors the controversies between a single-system account (distributed cognition) and a multiple-systems account (each system has its own processing-specific computational principle/representation) in various other domains, such as word reading [<xref rid="pone.0127618.ref041" ref-type="bibr">41</xref>,<xref rid="pone.0127618.ref042" ref-type="bibr">42</xref>], word recognition [<xref rid="pone.0127618.ref039" ref-type="bibr">39</xref>,<xref rid="pone.0127618.ref042" ref-type="bibr">42</xref>], priming effects [<xref rid="pone.0127618.ref038" ref-type="bibr">38</xref>], past-tense inflection [<xref rid="pone.0127618.ref043" ref-type="bibr">43</xref>,<xref rid="pone.0127618.ref044" ref-type="bibr">44</xref>], and action sequences [<xref rid="pone.0127618.ref045" ref-type="bibr">45</xref>,<xref rid="pone.0127618.ref046" ref-type="bibr">46</xref>]. In all of these domains, some behavioral data were advocated as counterevidence against a single-mechanism account. Indeed, it is not surprising that any theory can explain a phenomenon if the number of processing principles to assume is a free parameter. Such a multiple-systems account sacrifices the parsimony of a theory, and sometimes it is only informally providing a model (i.e., only words or boxes-and-arrows). The computationally implemented parallel-distributed processing models listed above (single-system accounts) have successfully reproduced such “counterevidence,” thereby providing more parsimonious accounts.</p>
<p>The history of verbal overshadowing has partially followed this same path. Thus, some experimental findings have been more easily explained by increasing the number of processing principles, each of which operates independently (TIPS: [<xref rid="pone.0127618.ref019" ref-type="bibr">19</xref>]), but it has never excluded the possibility that these phenomena can be underpinned by a fully-interactive system with distributed representations. The present study actually demonstrated this idea. Thus, there is no need to assume differential processing principles or operation-specific representations. This way, the present investigation can be located in the context of a broader controversy. Related to this, it is worth reiterating that the computational mechanism of verbal overshadowing we demonstrated (<xref rid="pone.0127618.g004" ref-type="fig">Fig 4</xref>) is consistent with priming effects that past PDP models have demonstrated [<xref rid="pone.0127618.ref038" ref-type="bibr">38</xref>]. Indeed, a recent large-scale replication study has demonstrated that verbal overshadowing is robustly replicable if face recognition immediately follows verbalization [<xref rid="pone.0127618.ref004" ref-type="bibr">4</xref>]. This procedure was exactly instantiated in the current model. Thus, our model prompts a future experimental study that will more directly test whether the effects of verbalization and those of priming are based on a common mechanism. In this way, verbal overshadowing will be located in the context of broader psychological issues.</p>
<p>In addition, the current study is relevant to face recognition itself, not just verbal overshadowing. As mentioned in the introduction, our model links the assumption of the verbal code and visual code, like Bruce and Young’s [<xref rid="pone.0127618.ref015" ref-type="bibr">15</xref>] face recognition model. A key issue is how interactive/modularistic these two are. Thus, in this context, an issue is whether face recognition is grounded in a distributed processing system, whose representation is also used in other domains such as language, or not. On this issue, neuroscience data have been advocated as a support for a modularistic view of face recognition. For example, the fusiform face area (FFA) in the right hemisphere is known to be activated to a greater extent when recognizing a face rather than other stimulus types [<xref rid="pone.0127618.ref047" ref-type="bibr">47</xref>–<xref rid="pone.0127618.ref049" ref-type="bibr">49</xref>]. Furthermore, damage in this area sometimes gives rise into a face-specific recognition impairment without reducing recognition performance for words [<xref rid="pone.0127618.ref050" ref-type="bibr">50</xref>]. Such a dissociation is readily explained by a multiple-module view that assumes a language-specific processing system (and its processing-specific representation) and a face-specific processing system independently. However, Plaut and Behrmann [<xref rid="pone.0127618.ref027" ref-type="bibr">27</xref>] instantiated a single-mechanism account into a computational model, in which representations were distributed across the layers within the model, and all of the units, links, and activations in the model contributed to recognition of all the stimulus types (faces, words, and houses). The present model targeted a more specific phenomenon, namely verbal overshadowing, but our model is clearly a variant of Plaut and Behrmann’s [<xref rid="pone.0127618.ref027" ref-type="bibr">27</xref>] face recognition model. That is, the present study extended the plausibility of Plaut and Behrmann’s [<xref rid="pone.0127618.ref027" ref-type="bibr">27</xref>] single-account theory of face recognition to verbal overshadowing. Given that verbal overshadowing is crucially relevant to daily life and legal issues (e.g., eyewitness testimony), the present study bridges the broad theoretical domain and the more applied domain within the same framework.</p>
</sec>
<sec id="sec024">
<title>Applied issues</title>
<p>Verbal overshadowing has been vigorously discussed in terms of relevance to applied issues. There are two unique contributions from the present study in this regard. First, one should not ignore the quality (accuracy) of verbal descriptions that have been generated before the recognition phase. To date, consensus has been that description accuracy does not necessarily correlate with the degree of recognition impairment [<xref rid="pone.0127618.ref001" ref-type="bibr">1</xref>,<xref rid="pone.0127618.ref006" ref-type="bibr">6</xref>,<xref rid="pone.0127618.ref008" ref-type="bibr">8</xref>,<xref rid="pone.0127618.ref025" ref-type="bibr">25</xref>]. Such a conclusion would undermine the importance of analyzing the verbal descriptions. However, we demonstrated that verbal descriptions certainly predicted recognition impairment in a complex manner. Specifically, when evaluating the credibility of a testimony, the foils should be carefully selected in terms of their relevance to the generated verbal descriptions, and the generated verbal descriptions should be regarded as including potentially crucial information. Second, the present study confirmed the accountability of the recoding interference hypothesis [<xref rid="pone.0127618.ref003" ref-type="bibr">3</xref>,<xref rid="pone.0127618.ref005" ref-type="bibr">5</xref>]. That is, the current study indicates that any theory about <italic>interference</italic> in the memory domain is potentially useful when discussing the effect of verbalization on non-verbal recognition, both theoretically and practically.</p>
</sec>
<sec id="sec025">
<title>Replicability</title>
<p>As mentioned above, there have been occasional failures to replicate verbal overshadowing, although the recent large-scale replication project demonstrated its robustness [<xref rid="pone.0127618.ref004" ref-type="bibr">4</xref>]. Therefore, it would be constructive to clarify the reasons for the occasional replication failures. In addition to the target-distractor similarity factor (e.g., [<xref rid="pone.0127618.ref006" ref-type="bibr">6</xref>,<xref rid="pone.0127618.ref007" ref-type="bibr">7</xref>]) and the description accuracy [<xref rid="pone.0127618.ref002" ref-type="bibr">2</xref>,<xref rid="pone.0127618.ref005" ref-type="bibr">5</xref>,<xref rid="pone.0127618.ref008" ref-type="bibr">8</xref>], the present study also clarified the effect of other random factors. Thus, there were individual differences in subjects regarding how the familiarity distribution of old items and that of new items change by verbalization (<xref rid="pone.0127618.g010" ref-type="fig">Fig 10</xref>). In addition, the present model demonstrated how noisy (i.e., affected by individual item differences) a single-trial testing method is. One may find such a conclusion unsurprising, but, it is illustrative to explicitly demonstrate the effects of these random factors in a computational model, rather than stating a mere verbal explanation. Taken together, we suggest that it is premature to argue whether verbal overshadowing exists or not, in an all-or-none manner. Indeed, several studies which involved a multiple-trial testing method had a good replicability [<xref rid="pone.0127618.ref017" ref-type="bibr">17</xref>,<xref rid="pone.0127618.ref018" ref-type="bibr">18</xref>,<xref rid="pone.0127618.ref051" ref-type="bibr">51</xref>]. Also, it is worth mentioning that although these studies are referred as collateral evidence for TIPS, our model shows no necessity of implementing a modularistic system/process like a TIPS.</p>
</sec>
<sec id="sec026">
<title>Other experimental studies</title>
<p>Finally, the limitations of the current study should be acknowledged to prompt future studies. In a strict sense, the present model did not cover all of the behavioral phenomena relevant to verbal overshadowing. For example, we did not consider whether our account can be extended to the visual imagery domain beyond the face recognition (e.g., [<xref rid="pone.0127618.ref052" ref-type="bibr">52</xref>]). However, we suggest that even if the same procedure (i.e., verbalization) reduces accuracy in two (or more) different non-verbal domains, there is no need to explain such decreased accuracy by a single theory. Unless there is a strong a priori reason to explain two phenomena by a single account, it would be more conservative to develop a theory within a domain. Our model also did not examine other effects found in verbal overshadowing literature such as verbal facilitation effect [<xref rid="pone.0127618.ref002" ref-type="bibr">2</xref>,<xref rid="pone.0127618.ref033" ref-type="bibr">33</xref>,<xref rid="pone.0127618.ref053" ref-type="bibr">53</xref>,<xref rid="pone.0127618.ref054" ref-type="bibr">54</xref>], and other similar phenomena such as a Navon effet [<xref rid="pone.0127618.ref055" ref-type="bibr">55</xref>,<xref rid="pone.0127618.ref056" ref-type="bibr">56</xref>]. In future studies, it should be examined whether the model can be extended to cover these phenomena. In addition, the present model should be added more realistic features of human face recognition. For example, the present model can describe limited facial information. If the model were permitted to describe a multiple of information, the model may have a better chance capturing correlation between incorrect description and identification accuracy [<xref rid="pone.0127618.ref002" ref-type="bibr">2</xref>]. Furthermore, it might be worth to examine how verbalization affects face recognition when a model implements more realistic facial input (i.e. input of facial features are updated continually by visual search). We made the present model as simple as possible for first step. We would like to emphasize that by presenting this model, we are providing a platform where other researchers can freely tackle these issues.</p>
</sec>
</sec>
<sec id="sec027">
<title>Supporting Information</title>
<supplementary-material id="pone.0127618.s001" xlink:href="info:doi/10.1371/journal.pone.0127618.s001" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S1 Fig</label>
<caption>
<title>Visual images of 64 faces.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0127618.s002" xlink:href="info:doi/10.1371/journal.pone.0127618.s002" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S2 Fig</label>
<caption>
<title>Visual images of 64 faces with Gaussian noise.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0127618.s003" xlink:href="info:doi/10.1371/journal.pone.0127618.s003" mimetype="image/tiff" position="float" xlink:type="simple">
<label>S3 Fig</label>
<caption>
<title>Multidimensional scaling analysis of the internal activities during verbalization of an irrelevant face and recognition of a target face.</title>
<p>(TIF)</p>
</caption>
</supplementary-material>
<supplementary-material id="pone.0127618.s004" xlink:href="info:doi/10.1371/journal.pone.0127618.s004" mimetype="application/zip" position="float" xlink:type="simple">
<label>S1 File</label>
<caption>
<title>Simulation data set.</title>
<p>(ZIP)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>Some parts of this study were published in Proceedings of the 35th Annual Conference of the Cognitive Science Society [<xref rid="pone.0127618.ref057" ref-type="bibr">57</xref>].</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0127618.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schooler</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Engstler-Schooler</surname> <given-names>TY</given-names></name> (<year>1990</year>) <article-title>Verbal overshadowing of visual memories: some things are better left unsaid</article-title>. <source>Cogn Psychol</source> <volume>22</volume>: <fpage>36</fpage>–<lpage>71</lpage>. <object-id pub-id-type="pmid">2295225</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meissner</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Sporer</surname> <given-names>SL</given-names></name>, <name name-style="western"><surname>Susa</surname> <given-names>KJ</given-names></name> (<year>2008</year>) <article-title>A theoretical review and meta-analysis of the description-identification relationship in memory for faces</article-title>. <source>Eur J Cogn Psychol</source> <volume>20</volume>: <fpage>414</fpage>–<lpage>455</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/09541440701728581" xlink:type="simple">10.1080/09541440701728581</ext-link></comment></mixed-citation></ref>
<ref id="pone.0127618.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meissner</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Brigham</surname> <given-names>JC</given-names></name> (<year>2001</year>) <article-title>A meta-analysis of the verbal overshadowing effect in face identification</article-title>. <source>Appl Cogn Psychol</source> <volume>15</volume>: <fpage>603</fpage>–<lpage>616</lpage>.</mixed-citation></ref>
<ref id="pone.0127618.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Alogna</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Attaya</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Aucoin</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Bahník</surname> <given-names>Š</given-names></name>, <name name-style="western"><surname>Birch</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Birt</surname> <given-names>AR</given-names></name> <etal>et al</etal>. (<year>2014</year>) <article-title>Registered Replication Report: Schooler and Engstler-Schooler (1990)</article-title>. <source>Perspect Psychol Sci</source> <volume>9</volume>: <fpage>556</fpage>–<lpage>578</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1177/1745691614545653" xlink:type="simple">10.1177/1745691614545653</ext-link></comment></mixed-citation></ref>
<ref id="pone.0127618.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Meissner</surname> <given-names>CA</given-names></name>, <name name-style="western"><surname>Brigham</surname> <given-names>JC</given-names></name>, <name name-style="western"><surname>Kelley</surname> <given-names>CM</given-names></name> (<year>2001</year>) <article-title>The influence of retrieval processes in verbal overshadowing</article-title>. <source>Mem Cognit</source> <volume>29</volume>: <fpage>176</fpage>–<lpage>186</lpage>. <object-id pub-id-type="pmid">11277460</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kitagami</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Sato</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Yoshikawa</surname> <given-names>S</given-names></name> (<year>2002</year>) <article-title>The influence of test-set similarity in verbal overshadowing</article-title>. <source>Appl Cogn Psychol</source> <volume>16</volume>: <fpage>963</fpage>–<lpage>972</lpage>.</mixed-citation></ref>
<ref id="pone.0127618.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wickham</surname> <given-names>LHV</given-names></name>, <name name-style="western"><surname>Swift</surname> <given-names>H</given-names></name> (<year>2006</year>) <article-title>Articulatory suppression attenuates the verbal overshadowing effect: a role for verbal encoding in face identification</article-title>. <source>Appl Cogn Psychol</source> <volume>20</volume>: <fpage>157</fpage>–<lpage>169</lpage>.</mixed-citation></ref>
<ref id="pone.0127618.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fallshore</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Schooler</surname> <given-names>JW</given-names></name> (<year>1995</year>) <article-title>Verbal vulnerability of perceptual expertise</article-title>. <source>J Exp Psychol Learn Mem Cogn</source> <volume>21</volume>: <fpage>1608</fpage>–<lpage>1623</lpage>. <object-id pub-id-type="pmid">7490581</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Perfect</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Hunt</surname> <given-names>LJ</given-names></name>, <name name-style="western"><surname>Harris</surname> <given-names>CM</given-names></name> (<year>2002</year>) <article-title>Verbal overshadowing in voice recognition</article-title>. <source>Appl Cogn Psychol</source> <volume>16</volume>: <fpage>973</fpage>–<lpage>980</lpage>.</mixed-citation></ref>
<ref id="pone.0127618.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Vanags</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Carroll</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Perfect</surname> <given-names>TJ</given-names></name> (<year>2005</year>) <article-title>Verbal overshadowing: a sound theory in voice recognition?</article-title> <source>Appl Cogn Psychol</source> <volume>19</volume>: <fpage>1127</fpage>–<lpage>1144</lpage>.</mixed-citation></ref>
<ref id="pone.0127618.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Melcher</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Schooler</surname> <given-names>J</given-names></name> (<year>1996</year>) <article-title>The misremembrance of wines past: Verbal and perceptual expertise differentially mediate verbal overshadowing of taste memory</article-title>. <source>J Mem Lang</source> <volume>245</volume>: <fpage>231</fpage>–<lpage>245</lpage>.</mixed-citation></ref>
<ref id="pone.0127618.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wilson</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Schooler</surname> <given-names>J</given-names></name> (<year>1991</year>) <article-title>Thinking too much: introspection can reduce the quality of preferences and decisions</article-title>. <source>J Pers Soc Psychol</source> <volume>60</volume>: <fpage>1</fpage>–<lpage>23</lpage>.</mixed-citation></ref>
<ref id="pone.0127618.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schooler</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Ohlsson</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Brooks</surname> <given-names>K</given-names></name> (<year>1993</year>) <article-title>Thoughts beyond words: When language overshadows insight</article-title>. <source>J Exp Psychol Gen</source> <volume>122</volume>: <fpage>166</fpage>–<lpage>183</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/0096-3445.122.2.166" xlink:type="simple">10.1037/0096-3445.122.2.166</ext-link></comment></mixed-citation></ref>
<ref id="pone.0127618.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lane</surname> <given-names>SM</given-names></name>, <name name-style="western"><surname>Schooler</surname> <given-names>JW</given-names></name> (<year>2004</year>) <article-title>Skimming the surface. Verbal overshadowing of analogical retrieval</article-title>. <source>Psychol Sci</source> <volume>15</volume>: <fpage>715</fpage>–<lpage>719</lpage>. <object-id pub-id-type="pmid">15482442</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Bruce</surname> <given-names>V</given-names></name>, <name name-style="western"><surname>Young</surname> <given-names>A</given-names></name> (<year>1986</year>) <article-title>Understanding face recognition</article-title>. <source>Br J Psychol</source> <volume>77</volume> (<issue>Pt 3</issue>): <fpage>305</fpage>–<lpage>327</lpage>. <object-id pub-id-type="pmid">3756376</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Melcher</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Schooler</surname> <given-names>JW</given-names></name> (<year>2004</year>) <article-title>Perceptual and conceptual training mediate the verbal overshadowing effect in an unfamiliar domain</article-title>. <source>Mem Cognit</source> <volume>32</volume>: <fpage>618</fpage>–<lpage>631</lpage>. <object-id pub-id-type="pmid">15478756</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brown</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Lloyd-Jones</surname> <given-names>TJ</given-names></name> (<year>2002</year>) <article-title>Verbal overshadowing in a multiple face presentation paradigm: effects of description instruction</article-title>. <source>Appl Cogn Psychol</source> <volume>16</volume>: <fpage>873</fpage>–<lpage>885</lpage>.</mixed-citation></ref>
<ref id="pone.0127618.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brown</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Lloyd-Jones</surname> <given-names>TJ</given-names></name> (<year>2003</year>) <article-title>Verbal overshadowing of multiple face and car recognition: effects of within- versus across-category verbal descriptions</article-title>. <source>Appl Cogn Psychol</source> <volume>17</volume>: <fpage>183</fpage>–<lpage>201</lpage>.</mixed-citation></ref>
<ref id="pone.0127618.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Schooler</surname> <given-names>JW</given-names></name> (<year>2002</year>) <article-title>Verbalization produces a transfer inappropriate processing shift</article-title>. <source>Appl Cogn Psychol</source> <volume>16</volume>: <fpage>989</fpage>–<lpage>997</lpage>.</mixed-citation></ref>
<ref id="pone.0127618.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Morris</surname> <given-names>CD</given-names></name>, <name name-style="western"><surname>Bransford</surname> <given-names>JD</given-names></name>, <name name-style="western"><surname>Franks</surname> <given-names>JJ</given-names></name> (<year>1977</year>) <article-title>Levels of processing versus transfer appropriate processing</article-title>. <source>J Verbal Learning Verbal Behav</source> <volume>16</volume>: <fpage>519</fpage>–<lpage>533</lpage>.</mixed-citation></ref>
<ref id="pone.0127618.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Maurer</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Grand</surname> <given-names>R Le</given-names></name>, <name name-style="western"><surname>Mondloch</surname> <given-names>CJ</given-names></name> (<year>2002</year>) <article-title>The many faces of configural processing</article-title>. <source>Trends Cogn Sci</source> <volume>6</volume>: <fpage>255</fpage>–<lpage>260</lpage>. <object-id pub-id-type="pmid">12039607</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tanaka</surname> <given-names>JW</given-names></name>, <name name-style="western"><surname>Farah</surname> <given-names>MJ</given-names></name> (<year>1993</year>) <article-title>Parts and wholes in face recognition</article-title>. <source>Q J Exp Psychol Sect A</source> <volume>46</volume>: <fpage>225</fpage>–<lpage>245</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/14640749308401045" xlink:type="simple">10.1080/14640749308401045</ext-link></comment></mixed-citation></ref>
<ref id="pone.0127618.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dodson</surname> <given-names>CS</given-names></name>, <name name-style="western"><surname>Johnson</surname> <given-names>MK</given-names></name>, <name name-style="western"><surname>Schooler</surname> <given-names>JW</given-names></name> (<year>1997</year>) <article-title>The verbal overshadowing effect: why descriptions impair face recognition</article-title>. <source>Mem Cognit</source> <volume>25</volume>: <fpage>129</fpage>–<lpage>139</lpage>. <object-id pub-id-type="pmid">9099066</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Westerman</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Larsen</surname> <given-names>J</given-names></name> (<year>1997</year>) <article-title>Verbal-overshadowing effect: Evidence for a general shift in processing</article-title>. <source>Am J Psychol</source> <volume>110</volume>: <fpage>417</fpage>–<lpage>428</lpage>. <object-id pub-id-type="pmid">9339538</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Finger</surname> <given-names>K</given-names></name> (<year>2002</year>) <article-title>Mazes and music: using perceptual processing to release verbal overshadowing</article-title>. <source>Appl Cogn Psychol</source> <volume>16</volume>: <fpage>887</fpage>–<lpage>896</lpage>.</mixed-citation></ref>
<ref id="pone.0127618.ref026"><label>26</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Young</surname> <given-names>RA</given-names></name> (<year>1987</year>) <article-title>The Gaussian derivative model for spatial vision: I. Retinal mechanisms</article-title>. <source>Spat Vis</source> <volume>2</volume>: <fpage>273</fpage>–<lpage>293</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1163/156856887X00222" xlink:type="simple">10.1163/156856887X00222</ext-link></comment> <object-id pub-id-type="pmid">3154952</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Plaut</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Behrmann</surname> <given-names>M</given-names></name> (<year>2011</year>) <article-title>Complementary neural representations for faces and words: a computational exploration</article-title>. <source>Cogn Neuropsychol</source> <volume>28</volume>: <fpage>251</fpage>–<lpage>275</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/02643294.2011.609812" xlink:type="simple">10.1080/02643294.2011.609812</ext-link></comment> <object-id pub-id-type="pmid">22185237</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Valentin</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Abdi</surname> <given-names>H</given-names></name> (<year>1996</year>) <article-title>Can a linear autoassociator recognize faces from new orientations?</article-title> <source>JOSA A</source> <volume>13</volume>: <fpage>717</fpage>.</mixed-citation></ref>
<ref id="pone.0127618.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hinton</surname> <given-names>G</given-names></name> (<year>1989</year>) <article-title>Connectionist learning procedures</article-title>. <source>Artif Intell</source> <volume>40</volume>: <fpage>185</fpage>–<lpage>234</lpage>.</mixed-citation></ref>
<ref id="pone.0127618.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Plaut</surname> <given-names>DC</given-names></name> (<year>2002</year>) <article-title>Graded modality-specific specialisation in semantics: A computational account of optic aphasia</article-title>. <source>Cogn Neuropsychol</source> <volume>19</volume>: <fpage>603</fpage>–<lpage>639</lpage>. Available: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/20957556" xlink:type="simple">http://www.ncbi.nlm.nih.gov/pubmed/20957556</ext-link>. Accessed 27 February 2014. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/02643290244000112" xlink:type="simple">10.1080/02643290244000112</ext-link></comment> <object-id pub-id-type="pmid">20957556</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Dilkina</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>McClelland</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Plaut</surname> <given-names>DC</given-names></name> (<year>2008</year>) <article-title>A single-system account of semantic and lexical deficits in five semantic dementia patients</article-title>. <source>Cogn Neuropsychol</source> <volume>25</volume>: <fpage>136</fpage>–<lpage>164</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/02643290701723948" xlink:type="simple">10.1080/02643290701723948</ext-link></comment> <object-id pub-id-type="pmid">18568816</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sporer</surname> <given-names>SL</given-names></name> (<year>1991</year>) <article-title>Deep—deeper—deepest? Encoding strategies and the recognition of human faces</article-title>. <source>J Exp Psychol Learn Mem Cogn</source> <volume>17</volume>: <fpage>323</fpage>–<lpage>333</lpage>. <object-id pub-id-type="pmid">1827834</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Winograd</surname> <given-names>E</given-names></name> (<year>1981</year>) <article-title>Elaboration and distinctiveness in memory for faces</article-title>. <source>J Exp Psychol Hum Learn</source> <volume>7</volume>: <fpage>181</fpage>–<lpage>190</lpage>. <object-id pub-id-type="pmid">7241060</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Tulving</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Schacter</surname> <given-names>DL</given-names></name>, <name name-style="western"><surname>Stark</surname> <given-names>HA</given-names></name> (<year>1982</year>) <article-title>Priming effects in word-fragment completion are independent of recognition memory</article-title>. <source>J Exp Psychol Learn Mem Cogn</source> <volume>8</volume>: <fpage>336</fpage>–<lpage>342</lpage>.</mixed-citation></ref>
<ref id="pone.0127618.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kintsch</surname> <given-names>W</given-names></name> (<year>1967</year>) <article-title>Memory and decision aspects of recognition learning</article-title>. <source>Psychol Rev</source> <volume>74</volume>: <fpage>496</fpage>–<lpage>504</lpage>. <object-id pub-id-type="pmid">4867890</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Yonelinas</surname> <given-names>AP</given-names></name> (<year>2002</year>) <article-title>The Nature of Recollection and Familiarity: A Review of 30 Years of Research</article-title>. <source>J Mem Lang</source> <volume>46</volume>: <fpage>441</fpage>–<lpage>517</lpage>.</mixed-citation></ref>
<ref id="pone.0127618.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Banks</surname> <given-names>W</given-names></name> (<year>1970</year>) <article-title>Signal Detection Theory and human memory</article-title>. <source>Psychol Bull</source> <volume>74</volume>: <fpage>81</fpage>–<lpage>99</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/h0029531" xlink:type="simple">10.1037/h0029531</ext-link></comment></mixed-citation></ref>
<ref id="pone.0127618.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Plaut</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>Booth</surname> <given-names>JR</given-names></name> (<year>2000</year>) <article-title>Individual and developmental differences in semantic priming: empirical and computational support for a single-mechanism account of lexical processing</article-title>. <source>Psychol Rev</source> <volume>107</volume>: <fpage>786</fpage>–<lpage>823</lpage>. <object-id pub-id-type="pmid">11089407</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Plaut</surname> <given-names>DC</given-names></name> (<year>1997</year>) <article-title>Structure and Function in the Lexical System: Insights from Distributed Models of Word Reading and Lexical Decision</article-title>. <source>Lang Cogn Process</source> <volume>12</volume>: <fpage>765</fpage>–<lpage>806</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/016909697386682" xlink:type="simple">10.1080/016909697386682</ext-link></comment></mixed-citation></ref>
<ref id="pone.0127618.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Norman</surname> <given-names>K A</given-names></name>, <name name-style="western"><surname>O’Reilly</surname> <given-names>RC</given-names></name> (<year>2003</year>) <article-title>Modeling hippocampal and neocortical contributions to recognition memory: a complementary-learning-systems approach</article-title>. <source>Psychol Rev</source> <volume>110</volume>: <fpage>611</fpage>–<lpage>646</lpage>. <object-id pub-id-type="pmid">14599236</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Plaut</surname> <given-names>DC</given-names></name>, <name name-style="western"><surname>McClelland</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Seidenberg</surname> <given-names>MS</given-names></name>, <name name-style="western"><surname>Patterson</surname> <given-names>K</given-names></name> (<year>1996</year>) <article-title>Understanding normal and impaired word reading: computational principles in quasi-regular domains</article-title>. <source>Psychol Rev</source> <volume>103</volume>: <fpage>56</fpage>–<lpage>115</lpage>. <object-id pub-id-type="pmid">8650300</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Coltheart</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Rastle</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Perry</surname> <given-names>C</given-names></name> (<year>2001</year>) <article-title>DRC: a dual route cascaded model of visual word recognition and reading aloud</article-title>. <source>Psychol Rev</source> <volume>108</volume>: <fpage>204</fpage>–<lpage>256</lpage>. <object-id pub-id-type="pmid">11212628</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Joanisse</surname> <given-names>MF</given-names></name>, <name name-style="western"><surname>Seidenberg</surname> <given-names>MS</given-names></name> (<year>1999</year>) <article-title>Impairments in verb morphology after brain injury: a connectionist model</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>96</volume>: <fpage>7592</fpage>–<lpage>7597</lpage>. <object-id pub-id-type="pmid">10377460</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Pinker</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ullman</surname> <given-names>MT</given-names></name> (<year>2002</year>) <article-title>The Past-Tense Debate</article-title>. <source>Trends Cogn Sci</source> <volume>6</volume>: <fpage>456</fpage>–<lpage>463</lpage>. <object-id pub-id-type="pmid">12457895</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Botvinick</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Plaut</surname> <given-names>DC</given-names></name> (<year>2004</year>) <article-title>Doing without schema hierarchies: a recurrent connectionist approach to normal and impaired routine sequential action</article-title>. <source>Psychol Rev</source> <volume>111</volume>: <fpage>395</fpage>–<lpage>429</lpage>. <object-id pub-id-type="pmid">15065915</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Cooper</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Shallice</surname> <given-names>T</given-names></name> (<year>2000</year>) <article-title>Contention scheduling and the control of routine activities</article-title>. <source>Cogn Neuropsychol</source> <volume>17</volume>: <fpage>297</fpage>–<lpage>338</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/026432900380427" xlink:type="simple">10.1080/026432900380427</ext-link></comment> <object-id pub-id-type="pmid">20945185</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name> (<year>1997</year>) <article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title>. <source>J Neurosci</source> <volume>17</volume>: <fpage>4302</fpage>–<lpage>4311</lpage>. <object-id pub-id-type="pmid">9151747</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref048"><label>48</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Grill-Spector</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Knouf</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name> (<year>2004</year>) <article-title>The fusiform face area subserves face perception, not generic within-category identification</article-title>. <source>Nat Neurosci</source> <volume>7</volume>: <fpage>555</fpage>–<lpage>562</lpage>. <object-id pub-id-type="pmid">15077112</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref049"><label>49</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Yovel</surname> <given-names>G</given-names></name> (<year>2006</year>) <article-title>The fusiform face area: a cortical region specialized for the perception of faces</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source> <volume>361</volume>: <fpage>2109</fpage>–<lpage>2128</lpage>. <object-id pub-id-type="pmid">17118927</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref050"><label>50</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name> (<year>2000</year>) <article-title>Domain specificity in face perception</article-title>. <source>Nat Neurosci</source> <volume>3</volume>: <fpage>759</fpage>–<lpage>763</lpage>. <object-id pub-id-type="pmid">10903567</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref051"><label>51</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lloyd-Jones</surname> <given-names>TJ</given-names></name>, <name name-style="western"><surname>Brown</surname> <given-names>C</given-names></name> (<year>2008</year>) <article-title>Verbal overshadowing of multiple face recognition: Effects on remembering and knowing over time</article-title>. <source>Eur J Cogn Psychol</source> <volume>20</volume>: <fpage>456</fpage>–<lpage>477</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/09541440701728425" xlink:type="simple">10.1080/09541440701728425</ext-link></comment></mixed-citation></ref>
<ref id="pone.0127618.ref052"><label>52</label><mixed-citation publication-type="other" xlink:type="simple">Brown C, Brandimonte M, Wickham L (2014) When Do Words Hurt? A Multiprocess View of the Effects of Verbalization on Visual Memory. J Exp Psychol Learn Mem Cogn.</mixed-citation></ref>
<ref id="pone.0127618.ref053"><label>53</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Brown</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Lloyd-Jones</surname> <given-names>TJ</given-names></name> (<year>2005</year>) <article-title>Verbal facilitation of face recognition</article-title>. <source>Mem Cognit</source> <volume>33</volume>: <fpage>1442</fpage>–<lpage>1456</lpage>. <object-id pub-id-type="pmid">16615392</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref054"><label>54</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Jones</surname> <given-names>TC</given-names></name>, <name name-style="western"><surname>Armstrong</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Casey</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Burson</surname> <given-names>RA</given-names></name>, <name name-style="western"><surname>Memon</surname> <given-names>A</given-names></name> (<year>2013</year>) <article-title>Verbal description benefits for faces when description conditions are unknown a priori</article-title>. <source>Q J Exp Psychol (Hove)</source> <volume>66</volume>: <fpage>1818</fpage>–<lpage>1839</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1080/17470218.2013.771688" xlink:type="simple">10.1080/17470218.2013.771688</ext-link></comment> <object-id pub-id-type="pmid">23480450</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref055"><label>55</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Hills</surname> <given-names>PJ</given-names></name>, <name name-style="western"><surname>Lewis</surname> <given-names>MB</given-names></name> (<year>2009</year>) <article-title>A spatial frequency account of the detriment that local processing of Navon letters has on face recognition</article-title>. <source>J Exp Psychol Hum Percept Perform</source> <volume>35</volume>: <fpage>1427</fpage>–<lpage>1442</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1037/a0015788" xlink:type="simple">10.1037/a0015788</ext-link></comment> <object-id pub-id-type="pmid">19803647</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref056"><label>56</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Macrae</surname> <given-names>CN</given-names></name>, <name name-style="western"><surname>Lewis</surname> <given-names>HL</given-names></name> (<year>2002</year>) <article-title>Do I know you? Processing orientation and face recognition</article-title>. <source>Psychol Sci</source> <volume>13</volume>: <fpage>194</fpage>–<lpage>196</lpage>. <object-id pub-id-type="pmid">11934008</object-id></mixed-citation></ref>
<ref id="pone.0127618.ref057"><label>57</label><mixed-citation publication-type="other" xlink:type="simple">Hatano A, Ueno T, Kitagami S, Kawaguchi J (2013) Why verbalization of facial features increases false positive responses on visually-similar distractors: A computational exploration of verbal overshadowing. Proceedings of the 35th Annual Conference of the Cognitive Science Society. pp. 2494–2499.</mixed-citation></ref>
</ref-list>
</back>
</article>