<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN" "http://jats.nlm.nih.gov/publishing/1.1d3/JATS-journalpublishing1.dtd">
<article article-type="research-article" dtd-version="1.1d3" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink">
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">PLoS ONE</journal-id>
<journal-id journal-id-type="publisher-id">plos</journal-id>
<journal-id journal-id-type="pmc">plosone</journal-id>
<journal-title-group>
<journal-title>PLOS ONE</journal-title>
</journal-title-group>
<issn pub-type="epub">1932-6203</issn>
<publisher>
<publisher-name>Public Library of Science</publisher-name>
<publisher-loc>San Francisco, CA USA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1371/journal.pone.0178423</article-id>
<article-id pub-id-type="publisher-id">PONE-D-16-40576</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Article</subject>
</subj-group>
<subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Face</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Medicine and health sciences</subject><subj-group><subject>Anatomy</subject><subj-group><subject>Head</subject><subj-group><subject>Face</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Perception</subject><subj-group><subject>Face recognition</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognition</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Learning and memory</subject><subj-group><subject>Memory</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject><subj-group><subject>Fear</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject><subj-group><subject>Fear</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject><subj-group><subject>Happiness</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Emotions</subject><subj-group><subject>Happiness</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Behavior</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Animal signaling and communication</subject><subj-group><subject>Vocalization</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Zoology</subject><subj-group><subject>Animal behavior</subject><subj-group><subject>Animal signaling and communication</subject><subj-group><subject>Vocalization</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Neuroscience</subject><subj-group><subject>Cognitive science</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Biology and life sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group><subj-group subj-group-type="Discipline-v3"><subject>Social sciences</subject><subj-group><subject>Psychology</subject><subj-group><subject>Cognitive psychology</subject><subj-group><subject>Attention</subject></subj-group></subj-group></subj-group></subj-group></article-categories>
<title-group>
<article-title>Memory for faces and voices varies as a function of sex and expressed emotion</article-title>
<alt-title alt-title-type="running-head">Memory for emotion expressions</alt-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4420-2216</contrib-id>
<name name-style="western">
<surname>S. Cortes</surname>
<given-names>Diana</given-names>
</name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" corresp="yes" xlink:type="simple">
<name name-style="western">
<surname>Laukka</surname>
<given-names>Petri</given-names>
</name>
<xref ref-type="corresp" rid="cor001">*</xref>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Lindahl</surname>
<given-names>Christina</given-names>
</name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western">
<surname>Fischer</surname>
<given-names>Håkan</given-names>
</name>
<xref ref-type="aff" rid="aff001"/>
</contrib>
</contrib-group>
<aff id="aff001"><addr-line>Department of Psychology, Stockholm University, Stockholm, Sweden</addr-line></aff>
<contrib-group>
<contrib contrib-type="editor" xlink:type="simple">
<name name-style="western">
<surname>Eder</surname>
<given-names>Andreas B.</given-names>
</name>
<role>Editor</role>
<xref ref-type="aff" rid="edit1"/>
</contrib>
</contrib-group>
<aff id="edit1"><addr-line>University of Würzburg, GERMANY</addr-line></aff>
<author-notes>
<fn fn-type="conflict" id="coi001">
<p>The authors have declared that no competing interests exist.</p>
</fn>
<fn fn-type="con">
<p><list list-type="simple"> <list-item>
<p><bold>Conceptualization:</bold> HF PL.</p></list-item> <list-item>
<p><bold>Data curation:</bold> PL DSC.</p></list-item> <list-item>
<p><bold>Formal analysis:</bold> DSC PL.</p></list-item> <list-item>
<p><bold>Funding acquisition:</bold> PL.</p></list-item> <list-item>
<p><bold>Investigation:</bold> DSC CL.</p></list-item> <list-item>
<p><bold>Methodology:</bold> HF PL.</p></list-item> <list-item>
<p><bold>Project administration:</bold> DSC PL HF CL.</p></list-item> <list-item>
<p><bold>Resources:</bold> HF PL.</p></list-item> <list-item>
<p><bold>Software:</bold> PL DSC HF.</p></list-item> <list-item>
<p><bold>Supervision:</bold> DSC PL HF.</p></list-item> <list-item>
<p><bold>Validation:</bold> HF PL DSC.</p></list-item> <list-item>
<p><bold>Visualization:</bold> DSC PL HF.</p></list-item> <list-item>
<p><bold>Writing – original draft:</bold> DSC PL HF.</p></list-item> <list-item>
<p><bold>Writing – review &amp; editing:</bold> DSC PL HF.</p></list-item></list>
</p>
</fn>
<corresp id="cor001">* E-mail: <email xlink:type="simple">petri.laukka@psychology.su.se</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<day>1</day>
<month>6</month>
<year>2017</year>
</pub-date>
<pub-date pub-type="collection">
<year>2017</year>
</pub-date>
<volume>12</volume>
<issue>6</issue>
<elocation-id>e0178423</elocation-id>
<history>
<date date-type="received">
<day>11</day>
<month>10</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>12</day>
<month>5</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-year>2017</copyright-year>
<copyright-holder>S. Cortes et al</copyright-holder>
<license xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">
<license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/" xlink:type="simple">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
</license>
</permissions>
<self-uri content-type="pdf" xlink:href="info:doi/10.1371/journal.pone.0178423"/>
<abstract>
<p>We investigated how memory for faces and voices (presented separately and in combination) varies as a function of sex and emotional expression (anger, disgust, fear, happiness, sadness, and neutral). At encoding, participants judged the expressed emotion of items in forced-choice tasks, followed by incidental Remember/Know recognition tasks. Results from 600 participants showed that accuracy (hits minus false alarms) was consistently higher for neutral compared to emotional items, whereas accuracy for specific emotions varied across the presentation modalities (i.e., faces, voices, and face-voice combinations). For the subjective sense of recollection (“remember” hits), neutral items received the highest hit rates only for faces, whereas for voices and face-voice combinations anger and fear expressions instead received the highest recollection rates. We also observed better accuracy for items by female expressers, and own-sex bias where female participants displayed memory advantage for female faces and face-voice combinations. Results further suggest that own-sex bias can be explained by recollection, rather than familiarity, rates. Overall, results show that memory for faces and voices may be influenced by the expressions that they carry, as well as by the sex of both items and participants. Emotion expressions may also enhance the subjective sense of recollection without enhancing memory accuracy.</p>
</abstract>
<funding-group>
<award-group id="award001">
<funding-source>
<institution-wrap>
<institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100004359</institution-id>
<institution>Vetenskapsrådet</institution>
</institution-wrap>
</funding-source>
<award-id>2012-801</award-id>
<principal-award-recipient>
<name name-style="western">
<surname>Laukka</surname>
<given-names>Petri</given-names>
</name>
</principal-award-recipient>
</award-group>
<funding-statement>This research was supported by the Swedish Research Council awarded to PL (2012-801). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. <ext-link ext-link-type="uri" xlink:href="http://www.vr.se/inenglish.4.12fff4451215cbd83e4800015152.html" xlink:type="simple">http://www.vr.se/inenglish.4.12fff4451215cbd83e4800015152.html</ext-link>.</funding-statement>
</funding-group>
<counts>
<fig-count count="3"/>
<table-count count="3"/>
<page-count count="19"/>
</counts>
<custom-meta-group>
<custom-meta id="data-availability">
<meta-name>Data Availability</meta-name>
<meta-value>All relevant data are within the paper and its Supporting Information file. Data are also available at Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/6arw6/" xlink:type="simple">https://osf.io/6arw6/</ext-link>).</meta-value>
</custom-meta>
</custom-meta-group>
</article-meta>
</front>
<body>
<sec id="sec001" sec-type="intro">
<title>Introduction</title>
<p>Recognizing and remembering specific people is a central aspect of daily interactions. These processes are often influenced by emotional and social nonverbal expressions that people infer from others’ facial appearance and tone of voice [<xref ref-type="bibr" rid="pone.0178423.ref001">1</xref>]. Sex also plays a key role in social remembrance, as research suggests that women perform better than men in face recognition tasks. In particular, several studies have reported evidence for a female own-sex bias in memory to the effect that women remember more female than male neutral faces (for a review, see [<xref ref-type="bibr" rid="pone.0178423.ref002">2</xref>]). We note that the previous studies on memory bias have focused heavily on the facial channel, leaving aside other important person characteristics such as the voice. In addition, previous studies have mainly included neutral stimuli (e.g., [<xref ref-type="bibr" rid="pone.0178423.ref002">2</xref>, <xref ref-type="bibr" rid="pone.0178423.ref003">3</xref>, <xref ref-type="bibr" rid="pone.0178423.ref004">4</xref>]), despite the importance of emotional expressions for human interactions. The current study therefore aims to investigate effects of item sex and participant sex, as well as the effect of emotion expression, on memory for both faces and voices.</p>
<sec id="sec002">
<title>Own-sex bias in memory for faces and voices</title>
<p>Results showing female own-sex bias for neutral faces are fairly consistent across studies (e.g., [<xref ref-type="bibr" rid="pone.0178423.ref002">2</xref>]), but results for men show more variability. Some studies have reported that men remember neutral faces of both sexes at similar levels [<xref ref-type="bibr" rid="pone.0178423.ref005">5</xref>, <xref ref-type="bibr" rid="pone.0178423.ref006">6</xref>], whereas others have reported that men, just like women, are more accurate at recognizing female compared to male faces [<xref ref-type="bibr" rid="pone.0178423.ref004">4</xref>, <xref ref-type="bibr" rid="pone.0178423.ref007">7</xref>]. There are even some reports of male own-sex memory bias, where male participants showed superior performance for male neutral faces [<xref ref-type="bibr" rid="pone.0178423.ref008">8</xref>, <xref ref-type="bibr" rid="pone.0178423.ref009">9</xref>].</p>
<p>It has been suggested that men and women may use different strategies during the encoding phase [<xref ref-type="bibr" rid="pone.0178423.ref002">2</xref>, <xref ref-type="bibr" rid="pone.0178423.ref007">7</xref>, <xref ref-type="bibr" rid="pone.0178423.ref010">10</xref>, <xref ref-type="bibr" rid="pone.0178423.ref011">11</xref>], with women focusing more than men on emotional expressions [<xref ref-type="bibr" rid="pone.0178423.ref012">12</xref>]. Fulton et al. [<xref ref-type="bibr" rid="pone.0178423.ref012">12</xref>] instructed participants to specifically attend to the expression of faces (happy or neutral) and demonstrated that men’s recognition of own sex faces improved, and was similar to women’s recognition of male faces, when directly allocating attention to the expression rather than the sex of the face. There is also evidence for neural correlates of own-sex bias in memory. For example, women showed higher activation in areas relevant to face processing during encoding for female faces compared to male faces while this pattern was not found for male participants [<xref ref-type="bibr" rid="pone.0178423.ref013">13</xref>].</p>
<p>Few previous studies have investigated own-sex bias for emotionally expressive faces, and it is currently unclear if and how own-sex bias varies as a function of emotion. Wang [<xref ref-type="bibr" rid="pone.0178423.ref011">11</xref>] investigated if there was a female memory advantage for neutral, angry and happy expressions. The results indicated that while women outperformed men in overall recognition accuracy for female faces, this effect was driven by happy faces. In contrast, Armony and Sergerie [<xref ref-type="bibr" rid="pone.0178423.ref014">14</xref>] reported an own-sex memory bias for female participants that was limited to fearful faces when presenting neutral, happy and fear expressions. Wang [<xref ref-type="bibr" rid="pone.0178423.ref015">15</xref>], however, failed to replicate findings of own-sex bias in a study including neutral, positive, fearful, angry, sad, surprised, and disgusted faces.</p>
<p>Emotions are not only expressed through facial behavior, vocal expressions are also prime sources of information about a person’s affective state, both separately and in combination with facial expressions. As far as we know there are no previous studies on sex bias in memory for emotional voices. However, in a related study Skuk and Schweinberger [<xref ref-type="bibr" rid="pone.0178423.ref016">16</xref>] investigated sex differences in identification of familiar emotionally neutral voices, and reported that men identified more male than female voices, whereas women were equally good at identifying voices from both sexes, and showed higher overall identification rates than men.</p>
</sec>
<sec id="sec003">
<title>Memory for emotional faces and voices</title>
<p>Previous research is generally inconclusive regarding the effect of emotional expressions on unfamiliar face identity memory. Several studies report that memory for positive faces is more accurate than for neutral or negative ones [<xref ref-type="bibr" rid="pone.0178423.ref017">17</xref>, <xref ref-type="bibr" rid="pone.0178423.ref018">18</xref>, <xref ref-type="bibr" rid="pone.0178423.ref019">19</xref>, <xref ref-type="bibr" rid="pone.0178423.ref020">20</xref>]. However, other studies have instead found the opposite effect with an advantage for negative faces [<xref ref-type="bibr" rid="pone.0178423.ref014">14</xref>, <xref ref-type="bibr" rid="pone.0178423.ref021">21</xref>, <xref ref-type="bibr" rid="pone.0178423.ref022">22</xref>], while yet some studies report no clear advantage for emotional compared to neutral faces [<xref ref-type="bibr" rid="pone.0178423.ref023">23</xref>]. We note that it is difficult to directly compare results across studies because they differ in their methodology (e.g., incidental versus intentional memory tasks), and aim to address slightly different research questions. While in some studies no identities were repeated during encoding phase [<xref ref-type="bibr" rid="pone.0178423.ref014">14</xref>, <xref ref-type="bibr" rid="pone.0178423.ref015">15</xref>, <xref ref-type="bibr" rid="pone.0178423.ref020">20</xref>], other studies showed each identity several times but displaying different emotions [<xref ref-type="bibr" rid="pone.0178423.ref017">17</xref>, <xref ref-type="bibr" rid="pone.0178423.ref018">18</xref>, <xref ref-type="bibr" rid="pone.0178423.ref020">20</xref>]. In addition, different stimuli from the same identities have been used at study and test phases, for example, emotional faces were presented only in the encoding session whereas neutral faces were shown in the test session [<xref ref-type="bibr" rid="pone.0178423.ref017">17</xref>, <xref ref-type="bibr" rid="pone.0178423.ref019">19</xref>, <xref ref-type="bibr" rid="pone.0178423.ref020">20</xref>, <xref ref-type="bibr" rid="pone.0178423.ref021">21</xref>].</p>
<p>Importantly, studies vary regarding which emotions they have included, with most of them including only few emotion categories. In one of the most comprehensive studies in this respect, Liu et al. [<xref ref-type="bibr" rid="pone.0178423.ref019">19</xref>] compared the effects of anger, disgust, fear, happiness, sadness, and surprise expressions on facial identity recognition by changing expressions in the retrieval phase. When the same expression was shown at both encoding and recognition, memory for happy expressions was better than for disgusted expressions, but not significantly different from the other expressions. Although they included a wider selection of emotions than prior studies, Liu et al. [<xref ref-type="bibr" rid="pone.0178423.ref019">19</xref>] did not examine recognition of neutral expressions, making it hard to draw conclusions about whether emotional faces show an advantage compared to non-emotional faces. We argue that it is essential that studies on recognition memory include several emotion categories as well as neutral expressions in order to elucidate which emotion expressions, if any, enhance accuracy.</p>
<p>There are only a couple of studies on memory for vocal emotion expressions. Armony, Chochol, Fecteau, and Belin [<xref ref-type="bibr" rid="pone.0178423.ref024">24</xref>] as well as Aubé, Peretz, and Armony [<xref ref-type="bibr" rid="pone.0178423.ref025">25</xref>] reported that unfamiliar emotional non-linguistic vocalizations (e.g., screams, laugher, cries) conveying fear, happiness, and sadness were better remembered than neutral vocalizations. Memory for voices thus seems to depend on the emotions that are being expressed, but just as for faces, the effect of individual emotions remains largely an open question.</p>
</sec>
<sec id="sec004">
<title>Recollection and familiarity processes</title>
<p>Studies have recently started to investigate the states of awareness associated with recognition of faces and voices. By using the Remember/Know procedure [<xref ref-type="bibr" rid="pone.0178423.ref026">26</xref>], it is possible to study if own-sex bias and memory for emotional expressions are influenced by recollection or familiarity processes. Remember responses are accompanied by recollection of details or specific contextual information (e.g., thoughts, feelings, and associations) about the stimuli that have been presented and are therefore related to episodic memory. In contrast, know responses are associated with feelings of familiarity about the stimuli, in the absence of retrieval of contextual information about the prior experience [<xref ref-type="bibr" rid="pone.0178423.ref026">26</xref>].</p>
<p>Findings indicate that emotions may affect remember and know judgments in different ways. More specifically, it has been suggested that emotions may facilitate the subjective sense of recollection (e.g. [<xref ref-type="bibr" rid="pone.0178423.ref027">27</xref>, <xref ref-type="bibr" rid="pone.0178423.ref028">28</xref>, <xref ref-type="bibr" rid="pone.0178423.ref029">29</xref>]). For example, Patel, Girard, and Green [<xref ref-type="bibr" rid="pone.0178423.ref030">30</xref>] reported that memory for angry, happy and fearful unfamiliar faces may depend more on recollection processes. In contrast, Johansson et al. [<xref ref-type="bibr" rid="pone.0178423.ref023">23</xref>] found that memory for happy and neutral faces instead may depend on familiarity while negative stimuli rely on recollection processes. The mechanisms behind why certain emotions are associated with recollection rather than familiarity processes still need to be addressed. Fearful and angry expressions are considered as highly arousing which may facilitate recollection processes [<xref ref-type="bibr" rid="pone.0178423.ref023">23</xref>, <xref ref-type="bibr" rid="pone.0178423.ref027">27</xref>], and although happy expressions can be considered as high in arousal too, they are probably not as relevant (or as aroused) as fearful or angry expressions and therefore classified as know responses. When participants are instructed to explicitly attend and process the displayed emotion, recollection rates for happiness improve [<xref ref-type="bibr" rid="pone.0178423.ref030">30</xref>]. Thus, recollection and familiarity processes may be influenced by both the emotional expression of the stimuli and the encoding strategy that is used.</p>
<p>Damjanovic and Hanley [<xref ref-type="bibr" rid="pone.0178423.ref031">31</xref>] assessed to what extent recognition of emotionally neutral famous faces and voices was accompanied by either recollection of specific episodes or a sense of familiarity without conscious recollection. It was easier to recall episodic information from faces than from voices, and voices were more associated with familiarity ratings compared to faces. These results held true even when participants were exposed to personally familiar faces and voices [<xref ref-type="bibr" rid="pone.0178423.ref032">32</xref>]. A possible explanation is that faces, which evoked more remember responses, may be related to episodic memory; whereas voices provide less information since no details or contextual information were recalled during their presentation [<xref ref-type="bibr" rid="pone.0178423.ref031">31</xref>, <xref ref-type="bibr" rid="pone.0178423.ref032">32</xref>, <xref ref-type="bibr" rid="pone.0178423.ref033">33</xref>]. Damjanovic and Hanley [<xref ref-type="bibr" rid="pone.0178423.ref031">31</xref>] therefore proposed that episodic memory is more robustly linked to face recognition than to voice recognition.</p>
</sec>
<sec id="sec005">
<title>Study rationale</title>
<p>Daily interactions are characterized by meeting people of different sexes who often express different emotions through their facial and vocal behaviors, and we argue that this implies that research on social memory bias should expand beyond the use of neutral faces as objects of study. The current study therefore includes male and female items and participants, three presentation modalities (faces only, voices only, and face-voice combinations), and stimuli expressing a fairly large number of expressions (anger, disgust, fear, happiness, and sadness) as well as neutral items. This design allows us to build upon the literature reviewed above by investigating the effects of sex and emotion expression on memory for both faces and voices.</p>
<p>We aim to address three interrelated research questions. First, we investigate if own-sex bias in memory accuracy is exclusive for faces (and women) or if it can also be observed for voices and face-voice combinations (and men). Second, we investigate the effects of emotion expression on memory accuracy (and own-sex bias). Here, we are especially interested in which emotion expressions, if any, enhance or decrease memory accuracy, and if the patterns of results for specific emotions are similar for faces, voices, and face-voice combinations. Third, we use the Remember/Know paradigm to investigate if the subjective sense of recollection and familiarity varies as a function of sex and expression, and if they show evidence for own-sex bias. Based on previous studies (e.g., [<xref ref-type="bibr" rid="pone.0178423.ref002">2</xref>, <xref ref-type="bibr" rid="pone.0178423.ref004">4</xref>]), we expect to replicate findings of female own-sex bias for faces. For the other research questions, our analyses provide novel data on issues that have not been previously addressed or for which previous studies provide inconclusive results.</p>
</sec>
</sec>
<sec id="sec006" sec-type="materials|methods">
<title>Methods</title>
<sec id="sec007">
<title>Participants</title>
<p>A total of 600 young adults were recruited from the Stockholm area through university bulletin boards and designated websites. All participants were right-handed, fluent in Swedish, and reported being in good health with no previous or current substance abuse or psychiatric medication. One participant withdrew from the study, and data from 3 participants was lost due to equipment failure, leading to a total of 596 participants (226 men, mean age = 23.4 years, <italic>SD</italic> = 3.16, range 18–34; 367 women, mean age = 22.9, <italic>SD</italic> = 3.16, range = 18–34). Data on age and sex was not available for 3 participants; data from these individuals were included in all analyses, except those involving sex as a variable of interest. Participants received movie vouchers or course credits in exchange for their participation. Written consent was obtained from all participants prior to data collection, and the study was approved by the Stockholm Area Regional Ethical Review Board. Data was collected in the framework of a larger project investigating genetic and neural correlates of emotional abilities, and the sample size was determined a-priori based on considerations of expected strength of phenotype-genotype correlations.</p>
</sec>
<sec id="sec008" sec-type="materials|methods">
<title>Materials</title>
<sec id="sec009">
<title>Face and voice stimuli</title>
<p>Face stimuli were retrieved from the FACES database [<xref ref-type="bibr" rid="pone.0178423.ref034">34</xref>], which contains color photos of young and middle-aged adults who portray various emotions. All photos were portraits of faces (335 x 419 pixels) including hair, with head and gaze oriented forward, and all expressers wore similar clothes. The voice stimuli were taken from the VENEC corpus [<xref ref-type="bibr" rid="pone.0178423.ref035">35</xref>], which contains recordings of young adult actors expressing various emotions by way of non-linguistic vocalizations. The vocalizations consisted of various human sounds (e.g., crying, laughter, shrieks) and non-linguistic interjections (e.g., “ah”, “hm”, “oh”), but contained no actual words. The amplitude of the voice stimuli was peak normalized using <italic>Adobe Audition</italic> software (Adobe Systems Inc., San Jose, CA, USA) to control for differences in recording level. The duration of voice stimuli varied between 1 to 4 seconds.</p>
<p>Twenty-four stimuli expressing anger, disgust, fear, happiness, neutrality and sadness were shown at encoding (4 stimuli/expression) for each presentation modality (faces only, voices only, and face-voice combinations). During the recognition memory task, the 24 previously encountered stimuli were interspersed with 24 new stimuli–resulting in a total of 48 stimuli per presentation modality. The selection criteria for both face and voice stimuli were (a) that the intended emotion of the item was recognized with high accuracy in previous studies [<xref ref-type="bibr" rid="pone.0178423.ref034">34</xref>, <xref ref-type="bibr" rid="pone.0178423.ref035">35</xref>], (b) that the item pool would include equal numbers of female and male expressers for each emotion and presentation modality, and (c) that each expresser identity would occur only once for each presentation modality. Face stimuli further contained equal proportions of young and middle-aged expressers for each condition. For the face-voice combinations, the face and the voice stimuli were presented simultaneously, and were matched in terms of expression and sex. The names of all stimulus items are available in the online supplemental <xref ref-type="supplementary-material" rid="pone.0178423.s001">S1 Table</xref> (for replication purposes).</p>
</sec>
</sec>
<sec id="sec010">
<title>Procedure</title>
<p>Testing was conducted individually using <italic>MediaLab</italic> software (Jarvis, 2010) to present stimuli and collect responses. Face stimuli were presented on 22-inch LED computer screens and participants listened to the voice stimuli at comfortable volume levels through high-quality headphones, with sound level kept constant across participants. All participants first took part in the encoding and recognition memory tasks (described below) and then participated in additional tasks (an emotion recognition task and a questionnaire battery focusing on socio-emotional abilities and functioning) not related to the current study. In addition, all participants provided saliva samples for genetic analyses. Results on the genetic correlates of social memory are reported elsewhere [<xref ref-type="bibr" rid="pone.0178423.ref036">36</xref>, <xref ref-type="bibr" rid="pone.0178423.ref037">37</xref>].</p>
<sec id="sec011">
<title>Encoding task</title>
<p>Participants were not told that they would undergo a memory task because we were interested in incidental learning. Instead they were instructed to judge the expression of faces, voices, and face-voice combinations in a forced-choice task, by clicking (using their dominant hand) on the label which best represented the expression conveyed by each stimulus. The alternatives they could choose among were the same as the intended expressions (anger, disgust, fear, happiness, neutral, and sadness), and a response was scored as correct if it matched the intended expression of the stimulus. Participants did not receive any feedback regarding their responses while performing the expression identification task. After the participants had finished the face encoding task, they similarly judged the expression of the voice stimuli which were then followed by the face-voice combinations. The order of stimuli was randomized within each encoding task (but all participants saw/heard exactly the same stimuli). The order of encoding tasks (i.e., faces, voices, and face-voice combinations) was, however, not randomized because we were not interested in comparing modalities in the current study. In this way, we could also keep possible order effects constant for the genetic association studies mentioned above [<xref ref-type="bibr" rid="pone.0178423.ref036">36</xref>, <xref ref-type="bibr" rid="pone.0178423.ref037">37</xref>].</p>
<p>Participants were instructed to make their judgments as quickly and accurately as possible, but there was no time limit on the judgments (the next stimulus appeared immediately after the participants made their judgments). Stimuli in the face only modality remained on the screen until the participants made their judgments, which usually took approximately 3–5 seconds. Participants were allowed to repeat the playback of voice stimuli as many times as needed to reach a decision, to compensate for the brief duration (1–4 seconds) of the vocalizations. However, in the face-voice combinations, the vocalizations were only played once and could not be repeated. Here, the faces and the voices had the same onset (they appeared simultaneously), but the offset differed because the face remained on the screen until the participants chose an answer. Individual response times were not recorded, but the average time required for the encoding tasks was between 8–10 minutes (approximately 3 min/task). <xref ref-type="table" rid="pone.0178423.t001">Table 1</xref> shows the emotion recognition rates (proportion of correct responses), which were high and ranged from 0.79 to 0.99. This means that accuracy was around 4.7 to 5.9 times higher than the proportion expected by chance guessing (the chance recognition rate in a 6-alternative forced-choice task is 0.16).</p>
<table-wrap id="pone.0178423.t001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0178423.t001</object-id>
<label>Table 1</label> <caption><title>Means (and standard deviations) for emotion recognition rates (proportion of correct responses) in the encoding task.</title></caption>
<alternatives>
<graphic id="pone.0178423.t001g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0178423.t001" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="justify">Modality</th>
<th align="justify">Anger</th>
<th align="justify">Disgust</th>
<th align="justify">Fear</th>
<th align="justify">Happiness</th>
<th align="justify">Neutral</th>
<th align="justify">Sadness</th>
<th align="justify">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td align="justify" style="background-color:#F2F2F2">Faces</td>
<td align="justify" style="background-color:#F2F2F2">0.95(0.12)</td>
<td align="justify" style="background-color:#F2F2F2">0.99(0.056)</td>
<td align="justify" style="background-color:#F2F2F2">0.93(0.15)</td>
<td align="justify" style="background-color:#F2F2F2">0.99(0.062)</td>
<td align="justify" style="background-color:#F2F2F2">0.83(0.22)</td>
<td align="justify" style="background-color:#F2F2F2">0.96(0.11)</td>
<td align="justify" style="background-color:#F2F2F2">0.94(0.06)</td>
</tr>
<tr>
<td align="justify">Voices</td>
<td align="justify">0.96(0.10)</td>
<td align="justify">0.94(0.12)</td>
<td align="justify">0.90(0.16)</td>
<td align="justify">0.89(0.18)</td>
<td align="justify">0.79(0.25)</td>
<td align="justify">0.93(0.14)</td>
<td align="justify">0.90(0.08)</td>
</tr>
<tr>
<td align="justify" style="background-color:#F2F2F2">Face-voice</td>
<td align="justify" style="background-color:#F2F2F2">0.99(0.05)</td>
<td align="justify" style="background-color:#F2F2F2">0.99(0.01)</td>
<td align="justify" style="background-color:#F2F2F2">0.99(0.05)</td>
<td align="justify" style="background-color:#F2F2F2">0.99(0.05)</td>
<td align="justify" style="background-color:#F2F2F2">0.91(0.17)</td>
<td align="justify" style="background-color:#F2F2F2">0.95(0.11)</td>
<td align="justify" style="background-color:#F2F2F2">0.97(0.04)</td>
</tr>
<tr>
<td align="justify"><bold>Total</bold></td>
<td align="justify">0.97(0.06)</td>
<td align="justify">0.98(0.04)</td>
<td align="justify">0.94(0.09)</td>
<td align="justify">0.96(0.07)</td>
<td align="justify">0.84(0.16)</td>
<td align="justify">0.94(0.08)</td>
<td align="justify">0.94(0.05)</td>
</tr>
</tbody>
</table>
</alternatives>
</table-wrap>
</sec>
<sec id="sec012">
<title>Recognition memory test</title>
<p>The three encoding tasks were directly followed by an equal number of surprise recognition memory tests, in which the participants were presented with the 24 old items from the encoding task interspersed with 24 new stimuli (the identities of which had not been previously encountered in the encoding tasks). The recognition tests were presented in the same fixed order as the encoding tasks (i.e., the face-only condition first, followed by the voice-only and the combined face-voice conditions), and stimuli were presented in random order across subjects within each block. The procedure was similar for all modalities and made use of the Remember/Know paradigm [<xref ref-type="bibr" rid="pone.0178423.ref026">26</xref>].</p>
<p>For each item, the participants were instructed to answer the following question with one of the response options “yes, remember”, “yes, know”, or “no”: “<italic>Have you seen this face before</italic>?” (for faces), “<italic>Have you heard this voice before</italic>?” (for voices), and “<italic>Has this person been present earlier in the experiment</italic>?” (for face-voice combinations). The following instructions were given prior to the memory task: “You will now be presented with a number of faces/voices/face-voice combinations. Some of these have been presented in the earlier task, whereas others are new. Try to remember if you have seen/heard the faces/voices before. If the face/voice/person induces a memory of something that you experienced (e.g., associations, thought, or feelings) at the time you first saw/heard the face/voice/person you should use the response ‘<italic>yes</italic>, <italic>remember’</italic>. If you instead think that the face/voice/person feels familiar, but you cannot remember any details from the last time you saw/heard the face/voice/person, then you should use the response ‘<italic>yes</italic>, <italic>know</italic>’. Finally, if you think that you have not seen/heard the face/voice/person before, you should use the response ‘<italic>no</italic>’.”</p>
<p>Participants made their judgments by clicking the appropriate box on the computer screen using a mouse. Similar to the encoding tasks, there were no time limits and participants were allowed to listen to the voice stimuli as many times as required to reach a decision. Individual response times were not recorded, but the average time required for the three memory tests was between 10–12 minutes (3.5 min/task).</p>
</sec>
</sec>
</sec>
<sec id="sec013" sec-type="results">
<title>Results</title>
<sec id="sec014">
<title>Recognition accuracy</title>
<p>We used the discrimination index (<italic>P</italic><sub>r</sub>) as our measure of recognition accuracy, following Snodgrass and Corvin [<xref ref-type="bibr" rid="pone.0178423.ref038">38</xref>]. <italic>P</italic><sub>r</sub> was calculated as the overall hit rate (collapsed across both remember and know responses) minus the false alarm rate.</p>
<p>To test for own-sex bias, we investigated the effects of item sex and participant sex on memory accuracy. Three separate mixed ANOVAs with participant sex as a between-groups variable, and item sex and expression (anger, disgust, fear, happiness, neutral, and sadness) as repeated measures, were conducted for the <italic>P</italic><sub>r</sub> rates for faces, voices, and face-voice-combinations. Main effects of expression were significant for all presentation modalities, but the main effect of participant sex was not significant for any modality, which suggests that male and female participants overall performed on a similar level on the memory tasks, and no 3-way interactions were significant. <xref ref-type="table" rid="pone.0178423.t002">Table 2</xref> shows how overall <italic>P</italic><sub>r</sub> rates (for each presentation modality) vary as function of expression for faces, voices, and face-voice combinations, which illustrates the main effects of emotion expression. For the sake of completeness, <italic>P</italic><sub>r</sub> rates for each condition (expression, item sex, and participant sex) are displayed in <xref ref-type="table" rid="pone.0178423.t003">Table 3</xref> separately for each presentation modality.</p>
<table-wrap id="pone.0178423.t002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0178423.t002</object-id>
<label>Table 2</label> <caption><title>Means (and standard deviations) for overall accuracy (<italic>P</italic><sub><italic>r</italic></sub>), remember hits (R), and know hits (K) for each emotion expression and presentation modality.</title></caption>
<alternatives>
<graphic id="pone.0178423.t002g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0178423.t002" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="left">Modality</th>
<th align="left">Index</th>
<th align="center">Anger</th>
<th align="center">Disgust</th>
<th align="center">Fear</th>
<th align="center">Happiness</th>
<th align="center">Neutral</th>
<th align="center">Sadness</th>
</tr>
</thead>
<tbody>
<tr>
<td align="justify" style="background-color:#F2F2F2">Faces</td>
<td align="justify" style="background-color:#F2F2F2"><italic>P</italic><sub>r</sub></td>
<td align="justify" style="background-color:#F2F2F2">0.66(0.26)</td>
<td align="justify" style="background-color:#F2F2F2">0.52(0.31)</td>
<td align="justify" style="background-color:#F2F2F2">0.68(0.27)</td>
<td align="justify" style="background-color:#F2F2F2">0.66(0.28)</td>
<td align="justify" style="background-color:#F2F2F2"><bold>0.73(0.26)<xref ref-type="table-fn" rid="t002fn002">***</xref></bold></td>
<td align="justify" style="background-color:#F2F2F2">0.56(0.28)</td>
</tr>
<tr>
<td align="justify">Voices</td>
<td align="justify"><italic>P</italic><sub>r</sub></td>
<td align="justify">0.27(0.31)</td>
<td align="justify">0.28(0.29)</td>
<td align="justify">0.31(0.32)</td>
<td align="justify">0.43(0.33)</td>
<td align="justify"><bold>0.53(0.31)<xref ref-type="table-fn" rid="t002fn002">***</xref></bold></td>
<td align="justify">0.30(0.30)</td>
</tr>
<tr>
<td align="justify" style="background-color:#F2F2F2">Face-Voice</td>
<td align="justify" style="background-color:#F2F2F2"><italic>P</italic><sub>r</sub></td>
<td align="justify" style="background-color:#F2F2F2">0.22(0.33)</td>
<td align="justify" style="background-color:#F2F2F2">0.38(0.32)</td>
<td align="justify" style="background-color:#F2F2F2">0.48(0.32)</td>
<td align="justify" style="background-color:#F2F2F2">0.36(0.33)</td>
<td align="justify" style="background-color:#F2F2F2"><bold>0.54(0.28)<xref ref-type="table-fn" rid="t002fn004">*</xref></bold></td>
<td align="justify" style="background-color:#F2F2F2">0.42(0.31)</td>
</tr>
<tr>
<td align="left">Faces</td>
<td align="left">R</td>
<td align="justify">0.55(0.30)</td>
<td align="justify">0.50(0.31)</td>
<td align="justify">0.55(0.31)</td>
<td align="justify">0.52(0.33)</td>
<td align="justify"><bold>0.61(0.33)<xref ref-type="table-fn" rid="t002fn002">***</xref></bold></td>
<td align="justify">0.43(0.29)</td>
</tr>
<tr>
<td align="left" style="background-color:#F2F2F2">Voices</td>
<td align="left" style="background-color:#F2F2F2">R</td>
<td align="justify" style="background-color:#F2F2F2"><bold>0.53(0.32)<xref ref-type="table-fn" rid="t002fn002">***</xref></bold></td>
<td align="justify" style="background-color:#F2F2F2">0.45(0.28)</td>
<td align="justify" style="background-color:#F2F2F2">0.48(0.29)</td>
<td align="justify" style="background-color:#F2F2F2">0.43(0.28)</td>
<td align="justify" style="background-color:#F2F2F2">0.38(0.31)</td>
<td align="justify" style="background-color:#F2F2F2">0.38(0.27)</td>
</tr>
<tr>
<td align="left">Face-Voice</td>
<td align="left">R</td>
<td align="justify">0.34(0.28)</td>
<td align="justify">0.32(0.28)</td>
<td align="justify"><bold>0.44(0.29)<xref ref-type="table-fn" rid="t002fn002">***</xref></bold></td>
<td align="justify">0.38(0.29)</td>
<td align="justify">0.34(0.28)</td>
<td align="justify">0.31(0.28)</td>
</tr>
<tr>
<td align="justify" style="background-color:#F2F2F2">Faces</td>
<td align="justify" style="background-color:#F2F2F2">K</td>
<td align="justify" style="background-color:#F2F2F2">0.28(0.26)</td>
<td align="justify" style="background-color:#F2F2F2">0.28(0.27)</td>
<td align="justify" style="background-color:#F2F2F2">0.30(0.27)</td>
<td align="justify" style="background-color:#F2F2F2">0.26(0.27)</td>
<td align="justify" style="background-color:#F2F2F2">0.25(0.29)</td>
<td align="justify" style="background-color:#F2F2F2"><bold>0.32(0.26)<xref ref-type="table-fn" rid="t002fn003">**</xref></bold></td>
</tr>
<tr>
<td align="justify">Voices</td>
<td align="justify">K</td>
<td align="justify">0.27(0.27)</td>
<td align="justify">0.28(0.25)</td>
<td align="justify">0.27(0.25)</td>
<td align="justify">0.30(0.25)</td>
<td align="justify"><bold>0.36(0.29)<xref ref-type="table-fn" rid="t002fn002">***</xref></bold></td>
<td align="justify">0.30(0.25)</td>
</tr>
<tr>
<td align="justify" style="background-color:#F2F2F2">Face-Voice</td>
<td align="justify" style="background-color:#F2F2F2">K</td>
<td align="justify" style="background-color:#F2F2F2">0.26(0.24)</td>
<td align="justify" style="background-color:#F2F2F2">0.31(0.26)</td>
<td align="justify" style="background-color:#F2F2F2">0.28(0.26)</td>
<td align="justify" style="background-color:#F2F2F2">0.26(0.23)</td>
<td align="justify" style="background-color:#F2F2F2">0.27(0.24)</td>
<td align="justify" style="background-color:#F2F2F2"><bold>0.32(0.27)<xref ref-type="table-fn" rid="t002fn004">*</xref></bold></td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t002fn001"><p><italic>Notes</italic>. Bold type indicates a value that was significantly different (Bonferroni tests) from the other values in the same row.</p></fn>
<fn id="t002fn002"><p>*** <italic>p</italic> &lt; .001</p></fn>
<fn id="t002fn003"><p>** <italic>p</italic> &lt; .01</p></fn>
<fn id="t002fn004"><p>* <italic>p</italic> &lt; .05</p></fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="pone.0178423.t003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0178423.t003</object-id>
<label>Table 3</label> <caption><title>Means (and standard deviations) for overall accuracy (<italic>P</italic><sub><italic>r</italic></sub>), remember hits (R), and know hits (K) presented separately for each condition (item sex, participant sex, and emotion expression) for each presentation modality.</title></caption>
<alternatives>
<graphic id="pone.0178423.t003g" mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0178423.t003" xlink:type="simple"/>
<table>
<colgroup>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
<col align="left" valign="middle"/>
</colgroup>
<thead>
<tr>
<th align="justify">Modality</th>
<th align="justify">Index</th>
<th align="justify">Participant and Item Sex</th>
<th align="justify">Anger</th>
<th align="justify">Disgust</th>
<th align="justify">Fear</th>
<th align="justify">Happiness</th>
<th align="justify">Neutral</th>
<th align="justify">Sadness</th>
</tr>
</thead>
<tbody>
<tr>
<td align="justify" rowspan="4" style="background-color:#F2F2F2">Faces</td>
<td align="justify" rowspan="4" style="background-color:#F2F2F2"><italic>P</italic><sub>r</sub></td>
<td align="justify" style="background-color:#F2F2F2">MM</td>
<td align="justify" style="background-color:#F2F2F2">0.65(0.35)</td>
<td align="justify" style="background-color:#F2F2F2">0.47(0.41)</td>
<td align="justify" style="background-color:#F2F2F2">0.62(0.38)</td>
<td align="justify" style="background-color:#F2F2F2">0.66(0.37)</td>
<td align="justify" style="background-color:#F2F2F2">0.69(0.38)</td>
<td align="justify" style="background-color:#F2F2F2">0.57(0.37)</td>
</tr>
<tr>
<td align="justify" style="background-color:#F2F2F2">MF</td>
<td align="justify" style="background-color:#F2F2F2">0.66(0.39)</td>
<td align="justify" style="background-color:#F2F2F2">0.58(0.40)</td>
<td align="justify" style="background-color:#F2F2F2">0.73(0.31)</td>
<td align="justify" style="background-color:#F2F2F2">0.66(0.37)</td>
<td align="justify" style="background-color:#F2F2F2">0.78(0.30)</td>
<td align="justify" style="background-color:#F2F2F2">0.54(0.40)</td>
</tr>
<tr>
<td align="justify" style="background-color:#F2F2F2">FM</td>
<td align="justify" style="background-color:#F2F2F2">0.61(0.37)</td>
<td align="justify" style="background-color:#F2F2F2">0.42(0.44)</td>
<td align="justify" style="background-color:#F2F2F2">0.63(0.39)</td>
<td align="justify" style="background-color:#F2F2F2">0.66(0.37)</td>
<td align="justify" style="background-color:#F2F2F2">0.67(0.35)</td>
<td align="justify" style="background-color:#F2F2F2">0.53(0.38)</td>
</tr>
<tr>
<td align="justify" style="background-color:#F2F2F2">FF</td>
<td align="justify" style="background-color:#F2F2F2">0.70(0.34)</td>
<td align="justify" style="background-color:#F2F2F2">0.61(0.39)</td>
<td align="justify" style="background-color:#F2F2F2">0.73(0.34)</td>
<td align="justify" style="background-color:#F2F2F2">0.64(0.37)</td>
<td align="justify" style="background-color:#F2F2F2">0.78(0.33)</td>
<td align="justify" style="background-color:#F2F2F2">0.59(0.39)</td>
</tr>
<tr>
<td align="justify" rowspan="4">Voices</td>
<td align="justify" rowspan="4"><italic>P</italic><sub>r</sub></td>
<td align="justify">MM</td>
<td align="justify">0.31(0.46)</td>
<td align="justify">0.18(0.39)</td>
<td align="justify">0.21(0.46)</td>
<td align="justify">0.47(0.41)</td>
<td align="justify">0.48(0.40)</td>
<td align="justify">0.38(0.41)</td>
</tr>
<tr>
<td align="justify">MF</td>
<td align="justify">0.26(0.40)</td>
<td align="justify">0.46(0.42)</td>
<td align="justify">0.41(0.44)</td>
<td align="justify">0.38(0.45)</td>
<td align="justify">0.57(0.43)</td>
<td align="justify">0.17(0.44)</td>
</tr>
<tr>
<td align="justify">FM</td>
<td align="justify">0.30(0.43)</td>
<td align="justify">0.07(0.41)</td>
<td align="justify">0.22(0.42)</td>
<td align="justify">0.46(0.41)</td>
<td align="justify">0.49(0.40)</td>
<td align="justify">0.46(0.39)</td>
</tr>
<tr>
<td align="justify">FF</td>
<td align="justify">0.23(0.44)</td>
<td align="justify">0.45(0.42)</td>
<td align="justify">0.39(0.46)</td>
<td align="justify">0.40(0.47)</td>
<td align="justify">0.58(0.43)</td>
<td align="justify">0.19(0.42)</td>
</tr>
<tr>
<td align="justify" rowspan="4" style="background-color:#F2F2F2">Faces-Voice</td>
<td align="justify" rowspan="4" style="background-color:#F2F2F2"><italic>P</italic><sub>r</sub></td>
<td align="justify" style="background-color:#F2F2F2">MM</td>
<td align="justify" style="background-color:#F2F2F2">0.25(0.45)</td>
<td align="justify" style="background-color:#F2F2F2">0.39(0.42)</td>
<td align="justify" style="background-color:#F2F2F2">0.48(0.44)</td>
<td align="justify" style="background-color:#F2F2F2">0.39(0.44)</td>
<td align="justify" style="background-color:#F2F2F2">0.50(0.40)</td>
<td align="justify" style="background-color:#F2F2F2">0.35(0.43)</td>
</tr>
<tr>
<td align="justify" style="background-color:#F2F2F2">MF</td>
<td align="justify" style="background-color:#F2F2F2">0.24(0.49)</td>
<td align="justify" style="background-color:#F2F2F2">0.35(0.43)</td>
<td align="justify" style="background-color:#F2F2F2">0.47(0.45)</td>
<td align="justify" style="background-color:#F2F2F2">0.32(0.47)</td>
<td align="justify" style="background-color:#F2F2F2">0.58(0.36)</td>
<td align="justify" style="background-color:#F2F2F2">0.43(0.40)</td>
</tr>
<tr>
<td align="justify" style="background-color:#F2F2F2">FM</td>
<td align="justify" style="background-color:#F2F2F2">0.17(0.44)</td>
<td align="justify" style="background-color:#F2F2F2">0.45(0.41)</td>
<td align="justify" style="background-color:#F2F2F2">0.47(0.42)</td>
<td align="justify" style="background-color:#F2F2F2">0.35(0.43)</td>
<td align="justify" style="background-color:#F2F2F2">0.46(0.40)</td>
<td align="justify" style="background-color:#F2F2F2">0.37(0.45)</td>
</tr>
<tr>
<td align="justify" style="background-color:#F2F2F2">FF</td>
<td align="justify" style="background-color:#F2F2F2">0.25(0.46)</td>
<td align="justify" style="background-color:#F2F2F2">0.34(0.46)</td>
<td align="justify" style="background-color:#F2F2F2">0.48(0.40)</td>
<td align="justify" style="background-color:#F2F2F2">0.38(0.46)</td>
<td align="justify" style="background-color:#F2F2F2">0.61(0.37)</td>
<td align="justify" style="background-color:#F2F2F2">0.52(0.40)</td>
</tr>
<tr>
<td align="justify" rowspan="4">Faces</td>
<td align="justify" rowspan="4">R</td>
<td align="justify">MM</td>
<td align="justify">0.51(0.38)</td>
<td align="justify">0.50(0.39)</td>
<td align="justify">0.46(0.39)</td>
<td align="justify">0.51(0.40)</td>
<td align="justify">0.61(0.40)</td>
<td align="justify">0.39(0.37)</td>
</tr>
<tr>
<td align="justify">MF</td>
<td align="justify">0.63(0.39)</td>
<td align="justify">0.52(0.40)</td>
<td align="justify">0.62(0.36)</td>
<td align="justify">0.50(0.39)</td>
<td align="justify">0.60(0.39)</td>
<td align="justify">0.44(0.39)</td>
</tr>
<tr>
<td align="justify">FM</td>
<td align="justify">0.45(0.37)</td>
<td align="justify">0.46(0.38)</td>
<td align="justify">0.48(0.39)</td>
<td align="justify">0.54(0.38)</td>
<td align="justify">0.60(0.39)</td>
<td align="justify">0.38(0.36)</td>
</tr>
<tr>
<td align="justify">FF</td>
<td align="justify">0.64(0.36)</td>
<td align="justify">0.54(0.39)</td>
<td align="justify">0.63(0.36)</td>
<td align="justify">0.51(0.39)</td>
<td align="justify">0.64(0.38)</td>
<td align="justify">0.49(0.36)</td>
</tr>
<tr>
<td align="justify" rowspan="4" style="background-color:#F2F2F2">Voices</td>
<td align="justify" rowspan="4" style="background-color:#F2F2F2">R</td>
<td align="justify" style="background-color:#F2F2F2">MM</td>
<td align="justify" style="background-color:#F2F2F2">0.50(0.37)</td>
<td align="justify" style="background-color:#F2F2F2">0.42(0.35)</td>
<td align="justify" style="background-color:#F2F2F2">0.41(0.36)</td>
<td align="justify" style="background-color:#F2F2F2">0.45(0.35)</td>
<td align="justify" style="background-color:#F2F2F2">0.39(0.37)</td>
<td align="justify" style="background-color:#F2F2F2">0.37(0.36)</td>
</tr>
<tr>
<td align="justify" style="background-color:#F2F2F2">MF</td>
<td align="justify" style="background-color:#F2F2F2">0.59(0.38)</td>
<td align="justify" style="background-color:#F2F2F2">0.49(0.38)</td>
<td align="justify" style="background-color:#F2F2F2">0.50(0.35)</td>
<td align="justify" style="background-color:#F2F2F2">0.42(0.37)</td>
<td align="justify" style="background-color:#F2F2F2">0.43(0.39)</td>
<td align="justify" style="background-color:#F2F2F2">0.38(0.33)</td>
</tr>
<tr>
<td align="justify" style="background-color:#F2F2F2">FM</td>
<td align="justify" style="background-color:#F2F2F2">0.46(0.40)</td>
<td align="justify" style="background-color:#F2F2F2">0.39(0.34)</td>
<td align="justify" style="background-color:#F2F2F2">0.45(0.37)</td>
<td align="justify" style="background-color:#F2F2F2">0.41(0.35)</td>
<td align="justify" style="background-color:#F2F2F2">0.35(0.35)</td>
<td align="justify" style="background-color:#F2F2F2">0.38(0.37)</td>
</tr>
<tr>
<td align="justify" style="background-color:#F2F2F2">FF</td>
<td align="justify" style="background-color:#F2F2F2">0.59(0.39)</td>
<td align="justify" style="background-color:#F2F2F2">0.48(0.36)</td>
<td align="justify" style="background-color:#F2F2F2">0.53(0.38)</td>
<td align="justify" style="background-color:#F2F2F2">0.43(0.37)</td>
<td align="justify" style="background-color:#F2F2F2">0.38(0.38)</td>
<td align="justify" style="background-color:#F2F2F2">0.37(0.33)</td>
</tr>
<tr>
<td align="justify" rowspan="4">Faces-Voice</td>
<td align="justify" rowspan="4">R</td>
<td align="justify">MM</td>
<td align="justify">0.35(0.37)</td>
<td align="justify">0.34(0.37)</td>
<td align="justify">0.43(0.39)</td>
<td align="justify">0.37(0.38)</td>
<td align="justify">0.35(0.37)</td>
<td align="justify">0.31(0.35)</td>
</tr>
<tr>
<td align="justify">MF</td>
<td align="justify">0.39(0.35)</td>
<td align="justify">0.31(0.35)</td>
<td align="justify">0.43(0.38)</td>
<td align="justify">0.41(0.38)</td>
<td align="justify">0.35(0.36)</td>
<td align="justify">0.31(0.35)</td>
</tr>
<tr>
<td align="justify">FM</td>
<td align="justify">0.27(0.34)</td>
<td align="justify">0.36(0.38)</td>
<td align="justify">0.41(0.36)</td>
<td align="justify">0.32(0.36)</td>
<td align="justify">0.29(0.35)</td>
<td align="justify">0.28(0.32)</td>
</tr>
<tr>
<td align="justify">FF</td>
<td align="justify">0.37(0.35)</td>
<td align="justify">0.26(0.34)</td>
<td align="justify">0.47(0.38)</td>
<td align="justify">0.44(0.37)</td>
<td align="justify">0.37(0.35)</td>
<td align="justify">0.33(0.35)</td>
</tr>
<tr>
<td align="justify" rowspan="4" style="background-color:#F2F2F2">Faces</td>
<td align="justify" rowspan="4" style="background-color:#F2F2F2">K</td>
<td align="justify" style="background-color:#F2F2F2">MM</td>
<td align="justify" style="background-color:#F2F2F2">0.27(0.34)</td>
<td align="justify" style="background-color:#F2F2F2">0.25(0.34)</td>
<td align="justify" style="background-color:#F2F2F2">0.36(0.36)</td>
<td align="justify" style="background-color:#F2F2F2">0.25(0.36)</td>
<td align="justify" style="background-color:#F2F2F2">0.26(0.37)</td>
<td align="justify" style="background-color:#F2F2F2">0.31(0.34)</td>
</tr>
<tr>
<td align="justify" style="background-color:#F2F2F2">MF</td>
<td align="justify" style="background-color:#F2F2F2">0.27(0.36)</td>
<td align="justify" style="background-color:#F2F2F2">0.26(0.34)</td>
<td align="justify" style="background-color:#F2F2F2">0.27(0.34)</td>
<td align="justify" style="background-color:#F2F2F2">0.29(0.34)</td>
<td align="justify" style="background-color:#F2F2F2">0.25(0.34)</td>
<td align="justify" style="background-color:#F2F2F2">0.32(0.34)</td>
</tr>
<tr>
<td align="justify" style="background-color:#F2F2F2">FM</td>
<td align="justify" style="background-color:#F2F2F2">0.30(0.33)</td>
<td align="justify" style="background-color:#F2F2F2">0.30(0.35)</td>
<td align="justify" style="background-color:#F2F2F2">0.33(0.35)</td>
<td align="justify" style="background-color:#F2F2F2">0.24(0.31)</td>
<td align="justify" style="background-color:#F2F2F2">0.28(0.36)</td>
<td align="justify" style="background-color:#F2F2F2">0.33(0.34)</td>
</tr>
<tr>
<td align="justify" style="background-color:#F2F2F2">FF</td>
<td align="justify" style="background-color:#F2F2F2">0.27(0.33)</td>
<td align="justify" style="background-color:#F2F2F2">0.30(0.34)</td>
<td align="justify" style="background-color:#F2F2F2">0.26(0.34)</td>
<td align="justify" style="background-color:#F2F2F2">0.27(0.32)</td>
<td align="justify" style="background-color:#F2F2F2">0.22(0.33)</td>
<td align="justify" style="background-color:#F2F2F2">0.30(0.33)</td>
</tr>
<tr>
<td align="justify" rowspan="4">Voices</td>
<td align="justify" rowspan="4">K</td>
<td align="justify">MM</td>
<td align="justify">0.26(0.34)</td>
<td align="justify">0.26(0.31)</td>
<td align="justify">0.26(0.33)</td>
<td align="justify">0.27(0.31)</td>
<td align="justify">0.32(0.37)</td>
<td align="justify">0.26(0.32)</td>
</tr>
<tr>
<td align="justify">MF</td>
<td align="justify">0.26(0.34)</td>
<td align="justify">0.28(0.36)</td>
<td align="justify">0.27(0.33)</td>
<td align="justify">0.28(0.32)</td>
<td align="justify">0.32(0.36)</td>
<td align="justify">0.27(0.31)</td>
</tr>
<tr>
<td align="justify">FM</td>
<td align="justify">0.31(0.34)</td>
<td align="justify">0.28(0.32)</td>
<td align="justify">0.29(0.33)</td>
<td align="justify">0.34(0.34)</td>
<td align="justify">0.36(0.36)</td>
<td align="justify">0.33(0.35)</td>
</tr>
<tr>
<td align="justify">FF</td>
<td align="justify">0.26(0.34)</td>
<td align="justify">0.28(0.32)</td>
<td align="justify">0.26(0.33)</td>
<td align="justify">0.29(0.33)</td>
<td align="justify">0.40(0.37)</td>
<td align="justify">0.30(0.31)</td>
</tr>
<tr>
<td align="justify" rowspan="4" style="background-color:#F2F2F2">Faces-Voice</td>
<td align="justify" rowspan="4" style="background-color:#F2F2F2">K</td>
<td align="justify" style="background-color:#F2F2F2">MM</td>
<td align="justify" style="background-color:#F2F2F2">0.26(0.32)</td>
<td align="justify" style="background-color:#F2F2F2">0.30(0.32)</td>
<td align="justify" style="background-color:#F2F2F2">0.31(0.36)</td>
<td align="justify" style="background-color:#F2F2F2">0.24(0.30)</td>
<td align="justify" style="background-color:#F2F2F2">0.25(0.33)</td>
<td align="justify" style="background-color:#F2F2F2">0.28(0.35)</td>
</tr>
<tr>
<td align="justify" style="background-color:#F2F2F2">MF</td>
<td align="justify" style="background-color:#F2F2F2">0.28(0.34)</td>
<td align="justify" style="background-color:#F2F2F2">0.28(0.35)</td>
<td align="justify" style="background-color:#F2F2F2">0.28(0.33)</td>
<td align="justify" style="background-color:#F2F2F2">0.27(0.34)</td>
<td align="justify" style="background-color:#F2F2F2">0.27(0.34)</td>
<td align="justify" style="background-color:#F2F2F2">0.28(0.33)</td>
</tr>
<tr>
<td align="justify" style="background-color:#F2F2F2">FM</td>
<td align="justify" style="background-color:#F2F2F2">0.27(0.33)</td>
<td align="justify" style="background-color:#F2F2F2">0.35(0.35)</td>
<td align="justify" style="background-color:#F2F2F2">0.32(0.33)</td>
<td align="justify" style="background-color:#F2F2F2">0.25(0.31)</td>
<td align="justify" style="background-color:#F2F2F2">0.29(0.32)</td>
<td align="justify" style="background-color:#F2F2F2">0.35(0.35)</td>
</tr>
<tr>
<td align="justify" style="background-color:#F2F2F2">FF</td>
<td align="justify" style="background-color:#F2F2F2">0.25(0.32)</td>
<td align="justify" style="background-color:#F2F2F2">0.30(0.33)</td>
<td align="justify" style="background-color:#F2F2F2">0.24(0.31)</td>
<td align="justify" style="background-color:#F2F2F2">0.29(0.31)</td>
<td align="justify" style="background-color:#F2F2F2">0.28(0.32)</td>
<td align="justify" style="background-color:#F2F2F2">0.33(0.35)</td>
</tr>
</tbody>
</table>
</alternatives>
<table-wrap-foot>
<fn id="t003fn001"><p><italic>Notes</italic>. Male participants, Male items (MM); Male participants, Female items (MF); Female participants, Male items (FM); Female participants, Female items (FF).</p></fn>
</table-wrap-foot>
</table-wrap>
<p>For faces, we observed a significant main effect of expression, <italic>F</italic>(5, 2960) = 53.40, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .08. This main effect was further explored using post-hoc multiple comparisons (Bonferroni tests) to control for familywise error rates. Recognition was most accurate for neutral faces (<italic>P</italic><sub>r</sub> = .73), which were better recognized than fearful faces (.68), which in turn were better recognized than happy and angry faces (.66), which were better recognized than sad (.56) and disgust (.52) faces (Bonferroni tests, <italic>ps</italic> ≤ .002; for post-hoc tests, we report <italic>p</italic>-values corrected for all pairwise combinations throughout the paper). There was also a main effect of item sex, <italic>F</italic>(1, 592) = 61.43, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .09, demonstrating that female faces (<italic>P</italic><sub>r</sub> = .67) were more accurately recognized than male faces (.61). The item sex by expression interaction was also significant, <italic>F</italic>(5, 2960) = 8.54, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .01, and post-hoc Bonferroni tests revealed that female faces were better recognized than male faces for disgust, fear, and neutral expressions (<italic>ps</italic> &lt; .001). The effect of main interest was the participant sex by item sex interaction, <italic>F</italic>(1, 592) = 5.25, <italic>p</italic> = .022, η<sub>p</sub><sup>2</sup> = .01, which revealed that female participants displayed a memory bias for female faces (Bonferroni tests, <italic>p</italic>s &lt; .003; see <xref ref-type="fig" rid="pone.0178423.g001">Fig 1</xref>). Both men and women performed significantly better for female compared to male faces but the difference between accuracy for female and male items was larger for female participants (see <xref ref-type="fig" rid="pone.0178423.g001">Fig 1</xref>).</p>
<fig id="pone.0178423.g001" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0178423.g001</object-id>
<label>Fig 1</label>
<caption>
<title>Memory accuracy (<italic>P</italic><sub><italic>r</italic></sub>) as a function of item sex and participant sex for faces, voices and face-voice combinations.</title>
<p>The <italic>p</italic>-values indicate post-hoc pairwise comparisons (Bonferroni tests) comparing male and female participants’ accuracy for male and female items, respectively. Error bars represent 95% <italic>CI</italic>. ** <italic>p</italic> = .003, *** <italic>p</italic> &lt; .001.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0178423.g001" xlink:type="simple"/>
</fig>
<p>For voices, we also observed a significant main effect of expression <italic>F</italic>(5, 2960) = 64.61, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .10. Neutral voices (<italic>P</italic><sub>r</sub> = .53) were better recognized than all emotional voices (Bonferroni tests, <italic>p</italic>s &lt; .001), followed by happy voices (.43) which in turn were better recognized (<italic>p</italic>s &lt; .001) than the remaining voices. A significant main effect of item sex, <italic>F</italic>(1, 592) = 14.94, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .02, showed that female voices (<italic>P</italic><sub>r</sub> = .38) were better recognized than male voices (.34). The item sex by expression interaction was also significant <italic>F</italic>(5, 2960) = 68.57, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .10, and post-hoc Bonferroni tests revealed that female voices were better recognized than male voices for disgust, fear, and neutral expressions (<italic>p</italic>s &lt; .013), but sad male voices were better recognized than sad female voices (<italic>p</italic> &lt; .001). With regard to own-sex bias, the participant sex by item sex interaction was not significant for voices, <italic>F</italic>(1, 592) = 0.18, <italic>p</italic> = .672. As shown in <xref ref-type="fig" rid="pone.0178423.g001">Fig 1</xref>, both female and male participants overall performed better for female vs. male items.</p>
<p>Finally, for the face-voice combinations, the main effect of expression was significant <italic>F</italic>(5, 2960) = 73.81, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .11, where neutral stimuli (<italic>P</italic><sub>r</sub> = .54) were better recognized than all other expressions (Bonferroni tests, <italic>p</italic>s ≤. 005), followed by fearful, sad, disgusted and happy stimuli which in turn were better recognized than angry stimuli (<italic>ps</italic> &lt; .001). The main effect of item sex was also significant, <italic>F</italic>(1, 592) = 7.49, <italic>p</italic> = .006, η<sub>p</sub><sup>2</sup> = .01, and female items (<italic>P</italic><sub>r</sub> = .42) were better recognized than male items (.39). The item sex by expression interaction, <italic>F</italic>(5, 2960) = 8.54, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .01, showed that neutral and sad female face-voice combinations were better recognized than male ones (Bonferroni tests, <italic>ps</italic> &lt; .001). The participant sex by item sex interaction was also significant, <italic>F</italic> (1, 592) = 5.51, <italic>p</italic> = .019, η<sub>p</sub><sup>2</sup> = .01, and again revealed own-sex bias for female participants. As shown in <xref ref-type="fig" rid="pone.0178423.g001">Fig 1</xref>, only female participants displayed significantly better accuracy for female compared to male face-voice combinations (Bonferroni test, <italic>p</italic> &lt; .001).</p>
</sec>
<sec id="sec015">
<title>Remember/Know responses</title>
<p>We tested for own-sex bias in participants’ subjective feelings of recollection (remember hits) and familiarity (know hits) in a similar way as we did for memory accuracy. Separate mixed ANOVAs were conducted, with participant sex as a between-groups variable and item sex and expression (6 levels) as repeated measures factors, for remember and know rates for faces, voices and face-voice-combinations. Main effects of expression were significant for both remember and know responses in all presentation modalities, the main effect of participant sex was significant only for know responses in the vocal modality (detailed below), and no 3-way interactions were significant. Hit rates were not corrected for false alarms in these analyses because we were primarily interested in the participants’ subjective feelings of recollection and familiarity, respectively. Remember and know rates for each presentation modality are shown in <xref ref-type="table" rid="pone.0178423.t002">Table 2</xref> (as a function of emotion expression) and <xref ref-type="table" rid="pone.0178423.t003">Table 3</xref> (as a function of expression, item sex, and participant sex).</p>
<sec id="sec016">
<title>Remember responses</title>
<p>For remember hits of faces, we observed significant main effects of both expression <italic>F</italic>(5, 2960) = 39.31, <italic>p</italic>&lt; .001, η<sub>p</sub><sup>2</sup> = .06, and item sex, <italic>F</italic>(1, 592) = 77.61, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .12. For the main effect of expression, post-hoc multiple comparisons (Bonferroni tests) revealed that neutral faces (remember hit rate, <italic>M</italic> = .62) evoked significantly more remember hits than did angry (.56), fearful (.55), happy (.52), and disgusted faces (.51), which in turn had higher recollection rates than sad faces (.43, all <italic>ps</italic> ≤ .001). The main effect of item sex suggests higher recollection rates for female (<italic>M</italic> = .57) compared to male faces (.50). The item sex by expression interaction was significant <italic>F</italic>(5, 2960) = 14.53, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .02, and post-hoc analyses showed that female faces had significantly higher recollection rates than male faces for anger, fear, and sadness (Bonferroni tests, <italic>ps</italic> &lt; .001). The participant sex by item sex interaction effect was also significant, <italic>F</italic>(1, 592) = 5.25, <italic>p</italic> = .022, η<sub>p</sub><sup>2</sup> = .01, and demonstrates an own-sex bias for female participants as illustrated in <xref ref-type="fig" rid="pone.0178423.g002">Fig 2</xref>. Both male and female participants showed higher remember rates for female vs. male items, but the difference between female and male items was larger for female participants (Bonferroni test, <italic>p</italic> &lt; .001).</p>
<fig id="pone.0178423.g002" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0178423.g002</object-id>
<label>Fig 2</label>
<caption>
<title>Recollection hit rates as a function of item sex and participant sex for faces, voices and face-voice combinations.</title>
<p>The <italic>p</italic>-values indicate post-hoc pairwise comparisons (Bonferroni tests) comparing male and female participants’ recollection rates for male and female items, respectively. Error bars represent 95% <italic>CI</italic>. *** <italic>p</italic> &lt; .001.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0178423.g002" xlink:type="simple"/>
</fig>
<p>For the recollection of voices, the main effect of expression was significant <italic>F</italic>(5, 2960) = 33.27, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .05. Post-hoc Bonferroni tests (all <italic>ps</italic> &lt; .001) indicated that recollection of angry voices (remember hit rate, <italic>M</italic> = .54) was higher than for all other expressions, and that fearful (.48) and disgusted (.46) voices evoked significantly more remember hits than did happy (.44), neutral (.39) or sad (.38) vocalizations. The main effect of item sex was significant, <italic>F</italic>(1, 592) = 41.64, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .07, demonstrating that the remember rates for female voices (<italic>M</italic> = .47) were higher than the rates for male voices (.42). The item sex by expression interaction <italic>F</italic>(5, 2960) = 6.71, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .01, revealed that angry, disgusted and fearful female voices were better recollected than the corresponding male voices (Bonferroni tests, <italic>ps</italic> ≤ .001). However, the participant sex by item sex interaction was not significant here, <italic>F</italic>(1, 592) = 0.22, <italic>p</italic> = .640. Overall, both female and male participants showed higher rates for female items than for male items, see <xref ref-type="fig" rid="pone.0178423.g002">Fig 2</xref>.</p>
<p>For face-voice combinations, there was a significant main effect of expression <italic>F</italic>(5, 2960) = 24.31, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .04, which showed that fearful expressions (remember hit rate, <italic>M</italic> = .44) received higher recollection rates than all other expressions, followed by happy expressions (.39) which in turn received higher rates than all remaining expressions except anger (.35; Bonferroni tests, <italic>ps ≤</italic> .004). The main effect of item sex was also significant, <italic>F</italic>(1, 592) = 14.51, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .02, where female items (<italic>M</italic> = .38) were better recollected than male items (.35). The item sex by expression interaction was significant, <italic>F</italic>(5, 2960) = 7.70, <italic>p</italic> &lt; .001. η<sub>p</sub><sup>2</sup> = .01. Post-hoc Bonferroni tests demonstrated that angry and happy female face-voice combinations showed higher recollection rates versus male items (<italic>p</italic>s &lt; .001), whereas for disgust, recollection of male items was higher than for female items (<italic>p =</italic> .005). Here the participant sex by item sex interaction was also significant, <italic>F</italic>(1, 592) = 6.06, <italic>p</italic> = .014, η<sub>p</sub><sup>2</sup> = .01, and demonstrated that female, but not male, participants displayed a memory bias for female face-voice combinations (Bonferroni test, <italic>p</italic> &lt; .001; see <xref ref-type="fig" rid="pone.0178423.g002">Fig 2</xref>).</p>
</sec>
<sec id="sec017">
<title>Know responses</title>
<p>Regarding know responses for faces, we observed a significant main effect of expression, <italic>F</italic>(5, 2960) = 7.03, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .01. Sad expressions received the highest familiarity rates (know hit rate, <italic>M</italic> = .32), followed by fearful (.31), disgusted and angry (.29), happy (.27) and neutral (.26) faces. Post-hoc Bonferroni tests showed that sad faces had higher familiarity rates than happy and neutral expressions (<italic>ps</italic> ≤ .002). There was a main effect of item sex <italic>F</italic>(1, 592) = 4.19, <italic>p</italic> = .041, η<sub>p</sub><sup>2</sup> = .01, revealing that male faces (<italic>M</italic> = .30) were overall more familiar than female faces (.28). The item sex by expression interaction was also significant, <italic>F</italic>(5, 2960) = 4.39, <italic>p</italic> &lt; .001 (Greenhouse-Geisser corrected), η<sub>p</sub><sup>2</sup> = .01, and showed that male fearful items were more familiar than female items (<italic>p</italic>s &lt; .001, Bonferroni tests). The participant sex by item sex interaction was not significant, <italic>F</italic>(1, 592) = 2.13, <italic>p</italic> = .145 (see <xref ref-type="fig" rid="pone.0178423.g003">Fig 3</xref>).</p>
<fig id="pone.0178423.g003" position="float">
<object-id pub-id-type="doi">10.1371/journal.pone.0178423.g003</object-id>
<label>Fig 3</label>
<caption>
<title>Familiarity hit rates as a function of item sex and participant sex for faces, voices and face-voice combinations.</title>
<p>Error bars represent 95% <italic>CI</italic>.</p>
</caption>
<graphic mimetype="image" position="float" xlink:href="info:doi/10.1371/journal.pone.0178423.g003" xlink:type="simple"/>
</fig>
<p>For voices, the main effect of expression was significant <italic>F</italic>(5, 2960) = 9.33, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .02. Post-hoc Bonferroni tests (<italic>ps</italic> ≤ .001) revealed that neutral voices (know hit rate, <italic>M</italic> = .36) had higher familiarity rates than all the emotional voices. There was also a significant main effect of participant sex <italic>F</italic>(1, 592) = 5.42, <italic>p</italic> = .020, η<sub>p</sub><sup>2</sup> = .01, which demonstrated that female participants (<italic>M</italic> = .32) performed significantly better than male participants (.28) according to familiarity rates. The participant sex by item sex interaction effect was not significant, <italic>F</italic>(1, 592) = 2.72, <italic>p</italic> = .100, see <xref ref-type="fig" rid="pone.0178423.g003">Fig 3</xref>.</p>
<p>Finally, for know responses of face-voice combinations, we observed a main effect of expression, <italic>F</italic>(5, 2960) = 5.50, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = .01. Post-hoc Bonferroni tests (<italic>p</italic>s ≤ .02) demonstrated that the familiarity of sad face-voice combinations (know hit rate, <italic>M</italic> = .32) was statistically higher than for neutral (.28), and happy and angry (.27) expressions. We also observed significant item sex by expression, <italic>F</italic>(5, 2960) = 2.84, <italic>p</italic> = .014. η<sub>p</sub><sup>2</sup> = .01, and participant sex by expression, <italic>F</italic>(5, 2960) = 2.25, <italic>p</italic> = .047 (Greenhouse-Geisser corrected), η<sub>p</sub><sup>2</sup> = .004, interaction effects (for values, see <xref ref-type="table" rid="pone.0178423.t003">Table 3</xref>). However, for these effects post-hoc Bonferroni tests revealed no significant pairwise differences. The participant sex by item sex interaction effect was not significant <italic>F</italic>(1, 592) = 2.89, <italic>p</italic> = .090, as seen in <xref ref-type="fig" rid="pone.0178423.g003">Fig 3</xref>.</p>
</sec>
</sec>
<sec id="sec018">
<title>Additional analyses</title>
<p>Because the emotion recognition rates in the encoding tasks showed some variability across conditions, we calculated the correlation between the emotion recognition rate for each item that was included in the encoding task and its corresponding <italic>P</italic><sub>r</sub> values. This correlation was not significant, <italic>r</italic> = -.10, <italic>p</italic> = .410 (<italic>N</italic> = 72), which suggests that memory performance was not associated with the ease or difficulty of emotion recognition in our sample.</p>
</sec>
</sec>
<sec id="sec019" sec-type="conclusions">
<title>Discussion</title>
<p>This study aimed to address three interrelated research questions about the effects of sex and emotion expression on memory for faces and voices, presented alone and in combination. The first aim was to investigate own-sex bias. As predicted, results showed evidence for own-sex bias, to the effect that female participants performed relatively better for female faces compared to male participants. We also observed female own-sex bias for face-voice combinations, whereas for voices both men and women recognized female items with higher accuracy than male items. The second aim was to investigate the effects of emotion expression on recognition memory. Here, results were similar for faces, voices, and face-voice combinations, and showed that neutral expressions had higher accuracy rates compared to emotional expressions. The accuracy for specific emotions, however, differed across presentation modalities. Finally, the third aim was to use the Remember/Know paradigm to investigate how the subjective sense of recollection and familiarity varies as a function of sex and emotion expression. In contrast to the findings for memory accuracy, we observed the highest recollection rates for emotional (anger and fear) items for voices and face-voice combinations, although neutral items received the highest recollection rates for faces. This indicates that emotions may enhance the subjective sense of recollection without enhancing the accuracy of recognition memory. Finally, own-sex bias was observed for recollection rates but not for familiarity rates, which suggests that own-sex bias may depend mainly on recollection processes.</p>
<sec id="sec020">
<title>Sex effects in memory for emotional faces and voices</title>
<p>We did not observe any overall memory advantage for female participants over male participants, contrary to several studies reviewed in [<xref ref-type="bibr" rid="pone.0178423.ref002">2</xref>]. It has been suggested that women may implicitly allocate more attention to the expression of faces than men [<xref ref-type="bibr" rid="pone.0178423.ref012">12</xref>], which entails more focus on local facial features relevant to emotional expressions such as the mouth and the eyes [<xref ref-type="bibr" rid="pone.0178423.ref039">39</xref>], whereas men may process faces more holistically or globally. This reasoning has received some support from data showing that face recognition in men improved when they were explicitly instructed to attend to the expression of faces [<xref ref-type="bibr" rid="pone.0178423.ref012">12</xref>]. In the current study, the encoding task consisted of an emotion recognition task, and we speculate that this task could have increased focus on expression features for men and women alike, which could have enhanced men’s overall performance compared to the studies reviewed in [<xref ref-type="bibr" rid="pone.0178423.ref002">2</xref>].</p>
<p>It has been suggested that female neutral faces are easier to recall in part because they are more distinctive than male faces [<xref ref-type="bibr" rid="pone.0178423.ref003">3</xref>]. In the current study, both men and women overall recognized female stimuli better than male stimuli, although this advantage was not as pronounced as previous studies have reported (e.g., [<xref ref-type="bibr" rid="pone.0178423.ref003">3</xref>, <xref ref-type="bibr" rid="pone.0178423.ref010">10</xref>]). We observed recognition advantage for female neutral items for all presentation modalities, and also for several emotion categories (i.e., disgust and fear for faces and voices, and sadness for face-voice combinations). It thus appears that although the recognition advantage for female items was present also for emotional items, it was less consistent for emotional compared to neutral items. A possible explanation for this observation could be that emotional expressions make female and male items more similar in terms of their distinctiveness, but more research is needed to understand the mechanisms behind memory advantage for female items.</p>
<p>We analyzed the interaction between item sex and participant sex, replicating previous findings of own-sex bias for female participants, who displayed relatively higher memory accuracy for female vs. male faces, in comparison to male participants (e.g., [<xref ref-type="bibr" rid="pone.0178423.ref002">2</xref>, <xref ref-type="bibr" rid="pone.0178423.ref010">10</xref>]). Although the underlying causes for this effect are not fully known, it has been suggested that women may attend and react more to female faces due to complex interactions of social and biological mechanisms [<xref ref-type="bibr" rid="pone.0178423.ref002">2</xref>], resulting in higher efficiency in encoding and retrieval processes [<xref ref-type="bibr" rid="pone.0178423.ref007">7</xref>, <xref ref-type="bibr" rid="pone.0178423.ref010">10</xref>, <xref ref-type="bibr" rid="pone.0178423.ref011">11</xref>]. Regarding biological mechanisms, Lovén et al. [<xref ref-type="bibr" rid="pone.0178423.ref013">13</xref>] demonstrated that activation of fusiform gyrus–which is associated with perception of faces [<xref ref-type="bibr" rid="pone.0178423.ref040">40</xref>]–is associated with female own-sex bias during encoding of neutral faces. Furthermore, amygdala activation in socioemotional memory may differ for men and women. For women, the left amygdala is more activated during the encoding of female faces, compared to male faces, but for men, the right amygdala is more activated during the encoding of male faces [<xref ref-type="bibr" rid="pone.0178423.ref014">14</xref>]. Our results also demonstrated, for the first time, female own-sex bias in memory for face-voice combinations, but no own-sex bias was apparent for voices. It thus remains a possibility that the own-sex bias for face-voice combinations was driven mainly by the facial component. We note that it has been suggested that information about sex and identity appear to be processed independently for faces, whereas this may not be the case for voices [<xref ref-type="bibr" rid="pone.0178423.ref041">41</xref>]. How (and why) memory biases for faces and voices differ remains an exciting topic for future research.</p>
<p>Turning next to the subjective sense of recollection, remember responses indicated a female own-sex bias for both faces and face-voice combinations, but there was no memory bias for know responses. Our findings thus suggest that it is recollection processes–rather than familiarity processes–that play a key role in own-sex bias. Notably, this pattern has also been found in studies of own-group biases in face recognition [<xref ref-type="bibr" rid="pone.0178423.ref042">42</xref>], which suggests that the mechanisms behind own-sex biases may be more similar to the mechanisms of own-race biases than previously thought. According to Palmer et al. [<xref ref-type="bibr" rid="pone.0178423.ref007">7</xref>], women’s recognition of female faces relies on attention at encoding, and divided attention manipulations at this phase decreases women’s recollection ratings of female faces more than for male faces. In our study, there were no attention manipulation tasks, which facilitated recollection rates, and this may explain why we found a memory bias only for recollection, and not familiarity, rates. Importantly, our findings contribute to the growing evidence showing a robust female own-sex bias even when manipulations vary between studies [<xref ref-type="bibr" rid="pone.0178423.ref002">2</xref>].</p>
<p>Finally, our results suggested that female own-sex bias can be observed for neutral as well as emotional items. However, in the absence of item sex × participant sex × expression interactions, the pattern of results do not give any clear indications about how own-sex bias varies as a function of emotion. Effects of emotional expressions on memory are discussed in detail below.</p>
</sec>
<sec id="sec021">
<title>Effects of emotion expression on memory for faces and voices</title>
<p>While emotional expressions are generally thought to facilitate social memory [<xref ref-type="bibr" rid="pone.0178423.ref018">18</xref>], our findings suggest this may not always be the case. Using a novel design where several emotion categories and neutral stimuli were included in the same test, we instead observed higher memory accuracy for neutral stimuli than for emotional stimuli. Our results are similar to what Johansson et al. [<xref ref-type="bibr" rid="pone.0178423.ref023">23</xref>] previously observed for faces, but we extend this observation to both faces and voices, and face-voice combinations as well. This suggests that the memory advantage for neutral stimuli was not specific to any presentation modality. This finding underscores the need for further research about the conditions in which neutral and emotional expressions, respectively, facilitate memory for socioemotional information. We utilized an emotion recognition task at encoding, and we speculate that this type of task may have preferentially focused participants’ attention on emotional features, leading to less attention on the identity features of emotional stimuli. Neutral stimuli may also have attracted attention because they stood out because of their relative novelty, given that emotional stimuli outnumbered neutral stimuli by five to one in our task. We also note that neutral expressions received slightly lower emotion recognition rates in the encoding task compared to the emotional expressions. However, we found no correlation between emotion recognition accuracy and memory performance, which suggests that it is unlikely that differences in emotion recognition rates would underlie differences in memory accuracy.</p>
<p>We further documented higher memory accuracy for some emotions than for others, suggesting that the processing of emotional stimuli varied across emotion categories. In particular, memory accuracy was in general higher for happiness and fear stimuli than for the other non-neutral expressions–although we note that accuracy for specific emotions also varied across presentation modalities. We speculate that the relatively high accuracy rates for happy expressions may be due to attentional biases toward pro-social stimuli [<xref ref-type="bibr" rid="pone.0178423.ref020">20</xref>], which signal approachability and approval [<xref ref-type="bibr" rid="pone.0178423.ref017">17</xref>]. Fear expressions are instead associated with threat to one’s well-being and survival, and may attract prioritized processing resources due to their adaptive significance [<xref ref-type="bibr" rid="pone.0178423.ref022">22</xref>].</p>
<p>Interestingly, the pattern of results for the subjective sense of recollection differed from the pattern for memory accuracy rates. We observed that fearful and angry stimuli received the highest recollection rates for voices and face-voice combinations, although for faces neutral stimuli again had the highest recollection rate. In this sense, anger stimuli stood out with high rates of remember responses, but low memory accuracy–which indicates that angry items received a relatively high degree of false positives in the recognition memory task (especially for voices and face-voice combinations). It can be speculated that angry stimuli may have received relatively less attention during encoding because they signal disapproval and social threat. However, during the memory task, angry items may instead have preferentially increased participants’ arousal, thereby increasing their likelihood to choose remember responses. Regarding the subjective sense of familiarity (know rates), sad stimuli had among the lowest recollection rates, yet the highest familiarity rates. It is possible that sad expressions were perceived as signaling a lower arousal affective state and thus failed to reach conscious recollection, possibly due to a more general assessment between encoding and retrieval of sad items [<xref ref-type="bibr" rid="pone.0178423.ref023">23</xref>]. This is consistent with studies that propose that sadness prompts empathic reactions (e.g., [<xref ref-type="bibr" rid="pone.0178423.ref043">43</xref>]), resulting in recall of the expression rather than the identity. Overall, the patterns of results for recollection and familiarity rates are in line with Phelps and Sharot [<xref ref-type="bibr" rid="pone.0178423.ref044">44</xref>], who have suggested that emotion enhances the subjective feeling of recollection but does not necessarily increase memory accuracy. Neutral stimuli may instead enhance recognition accuracy–for example by evoking a variety of contextual, often perceptual, details–but do not enhance the subjective feeling of remembering to the same extent as emotional stimuli, perhaps because they are not equally associated with arousal [<xref ref-type="bibr" rid="pone.0178423.ref028">28</xref>]. Notably, we expand upon previous findings by showing that the dissociation between objective and subjective remembering can also be observed for stimuli consisting of emotionally expressive faces and voices.</p>
</sec>
<sec id="sec022">
<title>Similarities and differences between memory for face and voices</title>
<p>Our aim was to compare the pattern of results across the face, voice, and face-voice conditions, and these results have been discussed above. However, we did not aim to directly compare the level of performance in the different presentation modalities, and did not design our study for this purpose. Nevertheless, in accordance with previous research [<xref ref-type="bibr" rid="pone.0178423.ref045">45</xref>], results indicated that faces (<italic>P</italic><sub>r</sub> = .64) were more accurately remembered than face-voice combinations (<italic>P</italic><sub>r</sub> = .41), which in turn were more accurate than voices (<italic>P</italic><sub>r</sub> = .36). The advantage for faces appears strong, but we note that a presentation order effect may have contributed to this finding. We assessed faces first, which may have artificially increased accuracy relative to voices and face-voice combinations due to more fatigue in the latter conditions. The recognition memory tasks may also have been more demanding for the voice and face-voice conditions, compared to the face only condition. For example, the tasks preceding face memory test consisted of the voice emotion and the face-voice-combination emotion recognition test, whereas the tasks prior to the face-voice-combination memory test were the face memory test and the voice memory test. In addition, previous research has reported that voice recognition is more vulnerable to distractor items compared to face recognition (e.g., [<xref ref-type="bibr" rid="pone.0178423.ref033">33</xref>]), and we note that this so called face primacy effect may also have contributed to the relatively higher recognition memory for faces compared to the other presentation modalities. Finally, stimulus presentation times at encoding were not kept constant across conditions in the current study, because participants were allowed to repeat the playback of voice only stimuli as many times as needed to reach a decision. This may have preferentially affected memory for voices, and thus helps to complicate a comparison of performance levels across presentation modalities.</p>
</sec>
</sec>
<sec id="sec023">
<title>Limitations</title>
<p>The current study is subject to several limitations. First, we acknowledge that our results are based on a limited stimulus set, with few exemplars for each combination of item sex, emotion expression, and presentation modality. We therefore encourage replication attempts in order to find out how well the pattern of results generalizes to other stimulus sets. Replication using naturalistic and/or dynamic emotion expressions (e.g., short videos of persons expressing emotions through their faces and voices) would be especially worthwhile. A second potential limitation is the fact that we did not measure reaction times in the encoding and memory tasks. Future studies should include such measures in order to investigate, for example, possible sex differences in the time allotted to various emotion expressions at encoding and how such possible differences map onto sex differences in memory performance. This enterprise could lead to important information about the mechanisms underlying the observed effects of sex and emotion expression. For facial expressions, studies could further utilize eye-tracking methods to explore possible effects of sex and emotion on gaze patterns at encoding and subsequent memory performance. A third potential drawback of the present study is that participants did not rate the stimuli according to attractiveness or distinctiveness. Some studies report that unattractive faces are recalled more often than attractive faces and that this relation is mediated by distinctiveness [<xref ref-type="bibr" rid="pone.0178423.ref046">46</xref>, <xref ref-type="bibr" rid="pone.0178423.ref047">47</xref>]. Thus, it is possible that in our study attractiveness and distinctiveness account, at least partially, for the memory advantage of neutral items and not the depicted expression per se. How attractiveness and distinctiveness ratings of faces and voices vary as a function of emotional expression and item sex remains an interesting topic for future studies on social memory.</p>
</sec>
<sec id="sec024" sec-type="conclusions">
<title>Conclusions</title>
<p>Our results suggest that memory for faces and voices are influenced by the expressions that they convey, with higher accuracy for neutral vs. emotional stimuli, and that emotional displays can increase the sense of recollection without increasing accuracy. In addition, participants overall displayed higher memory accuracy for female stimuli than for male stimuli, with female own-sex bias for both faces and face-voice combinations. Own-sex memory bias possibly relies on recollection rather than familiarity processes. In conclusion, we argue that our findings highlight the importance of jointly considering effects of expressed emotion, presentation modality, and sex in studies of social memory.</p>
</sec>
<sec id="sec025">
<title>Supporting information</title>
<supplementary-material id="pone.0178423.s001" mimetype="application/vnd.openxmlformats-officedocument.wordprocessingml.document" position="float" xlink:href="info:doi/10.1371/journal.pone.0178423.s001" xlink:type="simple">
<label>S1 Table</label>
<caption>
<title>Stimulus identifiers for encoding and recognition tasks.</title>
<p>(DOCX)</p>
</caption>
</supplementary-material>
</sec>
</body>
<back>
<ack>
<p>We thank Agneta Herlitz for comments on an earlier draft of this manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="pone.0178423.ref001"><label>1</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Todorov</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Said</surname> <given-names>CP</given-names></name>, <name name-style="western"><surname>Engell</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Oosterhof</surname> <given-names>NN</given-names></name>. <article-title>Understanding evaluation of faces on social dimensions</article-title>. <source>Trends Cogn Sci</source>. <year>2008</year>;<volume>12</volume>:<fpage>455</fpage>–<lpage>460</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2008.10.001" xlink:type="simple">10.1016/j.tics.2008.10.001</ext-link></comment> <object-id pub-id-type="pmid">18951830</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref002"><label>2</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Herlitz</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Loven</surname> <given-names>J</given-names></name>. <article-title>Sex differences and the own-gender bias in face recognition: A meta-analytic review</article-title>. <source>Vis Cogn</source>. <year>2013</year>;<volume>21</volume>:<fpage>1306</fpage>–<lpage>1336</lpage>.</mixed-citation></ref>
<ref id="pone.0178423.ref003"><label>3</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rehman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Herlitz</surname> <given-names>A.</given-names></name> <article-title>Women remember more faces than men do</article-title>. <source>Acta Psychol (Amst)</source>. <year>2007</year>;<volume>124</volume>:<fpage>344</fpage>–<lpage>355</lpage>.</mixed-citation></ref>
<ref id="pone.0178423.ref004"><label>4</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Verdichevski</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Steeves</surname> <given-names>JKE</given-names></name>. <article-title>Own-age and own-sex biases in recognition of aged faces</article-title>. <source>Acta Psychol (Amst)</source>. <year>2013</year>;<volume>144</volume>:<fpage>418</fpage>–<lpage>423</lpage>.</mixed-citation></ref>
<ref id="pone.0178423.ref005"><label>5</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lewin</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Herlitz</surname> <given-names>A.</given-names></name> <article-title>Sex differences in face recognition–Women’s faces make the difference</article-title>. <source>Brain Cogn</source>. <year>2002</year>;<volume>50</volume>:<fpage>121</fpage>–<lpage>128</lpage> <object-id pub-id-type="pmid">12372357</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref006"><label>6</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rehnman</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Herlitz</surname> <given-names>A.</given-names></name> <article-title>Higher face recognition ability in girls: Magnified by own-sex and own-ethnicity bias</article-title>. <source>Memory</source>. <year>2006</year>;<volume>14</volume>:<fpage>289</fpage>–<lpage>296</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/09658210500233581" xlink:type="simple">10.1080/09658210500233581</ext-link></comment> <object-id pub-id-type="pmid">16574585</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref007"><label>7</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Palmer</surname> <given-names>MA</given-names></name>, <name name-style="western"><surname>Brewer</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Horry</surname> <given-names>R</given-names></name>. <article-title>Understanding gender bias in face recognition: Effects of divided attention at encoding</article-title>. <source>Acta Psychol (Amst)</source>. <year>2013</year>;<volume>142</volume>:<fpage>362</fpage>–<lpage>369</lpage>.</mixed-citation></ref>
<ref id="pone.0178423.ref008"><label>8</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wolff</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Kemter</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Schweinberger</surname> <given-names>SR</given-names></name>, <name name-style="western"><surname>Wiese</surname> <given-names>H</given-names></name>. <article-title>What drives social in-group biases in face recognition memory? ERP evidence from the own-gender bias</article-title>. <source>Soc Cogn Affect Neurosci</source>. <year>2014</year>;<volume>9</volume>:<fpage>580</fpage>–<lpage>590</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/scan/nst024" xlink:type="simple">10.1093/scan/nst024</ext-link></comment> <object-id pub-id-type="pmid">23474824</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref009"><label>9</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wright</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Sladden</surname> <given-names>B</given-names></name>. <article-title>An own gender bias and the importance of hair in face recognition</article-title>. <source>Acta Psychol (Amst)</source>. <year>2003</year>;<volume>114</volume>:<fpage>101</fpage>–<lpage>114</lpage>.</mixed-citation></ref>
<ref id="pone.0178423.ref010"><label>10</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lovén</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Herlitz</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Rehnman</surname> <given-names>J</given-names></name>. <article-title>Women's own-gender bias in face recognition memory: The role of attention at encoding</article-title>. <source>Exp Psychol</source>. <year>2011</year>;<volume>58</volume>:<fpage>333</fpage>–<lpage>340</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1027/1618-3169/a000100" xlink:type="simple">10.1027/1618-3169/a000100</ext-link></comment> <object-id pub-id-type="pmid">21310695</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref011"><label>11</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname> <given-names>B</given-names></name>. <article-title>Gender difference in recognition memory for neutral and emotional faces</article-title>. <source>Memory</source>. <year>2013</year>;<volume>21</volume>:<fpage>991</fpage>–<lpage>1003</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/09658211.2013.771273" xlink:type="simple">10.1080/09658211.2013.771273</ext-link></comment> <object-id pub-id-type="pmid">23432017</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref012"><label>12</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Fulton</surname> <given-names>EK</given-names></name>, <name name-style="western"><surname>Bulluck</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Hertzog</surname> <given-names>C</given-names></name>. <article-title>Orienting to face expression during encoding improves men's recognition of own gender faces</article-title>. <source>Acta Psychol (Amst)</source>. <year>2015</year>;<volume>161</volume>:<fpage>18</fpage>–<lpage>24</lpage>.</mixed-citation></ref>
<ref id="pone.0178423.ref013"><label>13</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Lovén</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Svärd</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Ebner</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Herlitz</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Fischer</surname> <given-names>H</given-names></name>. <article-title>Face gender modulates women’s brain activity during face encoding</article-title>. <source>Soc Cogn Affect Neurosci</source>. <year>2014</year>;<volume>9</volume>:<fpage>1000</fpage>–<lpage>1005</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/scan/nst073" xlink:type="simple">10.1093/scan/nst073</ext-link></comment> <object-id pub-id-type="pmid">23698075</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref014"><label>14</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Armony</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Sergerie</surname> <given-names>K</given-names></name>. <article-title>Own-sex effects in emotional memory for faces</article-title>. <source>Neurosci Lett</source>. <year>2007</year>;<volume>426</volume>:<fpage>1</fpage>–<lpage>5</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neulet.2007.08.032" xlink:type="simple">10.1016/j.neulet.2007.08.032</ext-link></comment> <object-id pub-id-type="pmid">17870235</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref015"><label>15</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname> <given-names>B</given-names></name>. <article-title>Effects of positive emotion on consolidation memory for faces: the modulation of face valence and facial gender</article-title>. <source>Memory</source>. <year>2013</year>;<volume>21</volume>:<fpage>707</fpage>–<lpage>721</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/09658211.2012.753461" xlink:type="simple">10.1080/09658211.2012.753461</ext-link></comment> <object-id pub-id-type="pmid">23240904</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref016"><label>16</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Skuk</surname> <given-names>VG</given-names></name>, <name name-style="western"><surname>Schweinberger</surname> <given-names>SR</given-names></name>. <article-title>Gender differences in familiar voice identification</article-title>. <source>Hear Res</source>. <year>2013</year>;<volume>296</volume>:<fpage>131</fpage>–<lpage>140</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.heares.2012.11.004" xlink:type="simple">10.1016/j.heares.2012.11.004</ext-link></comment> <object-id pub-id-type="pmid">23168357</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref017"><label>17</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>D'Argembeau</surname> <given-names>AD</given-names></name>, <name name-style="western"><surname>Van der Linden</surname> <given-names>M</given-names></name>. <article-title>Facial expressions of emotion influence memory for facial identity in an automatic way</article-title>. <source>Emotion</source>. <year>2007</year>;<volume>7</volume>:<fpage>507</fpage>–<lpage>515</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/1528-3542.7.3.507" xlink:type="simple">10.1037/1528-3542.7.3.507</ext-link></comment> <object-id pub-id-type="pmid">17683207</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref018"><label>18</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Leigland</surname> <given-names>LA</given-names></name>, <name name-style="western"><surname>Schulz</surname> <given-names>LE</given-names></name>, <name name-style="western"><surname>Janowsky</surname> <given-names>JS</given-names></name>. <article-title>Age related changes in emotional memory</article-title>. <source>Neurobiol Aging</source>. <year>2004</year>;<volume>25</volume>:<fpage>1117</fpage>–<lpage>1124</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neurobiolaging.2003.10.015" xlink:type="simple">10.1016/j.neurobiolaging.2003.10.015</ext-link></comment> <object-id pub-id-type="pmid">15212836</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref019"><label>19</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Liu</surname> <given-names>CH</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Ward</surname> <given-names>J</given-names></name>. <article-title>Remembering faces with emotional expressions</article-title>. <source>Front Psychol</source>. <year>2014</year>;<volume>5</volume>:<fpage>1439</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2014.01439" xlink:type="simple">10.3389/fpsyg.2014.01439</ext-link></comment> <object-id pub-id-type="pmid">25540634</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref020"><label>20</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Shimamura</surname> <given-names>AP</given-names></name>, <name name-style="western"><surname>Ross</surname> <given-names>JG</given-names></name>, <name name-style="western"><surname>Bennett</surname> <given-names>HD</given-names></name>. <article-title>Memory for facial expressions: The power of a smile</article-title>. <source>Psychon Bull Rev</source>. <year>2006</year>;<volume>13</volume>:<fpage>217</fpage>–<lpage>222</lpage>. <object-id pub-id-type="pmid">16892984</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref021"><label>21</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Righi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Marzi</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Toscani</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Baldassi</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Ottonello</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Viggiano</surname> <given-names>MP</given-names></name>. <article-title>Fearful expressions enhance recognition memory: Electrophysiological evidence</article-title>. <source>Acta Psychol (Amst)</source>. <year>2012</year>;<volume>139</volume>:<fpage>7</fpage>–<lpage>18</lpage>.</mixed-citation></ref>
<ref id="pone.0178423.ref022"><label>22</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wang</surname> <given-names>B</given-names></name>. <article-title>Facial expression influences recognition memory for faces: Robust enhancement effect of fearful expression</article-title>. <source>Memory</source>. <year>2013</year>;<volume>21</volume>:<fpage>301</fpage>–<lpage>314</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/09658211.2012.725740" xlink:type="simple">10.1080/09658211.2012.725740</ext-link></comment> <object-id pub-id-type="pmid">23016604</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref023"><label>23</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Johansson</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Mecklinger</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Treese</surname> <given-names>AC</given-names></name>. <article-title>Recognition memory for emotional and neutral faces: An event-related potential study</article-title>. <source>J Cogn Neurosci</source>. <year>2004</year>;<volume>16</volume>:<fpage>1840</fpage>–<lpage>1853</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/0898929042947883" xlink:type="simple">10.1162/0898929042947883</ext-link></comment> <object-id pub-id-type="pmid">15701233</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref024"><label>24</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Armony</surname> <given-names>JL</given-names></name>, <name name-style="western"><surname>Chochol</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Fecteau</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Belin</surname> <given-names>P</given-names></name>. <article-title>Laugh (or cry) and you will be remembered: Influence of emotional expressions on memory for vocalizations</article-title>. <source>Psychol Sci</source>. <year>2007</year>;<volume>18</volume>:<fpage>1027</fpage>–<lpage>1029</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1467-9280.2007.02019.x" xlink:type="simple">10.1111/j.1467-9280.2007.02019.x</ext-link></comment> <object-id pub-id-type="pmid">18031406</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref025"><label>25</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Aubé</surname> <given-names>W</given-names></name>, <name name-style="western"><surname>Peretz</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Armony</surname> <given-names>JL</given-names></name>. <article-title>The effects of emotion on memory for music and vocalisations</article-title>. <source>Memory</source>. <year>2013</year>;<volume>21</volume>:<fpage>981</fpage>–<lpage>990</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/09658211.2013.770871" xlink:type="simple">10.1080/09658211.2013.770871</ext-link></comment> <object-id pub-id-type="pmid">23418992</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref026"><label>26</label><mixed-citation publication-type="book" xlink:type="simple"><name name-style="western"><surname>Gardiner</surname> <given-names>JM</given-names></name>, <name name-style="western"><surname>Richardson-Klavehn</surname> <given-names>A</given-names></name>. <chapter-title>Remembering and knowing</chapter-title>. In: <name name-style="western"><surname>Tulving</surname> <given-names>E</given-names></name>, <name name-style="western"><surname>Craik</surname> <given-names>FIM</given-names></name>, editors. <source>The Oxford handbook of memory</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>2000</year>. pp. <fpage>229</fpage>–<lpage>244</lpage>.</mixed-citation></ref>
<ref id="pone.0178423.ref027"><label>27</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ochsner</surname> <given-names>KN</given-names></name>. <article-title>Are affective events richly recollected or simply familiar? The experience and process of recognizing feelings past</article-title>. <source>J Exp Psychol Gen</source>. <year>2000</year>;<volume>129</volume>:<fpage>242</fpage>–<lpage>261</lpage>. <object-id pub-id-type="pmid">10868336</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref028"><label>28</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Rimmele</surname> <given-names>U</given-names></name>, <name name-style="western"><surname>Davachi</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Petrov</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Dougal</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Phelps</surname> <given-names>E</given-names></name>. <article-title>Emotion enhances the subjective feeling of remembering, despite lower accuracy for contextual details</article-title>. <source>Emotion</source>. <year>2011</year>;<volume>11</volume>:<fpage>553</fpage>–<lpage>562</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0024246" xlink:type="simple">10.1037/a0024246</ext-link></comment> <object-id pub-id-type="pmid">21668106</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref029"><label>29</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sharot</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Delgado</surname> <given-names>MR</given-names></name>, <name name-style="western"><surname>Phelps</surname> <given-names>E</given-names></name>. <article-title>How emotion enhances the feeling of remembering</article-title>. <source>Nat Neurosci</source>. <year>2004</year>;<volume>7</volume>:<fpage>1376</fpage>–<lpage>1380</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn1353" xlink:type="simple">10.1038/nn1353</ext-link></comment> <object-id pub-id-type="pmid">15558065</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref030"><label>30</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Patel</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Girard</surname> <given-names>TA</given-names></name>, <name name-style="western"><surname>Green</surname> <given-names>RE</given-names></name>. <article-title>The influence of indirect and direct emotional processing on memory for facial expressions</article-title>. <source>Cogn Emot</source>. <year>2012</year>;<volume>26</volume>:<fpage>1143</fpage>–<lpage>1152</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/02699931.2011.642848" xlink:type="simple">10.1080/02699931.2011.642848</ext-link></comment> <object-id pub-id-type="pmid">22404425</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref031"><label>31</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Damjanovic</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Hanley</surname> <given-names>JR</given-names></name>. <article-title>Recalling episodic and semantic information about famous faces and voices</article-title>. <source>Mem Cognit</source>. <year>2007</year>;<volume>35</volume>:<fpage>1205</fpage>–<lpage>1210</lpage>. <object-id pub-id-type="pmid">18035621</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref032"><label>32</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barsics</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Brédart</surname> <given-names>S</given-names></name>. <article-title>Recalling episodic information about personally known faces and voices</article-title>. <source>Conscious Cogn</source>. <year>2011</year>;<volume>20</volume>:<fpage>303</fpage>–<lpage>308</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.concog.2010.03.008" xlink:type="simple">10.1016/j.concog.2010.03.008</ext-link></comment> <object-id pub-id-type="pmid">20381380</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref033"><label>33</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Stevenage</surname> <given-names>VS</given-names></name>, <name name-style="western"><surname>Neil</surname> <given-names>JG</given-names></name>, <name name-style="western"><surname>Barlow</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Dyson</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Eaton-Brown</surname> <given-names>C</given-names></name>, <name name-style="western"><surname>Parsons</surname> <given-names>B</given-names></name>. <article-title>The effect of distraction on face and voice recognition</article-title>. <source>Psychol Res</source>. <year>2013</year>;<volume>77</volume>:<fpage>167</fpage>–<lpage>175</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s00426-012-0450-z" xlink:type="simple">10.1007/s00426-012-0450-z</ext-link></comment> <object-id pub-id-type="pmid">22926436</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref034"><label>34</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Ebner</surname> <given-names>NC</given-names></name>, <name name-style="western"><surname>Riediger</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Lindenberger</surname> <given-names>U</given-names></name>. <article-title>FACES: A database of facial expressions in young, middle-aged, and older women and men. Development and validation</article-title>. <source>Behav Res Methods</source>. <year>2010</year>;<volume>42</volume>:<fpage>351</fpage>–<lpage>362</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BRM.42.1.351" xlink:type="simple">10.3758/BRM.42.1.351</ext-link></comment> <object-id pub-id-type="pmid">20160315</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref035"><label>35</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Laukka</surname> <given-names>P</given-names></name>, <name name-style="western"><surname>Elfenbein</surname> <given-names>HA</given-names></name>, <name name-style="western"><surname>Söder</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Nordström</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Althoff</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Chui</surname> <given-names>W</given-names></name>, <etal>et al</etal>. <article-title>Cross-cultural decoding of positive and negative non-linguistic vocalizations</article-title>. <source>Front Psychol</source>. <year>2013</year>;<volume>4</volume>:<fpage>353</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2013.00353" xlink:type="simple">10.3389/fpsyg.2013.00353</ext-link></comment> <object-id pub-id-type="pmid">23914178</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref036"><label>36</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Henningsson</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Zettergren</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Hovey</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Jonsson</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Svärd</surname> <given-names>J</given-names></name>, <name name-style="western"><surname>Cortes</surname> <given-names>DS</given-names></name>, <etal>et al</etal>. <article-title>Association between polymorphisms in NOS3 and KCNH2 and social memory</article-title>. <source>Front Neurosci</source>. <year>2015</year>;<volume>9</volume>:<fpage>393</fpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fnins.2015.00393" xlink:type="simple">10.3389/fnins.2015.00393</ext-link></comment> <object-id pub-id-type="pmid">26539080</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref037"><label>37</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Karlsson</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Henningsson</surname> <given-names>S</given-names></name>, <name name-style="western"><surname>Hovey</surname> <given-names>D</given-names></name>, <name name-style="western"><surname>Zettergren</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Jonsson</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Cortes</surname> <given-names>DS</given-names></name>, <etal>et al</etal>. <article-title>Social memory associated with estrogen receptor polymorphisms in women</article-title>. <source>Soc Cogn Affect Neurosci</source>. <year>2016</year>;<volume>11</volume>:<fpage>877</fpage>–<lpage>883</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/scan/nsw010" xlink:type="simple">10.1093/scan/nsw010</ext-link></comment> <object-id pub-id-type="pmid">26955855</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref038"><label>38</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Snodgrass</surname> <given-names>JG</given-names></name>, <name name-style="western"><surname>Corwin</surname> <given-names>J</given-names></name>. <article-title>Pragmatics of measuring recognition memory: Applications to dementia and amnesia</article-title>. <source>J Exp Psychol Gen</source>. <year>1988</year>;<volume>117</volume>:<fpage>34</fpage>–<lpage>50</lpage>. <object-id pub-id-type="pmid">2966230</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref039"><label>39</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Beaudry</surname> <given-names>O</given-names></name>, <name name-style="western"><surname>Roy-Charland</surname> <given-names>A</given-names></name>, <name name-style="western"><surname>Perron</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Cormier</surname> <given-names>I</given-names></name>, <name name-style="western"><surname>Tapp</surname> <given-names>R</given-names></name>. <article-title>Featural processing in recognition of emotional facial expressions</article-title>. <source>Cogn Emot</source>. <year>2014</year>;<volume>28</volume>:<fpage>416</fpage>–<lpage>432</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/02699931.2013.833500" xlink:type="simple">10.1080/02699931.2013.833500</ext-link></comment> <object-id pub-id-type="pmid">24047413</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref040"><label>40</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Kanwisher</surname> <given-names>N</given-names></name>, <name name-style="western"><surname>Yovel</surname> <given-names>G</given-names></name>. <article-title>The fusiform face area: A cortical region specialized for the perception of faces</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>2006</year>;<volume>361</volume>:<fpage>2109</fpage>–<lpage>2128</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2006.1934" xlink:type="simple">10.1098/rstb.2006.1934</ext-link></comment> <object-id pub-id-type="pmid">17118927</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref041"><label>41</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Burton</surname> <given-names>AM</given-names></name>, <name name-style="western"><surname>Bonner</surname> <given-names>L</given-names></name>. <article-title>Familiarity influences judgments of sex: The case of voice recognition</article-title>. <source>Perception</source>. <year>2004</year>;<volume>33</volume>:<fpage>747</fpage>–<lpage>752</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1068/p3458" xlink:type="simple">10.1068/p3458</ext-link></comment> <object-id pub-id-type="pmid">15330367</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref042"><label>42</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Horry</surname> <given-names>R</given-names></name>, <name name-style="western"><surname>Wright</surname> <given-names>DB</given-names></name>, <name name-style="western"><surname>Tredoux</surname> <given-names>CG</given-names></name>. <article-title>Recognition and context memory for faces from own and other ethnic groups: A remember-know investigation</article-title>. <source>Mem Cognit</source>. <year>2010</year>;<volume>38</volume>:<fpage>134</fpage>–<lpage>141</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/MC.38.2.134" xlink:type="simple">10.3758/MC.38.2.134</ext-link></comment> <object-id pub-id-type="pmid">20173186</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref043"><label>43</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Sergerie</surname> <given-names>K</given-names></name>, <name name-style="western"><surname>Lepage</surname> <given-names>M</given-names></name>, <name name-style="western"><surname>Armony</surname> <given-names>JL</given-names></name>. <article-title>Influence of emotional expression on memory recognition bias: A functional magnetic resonance imaging study</article-title>. <source>Biol Psychiatry</source>. <year>2007</year>;<volume>62</volume>:<fpage>1126</fpage>–<lpage>1133</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.biopsych.2006.12.024" xlink:type="simple">10.1016/j.biopsych.2006.12.024</ext-link></comment> <object-id pub-id-type="pmid">17543896</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref044"><label>44</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Phelps</surname> <given-names>EA</given-names></name>, <name name-style="western"><surname>Sharot</surname> <given-names>T</given-names></name>. <article-title>How (and why) emotion enhances the subjective sense of recollection</article-title>. <source>Curr Dir Psychol Sci</source>. <year>2008</year>;<volume>17</volume>:<fpage>147</fpage>–<lpage>152</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1467-8721.2008.00565.x" xlink:type="simple">10.1111/j.1467-8721.2008.00565.x</ext-link></comment> <object-id pub-id-type="pmid">21399743</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref045"><label>45</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Barsics</surname> <given-names>C</given-names></name>. <article-title>Person recognition is easier from faces than from voices</article-title>. <source>Psychol Belg</source>. <year>2014</year>;<volume>54</volume>:<fpage>244</fpage>–<lpage>254</lpage>.</mixed-citation></ref>
<ref id="pone.0178423.ref046"><label>46</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Zhang</surname> <given-names>Y</given-names></name>, <name name-style="western"><surname>Kong</surname> <given-names>F</given-names></name>, <name name-style="western"><surname>Chen</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Jackson</surname> <given-names>T</given-names></name>, <name name-style="western"><surname>Han</surname> <given-names>L</given-names></name>, <name name-style="western"><surname>Meng</surname> <given-names>J</given-names></name>, <etal>et al</etal>. <article-title>Identifying cognitive preferences for attractive female faces: An event-related potential experiment using a study-test paradigm</article-title>. <source>J Neurosci Res</source>. <year>2011</year>;<volume>89</volume>:<fpage>1887</fpage>–<lpage>1893</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1002/jnr.22724" xlink:type="simple">10.1002/jnr.22724</ext-link></comment> <object-id pub-id-type="pmid">21805493</object-id></mixed-citation></ref>
<ref id="pone.0178423.ref047"><label>47</label><mixed-citation publication-type="journal" xlink:type="simple"><name name-style="western"><surname>Wiese</surname> <given-names>H</given-names></name>, <name name-style="western"><surname>Altmann</surname> <given-names>CS</given-names></name>, <name name-style="western"><surname>Schweinberger</surname> <given-names>SR</given-names></name>. <article-title>Effects of attractiveness on face memory separated from distinctiveness: Evidence from event-related brain potentials</article-title>. <source>Neuropsychologia</source>. <year>2014</year>;<volume>56</volume>:<fpage>26</fpage>–<lpage>36</lpage>. <comment>doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuropsychologia.2013.12.023" xlink:type="simple">10.1016/j.neuropsychologia.2013.12.023</ext-link></comment> <object-id pub-id-type="pmid">24406982</object-id></mixed-citation></ref>
</ref-list>
</back>
</article>